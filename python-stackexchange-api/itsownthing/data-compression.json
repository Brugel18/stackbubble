{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Is it not necessary to encode both the uppercase and lowercase letter while encoding a message with the <a href="http://en.wikipedia.org/wiki/Move-to-front_transform" rel="nofollow">move-to-front transform</a>? From an old computer science course exam, the problem was to encode <code>Matt_ate_the_mat</code> starting with an empty list.</p>\n\n<p>Using the author\'s solution methodology of not taking into account <code>M</code> versus <code>m</code> one arrives at</p>\n\n<p>$C(1)C^\u2217(M)\\\\\r\nC(2)C^\u2217(a)\\\\\r\nC(3)C^\u2217(t)\\\\\r\nC(1)\\\\\r\nC(4)C^\u2217(\\_)\\\\\r\nC(3)\\\\\r\nC(3)\\\\\r\nC(5)C^\u2217(e)\\\\\r\nC(4)\\\\\r\nC(3)\\\\\r\nC(6)C^\u2217(h)\\\\\r\nC(4)\\\\\r\nC(4)\\\\\r\nC(6)\\\\\r\nC(6)\\\\\r\nC(6)$</p>\n\n<p>Seeing that move-to-front works best with items that are repeated this works to one advantage as long as the difference between <code>M</code> and <code>m</code> in the original message is not important, correct?</p>\n\n<p>Though would it not change the last encodings if taking into account <code>m</code> to $C(7)C^*(m)$ or was this done for the sake of brevity within the exam?</p>\n', 'ViewCount': '155', 'Title': 'Distinguishing between uppercase and lowercase letters in the "move-to-front" method', 'LastEditorUserId': '39', 'LastActivityDate': '2012-03-14T05:21:34.080', 'LastEditDate': '2012-03-13T17:56:07.240', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '347', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '151', 'Tags': '<combinatorics><data-compression>', 'CreationDate': '2012-03-13T17:43:13.543', 'Id': '326'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '98', 'Title': 'Why are blocking artifacts serious when there is fast motion in MPEG?', 'LastEditDate': '2012-04-21T19:32:46.183', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Bear', 'PostTypeId': '1', 'OwnerUserId': '1197', 'Body': '<p>Why are blocking artifacts serious when there is fast motion in MPEG?</p>\n\n<p>Here is the guess I made:</p>\n\n<p>In MPEG, each block in an encoding frame is matched with a block in the reference frame.\nIf the difference of two blocks is small, only the difference is encoded using DCT. Is the reason blocking artifacts are serious that the difference of two blocks is too large and DCT cut the AC component?</p>\n', 'Tags': '<information-theory><data-compression><video>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-22T21:37:32.183', 'CommentCount': '2', 'AcceptedAnswerId': '1445', 'CreationDate': '2012-04-21T10:49:15.717', 'Id': '1413'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The only paper I can find about an implementation of a lossless video codec is an <a href="http://www.ffmpeg.org/~michael/ffv1.html" rel="nofollow">article by Michael Niedermayer</a> which explains the FF1V compression pipeline.</p>\n\n<p>I\'m wondering if anyone else has found any articles similar to this. I\'ve looked on IEEE and ACM and their articles mostly focus on one algorithm that\'s part of a compression pipeline.</p>\n', 'ViewCount': '45', 'Title': 'Lossless Video Compression Pipeline', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:38:44.553', 'LastEditDate': '2012-07-18T01:38:44.553', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2171', 'Tags': '<reference-request><data-compression><video>', 'CreationDate': '2012-07-15T19:38:34.497', 'Id': '2754'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>We\'re running some benchmarks for an approximative query-answering system. It\'s sufficient to just think of it as running some SQL queries with joins. We are counting the results returned as part of the benchmark. However, the results often contain a lot of redundancy, so just counting results seems coarse.</p>\n\n<p>Consider the following table containing results for a query like "<em>for the US, give me its states and its car manufacturers</em>":</p>\n\n<pre><code>================================\n|| ?us_state | ?us_car_manu   ||\n||============================||\n|| Alabama   | Chrysler       ||\n|| Alaska    | Chrysler       ||\n|| ...         ...            ||\n|| Wyoming   | Chrysler       ||\n|| Alabama   | General Motors ||\n|| Alaska    | General Motors ||\n|| ...         ...            ||\n|| Wyoming   | General Motors ||\n|| Alabama   | Ford           ||\n|| Alaska    | Ford           ||\n|| ...         ...            ||\n|| Wyoming   | Ford           ||\n===============================\n</code></pre>\n\n<p>All 200 (50 \xd7 4) results are of course unique. However, given that there is an inherent Cartesian product, the number of results flatters the amount of "information content" or "entropy" of the table: every additional car manufacturer adds fifty results for the fifty US states. (Again, this is just an example; I\'m not interested in better ways to represent or run this particular query.)</p>\n\n<p>As such, we\'re looking for a metric that will give an indication as to the (loosely speaking) redundancy-free content in the table for better comparison of <em>content</em> across different results for different queries. Other result tables may contain a mix of different types of Cartesian products (e.g., consider generalising the query to any country, where each country itself has its own product of states and car manufacturers, etc.).</p>\n\n<p>Currently we\'re working off a simple metric which just counts unique term\u2013position combinations: for the above example, the metric gives 50 + 4 = 54. This may be sufficient for comparison, but is not sensitive to the combination of terms for individual results.</p>\n\n<p>Thanks to Wikipedia, I\'m aware of\u2014but not familiar with\u2014the notion of <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy in information theory</a>. However, I\'m unclear on how the concept of entropy could be applied to this use-case. (I\'m not interested in the entropy of the result strings; each term can be considered a "symbol".) Roughly speaking, each query variable could be considered as a free variable with the result terms in that column providing a set of possible outcomes and their frequency of occurrence being used as a probability mass function. This way I could compute the Shannon entropy for each column. But thereafter, I don\'t know how columns can be combined, or how tuples or results can be considered ... if a notion of conditional entropy would be better, etc.</p>\n\n<p>And so ...</p>\n\n<blockquote>\n  <p>Does anyone have pointers to related material on the measure of entropy/redundancy/etc. in tables or similar structures? </p>\n  \n  <p>Otherwise, does anyone have any ideas on how to use Shannon entropy in a convincing way for tabular data?</p>\n</blockquote>\n', 'ViewCount': '227', 'Title': 'Measuring entropy for a table (e.g., SQL results)', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T02:38:23.283', 'LastEditDate': '2012-10-23T02:38:23.283', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2357', 'Tags': '<reference-request><information-theory><data-compression><database-theory><entropy>', 'CreationDate': '2012-08-03T20:36:20.237', 'FavoriteCount': '1', 'Id': '3029'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '325', 'Title': 'Compression of domain names', 'LastEditDate': '2012-08-08T06:51:13.347', 'AnswerCount': '1', 'Score': '11', 'OwnerDisplayName': 'eggyal', 'PostTypeId': '1', 'OwnerUserId': '2380', 'FavoriteCount': '1', 'Body': '<p>I am curious as to how one might <em>very compactly</em> compress the domain of an arbitrary <a href="http://en.wikipedia.org/wiki/Internationalized_domain_name">IDN</a> hostname (as defined by <a href="http://tools.ietf.org/html/rfc5890">RFC5890</a>) and suspect this could become an interesting challenge. A Unicode host or domain name (U-label) consists of a string of Unicode characters, typically constrained to one language depending on the top-level domain (e.g. Greek letters under <code>.gr</code>), which is encoded into an ASCII string beginning with <code>xn--</code> (the corresponding A-label).</p>\n\n<p>One can build data models not only from the formal requirements that</p>\n\n<ul>\n<li><p>each non-Unicode label be a string matching <code>^[a-z\\d]([a-z\\d\\-]{0,61}[a-z\\d])?$</code>;</p></li>\n<li><p>each A-label be a string matching <code>^xn--[a-z\\d]([a-z\\d\\-]{0,57}[a-z\\d])?$</code>; and</p></li>\n<li><p>the total length of the entire domain (A-labels and non-IDN labels concatenated with \'.\' delimiters) not exceed 255 characters</p></li>\n</ul>\n\n<p>but also from various heuristics, including:</p>\n\n<ul>\n<li><p>lower-order U-labels are often lexically, syntactically and semantically valid phrases in some natural language including proper nouns and numerals (unpunctuated except hyphen, stripped of whitespace and folded per <a href="http://tools.ietf.org/html/rfc3491">Nameprep</a>), with a preference for shorter phrases; and</p></li>\n<li><p>higher-order labels are drawn from a dictionary of SLDs and TLDs and provide context for predicting which natural language is used in the lower-order labels.</p></li>\n</ul>\n\n<p>I fear that achieving good compression of such short strings will be difficult without considering these specific features of the data and, furthermore, that existing libraries will produce unnecessary overhead in order to accomodate their more general use cases.</p>\n\n<p>Reading Matt Mahoney\'s online book <a href="http://mattmahoney.net/dc/dce.html">Data Compression Explained</a>, it is clear that a number of existing techniques could be employed to take advantage of the above (and/or other) modelling assumptions which ought to result in far superior compression versus less specific tools.</p>\n\n<p>By way of context, this question is an offshoot from a <a href="http://stackoverflow.com/questions/7792624/producing-compact-ciphertext-of-short-strings">previous one on SO</a>.</p>\n\n<hr>\n\n<p><strong>Initial thoughts</strong></p>\n\n<p>It strikes me that this problem is an excellent candidate for offline training and I envisage a compressed data format along the following lines:</p>\n\n<ul>\n<li><p>A Huffman coding of the "<a href="http://publicsuffix.org/">public suffix</a>", with probabilities drawn from some published source of domain registration or traffic volumes;</p></li>\n<li><p>A Huffman coding of which (natural language) model is used for the remaining U-labels, with probabilities drawn from some published source of domain registration or traffic volumes given context of the domain suffix;</p></li>\n<li><p>Apply some dictionary-based transforms from the specified natural language model; and</p></li>\n<li><p>An arithmetic coding of each character in the U-labels, with probabilities drawn from contextually adaptive natural language models derived from offline training (and perhaps online too, although I suspect the data may well be too short to provide any meaningful insight?).</p></li>\n</ul>\n', 'Tags': '<algorithms><strings><natural-lang-processing><data-compression>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-31T13:51:19.050', 'CommentCount': '5', 'CreationDate': '2011-10-18T02:19:42.587', 'Id': '3056'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '615', 'Title': 'Is Huffman Encoding always optimal?', 'LastEditDate': '2012-08-14T14:08:47.543', 'AnswerCount': '3', 'Score': '5', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '0', 'Body': '<p>The requirement of the encoding to be <em>prefix free</em> results in large trees due to the tree having to be complete. Is there a threshold where fixed-length non-encoded storage of data would be more efficient than encoding the data?</p>\n', 'Tags': '<information-theory><data-compression>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-14T14:08:47.543', 'CommentCount': '4', 'AcceptedAnswerId': '3179', 'CreationDate': '2012-08-07T01:25:12.653', 'Id': '3176'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Is Huffman coding <strong>always</strong> optimal since it uses Shanon's ideas?\nWhat about text, image, video, ... compression?</p>\n\n<p>Is this subject still active in the field? What classical or modern references should I read?</p>\n", 'ViewCount': '552', 'Title': 'Is there any theoretically proven optimal compression algorithm?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-30T12:56:35.700', 'LastEditDate': '2012-08-30T12:56:35.700', 'AnswerCount': '4', 'CommentCount': '6', 'AcceptedAnswerId': '3317', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2609', 'Tags': '<algorithms><information-theory><data-compression>', 'CreationDate': '2012-08-24T17:34:44.707', 'Id': '3316'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '212', 'Title': 'Approximating the Kolmogorov complexity', 'LastEditDate': '2013-12-10T11:06:26.913', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '702', 'FavoriteCount': '3', 'Body': '<p>I\'ve studied something about the <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">Kolmogorov Complexity</a>, read some articles and books from <a href="http://homepages.cwi.nl/~paulv/kolmogorov.html" rel="nofollow">Vitanyi and Li</a> and used the concept of <a href="http://complearn.org/ncd.html" rel="nofollow">Normalized Compression Distance</a> to verify the stilometry of authors (identify how each author writes some text and group documents by their similarity).</p>\n\n<p>In that case, data compressors were used to approximate the Kolmogorov complexity, since the data compressor could be used as a Turing Machine.</p>\n\n<p>Besides data compression and programming languages (in which you would write some kind of compressor), what else could be used to approximate the Kolmogorov complexity? Are there any other approaches that could be used?</p>\n', 'Tags': '<computability><approximation><data-compression><kolmogorov-complexity>', 'LastEditorUserId': '702', 'LastActivityDate': '2013-12-10T21:28:05.640', 'CommentCount': '4', 'AcceptedAnswerId': '3531', 'CreationDate': '2012-09-11T00:02:12.553', 'Id': '3501'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a sequence of $n$ integers in a small range $[0, k)$ and all the integers have the same frequency $f$ (so the size of the sequence is $n = f * k$). What I'm trying to do now is to compress this sequence while providing random access (what is the $i$-th integer). I'm more interested in achieving high compression at the expense of higher random access times.</p>\n\n<p>I haven't tried with Huffman coding since it assigns codes based on frequencies (and all my frequencies are the same). Perhaps I'm missing some simple encoding for this particular case.</p>\n\n<p>Any help or pointers would be appreciated.</p>\n\n<p>Thanks in advance.  </p>\n", 'ViewCount': '89', 'Title': 'Compression of sequence with Direct Access', 'LastActivityDate': '2012-09-20T22:11:22.337', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4642', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2892', 'Tags': '<algorithms><data-compression>', 'CreationDate': '2012-09-20T16:41:15.050', 'Id': '4639'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '494', 'Title': 'Minimum space needed to sort a stream of integers', 'LastEditDate': '2012-10-22T19:53:01.833', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4315', 'FavoriteCount': '1', 'Body': '<p>This question has gotten a lot of attention on SO:<br>\n<a href="http://stackoverflow.com/questions/12748246/sorting-1-million-8-digit-numbers-in-1mb-of-ram">Sorting 1 million 8-digit numbers in 1MB of RAM</a></p>\n\n<p>The problem is to sort a stream of 1 million 8-digit numbers (integers in the range $[0,\\: 99\\mathord{,}999\\mathord{,}999]$) using only 1 MB of memory ($2^{20}$ bytes = $2^{23}$ bits) and no external storage. The program must read values from an input stream and write the sorted result to an output stream.</p>\n\n<p>Obviously the entire input can\'t fit into memory, but clearly the result can be represented in under 1 MB since $2^{23} \\geq \\log_2 \\binom{10^8}{10^6} \\approx 8079302$ (it\'s a tight fit).</p>\n\n<p>So, what is the minimum amount of space needed to sort n integers with duplicates in this streaming manner, and is there an algorithm to accomplish the specified task?</p>\n', 'Tags': '<algorithms><sorting><space-complexity><data-compression><streaming-algorithm>', 'LastEditorUserId': '4315', 'LastActivityDate': '2012-10-23T10:13:54.180', 'CommentCount': '5', 'AcceptedAnswerId': '6246', 'CreationDate': '2012-10-22T18:13:46.890', 'Id': '6236'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>One can compress data with straight-line grammars. An algorithm that employs this technique is called <em>Sequitur</em>. If I understood correctly, Sequitur basically starts with one rule representing the input and then does these three steps till the grammar does not change anymore:</p>\n\n<ol>\n<li>For each rule, try to find any sequences of symbols in any other rule that match the rule's right hand side and replace these sequences by the rules left hand side.</li>\n<li>For each pair of adjacent symbols in any right hand side, find all non-overlapping other pairs of adjacent symbols that are equal to the original pair. If there are any other pairs, add a new nonterminal, replace all occurrences of these pairs by the new nonterminal and add a new rule that defines the nonterminal.</li>\n<li>For each nonterminal that appears exactly once on all right-hand sides of all rules, replace its occurrence by its definition, remove the nonterminal and the rule that defines it.</li>\n</ol>\n\n<p>For each (non-empty) input, can one guarantee that the above algorithm terminates?</p>\n", 'ViewCount': '251', 'Title': 'Will this algorithm terminate on any input?', 'LastEditorUserId': '2280', 'LastActivityDate': '2013-01-28T22:04:37.830', 'LastEditDate': '2012-10-29T06:14:04.147', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2280', 'Tags': '<algorithms><algorithm-analysis><formal-grammars><data-compression><correctness-proof>', 'CreationDate': '2012-10-28T21:15:53.433', 'FavoriteCount': '1', 'Id': '6360'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '313', 'Title': 'No compression algorithm can compress all input messages?', 'LastEditDate': '2012-12-21T09:47:42.073', 'AnswerCount': '3', 'Score': '5', 'OwnerDisplayName': 'Jack M', 'PostTypeId': '1', 'OwnerUserId': '5153', 'Body': '<p>I just started reading a book called Introduction to Data Compression, by Guy E. Blelloch. On page one, he states:</p>\n\n<blockquote>\n  <p>The truth is that if any one message is shortened by an algorithm, then some other message needs to be lengthened. You can verify this in practice by running GZIP on a GIF file. It is, in fact, possible to go further and show that for a set of input messages of fixed length, if one message is compressed, then the average length of the compressed messages over all possible inputs is always going to be longer than the original input messages. </p>\n  \n  <p><strong>Consider, for example, the 8 possible 3 bit messages. If one is compressed to two bits, it is not hard to convince yourself that two messages will have to expand to 4 bits, giving an average of 3 1/8 bits.</strong></p>\n</blockquote>\n\n<p>Really? I find it very hard to convince myself of that. In fact, here\'s a counter example. Consider the algorithm which accepts as input any 3-bit string, and maps to the following outputs:</p>\n\n<pre><code>000 -&gt; 0\n001 -&gt; 001\n010 -&gt; 010\n011 -&gt; 011\n100 -&gt; 100 \n101 -&gt; 101\n110 -&gt; 110\n111 -&gt; 111\n</code></pre>\n\n<p>So there you are - no input is mapped to a longer output. There are certainly no "two messages" that have expanded to 4 bits.</p>\n\n<p>So what exactly is the author talking about? I suspect either there\'s some implicit caveat that just isn\'t obvious to me, or he\'s using language that\'s far too sweeping.</p>\n\n<p><em>Disclaimer:</em> I realize that if my algorithm is applied iteratively, you do indeed lose data. Try applying it twice to the input 110: 110 -> 000 -> 0, and now you don\'t know which of 110 and 000 was the original input. However, if you apply it only once, it seems lossless to me. Is that related to what the author\'s talking about?</p>\n', 'Tags': '<data-compression><coding-theory>', 'LastEditorUserId': '1636', 'LastActivityDate': '2012-12-22T02:24:58.747', 'CommentCount': '2', 'AcceptedAnswerId': '7532', 'CreationDate': '2012-12-21T00:55:54.680', 'Id': '7531'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a set of strings.  My goal is to find a minimal set of longest prefixes which will match most of that set.</p>\n\n<p>For instance, if my set is:</p>\n\n<pre><code>abracadabra\nabracado\nabramu\nbanana\nbananasplit\nbananaicecream\nxylophone\nzebra\nzeitgeist\nzello\n</code></pre>\n\n<p>I would want the output to be:</p>\n\n<pre><code>banana (len 6, matches 3)\nabra (len 4, matches 3)\nxylophone (len 9, matches 1)\nze (len 2, matches 3)\n</code></pre>\n\n<p>Now, this question isn't yet properly specified.  That's because I'm ranking my results on two dimensions: maximum matches, and maximum length.  My goal is to find prefixes that cover as much as of the set as possible, which are as long as possible and thus less likely to occur in strings that <em>aren't</em> in the set (all other things being equal, of course).</p>\n\n<p>Ideally, I'd like to find a set of very long strings, ranked by how much of the set they cover.</p>\n\n<p>That's my goal.  Now I'll present my work, and where I need help.</p>\n\n<p><strong>FIRST</strong> Let's specify the problem better.  We want a set of prefixes.  For each prefix, we compute its length, and the number of matches it has in the set, and order the prefixes by their product.  I'm then free to pick the top X prefixes.</p>\n\n<p>I think that's a good specification.</p>\n\n<p><strong>NOW</strong> Comes an efficient implementation.  Brute force is to check every possible prefix against every string, which is complexity n * n * m (n being number of strings, m being average length of strings).</p>\n\n<p>An efficient algorithm?<br>\nSomething like this:   </p>\n\n<ol>\n<li>Build a prefix tree of the set</li>\n<li>Each leaf has value 1</li>\n<li>Work up the tree, with each parent equal to sum of its children , plus 1 if it has an entry</li>\n<li>Now we know each prefix and how many matches it has - I believe complexity is n log n</li>\n<li>Walk through the tree, counting length of each string (complexity n * m)</li>\n<li>And collect all the entries, sort them by length * value (complexity n log n)</li>\n</ol>\n\n<p>That algorithm is roughly n log n, which is efficient enough.  Will it work? How should it be improved? What's a simple way to implement it? </p>\n\n<p>Finally: Since all the data is in a Postgres relational database, I believe it would be simplest to do the algorithm using relational algebra with aggregate functions. Comments on this?</p>\n", 'ViewCount': '216', 'Title': 'Algorithm for determining minimal set of covering prefixes', 'LastEditorUserId': '1623', 'LastActivityDate': '2013-01-11T14:38:16.920', 'LastEditDate': '2013-01-10T16:50:17.210', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5368', 'Tags': '<algorithms><trees><data-compression><sets><coding-theory>', 'CreationDate': '2013-01-10T13:34:14.843', 'Id': '7868'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose you have a sequence generated by an i.i.d. process (such as repeatedly rolling a die and recording the values in order) parameterized by some K-dimensional vector $\\vec{\\gamma}$ (the probabilities associated with each side of the die), which is unknown. But, if you assume some distribution $Q$ on $\\vec{\\gamma}$, you can calculate the probability of a sequence (where $n(k)$ is the number of times symbol $k$ appears in the sequence): $$P(seq) = \\int P(seq|\\vec{\\gamma}')Q(\\vec{\\gamma}')d\\vec{\\gamma}' = \\int \\prod_{k=1}^K \\gamma_k' ^{n(k)}Q(\\vec{\\gamma}')d\\vec{\\gamma}'. $$\nThen $-\\log P(seq)$ is the code length. I believe this code length is equal to $$-n\\sum_{k=1}^K \\gamma_k\\log \\gamma_k+\\frac{K-1}{2}\\log n +O(1),$$ where the $O(1)$ term approaches a constant as $n \\rightarrow \\infty$. Can someone help me find the $O(1)$ term, or its value as $n\\rightarrow \\infty$ in terms of $\\vec{\\gamma}$, $n$ and $k$ (Or, even just proving that this limit exists would be good too.)?\n$n$ being the length of the sequence, $n=\\sum_{k=1}^K n(k)$.</p>\n", 'ViewCount': '42', 'Title': 'Bayesian Coding', 'LastEditorUserId': '683', 'LastActivityDate': '2013-01-29T16:24:58.593', 'LastEditDate': '2013-01-29T16:24:58.593', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6605', 'Tags': '<data-compression>', 'CreationDate': '2013-01-29T12:17:19.613', 'Id': '9274'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I find this a bit difficult to describe, but I am interested in the following idea :</p>\n\n<p>The LZ algorithm factors (verb) an input stream into adjacent factors, these are by definition the maximal prefixes of the piece of text that occur in the previous text (or equivalently the previous concatenation of LZ factors). </p>\n\n<p>I know and believe that in the long term (given infinite input and infinite window) this coding scheme can achieve the Shannon limit, that it will find all repeat patterns that exist. </p>\n\n<p>However in <em>any given finite text</em> (but with an unbounded window) how optimal is this?</p>\n\n<p>Do the choice of factors earlier in the input have potential detrimental effects later on? For instance, could LZ converge to a choice of factors that omits certain larger factors, or factor-choices that would result in a better cover of the input (i.e. a choice of factors that  cover more of the text?). </p>\n\n<p>Or is the optimality of LZ only constrained by the window limit, and the finite nature of a text? Please provide some kind of hand waving or intuitive proof.</p>\n', 'ViewCount': '142', 'Title': 'How optimal is Lempel-Ziv at reaching the Shannon limit?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-31T14:25:17.583', 'LastEditDate': '2013-01-30T21:52:12.133', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '9356', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4602', 'Tags': '<complexity-theory><combinatorics><data-compression><lempel-ziv>', 'CreationDate': '2013-01-30T15:05:50.820', 'Id': '9309'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have been experimenting with <a href="https://en.wikipedia.org/wiki/LZ77" rel="nofollow">LZ77</a> (naively $O(n^2)$ runtime, infinite window). Applying it to the 7th Fibonacci word $abaababaabaab$ yields the correct LZ factorization:</p>\n\n<p>$\\qquad a,b,a,aba,baaba,ab$.</p>\n\n<p>My question is about the behavior of LZ77 if we iterate it. My experiments suggest that reapplication of LZ77 to the input will yield no further patterns that were not found the first time. </p>\n\n<p>By reapplication I mean, where in the first instance we treat the factors of the string as the sequence of unit symbols \'a\' and \'b\', in the second application the factors are the LZ factors. I was hoping to discover (over larger various texts, like the Complete Sonnets of Shakespeare) increasing gains, and possibly, "multilevel" patterns found by LZ over the sequence of factors of the previous iterate. But none of this occurred. The sequence of factors after the second iteration is exactly the same as the first.</p>\n\n<p>So where is the bug in my thinking? Is there a simple proof of this given the definition of an LZ factor being the longest prefix from the current position occurring in the concatenation of the preceding LZ factors?</p>\n', 'ViewCount': '111', 'Title': 'Behavior of iterative application of LZ77', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-15T07:17:29.540', 'LastEditDate': '2013-04-15T07:17:29.540', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '5', 'OwnerDisplayName': 'Cris Stringfellow', 'PostTypeId': '1', 'OwnerUserId': '4602', 'Tags': '<strings><data-compression><lempel-ziv>', 'CreationDate': '2013-01-30T14:39:17.333', 'Id': '9311'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '100', 'Title': 'Lossless data compression must make some messages longer?', 'LastEditDate': '2013-02-19T23:50:53.463', 'AnswerCount': '2', 'Score': '4', 'OwnerDisplayName': 'Legendre', 'PostTypeId': '1', 'OwnerUserId': '6956', 'Body': '<p>I read on Wikipedia and in lecture notes that if a lossless data compression algorithm makes a message shorter, it must make another message longer.</p>\n\n<p>E.g. In this set of notes, it says:</p>\n\n<blockquote>\n  <p>Consider, for example, the 8 possible 3 bit messages. If one is\n  compressed to two bits, it is not hard to convince yourself that two\n  messages will have to expand to 4 bits, giving an average of 3 1/8\n  bits.</p>\n</blockquote>\n\n<p>There must be a gap in my understand because I thought I could compress all 3 bit messages this way: </p>\n\n<ul>\n<li>Encode: If it starts with a zero, delete the leading zero.</li>\n<li>Decode: If message is 3 bit, do nothing. If message is 2 bit, add a\nleading zero.</li>\n<li>Compressed set: 00,01,10,11,100,101,110,111</li>\n</ul>\n\n<p>What am I getting wrong? I am new to CS, so maybe there are some rules/conventions that I  missed?</p>\n', 'ClosedDate': '2013-02-21T05:49:18.790', 'Tags': '<data-structures><data-compression>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-02-19T23:50:53.463', 'CommentCount': '0', 'AcceptedAnswerId': '9939', 'CreationDate': '2013-02-19T17:32:59.783', 'Id': '9938'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to think if we can have an injective mapping $\\mathcal{f}$, say something like $\\Sigma_{bool}= \\{0,1\\}^* $ to $\\Sigma_{5} = \\{0,1,2,3,4\\}^* $ such that $|x| \\leq 2.|\\mathcal{f}(x)|$</p>\n\n<p>The motivation here is simple, consider I have a word $110110110110110110110$, so i can compress and represent that as $(110)^7$ or $110,111$ so what can i do if i am allowed to use symbols from $\\Sigma_5$?</p>\n\n<p>I hope my question made sense :)</p>\n', 'ViewCount': '81', 'Title': 'To what factor can I compress information from binary to Quinary', 'LastEditorUserId': '9612', 'LastActivityDate': '2013-08-20T14:39:13.760', 'LastEditDate': '2013-08-20T14:39:13.760', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13831', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9736', 'Tags': '<formal-languages><data-compression>', 'CreationDate': '2013-08-19T12:04:59.993', 'Id': '13817'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'll phrase my question using an intuitive and rather extreme example:</p>\n\n<p><strong>Is the expected compression ratio (using zip compression) of a children's book higher than that of a novel written for adults?</strong></p>\n\n<p>I read somewhere that specifically the compression ratio for zip compression can be considered an indicator for the information (as interpreted by a human being) contained in a text. Can't find this article anymore though.</p>\n\n<p>I am not sure how to attack this question. Of course no compression algorithm can grasp the meaning of verbal content. So what would a zip compression ratio reflect when applied to a text? Is it just symbol patterns - like word repetitions will lead to higher ratio - so basically it would just reflect the vocabulary?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>Another way to put my question would be whether there is a correlation which goes beyond repetition of words / restricted vocabulary.</p>\n", 'ViewCount': '100', 'Title': 'Is there a correlation of zip compression ratio and density of information provided by a text?', 'LastEditorUserId': '9994', 'LastActivityDate': '2013-09-06T01:57:55.437', 'LastEditDate': '2013-09-05T19:05:07.420', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '14161', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9994', 'Tags': '<information-theory><data-compression>', 'CreationDate': '2013-09-05T13:16:22.927', 'Id': '14150'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<pre><code>library(ggplot2)\n\ncompress &lt;- function(str) {\n  length(memCompress(paste(rep("a", str), collapse=""), type="bzip2"))\n  / nchar(paste(rep("a", str), collapse=""))\n}\n\ncr &lt;- data.frame(i = 1:10000, r = sapply(1:10000, compress))\n\nggplot(cr[cr$i&gt;=5000 &amp; cr$i&lt;=10000,], aes(x=i, y=r)) + geom_line()\n</code></pre>\n\n<p><img src="http://i.stack.imgur.com/4xuqJ.png" alt="enter image description here"></p>\n\n<p>The compression ratio starts out at 37 for "a" and hits the break-even at 39 "a"s (compression ratio = 1). The chart starts out pretty smooth and gets edgy suddenly for 98 "a"s and from there at increasingly small intervals.</p>\n\n<p>The local lows and smooth sections seem also quite erratic and random. Can somebody explain to me why bzip2 shows this behaviour in this example?</p>\n', 'ViewCount': '147', 'Title': 'Why is compression ratio using bzip2 for a sequence of "a"s so jumpy?', 'LastActivityDate': '2013-09-09T18:42:17.547', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14238', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '9994', 'Tags': '<data-compression>', 'CreationDate': '2013-09-09T13:11:14.557', 'Id': '14233'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have a compressed file and it is not possible to compress it more without loss of information. We say that this file is random or pseudorandom.</p>\n\n<p>So, if the randomness means not comprehensible and not compressible, I don't understand why ths file is, at the same time, information that my computer and I can understand.</p>\n\n<p>This file could be a book that my computer can show to me and read, and I can read and sum it ...so, it is really randomness?</p>\n\n<p>Note: I understand that if I can make a summary of a text or define it with less words, that not means that it could be possible to get all the information of this book again, of course but this book is not random for me.</p>\n\n<p>Note II: I undesrtand ramdoness as something that is not possible to reproduce with an smaller algorithm. I mean a string is random when I can't find an other smaller string that is an algorithm that can reproduce the first one.    </p>\n\n<p>Note III: I want to thank you all for your help. </p>\n", 'ViewCount': '155', 'Title': 'compressed information = randomness?', 'LastEditorUserId': '9631', 'LastActivityDate': '2013-10-04T07:52:52.077', 'LastEditDate': '2013-10-04T07:52:52.077', 'AnswerCount': '3', 'CommentCount': '13', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9631', 'Tags': '<information-theory><data-compression>', 'CreationDate': '2013-10-03T07:32:44.930', 'FavoriteCount': '0', 'Id': '14772'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I wish to estimate the compression ratio of a stream of bytes. Specifically, I\'m interested in <a href="http://en.wikipedia.org/wiki/DEFLATE" rel="nofollow">DEFLATE</a> compression. I\'m looking for some algorithm/heuristic that can estimate roughly the compressibility (more quickly than just deflating the stream). </p>\n\n<p>My main interest is not on the absolute value of the compression (though it would also be useful) but on comparison: given two streams, guess which will be better compressed.</p>\n\n<p>Another nice-to-have feature: be able to update the estimation incrementally.</p>\n\n<p>For the record: just estimating the zero-order entropy, by computing the frequencies of each byte value, does not perform well enough for me.</p>\n', 'ViewCount': '30', 'Title': 'Approximate estimation of stream compressibility?', 'LastEditorUserId': '268', 'LastActivityDate': '2013-11-28T15:14:34.670', 'LastEditDate': '2013-11-28T15:14:34.670', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11682', 'Tags': '<data-compression>', 'CreationDate': '2013-11-28T14:28:32.127', 'Id': '18443'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>The following array occupies 10000 slots in memory:</p>\n\n<pre><code>a = [0,1,2,3,4,5,6,7,8,9,10,...,10000]\n</code></pre>\n\n<p>But one could easily represent the same array as:</p>\n\n<pre><code>a = {len:10000, get: \u03bb idx -&gt; idx}\n</code></pre>\n\n<p>Which is much more compact. Similarly, there are several arrays that can be represented compactly:</p>\n\n<pre><code>a = {a:1000, get: \u03bb idx -&gt; idx * 2}\nIs a description for [0,2,4,6,8,10,...,2000]\n\na = {a:1000, get \u03bb idx -&gt; idx ^ 2}\nIs a description for [0,1,2,4,9,...1000000]\n\nAnd so on...\n</code></pre>\n\n<p>Providing so many arrays can be represented in much shorter ways than storing each element on memory, I ask:</p>\n\n<ol>\n<li>Is there a name for this phenomena?</li>\n<li>Is there a way to find the minimal representation for a specific array?</li>\n<li>Considering this probably depends on the language of description (in this case, I used an imaginary programming language with functions, objects and math operators). Is there a specific language that is optimal for finding that kind of minimal description for objects?</li>\n</ol>\n', 'ViewCount': '91', 'Title': 'How to find the minimal description for an array?', 'LastActivityDate': '2014-01-28T15:05:22.407', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><complexity-theory><formal-languages><programming-languages><data-compression>', 'CreationDate': '2014-01-04T23:53:15.990', 'Id': '19501'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've not clear how to pass from final interval to code value, for example:</p>\n\n<p>Suppose we have the set of <b>symbols</b>={0,1,2,3} with <b>probability</b>={0.2, 0.5, 0.2 , 0.1} and that we have to encode a <b>source</b> S={2,1,0,0,1,3};</p>\n\n<p>After the encoding process we'll have an interval $[ 0.7426, 0.7428 ) $.</p>\n\n<p>Now the final step is to find the shortest representation to transmit and in the book the chosen value is <b>0.10111110001</b> = <b>0.74267578125</b>.</p>\n\n<blockquote>\n  <p>How is it possible to calculate the shortest representation and the code value to transmit having the final interval?</p>\n</blockquote>\n", 'ViewCount': '37', 'Title': 'Arithmetic code: from interval to code value', 'LastEditorUserId': '4765', 'LastActivityDate': '2014-01-28T14:45:38.593', 'LastEditDate': '2014-01-24T14:47:50.177', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<information-theory><data-compression>', 'CreationDate': '2014-01-24T13:56:58.043', 'Id': '19939'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given normally distributed integers with a mean of 0 and a standard deviation $\\sigma$ around 1000, how do I compress those numbers (almost) perfectly?  Given the entropy of the Gaussian distribution, it should be possible to store any value $x$ using $$\\frac{1}{2} \\mathrm{log}_2(2\\pi\\sigma^2)+\\frac{x^2}{2\\sigma^2}\\rm{log}_2\\rm{e}$$<br>\nbits.  The way to accomplish this perfect compression would be <a href="http://en.wikipedia.org/wiki/Arithmetic_coding" rel="nofollow">arithmetic coding</a>.  In principle it\'s not too hard, I can calculate the interval boundaries from the <a href="http://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="nofollow">cumulative distribution function</a> of the gaussian.  In practice I hit considerable difficulties because when using floating point operations I cannot achieve perfect reproduction of the results, and I have no idea how to do this without FP operations.  Perfect reproduction is necessary because the uncompressing code must come up with exactly the same interval boundaries as the compressing code.  So the question is:  How do I compute the interval boundaries?  Or is there any other way to achieve (near) perfect compression of such data?</p>\n\n<p><strong>Edit:</strong>  As Raphael said, strictly speaking the normal distribution is defined only for continuous variables.  So what I mean here are integers x with a probability distribution function $$P(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\rm e^{-\\frac{x^2}{2\\sigma^2}}$$.  </p>\n\n<p><strong>Edit2:</strong>  As Yuval said, this distribution does not sum up exactly to 1, however, for $\\sigma&gt;100$  the difference from 1 is less than $10^{-1000}$ and hence it\'s more precise than any practical calculation would be.</p>\n', 'ViewCount': '101', 'Title': 'Compressing normally distributed data', 'LastEditorUserId': '12710', 'LastActivityDate': '2014-03-04T06:50:22.477', 'LastEditDate': '2014-01-31T17:46:33.920', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22261', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12710', 'Tags': '<information-theory><randomness><data-compression><entropy>', 'CreationDate': '2014-01-31T14:34:01.367', 'Id': '20156'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If my goal were to compress say 10,000 images and I could include a dictionary or some sort of common database that the compressed data for each image would reference, could I use a large dictionary shared by the entire catalog and therefore get much smaller file sizes?  Could this be expanded to work with images in general, i.e. to replace something like JPEG?</p>\n\n<p>Are there existing compression systems that operate like this, where there is a large common set of bits transmitted and loaded before decompression, that has been built by analyzing many images?</p>\n\n<p>For example, is there an existing computer science/machine learning research effort using sparse autoencoding over a large set of images and this concept of distributing a network derived from that encoding with the decompressor?</p>\n\n<p>Note: I have deleted quite a bit of context from this question because it was claimed that this made the question too "opinionated".  I also had to edit the title after someone else modified the title in such a way as to change the meaning of the question.  If you are looking for clarification about my question, please ask.</p>\n', 'ViewCount': '165', 'Title': 'Does there exist a data compression algorithm that uses a large dataset distributed with the encoder/decoder?', 'LastEditorUserId': '15323', 'LastActivityDate': '2014-04-06T22:59:48.903', 'LastEditDate': '2014-03-07T02:16:22.923', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15323', 'Tags': '<machine-learning><neural-networks><data-compression><computer-vision>', 'CreationDate': '2014-03-05T21:00:31.943', 'Id': '22317'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Which one is better approach do data compression before encrypting it  or Encrypt data before compression ? I think it is biased towards the application requirement.  Please share your views.</p>\n', 'ViewCount': '64', 'Title': 'Data Compression: Which one is Better "Compress before Encrypt" or "Encrypt before Compression"', 'LastActivityDate': '2014-03-10T02:10:22.340', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '15399', 'Tags': '<data-compression><encryption>', 'CreationDate': '2014-03-07T18:48:22.273', 'Id': '22378'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the following Huffman trees:<br>\n<img src="http://i.stack.imgur.com/Q3bzR.png" alt="enter image description here"></p>\n\n<p>I was asked if those trees can have the same corpus. My answer was no, based on these calculations:  </p>\n\n<p>For the right tree:<br>\n$a_1 \\le a_2$<br>\n$a_1 + a_2 \\le a_5$<br>\n$a_3 \\le a_4$<br>\n$a_1 + a_2 + a_5 \\le a_3 + a_4$</p>\n\n<p>For the left tree:<br>\n$a_1 \\le a_2$<br>\n$a_3 \\le a_4$<br>\n$a_1 + a_2 + a_3 + a_4 \\le a_5$  </p>\n\n<p>Adding the last equations from each tree we have that:<br>\n$2a_1 + 2a_2 \\le 0$ Which is a contradiction because frequency cannot be negative.</p>\n\n<p>Nevertheless, I understood that there is a possibility that the two trees would have the same corpus. For instance, consider $1,1,1,2,3$.</p>\n\n<p>So, where do my calculations go wrong?</p>\n', 'ViewCount': '18', 'Title': 'Do the two huffman trees have the same corpus?', 'LastEditorUserId': '12859', 'LastActivityDate': '2014-03-11T02:11:18.050', 'LastEditDate': '2014-03-11T02:11:18.050', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22463', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15485', 'Tags': '<data-structures><trees><information-theory><data-compression>', 'CreationDate': '2014-03-10T10:19:44.723', 'Id': '22459'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2223', 'LastEditorDisplayName': 'user15782', 'Title': 'Can PRNGs be used to magically compress stuff?', 'LastEditDate': '2014-03-26T15:26:43.373', 'AnswerCount': '6', 'Score': '23', 'OwnerDisplayName': 'user15782', 'PostTypeId': '1', 'FavoriteCount': '3', 'Body': '<p>This idea occurred to me as a kid learning to program and\non first encountering PRNG\'s. I still don\'t know how realistic\nit is, but now there\'s stack exchange.</p>\n\n<p>Here\'s a 14 year-old\'s scheme for an amazing compression algorithm: </p>\n\n<p>Take a PRNG and seed it with seed <code>s</code> to get a long sequence \nof pseudo-random bytes. To transmit that sequence to another party, \nyou need  only communicate a description of the PRNG, the appropriate seed \nand the length of the message. For a long enough sequence, that \ndescription would be much shorter then the sequence itself.</p>\n\n<p>Now suppose I could invert the process. Given enough time and \ncomputational resources, I could do a brute-force search and find \na seed (and PRNG, or in other words: a program) that produces my\ndesired sequence (Let\'s say an amusing photo of cats being mischievous).</p>\n\n<p>PRNGs repeat after a large enough number of bits have been generated,\nbut compared to "typical" cycles my message is quite short so this \ndosn\'t seem like much of a problem.</p>\n\n<p>Voila, an effective (if rube-Goldbergian) way to compress data.</p>\n\n<p>So, assuming:</p>\n\n<ul>\n<li>The sequence I wish to compress is finite and known in advance.</li>\n<li>I\'m not short on cash or time (Just as long as a finite amount \nof both is required)</li>\n</ul>\n\n<p>I\'d like to know:</p>\n\n<ul>\n<li>Is there a fundamental flaw in the reasoning behind the scheme? </li>\n<li>What\'s the standard way to analyse these sorts of thought experiments?</li>\n</ul>\n\n<p><em>Summary</em></p>\n\n<p>It\'s often the case that good answers make clear not only the answer, \nbut what it is that I was really asking. Thanks for everyone\'s patience \nand detailed answers. </p>\n\n<p>Here\'s my nth attempt at a summary of the answers:</p>\n\n<ul>\n<li>The PRNG/seed angle doesn\'t contribute anything, it\'s no more \nthen a program that produces the desired sequence as output.</li>\n<li>The pigeonhole principle: There are many more messages of \nlength > k then there are (message generating) programs of \nlength &lt;= k. So some sequences simply cannot be the output of a \nprogram shorter then the message. </li>\n<li>It\'s worth mentioning that the interpreter of the program \n(message) is necessarily fixed in advance. And it\'s design \ndetermines the (small) subset of messages which can be generated\nwhen a message of length k is received.</li>\n</ul>\n\n<p>At this point the original PRNG idea is already dead, but there\'s \nat least one last question to settle:</p>\n\n<ul>\n<li>Q: Could I get lucky and find that my long (but finite) message just \nhappens to be the output of a program of length &lt; k bits?</li>\n</ul>\n\n<p>Strictly speaking, it\'s not a matter of chance since the \nmeaning of every possible message (program) must be known \nin advance. Either it <em>is</em> the meaning of some message \nof &lt; k bits <em>or it isn\'t</em>.</p>\n\n<p>If I choose a random message of >= k bits randomly (why would I?),\nI would in any case have a vanishing probability of being able to send it\nusing less then k bits, and an almost certainty of not being able \nto send it at all using less then k bits.</p>\n\n<p>OTOH, if I choose a specific message of >= k bits from those which\nare the output of a program of less then k bits (assuming there is \nsuch a message), then in effect I\'m taking advantage of bits already\ntransmitted to the receiver (the design of the interpreter), which \ncounts as part of the message transferred.</p>\n\n<p>Finally:</p>\n\n<ul>\n<li>Q: What\'s all this <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy</a>/<a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">kolmogorov complexity</a> business?</li>\n</ul>\n\n<p>Ultimately, both tell us the same thing as the (simpler) piegonhole \nprinciple tells us about how much we can compress: perhaps \nnot at all, perhaps some, but certainly not as much as we fancy\n(unless we cheat).</p>\n', 'Tags': '<information-theory><randomness><data-compression>', 'LastActivityDate': '2014-03-26T15:26:43.373', 'CommentCount': '13', 'AcceptedAnswerId': '23020', 'CreationDate': '2014-03-24T17:02:03.550', 'Id': '23010'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I thought the Gibbs phenomenom is the result of Fourier analysis estimation (but was it Fourier Series estimation or can it also be Fourier Transform estimation?)</p>\n\n<p><img src="http://i.stack.imgur.com/A1Vt2.png" alt="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Gibbs_phenomenon_50.svg/285px-Gibbs_phenomenon_50.svg.png"></p>\n\n<p>JPEG uses the Discrete cosines Transform. A DCT is similar to a Fourier transform in the sense that it produces a kind of spatial frequency spectrum.</p>\n\n<p><img src="http://i.stack.imgur.com/H2AuR.jpg" alt="JPEG example JPG RIP 001.jpg  Lowest quality"> </p>\n\n<p>But what are the differences between the Gibbs phenomenom artefacts from Fourier and the artefacts from the Discrete Cosine?</p>\n', 'ViewCount': '32', 'Title': 'What are all the differences between the Gibbs phenomenon artefects and JPEG artefacts?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T02:14:05.370', 'LastEditDate': '2014-03-28T12:46:38.770', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16228', 'Tags': '<image-processing><data-compression><graphics><signal-processing>', 'CreationDate': '2014-03-28T08:56:20.763', 'Id': '23169'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>a few days ago I asked a question about the limits of compression:\n<a href="http://cs.stackexchange.com/questions/23010/can-prngs-be-used-to-magically-compress-stuff">Can PRNGs be used to magically compress-stuff?</a></p>\n\n<p>The idea common to all the answers was that if you consider all programs\nof length &lt; k (Let\'s call this set $C_k$), then there is only a finite number of possible outputs generated by programs in $C_k$ (let\'s call that set $S_k$).</p>\n\n<p>In the comments, I asked a followup question roughly equivalent to  "what determines which messages are possible outputs?" and have been thinking about this ever since.</p>\n\n<p>It seemes clear that what determines that is the program that executes\nthe program. If we ignore TM\'s and think in terms of source code, then \nthe output of a piece of code depends only on the syntax and semantics \nof the programming language. So the language design determines\nthe contents of $S_k$. There are some interesting corollaries:</p>\n\n<ul>\n<li>All program transformations that do no affect the AST nor enlarge the program beyond k bits will produce programs that generate the same output.</li>\n<li>All refactorings, which might alter the AST but preserve all input/output\nrelationships, will do the same if they do not inflate the size of the source code.</li>\n<li>a PL that provides multiple constructs ("There\'s more then one way to do it")\nfor achieving the same result (control flow constructs, for example) will , generally speaking, have more programs of length &lt; k which generate the same output and so a smaller $S_k$ then a similar but "slimmer" PL.</li>\n</ul>\n\n<p>This brings me to the following question: If I consider all programs\nin $C_k$ that generate the same output (it\'s an equivalence relation)\nand then proceed to compress them, what would be the results? Would\nthe equivalent programs compress down to roughly the same size or not?\nDoes the compressability depend on the meaning of the program (which\nis identical) or the bit sequence (which is not)?  </p>\n\n<p>A statement given in the answers to the previous question is\nthat the entropy of a bit sequence determines how compressible it is.\nIs that entropy determined by the input/output relationships encoded\nin the program, or by it\'s bit-representation in language X?\nWhat\'s the right way to think of this?</p>\n\n<p><strong>update</strong>\nSince a program describes a computational process\nrather then merely output, the notion of equivalence\nsuggested really doesn\'t indicate me much about\nthe relative complexity of programs which are equivalent \nby that definition.</p>\n', 'ViewCount': '76', 'LastEditorDisplayName': 'user15782', 'Title': "What determines the entropy of a program's source code?", 'LastActivityDate': '2014-03-30T11:05:16.263', 'LastEditDate': '2014-03-30T11:05:16.263', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23242', 'Score': '3', 'OwnerDisplayName': 'user15782', 'PostTypeId': '1', 'Tags': '<data-compression><kolmogorov-complexity>', 'CreationDate': '2014-03-30T02:17:13.703', 'Id': '23238'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm working with compression algorithms that use context-free grammars (e.g. RE-PAIR and SEQUITUR). These grammars look for frequently occurring digrams (pairs of adjacent symbols) in an input string and use recursive substitution with non-terminal symbols to achieve compression. </p>\n\n<p>What I'm wondering is whether there are classes of grammar and inference methods that are able to exploit sequential patterns that are based on non-adjacent co occurrence. </p>\n\n<p>Take the following input sequence:</p>\n\n<blockquote>\n  <p>XaXbXcYaYbYcZaZbZc</p>\n</blockquote>\n\n<p>No two adjacent symbols co-occur more than once anywhere here, so the sequential compression techniques I mention will not compress this string any further, however there is a clear pattern to the sequence and it must be compressible. How is it possible to do so?</p>\n", 'ViewCount': '74', 'Title': 'Compression of non-adjacent structure using grammar', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-08T21:52:35.873', 'LastEditDate': '2014-04-08T09:05:33.517', 'AnswerCount': '3', 'CommentCount': '5', 'AcceptedAnswerId': '23548', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12876', 'Tags': '<formal-languages><formal-grammars><data-compression>', 'CreationDate': '2014-04-08T07:49:56.070', 'Id': '23534'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I asked this on StackOverflow, but it was determined more appropriate to post here.</p>\n\n<p>So it goes like this..</p>\n\n<p>For people concerned about users having slow connections and timeout issues, it is known that it can\'t be just always solved with minification, latency reduction etc..</p>\n\n<p>I\'ll try to keep this question as short as possible, however retaining clarity where possible.</p>\n\n<p>I\'ve thought about a technique to further reduce uncompressable files (like mp3 and mp4), bear with me please.</p>\n\n<p>What if we could skip chunks/bits/frames of a file being similar and rebuild them locally? Ok, let\'s explain it with a short example.</p>\n\n<p>Assume you have many files in your computer, and that 3 of them are videos with a knife and a guy with blue eyes non-evenly spread over many frames. You go to this site and there\'s a post.. this post features a video of a serial killer\'s life. It is totally unrelated to the videos you have on your pc, except for the killer\'s knife, and the victim he kills having blue eyes.</p>\n\n<p>What we could do here, is, remotely and locally make the videos (in this case) have "metadata blocks" about the most frequent frames and do a bidirectional check. If the two happen to have the same/similar blocks, the remote server just sends the new blocks, and, we rebuild the missing ones locally.</p>\n\n<p>Like this: </p>\n\n<pre><code>Remote file =\n    MetadataBlock:\n        knife:\n            from (minute 3.12) to (minute 3.59)\n                and from (minute 4.40) to (minute 5.20)\n        blue eyes:\n            from (minute 0.06) to (minute 0.60)\n                and from (minute 1.20) to (minute 1.30)\n                and from (minute 6.50) to (minute 6.58)  \n\n\n\nLocal file =\n    MetadataBlock:\n        knife:\n            from (minute 8.00) to (minute 8.20)\n                and from (minute 40.00) to (minute 40.02)\n                and from (minute 42.00) to (minute 42.50)\n        blue eyes:\n            from (minute 3.05) to (minute 3.15)\n                and from (minute 6.08) to (minute 7.40)\n\nLocal file.knife.total = 1.12 minutes\n\nLocal file.blue_eyes.total = 1.42 minutes\n\nFileTotal = Local file.knife.total + Local file.blue_eyes.total\n\nFileTotal (2.54) * 3 / 1.5 = 5.08 minutes\n</code></pre>\n\n<hr>\n\n<p>Remote server remove the blue eyes and knife bits/frames, appending instructions to build them locally from MetadataBlock. </p>\n\n<p>As soon as the file is received, the os reads the instructions and execute them like: </p>\n\n<p><em>"Replace MetadataBlock(from(knife.minutes)) with local file 1 MetadataBlock(from(knife.minutes))"</em></p>\n\n<p>, the same goes for other metadata blocks such as blue eyes, black hair, shirt color etc..</p>\n\n<p>Is a technique like, or similar to this feasible, or just pure utopia?</p>\n', 'ViewCount': '23', 'Title': 'Compression technique exploiting common similarities', 'LastActivityDate': '2014-04-08T10:06:34.313', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16528', 'Tags': '<data-compression>', 'CreationDate': '2014-04-08T08:34:17.330', 'Id': '23537'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have two sound clips $C_1,C_2$ which are pretty similar.  I'd like to measure how perceptually similar they are, i.e., how similar a human would perceive the two to be.  Is there a way to use a lossy compression algorithm (e.g., a MP3, AAC, or Ogg Vorbis encoder) to compare the two clips?</p>\n\n<p>It occurs to me that that audio compressors already contain a good deal of knowledge about psychoacoustics and human perception of sound, built into them.  Is there a good way to use them to measure how similar the two clips are?</p>\n\n<p>Maybe something like $L(C_1 || C_2) / (L(C_1) + L(C_2))$, where $L(x)$ is the length of the compression of sound clip $x$, and $C_1 || C_2$ is the result of concatenating the two clips?  Or maybe find the highest bitrate such that \n$F(C_1)$ is close to $F(C_2))$ via a simple metric (e.g., L2 norm applied to the FFT spectrum), where $F(C)$ is the result of compressing $C$ at that bitrate followed by decompressing it?  Or something else along these lines?  Has anyone studied this?</p>\n\n<p>If it is relevant, the two clips are fairly similar: one was obtained by a transformation of the other.  They are aligned in time and have the same length.  Each is relatively short (at most a few seconds).  I've done some searching but haven't found any reference or research paper that discusses this sort of approach -- maybe I just haven't found it yet, though.</p>\n", 'ViewCount': '19', 'ClosedDate': '2014-04-16T08:19:31.513', 'Title': 'Measuring similarity of music, using lossy compression', 'LastActivityDate': '2014-04-16T00:54:43.233', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-compression>', 'CreationDate': '2014-04-16T00:54:43.233', 'Id': '23838'}