{'Body': '<p>I have a problem where, there are a set of nodes and dependencies between them. I want to cluster them based on the maximum number of dependencies.  Dependencies can be thought of as number of edges connected.  I want to group those with maximum dependencies.</p>\n\n<p>For example in the set $\\{1,2,3\\}$ and $\\{3,5,7\\}$ if $\\{3,5,7\\}$ have more dependencies i need to group $\\{3,5,7\\}$. I know the dependencies beforehand. </p>\n\n<blockquote>\n  <p>Which algorithm will help to solve this problem?</p>\n</blockquote>\n', 'ViewCount': '41', 'LastEditorDisplayName': 'user742', 'Title': 'How to cluster nodes based on the number of dependencies', 'LastActivityDate': '2013-09-20T09:29:13.827', 'LastEditDate': '2013-09-20T09:29:13.827', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '947', 'Tags': '<graph-theory><data-mining><cluster>', 'CreationDate': '2013-06-19T12:58:01.557', 'Id': '12760''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've been looking for an algorithm which divides an undirected, weighted, planar and simple graph into $k$ disjoint subgraphs. Here, the graph is sparse, $k$ is fixed, and there are no negative edge weights. After cutting, each subgraph must be connected (i.e. there must be a path between any two vertices of the subgraph which is only composed of vertices in that subgraph).</p>\n\n<p>However, unlike most existing work on graph partitioning out there, I don't intend to obtain subgraphs that contain the same approximate number of vertices. Instead, I would like these subgraphs to have similar sum of edge weights. In other words, I would like to minimize the sum of edge weights of the subgraph with maximal weight and ideally cut long (weighted) edges.</p>\n\n<p>Is there a name for this problem? I wasn't able to find anything about this on the web. Also, how can I approach this problem?</p>\n", 'ViewCount': '185', 'Title': 'Dividing a weighted planar graph into $k$ subgraphs with balanced weight', 'LastEditorUserId': '9431', 'LastActivityDate': '2013-11-06T15:14:17.837', 'LastEditDate': '2013-08-03T14:22:17.707', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9431', 'Tags': '<graph-theory><weighted-graphs><cluster><partition-problem>', 'CreationDate': '2013-08-02T04:01:20.273', 'FavoriteCount': '1', 'Id': '13571''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m not sure if this is the correct stack exchange or correct tags, but my question is as follows: </p>\n\n<p>I am working on a sort-of ratings system for players in a particular game. After allowing the ratings to develop for many games, I have set up a "database" (not sure if this is the correct term) for matches: a point in the database would consist of the score difference between the players (i.e if it\'s >0 player 1 won) and the rating discrepancy between the two players before this match.</p>\n\n<p>The idea is to use this database in order to predict the score difference of a match that has not yet occurred: I measure the rating discrepancy between the 2 players, and lookup in my database for the score differences in games of a "similar" rating discrepancy, and based on these games I am able to predict, say, a rough probability of each score difference occurring in this match that has not yet happened.</p>\n\n<p>I actually have two questions: </p>\n\n<p>1) what is a good approach to using the database to predict the probability of each score difference, i.e what qualifies as a "similar" rating discrepancy, how do I deal with extreme cases where the rating discrepancy is very large and I have few data examples of such matches (should I relax my definition of "similar"?), etc. A bit of googling shows that maybe I am looking for something relating to data clusters.</p>\n\n<p>2)how would I go about implementing an algorithm as above? are there good standard implementations of such classification algorithms? I am writing in C# if it makes a difference. </p>\n', 'ViewCount': '47', 'Title': 'Analysis and classification based on data points', 'LastActivityDate': '2013-11-06T20:13:58.630', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8247', 'Tags': '<data-structures><machine-learning><data-mining><cluster>', 'CreationDate': '2013-11-06T17:36:07.680', 'Id': '16775''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am a physicist, with little formal training in computer science - please don\'t assume I know even obvious things about computer science! </p>\n\n<p>Within the context of data analysis, I was interested in identifying clusters within a $d$-dimensional list of $n$ data-points, for which the dimensionality $d$ could be $\\sim100$, whilst the number of data-points could be $\\sim 1,000,000$, or perhaps more.</p>\n\n<p>I wanted the points with a cluster to be close together, with distance measured in the Euclidean manner,\n$$\nd(\\vec x,\\vec y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2} \n$$ \nAs long as the clustering was reasonably accurate, I wasn\'t bothered if the exactly correct result was obtained. i.e. if of my $\\sim1,000,000$, $\\sim1,000$ points were wrongly categorized, it wouldn\'t matter much.</p>\n\n<p>I have written a short algorithm that can perform typically at $\\mathcal{O}(n)$ (from trials of up to $n\\sim5,000,000$ and some theoretical analysis) and worst-case $\\mathcal{O}(n^2)$ (from my theoretical evaluation of the algorithm). The nature of the algorithm sometimes (but not always) avoids the so-called chaining problem in clustering, where dissimilar clusters are chained together because they have a few data-points that are close.</p>\n\n<p>The complexity is, however, sensitive to the <em>a priori</em> unknown number of clusters in the data-set. The typical complexity is, in fact, $\\mathcal{O}(n\\times c)$, with $c$ the number of clusters. </p>\n\n<p>Is that better than currently published algorithms? I know naively it is a  $\\mathcal{O}(n^3)$ problem. I have read of SLINK, that optimizes the complexitiy to $\\mathcal{O}(n^2)$. If so, is my algorithm useful? Or do the major uses of clustering algorithms require exact solutions?</p>\n\n<p>In real applications is $c\\propto n$?, such that my algorithm has no advantage. My naive feeling is that for real problems, the number of "interesting" clusters (i.e. not including noise) is a property of the physical system/situation being investigated, and is in fact a constant, with no significant dependence on $n$, in which case my algorithm looks useful.</p>\n', 'ViewCount': '83', 'Title': 'Is an $\\mathcal{O}(n\\times \\text{Number of clusters})$ clustering algorithm useful?', 'LastEditorUserId': '11757', 'LastActivityDate': '2013-12-06T00:49:12.167', 'LastEditDate': '2013-12-05T13:11:18.563', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '18668', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11757', 'Tags': '<algorithms><complexity-theory><cluster>', 'CreationDate': '2013-12-02T11:38:24.923', 'Id': '18534''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am working on my thesis which involves using ant based techniques for graph clustering. I am testing the algorithm currently and I was wondering if there is a way that I can visualize the clusters of a given graph.</p>\n\n<p>In this case, I have the graph as a file and another file corresponding to the clustering where each node is given its cluster. Is there a software using which I can visualize this graph where nodes belonging to the same cluster have like the same color or something?</p>\n\n<p>The graph is represented as an edge list, or in the ".net" format or Graph Modelling Language ".gml" format. I know there are a lot of programs out there for networks such as gephi, graphviz, networkx, pajek etc but I don\'t know how to do this. Any help/ideas would be greatly appreciated!</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '84', 'Title': 'Visualize Graph Clusters', 'LastActivityDate': '2013-12-04T18:06:46.093', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '18601', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5020', 'Tags': '<graphs><cluster>', 'CreationDate': '2013-12-04T03:37:08.370', 'FavoriteCount': '2', 'Id': '18596''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Is there an efficient way to cluster nodes in a graph using Jaccard similarity such that each cluster has at least k nodes? </p>\n\n<p>Jaccard similarity between nodes i and j: Let S be the set of neighbours of i and T be the set of neighbours of j. Then, the similarity between i and j is given by  $\\frac{|(S \\cap T)|}{|(S \\cup T)|}$.</p>\n', 'ViewCount': '49', 'Title': 'Is there an efficient way to cluster a graph according to Jaccard similarity?', 'LastActivityDate': '2014-02-18T23:36:13.597', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4868', 'Tags': '<graph-theory><data-mining><cluster>', 'CreationDate': '2013-12-20T21:54:54.990', 'Id': '19167''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm working on a problem where I have a matrix of values which I know will be in groups of similarly-valued elements.  I am trying to find these groups of similarly-valued elements.</p>\n\n<p>I'm going to define an error function for an element as distance from the mean value of its cluster. </p>\n\n<p>given k, I want to find the clustering that maximizes the number of elements below an error threshold.  If this isn't possible, I'd like to at least cluster the elements in any way I could and then examine this error.</p>\n\n<p>I've been examining the standard clustering algorithms (k-means, EM) and I can't seem to map this problem to the problems formulated in these algorithms.</p>\n\n<p>Any ideas?</p>\n", 'ViewCount': '20', 'Title': 'Clustering a matrix into similar groups', 'LastActivityDate': '2014-04-06T07:59:13.663', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16460', 'Tags': '<machine-learning><cluster>', 'CreationDate': '2014-04-04T21:40:04.940', 'Id': '23436''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am curious as to what steps one would reasonably need to take to perform an extraction-based text summarizer.</p>\n\n<p>I\'ve taken a look at some papers I\'ve found on Google such as <a href="https://wwws.cs.umn.edu/tech_reports_upload/tr2000/00-034.ps" rel="nofollow">this one</a>, which explains that UPGMA is the best clustering algorithm (out of the tested set). I\'ve also found <a href="http://acl.ldc.upenn.edu/I/I05/I05-2004.pdf" rel="nofollow">this one</a> to be interesting, regarding single and multi-document summarization.</p>\n\n<p>I\'m unclear as to whether I\'d need to combine these techniques for summarization or whether it would suffice to use a tool like Gensim to model a corpus of documents and just extract the sentences with the highest vector values.</p>\n', 'ViewCount': '23', 'Title': 'Document clustering for summarization', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-11T06:35:58.613', 'LastEditDate': '2014-04-11T06:35:58.613', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16190', 'Tags': '<machine-learning><data-mining><natural-lang-processing><cluster>', 'CreationDate': '2014-04-10T22:42:44.780', 'Id': '23661''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a problem in which I need to find an optimal graph cut that maximizes an objective over vertices (versus edge weights). I have looked at the literature but have not been able to find any similar problems to which I can map to. Perhaps someone can give some insight or point me to a similar problem from the graph theory literature.</p>\n\n<p>The problem is as follows, given a graph $G=(V,E)$, where there exists a path from any vertex $x_i$ to any other vertex $x_j$ (1 connected component) and each vertex has an associated weight $w_j$. Find a partition which removes $E_p$ edges $(E_p \\subset E)$ to create exactly $k$ connected components. The partition seeks to maximize a function over the $k$ subgraphs $\\sum\\limits_{i=1}^k f(W_i)$ where $W_i$ is the set of all weights on vertices in subgraph $i$. The $f$ I am specifically interested in is $f(W_i)=|W_i|\\,\\mathrm{mean}(W_i)^2 $, but I think any resources for a similar problem with a different $f$ would be helpful. </p>\n', 'ViewCount': '41', 'Title': 'Edge cuts with vertex weights', 'LastEditorUserId': '16985', 'LastActivityDate': '2014-04-23T08:03:23.563', 'LastEditDate': '2014-04-23T08:03:23.563', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16985', 'Tags': '<graph-theory><cluster><partition-problem>', 'CreationDate': '2014-04-22T12:27:06.003', 'FavoriteCount': '1', 'Id': '24026''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm using mat lab's implementation of hierarchical clustering algorithm, with pdist, linkage , cophenet, dendrogram, and cluster. But I'm not exactly sure where or how to find the time complexity</p>\n\n<p>It is an agglomerative type of hierarchal clustering, and although they're usually \n$O(n^3)$\nthis one in particular works pretty fast for 177 data points of 3 features (177x3), which got me curious. Is it $n^3$ or an optimal version?</p>\n", 'ViewCount': '15', 'Title': "What is the time complexity of Matlab's Hierarchical Clustering?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T11:39:59.340', 'LastEditDate': '2014-04-30T11:39:59.340', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6980', 'Tags': '<algorithms><runtime-analysis><cluster>', 'CreationDate': '2014-04-29T16:57:40.817', 'Id': '24229''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>One approach for clustering a high dimensional dataset is to use linear transformation, and the most common approaches are PCA and random projection (where random projection arises from the Johnson-Lindenstrauss Lemma). I was wondering why we can't use other random transformation  s like when our transformation matrix $R$ was drawn from a uniform distribution?</p>\n", 'ViewCount': '16', 'Title': 'Subspace clustering with random transformation', 'LastActivityDate': '2014-04-30T04:26:59.197', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17110', 'Tags': '<data-mining><linear-algebra><classification><cluster>', 'CreationDate': '2014-04-30T03:43:25.603', 'Id': '24249''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}