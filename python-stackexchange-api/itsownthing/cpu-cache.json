108_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '401', 'Title': 'Are generational garbage collectors inherently cache-friendly?', 'LastEditDate': '2012-03-25T22:35:09.003', 'AnswerCount': '2', 'Score': '24', 'PostTypeId': '1', 'OwnerUserId': '39', 'FavoriteCount': '4', 'Body': '<p>A typical <a href="http://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Generational_GC_.28ephemeral_GC.29">generational garbage collector</a> keeps recently allocated data in a separate memory region. In typical programs, a lot of data is short-lived, so collecting young garbage (a minor GC cycle) frequently and collecting old garbage infrequently is a good compromise between memory overhead and time spent doing GC.</p>\n\n<p>Intuitively, the benefit of a generational garbage collector compared with a single-region collector should increase as the latency ratio of main memory relative to cache increases, because the data in the young region is accessed often and kept all in one place. Do experimental results corroborate this intuition?</p>\n', 'Tags': '<programming-languages><computer-architecture><cpu-cache>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-07T16:20:58.843', 'CommentCount': '1', 'AcceptedAnswerId': '608', 'CreationDate': '2012-03-19T12:31:48.340', 'Id': '495'},108_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '229', 'Title': 'Research on evaluating the performance of cache-obliviousness in practice', 'LastEditDate': '2012-03-26T20:58:01.420', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '2', 'Body': u'<p><a href="http://en.wikipedia.org/wiki/Cache-oblivious_algorithm" rel="nofollow">Cache-oblivious algorithms and data structures</a> are a rather new thing, introduced by Frigo et al. in <a href="http://userweb.cs.utexas.edu/~pingali/CS395T/2009fa/papers/coAlgorithms.pdf" rel="nofollow">Cache-oblivious algorithms, 1999</a>. Prokop\'s <a href="http://www.google.fi/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http%3A%2F%2Fsupertech.csail.mit.edu%2Fpapers%2FProkop99.pdf&amp;ei=Dc1tT-aLI8bm4QSC4YjAAg&amp;usg=AFQjCNHWhtzqOQqUonQWHduna8_nbQYx2g&amp;sig2=Nf_YDGY3NZLj7q0FY6TZgw" rel="nofollow">thesis</a> from the same year introduces the early ideas as well.</p>\n\n<p>The paper by Frigo et al. present some experimental results showing the potential of the theory and of the cache-oblivious algorithms and data structures. Many cache-oblivious data structures are based on static search trees. Methods of storing and navigating these trees have been developed quite a bit, perhaps most notably by Bender et al. and also by Brodal et al. Demaine gives a nice <a href="http://www.cs.uwaterloo.ca/~imunro/cs840/DemaineCache.pdf" rel="nofollow">overview</a>.</p>\n\n<p>The experimental work of investigating the cache behaviour in practice was done at least by Ladner et al. in <a href="http://www.cs.amherst.edu/~ccm/cs34/papers/ladnerbst.pdf" rel="nofollow">A Comparison of Cache Aware and Cache Oblivious Static Search Trees Using Program Instrumentation, 2002</a>. Ladner et al. benchmarked the cache behaviour of algorithms solving the binary search problem, using the classic algorithm, cache-oblivious algorithm and cache-aware algorithm. Each algorithm was benchmarked with both implicit and explicit navigation methods. In addition to this, the thesis by <a href="http://www.diku.dk/forskning/performance-engineering/frederik/thesis.pdf" rel="nofollow">R\xf8nn, 2003</a> analyzed the same algorithms to quite high detail and also performed even more thorough testing of the same algorithms as Ladner et al.</p>\n\n<p><strong>My question is</strong></p>\n\n<blockquote>\n  <p>Has there been any newer research on <em>benchmarking</em> the cache behaviour of cache-oblivious algorithms in <em>practice</em> since? I\'m especially interested in the performance of the static search trees, but I would also be happy with any other cache-oblivious algorithms and data structures.</p>\n</blockquote>\n', 'Tags': '<algorithms><data-structures><computer-architecture><reference-request><cpu-cache>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-03-26T20:58:01.420', 'CommentCount': '1', 'AcceptedAnswerId': '741', 'CreationDate': '2012-03-24T13:51:44.963', 'Id': '740'},108_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2934', 'Title': 'What happens to the cache contents on a context switch?', 'LastEditDate': '2012-04-06T23:13:15.797', 'AnswerCount': '2', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '59', 'FavoriteCount': '5', 'Body': "<p>In a multicore processor, what happens to the contents of a core's cache (say L1) when a context switch occurs on that cache?</p>\n\n<p>Is the behaviour dependent on the architecture or is it a general behaviour followed by all chip manufacturers?</p>\n", 'Tags': '<computer-architecture><operating-systems><cpu-cache>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-25T15:07:48.650', 'CommentCount': '0', 'AcceptedAnswerId': '1093', 'CreationDate': '2012-04-06T20:42:26.020', 'Id': '1088'},108_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>In a recent CACM article [1], the authors present a way to improve scalability of shared  and coherent caches. The core ingredient is assuming the caches are <em>inclusive</em>, that is higher-level caches (e.g. L3, one global cache) contain all blocks which are stored in their descendant lower-level caches (e.g. L1, one cache per core).</p>\n\n<p>Typically, higher-level caches are larger than their respective descendant caches together. For instance, some models of the Intel Core i7 series with four cores have an 8MB shared cache (L3) and 256KB private caches (L2), that is the shared cache can hold eight times as many blocks as the private caches in total.</p>\n\n<p>This seems to suggest that whenever the shared cache has to evict a block (in order to load a new block) it can find a block that is shared with none of the private caches\xb2 (pigeon-hole principle). However, the authors write:</p>\n\n<blockquote>\n  <p>[We] can potentially eliminate all recalls, but only if the associativity, or number of places in which a specific block may be cached, of the shared cache exceeds the aggregate associativity of the private caches. With sufficient associativity, [the shared cache] is guaranteed to find a nonshared block [...]. Without this worst-case associativity, a pathological cluster of misses could lead to a situation in which all blocks in a set of the shared cache are truly shared.</p>\n</blockquote>\n\n<p>How is this possible, that is how can, say, 1MB cover 8MB? Clearly I miss some detail of how such cache hierarchies work. What does "associativity" mean here? "number of places in which a specific block may be cached" is not clear; I can only come up with the interpretation that a block can be stored multiple times in each cache, but that would make no sense at all. What would such a "pathological cluster of misses" look like?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1145/2209249.2209269">Why On-Chip Cache Coherence is Here to Stay</a> by M. M. K. Martin, M. D. Hill, D. J. Sorin  (2012)</li>\n<li>Assuming the shared caches knows which blocks are shared where. This can be achieved by explicit eviction notifications and tracking bit, which is also discussed in [1].</li>\n</ol>\n', 'ViewCount': '207', 'Title': 'Why can L3 caches hold only shared blocks?', 'LastActivityDate': '2012-08-02T14:29:37.697', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3004', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<terminology><computer-architecture><cpu-cache><shared-memory>', 'CreationDate': '2012-08-02T11:38:21.877', 'Id': '3001'},108_4:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Parallel processes interfere with each other in many ways, by competing for shared resources such as shared caches, memory, disks, etc.</p>\n\n<p>Would it be possible to determine latent factors just with a runtime analysis, by means of comparing the runtime of the respective applications while running them many times in different combinations? Or would the required sample size be too big for the method to be feasible?</p>\n\n<p>If comparing only the runtimes would not result in significant results, which variable would be best suited to observe? L1-cache-misses, L2-cache-misses, page faults, total memory usage, tlb-misses, etc.</p>\n', 'ViewCount': '48', 'Title': 'Determine interference factors in parallel computing', 'LastActivityDate': '2012-10-17T10:27:52.867', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4230', 'Tags': '<parallel-computing><cpu-cache><performance>', 'CreationDate': '2012-10-17T10:27:52.867', 'Id': '6120'},108_5:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p><strong>Question 1:</strong> What is the average access time for a 3-level memory system with access time $T_1$, $2T_1$ and $3T_1$? (Hit ratio $h_1$ = $h_2$ = 0.9)</p>\n\n<p>The solution given is: $0.9[T_1] + 0.1(0.9[2*T_1] + 0.1[3*T_1]) = 1.11[T_1]$  <strong>(Method 1)</strong></p>\n\n<p>Here, they have considered the page won't be copied to the lower level. Otherwise, it would have been like the following</p>\n\n<p><code>If a page is not there in cache, it would be copied from main memory to cache and then accessed.</code> $T_1 + 2T_1$</p>\n\n<p><code>If a page is not there even in main memory, it would be brought to main memory, then cache and then accessed.</code> $T_1 + 2T_1 + 3T_1$</p>\n\n<p>$0.9[T_1] + 0.1(0.9[T_1+2*T_1] + 0.1[T_1 + 2*T_2 + 3*T_1]) = 1.23[T_1]$  <strong>(Method 2)</strong></p>\n\n<p>I went through another similar problem.</p>\n\n<p><strong>Question 2</strong></p>\n\n<pre><code>Cache Access Time = 20ns\nMemory Access Time = 120ns\nHit Ratio = 0.8\nSome other useless information below...\nCache Block size = 16 words\nSet size = 2 blocks\nNumber of sets = 128\nSize of main memory address = 21bits\nWhat is the hit ratio if the average access time is increased by 40ns?\n(A) Remains same      (B) 0.921     (C) 0.467      (D) 0.592\n</code></pre>\n\n<p>I simply calculated it using <strong>Method 1</strong> as follows</p>\n\n<pre><code>Effective access time = 0.8*20 + 0.2*(120) = 40ns\nIncrease by 40ns, so new time = 80ns\n80 = h*20 + (1-h)*120\nHit ratio = 0.4\n</code></pre>\n\n<p>But this is not in the options</p>\n\n<p>But when I calculated it using <strong>Method 2</strong></p>\n\n<pre><code>Effective access time = 0.8*20 + 0.2*(20 + 120) = 44ns\nIncrease by 40ns, so new time = 84ns\n84 = h*20 + (1-h)*120\nHit ratio = 0.467\n</code></pre>\n\n<p>That is option (C)</p>\n\n<p>Here, the answer is coming using Method 2 but in the above question they are using Method 1.</p>\n\n<p><strong>How do I know which method to take while solving such problems? Whether would the missed page be brought into the lower memory (cache) or not?</strong></p>\n", 'ViewCount': '3408', 'Title': 'Doubt regarding cache hit ratios and access time', 'LastEditorUserId': '4422', 'LastActivityDate': '2013-11-01T14:27:12.103', 'LastEditDate': '2012-12-04T18:06:20.643', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7151', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4422', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2012-12-01T06:04:25.320', 'Id': '7071'},108_6:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '49', 'Title': 'Cache strategies, what reference article could I study?', 'LastEditDate': '2013-01-28T10:11:13.810', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2100', 'FavoriteCount': '1', 'Body': '<p>So as to optimize an application, I must implement data caching: not to recompute some data - those heavy on cpu but that don\'t change often.</p>\n\n<p>When playing with the idea, I imagined something like the way win32/MFC manages the windows screen i.e.:</p>\n\n<ul>\n<li>While a part is valid, it is not repainted.</li>\n<li>When a rectangle or a region is invalidated, this part is repainted during the next painting session - launched by the OS.</li>\n</ul>\n\n<p>I was imagining a way to validate and invalidate my cached value, so as to recompute only what is necessary when it is necessary.</p>\n\n<p>Then I read <a href="http://en.wikipedia.org/wiki/Cache_algorithms" rel="nofollow">this wikipedia page about Cache Algorithms</a>, and none of the listed algorithm was using the technique I explained above. So I feel ill at ease, and I need to read some work about caching. </p>\n\n<p>Do you know of some resources that I could rely on before I start implementing my own cache process ?</p>\n', 'Tags': '<reference-request><efficiency><cpu-cache>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-28T10:18:37.300', 'CommentCount': '3', 'AcceptedAnswerId': '9194', 'CreationDate': '2013-01-27T02:47:20.523', 'Id': '9187'},108_7:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Just a basic question to ask</p>\n\n<p><strong>Does the write through cache copies the whole block or just the byte which is updated?</strong></p>\n\n<p>I went through the following question</p>\n\n<p><em>Array A contains 256 elements of 4 bytes each. Its first element is stored at physical address 4096.\nArray B contains 512 elements of 4 bytes each. Its first element is stored at physical address 8192. Assume that only arrays A and B can be cached in an initially empty, physically addressed, physically tagged, direct mapped, 2K-byte cache with an 8 byte block size. The following loop is the executed</em></p>\n\n<pre><code>for(i=0; i&lt;256; i++)\nA[i] = A[i] + B[2*i];\n</code></pre>\n\n<p><em>How many bytes will be written to memory if the cache has a write-through policy?</em></p>\n\n<p>I calculated it as follows:</p>\n\n<p>The cache can store the whole array with 2 elements per block (block size = 8bytes, element size = 4bytes). For every write, the whole block will be copied. For $0^{th}$ element, the block containing $0^{th}$ and $1^{st}$ element would be written. The same would be done for $1^{st}$ element as well.</p>\n\n<p>So, for every iteration the 2 elements would be written. This makes the number of bytes as $256*2* (4bytes / element) = 2048bytes$.</p>\n\n<p>But in the solution, they have just calculated the number of loop iterations ($256$) multiplied by the element size ($4byte$) which makes the answer $1024bytes$.\nIf this is true, then the cache would update only the updated byte (not the whole block). </p>\n\n<p>Which is correct?</p>\n', 'ViewCount': '158', 'Title': 'Does the write through cache copies the whole block or just the byte which is updated?', 'LastActivityDate': '2013-01-29T16:41:15.240', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '9264', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4422', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-01-27T19:36:19.230', 'Id': '9221'},108_8:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>CPU caches are used by exploiting temporal and spatial locality. My question is who is responsible for managing these caches? Is this Operating system that identifies a particular access pattern and then manages (i.e store the data in) cache, using low level OS function calls? </p>\n', 'ViewCount': '136', 'Title': 'CPU Cache is managed by which software component?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T21:06:59.673', 'LastEditDate': '2013-02-24T16:08:45.417', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '10054', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-02-24T08:20:21.490', 'Id': '10043'},108_9:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider an array A[100] &amp; each element occupies 4 word. A 32 word cache is used and divided into 8 word blocks. What is the hit ratio for the following statement. Assume one block is read into cache in case of miss:</p>\n\n<pre><code>for(i=0;i&lt;100;i++)\n  A[i]=A[i]+10\n</code></pre>\n\n<p>each array element is of 4 word &amp; each cache block size is 8 words, so we load 8 words into cache.</p>\n\n<p>When program tries to read A[0], for the first time it will be miss. Hence it will be brought to memory (&amp; also A[1]). Next, A[1] will be hit. So, it will be like : </p>\n\n<ul>\n<li>A[0] - Miss  </li>\n<li>A[1] - Hit  </li>\n<li>A[2] - Miss  </li>\n<li>A[3] - Hit</li>\n<li>.....</li>\n</ul>\n\n<p>So hit ratio is 50% ? Am I wrong anywhere ? </p>\n\n<p>I also have one more doubt. First when it tries to access A[0], it will be miss. And then when it brings A[0] to cache, CPU tries to access A[0] again, now it should be considered as hit ? Like, </p>\n\n<ul>\n<li>A[0] (Read) - Miss</li>\n<li>A[0] (Write) - Hit</li>\n<li>A[1] (Read) - Hit</li>\n<li>A[1] (Write) - Hit</li>\n</ul>\n\n<p>If above is correct, then Hit ratio will be 75%.</p>\n\n<p>Any help regarding this is appreciated.</p>\n', 'ViewCount': '345', 'Title': 'Finding hit ratio of a cache', 'LastEditorUserId': '6665', 'LastActivityDate': '2014-04-29T12:23:20.487', 'LastEditDate': '2013-04-08T05:00:21.610', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-04-08T03:35:41.280', 'Id': '11132'},108_10:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Following question was asked in one of entrance exams for a graduation programme. Please help me try to solve it : </p>\n\n<blockquote>\n  <p>A computer system has an L1 cache, an L2 cache, and a main memory\n  unity connected as shown below. The block size in L1 cache is 4 words.\n  The block size in L2 cache is 16 words. The memory access times are 2\n  nanoseconds, 20 nanoseconds and 200 nanoseconds for L1 cache, L2 cache\n  and main memory unit respectively.</p>\n  \n  <p><img src="http://i.stack.imgur.com/7zo5X.gif" alt="cache-image"></p>\n  \n  <ol>\n  <li>When there is a miss in L1 cache and a hit in L2 cache, a block is\n  transferred from L2 cache to L1 cache. What is the time taken for this\n  transfer?</li>\n  </ol>\n  \n  <p>(A) 2 nanoseconds \n  (B) 20 nanoseconds\n  (C) 22 nanoseconds \n  (D) 88 nanoseconds</p>\n  \n  <ol>\n  <li>When there is a miss in both L1 cache and L2 cache, first a block\n  is transferred from main memory to L2 cache, and then a block is\n  transferred from L2 cache to L1 cache. What is the total time taken\n  for these transfers?</li>\n  </ol>\n  \n  <p>(A) 222 nanoseconds (B) 888 nanoseconds\n  (C) 902 nanoseconds (D) 968 nanoseconds</p>\n</blockquote>\n\n<p>First thing that came to my mind was, how to calculate the transfer time using the given access time. During a miss, a block of data is moved from main memory to cache. Then CPU will access it. So, wouldn\'t be access time > transfer time ?</p>\n\n<p>Then I thought, lets assume access time = transfer time &amp; do the calculation. </p>\n\n<p>Now first question. The question already states there is a miss in L1, so I will not consider L1 access time. Since there is a miss in L1 &amp; hit in L2, a entire block from L2 has to be moved to L1. L2 block size is 16 words, but data bus size is 4 words. </p>\n\n<p>So we have to move 4 words * 4 times. </p>\n\n<p>To transfer 4 word it takes 20 ns. To transfer 4 words, its 80ns. Isn\'t it the time transferred from L2 to L1 ? The question does not say anything about accessing L1 after moving the data. But 80ns is not in the option ! </p>\n\n<p>Similar case with second question also. </p>\n\n<p>Time to move main memory to L2 = 4 words * 4 times = 4 * 200 = 800ns </p>\n\n<p>Time to move L2 to L1 = 80ns [earlier calculation]</p>\n\n<p>So total time taken is 880ns. Which is again not in the option. </p>\n\n<p>Either I am doing a very big mistake or options are wrong or question isn\'t framed correctly. If I am doing anything wrong, please give me some hint &amp; I will try to work on this exercise again. </p>\n', 'ViewCount': '465', 'Title': 'Finding cache block transfer time in a 3 level memory system', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T18:04:13.117', 'LastEditDate': '2013-04-14T11:46:23.223', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '11311', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture><cpu-cache><memory-access>', 'CreationDate': '2013-04-14T09:56:15.187', 'Id': '11309'},108_11:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Regarding Processor Direct Cache, what is the proper mathematical technique for discovering how many words are loaded on a cache miss? </p>\n\n<p>For example if you have a direct mapped cache with a total data cache size of 32 words and a direct mapped cache of 4-word blocks. I believe I have a cache index of 8 cache blocks:</p>\n\n<p>32/4 = 8</p>\n\n<p>Example: Finding a block in python:</p>\n\n<pre><code>var1 = 56\nprint ((var1 / 8)% 8)\n</code></pre>\n\n<p>The answer: 7</p>\n\n<p>So finding where to store the blocks is simple enough but I'm uncertain on a miss exactly how <strong>MANY</strong> words will be loaded? </p>\n", 'ViewCount': '68', 'Title': 'How many words loaded on a cache miss', 'LastActivityDate': '2013-06-29T15:59:56.883', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8925', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-06-29T15:59:56.883', 'Id': '12967'},108_12:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Within the field of cache-oblivious algorithms the ideal cache model is used for determining the cache complexity of an algorithm.  One of the assumptions of the ideal cache model is that it models a "tall cache".  This is given by the statement $Z = \\Omega(L^2)$.  Where $Z$ is the size of the cache and $L$ is the size of the cache line.  What does $\\Omega$ represent?</p>\n', 'ViewCount': '69', 'Title': 'In the "tall cache assumption" what does $\\Omega$ represent?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-03T13:21:34.633', 'LastEditDate': '2013-07-03T13:21:34.633', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13053', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8986', 'Tags': '<asymptotics><landau-notation><computation-models><cpu-cache>', 'CreationDate': '2013-07-03T08:32:00.390', 'Id': '13052'},108_13:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Specifically:</p>\n\n<p>1) A <strong>direct-mapped cache</strong> with 4096 blocks/lines in which each block has 8 32-bit words. How many bits are needed for the tag and index fields, assuming a 32-bit address?</p>\n\n<p>2) Same question as 1) but for <strong>fully associative cache</strong>?</p>\n\n<p>Correct me if I'm wrong, is it:</p>\n\n<blockquote>\n  <p>tag bits = address bit length - exponent of index - exponent of offset? </p>\n  \n  <p>[Is the offset = 3 due to 2^3 = 8 or is it 5 from 2^5 = 32?] </p>\n</blockquote>\n", 'ViewCount': '4829', 'Title': 'How to calculate the tag, index and offset fields of different caches?', 'LastEditorUserId': '9161', 'LastActivityDate': '2013-12-15T09:15:20.893', 'LastEditDate': '2013-07-20T09:26:03.933', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '13359', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9161', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-07-20T09:08:01.743', 'Id': '13356'},108_14:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For inclusion to hold between two cache levels L1 and L2 in a multi-level cache hierarchy, which of the following are necessary?</p>\n\n<ol>\n<li>L1 must be a write-through cache.</li>\n<li>L2 must be a write-through cache.</li>\n<li>The associativity of L2 must be greater than that of L1.</li>\n<li>The L2 cache must be at least as large as the L1 cache.</li>\n</ol>\n\n<p>This was a multiple-choice question with the following possible answers:</p>\n\n<ul>\n<li>(A) 4 only </li>\n<li>(B) 1 and 4 only</li>\n<li>(C) 1, 2 and 4 only </li>\n<li>(D) 1, 2, 3 and 4</li>\n</ul>\n\n<p>I think L2 cache must be at least as large as the L1 cache but I am confused what the need for writeback is for this cache.</p>\n', 'ViewCount': '205', 'ClosedDate': '2014-01-24T17:13:52.553', 'Title': 'Multi-level cache for which inclusion holds', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-10T11:39:23.420', 'LastEditDate': '2013-09-06T15:07:56.157', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10012', 'Tags': '<cpu-cache>', 'CreationDate': '2013-09-06T13:34:16.340', 'Id': '14174'},108_15:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Scott Meyers describes <a href="http://www.aristeia.com/TalkNotes/ACCU2011_CPUCaches.pdf" rel="nofollow">here</a> that traversing a symmetric matrix row-wise performes significantly better over traversing it column-wise - which is also significantly counter-intuitive. </p>\n\n<p>The reasoning is connected with how the CPU-caches are utilized. But I do not really understand the explanation and I would like to get it because I think it is relevant to me.</p>\n\n<p>Is it possible to put it in more simple terms for somebody not holding a PhD in computer architecture and lacking experience in hardware-level programming?</p>\n', 'ViewCount': '183', 'Title': 'Performance of row- vs. column-wise matrix traversal', 'LastActivityDate': '2013-10-05T14:33:16.390', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14829', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9994', 'Tags': '<cpu-cache><performance>', 'CreationDate': '2013-10-05T11:20:40.243', 'Id': '14826'},108_16:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to calculate the average memory access time of a 2-level cache with a split L1 cache. I am given the 3 formulas below:</p>\n\n<p><strong>Given</strong></p>\n\n<p>Basic Formula:</p>\n\n<pre><code>AMAT = HitTimeL1 + MissRateL1 x MissPenaltyL1\n</code></pre>\n\n<p>2-Level Cache:</p>\n\n<pre><code>*Substitution: MissPenaltyL1 = HitTimeL2 + MissRateL2 x MissPenaltyL2\n\nAMAT = HitTimeL1 + MissRateL1 x (HitTimeL2 + MissRateL2 x MissPenaltyL2)\n</code></pre>\n\n<p>Split-Level Cache:</p>\n\n<pre><code>AMAT_SplitL1 = AMAT_InstructionCache + AMAT_DataCache\n</code></pre>\n\n<p><strong>Assumptions</strong></p>\n\n<p>Based on the formulas above I have substituted the 2-Level cache formulas into each of the split L1 caches. I am not sure if this is correct. If anyone could verify this for me or provide me with some documentation that supports this type of calculation that would be awesome. Thanks.</p>\n\n<pre><code>AMAT = (HitTimeL1Inst + MissRateL1Inst x (HitTimeL2 + MissRateL2 x MissPenaltyL2))\n     + (HitTimeL1Data + MissRateL1Data x (HitTimeL2 + MissRateL2 x MissPenaltyL2))\n</code></pre>\n', 'ViewCount': '896', 'ClosedDate': '2014-05-03T23:50:27.440', 'Title': 'Average Memory Access Time for Split/2-Level Cache', 'LastEditorUserId': '10776', 'LastActivityDate': '2014-04-24T14:16:30.577', 'LastEditDate': '2013-10-17T01:27:34.370', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10776', 'Tags': '<computer-architecture><cpu-cache><memory-access>', 'CreationDate': '2013-10-16T00:33:03.687', 'Id': '16121'},108_17:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Any CS class about caches will at some point address this classical formula (or a variant of it)</p>\n\n<pre><code>Effective_access_time = hit_time + miss_penalty * miss_rate\n</code></pre>\n\n<p>My question is simple: <strong>does this formula have a "name" ?</strong> (i.e. unambiguous, and ideally, well-accepted too). I found plenty of references to many <em>variants</em> of that concept. I\'d like to know whether there exists one name to denote them all. By the way, the question makes sense : there are plenty of variants of the following sentence... </p>\n\n<blockquote>\n  <p>In any square triangle, the square of the length of the hypotenuse is equal\n  to the sum of the squares of the lengths of the two shorter sides.</p>\n</blockquote>\n\n<p>...but we prefer to refer to them all by the name "Pythagoras\' theorem".</p>\n', 'ViewCount': '98', 'Title': u'Is there a prefered name for the \u201ceffective access time\u201d formula?', 'LastActivityDate': '2013-10-23T16:16:48.327', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6719', 'Tags': '<terminology><computer-architecture><cpu-cache>', 'CreationDate': '2013-10-23T16:16:48.327', 'FavoriteCount': '1', 'Id': '16368'},108_18:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '578', 'Title': 'Understanding the basic concepts in memory organisation', 'LastEditDate': '2013-11-02T10:30:47.990', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11125', 'FavoriteCount': '1', 'Body': '<p>(Before actually proceeding to the question, I want to confess that this is a homework question, please do consider it and help me in improving my understanding a bit more.)</p>\n\n<p>I have recently started learning computer organisation and architecture. I have gained fair understanding for how caches are organised, how mapping between cache and main memory takes place (direct , fully and set-associative mapping), what is a page table(what are pages, blocks etc.), i can that say I have basic knowledge of segmentation , paging, virtual address and physical addresses.( at the basic level ofcourse).</p>\n\n<p>Well I have come across this question:</p>\n\n<blockquote>\n  <p>A computer has 46-bit virtual address ,32- bit physical address, and a three \n     level page table organisation. The page table base-register stores the\n     base address of the first level table(t1), which occupies exactly one\n     page.Each entry of t1 stores the base address of the page of second level\n     table t2. Each entry of t2 stores the base address of the page of the third\n     level table t3. Each entry of t3 stores a page table entry (PTE). \n     The PTE is 32 bit in size. The processor used in the computer has a 1MB\n     16-way set associative virtually indexed physically tagged cache. The cache\n     block size is 64 Bytes.</p>\n</blockquote>\n\n<p>First of all I am facing difficulty in just imagining such type of a virtual computer. can any one help me by giving a simple steps on How to realize such a virtual computer on paper, or just how to understand what is given in the question. What is really asked? How would one represent a computer having a 46-bit virtual address and having three level page table?</p>\n\n<blockquote>\n  <p>What is virtually indexed and physically tagged cache?</p>\n</blockquote>\n\n<p>After reading what is given above, I feel that I just know the terms but I am unable to relate them together to solve problems. I will be glad If someone tries to explain how my thought process should be understand and apply these concepts practically to solve such types of problems.</p>\n\n<p>Some questions based on the above paragraph:</p>\n\n<ol>\n<li><p>What is the size of a page in KB in this computer?</p></li>\n<li><p>What is the minimum number of page colours needed to guarantee that no<br>\ntwo synonyms map to different sets in the processor cache of this computer?</p></li>\n</ol>\n\n<p>A good resource where such problems are actually taught to solve will a appreciated. Good articles and views are most welcome.</p>\n', 'Tags': '<memory-management><cpu-cache><virtual-memory>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-02T17:05:17.743', 'CommentCount': '2', 'AcceptedAnswerId': '16655', 'CreationDate': '2013-11-01T20:43:32.837', 'Id': '16630'},108_19:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know when we access elements  in rows it will be much faster than if it is accessed column wise. In matrix multiplication one of the matrices must be accessed column wise. \nIn GPUs with CUDA/OpenCL I could resolve this issue by explicitly copying both matrices row-wise into user-managed shared(cache)-memory (or local memory in OpenCL), and then can access data for matrix multiplication in anyway- no penalty for column access.</p>\n\n<p>My question is how do I avoid such column access if I am implementing Matrix-multiplication on CPU, where I do not have any access to such shared memory as in GPU?        </p>\n', 'ViewCount': '84', 'Title': 'How to avoid column wise access in matrix multiplication?', 'LastActivityDate': '2013-11-17T01:45:10.273', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><cpu-cache><performance>', 'CreationDate': '2013-11-16T13:01:29.680', 'FavoriteCount': '1', 'Id': '18072'},108_20:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to study for an exam and I realized I\'m confused about how the TLB and data cache work.</p>\n\n<p>I understand that the TLB is essentially a cache of most recently used physical addresses. However, I was looking at a diagram in my textbook (shown below), and I don\'t understand what\'s going on in it. It suddenly splits up the physical address and uses it to index the cache, I guess. But why is it showing the cache and data separately? and why is the byte offset just left floating? I\'m pretty sure the cache is supposed to store data as well. I don\'t think its sole purpose is to determine whether or not there\'s a hit or miss inside of it.</p>\n\n<p>I apologize for my ignorance in advance, but the book barely covers TLB\'s (it\'s like a little more than a page) and it doesn\'t do a very good job at explaining the relationship between a TLB and cache.</p>\n\n<p><img src="http://i.stack.imgur.com/MlEIQ.jpg" alt="Figure"></p>\n', 'ViewCount': '235', 'Title': 'How does a TLB and data cache work?', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-23T08:53:27.340', 'LastEditDate': '2013-11-25T05:57:16.537', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '18320', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<computer-architecture><cpu-cache><virtual-memory>', 'CreationDate': '2013-11-25T01:55:24.453', 'Id': '18313'},108_21:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There is a 64 KB 1-word cache, and a word is 32 bits.  From that I can derive that the length of the tag field is 16 bits, the length of index field is 14 bits, and, as my professor taught me, there is always 2 bits left behind for a byte offset.</p>\n\n<p>Why the offset field is 2 bits, other than it fills in the remaining 2 bits of the word, and what its contents is was never covered in the course.</p>\n\n<p>But when I looked around on Google, I read, and correct me if I am wrong, that the length of the offset field can vary.  Although I have found answers on how to determine the length, I could not find anything about determining its contents when a read hit/miss is performed.  My professor merely said "the byte offset is not used to select the word in the cache".</p>\n\n<p>Just looking for clarification.</p>\n', 'ViewCount': '45', 'Title': 'In cache addressing, what value is placed in the offset field?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-19T02:50:30.900', 'LastEditDate': '2014-01-19T02:20:14.857', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11780', 'Tags': '<cpu-cache><memory-hardware>', 'CreationDate': '2013-12-03T07:40:00.527', 'FavoriteCount': '1', 'Id': '18560'},108_22:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '45', 'Title': 'In cache-oblivious algorithms, how is recursive reduction of data performed?', 'LastEditDate': '2013-12-05T19:43:45.870', 'AnswerCount': '2', 'Score': '0', 'OwnerDisplayName': 'Nick Wiggill', 'PostTypeId': '1', 'OwnerUserId': '11852', 'FavoriteCount': '1', 'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Cache-oblivious_algorithm" rel="nofollow">wikipedia</a>:</p>\n\n<p>"Typically, a cache-oblivious algorithm works by a recursive divide and conquer algorithm, where the problem is divided into smaller and smaller subproblems. Eventually, one reaches a subproblem size that fits into cache, regardless of the cache size."</p>\n\n<p>(Is this recursive subdivision manual, or automated? Assuming automated...)</p>\n\n<p>In what way must one produce code in order for it to be suitable for reduction to a cache-oblivious form? Does this rely on the specific implementation of recursive reduction applicable in one\'s given case?</p>\n\n<p>Another way of putting the question is, would I need to write my code in a special way in order for some automated parser to run through the code and optimise it for cache-oblivious access?</p>\n\n<p>P.S. I\'m trying to get a handle on how to approach writing code for practical optimisation using the methods noted in the literature.</p>\n', 'Tags': '<cpu-cache>', 'LastEditorUserId': '11852', 'LastActivityDate': '2013-12-05T19:43:45.870', 'CommentCount': '4', 'CreationDate': '2013-11-27T18:42:42.693', 'Id': '18657'},108_23:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I was taught that when a reference is mapped to a cache block, X, for the first time, the word is stored in the cache block, bearing a tag and index that helps identify it for future reads.  Then, when a later reference is deemed to have the same tag and index for cache block X, the value inside the cache block is returned as the read value. Is it the same process for multi-word caches?</p>\n\n<p>I was given a direct-mapped cache: Four 4-word blocks, 32 bits per word, and these word address references...</p>\n\n<pre><code>Ref   Tag   Index   Hit/Miss\n134   8     1       Miss\n212   13    1       Miss\n135   8     1       Hit\n213   13    1       Hit\n</code></pre>\n\n<p>...were what I determine to be the hit/miss for the references because the tags and the index match, even if the value is wrong.  But the answer key tells me the hit/miss for the references are:</p>\n\n<pre><code>Ref   Tag   Index   Hit/Miss\n134   8     1       Miss\n212   13    1       Miss\n135   8     1       Miss\n213   13    1       Miss\n</code></pre>\n\n<p>...so there's more involved when it comes to multi-word caches?  ...or the answer key is wrong.  Can anyone clarify?</p>\n", 'ViewCount': '103', 'Title': 'What determines a hit/miss with cache memory?', 'LastEditorUserId': '13093', 'LastActivityDate': '2014-01-24T14:28:38.767', 'LastEditDate': '2014-01-24T14:28:38.767', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11887', 'Tags': '<memory-management><cpu-cache>', 'CreationDate': '2013-12-06T22:00:59.117', 'Id': '18696'},108_24:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the following very simple computer program:</p>\n\n<pre><code>for i = 1 to n:\n    y[i] = x[p[i]]\n</code></pre>\n\n<p>Here $x$ and $y$ are $n$-element arrays of bytes, and $p$ is an $n$-element array of words. Here $n$ is large, e.g., $n = 2^{31}$ (so that only a negligible fraction of the data fits in any kind of cache memory).</p>\n\n<p>Assume that $p$ consists of <strong>random numbers</strong>, uniformly distributed between $1$ and $n$.</p>\n\n<p>From the perspective of modern hardware, this should mean the following:</p>\n\n<ul>\n<li>reading $p[i]$ is cheap (sequential read)</li>\n<li>reading $x[p[i]]$ is very expensive (random reads; almost all reads are cache misses; we will have to fetch each individual byte from the main memory)</li>\n<li>writing $y[i]$ is cheap (sequential write).</li>\n</ul>\n\n<p>And this is indeed what I am observing. The program is very slow in comparison with a program that does only sequential reads and writes. Great.</p>\n\n<p>Now comes the question: <strong>how well does this program parallelise</strong> on modern multi-core platforms?</p>\n\n<hr>\n\n<p>My hypothesis was that this program does not parallelise well. After all, the bottleneck is the main memory. A single core is already wasting most of its time just waiting for some data from the main memory.</p>\n\n<p>However, this was <em>not</em> what I observed when I started experimenting with some algorithms where the bottleneck was this kind of operation!</p>\n\n<p>I simply replaced the naive for-loop with an OpenMP parallel for-loop (in essence, it will just split the range $[1,n]$ to smaller parts and run these parts on different CPU cores in parallel).</p>\n\n<p>On low-end computers, speedups were indeed minor. But on higher-end platforms I was surprised that I was getting excellent near-linear speedups. Some concrete examples (the exact timings may be a bit off, there is a lot of random variation; these were just quick experiments):</p>\n\n<ul>\n<li><p>2 x 4-core Xeon (in total 8 cores): factor 5-8 speedups in comparison to single-threaded version.</p></li>\n<li><p>2 x 6-core Xeon (in total 12 cores): factor 8-14 speedups in comparison to single-threaded version.</p></li>\n</ul>\n\n<p>Now this was totally unexpected. Questions:</p>\n\n<ol>\n<li><p>Precisely <strong>why does this kind of program parallelise so well</strong>? What happens in the hardware? (My current guess is something along these lines: the random reads from different thread are "pipelined" and the average rate of getting answers to these is much higher than in the case of a single thread.)</p></li>\n<li><p>Is it <strong>necessary to use multiple threads and multiple cores</strong> to obtain any speedups? If some kind of pipelining indeed takes place in the interface between the main memory and the CPU, couldn\'t a single-threaded application let the main memory know that it will soon need $x[p[i]]$, $x[p[i+1]]$, ... and the computer could start fetching the relevant cache lines from the main memory? If this is possible in principle, how do I achieve it in practice?</p></li>\n<li><p>What is the right <strong>theoretical model</strong> that we could use to analyse this kind of programs (and make <em>correct</em> predictions of the performance)?</p></li>\n</ol>\n\n<hr>\n\n<p><strong>Edit:</strong> There is now some source code and benchmark results available here: <a href="https://github.com/suomela/parallel-random-read" rel="nofollow">https://github.com/suomela/parallel-random-read</a></p>\n\n<p>Some examples of ballpark figures ($n = 2^{32}$):</p>\n\n<ul>\n<li>approx. 42 ns per iteration (random read) with a single thread</li>\n<li>approx. 5 ns per iteration (random read) with 12 cores.</li>\n</ul>\n', 'ViewCount': '358', 'Title': u'Parallelising random reads seems to work well \u2014 why?', 'LastEditorUserId': '232', 'LastActivityDate': '2013-12-14T11:41:53.687', 'LastEditDate': '2013-12-11T21:12:42.423', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '232', 'Tags': '<parallel-computing><cpu-cache><memory-hardware>', 'CreationDate': '2013-12-10T16:19:59.237', 'FavoriteCount': '2', 'Id': '18834'},108_25:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am implementing a sample <a href="http://en.wikipedia.org/wiki/MESI_protocol" rel="nofollow">MESI</a> simulator having two levels of cache (write back). I have added MESI status bits to both levels of cache. As it is a write back cache, the cache line is updated to L2 only when it is flushed. My doubts are</p>\n\n<ol>\n<li><p>what should be the behavior when a cache line with INVALID state is flushed from L1 cache. Will it just ignore the transaction? It seems that is the only possibility..but it doesn\'t seem right.</p></li>\n<li><p>Consider processor1(P1) modifying a cacheline shared by processor2(P2). Then that cache line in P2 will get status INVALID. If P2 has to update the same cache line in future and sees the state is INVALID, it should read the updated value from??what if it is still in modified state in P1(not yet written back to L2/Main memory)? </p></li>\n<li><p>Consider a similar situation that P1 has a cache line in MODIFIED state, P2 has the same line in INVALID state. When P3 tries to retrieve the same line, it broadcasts a request to all L1 caches. According to the theory,if P3 cant get the cache line from any other L1 caches, it sends request to L2/main memory. In this case where will P3 get the requested cache line from? From P1 or from the L2/main memory? Or will P1 update to the main memory first and then send the cache line to P3?</p></li>\n<li><p>I am using LRU for flushing the cache(write back). When flushing a cache if it is INVALID what should be the behavior? It just ignores the line?</p></li>\n</ol>\n', 'ViewCount': '60', 'Title': 'MESI Protocol Invalid cache line is attempted to be stored?', 'LastEditorUserId': '12182', 'LastActivityDate': '2013-12-18T14:39:11.233', 'LastEditDate': '2013-12-18T14:39:11.233', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12182', 'Tags': '<cpu-cache><protocols>', 'CreationDate': '2013-12-17T14:44:23.003', 'Id': '19066'},108_26:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I understand that MESI is a subset of the MOESI cache coherency protocol. But what does the Owned state in the MOESI protocol represent? What are the differences in state transition due to the extra Owned state in MOESI as compared to MESI? </p>\n\n<p>For example consider same cache line in processor P1 is in OWNED state &amp; processor P2 is in SHARED state. What happens when there is a write request to P2?</p>\n', 'ViewCount': '36', 'Title': 'Owned state in MOESI protocol-transitions?', 'LastActivityDate': '2014-03-25T23:30:53.750', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12182', 'Tags': '<computer-architecture><cpu-cache><protocols>', 'CreationDate': '2013-12-19T13:50:58.513', 'Id': '19119'},108_27:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider a system with a two-level paging scheme in which a regular memory access takes 150 nsec and servicing a page fault takes 8 millisec. An average instruction takes 100 nsec of CPU time and two memory accesses. The TLB hit ratio is 90%, and the page fault rate is one in every 10000 instructions. What is the effective average instruction execution time?<br>\na) 645 nsec b) 1050 nsec c) 1215 nsec d) 1230 nsec</p>\n\n<p>My Thinking :  </p>\n\n<p>Memory access = 150 ns </p>\n\n<p>No Page fault =  0.9999<br>\nPage fault     = 0.0001    </p>\n\n<p>TLB Hit =  0.9<br>\nTLB miss = 0.1   </p>\n\n<p>For TLB Hit  = 1 memory access<br>\nFor TLB miss = 3 memory access ( 2 for page table and 1 for actual data )   </p>\n\n<p>For TLB hit and miss equation will be   : ( 0.9 * 150 + 0.1 * 450 )   </p>\n\n<p>i.e. NO PAGE FAULT = 0.9999 * ( 0.9 * 150 + 0.1 * 450 )  </p>\n\n<p>For page fault : 0.0001 * ( 8 * 10 ^ 6 )  </p>\n\n<p>Average execution time = 0.9999 * ( 0.9 * 150 + 0.1 * 450 ) +  0.0001 * ( 8 * 10 ^ 6 )   </p>\n\n<p>Now my question is , what is the use of this line ? : An average instruction takes 100 nsec of CPU time and two memory accesses.  </p>\n\n<p>What is the meaning of average instruction ?<br>\nPlease explain the use and  meaning of the above line and correct me in above problem.</p>\n', 'ViewCount': '38', 'ClosedDate': '2014-01-19T02:10:59.667', 'Title': 'Finding TLB hit and miss', 'LastActivityDate': '2014-01-04T14:58:52.823', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<computer-architecture><cpu-cache><virtual-memory>', 'CreationDate': '2014-01-04T14:58:52.823', 'Id': '19497'},108_28:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Could you please check (and probably clarify) my understanding of the difference between these two concepts: Sequential Consistency and Cache Coherence?</p>\n\n<p>According to this paper[1] sequential consistency can be described as (not formally): </p>\n\n<blockquote>\n  <p><em>Sequential consistency</em> memory model specifies that the system must appear to execute all threads\u2019 loads and stores to all memory locations in a total order that respects the program order of each thread. Each load gets the value of the most recent store in that total order. </p>\n</blockquote>\n\n<p>In other words, system is sequentially consistent, if given memory events (loads and stores) of each thread we can order all these events such that: 1) for each thread the order of its events is preserved, and 2) the global order is serial (any load returns latest value stored).</p>\n\n<p>Now they [1] continue and describe coherence:</p>\n\n<blockquote>\n  <p>A definition of <em>coherence</em> that is analogous to the definition of Sequential Consistency is that a coherent system must appear to execute all threads\u2019 loads and stores to a single memory location in a total order that respects the program order of each thread. </p>\n</blockquote>\n\n<p>In other words, system is coherent, if given memory events of each thread <em>for each location</em> we can order events for that location, such that: 1) for each thread the order of its events <em>to that location</em> is preserved, and 2) <em>for each location</em> the order is serial.</p>\n\n<p>Finally, they point out the difference: </p>\n\n<blockquote>\n  <p>This definition highlights an important <em>distinction between coherence and consistency</em>: coherence is specified on a per-memory location basis, whereas consistency is specified with respect to all memory locations.</p>\n</blockquote>\n\n<p>Could you clarify please if the below is correct?:</p>\n\n<ol>\n<li><p>The difference is that for coherent systems we need a total order on all events for each location (thus the ordering between events for particular location), while for consistent systems the total order should be defined on the all events (and thus the ordering is also between events for different locations)?</p></li>\n<li><p>Does it mean that coherence is less strict that consistency? (which seems amusing!) I.e., there are traces that are coherent but not consistent, for example:</p>\n\n<pre><code>initially A=B=0\nprocess 1               process 2\nstore A := 1            load B (gets 1)\nstore B := 1            load A (gets 0) \n</code></pre>\n\n<p>This trace is coherent: </p>\n\n<ul>\n<li>for A the order is: <code>proc2 loads A(gets 0)</code>, <code>proc1 stores A:=1</code></li>\n<li>for B the order is: <code>proc1 stores B:=1</code>, <code>proc2 loads B(gets 1)</code></li>\n</ul>\n\n<p>But it is not consistent! Since if <code>proc2 load B</code> returns 1, then <code>proc1 store A := 1</code> already happened and <code>proc2 load A</code> should also return 1.</p></li>\n</ol>\n\n<p>[1]: A Primer on Memory Consistency and Cache Coherence <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00346ED1V01Y201104CAC016" rel="nofollow">http://www.morganclaypool.com/doi/abs/10.2200/S00346ED1V01Y201104CAC016</a></p>\n\n<p>Thanks!</p>\n', 'ViewCount': '55', 'Title': 'Memory Consistency vs Cache Coherence', 'LastActivityDate': '2014-01-28T19:02:54.547', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2260', 'Tags': '<cpu-cache><shared-memory>', 'CreationDate': '2014-01-28T19:02:54.547', 'Id': '20044'},108_29:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I noticed that in write-back cache memories, when cpu want to write on a block, it should fetch it from memory then update that block. so if block is going to be overwritten and changed by processor, so why we should read that block from memory?</p>\n', 'ViewCount': '26', 'Title': 'Why we need to read memory on a write-miss?', 'LastActivityDate': '2014-02-05T14:28:12.107', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '21319', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14407', 'Tags': '<cpu-cache>', 'CreationDate': '2014-02-05T14:28:12.107', 'Id': '21318'},108_30:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Should the size of its cache line, the width of bus between memory and CPU, and the size of its registers be all equal to the CPU bit?</p>\n\n<p>Is CPU bit determined by the size of its cache line, the width of bus between memory and CPU, or the size of its registers, something else?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '48', 'Title': 'CPU bit, its cache line, the bus between memory and CPU, and its registers?', 'LastActivityDate': '2014-02-06T02:03:49.730', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<cpu-cache><memory-hardware>', 'CreationDate': '2014-02-05T22:19:43.713', 'Id': '21331'},108_31:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>We all know how to use \u201cBig O\u201d notation to show how CPU instructions run increase as the size of the dataset increases.   E.g.  a quick sort is O(n log n).</p>\n\n<p>However for the last few years, instructions that don\u2019t access any data outside of the level 1 cache have been close to free compared to instructions that hit main memory.</p>\n\n<p>So is there some notation that is as clear as the \u201cBig O\u201d notation to help capture the sort of effect?     </p>\n', 'ViewCount': '24', 'Title': 'How do you compare algorithms based on scaling of their cache misses?', 'LastEditorUserId': '15725', 'LastActivityDate': '2014-03-15T14:58:13.743', 'LastEditDate': '2014-03-15T14:58:13.743', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15725', 'Tags': '<algorithm-analysis><cpu-cache>', 'CreationDate': '2014-03-15T14:49:07.390', 'Id': '22648'},108_32:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '39', 'Title': 'Does exploiting a spatial Locality in Cache always leads to a lower miss rate?', 'LastEditDate': '2014-03-26T10:00:52.293', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14769', 'FavoriteCount': '1', 'Body': "<p>I've read that, incorporating many words(spatial locality) per cache blocks leads to lower miss rate. Is it the case always? \nOne possibility of such approach is to make a single cache block of size equal to the size of the cache, but that would be meaningless as far as benefits of memory hierarchy are concerned. Isn't it so?</p>\n", 'Tags': '<computer-architecture><memory-management><cpu-cache>', 'LastEditorUserId': '14769', 'LastActivityDate': '2014-03-26T10:00:52.293', 'CommentCount': '0', 'AcceptedAnswerId': '22974', 'CreationDate': '2014-03-23T05:56:25.657', 'Id': '22960'},108_33:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have completed a computer architecture course and the last topic i have learned was cache memories. I have peeked in random tests of course from the last few years and all of them have a given mips assembler with a question about a cache.</p>\n\n<p>I have no idea how to calculate things like miss rate or hit rate from the given code.\nI know that what I should notice is lw,sw,lb and other memory stuff but i have no idea how to approach questions like that, and i always have alot of mistakes trying to solve them.</p>\n\n<p>How can i calculate the hit rate of a given cache using a given code?</p>\n\n<p>Edit: I see that nobody is answering, so I will give an example of a question:</p>\n\n<p>Given the following code:</p>\n\n<blockquote>\n  <p>1 ADDI R30, R0, 0x2000 </p>\n  \n  <p>2 ORI R10, R0, -10 </p>\n  \n  <p>LOOP: </p>\n  \n  <p>3 LW R2, 0x4000(R30)</p>\n  \n  <p>4 LW R3, 0x8000(R30) </p>\n  \n  <p>5 SUB R4, R3, R2</p>\n  \n  <p>6 BGE R4, R0, 1 </p>\n  \n  <p>7 SW R4, 0xC000(R30) </p>\n  \n  <p>8 ADDI R30, R30, 4</p>\n  \n  <p>9 ADDI R10, R10, 1</p>\n  \n  <p>10 BGE R0, R10, LOOP</p>\n  \n  <p>END</p>\n</blockquote>\n\n<p>And the following cache:\nDirect-Mapped cache of size 16KB.\nBlock size is 32 bits.</p>\n\n<p>Now, I know that the set is of size 9 bits, but how do I calculate the hit rate of the cache in the data accesses? (SW and LW)</p>\n\n<p>I have asked the lecturer and he told me the hit rate is 0%, but he left before I could ask why. That's why I'm asking it here.</p>\n", 'ViewCount': '37', 'Title': 'Caches: connection between a given code and a cache', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-03-25T22:23:46.277', 'LastEditDate': '2014-03-25T18:57:02.623', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2014-03-25T16:36:59.733', 'Id': '23036'},108_34:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>If I have 2 CPU's of the same manufacturer... say AMD</p>\n\n<p>Both are Quad-Core, Both are rated at 3.6Ghz</p>\n\n<p>1 is 100W, the other is 65W</p>\n\n<p>Will the one with the higher wattage out-perform the lower one and why?</p>\n", 'ViewCount': '17', 'ClosedDate': '2014-04-16T17:15:44.867', 'Title': 'Difference in CPU Wattage Question', 'LastActivityDate': '2014-04-15T21:18:42.400', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '16806', 'Tags': '<cpu-cache><cpu-pipelines>', 'CreationDate': '2014-04-15T20:56:43.810', 'Id': '23828'}