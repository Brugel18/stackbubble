{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3042', 'Title': 'How does one know which notation of time complexity analysis to use?', 'LastEditDate': '2013-06-06T14:12:06.230', 'AnswerCount': '3', 'Score': '41', 'PostTypeId': '1', 'OwnerUserId': '110', 'FavoriteCount': '18', 'Body': '<p>In most introductory algorithm classes, notations like $O$ (Big O) and $\\Theta$ are introduced, and a student would typically learn to use one of these to find the time complexity.</p>\n\n<p>However, there are other notations, such as $o$, $\\Omega$ and $\\omega$. Are there any specific scenarios where one notation would be preferable to another?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2014-04-01T22:12:52.083', 'CommentCount': '1', 'AcceptedAnswerId': '61', 'CreationDate': '2012-03-07T01:42:10.933', 'Id': '57'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wrote</p>\n\n<p>$\\qquad \\displaystyle \\sum\\limits_{i=1}^n \\frac{1}{i} = \\sum\\limits_{i=1}^n \\cal{O}(1) = \\cal{O}(n)$</p>\n\n<p>but my friend says this is wrong. From the TCS cheat sheet I know that the sum is also called $H_n$ which has logarithmic growth in $n$. So my bound is not very sharp, but is sufficient for the analysis I needed it for.</p>\n\n<p>What did I do wrong?</p>\n\n<p><strong>Edit</strong>:\nMy friend says that with the same reasoning, we can prove that</p>\n\n<p>$\\qquad \\displaystyle \\sum\\limits_{i=1}^n i = \\sum\\limits_{i=1}^n \\cal{O}(1) = \\cal{O}(n)$</p>\n\n<p>Now this is obviously wrong! What is going on here?</p>\n', 'ViewCount': '219', 'Title': 'What goes wrong with sums of Landau terms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-16T21:32:29.560', 'LastEditDate': '2012-03-15T07:06:28.017', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '389', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-03-14T13:12:56.093', 'Id': '366'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What are some good online resources that will help me better understand BigO notation, running time &amp; invariants?</p>\n\n<p>I'm looking for lectures, interactive examples if possible. </p>\n", 'ViewCount': '464', 'Title': 'BigO, Running Time, Invariants - Learning Resources', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-29T08:45:22.170', 'LastEditDate': '2012-03-26T05:30:58.720', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '326', 'Tags': '<algorithms><landau-notation><education><runtime-analysis>', 'CreationDate': '2012-03-21T03:05:09.847', 'FavoriteCount': '2', 'Id': '569'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to understand what is wrong with the following proof of the following recurrence </p>\n\n<p>$$\r\nT(n) = 2\\,T\\!\\left(\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\right)+n \r\n$$\n$$\r\nT(n) \\leq 2\\left(c\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\right)+n \\leq cn+n = n(c+1) =O(n)\r\n$$</p>\n\n<p>The documentation says it's wrong because of the inductive hypothesis that\n$$\r\nT(n) \\leq cn\r\n$$\nWhat Am I missing?</p>\n", 'ViewCount': '418', 'Title': 'Error in the use of asymptotic notation', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-26T18:24:32.487', 'LastEditDate': '2012-03-26T18:24:32.487', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '777', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '736', 'Tags': '<algorithms><landau-notation><asymptotics><recurrence-relation>', 'CreationDate': '2012-03-25T21:16:46.657', 'Id': '772'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '767', 'Title': 'Sorting functions by asymptotic growth', 'LastEditDate': '2013-06-06T16:01:40.233', 'AnswerCount': '5', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '776', 'FavoriteCount': '12', 'Body': '<p>Assume I have a list of functions, for example </p>\n\n<p>$\\qquad n^{\\log \\log(n)},  2^n, n!, n^3, n \\ln n, \\dots$</p>\n\n<p>How do I sort them asymptotically, i.e. after the relation defined by</p>\n\n<p>$\\qquad f \\leq_O g \\iff f \\in O(g)$,</p>\n\n<p>assuming they are indeed pairwise comparable (see also <a href="http://cs.stackexchange.com/questions/1780/are-the-functions-always-asymptotically-comparable">here</a>)? Using the definition of $O$ seems awkward, and it is often hard to prove the existence of suitable constants $c$ and $n_0$.</p>\n', 'Tags': '<asymptotics><landau-notation><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-11-19T13:21:17.813', 'CommentCount': '1', 'AcceptedAnswerId': '827', 'CreationDate': '2012-03-27T15:47:31.160', 'Id': '824'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '360', 'Title': 'How to prove that  $n(\\log_3(n))^5 = O(n^{1.2})$?', 'LastEditDate': '2012-06-02T00:51:16.560', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '886', 'FavoriteCount': '1', 'Body': "<p>This a homework question from Udi Manber's book. Any hint would be nice :)</p>\n\n<p>I must show that:</p>\n\n<blockquote>\n  <p>$n(\\log_3(n))^5 = O(n^{1.2})$</p>\n</blockquote>\n\n<p>I tried using Theorem 3.1 of book:</p>\n\n<blockquote>\n  <p>$f(n)^c = O(a^{f(n)})$ (for $c &gt; 0$, $a &gt; 1$)</p>\n</blockquote>\n\n<p>Substituing:</p>\n\n<p>$(\\log_3(n))^5 = O(3^{\\log_3(n)}) = O(n) $</p>\n\n<p>but $n(\\log_3(n))^5 = O(n\\cdot n) = O(n^2) \\ne O(n^{1.2})$</p>\n\n<p>Thank you for any help.</p>\n", 'Tags': '<asymptotics><landau-notation><mathematical-analysis>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:24:44.133', 'CommentCount': '5', 'AcceptedAnswerId': '975', 'CreationDate': '2012-04-02T00:59:41.290', 'Id': '974'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '237', 'Title': '$\\log^*(n)$ runtime analysis', 'LastEditDate': '2012-04-18T19:11:59.100', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1132', 'FavoriteCount': '1', 'Body': "<p>So I know that $\\log^*$ means iterated logarithm, so $\\log^*(3)$ = $(\\log\\log\\log\\log...)$ until $n \\leq 1$.</p>\n\n<p>I'm trying to solve the following:</p>\n\n<p>is </p>\n\n<blockquote>\n  <p>$\\log^*(2^{2^n})$</p>\n</blockquote>\n\n<p>little $o$, little $\\omega$, or $\\Theta$ of</p>\n\n<blockquote>\n  <p>${\\log^*(n)}^2$</p>\n</blockquote>\n\n<p>In terms of the interior functions, $\\log^*(2^{2^n})$ is much bigger than $\\log^*(n)$, but squaring the $\\log^*(n)$ is throwing me off. </p>\n\n<p>I know that $\\log(n)^2$ is $O(n)$, but I don't think that property holds for the iterative logarithm.</p>\n\n<p>I tried applying the master method, but I'm having trouble with the properties of a $\\log^*(n)$ function. I tried setting n to be max (i.e. $n = 5$), but this didn't really simplify the problem.</p>\n\n<p>Does anyone have any tips as to how I should approach this?</p>\n", 'Tags': '<asymptotics><landau-notation><mathematical-analysis>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-18T19:11:59.100', 'CommentCount': '0', 'AcceptedAnswerId': '1340', 'CreationDate': '2012-04-18T14:32:10.103', 'Id': '1339'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I asked a (seed) question about sums of Landau terms <a href="http://cs.stackexchange.com/questions/366/what-goes-wrong-with-sums-of-landau-terms">before</a>, trying to gauge the dangers of abusing asymptotics notation in arithmetics, with mixed success.</p>\n\n<p>Now, <a href="http://cs.stackexchange.com/a/2803/98">over here</a> our recurrence guru <a href="http://cs.stackexchange.com/a/2803/98">JeffE</a> does essentially this:</p>\n\n<p>$\\qquad \\displaystyle \\sum_{i=1}^n \\Theta\\left(\\frac{1}{i}\\right) = \\Theta(H_n)$</p>\n\n<p>While the end result is correct, I think this is wrong. Why? If we add in all the existence of constants implied (only the upper bound), we have</p>\n\n<p>$\\qquad \\displaystyle \\sum_{i=1}^n c_i \\cdot \\frac{1}{i} \\leq c \\cdot H_n$.</p>\n\n<p>Now how do we compute $c$ from $c_1, \\dots, c_n$? The answer is, I believe, that we can not: $c$ has to bound for all $n$ but we get <em>more</em> $c_i$ as $n$ grows. We don\'t know anything about them; $c_i$ may very well depend on $i$, so we can not assume a bound: a finite $c$ may not exist.</p>\n\n<p>In addition, there is this subtle issue of which variable goes to infinity on the left-hand side -- $i$ or $n$? Both? If $n$ (for the sake of compatibility), what is the meaning of $\\Theta(1/i)$, knowing that $1 \\leq i \\leq n$? Does it not only mean $\\Theta(1)$? If so, we can\'t bound the sum better than $\\Theta(n)$.</p>\n\n<p>So, where does that leave us? It it a blatant mistake? A subtle one? Or is it just the usual abuse of notation and we should not look at $=$ signs like this one out of context? Can we formulate a (rigorously) correct rule to evalutate (certain) sums of Landau terms?</p>\n\n<p>I think that the main question is: what is $i$? If we consider it constant (as it <em>is</em> inside the scope of the sum) we can easily build counterexamples. If it is not constant, I have no idea how to read it.</p>\n', 'ViewCount': '243', 'Title': 'Sums of Landau terms revisited', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-01T14:11:11.147', 'LastEditDate': '2012-07-18T22:01:47.640', 'AnswerCount': '3', 'CommentCount': '19', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2012-07-18T15:35:15.237', 'FavoriteCount': '1', 'Id': '2814'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '461', 'Title': 'Solving the recurrence relation $T(n) = 2T(\\lfloor n/2 \\rfloor) + n$', 'LastEditDate': '2012-07-31T20:30:24.733', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '778', 'FavoriteCount': '1', 'Body': '<p>Solving the recurrence relation $T(n) = 2T(\\lfloor n/2 \\rfloor) + n$.<br>\nThe book from which this example is, falsely claims that $T(n) = O(n)$ by guessing $T(n) \\leq cn$ and then arguing  </p>\n\n<p>$\\qquad \\begin{align*} T(n) &amp; \\leq 2(c \\lfloor n/2 \\rfloor ) + n \\\\ &amp;\\leq cn +n \\\\ &amp;=O(n) \\quad \\quad \\quad \\longleftarrow \\text{ wrong!!} \\end{align*}$  </p>\n\n<p>since $c$ is constant.The error is that we have not proved the <em>exact</em> form of the inductive hypothesis.</p>\n\n<p>Above I have exactly quoted what the book says. Now my question is why cannot we write $cn+n=dn$ where $d=c+1$ and now we have $T(n) \\leq dn$ and hence $T(n) = O(n)$?</p>\n\n<p>Note: </p>\n\n<ol>\n<li>The correct answer is $T(n) =O(n \\log n).$  </li>\n<li>The book I am referring here is <em>Introduction to algorithms</em> by Cormen et al., page 86, 3rd edition.</li>\n</ol>\n', 'Tags': '<proof-techniques><asymptotics><recurrence-relation><landau-notation><induction>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-01T07:09:01.457', 'CommentCount': '1', 'AcceptedAnswerId': '2972', 'CreationDate': '2012-07-31T18:15:32.510', 'Id': '2971'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '802', 'Title': 'What is the meaning of $O(m+n)$?', 'LastEditDate': '2012-08-13T22:22:38.217', 'AnswerCount': '2', 'Score': '26', 'OwnerDisplayName': 'Frank', 'PostTypeId': '1', 'OwnerUserId': '9667', 'FavoriteCount': '5', 'Body': '<p>This is a basic question, but I\'m thinking that $O(m+n)$ is the same as $O(\\max(m,n))$, since the larger term should dominate as we go to infinity? Also, that would be different from $O(\\min(m,n))$. Is that right? I keep seeing this notation, especially when discussing graph algorithms. For example, you routinely see: $O(|V| + |E|)$ (e.g. see <a href="http://algs4.cs.princeton.edu/41undirected/">here</a>).</p>\n', 'Tags': '<terminology><asymptotics><mathematical-analysis><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-13T22:22:38.217', 'CommentCount': '1', 'AcceptedAnswerId': '3150', 'CreationDate': '2012-08-13T15:29:47.517', 'Id': '3149'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ be a continuous positive function, where $f(n)$ is integer for each integer $n$. Prove or disprove whether the following always holds:</p>\n\n<p>$\\qquad f(n+1) = \\Theta(f(n))$</p>\n', 'ViewCount': '301', 'Title': 'Asymptotic growth rate of $f(n)$ and $f(n+1)$', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-17T16:01:14.057', 'LastEditDate': '2012-08-17T09:40:13.643', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '5', 'OwnerDisplayName': 'Anirban Ghosh', 'PostTypeId': '1', 'OwnerUserId': '2556', 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'CreationDate': '2012-08-16T19:01:06.800', 'Id': '3231'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So I have this question to prove a statement:</p>\n\n<p>$O(n)\\subset\\Theta(n)$...</p>\n\n<p>I don't need to know how to prove it, just that in my mind this makes no sense and I think it should rather be that $\\Theta(n)\\subset O(n)$. </p>\n\n<p>My understanding is that $O(n)$ is the set of all functions who do no worse than $n$ while $\\Theta(n)$ is the set of all functions that do no better and no worse than n. </p>\n\n<p>Using this, I can think of the example of a constant function say $g(n)=c$. This function will surely be an element of $O(n)$ as it will do no worse than $n$ as $n$ approaches a sufficiently large number. </p>\n\n<p>However, the same function $g$ would not be an element of $\\Theta(n)$ as g does do better than $n$ for large $n$... Then since $g  \\in  O(n)$ and $g \\not\\in \\Theta(n)$, then $O(n)\\not\\in\\Theta(n)$  </p>\n\n<p>So is the question perhaps wrong ? I've learnt it is dangerous to make that assumption and usually I have missed something, I just can't see what it might be in this case. </p>\n\n<p>Any thoughts ? \nThanks a lot.. </p>\n", 'ViewCount': '205', 'Title': 'Is $O$ contained in $\\Theta$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-18T06:21:28.647', 'LastEditDate': '2012-08-18T06:21:28.647', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '3247', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2562', 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'CreationDate': '2012-08-17T14:53:47.270', 'Id': '3240'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Big Theta notation used for defining the running time of an algorithm, are the constants $c_1$ and $c_2$ <em>different</em> for every value of $n$?</p>\n\n<p>Definition:</p>\n\n<p>$\\qquad \\displaystyle \\Theta (g(n)) = \\{ f(n): \\exists\\, c_1,c_2,n_0&gt;0. \\forall n \\geq n_0.\\ \\ c_1g(n) \\leq f(n) \\leq c_2g(n) \\}.$</p>\n', 'ViewCount': '216', 'Title': 'Value of constants in Big Theta notation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-10T14:08:00.560', 'LastEditDate': '2012-09-08T21:39:00.347', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'sai', 'PostTypeId': '1', 'Tags': '<terminology><asymptotics><mathematical-analysis><landau-notation>', 'CreationDate': '2012-09-08T05:00:24.313', 'Id': '3470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '862', 'Title': 'Changing variables in recurrence relations', 'LastEditDate': '2012-09-09T23:16:22.610', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'goooser', 'PostTypeId': '1', 'OwnerUserId': '2768', 'FavoriteCount': '1', 'Body': '<p>Currently, I am self-studying Intro to Algorithms (CLRS) and there is one particular method they outline in the book to solve recurrence relations. </p>\n\n<p>The following method can be illustrated with this example. Suppose we have the recurrence</p>\n\n<p>$$T(n) = 2T(\\sqrt n) + \\log n$$</p>\n\n<p>Initially they make the substitution m = lg(n), and then plug it back in to the recurrence and get:</p>\n\n<p>$$T(2^m) = 2T(2^{\\frac{m}{2}}) + m$$</p>\n\n<p>Up to this point I understand perfectly. This next step is the one that\'s confusing to me.</p>\n\n<p>They now "rename" the recurrence $S(m)$ and let $S(m) = T(2^m)$, which apparently produces</p>\n\n<p>$$S(m) = 2S(m/2) + m$$</p>\n\n<p>For some reason it\'s not clear to me why this renaming works, and it just seems like cheating. Can anyone explain this better? </p>\n', 'Tags': '<asymptotics><recurrence-relation><mathematical-analysis><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-09T23:16:22.610', 'CommentCount': '0', 'CreationDate': '2012-09-09T20:03:08.237', 'Id': '3482'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '253', 'Title': 'Complexity inversely propotional to $n$', 'LastEditDate': '2012-09-11T01:36:36.780', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1272', 'FavoriteCount': '1', 'Body': '<p>Is it possible an algorithm complexity decreases by input size? Simply $O(1/n)$ possible?</p>\n', 'Tags': '<algorithms><time-complexity><asymptotics><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-11T01:36:36.780', 'CommentCount': '2', 'AcceptedAnswerId': '3497', 'CreationDate': '2012-09-10T15:09:25.010', 'Id': '3495'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '620', 'Title': 'Solving $T(n)= 3T(\\frac{n}{4}) + n\\cdot \\lg(n)$ using the master theorem', 'LastEditDate': '2013-09-28T12:36:33.403', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1758', 'FavoriteCount': '1', 'Body': '<p><em>Introduction to Algorithms</em>, 3rd edition (p.95) has an example of how to solve the recurrence</p>\n\n<p>$$\\displaystyle T(n)= 3T\\left(\\frac{n}{4}\\right) + n\\cdot \\log(n)$$</p>\n\n<p>by applying the Master Theorem.</p>\n\n<p>I am very confused by how it is done. So, $a=3, b=4, f(n) = n\\cdot \\log(n)$<br>\nFirst step is to compare $n^{\\log_b a} = n^{\\log_4 3}= O(n^{0.793})$ with $f(n)$.</p>\n\n<p>I have no clue on how they compared this. The book explains: </p>\n\n<blockquote>\n  <p>$f(n) = \\Omega (n^{\\log_4 3+\\epsilon })$, where $\\epsilon \\approx 0.2$, case 3 applies if we can show that the regularity condition holds for $f(n).$ </p>\n</blockquote>\n\n<p>Followed by: </p>\n\n<blockquote>\n  <p>For sufficiently large n, we have that: $af\\left(\\frac{n}{b}\\right) = 3\\left(\\frac{n}{4}\\right)\\log\\left(\\frac{n}{5}\\right) \\le\\left(\\frac{3}{4}\\right)n \\log n = cf(n)~ for~ c=\\frac{3}{4}.$</p>\n</blockquote>\n\n<p>Where did $3\\left(\\frac{n}{4}\\right)$ come from?</p>\n', 'Tags': '<asymptotics><recurrence-relation><landau-notation><mathematical-analysis><master-theorem>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-28T12:36:33.403', 'CommentCount': '0', 'AcceptedAnswerId': '3505', 'CreationDate': '2012-09-11T03:32:38.143', 'Id': '3504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1816', 'Title': 'n*log n and n/log n against polynomial running time', 'LastEditDate': '2012-09-16T06:11:06.343', 'AnswerCount': '6', 'Score': '9', 'OwnerDisplayName': 'mihsathe', 'PostTypeId': '1', 'OwnerUserId': '2957', 'FavoriteCount': '2', 'Body': '<p>I understand that $\\Theta(n)$ is faster than $\\Theta(n\\log n)$ and slower than $\\Theta(n/\\log n)$. What is difficult for me to understand is how to actually compare $\\Theta(n \\log n)$ and $\\Theta(n/\\log n)$ with $\\Theta(n^f)$ where $0 &lt; f &lt; 1$.</p>\n\n<p>For example, how do we decide $\\Theta(n/\\log n)$ vs. $\\Theta(n^{2/3})$ or $\\Theta(n^{1/3})$</p>\n\n<p>I would like to have some directions towards proceeding in such cases. Thank you.</p>\n', 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-16T06:11:06.343', 'CommentCount': '0', 'AcceptedAnswerId': '3564', 'CreationDate': '2012-09-08T03:33:33.247', 'Id': '3563'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1040', 'Title': 'Why does heapsort run in $\\Theta(n \\log n)$ instead of $\\Theta(n^2 \\log n)$ time?', 'LastEditDate': '2012-09-16T22:11:13.370', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1758', 'FavoriteCount': '1', 'Body': '<p>I am reading section 6.4 on Heapsort algorithm in CLRS, page 160.</p>\n\n<pre><code>HEAPSORT(A)  \n1 BUILD-MAX-HEAP(A)  \n2 for i to A.length downto 2  \n3   exchange A[i] with A[i]\n4   A.heap-size = A.heap-size-1  \n5   MAX-HEAPIFY(A,1)\n</code></pre>\n\n<p>Why is the running time, according to the book is $\\Theta (n\\lg{n})$ rather than $\\Theta (n^2\\lg{n})$ ? <code>BUILD-MAX-HEAP(A)</code> takes $\\Theta(n)$, <code>MAX-HEAPIFY(A,1)</code> takes $\\Theta(\\lg{n})$ and repeated $n-1$ times (line 3).</p>\n', 'Tags': '<algorithms><algorithm-analysis><landau-notation><sorting>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-09-16T22:11:13.370', 'CommentCount': '0', 'AcceptedAnswerId': '4579', 'CreationDate': '2012-09-16T20:47:58.343', 'Id': '4578'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was given a homework assignment with Big O. I'm stuck with nested for loops that are dependent on the previous loop. Here is a changed up version of my homework question, since I really do want to understand it:</p>\n\n<pre><code>sum = 0;\nfor (i = 0; i &lt; n; i++ \n    for (j = 0; j &lt; i; j++) \n        sum++;\n</code></pre>\n\n<p>The part that's throwing me off is the <code>j &lt; i</code> part. It seems like it would execute almost like a factorial, but with addition. Any hints would be really appreciated.</p>\n", 'ViewCount': '2880', 'Title': 'Big O: Nested For Loop With Dependence', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-18T20:24:32.557', 'LastEditDate': '2012-09-17T17:55:35.750', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1000229', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><landau-notation>', 'CreationDate': '2012-09-07T23:38:26.383', 'Id': '4590'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Substation method fails to prove that $T(n)=\\Theta(n^2) $ for the recursion $T(n)=4T(n/2) + n^2$, since you end up with $T(n) &lt; cn^2 \\leq cn^2 + n^2$.</p>\n\n<p>I don't understand how to subtract off lower-order term to prove that substitution works.  </p>\n\n<p>Came up with: $T(n) \\leq cn^2 - bn^2$</p>\n\n<p>Assume it holds for $T(n/2) \\leq c(n/2)^2 - b(n/2)^2$  </p>\n\n<p>$T(n) \\leq 4(c(n/2)^2 - b(n/2)^2) + n^2 = cn^2- bn^2 + n^2 $ </p>\n\n<p>However, there is no way to solve $cn^2- bn^2 + n^2 \\leq cn^2 - bn^2 $ for $b$</p>\n", 'ViewCount': '455', 'Title': 'Subtracting lower-order term to prove subtitution method works', 'LastEditorUserId': '2499', 'LastActivityDate': '2012-09-26T03:13:49.490', 'LastEditDate': '2012-09-19T12:00:43.927', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '4733', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1758', 'Tags': '<asymptotics><recurrence-relation><landau-notation>', 'CreationDate': '2012-09-19T07:43:01.930', 'Id': '4612'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have been reading <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow"><em>Introduction to Algorithms</em> by Cormen et al.</a> and I\'m reading <a href="http://books.google.co.uk/books?id=NLngYyWFl_YC&amp;pg=PA73" rel="nofollow">the statement of the Master theorem starting on page 73</a>. In case 3 there is also a regularity condition that needs to be satisfied to use the theorem:</p>\n\n<blockquote>\n  <p>... 3. If </p>\n  \n  <p>$\\qquad \\displaystyle f(n) = \\Omega(n^{\\log_b a + \\varepsilon})$</p>\n  \n  <p>for some constant $\\varepsilon &gt; 0$ and if</p>\n  \n  <p>$\\qquad \\displaystyle af(n/b) \\leq cf(n)$ \xa0\xa0\xa0\xa0\xa0[<strong>this is the regularity condition</strong>]</p>\n  \n  <p>for some constant $c &lt; 1$ and for all sufficiently large $n$, then .. </p>\n</blockquote>\n\n<p>Can someone tell me why the regularity condition is needed? How does the theorem fail if the condition is not satisfied?</p>\n', 'ViewCount': '2056', 'Title': 'Why is there the regularity condition in the master theorem?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-04T09:41:58.807', 'LastEditDate': '2012-10-04T09:41:58.807', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '7', 'OwnerDisplayName': 'GrowinMan', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><mathematical-analysis><landau-notation><master-theorem>', 'CreationDate': '2012-10-02T03:54:52.210', 'FavoriteCount': '2', 'Id': '4854'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Representing numeric values using <a href="http://en.wikipedia.org/wiki/Positional_notation">positional notation</a> is one of the milestones in the history of arithmetic. Babylons used a base 60 system, Maya a base 20 system; base 10 system became "the standard" used by modern civilizations; digital computers use the <a href="http://en.wikipedia.org/wiki/Binary_numeral_system">Binary numeral system</a>, ....</p>\n\n<p>But if we look at nature, we found that life itself "heavily rely" on an alphabet of 4 symbols: the <a href="http://en.wikipedia.org/wiki/DNA">DNA</a> has four <a href="http://en.wikipedia.org/wiki/Base_%28chemistry%29">(chemical) bases</a>: adenine, cytosine, guanine and thymine (A, C, G, T) that are used to store the "instructions and information" to generate and drive the parts of a living organism.</p>\n\n<blockquote>\nBut on a higher level, are there <em>"natural algorithms"</em> (algorithms found in natural processes, in animal behaviours or in everyday human behaviours) that take advantages of "numeral systems" other than the <a href="http://en.wikipedia.org/wiki/Unary_numeral_system">unary representation</a>. \n</blockquote>\n\n<p>To be more precise I would like to know whether or not natural processes or living creatures make use of a finite, discrete alphabet of "symbols" and use them in a manner similar to a positional notation: the symbols are placed together and used as a whole to  represent "something" (an action, an information, an object, ...) among many other possibilites ... a sort of "index" in an exponential number of possibilites.</p>\n\n<p>Another (obvious) non numeric example is the human language where (in general) a combination of finite number of sounds ("alphabet") are combined to form the words.</p>\n', 'ViewCount': '99', 'Title': 'Numeral systems other than unary used in nature or in animal and human behaviours', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-19T21:54:13.267', 'LastEditDate': '2013-11-19T21:54:13.267', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '140', 'Tags': '<integers><notation><numeral-representations>', 'CreationDate': '2012-10-06T19:31:03.573', 'Id': '4906'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let\'s say I have a graph $|G|$ with $|E|=O(V^2)$ edges. I want to run BFS on $G$ which has a running time of $O(V+E)$.</p>\n\n<p>It feels natural to write that the running time on this graph would be $O(O(V^2)+V)$ and then simplify to $O(V^2)$.</p>\n\n<p>Are there any pitfalls to using such a "remove-the-nested-O" shortcut (not just in this case, but more generally)?</p>\n', 'ViewCount': '329', 'Title': 'Nested Big O-notation', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-10-09T21:54:10.183', 'LastEditDate': '2012-10-07T14:31:31.387', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '4919', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2012-10-07T07:26:22.087', 'Id': '4913'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Are $\\log_{10}(x)$ and $\\log_{2}(x)$ in the same big-O class of functions? In other words, can one say that $\\log_{10}(x)=O(\\log x)$ and $\\log_{2}(x)=O(\\log x)$?</p>\n', 'ViewCount': '184', 'Title': 'Are $\\log_{10}(x)$ and $\\log_2(x)$ in the same big-O class of functions?', 'LastEditorUserId': '31', 'LastActivityDate': '2012-10-11T20:15:57.117', 'LastEditDate': '2012-10-09T08:14:08.203', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '4969', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<landau-notation>', 'CreationDate': '2012-10-09T06:25:15.087', 'Id': '4968'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '299', 'Title': 'How to prove $(n+1)! = O(2^{(2^n)})$', 'LastEditDate': '2012-10-16T08:03:18.780', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3004', 'FavoriteCount': '2', 'Body': "<p>I am trying to prove $(n+1)! = O(2^{(2^n)})$. I am trying to use L'Hospital rule but I am stuck with infinite derivatives.</p>\n\n<p>Can anyone tell me how i can prove this?</p>\n", 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-28T07:23:20.603', 'CommentCount': '1', 'AcceptedAnswerId': '6080', 'CreationDate': '2012-10-14T22:13:27.907', 'Id': '6075'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>While studying master method at recurrences topic I'm stacked at a point. It is written in the book as:</p>\n\n<blockquote>\n  <p>$T(n) = 3T(n/4) + n \\log n$,</p>\n  \n  <p>we have $a = 3, b = 4$, </p>\n  \n  <p>$f(n) = n \\log n$, and </p>\n  \n  <p>$n^{\\log_b(a)} = n^{\\log_4 3} = O(n^{0.793})$.</p>\n  \n  <p>Since  $f(n) = \\Omega(n^{\\log_4( 3)+\\varepsilon} )$, where  $\\varepsilon \\approx0.2$ ....</p>\n</blockquote>\n\n<p>The authors means that the $n\\log n = \\Omega(n)$. How will we know this? Is $n \\log n = \\Omega(n)$ true? Or \nsomething is wrong?</p>\n", 'ViewCount': '199', 'Title': 'Big Omega of $n \\log n$', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-09T08:28:06.703', 'LastEditDate': '2012-11-08T19:10:26.370', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user1308990', 'PostTypeId': '1', 'Tags': '<asymptotics><landau-notation><master-theorem>', 'CreationDate': '2012-10-22T10:26:01.877', 'Id': '6564'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '390', 'Title': 'Confusion about big-O notation comparison of two functions', 'LastEditDate': '2012-11-15T13:23:16.190', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4601', 'FavoriteCount': '0', 'Body': '<p>On page 16 of this <a href="http://www.cs.berkeley.edu/~vazirani/algorithms/all.pdf" rel="nofollow">algorithms book</a>, it states:</p>\n\n<blockquote>\n  <p>For example, suppose we are choosing between two algorithms for a particular computational task. One takes $f_1(n) = n^2$ steps, while the other takes $f_2(n) = 2n + 20$ steps (Figure 0.2).</p>\n</blockquote>\n\n<p>He then goes on to say:</p>\n\n<blockquote>\n  <p>This superiority ... (of $f_2$ over $f_1$) ... is captured by the big-O notation: $f_2 = O(f_1)$, because ...</p>\n</blockquote>\n\n<p>Now my problem is that in the original quote, he said that $f_1(n) = n^2$ steps and $f_2(n) = 2n+20$ steps, so thus $f_1 = O(n^2)$ and $f_2 = O(n)$ (big-O is defined in Section 0.3). But the second quote above states $f_2 = O(f_1)$, which means $f_2 = O(n^2)$ and contradicts his definition of big-O notation. What have I missed?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-15T22:33:41.427', 'CommentCount': '2', 'AcceptedAnswerId': '6689', 'CreationDate': '2012-11-15T12:39:23.337', 'Id': '6676'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A book I am reading demonstrates how $5n^3 + 2n^2 + 22n + 6 = O(n^3)$, which I believe is true. After all, there exists a value $c$ for which $cn^3$ is always greater than $5n^3 + 2n^2 + 22n + 6$ for all $n$ greater than or equal to some value $n_0$.</p>\n\n<p>However, the book then casually notes that $c = 5$ and $n_0 = 10$. Where did these values come from? What algebraic calculations were done (if any) to derive the $c$ and $n_0$ values?</p>\n', 'ViewCount': '78', 'Title': 'Finding $c$ and $n_0$ for a big-O bound', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-17T20:33:44.143', 'LastEditDate': '2012-11-17T08:26:08.783', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '6722', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-17T00:44:33.033', 'Id': '6700'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Hopefully this is the right section. I need to prove that $2^{(log(n)^{1/2})}$ is $O(n^a)$. From the basic principle of Big-O notation, I know I need to find two numbers $c$ and $N$ so that $f(n) \\le c\\cdot g(n)$ for all values of $n \\ge N$. In this case, $f(n)$ is $2^{(log(n)^{1/2})}$, and $g(n)$ is $n^a$.</p>\n\n<p>The first problem in the set had me prove that $2^{n+a}$ is $O(2^n)$. For this problem, I had separated the $f(n)$ to $2^n \\cdot 2^a$. I was guided by another user to set $c = 2^a$. This way, with $g(n) = 2^n$, $cg(n) = 2^a2^n$, which is obviously $\\ge f(n)$, $2^{n+a}$, for all values of $n \\ge 0$. This problem, however, I don't find as easy.</p>\n\n<p>Could someone point me in the right direction? I've been trying to rearrange the problem and solve it for a while now.</p>\n", 'ViewCount': '83', 'Title': 'Prove that $2^{(log(n)^{1/2})}$ is $O(n^a)$', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-17T09:03:08.617', 'LastEditDate': '2012-11-17T09:03:08.617', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4622', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-17T03:57:13.407', 'FavoriteCount': '1', 'Id': '6702'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '280', 'Title': 'When does $1.00001^n$ exceed $n^{100001}$?', 'LastEditDate': '2012-11-17T09:49:53.533', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '1', 'Body': '<p>I have been told than $n^{1000001} = O(1.000001^n)$. If that\'s the case, there must be some value $n$ at which $1.000001^n$ exceeds $n^{1000001}$.</p>\n\n<p>However, when I consult Wolfram Alpha, I get a negative value for when that occurs.\n<a href="http://www.wolframalpha.com/input/?i=1.000001%5Ex+%3D+x%5E1000001" rel="nofollow">http://www.wolframalpha.com/input/?i=1.000001%5Ex+%3D+x%5E1000001</a></p>\n\n<p>Why is that? Shouldn\'t this value be really big instead of negative?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-27T22:13:35.007', 'CommentCount': '2', 'AcceptedAnswerId': '6713', 'CreationDate': '2012-11-17T09:22:49.630', 'Id': '6711'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm currently studying for an algorithms midterm in about 2 days and am reading from the beginning of the course, and stumbled upon this when I actually looked at the examples.</p>\n\n<p>The question equation: $f(n) = 6n^3 + n^2\\log n$</p>\n\n<p>The exact line written for the answer is: $f(n) \\leq 6n^3 + n^2 \\centerdot n \\text{, for all }n \\geq 1, \\text{since} \\log n \\leq n$</p>\n\n<p>First of all, I don't really see why the logarithm was removed or why it actually matters when the dominant piece is the $6n^3$. I also don't get why it's $n \\geq 1$ instead of $n \\geq 6$ (unless it's a continuation of the first one.</p>\n\n<p>Been staring at it for about 15 minutes and still not getting how it comes down to $n \\geq 1$. Would anybody be kind enough to give me a hint as to what's wrong?</p>\n", 'ViewCount': '179', 'Title': 'Why is this $f(n) \\leq 6n^3 + n^2 \\log n \\in O(n^3)$ for all $n \\geq 1$?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-22T08:08:46.570', 'LastEditDate': '2012-11-22T08:08:46.570', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '6826', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4681', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-21T18:47:36.573', 'Id': '6823'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/2789/solving-or-approximating-recurrence-relations-for-sequences-of-numbers">Solving or approximating recurrence relations for sequences of numbers</a>  </p>\n</blockquote>\n\n\n\n<p>I know that the solution for $T(n) = 2 T(n/2) + O(n)$ is  $ T(n) = O(n \\log(n))$</p>\n\n<p>But how do you get to that point? I don\'t understand when it says put t into the equation repeatedly until it drops out...</p>\n\n<p>Any help?</p>\n', 'ViewCount': '880', 'ClosedDate': '2012-11-23T22:41:35.970', 'Title': 'Solving the big-Oh notation for $T(n) = 2 T(n/2) + O(n)$', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-23T22:17:04.403', 'LastEditDate': '2012-11-23T13:16:33.447', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'OwnerDisplayName': 'user1846486', 'PostTypeId': '1', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-23T09:08:32.463', 'Id': '6855'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '325', 'Title': 'What is the time complexity of the following program?', 'LastEditDate': '2012-12-23T09:30:04.240', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4732', 'FavoriteCount': '2', 'Body': '<p>Please help me calculate the time complexity of the following program.</p>\n\n<pre><code>int fun (int n) {\n  if (n &lt;= 2)\n   return 1;\n  else\n   return fun(sqrt(n)) + n;\n}\n</code></pre>\n\n<p>Please explain.</p>\n\n<p>There were four choices given.</p>\n\n<ol>\n<li>$\\Theta(n^2)$</li>\n<li>$\\Theta(n \\log n)$</li>\n<li>$\\Theta(\\log n)$</li>\n<li>$\\Theta(\\log \\log n)$</li>\n</ol>\n', 'Tags': '<algorithm-analysis><runtime-analysis><landau-notation>', 'LastEditorUserId': '3016', 'LastActivityDate': '2012-12-23T09:30:04.240', 'CommentCount': '0', 'AcceptedAnswerId': '6904', 'CreationDate': '2012-11-26T02:06:18.790', 'Id': '6901'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The title of the question expresses what I\'m looking for - this is to help me better understand the prerequisites for the <a href="http://en.wikipedia.org/wiki/Time_hierarchy_theorem#Non-deterministic_time_hierarchy_theorem">Non-Deterministic Time Hierarchy Theorem</a></p>\n\n<p>For instance, the Arora-Barak book explains the theorem using $g(n) = n$ and $G(n) = n^{1.5}$ - but, I can see that $n \\in o(n^{1.5})$ as well! So, I\'m trying to better understand what "extra" time is guaranteed by specifying that in order for $\\text{NTIME}(g(n))$ to be a proper subset of $\\text{NTIME}(G(n))$, $g(n + 1) = o(G(n))$, <strong>not</strong> $g(n) = o(G(n))$...  </p>\n', 'ViewCount': '127', 'Title': 'Two functions $g(n)$, $G(n)$ such that $g(n) = o(G(n))$ but $g(n+1) \\neq o(G(n))$', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-27T00:09:41.660', 'LastEditDate': '2012-11-26T22:05:53.567', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '6932', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '476', 'Tags': '<complexity-theory><time-complexity><asymptotics><landau-notation>', 'CreationDate': '2012-11-26T20:16:07.620', 'Id': '6929'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '428', 'LastEditorDisplayName': 'user742', 'Title': 'Why is $3^n = 2^{O(n)}$ true?', 'LastEditDate': '2012-12-03T08:39:00.760', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '1', 'Body': '<p>$3^n = 2^{O(n)}$ is apparently true. I thought that it was false though because $3^n$ grows faster than any exponential function with a base of 2.</p>\n\n<p>How is $3^n = 2^{O(n)}$ true?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastActivityDate': '2012-12-03T08:39:00.760', 'CommentCount': '3', 'AcceptedAnswerId': '7092', 'CreationDate': '2012-12-01T20:53:46.493', 'Id': '7091'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The following is an excerpt from <a href="http://www.amazon.ca/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1355097851&amp;sr=1-1" rel="nofollow">CLRS</a>:</p>\n\n<blockquote>\n  <p>$\\Theta(g(n))= \\{ f(n) \\mid  \\text{ $\\exists c_1,c_2,n_0&gt;0$ such that $0 \\le c_1 g(n) \\le f(n) \\le c_2g(n)$ for all $n \\ge n_0$}\\}$.</p>\n</blockquote>\n\n<p>Assuming $n \\in \\mathbb{N}$, I was unable to find $f(n)$ and $g(n)$ such that the bound does not apply for all $n$.</p>\n\n<p><strong>Note:</strong> This question was asked with the flawed assumption that $f(n)$ and $g(n)$ necessarily have natural domains.</p>\n', 'ViewCount': '234', 'Title': 'If $f(n) = \\Theta(g(n))$, do both functions bound each other for all $n$ or only sufficiently large $n$?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-10T20:25:50.367', 'LastEditDate': '2012-12-10T12:17:23.270', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7282', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4267', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-12-10T00:31:06.167', 'Id': '7281'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '138', 'Title': 'Computing time complexity', 'LastEditDate': '2012-12-26T14:53:25.097', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Rastegar', 'PostTypeId': '1', 'OwnerUserId': '5202', 'Body': '<p>If we have an algorithm such that its complexity is $\\Theta(m + n^2)$ and we know that $0 &lt; m &lt; n^2$, then its complexity becomes $\\Theta(n^2)$. But if we had an algorithm such that its complexity was $\\Theta(m\\log n)$ and $0 &lt; m &lt; n^2$, could we conclude that its complexity is $\\Theta(n^2\\log n)$?</p>\n', 'Tags': '<time-complexity><landau-notation>', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-26T18:06:57.817', 'CommentCount': '4', 'AcceptedAnswerId': '7606', 'CreationDate': '2012-12-26T11:39:49.503', 'Id': '7604'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How is an algorithm with complexity $O(n \\log n)$ also in $O(n^2)$? I'm not sure exactly what its saying here, I feel it may be something to do with the fact that big-oh is saying less than or equal to, but I am not fully sure. Any have any ideas? Thanks.</p>\n", 'ViewCount': '58', 'Title': 'How is this algorithm in these two complexities?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-12T16:37:28.597', 'LastEditDate': '2013-01-12T16:37:28.597', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7856', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3088', 'Tags': '<algorithms><asymptotics><landau-notation>', 'CreationDate': '2013-01-09T21:20:29.653', 'Id': '7855'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have an algebraic specification in the form: $\\{S,F,w\\}$ where $S$ are the sorts, $F$ are the functions and $w$ are the set of equations.\nFor example, the specification for natural numbers:</p>\n\n<ul>\n<li>$S = \\{\\mathrm{int}\\}$</li>\n<li>$F = \\{\n         0: \\mathrm{int}, \\;\n         \\mathrm{succ} : \\mathrm{int}\\rightarrow\\mathrm{int}, \\;\n         \\mathrm{pred}: \\mathrm{int}\\rightarrow\\mathrm{int}\n       \\}$</li>\n<li>$w = \\{\n         \\mathrm{succ}(\\mathrm{pred}(x)) = x, \\;\n         \\mathrm{pred}(\\mathrm{succ}(x)) = x\n       \\}$</li>\n</ul>\n\n<p>My question is, why and where do we need homomorphisms and isomorphisms in this case? How do homomorphisms and isomorphisms look like between algebras ?</p>\n', 'ViewCount': '84', 'Title': 'Homomorphisms and isomorphisms in an equational calculus', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-21T20:51:55.753', 'LastEditDate': '2013-01-21T20:51:55.753', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1584', 'Tags': '<semantics><denotational-semantics>', 'CreationDate': '2013-01-17T22:21:50.770', 'Id': '9005'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am familiar with asymptotic notations like Big-O ,little-o. But while I am reading some papers people are using the notations like  $O(\\epsilon^{1/2^d})$, $O(d)^d$ etc. I couldn't understand these notations properly. Is there any way (Lecture notes or video lectures with examples) to understand these things clearly.\nThank You.  </p>\n", 'ViewCount': '162', 'Title': 'asymptotic notations with two exponents', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-31T07:03:28.657', 'LastEditDate': '2013-01-30T21:33:53.430', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-01-30T06:37:15.033', 'Id': '9299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '240', 'Title': "Infinite chain of big $O's$", 'LastEditDate': '2014-01-31T14:32:53.120', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2499', 'FavoriteCount': '1', 'Body': "<p>First, let me write the definition of big $O$ just to make things explicit.</p>\n\n<p>$f(n)\\in O(g(n))\\iff \\exists c, n_0\\gt 0$ such that $0\\le f(n)\\le cg(n), \\forall n\\ge n_0$</p>\n\n<p>Let's say we have a finite number of functions: $f_1,f_2,\\dots f_n$ satisftying:</p>\n\n<p>$O(f_1)\\subseteq O(f_2)\\dots \\subseteq O(f_n)$</p>\n\n<p>By transitivity of $O$, we have that: $O(f_1)\\subseteq O(f_n)$</p>\n\n<p>Does this hold if we have an infinite chain of $O's$? In other words, is $O(f_1) \\subseteq O(f_\\infty)$?</p>\n\n<p>I'm having trouble imagining what's going on.</p>\n", 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '2499', 'LastActivityDate': '2014-01-31T14:32:53.120', 'CommentCount': '3', 'AcceptedAnswerId': '9507', 'CreationDate': '2013-02-05T15:24:05.437', 'Id': '9506'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If I have some function whose time complexity is O(<em>mn</em>), where <em>m</em> and <em>n</em> are the sizes of its two inputs, would we call its time complexity "linear" (since it\'s linear in both <em>m</em> and <em>n</em>) or "quadratic" (since it\'s a product of two sizes)? Or something else?</p>\n\n<p>I feel calling it "linear" is confusing because O(m + n) is also linear but much faster, but I feel like calling it "quadratic" is also weird because it\'s linear in each variable separately.</p>\n', 'ViewCount': '533', 'Title': 'Is O(mn) considered "linear" or "quadratic" growth?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-07T20:34:03.160', 'LastEditDate': '2013-02-06T07:41:49.433', 'AnswerCount': '5', 'CommentCount': '7', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-02-05T22:54:48.270', 'FavoriteCount': '1', 'Id': '9523'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question if I have my $k=300$\nand my loop is like this :</p>\n\n<pre><code>for( int x = 0 ; x&lt;n ; x--){\n    for(int y=0 ; y&lt;k; y++){\n        ...\n    }\n}\n</code></pre>\n\n<p>Is this still $O(n^2)$? If no, why?\nThank you :)</p>\n', 'ViewCount': '104', 'Title': 'Big O time complexity', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-02-09T09:43:54.210', 'LastEditDate': '2013-02-09T09:43:54.210', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '9614', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6786', 'Tags': '<time-complexity><landau-notation>', 'CreationDate': '2013-02-09T03:53:59.947', 'Id': '9613'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to find out why $(\\log(n))^{99} = o(n^{\\frac{1}{99}})$. I tried to find the limit as this fraction goes to zero.</p>\n\n<p>$$\n\\lim_{n \\to \\infty} \\frac{ (\\log(n))^{99} }{n^{\\frac{1}{99}}}\n$$</p>\n\n<p>But I'm not sure how I can reduce this expression.</p>\n", 'ViewCount': '154', 'Title': 'Why is $(\\log(n))^{99} = o(n^{\\frac{1}{99}})$', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-19T00:20:34.973', 'LastEditDate': '2013-02-17T15:18:48.700', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '9854', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-02-17T03:52:56.133', 'Id': '9852'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Many a times if the complexities are having constants such as 3n, we neglect this constant and say O(n) and not O(3n). I am unable to understand how can we neglect such three fold change? Some thing is varying 3 times more rapidly than other! Why do we neglect this fact?  </p>\n', 'ViewCount': '248', 'Title': 'Justification for neglecting constants in Big O', 'LastEditorUserId': '157', 'LastActivityDate': '2013-02-21T05:14:05.497', 'LastEditDate': '2013-02-21T05:14:05.497', 'AnswerCount': '5', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<complexity-theory><asymptotics><landau-notation>', 'CreationDate': '2013-02-20T07:12:59.983', 'FavoriteCount': '2', 'Id': '9957'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $\\Theta$ and $o$ be defined as usual (Landau-notation).\nFor two equivalence classes defined by $\\Theta$ we define\n$$\\Theta(f) &lt;_o \\Theta(g) :\\Leftrightarrow f \\in o(g)\\qquad.$$\nLet $$\\mathbb{F}:= \\{\\Theta(f)\\mid f:\\mathbb{N}\\rightarrow\\mathbb{N}\\}$$</p>\n\n<p><strong>My question</strong>:\nIs $\\langle \\mathbb{F},&lt;_o\\rangle$ Dedekind-complete, i.e. given a set $F\\subseteq \\mathbb{F}$ with an upper bound $f_u,\\forall f \\in Ff\\in o(f_u)$ are there $f_\\inf$ and $f_\\sup$ s.t.\n$$\\forall \\Theta(f)\\in F: \\Theta(f_\\inf) &lt;_o \\Theta(f) \\qquad{(1)}$$\n$$\\forall g: (g\\text{ fulfills (1)} \\Rightarrow g \\in o(f_\\inf))$$</p>\n\n<p>($f_\\sup$ is defined analogously)?</p>\n\n<p>Note: We don't need to assume a lower bound, since there is no infinite strictly decreasing series of $n \\in \\mathbb{N}$, i.e. $\\Theta(0)$ is always a lower bound.</p>\n\n<p>Background: I'd like to consider $\\inf \\{f \\mid L \\in N/DTIME/SPACE(f)\\}$ for some Language $L$, but that only makes sense if such a (class of) function(s) exist(s). </p>\n", 'ViewCount': '77', 'Title': 'Is $\\{\\Theta(f)|f:\\mathbb{N}\\rightarrow\\mathbb{N}\\}$ Dedekind-complete?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-07T20:26:41.367', 'LastEditDate': '2013-03-07T20:26:41.367', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '10350', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '6716', 'Tags': '<landau-notation>', 'CreationDate': '2013-03-06T23:42:25.453', 'Id': '10341'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>One of my lectures makes the following statement:</p>\n\n<p>$$( f(n)=O(n) \\land f(n)\\neq o(n) )\\implies f(n)=\\Theta(n)$$</p>\n\n<p>Maybe I'm missing something in the definitions, but for example bubble sort is $O(n^2)$ and not $o(n^2)$ but it's also not $\\theta(n^2)$ since it's best case run time $\\Omega(n)$.</p>\n\n<p>What am I missing here?</p>\n", 'ViewCount': '106', 'Title': '$( f(n)=O(n) \\land f(n) \\neq o(n) ) \\implies f(n)=\\Theta(n)$', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T17:22:18.050', 'LastEditDate': '2013-03-07T17:22:18.050', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10353', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-03-07T07:47:57.967', 'Id': '10352'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '743', 'Title': 'What is an Efficient Algorithm?', 'LastEditDate': '2013-03-12T14:10:14.620', 'AnswerCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6728', 'FavoriteCount': '1', 'Body': '<p>From the point of view of asymptotic behavior, what is considered an "efficient" algorithm?  What is the standard / reason for drawing the line at that point?  Personally, I would think that anything which is what I might naively call "sub-polynomial", such that $f(n) = o(n^2)$ such as $n^{1+\\epsilon}$ would be efficient and anything which is $\\Omega(n^2)$ would be "inefficient".  However, I\'ve heard anything which is of any polynomial order be called efficient.  What\'s the reasoning?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-12T17:39:15.060', 'CommentCount': '1', 'AcceptedAnswerId': '10477', 'CreationDate': '2013-03-12T10:11:19.533', 'Id': '10472'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've found in many exercises where I'm asked to show that $f(n)=\\Theta(g(n))$ where the two functions are of the same order of magnitude I have difficulty finding a constant $c$ and a value $n_0$ for the lower bound.  I'm using Corman's definition of $\\Theta$: </p>\n\n<p>$$\\exists c_1,c_2&gt;0\\in\\mathbb{R}:\\forall n\\geq n_0: 0 \\leq c_1 g(n)\\leq f(n)\\leq c_2 g(n)$$</p>\n\n<p>Showing the upper bound usually doesn't give me too much trouble, but for the lower bound I allot of times find myself using limits.  And even though I'm getting the right answers, I'm a bit worried that my method isn't very rigorous and that maybe I'm doing a bit of hand waving in the process.</p>\n\n<p>For example, problem 2.17 from Skiena's Algorithm Design Manual:</p>\n\n<p>Show that for any $a,b\\in \\mathbb{R}: b&gt;0$ that $(n+a)^b = \\Theta(n^b)$</p>\n\n<p>In this case I used limits to help find both constants.  </p>\n\n<p>For the upper limit I decided to look for some $c$ such that $(n+a)^b \\leq c^bn^b$.  So taking the $b$th root of each side and dividing by $n$ I have $\\frac{n+a}{n}\\leq c$ which gives me $1 + \\frac{a}{n} \\leq c$.  For any $a\\in\\mathbb{R}$, $\\lim_{n\\to\\infty }1+\\frac{a}{n}=1$.  If I pick $n_0&gt;|a|$, then for $a&lt;0$ the expression approaches 1 from the left starting arbitrarily close to $0$.  If $a&gt;0$ then the expression approaches 1 from the right starting arbitrarily close to 2. So choosing $c=2$ will satisfy the inequality and we have $c_2=2^b$.</p>\n\n<p>Now for the lower bound.  I'm looking at the same expression except with the inequality pointing the other way.  In this case I'm trying to find $n_0$ and $c$ such that $c\\leq 1+\\frac{a}{n}$.  The value of $n_0$ has to be greater than $|a|$ because otherwise we would have $c\\leq 0$ which isn't allowed.  This puts us in the same range of values between $0$ and $2$ approaching 1 from each side.  So I choose any $c,n_0$ such that $n_0&gt;|a|$ and $0 &lt; c\\leq 1-|\\frac{a}{n_0}|$.  So I could choose $n_0=3|a|$ and $c=\\frac{2}{3}$.  </p>\n\n<p>Thus we have $0 &lt; (\\frac{2}{3})^bn^b \\leq (n+a)^b \\leq 2^bn^b$ for any $n \\geq 3|a|$.</p>\n\n<p><strong>Is there an easier way to do this?</strong>  </p>\n\n<p>Normally when looking for upper limit constants where the two functions are of the same magnitude I simply eliminate negative lower order terms and change positive ones into multiples of the highest order term such as : </p>\n\n<p>$$3n^2+15n-5\\leq 3n^2+15n^2=18n^2$$</p>\n\n<p>But when looking for the constant for the lower bound I find myself typically resorting to looking at limits.  <strong>Is there any kind of short cut to finding the lower bound constant like there is for the upper bound constant?</strong></p>\n", 'ViewCount': '216', 'Title': 'Methods for Finding Asymptotic Lower Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-15T00:40:06.203', 'LastEditDate': '2013-03-14T14:32:45.263', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10512', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<asymptotics><landau-notation><lower-bounds>', 'CreationDate': '2013-03-13T18:57:03.363', 'Id': '10511'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When dealing with Landau notation, $\\Theta, O,\\Omega,o,\\omega$, why do some texts choose the Corman style definitions, i.e.:</p>\n\n<p>$$o(g(n))=\\{ f(n): \\forall c&gt;0:\\exists n_0&gt;0:\\; 0\\leq f(n) &lt; cg(n): \\; \\forall n\\geq n_0 \\}$$</p>\n\n<p>and some texts use limit based definitions such as:</p>\n\n<p>$$\\lim_{n\\to\\infty}\\frac{f(n)}{g(n)}=0\\Rightarrow f(n)\\in o(g(n))$$</p>\n\n<p>Is there any inherent advantage to one definition or the other?  Or is it more a matter of the author's personal preference? </p>\n", 'ViewCount': '114', 'Title': "Landau Notation, Definitions: Limits vs. Corman's", 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-15T19:53:19.617', 'LastEditDate': '2013-03-15T07:46:31.157', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '10542', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-03-15T07:19:17.640', 'Id': '10531'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '586', 'Title': 'Construct two functions $f$ and $g$ satisfying $f \\ne O(g), g \\ne O(f)$', 'LastEditDate': '2013-03-16T07:00:17.103', 'AnswerCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '7300', 'FavoriteCount': '1', 'Body': u'<p>Construct two functions $  f,g:   R^+ \u2192 R^+ $ satisfying:</p>\n\n<ol>\n<li>$f, g$ are continuous;</li>\n<li>$f, g$ are monotonically increasing;</li>\n<li>$f \\ne O(g)$ and $g \\ne O(f)$.</li>\n</ol>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '3011', 'LastActivityDate': '2014-03-05T13:09:32.820', 'CommentCount': '2', 'AcceptedAnswerId': '10549', 'CreationDate': '2013-03-16T06:54:52.737', 'Id': '10548'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is a Data structures &amp; Algorithms question. For instance I have the following grades of functions: $O(1), O(2^n), O(n \\log n), O(e^n), O(n^3), O(n^{1/3})$ and $O(\\log \\log n)$  </p>\n\n<p>I need to show and proof to which of these function grades does the function: $n^7$ belong to. \nI didn't get a chance to ask my professor on this topic so I'm not sure how to solve this problem.</p>\n", 'ViewCount': '54', 'ClosedDate': '2013-03-18T10:24:46.367', 'Title': 'Show that a function belongs to grade of incline', 'LastEditorUserId': '157', 'LastActivityDate': '2013-03-18T10:14:45.007', 'LastEditDate': '2013-03-17T21:26:38.540', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'user1125177', 'PostTypeId': '1', 'Tags': '<complexity-theory><asymptotics><landau-notation>', 'CreationDate': '2013-03-17T20:11:18.047', 'Id': '10588'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When dealing with the analysis of time and space complexity of algorithms, is it safe to assume that any function which has tight bounds ( i.e. $f(n)=\\Theta(g(n))$ is asymptotically positive and asymptotically monotonically increasing.  I mean that for all $n$ greater than or equal to some $n_0$ both those properties hold?  </p>\n', 'ViewCount': '313', 'Title': 'Asymptotic Properties of Functions in Complexity Analysis', 'LastActivityDate': '2013-03-20T21:07:13.017', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10666', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><asymptotics><landau-notation>', 'CreationDate': '2013-03-20T20:07:26.793', 'Id': '10664'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '180', 'Title': 'Time complexity based on two variables', 'LastEditDate': '2013-04-02T07:59:08.437', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'Naji', 'PostTypeId': '1', 'OwnerUserId': '7531', 'Body': "<p>Suppose we have a function based on two inputs of length $m,n$. Therefore the time complexity of the function is calculated by $T(m,n)$. Suppose that we have:</p>\n\n<ul>\n<li>$T(m,c)\\in O(m^2)$ for any constant $c$.</li>\n<li>$T(c',n)\\in O(n^2)$ for any constant $c'$.</li>\n</ul>\n\n<p>What can we say about $T(m,n)$?</p>\n", 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T07:59:08.437', 'CommentCount': '2', 'AcceptedAnswerId': '10956', 'CreationDate': '2013-04-01T03:44:10.490', 'Id': '10955'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '156', 'Title': 'What type of formal notation is being used here to represent functional algorithms?', 'LastEditDate': '2013-05-03T02:16:49.393', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7129', 'FavoriteCount': '1', 'Body': '<p>Interested in learning more about algorithm design in functional programming, I picked up Andrew Bird\'s <a href="http://rads.stackoverflow.com/amzn/click/0521513383" rel="nofollow">Pearls of Functional Algorithm Design</a>.  I have experience with a number of programming languages, but my only experience with functional programming is in Scala.  I understood that I would have to pick-up Standard ML and Haskell from the description of the book, but when I started reading the first section, I wasn\'t familiar with some of the operators being used.</p>\n\n<p>Here are some examples of function definitions from the first chapter of the book (free to preview on Amazon):</p>\n\n<hr>\n\n<p><img src="http://i.stack.imgur.com/mVx5z.png" alt="weird syntax"></p>\n\n<p>I have seen "^" and "v" used to represent "and" and "or," but some of the other syntax (like <code>False (0,n)</code>) still throws me off.</p>\n\n<p><img src="http://i.stack.imgur.com/aSG9Z.png" alt="more weird syntax"></p>\n\n<p>In this one, I\'m not sure what the <code>accumArray(+)...</code> is referring to.  I\'m thinking it\'s like a fold method using addition, but I don\'t understand the rest of the line.</p>\n\n<p><img src="http://i.stack.imgur.com/JPWEZ.png" alt="kinda weird"></p>\n\n<p>Here, the author has done a good job of describing that \\\\ is <a href="http://en.wikipedia.org/wiki/Set_difference#Relative_complement" rel="nofollow">set difference</a> and the two vertical lines crossed with a horizontal one is <a href="http://en.wikipedia.org/wiki/Union_%28set_theory%29" rel="nofollow">union</a>.  However, I\'ve never seen anything like that union symbol before.</p>\n\n<hr>\n\n<p>I don\'t want to know what each of these examples means as much as I want to know <strong>what library of formal representation is Bird using to represent these algorithms</strong>, and also, if a specific programming language (Haskell/SML?) syntax is being used as well in conjunction with these special symbols.</p>\n', 'Tags': '<algorithms><formal-languages><functional-programming><notation>', 'LastEditorUserId': '7129', 'LastActivityDate': '2013-05-03T02:16:49.393', 'CommentCount': '5', 'AcceptedAnswerId': '11710', 'CreationDate': '2013-05-01T17:48:19.410', 'Id': '11707'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Some authors define $\\Omega$  in a slightly different way: let\u2019s use\n$ \\overset{\\infty}{\\Omega}$\n(read \u201comega infinity\u201d) for this alternative definition. We say that $f(n) = \\overset{\\infty}{\\Omega}(g(n))$\nif there exists a positive constant $c$ such that $f(n) \\geq c\\cdot g(n) \\geq   0$ for infinitely many integers $n$, whereas the usual $\\Omega$ requires that this holds for all integers greater than a certain $n_0$. </p>\n\n<p>Show that for any two functions $f(n)$ and $g(n)$ that are asymptotically nonnegative,\neither $f(n) = O(g(n))$ or $f(n)= \\overset{\\infty}{\\Omega}(g(n))$ or both, whereas this is not true if we use $\\Omega$ in place of $\\overset{\\infty}{\\Omega}$. </p>\n\n<p>I am trying learn Algorithms. But I am unable to prove this. Can the experts help me ?</p>\n', 'ViewCount': '328', 'Title': 'Variations of Omega and Omega infinity', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-05-21T22:54:15.767', 'LastEditDate': '2013-05-21T21:34:08.090', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8263', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-05-20T03:58:41.007', 'Id': '12147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '171', 'Title': 'What does the R superscript notation mean in regular/formal languages?', 'LastEditDate': '2013-06-02T03:05:54.667', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8482', 'FavoriteCount': '1', 'Body': "<p>What does the capital R superscript notation mean in regular languages?  I am working on a homework assignment and don't recall my professor mentioning what the what the R superscript means.  For example in this syntax:</p>\n\n<p>$L = \\{ww^R\\mid w \\in \\Sigma^{\\ast} \\}$</p>\n", 'Tags': '<formal-languages><regular-languages><formal-grammars><notation>', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-06-02T03:05:54.667', 'CommentCount': '0', 'AcceptedAnswerId': '12422', 'CreationDate': '2013-06-02T02:11:28.330', 'Id': '12421'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A nice easy one.</p>\n\n<p>I'm interested to know the most standard and accepted way to denote the update of a mapping in the context of a theoretical computer science paper.</p>\n\n<p>By example, suppose we have a mapping $m$:</p>\n\n<p>$$\nm = \\{1 \\mapsto 2, 2 \\mapsto 3, 3 \\mapsto 4\\}\n$$</p>\n\n<p>Now suppose we wish to mutate the $m$ such that $~m(1) = 666$. How would this be best denoted?</p>\n\n<p>Off the top of my head, I have seen this denoted as $m \\leftarrow m \\circ \\{1 \\mapsto 666\\}$ or even $m \\leftarrow m[1 \\mapsto 666]$. How would you do it?</p>\n\n<p>Cheers</p>\n", 'ViewCount': '61', 'Title': 'Correct mapping "update" notation for computer science papers', 'LastActivityDate': '2013-06-14T13:18:40.580', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'OwnerDisplayName': 'vext01', 'PostTypeId': '1', 'OwnerUserId': '7764', 'Tags': '<notation>', 'CreationDate': '2013-06-14T12:14:02.900', 'Id': '12673'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Denotational semantics are abstract functions that give meaning to statements of a programming language. They accept an expression and return a value.</p>\n\n<p>But how do you define denotational semantics for languages that define, for example, functions (and later use these functions), or global variables, and change some global state of the program?</p>\n\n<p>I think the simplest way to do it would be to define the semantics function so that it returns a pair of values and a "global state" instead of just a value.</p>\n\n<p>But is there a place I can read about it some more? I am sure someone have done something like that. For example, is there denotational semantics for languages like Java or C?</p>\n\n<p>It would be nice to find a book that focuses on denotational semantics for programming languages which are not necessarily purely functional (though I think functional languages, like LISP, can also change a global state).</p>\n', 'ViewCount': '181', 'Title': 'how do you define functions with denotational semantics?', 'LastActivityDate': '2013-06-16T17:10:03.140', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8692', 'Tags': '<programming-languages><denotational-semantics>', 'CreationDate': '2013-06-16T15:10:41.253', 'Id': '12697'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Within the field of cache-oblivious algorithms the ideal cache model is used for determining the cache complexity of an algorithm.  One of the assumptions of the ideal cache model is that it models a "tall cache".  This is given by the statement $Z = \\Omega(L^2)$.  Where $Z$ is the size of the cache and $L$ is the size of the cache line.  What does $\\Omega$ represent?</p>\n', 'ViewCount': '69', 'Title': 'In the "tall cache assumption" what does $\\Omega$ represent?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-03T13:21:34.633', 'LastEditDate': '2013-07-03T13:21:34.633', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13053', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8986', 'Tags': '<asymptotics><landau-notation><computation-models><cpu-cache>', 'CreationDate': '2013-07-03T08:32:00.390', 'Id': '13052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am a newbie here I recently read tutorials on java about collection framework. <code>The time complexity of various objects in reading/writing data</code> What does it mean? what is <code>O</code> notations? can anyone make me understand these notations and about time complexity measures using these notations</p>\n', 'ViewCount': '25', 'ClosedDate': '2013-09-05T11:08:25.157', 'Title': 'What are these notations O(1)/O(n)/O(log n)?', 'LastEditorUserId': '9990', 'LastActivityDate': '2013-09-05T10:58:18.283', 'LastEditDate': '2013-09-05T10:58:18.283', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9990', 'Tags': '<performance><notation>', 'CreationDate': '2013-09-05T10:49:57.843', 'Id': '14144'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am new to Advanced Algorithms and I have studied various samples on Google and StackExchange. What I understand is:</p>\n\n<ol>\n<li><p>We use $O(\\log n)$ complexity when  there is division of any $n$ number on each recursion (especially in divide and conquer).</p></li>\n<li><p>I know that for binary search, we have time complexity $O(n \\log n)$, I understood $\\log n$ is because each time it halves the full $n$ size number list in a recursive manner until it finds the required element. But why is it multiplied with $n$ even we just traverse half of the $n$ size element for each execution so why we multiply $\\log n$ with $n$?</p></li>\n<li><p>Please give me any example explaining the complexity $O(n^2 \\log n)$. I hope this will help me in understanding much better the above two questions.</p></li>\n</ol>\n', 'ViewCount': '218', 'Title': 'Confusion regarding several time complexities including the logarithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:56:44.833', 'LastEditDate': '2013-09-16T08:56:44.833', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10129', 'Tags': '<algorithms><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2013-09-13T23:31:05.913', 'Id': '14299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '120', 'Title': 'Is $\\Theta$ symmetric?', 'LastEditDate': '2013-09-16T08:36:54.397', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'user65165', 'PostTypeId': '1', 'OwnerUserId': '7307', 'Body': '<p>For example if \n$$ f(x)= \\Theta (g(x)) $$</p>\n\n<p>from the definition of the theta notation, there exist c1 and c2 constants such that</p>\n\n<p>$$c_1 g(x) \\le f(x) \\le c_2 g(x)$$</p>\n\n<p>then if only we took the constants $1/c_1$ and $1/c_2$ we could say from the definition that </p>\n\n<p>$$ g(x)= \\Theta (f(x)) $$</p>\n\n<p>Right?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:36:54.397', 'CommentCount': '1', 'AcceptedAnswerId': '14341', 'CreationDate': '2013-09-15T18:58:05.713', 'Id': '14339'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>If the running time of an algorithm scales linearly with the size of its input, we say it has $O(N)$ complexity, where we understand <code>N</code> to represent input size.</p>\n\n<p>If the running time does not vary with input size, we say it\'s $O(1)$, which is essentially saying it varies proportionally to 1; i.e., doesn\'t vary at all (because 1 is constant).</p>\n\n<p>Of course, 1 is not the only constant. <em>Any</em> number could have been used there, right? (Incidentally, I think this is related to the common mistake many CS students make, thinking "$O(2N)$" is any different from $O(N)$.)</p>\n\n<p>It seems to me that 1 was a sensible choice. Still, I\'m curious if there is more to the etymology there\u2014why not $O(0)$, for example, or $O(C)$ where $C$ stands for "constant"? Is there a story there, or was it just an arbitrary choice that has never really been questioned?</p>\n', 'ViewCount': '225', 'Title': 'Why is it O(1) (and not, say, O(2))?', 'LastEditorUserId': '9574', 'LastActivityDate': '2013-09-19T17:17:20.410', 'LastEditDate': '2013-09-18T20:36:25.443', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9574', 'Tags': '<terminology><time-complexity><asymptotics><landau-notation><history>', 'CreationDate': '2013-09-18T16:39:15.317', 'FavoriteCount': '2', 'Id': '14416'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading a paper and it uses the expression "polynomial delay" which I don\'t understand. It is used in conjonction with the big O notation, which I\'m familiar with. </p>\n\n<p>Here is a exemple sentence showing how it is used:</p>\n\n<pre><code>`The loop is executed in time O(n^2(e+n)), which makes a polunomial delay for the output of the data.`\n</code></pre>\n\n<p>I\'ve try searching a bit but I can only seem to find other paper and no definition.</p>\n\n<p>Thank you</p>\n', 'ViewCount': '84', 'Title': 'Polynomial delay', 'LastActivityDate': '2013-09-26T03:04:06.883', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14617', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10340', 'Tags': '<terminology><landau-notation>', 'CreationDate': '2013-09-26T02:11:00.670', 'Id': '14614'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is a simple transition system of beverage vending machine:\n<img src="http://i.stack.imgur.com/bgJM8.png" alt="beverage vending machine"> </p>\n\n<p>The exemplary execution fragments can look like this:</p>\n\n<p><img src="http://i.stack.imgur.com/F0kPo.png" alt="enter image description here"></p>\n\n<p>Now, imagine we have multi-process TS where processes are identical and communicate over channels. I don\'t understand the semantics of how to present an execution fragment that runs across multiple processes. Should I specify the whole execution in one line? If yes how do I denote that the state belongs to some certain process? Or maybe I present execution fragment on each process in the separate line? I\'d love to see a "meta-example" of how that could look like. </p>\n\n<p>I am referring to this specific <a href="http://www-i2.informatik.rwth-aachen.de/i2/fileadmin/user_upload/documents/MC11/sheet02.pdf" rel="nofollow">exercise sheet</a>, task 3.) b). The figures are from the book "Principles of Model Checking" by Christel Baier and Joost-Pieter Katoen p. 22 and p. 25.  </p>\n', 'ViewCount': '65', 'Title': 'Example of execution fragment of multi-process transition system', 'LastActivityDate': '2013-10-08T17:15:44.600', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10582', 'Tags': '<formal-languages><automata><model-checking><denotational-semantics>', 'CreationDate': '2013-10-08T17:15:44.600', 'Id': '14915'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm just wondering what the correct notation is when referring to an average case complexity of an algorithm that was calculated by doing empirical analysis.</p>\n\n<p>For example, I have tested my algorithm and fitted the results to the curve $f(n)=2.65\\times 10^{-15}\\cdot(2.17^{n})$ and in my report right now I'm saying something like: </p>\n\n<blockquote>\n  <p><em>the average case complexity was found to be $\\approx 2.65\\times 10^{-15}\\cdot(2.17^{n})$</em>, </p>\n</blockquote>\n\n<p>but I would rather say something like </p>\n\n<blockquote>\n  <p><em>the average case complexity is $\\in \\Theta(2.17^{n})$</em>. </p>\n</blockquote>\n\n<p>But I'm not sure if this is technically correct because the result hasn't been theoretically proven, only empirically tested and fitted to the curve.</p>\n", 'ViewCount': '69', 'Title': 'Notation for average case complexity of an algorithm', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-09T17:53:03.743', 'LastEditDate': '2013-10-09T17:26:34.530', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14961', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2562', 'Tags': '<asymptotics><notation>', 'CreationDate': '2013-10-09T17:12:05.483', 'Id': '14960'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $O(n)$ be "Big-O" of $n$ and $o(n)$ be "Small-O" of $n$.</p>\n\n<p>It is a well-known fact that $O(n \\log{n}) \\subset O(n^{1 + \\epsilon})$ for any $\\epsilon &gt; 0$. Can we omit the $\\epsilon$, and just type $O(n \\log{n}) \\subset O(n^{1 + o(1)})$?</p>\n', 'ViewCount': '106', 'Title': 'Is $\\log{n}$ bounded from above by $n^{o(1)}$?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-19T16:34:55.637', 'LastEditDate': '2013-10-18T21:20:54.777', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '16211', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8887', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-10-18T20:53:30.907', 'Id': '16210'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What notation is used to discuss the coefficients of functions in big-O notation?</p>\n\n<p>I have two functions:</p>\n\n<ul>\n<li>$f(x) = 7x^2 + 4x +2$</li>\n<li>$g(x) = 3x^2 + 5x +4$</li>\n</ul>\n\n<p>Obviously, both functions are $O(x^2)$, indeed $\\Theta(x^2)$, but that doesn't allow a comparison further than that.  How do I discuss the the coefficients 7 and 3. Reducing the coefficient to 3 doesn't change the asymptotic complexity but it still makes a significant difference to runtime/memory usage.<br/></p>\n\n<p>Is it <b>wrong</b> to say that $f$ is $O(7x^2)$ and $g$ is $O(3x^2)$ ?\nIs there other notation that does take coefficients into consideration? Or what would be the best way to discuss this?</p>\n", 'ViewCount': '111', 'Title': 'How to discuss coefficients in big-O notation', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-28T07:34:55.270', 'LastEditDate': '2013-10-28T07:28:08.003', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '16347', 'Score': '9', 'OwnerDisplayName': 'El Bee', 'PostTypeId': '1', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-10-07T23:11:30.400', 'Id': '16346'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>My Problem is:</strong> to define a "repeat until"-construct in terms of Denotational semantics. I made an attempt and now i need to know if i made it right.</p>\n\n<p><strong>The Conditions are</strong>: i used the language "While" as specified in "Semantics with Applications" by Nielson &amp; Nielson (1992) (<a href="http://www.daimi.au.dk/~bra8130/Wiley_book/wiley.pdf" rel="nofollow">pdf</a>) (<a href="http://www.worldcat.org/oclc/24671378" rel="nofollow">WorldCat</a>). I do not want to use the help of the While-construct. </p>\n\n<p>In denotational semantics, we are interested in the effect of a program, rather than in "how" it is executed. Thats why semantic functions are defined compositionally. The corresponding definitions for the Denotational semantics (or "direct style semantics") can be found on page 86 in the Book from Nielson &amp; Nielson (they made it avaible over the Internet).</p>\n\n<p><strong>My Approach is</strong>:\n$$\\mathcal{S}_{\\text{ds}} \\lbrack\\lbrack\\text{repeat } S \\text{ until } b\\rbrack\\rbrack = \\text{FIX }F\\\\ \\text{where }F\\ g = \\mathcal{S}_{\\text{ds}}\\lbrack\\lbrack S\\rbrack\\rbrack\\circ\\text{cond}(\\mathcal{B}\\lbrack\\lbrack b\\rbrack\\rbrack , \\mathcal{S}_{\\text{ds}}\\lbrack\\lbrack S\\rbrack\\rbrack\\circ g, id)$$</p>\n\n<p>As you might see, my approach is quite similar to the definition of while, but i cannot see a mistake in it.</p>\n\n<p><strong>post scriptum</strong>: Bounty given, and second edit: yes, i meant $$\\mathcal{S}_{\\text{ds}} \\lbrack\\lbrack\\text{repeat } S \\text{ until } b\\rbrack\\rbrack$$ .. typo corrected. </p>\n', 'ViewCount': '190', 'Title': 'Defining a "repeat until"-construct in Denotational semantics', 'LastEditorUserId': '6828', 'LastActivityDate': '2013-11-22T20:36:29.933', 'LastEditDate': '2013-11-18T19:21:20.790', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18119', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6828', 'Tags': '<semantics><denotational-semantics>', 'CreationDate': '2013-11-15T19:03:40.283', 'Id': '18054'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a question asking about a language L with the property: there is a TM that decides L in time O(n^2013 / (log(n))^2012), and if there is a TM that decides L in time O(n^2012.9).</p>\n\n<p>My confusion comes from the first big O given, from what I understand the numerator would dominate as the TM grows towards infinity, so it would end up being O(n^2013), which would grow faster then O(n^2012.9), so there could not be a TM that decides L in time O)n^2012.9). But I'm not sure how to go about proving this. Is there some theorem that let's you compare O(n^2/log(n)) with O(n) or something to that extent?</p>\n", 'ViewCount': '42', 'ClosedDate': '2013-12-10T09:00:33.683', 'Title': 'Big O confusion', 'LastActivityDate': '2013-12-10T03:30:33.863', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11300', 'Tags': '<complexity-theory><time-complexity><landau-notation>', 'CreationDate': '2013-12-10T01:44:42.587', 'FavoriteCount': '1', 'Id': '18808'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So In a review section, our professor asked:</p>\n\n<p>Given integers $N$ and $M$</p>\n\n<p>Is $O(N+M)$ exponential or polynomial.</p>\n\n<p>It's exponential, but I just don't see how that is. I would have thought it's linear.</p>\n", 'ViewCount': '105', 'Title': 'Is $O(N+M)$ exponential or polynomial?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-12T11:18:31.617', 'LastEditDate': '2013-12-12T11:18:31.617', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '18914', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12047', 'Tags': '<landau-notation>', 'CreationDate': '2013-12-12T03:48:01.187', 'Id': '18910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'd like to have this feature in my application programming language (which these days, is Scala), but when I went to learn more about it on the internets, I realized I don't know the name of it.  I'm talking about the ability to do this (in Scala-ish pseudocode):</p>\n\n<pre><code>// Define some types that correspond to table rows in relational DB\nclass User \n   val id: Int\n   val name: String\n   val email: String\n   val created: DateTime\n\nclass Comment\n   val id: Int\n   val text: String\n   val userId: Int\n   val created: DateTime\n\n// Call a select/join query function\nval list = select c.id, c.text, u.name\n           from comments c join user u on c.userId = u.id ...\n</code></pre>\n\n<p>And then <code>list</code> gets a type of something like <code>List[R]</code> where <code>R</code> is an unnamed record type with properties <code>id</code>, <code>text</code>, <code>name</code>, or maybe <code>c.id</code>,<code>c.text</code>,<code>u.name</code>.  </p>\n\n<p>Some languages, like Scala, support you writing that select function so it returns a tuple type <code>(Int, String, String)</code>, but not a type with <em>named</em> fields.  </p>\n\n<p>Is there a name for that?</p>\n", 'ViewCount': '46', 'Title': "In type systems, is there a name for SQL's way of cutting and combining record types into new types?", 'LastEditorUserId': '268', 'LastActivityDate': '2013-12-20T18:10:56.257', 'LastEditDate': '2013-12-19T23:13:15.347', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '19162', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '12246', 'Tags': '<type-theory><type-checking><notation>', 'CreationDate': '2013-12-19T23:07:48.913', 'Id': '19134'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been working and experimenting with an algorithm that <em>may</em> take time $O^*(2^\\sqrt{n})$.  Here $O^*(f(n))$ simply neglects all polynomial terms.  I\'ve seen <a href="http://www.scottaaronson.com/blog/?p=394#comment-13888" rel="nofollow">a comment on Scott Aaronson\'s blog</a> that mentions graph isomorphism algorithms run in this time, too.  Actually, $O(2^\\sqrt{n})$.  Is there a designation for this time?</p>\n\n<p>I thought perhaps quasi-exponential may describe it, but I\'m wondering if there is already a name/designation for it.</p>\n', 'ViewCount': '66', 'Title': 'Is there a designation for this not-quite-exponential time?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-07T14:07:38.353', 'LastEditDate': '2014-02-06T21:13:30.697', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21404', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1667', 'Tags': '<terminology><landau-notation>', 'CreationDate': '2014-02-06T19:14:11.860', 'Id': '21397'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '194', 'Title': u'What is the result of multiplying O(n) and \u03a9(n)?', 'LastEditDate': '2014-02-12T20:07:43.373', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11837', 'FavoriteCount': '2', 'Body': '<p>If $f(x) = \\Omega(n)$ and  $g(x)= O(n)$, what would be the order of growth of $f(x) \\cdot g(x)$ ?</p>\n\n<p>First I figured it should $\\Theta(n)$ , as two extremes would cancel each other and the order of growth will be same as $n$</p>\n\n<p>But, where I came across this question, the answer given was $\\Omega(n)$, and no proof was mentioned. Well, I didn\'t understand why, but intuitively I convinced myself as "you can\'t know for sure the upper limit of growth for $f(x) \\cdot g(x)$ so you can\'t say it\'s $O(n)$, but you can be sure that it won\'t be lower than $\\Omega(n)$"</p>\n\n<p>Can someone help me in understanding this, in a more believable way?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-13T10:06:23.037', 'CommentCount': '1', 'AcceptedAnswerId': '21576', 'CreationDate': '2014-02-12T19:20:56.130', 'Id': '21575'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that:</p>\n\n<p>If $f(n) = O(g(n))$ , then there are constants $M$ and $x_0$ , such that </p>\n\n<p>$f(n) &lt;= M*g(n), \\forall n &gt; n_0$</p>\n\n<p>The other, plain English way of defining it is,</p>\n\n<p>If $f(n)=O(g(n))$ then for large $n$ , $f(n)$ would <em>grow</em> as fast as $g(n)$.</p>\n\n<p>I got confused when comparing $2^n$ with $2^{2n}$. Here , $f(n) = 2^n$ and $g(n) = 2^{2n}$. Clearly , $f(n)$ is smaller than $g(n)$ by a factor of $2^n$. So there will be constants $A$ and $x_0$ such that the first definition above is met.</p>\n\n<p>However, for large $n$ , $2^{2n}$ would grow much faster than $2^n$, leaving $2^n$ far behind. That is $2^{2n}$  won't be an asymptotic/tight bound for $2^n$ .</p>\n\n<p>So, is $2^n = O(2^{2n})$ or not? (or did I just create a confusing situation out of nothing)</p>\n", 'ViewCount': '69', 'Title': 'Big O relation between $2^n$ and $2^{2n}$', 'LastActivityDate': '2014-02-16T03:48:30.417', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '21688', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11837', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics><landau-notation>', 'CreationDate': '2014-02-15T17:49:54.007', 'Id': '21675'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '135', 'Title': 'How to deal with questions having two or more asymptotic notations', 'LastEditDate': '2014-02-17T09:34:55.493', 'AnswerCount': '2', 'Score': '2', 'OwnerDisplayName': 'geek90', 'PostTypeId': '1', 'OwnerUserId': '14749', 'Body': "<p>The following was asked as part of a homework assignment and I am not asking for the solution to these but rather tips or resources on how to solve this and similar questions, </p>\n\n<p>Let $f(n)$ and $g(n)$ be two functions from $\\mathbb{N}^+$ to $\\mathbb{R}^+$. Prove or disprove the following assertions. To disprove, you only need to give a counter example for functions $f(n)$ and/or $g(n)$ which make the assertion false. Consider the following.</p>\n\n<p>$$\\Omega(\\Theta(f(n))) = \\Omega(f(n))$$</p>\n\n<p>According to me the answer is false in this case, my reasoning is that whenever we say $f(n)=\\Theta(g(n))$ we are saying that $f(n)$ will always lie between the limits of $g(n)$.\nHere, let's say $x(n) = \\Theta(f(n))$ so it will always lie between the function $f(n)$ then we take $\\Omega(x(n))$ which would mean that $y(n)$ will always be greater than $x(n)$, and on the right hand side we compare it to some $a(n)$ which is equal $\\Omega(f(n))$ meaning that $a(n)$ will always be greater than $f(n)$. Hence we cannot say correctly if the relation will hold, it may or may not be true.</p>\n\n<p>Is this the correct way of looking at the problem, is there a better way to solve these questions?</p>\n", 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-22T16:46:45.470', 'CommentCount': '1', 'AcceptedAnswerId': '21725', 'CreationDate': '2014-02-17T06:15:22.287', 'Id': '21724'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What does superscript + mean in relational algebra?  Suppose $A$ is a subset of attributes of all attributes in a relation $R$.  What does $A^+$ mean?</p>\n', 'ViewCount': '21', 'Title': 'What does superscript + mean in relational algebra?', 'LastActivityDate': '2014-03-12T01:18:11.810', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15575', 'Tags': '<relational-algebra><notation>', 'CreationDate': '2014-03-12T00:48:36.790', 'Id': '22520'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '179', 'Title': 'Why does merge sort run in $O(n^2)$ time?', 'LastEditDate': '2014-03-16T15:08:35.860', 'AnswerCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12448', 'FavoriteCount': '1', 'Body': '<p>I have been learning about Big O, Big Omega, and Big Theta. I have been reading many SO questions and answers to get a better understanding of the notations. From my understanding, it seems that <strong>Big O is the upper bound</strong> running time/space of the algorithm, <strong>Big Omega is the lower bound</strong> running time/space of the algorithm and <strong>Big Theta is like the in between of the two</strong>.</p>\n\n<p>This particular <a href="http://stackoverflow.com/questions/10376740/big-theta-notation-what-exactly-does-big-theta-represent?lq=1">answer</a> on SO stumbled me with the following statement </p>\n\n<blockquote>\n  <p>For example, merge sort worst case is both ${\\cal O}(n\\log n$) and $\\Omega(n\\log n)$ -\n  and thus is also $\\Theta(n\\log n)$, but it is also ${\\cal O}(n^2)$, since $n^2$ is\n  asymptotically "bigger" than it. However, it is NOT $\\Theta(n^2)$, Since\n  the algorithm is not $\\Omega(n^2)$</p>\n</blockquote>\n\n<p>I thought merge sort is ${\\cal O}(n\\log n)$ but it seems it is also ${\\cal O}(n^2)$ because $n^2$ is asymptotically bigger than it. Can someone explain this to me?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-16T15:08:35.860', 'CommentCount': '5', 'AcceptedAnswerId': '22674', 'CreationDate': '2014-03-16T00:41:55.560', 'Id': '22662'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '55', 'Title': 'O(f) vs O(f(n))', 'LastEditDate': '2014-03-21T20:54:17.343', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10901', 'FavoriteCount': '0', 'Body': '<p>I first learned about the Big O notation in an intro to Algorithms class. He showed us that function $g \\in O(f(n))$ <br>\nAfterwords in Discrete Math another Professor, without knowing of the first, told us that we should never do that and that it should be done as $g \\in O(f)$ where g and f are functions. \nThe question is which one of these is right, why, and if they both are, what is the difference?  </p>\n', 'Tags': '<terminology><asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-21T21:04:42.580', 'CommentCount': '3', 'AcceptedAnswerId': '22914', 'CreationDate': '2014-03-21T17:58:00.083', 'Id': '22908'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have an algorithm that has a performance of $O(n + 2)$. Here if n gets really large the 2 becomes insignificant. In this case it's perfectly clear the real performance is $O(n)$.</p>\n\n<p>However, say another algorithm has a performance of $O(n^2/2)$. Here if n gets really large then $n^2/2$ is exactly half of $n^2$, which is not significantly smaller than $n^2$. So why we drop 1/2 from $O(n^2/2)$ and it becomes $O(n^2)$?</p>\n", 'ViewCount': '392', 'Title': 'Why is constant always dropped from big O analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T02:45:53.917', 'LastEditDate': '2014-04-12T15:22:48.923', 'AnswerCount': '3', 'CommentCount': '5', 'AcceptedAnswerId': '23704', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16713', 'Tags': '<terminology><algorithm-analysis><asymptotics><landau-notation>', 'CreationDate': '2014-04-12T14:54:50.047', 'Id': '23703'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the following simple algorithm to find duplicate characters in a string:</p>\n\n<pre><code>for i = 1 -&gt; n\n    for j = i + 1 -&gt; n\n        if A[i] == A[j] return true\nreturn false \n</code></pre>\n\n<p>Why is the running time of this algorithm $\\mathcal{O}(n^2)$?\nIf the first iteration is $n$ steps then, $n-1, n-2,n-3,..,1$ it seems to me that \nadding all these would never be $n^2$ or am I wrong?</p>\n', 'ViewCount': '48', 'Title': 'Confusion with the Running Time of an algorithm that finds duplicate character', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T07:47:17.597', 'LastEditDate': '2014-04-14T07:47:17.597', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '23749', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16747', 'Tags': '<terminology><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2014-04-13T19:40:14.433', 'Id': '23748'}},