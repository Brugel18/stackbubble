{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We\'re in a shared memory concurrency model where all reads and writes to integer variables are atomic.  </p>\n\n<ul>\n<li><code>do:</code> $S_1$ <code>in parallel with:</code> $S_2$&#160;\xa0 means to execute $S_1$ and $S_2$ in separate threads, concurrently.</li>\n<li><code>atomically(</code>$E$<code>)</code> \xa0 means to evaluate $E$ atomically, i.e. all other threads are stopped during the execution of $E$.</li>\n</ul>\n\n<p>Consider the following program:</p>\n\n<pre><code>x = 0; y = 4\ndo:                 # thread T1\n    while x != y:\n        x = x + 1; y = y - 1\nin parallel with:   # thread T2\n    while not atomically (x == y): pass\n    x = 0; y = 2\n</code></pre>\n\n<p>Does the program always terminate? When it does terminate, what are the possible values for <code>x</code> and <code>y</code>?</p>\n\n<p><sub> Acknowledgement: this is a light rephrasing of exercise 2.19 in <a href="http://www.cs.arizona.edu/~greg/mpdbook/" rel="nofollow"><em>Foundations of Multithreaded, Parallel, and Distributed Programming</em></a> by Gregory R. Andrews. </sub>  </p>\n', 'ViewCount': '150', 'Title': 'termination of two concurrent threads with shared variables', 'LastEditorUserId': '39', 'LastActivityDate': '2012-03-26T16:55:14.447', 'LastEditDate': '2012-03-26T16:55:14.447', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '448', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<concurrency><shared-memory><imperative-programming>', 'CreationDate': '2012-03-16T23:58:16.950', 'Id': '443'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In a recent CACM article [1], the authors present a way to improve scalability of shared  and coherent caches. The core ingredient is assuming the caches are <em>inclusive</em>, that is higher-level caches (e.g. L3, one global cache) contain all blocks which are stored in their descendant lower-level caches (e.g. L1, one cache per core).</p>\n\n<p>Typically, higher-level caches are larger than their respective descendant caches together. For instance, some models of the Intel Core i7 series with four cores have an 8MB shared cache (L3) and 256KB private caches (L2), that is the shared cache can hold eight times as many blocks as the private caches in total.</p>\n\n<p>This seems to suggest that whenever the shared cache has to evict a block (in order to load a new block) it can find a block that is shared with none of the private caches\xb2 (pigeon-hole principle). However, the authors write:</p>\n\n<blockquote>\n  <p>[We] can potentially eliminate all recalls, but only if the associativity, or number of places in which a specific block may be cached, of the shared cache exceeds the aggregate associativity of the private caches. With sufficient associativity, [the shared cache] is guaranteed to find a nonshared block [...]. Without this worst-case associativity, a pathological cluster of misses could lead to a situation in which all blocks in a set of the shared cache are truly shared.</p>\n</blockquote>\n\n<p>How is this possible, that is how can, say, 1MB cover 8MB? Clearly I miss some detail of how such cache hierarchies work. What does "associativity" mean here? "number of places in which a specific block may be cached" is not clear; I can only come up with the interpretation that a block can be stored multiple times in each cache, but that would make no sense at all. What would such a "pathological cluster of misses" look like?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1145/2209249.2209269">Why On-Chip Cache Coherence is Here to Stay</a> by M. M. K. Martin, M. D. Hill, D. J. Sorin  (2012)</li>\n<li>Assuming the shared caches knows which blocks are shared where. This can be achieved by explicit eviction notifications and tracking bit, which is also discussed in [1].</li>\n</ol>\n', 'ViewCount': '207', 'Title': 'Why can L3 caches hold only shared blocks?', 'LastActivityDate': '2012-08-02T14:29:37.697', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3004', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<terminology><computer-architecture><cpu-cache><shared-memory>', 'CreationDate': '2012-08-02T11:38:21.877', 'Id': '3001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to implement a bitonic counting network as a shared memory structure. \nI read through <a href="http://www.cs.yale.edu/homes/aspnes/papers/ahs.pdf" rel="nofollow">this paper</a> and understand how it works in principle and that we have the step property. Thus, we have n output variables/shared memory variables that count the arriving tokens. Whenever a token arrives, this value is incremented (by 1 or by n?). </p>\n\n<p>How do I get the current count of this network, without issuing another token? Getting the maximum of n shared variables (the outputs) doesn\'t seem like a good solution (not lock and wait free!)</p>\n', 'ViewCount': '52', 'Title': '(Bitonic) counting network: how to get the current count?', 'LastActivityDate': '2013-05-13T09:28:00.990', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6658', 'Tags': '<shared-memory><counting>', 'CreationDate': '2013-05-13T09:28:00.990', 'Id': '11986'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Could you please check (and probably clarify) my understanding of the difference between these two concepts: Sequential Consistency and Cache Coherence?</p>\n\n<p>According to this paper[1] sequential consistency can be described as (not formally): </p>\n\n<blockquote>\n  <p><em>Sequential consistency</em> memory model specifies that the system must appear to execute all threads\u2019 loads and stores to all memory locations in a total order that respects the program order of each thread. Each load gets the value of the most recent store in that total order. </p>\n</blockquote>\n\n<p>In other words, system is sequentially consistent, if given memory events (loads and stores) of each thread we can order all these events such that: 1) for each thread the order of its events is preserved, and 2) the global order is serial (any load returns latest value stored).</p>\n\n<p>Now they [1] continue and describe coherence:</p>\n\n<blockquote>\n  <p>A definition of <em>coherence</em> that is analogous to the definition of Sequential Consistency is that a coherent system must appear to execute all threads\u2019 loads and stores to a single memory location in a total order that respects the program order of each thread. </p>\n</blockquote>\n\n<p>In other words, system is coherent, if given memory events of each thread <em>for each location</em> we can order events for that location, such that: 1) for each thread the order of its events <em>to that location</em> is preserved, and 2) <em>for each location</em> the order is serial.</p>\n\n<p>Finally, they point out the difference: </p>\n\n<blockquote>\n  <p>This definition highlights an important <em>distinction between coherence and consistency</em>: coherence is specified on a per-memory location basis, whereas consistency is specified with respect to all memory locations.</p>\n</blockquote>\n\n<p>Could you clarify please if the below is correct?:</p>\n\n<ol>\n<li><p>The difference is that for coherent systems we need a total order on all events for each location (thus the ordering between events for particular location), while for consistent systems the total order should be defined on the all events (and thus the ordering is also between events for different locations)?</p></li>\n<li><p>Does it mean that coherence is less strict that consistency? (which seems amusing!) I.e., there are traces that are coherent but not consistent, for example:</p>\n\n<pre><code>initially A=B=0\nprocess 1               process 2\nstore A := 1            load B (gets 1)\nstore B := 1            load A (gets 0) \n</code></pre>\n\n<p>This trace is coherent: </p>\n\n<ul>\n<li>for A the order is: <code>proc2 loads A(gets 0)</code>, <code>proc1 stores A:=1</code></li>\n<li>for B the order is: <code>proc1 stores B:=1</code>, <code>proc2 loads B(gets 1)</code></li>\n</ul>\n\n<p>But it is not consistent! Since if <code>proc2 load B</code> returns 1, then <code>proc1 store A := 1</code> already happened and <code>proc2 load A</code> should also return 1.</p></li>\n</ol>\n\n<p>[1]: A Primer on Memory Consistency and Cache Coherence <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00346ED1V01Y201104CAC016" rel="nofollow">http://www.morganclaypool.com/doi/abs/10.2200/S00346ED1V01Y201104CAC016</a></p>\n\n<p>Thanks!</p>\n', 'ViewCount': '55', 'Title': 'Memory Consistency vs Cache Coherence', 'LastActivityDate': '2014-01-28T19:02:54.547', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2260', 'Tags': '<cpu-cache><shared-memory>', 'CreationDate': '2014-01-28T19:02:54.547', 'Id': '20044'}},