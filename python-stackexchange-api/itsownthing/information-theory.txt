{'Body': u'<p>Specifying any arbitrary 9x9 grid requires giving the position and value of each square. A na\xefve encoding for this might give 81 (x, y, value) triplets, requiring 4 bits for each x, y, and value (1-9 = 9 values = 4 bits) for a total of 81x4x3 = 972 bits. By numbering each square, one can reduce the positional information to 7 bits, dropping a bit for each square and a total of 891 bits. By specifying a predetermined order, one can reduce this more drastically to just the 4 bits for each value for a total of 324 bits. However, a sudoku can have missing numbers. This provides the potential for reducing the number of numbers that have to be specified, but may require additional bits for indicating positions.  Using our 11-bit encoding of (position, value), we can specify a puzzle with $n$ clues with $11n$ bits, e.g. a minimal (17) puzzle requires 187 bits.  The best encoding I\'ve thought of so far is to use one bit for each space to indicate whether it\'s filled and, if so, the following 4 bits encode the number. This requires $81+4n$ bits, 149 for a minimal puzzle ($n=17$). Is there a more efficient encoding, preferably without a database of each valid sudoku setup? (Bonus points for addressing a general $n$ from $N \\times N$ puzzle)</p>\n\n<p>It just occurred to me that many puzzles will be a rotation of another, or have a simple permutation of digits.  Perhaps that could help reduce the bits required.  </p>\n\n<p>According to <a href="http://en.wikipedia.org/wiki/Sudoku#Mathematics_of_Sudoku">Wikipedia</a>, </p>\n\n<blockquote>\n  <p>The number of classic 9\xd79 Sudoku solution grids is 6,670,903,752,021,072,936,960 (sequence A107739 in OEIS), or approximately $6.67&#215;10^{21}$.</p>\n</blockquote>\n\n<p>If I did my math right ($\\frac{ln{(6,670,903,752,021,072,936,960)}}{ln{(2)}}$), that comes out to 73 (72.498) bits of information for a lookup table.</p>\n\n<p>But:</p>\n\n<blockquote>\n  <p>The number of essentially different solutions, when symmetries such as rotation, reflection, permutation and relabelling are taken into account, was shown to be just 5,472,730,538[15] (sequence A109741 in OEIS).</p>\n</blockquote>\n\n<p>That gives 33 (32.35) bits, so it\'s possible that a clever method of indicating which permutation to use could get below the full 73 bits.</p>\n', 'ViewCount': '312', 'Title': 'Efficient encoding of sudoku puzzles', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T16:36:39.563', 'LastEditDate': '2012-04-22T16:36:39.563', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '12', 'Tags': '<combinatorics><modelling><information-theory><sudoku>', 'CreationDate': '2012-03-09T17:02:17.730', 'FavoriteCount': '0', 'Id': '165'}{'ViewCount': '1940', 'Title': 'Why is encrypting with the same one-time-pad not good?', 'LastEditDate': '2012-03-15T07:22:59.783', 'AnswerCount': '4', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '157', 'FavoriteCount': '1', 'Body': '<p>To encrypt a message $m_1$ with a one-time-pad key $k$ you do\n$Enc(m_1,k) = m_1 \\oplus k$. </p>\n\n<p>If you use the same $k$ to encrypt a different message $m_2$ you get\n$Enc(m_2,k) = m_2 \\oplus k$, and if you perform Xor of the two ciphertext you get\n$$( m_1 \\oplus k) \\oplus ( m_2 \\oplus k) = m_1 \\oplus m_2$$</p>\n\n<p>so, OK, there is some information leakage becuse you learn $m_1 \\oplus m_2$, but why is it not secure? I have no way to learn (say) $m_1$ unless I know $m_2$. So why is it wrong to use $k$ twice??</p>\n', 'Tags': '<cryptography><information-theory><encryption>', 'LastEditorUserId': '157', 'LastActivityDate': '2012-03-25T07:11:47.277', 'CommentCount': '5', 'AcceptedAnswerId': '365', 'CreationDate': '2012-03-14T06:52:07.727', 'Id': '349'}{'Body': '<p>In coding theory, \'how good a code is\' means how many channel errors can be corrected, or better put, the maximal noise level that the code can deal with.</p>\n\n<p>In order to get better codes, the codes are designed using a large alphabet (rather than binary one). And then, the code is good if it can deal with a large rate of erroneous "symbols".</p>\n\n<p><strong>Why isn\'t this consider cheating?</strong> I mean, shouldn\'t we only care about what happens when we "translate" each symbol into a binary string?  The "rate of bit error" is different than the rate of "symbol error". For instance, the rate of bit-error cannot go above 1/2 while (if I understand this correctly), with large enough alphabet, the symbol-error can go up to $1-\\epsilon$. Is this because we <em>artificially</em> restrict the channel to change only "symbols" rather than bits, or is it because the code is actually better?</p>\n', 'ViewCount': '141', 'Title': 'Error-correcting rate is misleading', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-25T14:48:07.313', 'LastEditDate': '2012-03-25T14:48:07.313', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '157', 'Tags': '<information-theory><coding-theory>', 'CreationDate': '2012-03-24T03:54:28.110', 'FavoriteCount': '1', 'Id': '726'}{'ViewCount': '290', 'Title': 'Difference between "information" and "useful information" in algorithmic information theory', 'LastEditDate': '2012-04-22T15:52:04.393', 'AnswerCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '877', 'FavoriteCount': '3', 'Body': '<p>According to <a href="http://en.wikipedia.org/wiki/Algorithmic_information_theory#Overview">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>Informally, from the point of view of algorithmic information theory, the information content of a string is equivalent to the length of the shortest possible self-contained representation of that string.</p>\n</blockquote>\n\n<p>What is the analogous informal rigorous definition of "useful information"? Why is "useful information" not taken as the more natural or more fundamental concept; naively it seems a purely random string must by definition contain zero information, so I\'m trying to get my head around the fact that it is considered to have maximal information by the standard definition.</p>\n', 'Tags': '<information-theory><terminology><kolmogorov-complexity>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-28T19:54:34.903', 'CommentCount': '1', 'AcceptedAnswerId': '946', 'CreationDate': '2012-04-01T14:16:41.417', 'Id': '945'}{'ViewCount': '98', 'Title': 'Why are blocking artifacts serious when there is fast motion in MPEG?', 'LastEditDate': '2012-04-21T19:32:46.183', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Bear', 'PostTypeId': '1', 'OwnerUserId': '1197', 'Body': '<p>Why are blocking artifacts serious when there is fast motion in MPEG?</p>\n\n<p>Here is the guess I made:</p>\n\n<p>In MPEG, each block in an encoding frame is matched with a block in the reference frame.\nIf the difference of two blocks is small, only the difference is encoded using DCT. Is the reason blocking artifacts are serious that the difference of two blocks is too large and DCT cut the AC component?</p>\n', 'Tags': '<information-theory><data-compression><video>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-22T21:37:32.183', 'CommentCount': '2', 'AcceptedAnswerId': '1445', 'CreationDate': '2012-04-21T10:49:15.717', 'Id': '1413'}{'Body': '<p>I have been reading on security policies and the question wether <a href="https://en.wikipedia.org/wiki/Bell-LaPadula_model" rel="nofollow">Bell-LaPadula</a> can be used to implement <a href="https://en.wikipedia.org/wiki/Chinese_wall" rel="nofollow">Chinese Wall</a>. Does anyone know more about it?</p>\n', 'ViewCount': '354', 'Title': 'Can the Bell-LaPadula model emulate the Chinese Wall model?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-28T18:31:01.520', 'LastEditDate': '2012-04-23T06:33:54.997', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1045', 'Tags': '<information-theory><security><access-control>', 'CreationDate': '2012-04-23T02:34:45.640', 'Id': '1454'}{'Body': '<p>Let $a \\neq b$ be two integers from the interval $[1, 2^n].$ Let $p$ be a random prime with $ 1 \\le p \\le n^c.$ Prove that\n$$\\text{Pr}_{p \\in \\mathsf{Primes}}\\{a \\equiv  b \\pmod{p}\\} \\le c \\ln(n)/(n^{c-1}).$$</p>\n\n<p>Hint: As a consequence of the prime number theorem, exactly $n/ \\ln(n) \\pm o(n/\\ln(n))$ many numbers from $\\{ 1, \\ldots, n \\}$ are prime.</p>\n\n<p>Conclusion: we can compress $n$ bits to $O(\\log(n))$ bits and get a quite small false-positive rate.</p>\n\n<p>My question is how can i proove that $$\\text{Pr}_{p \\in \\mathsf{Primes}}\\{a \\equiv  b \\pmod{p}\\} \\le c \\ln(n)/(n^{c-1})$$?</p>\n', 'ViewCount': '118', 'Title': 'Prove fingerprinting', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-06T23:19:40.733', 'LastEditDate': '2012-05-06T22:16:55.400', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '1704', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<probability-theory><information-theory><coding-theory><number-theory>', 'CreationDate': '2012-05-06T18:34:00.527', 'Id': '1692'}{'Body': '<p>I have a problem with the application of the <a href="http://en.wikipedia.org/wiki/Shannon_expansion" rel="nofollow">Shannon expansion</a> for to obtain the negation of a formula boolean, than will need for implement the negation operator on OBDD (<a href="http://en.wikipedia.org/wiki/Binary_decision_diagram" rel="nofollow">Order Binary Decision Diagram</a>) that is, show that:</p>\n\n<p>$\\qquad \\displaystyle \\neg f(x_1,\\ldots,x_n) = (\\neg x_1 \\wedge \\neg f|_{x_1=0}) \\vee  (x_1 \\wedge \\neg f|_{x_1=1})$</p>\n\n<p>where $f|_{x_i=b}$ is the function boolean in which replaces $x_i$ with b, that is:</p>\n\n<p>$\\qquad \\displaystyle f|_{x_i=b}(x_1,\\ldots,x_n)=f(x_1,\\ldots,x_{i-1},b,x_{i+1},\\ldots,x_n)$.</p>\n\n<p>The proof says:</p>\n\n<p>$\\qquad \\displaystyle\\neg f(x_1,\\ldots,x_n) = \\neg((\\neg x_1 \\wedge f|_{x_1=0}) \\vee  (x_1 \\wedge f|_{x_1=1}))$. </p>\n\n<p>Applying the negation (skip the intermediate steps), we get:</p>\n\n<p>$\\qquad \\displaystyle (x_1 \\wedge \\neg x_1) \\vee (\\neg x_1 \\wedge \\neg f|_{x_1=0}) \\vee  (x_1 \\wedge \\neg f|_{x_1=1}) \\vee (\\neg f|_{x_1=0} \\wedge \\neg f|_{x_1=1}) $. </p>\n\n<p>Now $(x_1 \\wedge \\neg x_1)= \\mathrm{false}$ can be dropped, which leads to</p>\n\n<p>$\\qquad \\displaystyle (\\neg x_1 \\wedge \\neg f|_{x_1=0}) \\vee  (x_1 \\wedge \\neg f|_{x_1=1}) \\vee (\\neg f|_{x_1=0} \\wedge \\neg f|_{x_1=1}) $ </p>\n\n<p>which in turn is, finally, equal to  </p>\n\n<p>$\\qquad \\displaystyle (\\neg x_1 \\wedge \\neg f|_{x_1=0}) \\vee  (x_1 \\wedge \\neg f|_{x_1=1})$.</p>\n\n<p>Why does this hold?</p>\n', 'ViewCount': '153', 'Title': "Operations on OBDD: negation through Shannon's expansion", 'LastEditorUserId': '472', 'LastActivityDate': '2012-07-04T11:00:31.067', 'LastEditDate': '2012-07-04T11:00:31.067', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2597', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1999', 'Tags': '<logic><information-theory>', 'CreationDate': '2012-07-03T09:02:20.773', 'Id': '2595'}{'Body': '<p>There is a time series of say $100$ data points. I wish to assign symbols of $0, 1, 2$ for each unique data point. The issue is I have tried but got stuck since no matter I specify the symbols, the program just outputs probability of $1$\'s and $0$\'s. The following are the questions:</p>\n\n<ol>\n<li>How to find probability or correct my code so that it outputs probablities when number of symbols size > 2?</li>\n<li>How to calculate entropy annd mutual information for this case. I don\'t know although I have read Matlab\'s entropy calculation <a href="http://www.mathworks.com/matlabcentral/fileexchange/14888" rel="nofollow">Mutual Information &amp; Entropy</a> but alas cannot follow how to apply in this case.</li>\n</ol>\n', 'ViewCount': '186', 'Title': 'Time series probability and mutual information', 'LastEditorUserId': '472', 'LastActivityDate': '2012-07-27T09:16:56.967', 'LastEditDate': '2012-07-27T09:16:56.967', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2693', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '783', 'Tags': '<probability-theory><information-theory>', 'CreationDate': '2012-07-11T03:18:57.770', 'Id': '2688'}{'Body': '<p><a href="http://www.scholarpedia.org/article/Kolmogorov-Sinai_entropy" rel="nofollow">Kolmogorov-Sinai entropy</a> (KS) explains the mathematical concept behind KS entropy. </p>\n\n<p>$$h ( T ) =\\sup\\limits_{\\xi} \\, h ( T , \\xi )$$</p>\n\n<p>defines the formula for KS where the left-hand side is nothing but the Shannon\'s entropy. In many papers I have seen that KS is defined as the supremum over source entropy. These conflicting views raises the following questions</p>\n\n<ol>\n<li>What is the difference between Shannon\'s entropy and source entropy (source entropy given by $\\lim \\frac{1}{n}\\cdot h(T)$ where $n$ is the length of the word or sequence,unsure though). </li>\n<li><p>Would source entropy be the entropy of the raw signal before it has been quantized or should it be that of the quantized?</p></li>\n<li><p>Is \'n\' the length of the data series or the number of quantization levels></p></li>\n<li><a href="http://mathworld.wolfram.com/TopologicalEntropy.html" rel="nofollow">Topological entropy</a> (TS) explains topological entropy whose formula looks exactly as KS. So, what is the difference since KS is also known as metric entropy and not known as Topological entropy?</li>\n</ol>\n', 'ViewCount': '174', 'Title': 'Source entropy and other questions related to information theory', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T15:06:28.490', 'LastEditDate': '2012-10-23T02:38:01.893', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '783', 'Tags': '<reference-request><information-theory><entropy>', 'CreationDate': '2012-07-25T00:41:25.497', 'Id': '2902'}{'Body': u'<p>We\'re running some benchmarks for an approximative query-answering system. It\'s sufficient to just think of it as running some SQL queries with joins. We are counting the results returned as part of the benchmark. However, the results often contain a lot of redundancy, so just counting results seems coarse.</p>\n\n<p>Consider the following table containing results for a query like "<em>for the US, give me its states and its car manufacturers</em>":</p>\n\n<pre><code>================================\n|| ?us_state | ?us_car_manu   ||\n||============================||\n|| Alabama   | Chrysler       ||\n|| Alaska    | Chrysler       ||\n|| ...         ...            ||\n|| Wyoming   | Chrysler       ||\n|| Alabama   | General Motors ||\n|| Alaska    | General Motors ||\n|| ...         ...            ||\n|| Wyoming   | General Motors ||\n|| Alabama   | Ford           ||\n|| Alaska    | Ford           ||\n|| ...         ...            ||\n|| Wyoming   | Ford           ||\n===============================\n</code></pre>\n\n<p>All 200 (50 \xd7 4) results are of course unique. However, given that there is an inherent Cartesian product, the number of results flatters the amount of "information content" or "entropy" of the table: every additional car manufacturer adds fifty results for the fifty US states. (Again, this is just an example; I\'m not interested in better ways to represent or run this particular query.)</p>\n\n<p>As such, we\'re looking for a metric that will give an indication as to the (loosely speaking) redundancy-free content in the table for better comparison of <em>content</em> across different results for different queries. Other result tables may contain a mix of different types of Cartesian products (e.g., consider generalising the query to any country, where each country itself has its own product of states and car manufacturers, etc.).</p>\n\n<p>Currently we\'re working off a simple metric which just counts unique term\u2013position combinations: for the above example, the metric gives 50 + 4 = 54. This may be sufficient for comparison, but is not sensitive to the combination of terms for individual results.</p>\n\n<p>Thanks to Wikipedia, I\'m aware of\u2014but not familiar with\u2014the notion of <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy in information theory</a>. However, I\'m unclear on how the concept of entropy could be applied to this use-case. (I\'m not interested in the entropy of the result strings; each term can be considered a "symbol".) Roughly speaking, each query variable could be considered as a free variable with the result terms in that column providing a set of possible outcomes and their frequency of occurrence being used as a probability mass function. This way I could compute the Shannon entropy for each column. But thereafter, I don\'t know how columns can be combined, or how tuples or results can be considered ... if a notion of conditional entropy would be better, etc.</p>\n\n<p>And so ...</p>\n\n<blockquote>\n  <p>Does anyone have pointers to related material on the measure of entropy/redundancy/etc. in tables or similar structures? </p>\n  \n  <p>Otherwise, does anyone have any ideas on how to use Shannon entropy in a convincing way for tabular data?</p>\n</blockquote>\n', 'ViewCount': '227', 'Title': 'Measuring entropy for a table (e.g., SQL results)', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T02:38:23.283', 'LastEditDate': '2012-10-23T02:38:23.283', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2357', 'Tags': '<reference-request><information-theory><data-compression><database-theory><entropy>', 'CreationDate': '2012-08-03T20:36:20.237', 'FavoriteCount': '1', 'Id': '3029'}{'ViewCount': '615', 'Title': 'Is Huffman Encoding always optimal?', 'LastEditDate': '2012-08-14T14:08:47.543', 'AnswerCount': '3', 'Score': '5', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '0', 'Body': '<p>The requirement of the encoding to be <em>prefix free</em> results in large trees due to the tree having to be complete. Is there a threshold where fixed-length non-encoded storage of data would be more efficient than encoding the data?</p>\n', 'Tags': '<information-theory><data-compression>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-14T14:08:47.543', 'CommentCount': '4', 'AcceptedAnswerId': '3179', 'CreationDate': '2012-08-07T01:25:12.653', 'Id': '3176'}{'Body': '<p>I am looking at a modelling tool and are trying to determine all the types of ways that you can model (at a rudimentary level) </p>\n\n<p>I remember seeing a list of ways in which you can connect or categorise information elements. basically the types were as follows:</p>\n\n<ul>\n<li>Lists - constitute a list of information elements</li>\n<li>Hierarchies- visualise information in a parent-child relationship (ie organisational chart)</li>\n<li><p>Flows - connect elements in a logical (lateral) flow (ie process model)</p>\n\n<p>Have you seen any reference to these types, or can you elaborate on the full list of "model types"</p></li>\n</ul>\n', 'ViewCount': '74', 'Title': 'What are the rudimentary types of information connectivity i.e. model types?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-14T20:46:24.903', 'LastEditDate': '2012-08-14T20:46:24.903', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2512', 'Tags': '<data-structures><information-theory><modelling><structured-data>', 'CreationDate': '2012-08-14T16:01:08.207', 'FavoriteCount': '1', 'Id': '3182'}{'Body': "<p>Is Huffman coding <strong>always</strong> optimal since it uses Shanon's ideas?\nWhat about text, image, video, ... compression?</p>\n\n<p>Is this subject still active in the field? What classical or modern references should I read?</p>\n", 'ViewCount': '552', 'Title': 'Is there any theoretically proven optimal compression algorithm?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-30T12:56:35.700', 'LastEditDate': '2012-08-30T12:56:35.700', 'AnswerCount': '4', 'CommentCount': '6', 'AcceptedAnswerId': '3317', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2609', 'Tags': '<algorithms><information-theory><data-compression>', 'CreationDate': '2012-08-24T17:34:44.707', 'Id': '3316'}{'Body': "<p>The only examples I've seen use bits as a measurement of entropy, but all these examples happen to use binary code alphabets. If we wanted to see how well a coding with a code alphabet of length n works, would we measure entropy in units of n?</p>\n\n<p>Or would it make sense to stay using bits if we're comparing codings with binary and n-length code alphabets?</p>\n", 'ViewCount': '308', 'Title': 'What units should Shannon entropy be measured in?', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T02:37:05.093', 'LastEditDate': '2012-10-23T02:37:05.093', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6243', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1554', 'Tags': '<information-theory><entropy>', 'CreationDate': '2012-10-22T18:53:17.060', 'Id': '6237'}{'Body': u"<p>I'm reading a paper that refers to the limit as n goes to infinity of R\xe9nyi entropy. It defines it as ${{H}_{n}}\\left( X \\right)=\\dfrac{1}{1-n} \\log_2 \\left( \\sum\\limits_{i=1}^{N}{p_{i}^{n}} \\right)$. It then says that the limit as $n\\to \\infty $ is $-\\log_2 \\left( p_1 \\right)$. I saw another article that uses the maximum of the ${{p}_{i}}'s$ instead of ${{p}_{1}}$. I think that this works out fairly easily if all of the ${{p}_{i}}'s$ are equal (a uniform distribution). I have no idea how to prove this for anything other than a uniform distribution. Can anyone show me how it's done?</p>\n", 'ViewCount': '191', 'Title': u'R\xe9nyi entropy at infinity or min-entropy', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T03:33:07.530', 'LastEditDate': '2012-10-23T02:36:30.930', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6248', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2016', 'Tags': '<information-theory><entropy>', 'CreationDate': '2012-10-22T21:39:37.093', 'Id': '6244'}{'Body': "<p>Let's start with the comparison sorting lower bound proof, which I'll summarize as follows:</p>\n\n<ol>\n<li>For $n$ distinct numbers, there are $n!$ possible orderings.</li>\n<li>There is only one correct sorted sequence of the $n$ numbers.</li>\n<li>We are given that comparison ($&lt;$) is the only operation we have that can narrow down the $n!$ possible orderings, and each comparison has only two possible outcomes.</li>\n<li>So $\\log_2(n!)$ comparisons are required.</li>\n</ol>\n\n<p>Now consider the following generalization of the argument above:</p>\n\n<ol>\n<li>Define a space of possibilities and its size (in the case of sorting, $n!$ orderings)</li>\n<li>Define the goal state and its size (in this case, only one correctly sorted answer)</li>\n<li>Define the amount of information that is gained at each step of the computation (in this case, one bit since there are only two possible outcomes per comparison)</li>\n<li>Calculate the information difference between the space of possibilities (step 1) and the goal space (step 2) and divide by the information gain per step (step 3) to yield the lower bound on the number of steps (in this case, $(\\log_2(n!)$ - $\\log_2(1))$ / 1 = $\\log_2(n!)$).</li>\n</ol>\n\n<p>Please answer all the following questions. I'm less concerned with the correctness of the particulars of step 4 than I am with the correctness of steps 1 - 3.</p>\n\n<ol>\n<li>Are there any problems with the generalized argument?</li>\n<li>If the problems can be fixed, what are the fixes?</li>\n<li>If the problems can't be fixed, please point out the fatal ones and provide directions to sources which describe these lower-bounds proofs and their pitfalls.</li>\n</ol>\n", 'ViewCount': '274', 'Title': 'Generalizing the Comparison Sorting Lower Bound Proof', 'LastEditorUserId': '19', 'LastActivityDate': '2012-12-23T05:23:41.497', 'LastEditDate': '2012-11-08T16:36:29.557', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1295', 'Tags': '<proof-techniques><sorting><information-theory><check-my-proof><lower-bounds>', 'CreationDate': '2012-11-08T15:48:38.323', 'FavoriteCount': '1', 'Id': '6562'}{'Body': "<p>Data structures are seen as important, equal to algorithms. This view is especially encouraged in situations, where appropriate data structure is the main factor that allows an algorithm to exist and to perform at satisfying complexity.</p>\n\n<p>However, despite the additional features of data structures (e.g. organization, like in trees storing entries according to less-like relation), all that data structures do is keeping information unchanged between algorithm's actions. Can this generic feature of data structures be izolated and defined in abstract way?</p>\n", 'ViewCount': '132', 'Title': 'Generalized data structure', 'LastActivityDate': '2012-11-30T07:13:01.040', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4773', 'Tags': '<algorithms><data-structures><information-theory>', 'CreationDate': '2012-11-28T04:32:26.293', 'Id': '6980'}{'Body': '<p>Suppose through $\\ell_1$ minimization I obtained two sparse probability distributions $P, Q$ which may contain many zero terms. Then I would like to compute the KL-Divergence of them $D(P || Q) = \\sum_i {p_i\\log(\\frac{p_i}{q_i})}$.  However, since the probability distribution is sparse, it might occur that $p_i \\not= 0$ and $q_i = 0$. In that case, KL-Divergence is not well defined.  One solution is to incorporate Dirichlet Prior. However, I am afraid by doing so the sparsity of the probability distributions is violated.  Is there any other way to compute the KL-Divergence of the two probability distributions?</p>\n', 'ViewCount': '91', 'Title': '$\\ell_1$ Minimization of Probability Distribution and KL-Divergence', 'LastEditorUserId': '19', 'LastActivityDate': '2012-12-14T15:07:19.970', 'LastEditDate': '2012-12-14T15:07:19.970', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7394', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<machine-learning><information-theory>', 'CreationDate': '2012-12-14T00:14:24.593', 'Id': '7392'}{'Body': "<p>I'm a pure mathematician interested in learning about information theory. Unfortunately, I'm about as pure as they come - my specialty is mathematical logic, and I have absolutely no experience with programming or anything of a computational nature. I tried looking at Shannon's original paper, but the profusion of numbers intimidated me (I'm a mathematician after all!) When he starts discussing the particulars of coding specific alphabets, my brain begins to asking whether this attention to detail is really necessary and soon zones out... It took Church's thesis for granted some time ago. (Pathetic, I know.)</p>\n\n<p>I'm wondering if there's some introduction to information theory written with people like me in mind. Something which uses the language of measure theory and recursion theory, and works in the greatest generality. Perhaps an expert in the area has written some notes in this direction as an idle folly? </p>\n\n<p>I realize that this might not be the place to ask this question, and apologize if this is the case. </p>\n", 'ViewCount': '124', 'Title': "Information theory from a (very pure) mathematician's perspective", 'LastActivityDate': '2013-01-30T22:19:54.050', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6618', 'Tags': '<information-theory>', 'CreationDate': '2013-01-30T05:40:30.553', 'FavoriteCount': '1', 'Id': '9296'}{'Body': '<p>I am learning about information theory and mutual information. However, I am quite confused with MI(Mutual information) vs. PMI(Pointwise mutual information) especially signs of MI and PMI values. Here are my questions.    </p>\n\n<ul>\n<li><p>Is MI values a non-negative value or it can be either positive or negative? If it is always a non-negative value, why is it ?</p></li>\n<li><p>As I search online, the PMI can be positive or negative values and the MI is the expected value of all possible PMI. However, expected value can be positive or negative. If MI is really the expected value of PMI, why is it always positive ?</p></li>\n</ul>\n\n<p>Did I misunderstand anything of MI and PMI here ? Thank you very much,</p>\n', 'ViewCount': '320', 'Title': 'Pointwise mutual information vs. Mutual information?', 'LastActivityDate': '2013-03-09T07:54:28.433', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10401', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7161', 'Tags': '<probability-theory><information-theory><entropy>', 'CreationDate': '2013-03-09T00:08:48.040', 'Id': '10396'}{'Body': '<p>I went to listen to a workshop and someone from the audience asked the presenter how the moments can improve the <a href="https://en.wikipedia.org/wiki/Mutual_information" rel="nofollow">mutual information</a>. I am learning about MI (Mutual Information) so didn\'t have enough knowledge to understand what it means. Then, I did some research but I still have some confusion. I am wondering if someone who has more knowledge about this can clarify things for me. Here are my questions:</p>\n\n<ul>\n<li><p>Mutual information is usually calculated by bin functions to estimate the probability of two random variables which can be a case of two vectors $X$ and $Y$. Is the moment generating function another way to estimate probability?</p></li>\n<li><p>If moment generating functions can present the probability of $X$ and $Y$, how do we calculate it? </p></li>\n<li><p>Does a MI have a moment generating function?</p></li>\n<li><p>If MI has a moment generating function, how can we present a MI of $X$ and $Y$ by its moment functions?</p></li>\n</ul>\n', 'ViewCount': '96', 'Title': 'Mutual information and moment generating functions', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-17T19:08:39.583', 'LastEditDate': '2013-03-17T19:08:39.583', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10524', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7161', 'Tags': '<probability-theory><information-theory><bioinformatics>', 'CreationDate': '2013-03-14T00:20:20.677', 'Id': '10513'}{'Body': u'<p>I was just reading from wikipedia the following about <a href="http://en.wikipedia.org/wiki/Information">information</a>:</p>\n\n<blockquote>\n  <p>From the stance of information theory, information is taken as a\n  sequence of symbols from an alphabet, say an input alphabet \u03c7, and an\n  output alphabet \u03d2. Information processing consists of an input-output\n  function that maps any input sequence from \u03c7 into an output sequence\n  from \u03d2. The mapping may be probabilistic or determinate. It may have\n  memory or be memoryless.</p>\n</blockquote>\n\n<p>I\'m just guessing here, but is the input alphabet the two states of a bit (0 and 1) and the output alphabet governed by the datatypes of the executing program when those bits are accessed?</p>\n', 'ViewCount': '99', 'Title': 'How are data types related to information theory?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-20T11:06:43.423', 'LastEditDate': '2013-03-20T11:06:43.423', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '6', 'OwnerDisplayName': 'Aaron Anodide', 'PostTypeId': '1', 'OwnerUserId': '7353', 'Tags': '<programming-languages><information-theory>', 'CreationDate': '2013-03-20T00:49:08.857', 'Id': '10631'}{'ViewCount': '300', 'Title': "What's harder: Shuffling a sorted deck or sorting a shuffled one?", 'LastEditDate': '2013-04-14T15:00:30.950', 'AnswerCount': '2', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '4197', 'FavoriteCount': '1', 'Body': '<p>You have an array of $n$ distinct elements. You have access to a comparator (a black box function taking two elements $a$ and $b$ and returning true iff $a &lt; b$) and a truly random source of bits (a black box function taking no arguments and returning an independently uniformly random bit). Consider the following two tasks:</p>\n\n<ol>\n<li>The array is currently sorted. Produce a uniformly (or approximately uniformly) randomly selected permutation.</li>\n<li>The array consists of some permutation selected uniformly at random by nature. Produce a sorted array.</li>\n</ol>\n\n<p>My question is</p>\n\n<blockquote>\n  <p>Which task requires more energy asymptotically?</p>\n</blockquote>\n\n<p>I am unable to define the question more precisely because I don\'t know enough about the connection between information theory, thermodynamics, or whatever else is needed to answer this question. However, I think the question can be made well-defined (and am hoping someone helps me with this in an answer!).</p>\n\n<p>Now, algorithmically, my intuition is that they are equal. Notice that every sort is a shuffle in reverse, and vice versa. Sorting requires $\\log n! \\approx n \\log n$ comparisons, while shuffling, since it picks a random permutation from $n!$ choices, requires $\\log n! \\approx n \\log n$ random bits. Both shuffling and sorting require about $n$ swaps.</p>\n\n<p>However, I feel like there should be an answer applying <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer\'s principle</a>, which says that it requires energy to "erase" a bit. Intuitively, I think this means that sorting the array is more difficult, because it requires "erasing" $n \\log n$ bits of information, going from a low-energy, high-entropy ground state of disorder to a highly ordered one. But on the other hand, for any given computation, sorting just transforms one permutation to another one. Since I\'m a complete non-expert here, I was hoping someone with a knowledge of the connection to physics could help "sort" this out!</p>\n\n<p>(The question didn\'t get any answers on <a href="http://math.stackexchange.com/questions/359911/which-takes-more-energy-shuffling-a-sorted-deck-or-sorting-a-shuffled-one">math.se</a>, so I\'m reposting it here. Hope that is ok.)</p>\n', 'Tags': '<algorithms><algorithm-analysis><information-theory><entropy>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-21T14:19:51.400', 'CommentCount': '15', 'AcceptedAnswerId': '11452', 'CreationDate': '2013-04-14T03:49:03.497', 'Id': '11299'}{'Body': '<p>We have well established theory for measuring the information content and randomness of binary strings. Notions such as Shanon entropy and Kolmogorov-complexity were developed for binary strings.</p>\n\n<p>For a binary square matrix, it is not sufficient to just convert the matrix into binary string and measure its information content or its randomness since naive unraveling of the binary matrix into binary string would lose the adjacency information in each row and in each column.</p>\n\n<p>My question: What are the analogous notions for measuring the information content and randomness of binary square matrix?</p>\n', 'ViewCount': '111', 'Title': 'Notions of information content and randomness of binary square matrix', 'LastEditorUserId': '96', 'LastActivityDate': '2013-04-27T14:40:58.330', 'LastEditDate': '2013-04-27T13:53:24.110', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '96', 'Tags': '<information-theory><entropy>', 'CreationDate': '2013-04-27T13:39:52.133', 'Id': '11601'}{'Body': "<p>Here is my problem. I have to compute the amount of information that is possible to encode in a string of bits. This string of bits represent a stream.\nLet us call such stream as $X_1,X_2,X_3,...,X_n$. Important: this succession of Random variable is not a Markov Process, but a process with memory!!\nI suppose that the right measure would be based on the information entropy of the collection of random variables:\n$H(X_1,X_2,X_3,...,X_n)=\\sum_i H(X_i|X_{i-1},...,X_1)$</p>\n\n<p>Am I right?</p>\n\n<p>However I have more doubts:</p>\n\n<ul>\n<li>If $n \\to \\infty$ then a better measure would be the entropy rate, isn't it?.</li>\n<li>What is meaning of $H(X_n)$, i.e. the entropy of only the last random variable?</li>\n<li>In term of information theory, what is the meaning of $H(X_1, X_n)$ or $H(X_1| X_n)$ ?</li>\n</ul>\n\n<p>A point is that I can compute both $H(X_1, X_n)$ and $H(X_1| X_n)$ for my process, but the computation of $H(X_1,X_2,X_3,...,X_n)$ remains very hard, as the process is non markovian.</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '70', 'Title': 'Information of a stream of bits', 'LastEditorUserId': '8353', 'LastActivityDate': '2013-05-28T03:30:45.200', 'LastEditDate': '2013-05-25T18:53:15.680', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-1', 'OwnerDisplayName': 'altroware', 'PostTypeId': '1', 'OwnerUserId': '8353', 'Tags': '<information-theory><coding-theory>', 'CreationDate': '2013-05-25T15:47:53.660', 'Id': '12273'}{'Body': "<p>I'll phrase my question using an intuitive and rather extreme example:</p>\n\n<p><strong>Is the expected compression ratio (using zip compression) of a children's book higher than that of a novel written for adults?</strong></p>\n\n<p>I read somewhere that specifically the compression ratio for zip compression can be considered an indicator for the information (as interpreted by a human being) contained in a text. Can't find this article anymore though.</p>\n\n<p>I am not sure how to attack this question. Of course no compression algorithm can grasp the meaning of verbal content. So what would a zip compression ratio reflect when applied to a text? Is it just symbol patterns - like word repetitions will lead to higher ratio - so basically it would just reflect the vocabulary?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>Another way to put my question would be whether there is a correlation which goes beyond repetition of words / restricted vocabulary.</p>\n", 'ViewCount': '100', 'Title': 'Is there a correlation of zip compression ratio and density of information provided by a text?', 'LastEditorUserId': '9994', 'LastActivityDate': '2013-09-06T01:57:55.437', 'LastEditDate': '2013-09-05T19:05:07.420', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '14161', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9994', 'Tags': '<information-theory><data-compression>', 'CreationDate': '2013-09-05T13:16:22.927', 'Id': '14150'}{'Body': "<p>Suppose I have a compressed file and it is not possible to compress it more without loss of information. We say that this file is random or pseudorandom.</p>\n\n<p>So, if the randomness means not comprehensible and not compressible, I don't understand why ths file is, at the same time, information that my computer and I can understand.</p>\n\n<p>This file could be a book that my computer can show to me and read, and I can read and sum it ...so, it is really randomness?</p>\n\n<p>Note: I understand that if I can make a summary of a text or define it with less words, that not means that it could be possible to get all the information of this book again, of course but this book is not random for me.</p>\n\n<p>Note II: I undesrtand ramdoness as something that is not possible to reproduce with an smaller algorithm. I mean a string is random when I can't find an other smaller string that is an algorithm that can reproduce the first one.    </p>\n\n<p>Note III: I want to thank you all for your help. </p>\n", 'ViewCount': '155', 'Title': 'compressed information = randomness?', 'LastEditorUserId': '9631', 'LastActivityDate': '2013-10-04T07:52:52.077', 'LastEditDate': '2013-10-04T07:52:52.077', 'AnswerCount': '3', 'CommentCount': '13', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9631', 'Tags': '<information-theory><data-compression>', 'CreationDate': '2013-10-03T07:32:44.930', 'FavoriteCount': '0', 'Id': '14772'}{'Body': '<p>Suppose I have $n$ independent observations $x_1,\\dots,x_n$ from some unknown distribution over a known alphabet $\\Sigma$, and I want to estimate the entropy of the distribution.  I can count the frequency $f_s$ of each symbol $s \\in \\Sigma$ among the observations; how should I use them to estimate the Shannon entropy of the source?</p>\n\n<hr>\n\n<p>The obvious approach is to estimate the probability of each symbol $s$ as $\\Pr[X=s]=f_s/n$, and then calculate the entropy using the standard formula for Shannon entropy.  This leads to the following estimate of the entropy $H(X)$:</p>\n\n<p>$$\\text{estimate}(H(X)) = - \\sum_{s \\in \\Sigma} {f_s \\over n} \\lg (f_s/n).$$</p>\n\n<p>However, this feels like it might not produce the best estimate.  Consider, by analogy, the problem of estimating the probability of symbol $s$ based upon its frequency $f_s$.  The naive estimate $f_s/n$ is likely an underestimate of its probability.  For instance, if I make 100 observations of birds in my back yard and none of them were a hummingbird, should my best estimate of the probability of seeing a hummingbird on my next observation be exactly 0?  No, instead, it\'s probably more realistic to estimate the probability is something small but not zero.  (A zero estimate means that a hummingbird is absolutely impossible, which seems unlikely.)</p>\n\n<p>For the problem of estimating the probability of symbol $s$, there are a number of standard techniques for addressing this problem.  <a href="https://en.wikipedia.org/wiki/Laplace_smoothing" rel="nofollow">Additive smoothing</a> (aka Laplace smoothing) is one standard technique, where we estimate the probability of symbol $s$ as $\\Pr[X=s] = (f_s + 1)/(n+|\\Sigma|)$.  Others have proposed Bayesian smoothing or other methods.  These methods are widely used in natural language processing and document analysis, where just because a word never appears in your document set doesn\'t mean that the word has probability zero.  In natural language processing, this also goes by the name <a href="https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques" rel="nofollow">smoothing</a>.</p>\n\n<p>So, taking these considerations into account, how should I estimate the entropy, based upon observed frequency counts?  Should I apply additive smoothing to get an estimate of each of the probabilities $\\Pr[X=s]$, then use the standard formula for Shannon entropy with those probabilities?  Or is there a better method that should be used for this specific problem?</p>\n', 'ViewCount': '108', 'Title': 'Estimate entropy, based upon observed frequency counts', 'LastActivityDate': '2013-10-13T18:18:02.167', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<information-theory><statistics><natural-lang-processing><entropy><information-retrieval>', 'CreationDate': '2013-10-11T21:26:30.400', 'Id': '15010'}{'Body': '<p>If we consider polynomial-time (or log-space) computable reductions $&lt;_p^m$ as  transformations between computational problems, then the following definitions of known complexity classes suggest the conservation of information under "efficient" transformations. Assuming $P\\ne NP$, it seems that information flows only from easy problems to hard problems thorough efficient reductions.</p>\n\n<p>$P=\\{L| L&lt;_p Horn3SAT, \\bar L &lt;_p Horn3SAT \\}$ </p>\n\n<p>$NP=\\{L| L&lt;_p 3SAT \\}$</p>\n\n<p>$CoNP=\\{L| \\bar L&lt;_p 3SAT \\}$</p>\n\n<p>$NPC=\\{L| L&lt;_p 3SAT, 3SAT&lt;_p L \\}$</p>\n\n<p>$PC=\\{L| L&lt;_p Horn3SAT, Horn3SAT&lt;_p L \\}$</p>\n\n<blockquote>\n  <p>Is there a notion of computational hardness in terms of information flow that explains this apparent asymmetry of information flow between natural computational problems?</p>\n</blockquote>\n\n<p>I am aware of the theorem that $P=NP$ if and only if a sparse set is $NP$-complete. I\'m looking for notions different than set sparsity.</p>\n', 'ViewCount': '47', 'Title': 'Notions of computational hardness in terms of information flow?', 'LastEditorUserId': '96', 'LastActivityDate': '2013-11-20T11:10:22.187', 'LastEditDate': '2013-11-20T11:10:22.187', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '96', 'Tags': '<complexity-theory><information-theory>', 'CreationDate': '2013-11-20T10:48:22.983', 'Id': '18195'}{'Body': '<p>I\'m trying to understand how <a href="http://en.wikipedia.org/wiki/Lempel-Ziv-Welch" rel="nofollow">LZW</a> decodes a string.</p>\n\n<p>For example suppose that we have a dictionary where:</p>\n\n<ul>\n<li>a=0</li>\n<li>b=1</li>\n</ul>\n\n<p>and we have to encode the string "<strong>aabbabaabb</strong>", so the output of the encoding process (if I have not made mistakes) produces the string "<strong>0011324</strong>" and the dictionary became(using a tree representation):</p>\n\n<p><img src="http://f.cl.ly/items/2j2F0G0t1P120j3A2X0Z/tree.jpg" width="400" height="300"></p>\n\n<p>The decoder will just know "0011324" and the starting dictionary where there are just a=0 and b=1.</p>\n\n<blockquote>\n  <p>How does it work the decoding process with just these 2 informations?</p>\n</blockquote>\n', 'ViewCount': '95', 'Title': 'LZW decoding process', 'LastActivityDate': '2013-11-22T14:00:53.203', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<information-theory><lempel-ziv>', 'CreationDate': '2013-11-22T12:07:21.620', 'Id': '18257'}{'Body': '<p>If bits are the base unit of information, why are bytes treated like the base unit? </p>\n\n<p>For example, usually values are expressed in Mega/Giga/Tera/Exa bytes instead of bits. I am aware that bits are sometimes used (e.g. sometimes for internet speed), but generally to me it seems like bytes are used as if they are the base unit instead of bits. </p>\n', 'ViewCount': '46', 'Title': 'Why are bytes treated like the base unit?', 'LastActivityDate': '2013-12-10T01:02:05.033', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '18804', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11968', 'Tags': '<information-theory>', 'CreationDate': '2013-12-09T22:47:31.387', 'Id': '18796'}{'Body': '<p>Some researchers are <a href="http://www.nature.com/nmat/journal/v6/n11/abs/nmat2023.html" rel="nofollow">trying to get a memory cell capable of having 3 states</a> instead of 2.</p>\n\n<p>1) How many memory cells, in principle and as a rough estimate, does a typical 1 megabyte memory chip has? is it 8*1024 cells?</p>\n\n<p>2) If you have N memory cells, each has x logical levels, what is the number of possible representations that we can get out of it? is it x^N representations?</p>\n\n<p>Let\'s take Binary system as an example. Say we have 8*1024 memory cells. We can store \nonly one of the possible 2^N representations in each cell, so at the end the number of stored bits is just N=8*1024. \nNow, for Ternary system, the gain is log2(3) ~ 1.58 times so we can store ~ 1.58 * 8 * 1024 "bits" or 8*1024 "trits".</p>\n\n<p>Correct?\nEDIT: sorry I meant 8*1024*1024.</p>\n', 'ViewCount': '27', 'Title': 'Information capacity of Ternary-based system over Binary-based', 'LastEditorUserId': '11647', 'LastActivityDate': '2013-12-18T20:33:42.583', 'LastEditDate': '2013-12-18T20:33:42.583', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11647', 'Tags': '<information-theory>', 'CreationDate': '2013-12-18T05:27:19.933', 'Id': '19084'}{'Body': "<p>I've not clear how to pass from final interval to code value, for example:</p>\n\n<p>Suppose we have the set of <b>symbols</b>={0,1,2,3} with <b>probability</b>={0.2, 0.5, 0.2 , 0.1} and that we have to encode a <b>source</b> S={2,1,0,0,1,3};</p>\n\n<p>After the encoding process we'll have an interval $[ 0.7426, 0.7428 ) $.</p>\n\n<p>Now the final step is to find the shortest representation to transmit and in the book the chosen value is <b>0.10111110001</b> = <b>0.74267578125</b>.</p>\n\n<blockquote>\n  <p>How is it possible to calculate the shortest representation and the code value to transmit having the final interval?</p>\n</blockquote>\n", 'ViewCount': '37', 'Title': 'Arithmetic code: from interval to code value', 'LastEditorUserId': '4765', 'LastActivityDate': '2014-01-28T14:45:38.593', 'LastEditDate': '2014-01-24T14:47:50.177', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<information-theory><data-compression>', 'CreationDate': '2014-01-24T13:56:58.043', 'Id': '19939'}{'Body': '<p>Given normally distributed integers with a mean of 0 and a standard deviation $\\sigma$ around 1000, how do I compress those numbers (almost) perfectly?  Given the entropy of the Gaussian distribution, it should be possible to store any value $x$ using $$\\frac{1}{2} \\mathrm{log}_2(2\\pi\\sigma^2)+\\frac{x^2}{2\\sigma^2}\\rm{log}_2\\rm{e}$$<br>\nbits.  The way to accomplish this perfect compression would be <a href="http://en.wikipedia.org/wiki/Arithmetic_coding" rel="nofollow">arithmetic coding</a>.  In principle it\'s not too hard, I can calculate the interval boundaries from the <a href="http://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="nofollow">cumulative distribution function</a> of the gaussian.  In practice I hit considerable difficulties because when using floating point operations I cannot achieve perfect reproduction of the results, and I have no idea how to do this without FP operations.  Perfect reproduction is necessary because the uncompressing code must come up with exactly the same interval boundaries as the compressing code.  So the question is:  How do I compute the interval boundaries?  Or is there any other way to achieve (near) perfect compression of such data?</p>\n\n<p><strong>Edit:</strong>  As Raphael said, strictly speaking the normal distribution is defined only for continuous variables.  So what I mean here are integers x with a probability distribution function $$P(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\rm e^{-\\frac{x^2}{2\\sigma^2}}$$.  </p>\n\n<p><strong>Edit2:</strong>  As Yuval said, this distribution does not sum up exactly to 1, however, for $\\sigma&gt;100$  the difference from 1 is less than $10^{-1000}$ and hence it\'s more precise than any practical calculation would be.</p>\n', 'ViewCount': '101', 'Title': 'Compressing normally distributed data', 'LastEditorUserId': '12710', 'LastActivityDate': '2014-03-04T06:50:22.477', 'LastEditDate': '2014-01-31T17:46:33.920', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22261', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12710', 'Tags': '<information-theory><randomness><data-compression><entropy>', 'CreationDate': '2014-01-31T14:34:01.367', 'Id': '20156'}{'Body': u'<p>Difference between \u201caverage length\u201d and \u201centropy\u201d gives the percent of optimal. The optimal case is when the average length of a code is equal to the entropy. For example if average length is 1 and entropy is 0.72: (1- 0.72) = 0.28 -> 28% worse than optimal.</p>\n\n<p>If both \u201caverage length\u201d and \u201centropy\u201d are 1, the compression is optimal.</p>\n\n<p>But what does it mean if the result is negative value?</p>\n\n<p>Note: entropy : -(pr*lg(pr)).\nAverage length: probability*number of bit</p>\n', 'ViewCount': '52', 'Title': 'Gap between the average length of a Huffman code and its entropy', 'LastEditorUserId': '31', 'LastActivityDate': '2014-02-14T14:29:24.373', 'LastEditDate': '2014-02-14T14:29:24.373', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'user21415', 'PostTypeId': '1', 'Tags': '<information-theory>', 'CreationDate': '2014-02-03T14:25:22.460', 'Id': '21351'}{'ViewCount': '50', 'Title': 'Showing that the entropy of i.i.d. random variables is the sum of entropies', 'LastEditDate': '2014-02-11T11:03:53.987', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Danny', 'PostTypeId': '1', 'OwnerUserId': '13249', 'Body': "<p>The shannon entropy of a random variable $Y$ (with possible outcomes $\\Sigma=\\{\\sigma_{1},...,\\sigma_{k}\\}$) is given by<br>\n$H(Y)=-\\sum\\limits_{i=1}^{k}P(Y=\\sigma_{i})\\;\\log(P(Y=\\sigma_{i}))$.  </p>\n\n<p>For a second random variable $X=X_{1}X_{2}...X_{n}$, where all $X_{i}$'s are independent and equally distributed (each $X_{i}$ is a copy of the same random variable $Y$), the following equation is known to be true:  </p>\n\n<blockquote>\n  <p>$H(X)=n\\cdot H(Y)$</p>\n</blockquote>\n\n<p>I want to prove this simple equation, where the outcomes from $Y$ are interpreted as symbols from an alphabet $\\Sigma$ and therefore $X$ is the random variable for strings of length $n$ (based on the distribution of $Y$).</p>\n\n<p>It is easy to see, that\n$P(X=w)=P(X=w_{1}...w_{n})=P(X_{1}=w_{1})\\;\\cdot\\;...\\;\\cdot\\; P(X_{n}=w_{n})=\\prod\\limits_{i=1}^{n}P(Y=w_{i})$<br>\n... but my approach to prove $H(X)=n\\cdot H(Y)$ seems to be in a dead point<br>\n(every word $w$ has the form $w=w_{1}...w_{n}$ with $w_{i}\\in\\Sigma$ and  let $\\large |w|_{\\sigma_{i}}$ be the number of occurrences of $\\sigma_{i}$ in $w$):  </p>\n\n<p>$H(X)=-\\sum\\limits_{w\\in\\Sigma^{n}}P(X=w)\\;\\log(P(X=w))$<br>\n$=-\\sum\\limits_{w\\in\\Sigma^{n}}\\left(\\prod\\limits_{i=1}^{n}P(Y=w_{i})\\right)\\;\\log\\left(\\prod\\limits_{i=1}^{n}P(Y=w_{i})\\right)$<br>\n$=-\\sum\\limits_{w\\in\\Sigma^{n}}\\left(\\prod\\limits_{i=1}^{n}P(Y=w_{i})\\right)\\left(\\sum\\limits_{i=1}^{n}\\log\\left(P(Y=w_{i})\\right)\\right)$<br>\n$=-\\sum\\limits_{w\\in\\Sigma^{n}}\\left(\\prod\\limits_{i=1}^{k}P(Y=\\sigma_{i})^{\\large |w|_{\\sigma_{i}}}\\right)\\left(\\sum\\limits_{i=1}^{k}\\large |w|_{\\sigma_{i}}\\normalsize \\;\\log\\left(P(Y=\\sigma_{i})\\right)\\right)$  </p>\n\n<p>So, I am able to change some indices from word length to the length of the alphabet, which is used in $H(Y)$. But what now? Any help?</p>\n", 'Tags': '<information-theory><entropy>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-11T11:03:53.987', 'CommentCount': '0', 'AcceptedAnswerId': '21526', 'CreationDate': '2014-02-06T15:59:58.167', 'Id': '21525'}{'Body': '<p>Consider the following Huffman trees:<br>\n<img src="http://i.stack.imgur.com/Q3bzR.png" alt="enter image description here"></p>\n\n<p>I was asked if those trees can have the same corpus. My answer was no, based on these calculations:  </p>\n\n<p>For the right tree:<br>\n$a_1 \\le a_2$<br>\n$a_1 + a_2 \\le a_5$<br>\n$a_3 \\le a_4$<br>\n$a_1 + a_2 + a_5 \\le a_3 + a_4$</p>\n\n<p>For the left tree:<br>\n$a_1 \\le a_2$<br>\n$a_3 \\le a_4$<br>\n$a_1 + a_2 + a_3 + a_4 \\le a_5$  </p>\n\n<p>Adding the last equations from each tree we have that:<br>\n$2a_1 + 2a_2 \\le 0$ Which is a contradiction because frequency cannot be negative.</p>\n\n<p>Nevertheless, I understood that there is a possibility that the two trees would have the same corpus. For instance, consider $1,1,1,2,3$.</p>\n\n<p>So, where do my calculations go wrong?</p>\n', 'ViewCount': '18', 'Title': 'Do the two huffman trees have the same corpus?', 'LastEditorUserId': '12859', 'LastActivityDate': '2014-03-11T02:11:18.050', 'LastEditDate': '2014-03-11T02:11:18.050', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22463', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15485', 'Tags': '<data-structures><trees><information-theory><data-compression>', 'CreationDate': '2014-03-10T10:19:44.723', 'Id': '22459'}{'Body': '<p>Let $G$ be a graph whose Shannon Capacity is $\\Theta(G)$. Is there any graph product for which the Shannon Capacity is $\\Theta(G)^k$ where $k$ is the number of times the product is taken?</p>\n', 'ViewCount': '19', 'Title': 'On Shannon Capacity', 'LastActivityDate': '2014-03-18T20:08:37.647', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22768', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9753', 'Tags': '<graph-theory><information-theory>', 'CreationDate': '2014-03-18T18:28:33.060', 'Id': '22764'}{'ViewCount': '2223', 'LastEditorDisplayName': 'user15782', 'Title': 'Can PRNGs be used to magically compress stuff?', 'LastEditDate': '2014-03-26T15:26:43.373', 'AnswerCount': '6', 'Score': '23', 'OwnerDisplayName': 'user15782', 'PostTypeId': '1', 'FavoriteCount': '3', 'Body': '<p>This idea occurred to me as a kid learning to program and\non first encountering PRNG\'s. I still don\'t know how realistic\nit is, but now there\'s stack exchange.</p>\n\n<p>Here\'s a 14 year-old\'s scheme for an amazing compression algorithm: </p>\n\n<p>Take a PRNG and seed it with seed <code>s</code> to get a long sequence \nof pseudo-random bytes. To transmit that sequence to another party, \nyou need  only communicate a description of the PRNG, the appropriate seed \nand the length of the message. For a long enough sequence, that \ndescription would be much shorter then the sequence itself.</p>\n\n<p>Now suppose I could invert the process. Given enough time and \ncomputational resources, I could do a brute-force search and find \na seed (and PRNG, or in other words: a program) that produces my\ndesired sequence (Let\'s say an amusing photo of cats being mischievous).</p>\n\n<p>PRNGs repeat after a large enough number of bits have been generated,\nbut compared to "typical" cycles my message is quite short so this \ndosn\'t seem like much of a problem.</p>\n\n<p>Voila, an effective (if rube-Goldbergian) way to compress data.</p>\n\n<p>So, assuming:</p>\n\n<ul>\n<li>The sequence I wish to compress is finite and known in advance.</li>\n<li>I\'m not short on cash or time (Just as long as a finite amount \nof both is required)</li>\n</ul>\n\n<p>I\'d like to know:</p>\n\n<ul>\n<li>Is there a fundamental flaw in the reasoning behind the scheme? </li>\n<li>What\'s the standard way to analyse these sorts of thought experiments?</li>\n</ul>\n\n<p><em>Summary</em></p>\n\n<p>It\'s often the case that good answers make clear not only the answer, \nbut what it is that I was really asking. Thanks for everyone\'s patience \nand detailed answers. </p>\n\n<p>Here\'s my nth attempt at a summary of the answers:</p>\n\n<ul>\n<li>The PRNG/seed angle doesn\'t contribute anything, it\'s no more \nthen a program that produces the desired sequence as output.</li>\n<li>The pigeonhole principle: There are many more messages of \nlength > k then there are (message generating) programs of \nlength &lt;= k. So some sequences simply cannot be the output of a \nprogram shorter then the message. </li>\n<li>It\'s worth mentioning that the interpreter of the program \n(message) is necessarily fixed in advance. And it\'s design \ndetermines the (small) subset of messages which can be generated\nwhen a message of length k is received.</li>\n</ul>\n\n<p>At this point the original PRNG idea is already dead, but there\'s \nat least one last question to settle:</p>\n\n<ul>\n<li>Q: Could I get lucky and find that my long (but finite) message just \nhappens to be the output of a program of length &lt; k bits?</li>\n</ul>\n\n<p>Strictly speaking, it\'s not a matter of chance since the \nmeaning of every possible message (program) must be known \nin advance. Either it <em>is</em> the meaning of some message \nof &lt; k bits <em>or it isn\'t</em>.</p>\n\n<p>If I choose a random message of >= k bits randomly (why would I?),\nI would in any case have a vanishing probability of being able to send it\nusing less then k bits, and an almost certainty of not being able \nto send it at all using less then k bits.</p>\n\n<p>OTOH, if I choose a specific message of >= k bits from those which\nare the output of a program of less then k bits (assuming there is \nsuch a message), then in effect I\'m taking advantage of bits already\ntransmitted to the receiver (the design of the interpreter), which \ncounts as part of the message transferred.</p>\n\n<p>Finally:</p>\n\n<ul>\n<li>Q: What\'s all this <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy</a>/<a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">kolmogorov complexity</a> business?</li>\n</ul>\n\n<p>Ultimately, both tell us the same thing as the (simpler) piegonhole \nprinciple tells us about how much we can compress: perhaps \nnot at all, perhaps some, but certainly not as much as we fancy\n(unless we cheat).</p>\n', 'Tags': '<information-theory><randomness><data-compression>', 'LastActivityDate': '2014-03-26T15:26:43.373', 'CommentCount': '13', 'AcceptedAnswerId': '23020', 'CreationDate': '2014-03-24T17:02:03.550', 'Id': '23010'}{'ViewCount': '56', 'Title': 'Information loss of a 9-input majority gate', 'LastEditDate': '2014-04-21T03:24:33.107', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8367', 'Body': '<p>According to information theory, the logic gates AND, NAND, OR, NOR all lose 1.189 bits of information each with two bits of information at their inputs and with all inputs being independently and uniformly distributed.</p>\n\n<p>When gates are used in combination with other gates in a circuit, to calculate the entropy loss for the entire circuit you have to use what are called "mixture probabilities" or "mixing distributions" to calculate the entropy loss, the loss for each gate being dependent on the entropy loss history of the preceding gates.</p>\n\n<p>Can anyone calculate the entropy loss of a majority gate with 9 inputs, all equally likely $\\{1,0\\}$?</p>\n', 'ClosedDate': '2014-04-23T16:48:12.220', 'Tags': '<information-theory>', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-21T03:24:33.107', 'CommentCount': '6', 'AcceptedAnswerId': '23902', 'CreationDate': '2014-04-18T02:09:38.220', 'Id': '23901'}