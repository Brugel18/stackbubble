{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '172', 'Title': 'Overflow safe summation', 'LastEditDate': '2012-04-23T22:15:35.557', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '1', 'Body': '<p>Suppose I am given $n$ fixed width integers (i.e. they fit in a register of width $w$), $a_1, a_2, \\dots a_n$ such that their sum $a_1 + a_2 + \\dots + a_n = S$ also fits in a register of width $w$.</p>\n\n<p>It seems to me that we can always permute the numbers to $b_1, b_2, \\dots b_n$ such that each prefix sum $S_i = b_1 + b_2 + \\dots + b_i$ also fits in a register of width $w$.</p>\n\n<p>Basically, the motivation is to compute the sum $S = S_n$ on fixed width register machines without having to worry about integer overflows at any intermediate stage.</p>\n\n<p>Is there a fast (preferably linear time) algorithm to find such a permutation (assuming the $a_i$ are given as an input array)? (or say if such a permutation does not exist).</p>\n', 'Tags': '<algorithms><arrays><integers><numerical-analysis>', 'LastEditorUserId': '139', 'LastActivityDate': '2012-04-23T22:15:35.557', 'CommentCount': '5', 'AcceptedAnswerId': '1425', 'CreationDate': '2012-04-21T23:39:36.593', 'Id': '1424'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Suppose I am given an array of $n$ fixed width integers (i.e. they fit in a register of width $w$), $a_1, a_2, \\dots a_n$. I want to compute the sum $S = a_1 + \\ldots + a_n$ on a machine with 2\'s complement arithmetic, which performs additions modulo $2^w$ with wraparound semantics. That\'s easy \u2014 but the sum may overflow the register size, and if it does, the result will be wrong.</p>\n\n<p>If the sum doesn\'t overflow, I want to compute it, and to verify that there is no overflow, as fast as possible. If the sum overflows, I only want to know that it does, I don\'t care about any value.</p>\n\n<p>Naively adding numbers in order doesn\'t work, because a partial sum may overflow. For example, with 8-bit registers, $(120, 120, -115)$ is valid and has a sum of $125$, even though the partial sum $120+120$ overflows the register range $[-128,127]$.</p>\n\n<p>Obviously I could use a bigger register as an accumulator, but let\'s assume the interesting case where I\'m already using the biggest possible register size.</p>\n\n<p>There is a well-known technique to <a href="http://cs.stackexchange.com/a/1425">add numbers with the opposite sign as the current partial sum</a>. This technique avoids overflows at every step, at the cost of not being cache-friendly and not taking much advantage of branch prediction and speculative execution.</p>\n\n<p>Is there a faster technique that perhaps takes advantage of the permission to overflow partial sums, and is faster on a typical machine with an overflow flag, a cache, a branch predictor and speculative execution and loads?</p>\n\n<p>(This is a follow-up to <a href="http://cs.stackexchange.com/questions/1424/overflow-safe-summation">Overflow safe summation</a>)</p>\n', 'ViewCount': '203', 'Title': 'Detecting overflow in summation', 'LastEditorUserId': '39', 'LastActivityDate': '2014-01-31T16:51:25.577', 'LastEditDate': '2012-04-22T15:03:49.187', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<algorithms><arrays><integers><numerical-analysis>', 'CreationDate': '2012-04-22T01:16:19.560', 'FavoriteCount': '1', 'Id': '1426'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have trouble understanding the cyclic coordinate method. How does it differ with the <a href="http://en.wikipedia.org/wiki/Pattern_search_%28optimization%29" rel="nofollow">Hook and Jeeves method</a> and the <a href="http://en.wikipedia.org/wiki/Rosenbrock_methods" rel="nofollow">Rosenbrock method</a>?</p>\n\n<p>From a past exam text:</p>\n\n<blockquote>\n  <p>Describe the cyclic coordinate method and outline the similarities and the \n  differences between the Cyclic Coordinate method, the Hooke and Jeeves \n  method, and the Rosenbrock method.</p>\n</blockquote>\n\n<p>I would appreciate a good reference, I\'m having trouble finding any.</p>\n', 'ViewCount': '258', 'Title': 'Cyclic coordinate method: how does it differ from Hook & Jeeves and Rosenbrock?', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-27T17:35:21.360', 'LastEditDate': '2012-11-27T17:35:21.360', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'qurty', 'PostTypeId': '1', 'Tags': '<algorithms><reference-request><optimization><numerical-analysis>', 'CreationDate': '2012-05-06T17:32:23.577', 'Id': '1792'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am seeking a fast algorithm to compute the following function, a quantile of the <a href="http://en.wikipedia.org/wiki/Poisson_distribution" rel="nofollow">Poisson distribution</a>:\n$$f(n, \\lambda) = e^{-\\lambda} \\sum_{k=0}^{n} \\frac{\\lambda^k}{k!} $$</p>\n\n<p>I can think of an algorithm in $O(n)$, but considering the structure of the series, there is probably a $O(1)$ solution (or at least a good $O(1)$ approximation). Any take?</p>\n', 'ViewCount': '222', 'Title': 'Fast Poisson quantile computation', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-11T22:53:46.723', 'LastEditDate': '2012-05-11T22:33:12.843', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1457', 'Tags': '<algorithms><numerical-analysis>', 'CreationDate': '2012-05-11T14:33:03.150', 'Id': '1796'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am doing FFT-based multiplication of polynomials with integer coefficients (long integers, in fact). The coefficients have a maximum value of $BASE-1, \\quad BASE \\in \\mathbb{n},\\quad BASE &gt; 1$. </p>\n\n<p>I would like to put forward a formal argument that if we use complex DFT for computing a convolution on a physical machine, it will yield incorrect results at some transform length $n\\in \\mathbb{N}$. </p>\n\n<p>What was easy to prove was the fact that at some big $n$ computing the convolution with DFT will not at all be possible, since, for example, the following difference of primitive roots modulo $n$:  $\\omega_n^1 - \\omega_n^2 \\rightarrow 0$ when $n \\rightarrow \\infty$, and if we are restricted by some machine epsilon $\\epsilon$, at some $n$ it will make the values indistinguishable and interpolation impossible.</p>\n\n<p>But the boundary I\'ve received using such an argument was way too big: only for $n=2^{60}$ I\'ve received $\\omega_n^1 - \\omega_n^2$ that had both components, $Re$ and $Im$, less than representable by $double$-precision type. This certainly is a boundary, but not very practical one.</p>\n\n<p>What I would like to show (if it is possible), is that much earlier than interpolation becomes theoretically impossible, the round-off errors will start to give wrong coefficients in the convolution, so that</p>\n\n<p>$$a\\cdot b \\neq IDFT(DFT(a)\\times DFT(b)),$$</p>\n\n<p>where $DFT$ and $IDFT$ are algorithm implementations that I use to calculate the Fourier transform. </p>\n\n<p>Maybe it is possible to make use of the fact that the value of the primitive root modulo $n$, $\\omega_n = \\exp(-2\\pi i / n)$, is an irrational number for the majority of $n$\'s. It will thereby be computed with inevitable error $\\psi$, defined as the value needed to "round off" everything that\'s less than the machine epsilon $\\epsilon$. Thus all the values used for DFT,</p>\n\n<p>$$\\omega_n^0, \\omega_n^1, ..., \\omega_n^{n-1},$$</p>\n\n<p>except for $\\omega_n^0$ will also be computed with errors. </p>\n\n<p>Since I\'m not a good mathematician at all, I don\'t know if and how I could use this fact to prove that the situation is going to worsen with increasing $n$ and that eventually the convolution is going to be computed incorrectly. </p>\n\n<p>I would also like to have and argument for OR against the following claim: for fixed $n$, the maximal error will be produced when all the coefficients of both polynomials are $BASE-1$.</p>\n\n<p>Thank you very much in advance!</p>\n', 'ViewCount': '101', 'Title': 'An argument for error accumulation during complex DFT', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-14T16:26:36.650', 'LastEditDate': '2012-05-14T15:45:22.677', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1198', 'Tags': '<algorithms><proof-techniques><numerical-analysis>', 'CreationDate': '2012-05-12T21:44:19.307', 'Id': '1814'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need a super-fast method for ordinary differential equations. Should I use the <a href="http://en.wikipedia.org/wiki/Midpoint_method" rel="nofollow">midpoint method</a>? I need this for a <a href="http://en.wikipedia.org/wiki/Reaction%E2%80%93diffusion_system" rel="nofollow">reaction-diffusion system</a> (Gray-Scott).</p>\n', 'ViewCount': '81', 'Title': "Which method for ODE instead of Euler's?", 'LastEditorUserId': '472', 'LastActivityDate': '2012-07-21T20:37:26.090', 'LastEditDate': '2012-07-21T20:37:26.090', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2240', 'Tags': '<algorithms><numerical-analysis>', 'CreationDate': '2012-07-21T20:02:43.227', 'Id': '2854'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '197', 'Title': 'Floating point rounding', 'LastEditDate': '2012-08-14T20:48:35.540', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '565', 'FavoriteCount': '2', 'Body': '<p>Can an IEEE-754 floating point number &lt; 1 (i.e. generated with a random number generator which generates a number >= 0.0 and &lt; 1.0) ever be multiplied by some integer (in floating point form) to get a number equal to or larger than that integer due to rounding?</p>\n\n<p>i.e.</p>\n\n<pre><code>double r = random() ; // generates a floating point number in [0, 1)\ndouble n = some_int ;\nif (n * r &gt;= n) {\n    print \'Rounding Happened\' ;\n}\n</code></pre>\n\n<p>This might be equivalent to saying that does there exist an N and R such that if R is the largest number less than 1 which can be represented in IEEE-754 then N * R >= N (where * and >= are appropriate IEEE-754 operators)</p>\n\n<p>This comes from <a href="http://stackoverflow.com/questions/1400505/postgresql-random-number-range-1-10/1400752#comment15929846_1400752">this question</a> based on <a href="http://www.postgresql.org/docs/9.1/static/datatype-numeric.html#DATATYPE-FLOAT">this documentation</a> and the postgresql <a href="http://www.postgresql.org/docs/8.2/static/functions-math.html#FUNCTIONS-MATH-FUNC-TABLE">random function</a></p>\n', 'Tags': '<numerical-analysis><floating-point><rounding>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-14T23:02:56.757', 'CommentCount': '3', 'AcceptedAnswerId': '3186', 'CreationDate': '2012-08-14T18:45:13.943', 'Id': '3185'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I like to implement an arc-length Parameterization of a cubic bezier curve. So far I have implemented the method of calculating the arc length of the curve and now I\'m stuck at calculating the times to divide the original curve into equal arc length segments.</p>\n\n<p>What i have:</p>\n\n<ul>\n<li>$m = 5$ The number of segments to create</li>\n<li>$i = 0,1, ...., m$ </li>\n<li>$s = \\text{arc length}$</li>\n<li>$l = s / m$</li>\n<li>$t_0,t_1,...,t_n$ The parameter values of the original bezier curve.</li>\n</ul>\n\n<p>The formula I have:\n$$\\int_{t_0}^{\\tilde{t_i}} ds/dt = i * \\tilde{l}$$</p>\n\n<p>To calculate the value of $\\tilde{t_i}$ I would have to go through 2 steps:</p>\n\n<ol>\n<li>Calculate a spline segment indexed by $j$ which satisfies $\\sum_{p=0}^{j-1} l_p \\le i * \\tilde{l} &lt; \\sum_{p=0}^{j}l_p$</li>\n<li>Compute $\\tilde{t_i}$ such that $\\int_{t_j}^{\\tilde{t_i}}ds/dt * dt = i  * \\tilde{l} - \\sum_{p=0}^{j-1}l_p$</li>\n</ol>\n\n<p>To my questions:</p>\n\n<ol>\n<li><p>What is $\\tilde{l}$? I know it is an approximated value but is it neccesery to be different or could just use $l$?</p></li>\n<li><p>The first calculation of $j$ makes sense, but how would I solve the second to $\\tilde{t_i}$?</p></li>\n</ol>\n\n<p>Maybe I just don\'t understand correctly what the integral $\\int_a^bds/dt*dt$ is or how I can calculate it programmatically.</p>\n\n<p>I am following <a href="http://homepage.cs.uiowa.edu/~hank/publications/images/ArcLength03.pdf" rel="nofollow">this paper</a>.</p>\n', 'ViewCount': '282', 'Title': 'Arc-Length parameterization of a cubic bezier curve', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T20:52:56.190', 'LastEditDate': '2012-10-11T20:52:56.190', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '3156', 'Tags': '<algorithms><computational-geometry><numerical-analysis>', 'CreationDate': '2012-10-11T15:33:30.873', 'Id': '5021'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have rank-deficient matrix $M \\in \\mathbb{R}^{n\\times m}$ with $\\text{rank}(M) = k$ and I want to find a <a href="http://en.wikipedia.org/wiki/Rank_factorization" rel="nofollow">rank factorization</a> $M = PQ$ with $P \\in \\mathbb{R}^{n \\times k}$ and $Q \\in \\mathbb{R}^{k \\times m}$. </p>\n\n<p>A popular approach is to compute the singular value decomposition (SVD) $M = UDV^*$ and keep the columns of $U$ and rows of $V$ corresponding to the non-zero singular values. This is a great approach, especially since it behaves nicely under noise. However, SVD seems to compute more than I need for just rank factorization (and the noise tolerance is cool, but not necessary). </p>\n\n<p><strong>What are the other approaches I can use?</strong> In particular, I am interested in algorithms that have <em>one</em> (or more) of the following properties:</p>\n\n<ol>\n<li>Outperform SVD asymptotically.</li>\n<li>Outperform SVD in practice, or on special inputs (for a reasonably interesting class of special inputs).</li>\n<li>Performance under small perturbation of $M$ is well understood.</li>\n</ol>\n\n<p>I am fine with giving $k$ to the algorithm ahead of time. Note that SVD does not require this (unless we are doing a perturbation analysis, but even then we usually give a bound on perturbation size and determine $k$ at run-time based on that).</p>\n', 'ViewCount': '202', 'Title': 'Alternatives to SVD for rank factorization', 'LastEditorUserId': '55', 'LastActivityDate': '2013-01-06T21:42:05.483', 'LastEditDate': '2013-01-06T21:42:05.483', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><machine-learning><numerical-analysis><linear-algebra>', 'CreationDate': '2012-11-05T20:45:27.737', 'Id': '6497'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In a 32-bit floating number with normalized mantissa and excess-64 exponent base 16, the number $16^{-65}$ denotes</p>\n\n<ol>\n<li><p>Floating point overflow.</p></li>\n<li><p>Negative floating point overflow.</p></li>\n<li><p>All 0\'s in the exponent and mantissa fields.</p></li>\n<li><p>The minimum representable positive number .</p></li>\n</ol>\n\n<p>I think that minimum representable number should be $1 \\times 16^{-63}$\nbecause the minimum mantissa should be 1 and and the possible exponent range in bias form is from 1 to 127 (where 1 corresponds to most negative exponent i.e. -63, and 127 corresponds to most positive exponent i.e. 63)</p>\n\n<p>So according to me, the answer is: A positive floating point underflow.\nPlease correct me if i am wrong. The IEEE-754 representation is confusing me. </p>\n\n<p>Someone also told me something along the lines of " the mantissa part is always taken as 0.M if the base is something other than 2". However I don\'t have any reference for this statement.</p>\n', 'ViewCount': '272', 'Title': 'In a 32-bit floating number with normalized mantissa and excess-64 exponent base 16, the number $16^{-65}$ denotes', 'LastEditorUserId': '4763', 'LastActivityDate': '2013-01-13T00:37:46.217', 'LastEditDate': '2012-12-13T18:03:21.537', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4763', 'Tags': '<numerical-analysis><floating-point><rounding>', 'CreationDate': '2012-12-13T16:19:41.657', 'Id': '7382'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When doing mental calculus one can do:</p>\n\n<ul>\n<li>Given an integer k, sum all the digits (in base 10),\nand if the result is a multiple of 3, then k is a multiple of 3.</li>\n</ul>\n\n<p>Do you know of any algorithm working similarily but operating on binary numbers digits (bits)? </p>\n\n<ul>\n<li><p>At first, I was thinking of using the ready made functions of my\nlanguage converting integer to ascii to perform the convertion from\nbase 2 to base 10, then apply the mental calculus trick. But of\ncourse then I could also encode the base convertion 2 to 10 myself.\nI have not done it yet, but I'll give it a try.</p></li>\n<li><p>Then I have thought of euclidian division in base 2...</p></li>\n</ul>\n\n<p>However I wonder if there are other means, algorithms.</p>\n", 'ViewCount': '344', 'Title': 'Algorithms computing if a number is a multiple of 3', 'LastActivityDate': '2013-01-11T21:54:05.297', 'AnswerCount': '4', 'CommentCount': '0', 'AcceptedAnswerId': '7880', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2100', 'Tags': '<algorithms><numerical-analysis>', 'CreationDate': '2013-01-11T01:52:36.497', 'Id': '7879'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have been posed with a question whereby a computer truncates numbers to x number of digits. Due to this, if this computer is trying to store a decimal number which has a binary equivalent greater than x, it truncates the remaining digits producing a different binary number. However, this binary number is still an 'approximation' of what it should be, but of course is the equivalent of a different decimal number (close to what we were trying to store initially). </p>\n\n<p>What problems can occur due to this incorrect storage of data?</p>\n", 'ViewCount': '111', 'Title': 'Implications of truncation of numbers when converted into binary', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-28T00:18:41.680', 'LastEditDate': '2013-01-28T00:18:41.680', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6571', 'Tags': '<approximation><numerical-analysis><floating-point>', 'CreationDate': '2013-01-26T19:54:34.997', 'Id': '9177'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '251', 'Title': 'Computing inverse matrix when an element changes', 'LastEditDate': '2013-02-18T03:19:37.600', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '867', 'FavoriteCount': '1', 'Body': "<p>Given an $n \\times n$ matrix $\\mathbf{A}$. Let the inverse matrix of $\\mathbf{A}$ be $\\mathbf{A}^{-1}$ (that is, $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$). Assume that one element in $\\mathbf{A}$ is changed (let's say $a _{ij}$ to $a' _{ij}$). The objective is to find $\\mathbf{A}^{-1}$ after this change. Is there a method to find this objective that is more efficient than re-calculating the inverse matrix from scratch. </p>\n", 'Tags': '<algorithms><numerical-analysis><online-algorithms>', 'LastEditorUserId': '683', 'LastActivityDate': '2013-02-18T14:24:30.640', 'CommentCount': '3', 'AcceptedAnswerId': '9881', 'CreationDate': '2013-02-18T01:04:51.930', 'Id': '9875'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem that can be viewed in two different ways:</p>\n\n<ol>\n<li><p>Compute an $n$-dimensional integral, numerical context. The domain of integration is an $n$-dimensional hyper-cube of side $L$.</p></li>\n<li><p>Count (just count) the roots of an $n$-dimensional function (not a polynomial).</p></li>\n</ol>\n\n<p>Solving just one of them is sufficient for solving the original problem.\nI know that simple algorithms for numerical integration would take $O(L^n)$, taking linear time per dimension. But I am not sure if there an asymptotically faster algorithms for (1). </p>\n\n<p>For (2), I am aware of algorithms that can find roots (Newton and Bisection), but I am not sure about the best algorithms just for counting how many roots are in a non-polynomial $n$-dimensional function.</p>\n\n<p>What are the best algorithms for (2)? Are they better than the fastest of (1)?</p>\n', 'ViewCount': '79', 'Title': 'numerical integral vs counting roots', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-14T19:29:09.583', 'LastEditDate': '2013-04-08T21:53:00.623', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2588', 'Tags': '<algorithms><numerical-analysis><counting>', 'CreationDate': '2013-04-08T21:02:35.247', 'Id': '11147'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>(This question might be legitimately crossposted to stackoverflow or mathoverflow or programming StackExchanges.)</p>\n\n<h1>Preface</h1>\n\n<p>I\'m reading this paper on solving linear systems of equations <code>Ax = b</code> using an explicit inverse (held by conventional wisdom as a big no-no): Druinsky &amp; Toledo, "How accurate is inv(A)*b?", <a href="http://arxiv.org/abs/1201.6035" rel="nofollow">http://arxiv.org/abs/1201.6035</a></p>\n\n<p>It asserts that in non-pathological cases, calculating a solution to <code>Ax=b</code> via <code>inv(A)*b</code> using the explicit inverse of a matrix produces solutions that are as accurate and as stable as preferred methods such as via LU factorization. It goes on to describe exactly which situations using "inv" might be <em>bad</em> (technically, when the right-hand side, <code>b</code>, is nearly orthogonal to the subspace spanned by left-singular vectors of <code>A</code> with low singular values). Even in these cases, the paper asserts, the solutions of <code>inv(A)*b</code> are <em>as accurate</em> as preferred methods, they\'re just not backwards stable.</p>\n\n<h1>Question</h1>\n\n<p>My question is: what are the specific drawbacks of using an algorithm that isn\'t guaranteed to be backwards stable? If all I wanted was some solutions to <code>Ax=b</code>, and I was guaranteed accurate results, does it matter that the solutions aren\'t backwards stable? Is there any specific examples of when it\'s ok to use an accurate but backwards-unstable algorithm? </p>\n\n<h1>Furthermore</h1>\n\n<p>My experimentation in Matlab/Octave shows me that the difference between a backwards-unstable and -stable algorithm is that slight perturbations to the solution result in bigger errors when going back through the linear system:</p>\n\n<pre><code>norm(A * (inv(A) * b) - b)\n</code></pre>\n\n<p>is much larger than</p>\n\n<pre><code>norm(A * (A \\ b) - b)\n</code></pre>\n\n<p>for a "bad" <code>b</code> (nearly orthogonal to the subspace of left-singular vectors with low singular values), where <code>\\</code> solves linear systems using Gaussian elimination (<a href="http://www.mathworks.com/help/matlab/ref/mldivide.html" rel="nofollow">http://www.mathworks.com/help/matlab/ref/mldivide.html</a>). The solutions themselves have the same accuracy, i.e., <code>norm(A\\b - true_x)</code> is about the same as <code>norm(inv(A)*b - true_x)</code>.</p>\n\n<p>I can imagine that in the case that </p>\n\n<ol>\n<li>I <em>only</em> cared about getting a solution to <code>Ax=b</code> and </li>\n<li>knew I would <em>never</em> propagate the solution back through <code>A</code>, </li>\n</ol>\n\n<p>could I justify using <code>inv(A)</code> when I didn\'t want to bother checking that <code>b</code> isn\'t "bad", i.e., without ensuring that the solution was backwards-stable. To me, this doesn\'t seem worth it, even without the speed and convenience advantages of using LU or Cholesky or QR decompositions. </p>\n', 'ViewCount': '63', 'Title': 'What are the drawbacks of using an algorithm that is not backwards stable?', 'LastEditorUserId': '8216', 'LastActivityDate': '2013-06-19T03:09:26.330', 'LastEditDate': '2013-06-10T13:40:48.020', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8216', 'Tags': '<linear-algebra><numerical-analysis>', 'CreationDate': '2013-06-10T13:07:49.347', 'FavoriteCount': '1', 'Id': '12597'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to understand how <a href="https://en.wikipedia.org/wiki/Auto_derivatives" rel="nofollow">Automatic Differentiation</a> (AD) works.<br>\nFor simple algebraic operation, I get the chain rule thing.\nBut, when the code contains conditional statement like</p>\n\n<pre><code>1: test_sign = x*y &lt; 0\n2: if test_sign :\n3:     biggest = max(x,y)\n4: else\n5:     smallest = min(x,y) \n</code></pre>\n\n<p>Does AD work?<br>\nIf it works, can you explain why?  </p>\n\n<p>When the code executes line 1, how does AD interpret the inequality <code>&lt;</code>?<br>\nLet\'s say, my AD is in foward mode, what is the differential for the <code>if then else</code> branch?  </p>\n\n<p>My understanding is that the above code use non differentiable function, in the sense that left and right derivatives are not the same. So, how does AD pick the good derivative?</p>\n\n<p>Also, if I\'m in forward mode, it is possible that line 5 is never visited. But when AD bumps <code>x</code> to <code>x+h</code>, line 5 should be evaluated. So the AD tape will incorrectly always differentiate line 3, instead of differentiation line 5.</p>\n', 'ViewCount': '62', 'Title': 'Does Automatic Differentiation handle conditional branches, if yes how?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T09:39:52.550', 'LastEditDate': '2013-09-09T09:39:52.550', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10018', 'Tags': '<numerical-analysis><computer-algebra>', 'CreationDate': '2013-09-07T06:26:33.620', 'Id': '14183'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have this problem:</p>\n\n<blockquote>\n  <p>Consider the problem of calculating the integral</p>\n  \n  <p>$$y_n =\\int_{0}^{1} \\dfrac{x^n}{x+10} \\mathrm{d}x $$\n  for a positive integer $n$.</p>\n  \n  <p>Observe that $$y_n + 10y_{n-1} = \\int_{0}^{1} \\dfrac{x^n +10x^{n-1}}{x+10} \\mathrm{d}x  = \\int_{0}^{1} x^{n-1}\\mathrm{d}x = \\dfrac{1}{n}$$</p>\n  \n  <p>and that using this relationship in a forward recursion leads to a numerically unstable procedure.</p>\n  \n  <ol>\n  <li><p>Derive a formula for approximately computing these integrals based on evaluating $y_{n-1}$ given $y_n$.</p></li>\n  <li><p>Show that for any given value $\\epsilon &gt; 0$ and positive integer $n_0$, there exists an integer $n_1 \\geq n_0$ such that taking $y_{n_1} = 0$ as a starting value will produce integral evaluations $y_n$ with an absolute error smaller than $\\epsilon$ for all  $0 &lt; n \\leqslant n_0$. </p></li>\n  <li><p>Explain why your algorithm is stable.</p></li>\n  </ol>\n</blockquote>\n\n<p>Here is what I have so far,</p>\n\n<p>for part 1.</p>\n\n<p>$$y_{n-1} = \\dfrac{1}{10} \\left(\\dfrac{1}{n} - y_n\\right)$$</p>\n\n<p>and for part 3.</p>\n\n<p>The algorithm is stable because the magnitude of roundoff errors gets divided by 10 each time the recursion is applied.</p>\n\n<p>I really don't know how to start on the proof for part 2., any hints and help would be appreciated.</p>\n", 'ViewCount': '59', 'Title': 'Error accumulation in a numerical integration', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:12:24.380', 'LastEditDate': '2013-09-18T09:12:24.380', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10189', 'Tags': '<algorithms><numerical-analysis><error-estimation><numerical-algorithms>', 'CreationDate': '2013-09-18T00:43:25.283', 'Id': '14394'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My question is about numerical methods for inverting integral transforms;</p>\n\n<p>I'm trying to numerically invert the following integral transform:</p>\n\n<p>$$F(y) = \\int_{0}^{\\infty} y\\exp{\\left[-\\frac{1}{2}(y^2 + x^2)\\right]} I_0\\left(xy\\right)f(x)\\;\\mathrm{d}x$$</p>\n\n<p>So for a given $F(y)$ I need to approximate $f(x)$\nwhere:</p>\n\n<ul>\n<li><strong>$f(x)$ and $F(y)$ are real and positive</strong> (they are continuous probability distributions)</li>\n<li><strong>$x,y$ are real and positive</strong> (they are magnitudes)</li>\n</ul>\n\n<p>I have a very messy and brute force method for doing this at the minute: </p>\n\n<p>I define $f(x)$ and the spline over a series of points, the values of the splined points are 'guessed' by random sampling, which yields a predicted $F(y)$. A basic genetic algorithm I wrote up minimises the difference between the predicted and measured $F(y)$ array. I then take the $f(x)$ which the algorithm converges to as my answer for the inversion.</p>\n\n<p>This approach works fairly well for some simple cases, but it feels messy to me and not particularly robust.</p>\n\n<p><strong>Can anyone give me guidance on better ways of solving this problem?</strong></p>\n\n<p>Thanks for your time &amp; help!</p>\n", 'ViewCount': '40', 'Title': 'Numerically approximating an inverse integral transform?', 'LastActivityDate': '2013-11-27T05:40:37.663', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user11649', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><numerical-analysis><numerical-algorithms>', 'CreationDate': '2013-11-27T05:40:37.663', 'Id': '18406'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that I want to optimize a unimodal function defined on some real interval.  I can use the well-known algorithm as described in Wikipedia under the name of <a href="http://en.wikipedia.org/wiki/Ternary_search" rel="nofollow">ternary search</a>.</p>\n\n<p>In case of the algorithm that repeatedly halving intervals, it is common to reserve the term <em>binary search</em> for discrete problems and to use the term <em>bisection method</em> otherwise.  Extrapolating this convention, I suspect that the term <em>trisection method</em> might apply to the algorithm that solves my problem.</p>\n\n<p>My question is whether it is common among academics, and is safe to use in, e.g., senior theses, to apply the term <em>ternary search</em> even if the algorithm is applied to a continuous problem.  I need a reputable source for this.  I\'m also interested whether the term <em>trisection method</em> actually exists.</p>\n', 'ViewCount': '96', 'Title': 'Is "ternary search" an appropriate term for the algorithm that optimizes a unimodal function on a real interval?', 'LastEditorUserId': '12861', 'LastActivityDate': '2014-04-20T03:07:25.000', 'LastEditDate': '2014-01-16T03:45:59.587', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12861', 'Tags': '<algorithms><terminology><numerical-analysis><numerical-algorithms>', 'CreationDate': '2014-01-15T02:37:13.467', 'Id': '19734'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have this programming problem, but I really cant figure out what it wants me to do. Heres what it is:</p>\n\n<blockquote>\n  <p>The cube root of a number can be found based on the observation that, if $t$ is an approximation of the cube root of $a$, then $\\tfrac13\\left(\\tfrac{a}{t^2} + 2t\\right)$ is a better approximation. </p>\n  \n  <p>Create a method <code>double betterCubeRoot(double a, double t)</code> that will find the cube root of $a$ accurate enough so that the difference between $t^3$ and $a$ is less than 0.0001. Use recursion.</p>\n  \n  <p>Then write the method <code>double cubeRoot(double a)</code> that makes use of the method with 1 as the initial value of $t$. Write a program that will test values both positive and negative.</p>\n</blockquote>\n\n<p>Can someone please explain what this means. I dont understand how I am supposed to find the cube root of 'a' using the given equation. Thanks.</p>\n\n<p>Here is what i have done:</p>\n\n<pre><code>public static double betterCubeRoot (double a, double t)\n{\n    double tCubed = Math.pow (t, 3);\n    double dif = Math.abs (tCubed - a);\n\n    double eq = ((a / (t * t)) + 2 * t) / 3;\n\n    if (dif &lt; 0.001)\n    {\n        return eq;\n    }\n    else\n    {            \n        return betterCubeRoot (eq, t) ;\n    }\n}\n</code></pre>\n", 'ViewCount': '37', 'Title': 'Can someone interpret what this is asking for', 'LastEditorUserId': '15182', 'LastActivityDate': '2014-03-02T02:44:26.557', 'LastEditDate': '2014-03-02T02:44:26.557', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15182', 'Tags': '<approximation><numerical-analysis>', 'CreationDate': '2014-03-02T01:24:38.247', 'Id': '22177'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am working on an algorithm that requires me to interpolate a couple trillion positive discrete points with f(x) having low finite value (for example 0 - 5). It there a specialized algorithm specific for this reduced problem that can provide 0 error interpolation?\nCould you maybe point me to a few papers?</p>\n\n<p>EDIT: I noticed that i can make the trillions of points become less, but then the max value of f(x) rises exponentially (if i cut them by half, the max value gets squared). If i can get for example 4 points and interpolate with a 3rd degree function, is it guaranteed to pass through all points?</p>\n', 'ViewCount': '31', 'Title': '0 error interpolation for discrete finite value points', 'LastEditorUserId': '16647', 'LastActivityDate': '2014-04-10T15:24:00.573', 'LastEditDate': '2014-04-10T15:24:00.573', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16647', 'Tags': '<numerical-analysis><numerical-algorithms>', 'CreationDate': '2014-04-10T14:49:27.107', 'Id': '23642'}}