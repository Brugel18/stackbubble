268_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Is there any book or tutorial that teaches us how to efficiently apply the common algorithms (sorting, searching, etc.) on large data (i.e. data that cannot be fully loaded into main memory) and how to efficiently apply those algorithms considering the cost of block transfer from external memory ? For example, almost all algorithm textbooks say that B and B+-trees can be used to store data on disk. However, actually how this can be done, especially handling the pointers where the data is present on disk is not explained. Similarly, though many books teach searching techniques, they do not consider data present in secondary memory. </p>\n\n<p>I have checked Knuth's book. Although it discusses these ideas, I still did not understand how to actually apply them in a high-level language. Is there any reference that discusses these details?</p>\n", 'ViewCount': '261', 'Title': 'Applying algorithms on large data', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-18T21:47:06.810', 'LastEditDate': '2012-10-18T21:21:53.663', 'AnswerCount': '3', 'CommentCount': '10', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2596', 'Tags': '<algorithms><efficiency><memory-management><big-data>', 'CreationDate': '2012-08-22T11:10:59.273', 'Id': '3287'},268_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to create a bloom filter of 208 million URLs. What would be a good choice of bit vector size and number of hash functions? I tried a bit vector of size 1 GB and 4 hash functions, but it resulted in too many false positives while reading.</p>\n\n<p>I have a huge web corpus containing web content of billions of URLs. I need to process the web content of URLs satisfying certain criteria: the URL should have appeared in web search results in the past 7 days at least 5 times. This is represented by a list of 208 million URLs. Joining the list directly with the web corpus is not feasible because of volume. So I am considering creation of a bloom filter out of the list and then using the bloom filter to prune out unnecessary URLs from the web corpus.</p>\n', 'ViewCount': '387', 'Title': 'Bloom Filter for 208 million URLs', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:17:46.310', 'LastEditDate': '2013-06-26T13:17:46.310', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2881', 'Tags': '<data-structures><probabilistic-algorithms><searching><big-data><bloom-filters>', 'CreationDate': '2012-09-19T15:58:11.313', 'Id': '4616'},268_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>What are the types of things that need to be considered if I need to sort a large random array of 0s and 1s? </p>\n\n<p>You can assume large array is in the order of million or billions.  </p>\n\n<p>I understand there are tons of sorting algorithms out there (quick, merge, radix,.etc.) and there are so many different data structures out there (trees, skip lists, linked lists, etc.) </p>\n\n<p>If somebody asks me to sort this large array, do I simply jump to Quick Sort and say that's the best solution? If not, what am I supposed to be thinking about? </p>\n\n<p>I'm not even sure if I know the right answer to this question, but I would really appreciate it if somebody in the community can give some advise. </p>\n\n<p>Thanks.</p>\n", 'ViewCount': '673', 'Title': 'If I have a large random array of 0s and 1s that I want to sort what kind of an algorithm and data structures should I consider?', 'LastActivityDate': '2012-12-09T19:51:24.157', 'AnswerCount': '4', 'CommentCount': '4', 'AcceptedAnswerId': '7264', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1480', 'Tags': '<sorting><big-data>', 'CreationDate': '2012-12-09T02:44:17.477', 'Id': '7262'},268_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There are a number of collections of network (or graph) data sets freely available on the web, e.g.</p>\n\n<ul>\n<li><a href="http://snap.stanford.edu/data/index.html" rel="nofollow">http://snap.stanford.edu/data/index.html</a></li>\n<li><a href="http://www.cc.gatech.edu/dimacs10/downloads.shtml" rel="nofollow">http://www.cc.gatech.edu/dimacs10/downloads.shtml</a></li>\n</ul>\n\n<p>I am looking for dynamic network data sets, i.e. networks whose structure varies over time. They seem to be quite rare: The SNAP collection contains only a few "temporal" graphs.</p>\n\n<p>Do you known any other possible sources?</p>\n', 'ViewCount': '56', 'Title': 'Looking for dynamic network data sets', 'LastActivityDate': '2013-07-31T15:18:51.787', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9450', 'Tags': '<graphs><data-sets><social-networks><big-data>', 'CreationDate': '2013-07-31T15:18:51.787', 'Id': '13542'}