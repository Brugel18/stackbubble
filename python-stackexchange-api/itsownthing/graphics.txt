{'Body': '<p>When placing geometric objects in a quadtree (or octree), you can place objects that are larger than a single node in a few ways:</p>\n\n<ol>\n<li>Placing the object\'s reference in every leaf for which it is contained</li>\n<li>Placing the object\'s reference in the deepest node for which it is fully contained</li>\n<li>Both #1 and #2</li>\n</ol>\n\n<p>For example:</p>\n\n<p><img src="http://i.stack.imgur.com/Z2Bj7.jpg" alt="enter image description here"></p>\n\n<p>In this image, you could either place the circle in all four of the leaf nodes (method #1) or in just the root node (method #2) or both (method #3).</p>\n\n<p>For the purposes of querying the quadtree, which method is more commonplace and why?</p>\n', 'ViewCount': '125', 'Title': 'Which method is preferred for storing large geometric objects in a quadtree?', 'LastEditorUserId': '11', 'LastActivityDate': '2012-03-06T20:22:05.510', 'LastEditDate': '2012-03-06T19:47:07.427', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '11', 'Tags': '<graphics><data-structures><computational-geometry>', 'CreationDate': '2012-03-06T19:34:22.793', 'Id': '7'}{'ViewCount': '443', 'Title': 'Ray Tracing versus object-based rendering?', 'LastEditDate': '2012-03-18T17:17:42.680', 'AnswerCount': '3', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '45', 'FavoriteCount': '2', 'Body': '<p>Intro graphics courses usually have a project that asks you to build a ray tracer to render a scene. Many graphics students entering grad school say that they want to work on ray tracing. And yet it seems that ray tracing is a dead field in venues like SIGGRAPH etc. </p>\n\n<p>Is ray tracing really the <em>best</em> way to render a scene accurately with all desired illumination etc, and is it just the slow (read non-interactive) performance of ray tracers that makes them uninteresting, or is there something else ?</p>\n', 'Tags': '<graphics>', 'LastEditorUserId': '45', 'LastActivityDate': '2012-10-23T13:02:03.447', 'CommentCount': '5', 'AcceptedAnswerId': '490', 'CreationDate': '2012-03-17T05:45:39.950', 'Id': '450'}{'Body': "<p>In a Whitted ray tracer, each ray-object intersection spawns a transmitted ray (if the object was translucent), a reflected ray and a shadow ray. The shadow ray contributes the direct lighting component.</p>\n\n<p>But what happens if the shadow ray intersects a transparent object? Is the direct lighting component ignored? How will diffuse objects submerged in water be lit if they don't get any direct light contributions from the shadow ray?</p>\n", 'ViewCount': '173', 'Title': 'Is the shadow ray in a Whitted ray tracer occluded by transparent objects?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-12T08:31:00.987', 'LastEditDate': '2012-03-28T21:54:54.857', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2703', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '799', 'Tags': '<graphics>', 'CreationDate': '2012-03-28T11:20:25.353', 'Id': '850'}{'Body': '<p>For fun I am trying to make a wire-frame viewer for the <a href="http://0x10c.com/doc/dcpu-16.txt" rel="nofollow">DCPU-16</a>. I understand how do do everything except how to hide the lines that are hidden in the wire frame. All of the questions here on SO all assume you have access to OpenGL, unfortunately I do not have access to anything like that for the DCPU-16 (or any kind of hardware acceleration).</p>\n\n<p>I found a fairly good description of Appel\'s algorithm on <a href="http://books.google.com/books?id=aVQnUfL3yEwC&amp;lpg=PA251&amp;ots=zCOEvuKqve&amp;dq=Arthur%20Appel%27s%20algorithm.&amp;pg=PA252#v=onepage&amp;q&amp;f=true" rel="nofollow">Google Books</a>. However there is one issue I am having trouble figuring out.</p>\n\n<blockquote>\n  <p>Appel defined contour line as an edge shared by a front-facing and a\n  back-facing polygon, or unshared edge of a front facing polygon that\n  is not part of a closed polyhedron. An edge shared by two front-facing\n  polygons causes no change in visibility and therefore is not a contour\n  line. In Fig. 8.4, edges AB, EF, PC, GK and CH are contour lines,\n  whereas edges ED, DC and GI are not.</p>\n</blockquote>\n\n<p><img src="http://i.stack.imgur.com/Gajc7.png" alt="Fig. 8.4"></p>\n\n<p>I understand the rules of the algorithm and how it works once you have your contour lines, however I do not understand is what I need to do to determine if a edge is "<em>shared by a front-facing and a back-facing polygon, or unshared edge of a front facing polygon that is not part of a closed polyhedron</em>" from a coding point of view. I can look at a shape and I can know what lines are contour lines in my head but I don\'t have a clue on how to transfer that "understanding" in to a coded algorithm.</p>\n\n<hr>\n\n<h2>Update</h2>\n\n<p>I have made some progress in determining contour lines. I found <a href="http://www.eng.buffalo.edu/courses/mae573/handouts/lecture13.pdf" rel="nofollow">these</a> <a href="http://www.eng.buffalo.edu/courses/mae573/handouts/appel.pdf" rel="nofollow">two</a> lecture notes from a University of Buffalo class on computer graphics.</p>\n\n<p><img src="http://i.stack.imgur.com/xoe49.png" alt="enter image description here"></p>\n\n<blockquote>\n  <p>Consider the edges. These fall into three categories. </p>\n  \n  <ol>\n  <li>An edge joining two invisible faces is itself invisible. This will be deleted from the list and ignored. </li>\n  <li>An edge joining two potentially-visible faces is called a \'material edge\' and will require further processing. </li>\n  <li>An edge joining a potentially-visible face and an invisible face is a special case of a \'material edge\' and is also called a \'contour\n  edge\'.</li>\n  </ol>\n</blockquote>\n\n<p>Using the above two pieces of information I am able to get closer to being able to write this out as code, but I still have a long way to go.</p>\n', 'ViewCount': '299', 'Title': "How to find contour lines for Appel's Hidden Line Removal Algorithm", 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-03T16:18:12.797', 'LastEditDate': '2012-08-12T20:25:09.697', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '9', 'OwnerDisplayName': 'Scott Chamberlain', 'PostTypeId': '1', 'OwnerUserId': '2710', 'Tags': '<algorithms><computational-geometry><graphics>', 'CreationDate': '2012-04-22T21:16:34.033', 'Id': '3139'}{'Body': '<p>I have read many documents about <a href="https://en.wikipedia.org/wiki/Convolution" rel="nofollow"><em>convolution</em></a> in image processing, and most of them say about its formula, some additional parameters. No one explains the intuition and real meaning behind doing convolution on an image. For example, intuition of derivation on the graph is make it more linear for example.</p>\n\n<p>I think a quick summary of the definition is: convolution is multiplied overlap square between image and kernel, after that sum again and put it into anchor. And this doesn\'t make any sense with me.</p>\n\n<p>According to <a href="http://www.aishack.in/2010/08/image-convolution-examples/" rel="nofollow">this article about convolution</a> I cannot imagine why convolution can do some "unbelievable" things. For example, line and edge detection on the last page of this link. Just choose appropriate convolution kernel can make a nice effects (detect line or detect edge). </p>\n\n<p>Can anyone provide some intuition (doesn\'t need to have to be a neat proof) on how it can do that?</p>\n', 'ViewCount': '241', 'Title': 'Intuition for convolution in image processing', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-13T20:38:44.307', 'LastEditDate': '2012-08-16T09:55:11.913', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2538', 'Tags': '<terminology><intuition><graphics><computer-vision>', 'CreationDate': '2012-08-16T08:41:24.633', 'Id': '3215'}{'Body': '<p>What is a good textbook for a seminar about </p>\n\n<ul>\n<li>virtual environments,  virtual worlds,  augmented reality  and\nsimilar topics?</li>\n</ul>\n\n<p>The seminar is at the graduate studies level and is supposed to give introduction to virtual environments, technologies and applications as well as  current research directions.</p>\n\n<p>A creative commons license and availability online would be a plus.</p>\n\n<p>Here are my own results after some search:</p>\n\n<ul>\n<li><a href="http://www.intechopen.com/books/augmented-reality" rel="nofollow">Augmented Reality</a> - open access book at Intechopen</li>\n</ul>\n\n<p>I was suggested these books (no online access) at another source (ResearchGate):</p>\n\n<ul>\n<li>"Virtual Reality Technology" (Grigore C. Burdea, Philippe Coiffet)</li>\n<li>"Understanding Virtual Reality: Interface, Application, and Design" (William R. Sherman, Alan B. Craig) </li>\n<li>"Developing Virtual Reality Applications:Foundations of Effective Design" (Alan Craig , William R. Sherman)</li>\n<li>"Stepping into Virtual Reality " (Mario Gutierrez, F. Vexo, Daniel Thalmann)</li>\n</ul>\n\n<p>In your answer please comment, why do you suggest a particular source and why it is fundamental or current in the field.</p>\n', 'ViewCount': '39', 'Title': 'A text for virtual environments seminar', 'LastEditorUserId': '3019', 'LastActivityDate': '2012-10-04T07:36:44.627', 'LastEditDate': '2012-10-04T07:36:44.627', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3019', 'Tags': '<reference-request><books><graphics><computer-games><hci>', 'CreationDate': '2012-09-30T20:44:48.760', 'Id': '4821'}{'Body': '<p>When dealing with thresholding in digital image processing, what do the following terms mean?</p>\n\n<ul>\n<li>contrasting object</li>\n<li>contrasting background</li>\n</ul>\n', 'ViewCount': '37', 'Title': 'Contrasting object and background', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T12:18:43.807', 'LastEditDate': '2012-10-11T12:18:43.807', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3153', 'Tags': '<graphics><image-processing>', 'CreationDate': '2012-10-11T09:37:56.967', 'Id': '5017'}{'Body': '<p>The <a href="https://en.wikipedia.org/wiki/Radon_transform" rel="nofollow">Radon transform</a> is used to take 2d projections of an object and create a 3d representation.</p>\n\n<p>It seems like it would be possible to apply such a transform in 3d graphics in games (although possibly too slow to be practical).</p>\n\n<p>For example, a very simple way to display an object is to use a 3d rectangle and texture map each side. This is relatively fast but the 3d detail is limited. When a side is parallel with the visual plane it will represent the detail 100% (so the visual detail would be limited to that of the texture map). Of course it won\'t represent external 3d effects properly, like lighting.</p>\n\n<p>But by using the Radon transform one could gain a true 3d approximation of the object from the six textures/projections used. By increasing the number of textures/projections the approximation is better.</p>\n\n<p>I\'m curious if the idea has potential. Possibly for high-quality 3d models it might pay off in performance and size. Of course 3d models can be optimized to limit their size which also increases speed but visually doesn\'t change much.</p>\n', 'ViewCount': '207', 'Title': 'Radon transform for advanced 3d graphics and games?', 'LastEditorUserId': '4244', 'LastActivityDate': '2013-07-17T02:17:03.387', 'LastEditDate': '2012-10-19T06:48:32.120', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4244', 'Tags': '<algorithms><computational-geometry><efficiency><graphics>', 'CreationDate': '2012-10-18T05:20:19.123', 'FavoriteCount': '1', 'Id': '6147'}{'Body': '<p>Instancing in GPU\'s, from my little knowledge, simply draw the same object at multiple locations. Each "instance" is just a copy of the same model at a different location. Any local effects, such as animation, would effect all instances. The performance improvement comes from gpu being able to deal with virtually all the instance aspects. </p>\n\n<p>One could create several variations of the same object and instance each one. This would be very useful to create variations(say for leaves on a tree) but, while allowing for rather large number of instances(basically limited by the gpu) and variation increases the the cpu\'s involvement by the number of variations(from the previous case).</p>\n\n<p>If we could send the changes to the gpu then we could get back to the first case(or very close) and achieve very high levels of variations with limited cpu involvement.</p>\n\n<p>For example,</p>\n\n<p>Suppose we wanted to populate a jungle with a lot of animals. Each animal object will have many instances(many tigers and birds but all using the same template). If we had one long animation containing all the possible animations of each animal and send it to the gpu we then would simply have to tell the gpu which part of the animation each instance was in(and we could have a default animation to limit cpu-gpu communications).</p>\n\n<p>This would effectively let us control all the instances with just a animation position indicator(and possible a few other parameters such as scale) and coordinate position. The gpu would take care of all the rest.</p>\n\n<p>The problem, of course, is that that each instance would not have any logic associated with it. This is not a big deal though for most computer objects. The cpu generally has to take care of the logic anyways but in this case, instead to tell send all that information to the gpu we can just send it a few bytes for each instance.</p>\n\n<p>For example, suppose in our jungle we have a few cheetahs and on our cpu we are keeping track of which ones are doing what and simply sending which animation we want them to use to the cpu(instead of sending all the vertex data and/or telling it how to transform them).</p>\n\n<p>This seems like it would be much faster and essentially just boils down to moving some of the code from the cpu to the gpu(which as been what is happening all along).</p>\n\n<p>Using such a method is not ideal because of the lack of physical modeling per instance but for far enough away objects were intersecting objects won\'t be seen it won\'t matter.</p>\n\n<p>We could have thousands of birds, for example, all seemingly moving individually but generated from one model with a few animations.... the cpu doing very little but sending which animations to use to the gpu. (and of course deciding what each bird should do)</p>\n\n<p>Anyways, this is a thought but it seems to me the goal of future gpu\'s is to deal with all the visual aspects while the cpu deals with the logic(and the communication between the too is to be minimized).</p>\n\n<p>Is my interpretation of instancing with animation\'s flawed or could it be implemented and be useful to 3d games?</p>\n', 'ViewCount': '46', 'Title': 'Extreme instancing for computer graphics', 'LastActivityDate': '2012-10-19T13:24:39.653', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4244', 'Tags': '<graphics><computer-games>', 'CreationDate': '2012-10-19T13:24:39.653', 'FavoriteCount': '1', 'Id': '6169'}{'Body': "<p>Intro:</p>\n\n<p>I'm working with huge data set that i need to plot in browser, and since there may be up to 1M points my idea was to create different representations for different zoom levels</p>\n\n<p>lets say i have 100k points, i would average two-by-two until i get 50k, then i would repeat that until i get below 500 points (my arbitrary threshold)</p>\n\n<p>so on the most zoomed-out level i would draw all 500 points, or part of it, depending of the chart size, and as i zoom in, i would switch to next zoom level (and stream data if user drags selection l/r), and ultimately if user wants to see fine grain details he can zoom to 0 zoom level and see all the fine details.</p>\n\n<p>I actually created this prototype, and its working quite well, except for one thing: side-effect of this is, as you can imagine, that peaks are lost in those iterations of averaging.</p>\n\n<p>I did some research and find about Douglas-Peucker algorithm, and how it can perserve peaks, i did some tests, and it works quite well, but the problem with that is that if it encounters a series of data (y values) [1,1,1,1,5,6,1,1,1,1,1,1] it will smooth that to something like [1,6,1,1] which doesn't work for me since i need to keep ratio of zoom levels like this</p>\n\n<p>n (length of original data) > n/2 > n/4 > n/8 > .....</p>\n\n<p>I read quite few papers on line smoothing, but all algorithms that i found are accepting distance threshold, that they use for smoothing as a parameter, and none of those can accept number of desired output elements, and also, since their goal is to smooth the line, they will transform sequence like this (y values) [1,1,1,1,1,1,1,1,1,1,1] into [1,1]</p>\n\n<p>So, finally, my question:</p>\n\n<p>Is there an algoritm that:</p>\n\n<ul>\n<li>instead of usual distance threshold accepts the desired number of output elements</li>\n<li>tries to perserve peaks (as Douglas-Peucker does)</li>\n<li>will smooth data uniformly, so even if it gets (y values) [1,1,1,1,1,1] and i say i want 3 outputs, event if it IS in theory correct to smooth as [1,1] i would need to get [1,1,1] instead</li>\n</ul>\n\n<p>Also, please don't be confused by lack of X axis information because it is irrelevant since all data are measured from 1 to n in steps of 1, so there are no N/A values, or blank spots, or values like [1.3,1.4,3]</p>\n\n<p>x is always [1,2,3....n]</p>\n", 'ViewCount': '366', 'Title': 'Line smoothing algorithm that perserve data uniformity', 'LastEditorUserId': '4366', 'LastActivityDate': '2012-10-30T18:22:35.023', 'LastEditDate': '2012-10-26T14:23:52.147', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4366', 'Tags': '<computational-geometry><approximation><graphics>', 'CreationDate': '2012-10-26T04:04:35.900', 'FavoriteCount': '2', 'Id': '6321'}{'Body': '<p>I have two vector points $p_1$ and $p_2$. Each point has a color value $c_1$ and $c_2$.\nNow using <em>linear interpolation</em>, I would like to get the color value at point $p_3$.</p>\n\n<p>Concrete example:</p>\n\n<p>$\\qquad \\displaystyle p_1 = (1, 3) \\text{ with } c_1 = (2, 4, 1)$<br>\n$\\qquad \\displaystyle p_2 = (4, 4) \\text{ with } c_2 = (3, 1, 2)$</p>\n\n<p>What color does $p_3 = (2,3)$ have?</p>\n', 'ViewCount': '384', 'Title': 'Computer graphics: Linear Interpolation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-31T16:32:43.690', 'LastEditDate': '2012-10-29T09:41:52.477', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4386', 'Tags': '<graphics>', 'CreationDate': '2012-10-29T00:01:24.353', 'Id': '6366'}{'Body': '<p>I have a color (say $R=100, G=150, B=130$). How do I compute its intensity?</p>\n\n<p>Do I just sum up all three components? Or are the colors not evenly weighted?</p>\n', 'ViewCount': '401', 'Title': 'How do I compute the luminance of a pixel?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-08T10:12:51.993', 'LastEditDate': '2013-05-08T10:12:51.993', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11882', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<graphics>', 'CreationDate': '2013-05-08T02:25:29.493', 'Id': '11876'}{'Body': '<p>I am self-studying computer graphics (both 2d and 3d) and was wondering if someone could provide me with some references like websites, literature, etc.  </p>\n\n<p>I am a complete beginner to computer graphics so I would like to go bottom-up. I have been searching Google but mostly what I get are lecture notes which are, quite frankly, not-so-well explained. They are intended for students who attended those lectures.  </p>\n\n<p>So, references please for self-study.</p>\n', 'ViewCount': '137', 'Title': 'Computer Graphics reference request', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-04T09:50:16.193', 'LastEditDate': '2013-09-04T09:22:37.280', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '8418', 'Tags': '<reference-request><graphics><computer-vision>', 'CreationDate': '2013-05-28T20:46:56.150', 'Id': '12344'}{'Body': '<p>Consider this small picture of a sunflower, and its histogram:</p>\n\n<p><img src="http://i.stack.imgur.com/Bbh3z.jpg" alt="Picture of a sunflower"> <img src="http://i.stack.imgur.com/yZ9fs.jpg" alt="Histogram for the sunflower picture"></p>\n\n<blockquote>\n  <p>What would the Fourier transform of the first picture look like?  Is there any relationship between the histogram and the Fourier transform?</p>\n</blockquote>\n', 'ViewCount': '204', 'Title': 'What the difference between the Fourier Transform of an image and an image histogram?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-06T21:03:53.853', 'LastEditDate': '2013-06-06T21:03:53.853', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8005', 'Tags': '<image-processing><graphics><fourier-transform>', 'CreationDate': '2013-06-06T18:36:09.453', 'Id': '12497'}{'Body': "<p>It is easy to rotate pixel array if the angle is a multiple of 90 degree. </p>\n\n<p>For example, (quote from <em>Computer Graphics with OpenGL</em>)</p>\n\n<blockquote>\n  <p>We can rotate a two-dimensional object or pattern 90' counterwise by\n  reversing the pixel values in each row of the array, then\n  interchanging rows and columns.</p>\n</blockquote>\n\n<p>But I could not understand how to rotate pixel array when the angle is not a multiple of 90 degree from the textbook.</p>\n\n<blockquote>\n  <p>Each desitination pixel area is mapped onto the rotated array and the\n  amout of overlap with the rotated pixel areas is calculated.</p>\n</blockquote>\n\n<p>Why does this means?</p>\n", 'ViewCount': '122', 'Title': 'How to do pixel array rotation whose angle not a multiple of 90 degree?', 'LastActivityDate': '2013-06-29T22:09:04.323', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8926', 'Tags': '<graphics>', 'CreationDate': '2013-06-29T16:08:51.173', 'Id': '12968'}{'Body': '<p>Consider how one would represent the following image in vector graphics:</p>\n\n<p><img src="http://www.321coloringpages.com/images/heart-coloring-pages-3/heart-coloring-pages-4.png" alt="A heart shape with mirrored spiral tails at the bottom."></p>\n\n<p>Pretty simple, right?  The entire shape can be represented by a single path element.</p>\n\n<p>But suppose additionally that you want to color the heart at the top red.  The path element is an open shape, so trying to fill it results in an appropriately red heart but also implementation-dependent bleeding between the spiral endpoints.</p>\n\n<p>Obviously, one could just draw the heart and the spiral tails as separate elements, but then the vector graphics representation no longer mirrors how a human being would draw the same image, and makes it more difficult to manipulate as a single object.  One needs a way to communicate to the computer that two particular path segments within the larger path intersect in such a manner that they close a sub-shape.</p>\n\n<p>Is there a vector graphics format capable of doing this?  More relevantly, how is it implemented and are there any papers on it?</p>\n', 'ViewCount': '188', 'Title': 'Closing shapes at non-endpoints', 'LastActivityDate': '2013-09-03T18:11:41.823', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '14108', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9947', 'Tags': '<computational-geometry><graphics>', 'CreationDate': '2013-09-02T14:30:47.570', 'Id': '14086'}{'Body': "<p>In a three dimensional Cartesian space, given a ray in its parametric form: \n$r(t) = [1, 1, 1] + t[-1, -1, -1]$ find the intersection points between the ray and the following primitives. </p>\n\n<p><strong>a)</strong> A sphere centered at the origin with a radius of $1$, and</p>\n\n<p><strong>b)</strong> a triangle with vertices $\\{[1,0,0]^T , [0,1,0]^T , [0,0,1]^T \\}$.</p>\n\n<p>Represent the intersections in their Cartesian coordinates as well as the ray parameters. Write down the intermediate steps of your derivations clearly.</p>\n\n<p>The next part asks me to choose another primitive 3D shape, decide on implicit or parametric surface functions, then make an equation for ray-surface intersection and derive the analytic solution. </p>\n\n<p>I am a little confused on this assignment because I haven't had a physics class before and I don't understand the basics of 3D shapes' geometry or vectors. It's not entirely clear to me what I am supposed to find as an answer or in what form the answers should be written. I looked up the formulas for sphere and triangle intersection, but I can't find a similar example, so I don't know how to solve the problems.</p>\n", 'ViewCount': '92', 'Title': 'Sphere, triangle, and other primitive shape intersection', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-19T08:24:50.833', 'LastEditDate': '2013-09-19T08:24:50.833', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10215', 'Tags': '<computational-geometry><graphics>', 'CreationDate': '2013-09-19T02:53:54.783', 'Id': '14430'}{'Body': "<p>I'm looking for an actual definition of a stream processing unit (SPU), preferably sourced. The context is not specifically graphics cards.</p>\n\n<p>I can find many, many articles explaining what they contain, or what they do in specific cases, but I need a definition for an informal class presentation. I have a general sense of what they are, but can't find much literature to back that up. However, even an informal definition would help.</p>\n\n<p>Would really appreciate any input.</p>\n", 'ViewCount': '31', 'Title': 'Stream processor definition', 'LastActivityDate': '2013-10-26T19:26:32.913', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10991', 'Tags': '<parallel-computing><graphics><signal-processing>', 'CreationDate': '2013-10-26T19:26:32.913', 'FavoriteCount': '1', 'Id': '16452'}{'Body': "<p>I'm having trouble understanding the mechanics of the midpoint algorithm. I understand the gist of what it does; it keeps us within a half a pixel of where the actual line should be printed. It does this by updating this $d$ value for each pixel that we traverse. </p>\n\n<p>However, even after drawing some samples and observing what the $d$ value does, I can't figure out the true inner workings of it, and how updating it by these $\\Delta_e$ and $\\Delta_{ne}$ symbols (which are also a bit of a mystery to me) keeps our pixel placement in check.</p>\n\n<p>Can someone summarize it in simple terms for the good of all humanity?</p>\n\n<p><strong>The Algorithm:</strong></p>\n\n<pre><code>Line (x1, y1, x2, y2)\n    begin\n    int x, y, dx, dy, d, deltaE, deltaNE;\n    x &lt;- x1;        y  &lt;- y1;\n    dx &lt;- x2 - x1;  dy &lt;- y2 - y1;\n    d &lt;- 2*dy - dx;\n    deltaE &lt;- 2*dy;     deltaNE &lt;- 2*(dy - dx);\n\n    PlotPixel(x, y);\n    while ( x &lt; x2) do\n        if (d &lt; 0) then\n        begin\n            d &lt;- d + deltaE;\n            x &lt;- x + 1;\n        end;\n        else begin\n            d &lt;- d + deltaNE;\n            x &lt;- x + 1;\n            y &lt;- y + 1;\n        end;\n        PlotPixel (x, y);\n    end;\nend;\n</code></pre>\n", 'ViewCount': '146', 'Title': 'How does the midpoint line drawing algorithm work?', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-20T00:32:33.500', 'LastEditDate': '2013-12-20T00:32:33.500', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><graphics>', 'CreationDate': '2013-11-19T14:34:50.693', 'Id': '18156'}{'Body': '<p>If you resize an 1000x1000 raster image to 10x10 and resize it back to 1000x1000 is this considered to be an example of <a href="https://en.wikipedia.org/wiki/Aliasing" rel="nofollow">aliasing</a>?</p>\n', 'ViewCount': '32', 'ClosedDate': '2014-03-30T02:33:04.923', 'Title': 'Resizing and aliasing in computer science', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-28T12:48:00.597', 'LastEditDate': '2014-03-28T12:48:00.597', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16228', 'Tags': '<terminology><image-processing><graphics>', 'CreationDate': '2014-03-28T08:24:19.157', 'Id': '23167'}{'Body': '<p>I thought the Gibbs phenomenom is the result of Fourier analysis estimation (but was it Fourier Series estimation or can it also be Fourier Transform estimation?)</p>\n\n<p><img src="http://i.stack.imgur.com/A1Vt2.png" alt="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Gibbs_phenomenon_50.svg/285px-Gibbs_phenomenon_50.svg.png"></p>\n\n<p>JPEG uses the Discrete cosines Transform. A DCT is similar to a Fourier transform in the sense that it produces a kind of spatial frequency spectrum.</p>\n\n<p><img src="http://i.stack.imgur.com/H2AuR.jpg" alt="JPEG example JPG RIP 001.jpg  Lowest quality"> </p>\n\n<p>But what are the differences between the Gibbs phenomenom artefacts from Fourier and the artefacts from the Discrete Cosine?</p>\n', 'ViewCount': '32', 'Title': 'What are all the differences between the Gibbs phenomenon artefects and JPEG artefacts?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T02:14:05.370', 'LastEditDate': '2014-03-28T12:46:38.770', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16228', 'Tags': '<image-processing><data-compression><graphics><signal-processing>', 'CreationDate': '2014-03-28T08:56:20.763', 'Id': '23169'}{'Body': '<p>I  am  learning  Canvas  Tag  of  html5  and  i  am  building  an  animation  using  it.Now  Everything  works  fine  for  animating  two  objects  or  lines,but  as  third  object is added the  canvas  tag  is  unable  to  render  the  animation  properly.Here  is  the  broken  Animation  Code :<a href="http://pastebin.com/VWEF4MLR" rel="nofollow">http://pastebin.com/VWEF4MLR</a></p>\n\n<p>If  you  will  comment  any  one  arc  out  of  three  i.e. </p>\n\n<pre><code>//ctx.arc(0+x,0+x,10, 0, Math.PI*2, true);\n</code></pre>\n\n<p>&amp;  then  if  the  example  is  run  it  works  flawlessly.Now, I  want  to  know  that  there  is  bug  in  my way  of  coding  or  any  other  flaw.Any  help  would  be  appreciated.</p>\n', 'ViewCount': '11', 'ClosedDate': '2014-04-25T07:06:38.390', 'Title': 'Why canvas breaks for more than two objects', 'LastActivityDate': '2014-04-25T06:19:19.113', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '17063', 'Tags': '<graphics>', 'CreationDate': '2014-04-25T06:19:19.113', 'Id': '24097'}