{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a system of linear equations $Ax=0$, where $A$ is a $n\\times n$ matrix with rational entries. Assume that the rank of $A$ is $&lt;n$. What is the complexiy to check\nwhether it has a solution $x$ such that all entries of $x$ are stricly greater than 0 (namely, $x$ is a positive vector)? Of course, one can use Gauss elimination, but this seems not to be optimal.</p>\n', 'ViewCount': '573', 'Title': 'Complexity of checking whether linear equations have a positive solution', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-28T10:35:29.087', 'LastEditDate': '2012-04-26T00:21:41.243', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1236', 'Tags': '<algorithms><complexity-theory><linear-algebra>', 'CreationDate': '2012-04-25T12:24:02.827', 'FavoriteCount': '1', 'Id': '1500'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given three matrices $A, B,C \\in \\mathbb{Z}^{n \\times n}$ we want to test whether $AB  \\neq C$. Assume that the arithmetic operations $+$ and $-$ take constant time when applied to numbers from $\\mathbb{Z}$.</p>\n\n<p>How can I state an algorithm with one-sided error that runs in $O(n^2)$ time and prove its correctness?</p>\n\n<p>I tried it now for several hours but I can't get it right. I think I have to use the fact that for any $x \\in \\mathbb{Z}^n$ at most half of the vectors $s \\in S = \\left\\{1, 0\\right\\}^n$  satisfy $x \\cdot s = 0$, where $x \\cdot s$ denotes the scalar product$\\sum_{i=1}^{n} x_is_i$.</p>\n", 'ViewCount': '232', 'Title': 'Probabilistic test of matrix multiplication with one-sided error', 'LastEditorUserId': '41', 'LastActivityDate': '2013-05-24T03:04:37.160', 'LastEditDate': '2012-05-12T20:19:48.560', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1811', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><probabilistic-algorithms><matrices><linear-algebra>', 'CreationDate': '2012-05-12T19:15:56.887', 'Id': '1809'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have rank-deficient matrix $M \\in \\mathbb{R}^{n\\times m}$ with $\\text{rank}(M) = k$ and I want to find a <a href="http://en.wikipedia.org/wiki/Rank_factorization" rel="nofollow">rank factorization</a> $M = PQ$ with $P \\in \\mathbb{R}^{n \\times k}$ and $Q \\in \\mathbb{R}^{k \\times m}$. </p>\n\n<p>A popular approach is to compute the singular value decomposition (SVD) $M = UDV^*$ and keep the columns of $U$ and rows of $V$ corresponding to the non-zero singular values. This is a great approach, especially since it behaves nicely under noise. However, SVD seems to compute more than I need for just rank factorization (and the noise tolerance is cool, but not necessary). </p>\n\n<p><strong>What are the other approaches I can use?</strong> In particular, I am interested in algorithms that have <em>one</em> (or more) of the following properties:</p>\n\n<ol>\n<li>Outperform SVD asymptotically.</li>\n<li>Outperform SVD in practice, or on special inputs (for a reasonably interesting class of special inputs).</li>\n<li>Performance under small perturbation of $M$ is well understood.</li>\n</ol>\n\n<p>I am fine with giving $k$ to the algorithm ahead of time. Note that SVD does not require this (unless we are doing a perturbation analysis, but even then we usually give a bound on perturbation size and determine $k$ at run-time based on that).</p>\n', 'ViewCount': '202', 'Title': 'Alternatives to SVD for rank factorization', 'LastEditorUserId': '55', 'LastActivityDate': '2013-01-06T21:42:05.483', 'LastEditDate': '2013-01-06T21:42:05.483', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><machine-learning><numerical-analysis><linear-algebra>', 'CreationDate': '2012-11-05T20:45:27.737', 'Id': '6497'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p><strong>Topic:</strong><br>\nThe <a href="http://risorse.dei.polimi.it/maxfs/" rel="nofollow">maximum feasible subsystem problem</a>, which is generally NP-hard [1].</p>\n\n<p><strong>Question:</strong><br>\nAre there special algorithms in case of only 2 variables (2D linear constraints)? The problem seems to be a lot less complex in my humble opinion but is this true? Can you guys point me to any algorithms/papers for this limited case with reduced computational complexity?</p>\n\n<p><strong>References:</strong><br>\n[1] E. Amaldi and V. Kann, <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=E5F2A09B65113E8B30F82BEF29FF00FD?doi=10.1.1.43.9283&amp;rep=rep1&amp;type=pdf" rel="nofollow">The complexity and approximability of finding maximum feasible subsystems of linear relations</a> Theoretical Computer Science, vol. 147, pp. 181\u2013210, 1993.</p>\n', 'ViewCount': '43', 'Title': 'Maximum feasible subsystem problem (MaxFS) in 2 variables', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-19T13:41:31.257', 'LastEditDate': '2012-11-19T13:41:31.257', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4653', 'Tags': '<computational-geometry><linear-programming><linear-algebra>', 'CreationDate': '2012-11-19T13:11:51.377', 'Id': '6765'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So, I was trying to learn machine learning, and, after watching a couple of Andrew Ng's lectures decided to try and write a simple piece of code to determine what someone's salary would be based on the number of years they worked at a company by this simple linear function:</p>\n\n<p>salary = w1 * numberOfYears + w0</p>\n\n<p>I then used the least square method to calculate the error between the predicted results and the actual results and tried to minimize the error of the gradient of that function:</p>\n\n<p>Error = (1/2) [ sum of (predicted - actual)^2 for all samples (x, y) ]</p>\n\n<p>The predicted value is the one calculated by the linear equation above, and actual = y.  w1 and w0 are weights to the function that are set to some initial values (0 and 0 for example) and are changed to decrease the error throughout the process of gradient descent.</p>\n\n<p>Steps to the algorithm:</p>\n\n<p>1)  Choose some (random) initial values for the model parameters (w).<br />\n2)  Calculate the gradient G of the error function with respect to each model parameter.<br />\n3)  Change the model parameters so that we move a short distance in the direction of the greatest rate of decrease of the error, i.e., in the direction of -G.<br />\n4)  Repeat steps 2 and 3 until G gets close to zero.</p>\n\n<p>In my implementation I have this code:</p>\n\n<pre><code>for(int i = 0; i &lt; numSamples; i++) {\n    double error = prediction(data.X[i]) - data.Y[i];\n    while(nearsZero(error)) {\n        for(int j = 0; j &lt; w.length; j++) {\n            w[j] -=  0.1 * error * data.X[i][j];\n        }\n        error = data.Y[i] - prediction(data.X[i]);\n    }\n}\n</code></pre>\n\n<p>If I run this with only one sample (meaning ignore the outermost loop), then the weights continuously subtract a value that is continuously nearing infinity (or -infinity).  If I change the w[j] -= ... to a w[j] += ... then the function works for the first value and sets initial w that will solve it for one sample (but that isn't what I was supposed to do as far as I can tell).  Also, when I add multiple samples the numbers just do the same thing on the second sample that they were doing on the first sample when I had -=.</p>\n\n<p>I feel like gradient descent doesn't make sense here because it was demoed on graphs like z=y^2+x^2, which looks like a big bowl with one central min that it will find eventually.</p>\n\n<p>On the graph in my problem the graph looks like a parabola that extends to infinity in both directions and has infinite min values along a line.</p>\n\n<p>I'm not sure what it is that I am doing wrong, but I would assume that it's something to do with my understanding of the problem.  If anyone knows how to get this working I would appreciate it.</p>\n\n<p><strong>EDIT:</strong> Updated data.X[i] in the while loop to data.X[i][j].  Each data point i should have some value Y[i] and an array of X values at X[i].</p>\n", 'ViewCount': '386', 'Title': 'Machine Learning: how to correctly calculate gradient descent for simple linear problem', 'LastEditorUserId': '4737', 'LastActivityDate': '2012-11-28T16:36:50.723', 'LastEditDate': '2012-11-28T16:36:50.723', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4737', 'Tags': '<machine-learning><linear-algebra>', 'CreationDate': '2012-11-27T22:31:50.273', 'Id': '6972'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I had the choice of choosing one out of the following two optimization problems which I could use to solve my problem. Which choice is the fastest? How much of a trade-off would it be?  Is the improvement in speed by many factors!?</p>\n\n<ol>\n<li><p>Minimizing a convex function $L(X)$ in one matrix variable with orthogonality constraints over the matrix-essentially in my case this ends up to solving an eigen-decomposition.</p></li>\n<li><p>Minimizing the same convex function $L(X)$ with linear constraints in $X$.</p></li>\n</ol>\n\n<p>I know that 2.) should be faster. But what is the direction of work I need to do- to compare the improvement in speed-especially in terms of using the fastest available eigen solver for 1.)-what would be the corresponding fastest approach to solve 2.)?</p>\n', 'ViewCount': '61', 'Title': 'Time - Complexity Convex Optimization and Eigen Decomposition', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T00:32:56.540', 'LastEditDate': '2012-12-06T10:17:22.360', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'VSPC', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity><optimization><efficiency><linear-algebra>', 'CreationDate': '2012-10-16T18:17:29.447', 'Id': '7206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have to solve system of linear algebraic equations $AX=B$, where $A$ is a two-dimensional matrix with all elements of main diagonal equal to zero.</p>\n\n<p>How to solve this problem? Iterational methods are not applied in this case.</p>\n\n<p>One way is <a href="http://en.wikipedia.org/wiki/LU_decomposition" rel="nofollow">LU Decomposition</a> method with reordering rows of $A$ to get entries in the main diagonal that are not zero, using permutation matrix. How can we quickly reorder the rows of the matrix or find the permutation matrix?</p>\n\n<p>Note that the matrix dimensions are large and I have to write a program to solve SLAE in C# language, so I do not need any Matlab or Mathematica functions. Thanks!</p>\n', 'ViewCount': '80', 'Title': 'LU decomposition with pivoting', 'LastEditorUserId': '4751', 'LastActivityDate': '2012-12-27T13:05:54.980', 'LastEditDate': '2012-12-27T12:57:46.530', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7619', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5207', 'Tags': '<matrices><linear-algebra>', 'CreationDate': '2012-12-27T08:29:05.987', 'Id': '7617'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '852', 'Title': 'What is the complexity of this matrix transposition?', 'LastEditDate': '2013-02-26T02:44:32.997', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5391', 'FavoriteCount': '1', 'Body': "<p>I'm working on some exercises regarding graph theory and complexity. \nNow I'm asked to give an algorithm that computes a transposed graph of $G$, $G^T$ given the adjacency matrix of $G$. So basically I just have to give an algorithm to transpose an $N \\times N$ matrix.</p>\n\n<p>My first thought was to loop through all rows and columns and simply swapping values in each of the $M[i,j]$ place. Giving a complexity of $O(n^2)$ But I immediately realized there's no need to swap more than once, so I can skip a column every time e.g. when I've iterated over row i, there's no need to start iteration of the next row at column i, but rather at column i + 1.</p>\n\n<p>This is all well and good, but how do I determine the complexity of this. When I think about a concrete example, for instance a 6x6 matrix this leads to 6 + 5 + 4 + 3 + 2 + 1 swaps (disregarding the fact that position [i,i] is always in the right position if you want to transpose a $N \\times N$ matrix, so we could skip that as well).\nThis looks alot like the well-known arithmetic series which simplifies to $n^2$, which leads me to think this is also $O(n^2)$. There are actually $n^2/2$ swaps needed, but by convention the leading constants may be ignored, so this still leads to $O(n^2)$. Skipping the i,i swaps leads to $n^2/2 - n$ swaps, which still is $O(n^2)$, but with less work still..</p>\n\n<p>Some clarification would be awesome :)</p>\n", 'Tags': '<graph-theory><time-complexity><algorithm-analysis><linear-algebra><adjacency-matrix>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-25T22:31:32.400', 'CommentCount': '3', 'AcceptedAnswerId': '10082', 'CreationDate': '2013-02-25T13:54:53.913', 'Id': '10081'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have implemented Gaussian elimination for solving system of linear equations in the field of modulo prime remainders. If there is a pivot equal to zero I assume the system has no solution but how to calculate number of solutions of such systems when all pivots are non-zero? (i.e. one and more solutions)</p>\n', 'ViewCount': '139', 'Title': 'Counting solutions to system of linear equations modulo prime', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-17T18:10:48.970', 'LastEditDate': '2013-03-17T18:10:48.970', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10569', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7287', 'Tags': '<combinatorics><linear-algebra>', 'CreationDate': '2013-03-16T23:52:12.520', 'Id': '10567'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here is the problem: we are given vectors $v_1, \\ldots, v_k$ lying in $\\mathbb{R}^n$ which are orthogonal. We assume that the entries of $v_i$ are rational, with numerator and denominator taking $K$ bits to describe. We would like to find vectors $v_{k+1}, \\ldots, v_n$ such that $v_1, \\ldots, v_n$ is an orthogonal basis for $\\mathbb{R}^n$. </p>\n\n<p>I would like to say this can be done in polynomial time in $n$ and $K$. However, I'm not sure this is the case. <strong>My question</strong> is to provide a proof that this can indeed be done in polynomial time. </p>\n\n<p>Here is where I am stuck.  Gram-Schmidt suggests to use the following iterative process. Suppose we currently have the collection $v_1, \\ldots, v_l$. Take the basis vectors $e_1, \\ldots, e_n$, go through them through them one by one, and if some $e_i$ is not in the span of the $v_1, \\ldots, v_l$, then set $v_{l+1} = P_{{\\rm span}(v_1, \\ldots, v_l)^\\perp} e_i$ (here $P$ is the projection operator). Repeat. </p>\n\n<p>This works in the sense that the number of additions and multiplications is polynomial in $n$. But what happens to the bit-sizes of the entries? The issue is that the projection of $e_i$ onto, say, $v_1$ may have denominators which need $2K$ bits or more to describe - because $P_{v_1}(e_i)$ is $v_1$ times its $i$'th entry, divided by $||v_1||$. Just $v_1$ times its $i$'th entry may already need $2K$ bits to describe. </p>\n\n<p>By a similar argument, it seems that each time I do this, the number of bits doubles. By the end, I may need $2^{\\Omega(n)}$ bits to describe the entries of the vector. How do I prove this does not happen? Or perhaps should I be doing things differently to avoid this?</p>\n", 'ViewCount': '90', 'Title': 'Can you complete a basis in polynomial time?', 'LastActivityDate': '2013-03-30T20:26:18.397', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10921', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7498', 'Tags': '<algorithms><linear-algebra>', 'CreationDate': '2013-03-30T04:09:45.353', 'Id': '10906'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to solve a system of inequalities in the following form:\n$\\ x_i - x_j \\leq w $</p>\n\n<p>I know these inequalities can be solved using <code>Bellman-Ford</code> algorithm. But there is also another condition. I want to find the solution that maximizes $\\ x_n - x_1$</p>\n\n<p>As far as I know the default <code>Bellman-Ford</code> algorithm minimizes it. How do I do that?</p>\n', 'ViewCount': '405', 'Title': 'Solving system of linear inequalities', 'LastActivityDate': '2013-08-26T17:37:29.993', 'AnswerCount': '4', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'Kia.celever', 'PostTypeId': '1', 'Tags': '<algorithms><linear-algebra><linear-programming><shortest-path>', 'CreationDate': '2013-04-19T09:14:20.977', 'Id': '11445'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an expression $$Ax+By+Cz.$$ where $A$, $B$ and $C$ are positive constants $\\ge1$. The variables $x$, $y$ and $z$ are non-negative integers. I am also given a number $T$. </p>\n\n<p>I want to find the largest integer value such that it is less than $T$ and not satisfied by $Ax+By+Cz$, how can I do it without using brute force. </p>\n', 'ViewCount': '82', 'Title': 'Finding the required value of an algebric expression', 'LastEditorUserId': '8110', 'LastActivityDate': '2013-05-14T02:02:03.263', 'LastEditDate': '2013-05-12T21:43:10.770', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><optimization><linear-programming><linear-algebra>', 'CreationDate': '2013-05-10T17:11:35.057', 'Id': '11940'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been using <a href="http://netlib2.cs.utk.edu/linalg/html_templates/node92.html" rel="nofollow">Harwell Boeing</a> format, also known as Compressed Column Strorage (CCS) in order to store Sparse Matrices. </p>\n\n<p>Could you please suggest me some other way to store/represent sparse matrices?</p>\n', 'ViewCount': '201', 'ClosedDate': '2014-03-14T21:02:58.907', 'Title': 'how to represent Sparse Matrices', 'LastActivityDate': '2014-03-14T09:52:01.290', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1228', 'Tags': '<data-structures><linear-algebra>', 'CreationDate': '2013-05-14T15:31:11.937', 'FavoriteCount': '0', 'Id': '12020'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>(This question might be legitimately crossposted to stackoverflow or mathoverflow or programming StackExchanges.)</p>\n\n<h1>Preface</h1>\n\n<p>I\'m reading this paper on solving linear systems of equations <code>Ax = b</code> using an explicit inverse (held by conventional wisdom as a big no-no): Druinsky &amp; Toledo, "How accurate is inv(A)*b?", <a href="http://arxiv.org/abs/1201.6035" rel="nofollow">http://arxiv.org/abs/1201.6035</a></p>\n\n<p>It asserts that in non-pathological cases, calculating a solution to <code>Ax=b</code> via <code>inv(A)*b</code> using the explicit inverse of a matrix produces solutions that are as accurate and as stable as preferred methods such as via LU factorization. It goes on to describe exactly which situations using "inv" might be <em>bad</em> (technically, when the right-hand side, <code>b</code>, is nearly orthogonal to the subspace spanned by left-singular vectors of <code>A</code> with low singular values). Even in these cases, the paper asserts, the solutions of <code>inv(A)*b</code> are <em>as accurate</em> as preferred methods, they\'re just not backwards stable.</p>\n\n<h1>Question</h1>\n\n<p>My question is: what are the specific drawbacks of using an algorithm that isn\'t guaranteed to be backwards stable? If all I wanted was some solutions to <code>Ax=b</code>, and I was guaranteed accurate results, does it matter that the solutions aren\'t backwards stable? Is there any specific examples of when it\'s ok to use an accurate but backwards-unstable algorithm? </p>\n\n<h1>Furthermore</h1>\n\n<p>My experimentation in Matlab/Octave shows me that the difference between a backwards-unstable and -stable algorithm is that slight perturbations to the solution result in bigger errors when going back through the linear system:</p>\n\n<pre><code>norm(A * (inv(A) * b) - b)\n</code></pre>\n\n<p>is much larger than</p>\n\n<pre><code>norm(A * (A \\ b) - b)\n</code></pre>\n\n<p>for a "bad" <code>b</code> (nearly orthogonal to the subspace of left-singular vectors with low singular values), where <code>\\</code> solves linear systems using Gaussian elimination (<a href="http://www.mathworks.com/help/matlab/ref/mldivide.html" rel="nofollow">http://www.mathworks.com/help/matlab/ref/mldivide.html</a>). The solutions themselves have the same accuracy, i.e., <code>norm(A\\b - true_x)</code> is about the same as <code>norm(inv(A)*b - true_x)</code>.</p>\n\n<p>I can imagine that in the case that </p>\n\n<ol>\n<li>I <em>only</em> cared about getting a solution to <code>Ax=b</code> and </li>\n<li>knew I would <em>never</em> propagate the solution back through <code>A</code>, </li>\n</ol>\n\n<p>could I justify using <code>inv(A)</code> when I didn\'t want to bother checking that <code>b</code> isn\'t "bad", i.e., without ensuring that the solution was backwards-stable. To me, this doesn\'t seem worth it, even without the speed and convenience advantages of using LU or Cholesky or QR decompositions. </p>\n', 'ViewCount': '63', 'Title': 'What are the drawbacks of using an algorithm that is not backwards stable?', 'LastEditorUserId': '8216', 'LastActivityDate': '2013-06-19T03:09:26.330', 'LastEditDate': '2013-06-10T13:40:48.020', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8216', 'Tags': '<linear-algebra><numerical-analysis>', 'CreationDate': '2013-06-10T13:07:49.347', 'FavoriteCount': '1', 'Id': '12597'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I searched linear solver library and found out PETSc library which considered to be powerful and useful library. PETSc consists implementations of various iterative methods with preconditioners and sparse matrix storing methods. All methods are realized sequentially and in parallel using MPI.</p>\n\n<p>I was very glad for creaters of PETSc. I downloaded it and installed. However, when I start reading user's guide I encountered following text:</p>\n\n<pre><code>PETSc should not be used to attempt to provide a \u201cparallel linear solver\u201d in an otherwise sequential\ncode. Certainly all parts of a previously sequential code need not be parallelized but the matrix\ngeneration portion must be parallelized to expect any kind of reasonable performance. Do not expect\nto generate your matrix sequentially and then \u201cuse PETSc\u201d to solve the linear system in parallel.\n</code></pre>\n\n<p>I was surprised! <em>Did PETSc developers really parallelize only matrix generating part? What is a benefit of using PETSc as parallel solver if linear system solving part runs sequentially</em>?</p>\n", 'ViewCount': '118', 'Title': 'Does PETSc really give speedup?', 'LastActivityDate': '2013-06-14T12:35:01.667', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12671', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5207', 'Tags': '<parallel-computing><linear-algebra>', 'CreationDate': '2013-06-14T11:56:21.917', 'Id': '12670'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $B=\\{b_1=g_1,\\cdots,b_n=g_n\\}$ be a set of binary variables $b_i$ and their corresponding values $g_i \\in \\{0,1\\}$. Let $M=\\{\\sum_{e \\in A}e \\;:\\; A \\subset B\\}$, i.e., $M$ is the set of all possible linear combinations of the equations in $B$.</p>\n\n<p>Given $S_i \\subset B$ for $i=1,\\cdots,m$, is that possible to compute, in polynomial time, a\n$K \\subset M$ with minimum size such that $S_i \\cup K$ is a full rank system of equations (i.e., the values of all of the variables can be obtained by solving $S_i \\cup K$)?</p>\n\n<p>An example: Let $B=\\{b_1=1,b_2=0,b_3=1\\}$, $S_1=\\{b_1=1,b_2=0\\}$, and $S_2=\\{b_2=0,b_3=1\\}$. \n$K=\\{b_1+b_3=0\\}$ is the solution because both $S_1\\cup K$ and $S_2 \\cup K$ can be solved uniquely and $K$ has the minimum size 1.</p>\n', 'ViewCount': '25', 'Title': 'Is this problem in P: Finding a common key for a collection of systems of equations?', 'LastActivityDate': '2013-06-20T02:54:32.613', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1672', 'Tags': '<complexity-theory><time-complexity><np-hard><polynomial-time><linear-algebra>', 'CreationDate': '2013-06-20T02:54:32.613', 'Id': '12776'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to implement and optimize the <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/TINYMT/" rel="nofollow">Tiny Mersenne Twister (TinyMT)</a> algorithm as required by an API I am developing with my team at work. The algorithm utilizes a C structure with 32-bit unsigned integers "mat1", "mat2", "tmat", and an array called "status" which is four 32-bit unsigned integers wide.</p>\n\n<p>I am relatively new to the subject of random number generation; however, I have been able to teach myself a lot about the subject over the past couple of weeks. I know what the purpose of a seed is, different methods such as linear congruent, LSFR, GFSR, etc. So I\'ve been doing my "homework" and researching the topic to the best of my ability (contrary to what the guys at Stack Overflow think). Unfortunately, the Mersenne Twister in general is extremely poorly documented and very few documents exist to explain the code and math side-by-side. The TinyMT\'s documentation is even worse, it\'s virtually non-existent! So developing accurate Doxygen comments for this part of the API is going to be tricky.</p>\n\n<p>With that said, hopefully somebody more qualified than I can help me out here. What is the significance of the aforementioned parameters? What do they do, what do they mean, what do they stand for, etc? My guess would be as follows:</p>\n\n<ul>\n<li>mat1 - Matrix 1</li>\n<li>mat2 - Matrix 2</li>\n<li>tmat - Tempering Matrix</li>\n<li>status - 127 bit wide "seed" (where the last bit goes, I\'m not sure)</li>\n</ul>\n\n<p>Given that the user provides values for "mat1", "mat2", and "tmat," are there any precautions they need to take before supplying values for them? Again, this is for an API and its documentation, so I would like to be able to give the customers a good idea of what they need to use the RNG and hopefully make the lives of other fellow programmers easier. Thanks!</p>\n', 'ViewCount': '76', 'Title': 'Significance of parameters in Tiny Mersenne Twister algorithm', 'LastActivityDate': '2013-06-26T02:46:19.197', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12904', 'Score': '1', 'OwnerDisplayName': 'audiFanatic', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<linear-algebra><randomized-algorithms><matrices>', 'CreationDate': '2013-06-25T18:06:30.620', 'Id': '12902'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A question about Laplace\'s approximation:</p>\n\n<p>In Laplace\'s method, we need to find the mode of a function and take second order Taylor\'s expansion. The first order term will vanish (since the gradient is zero at local optimum), and the second order term will be used to give a covariance of the gaussian. I am wondering if I am doing MCMC and the "point" I found is not local optimum (a.k.a gradient is not zero), will that influence my covariance matrix? Will the matrix be non-positive-definite something like that ?</p>\n', 'ViewCount': '30', 'Title': "Laplace's Approximation for graphical models", 'LastActivityDate': '2013-07-19T12:57:47.637', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9257', 'Tags': '<machine-learning><matrices><linear-algebra>', 'CreationDate': '2013-07-19T12:57:47.637', 'Id': '13349'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a linear system of the form:</p>\n\n<p>$$\\begin{array}{c}\nx_r = a \\quad x_j = b \\\\\nc_1x_1 + c_2x_2 + \\ldots + c_nx_n = N \\\\\nx_1+x_2 + x_3 + \\ldots + x_n = k\\\\\n0 \\le a,b,x_1,x_2,x_3...x_n \\le 1\\\\\nk \\ge 0\n\\end{array}$$</p>\n\n<p>How quickly can the feasibility of the system be checked? To clarify: $x_r,x_j$ are members of $x_1,x_2...x_n$. Would it be $O(n^{3.5})$ since I believe that is the general complexity for running a linear program or would it be less? Can one use gaussian elimination to quickly reduce the first 4 equations in $O(n^3)$ and after that systematically move through the equations starting from the terms with largest coefficient and moving to terms with smallest coefficient assigning values that bring the equations as close to satisfactory as possible?</p>\n\n<p>Additional info:</p>\n\n<p>I am assuming that the number of variables scales linearly. $n \\ne N$ (I think that was clear though). </p>\n', 'ViewCount': '439', 'Title': 'Checking Feasibility of Linear Program in Polynomial Time', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T06:23:52.420', 'LastEditDate': '2013-07-22T06:23:52.420', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '13371', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9271', 'Tags': '<complexity-theory><time-complexity><computational-geometry><linear-programming><linear-algebra>', 'CreationDate': '2013-07-20T23:16:43.717', 'Id': '13370'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If $u,v \\in \\mathbb{R}^d$ are two $d$-dimensional vectors, say that $u\\le v$ if $u_i \\le v_i$ for all $i=1,\\dots,d$.  In other words, comparisons on vectors will be pointwise.</p>\n\n<p>Let $S,T$ be two subsets of $\\mathbb{N}^d$ of size $m$.  Is there an efficient way to test whether there exists $s\\in S, t \\in T$ such that $s\\le t$?  The naive algorithm does $m^2$ comparisons between vectors; is there a more efficient algorithm?</p>\n\n<p>If $d=1$, this is very easy: we simply find the smallest element in $S$ and the largest element in $T$, which can be done with $O(m)$ comparisons.  But already when $d=2$, it seems much harder.  Any ideas?</p>\n', 'ViewCount': '132', 'Title': 'Comparing sets of vectors', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-29T22:53:35.173', 'LastEditDate': '2013-08-26T02:14:02.180', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-structures><computational-geometry><linear-algebra>', 'CreationDate': '2013-08-26T02:08:59.753', 'FavoriteCount': '1', 'Id': '13927'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What are some applications that require computing the <a href="http://en.wikipedia.org/wiki/Permanent" rel="nofollow">permanent of a matrix?</a></p>\n\n<p>One application I know of is related to graph theory and matchings. Apparently, the number of perfect matchings of a bipartite graph is the permanent of its incidence matrix.</p>\n\n<p>I am curious to know more applications of matrix permanent.</p>\n', 'ViewCount': '89', 'Title': 'What are some applications of computing the permanent of a matrix?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-19T17:36:59.153', 'LastEditDate': '2013-09-19T13:34:03.973', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9736', 'Tags': '<linear-algebra><matrices>', 'CreationDate': '2013-09-19T11:10:01.347', 'Id': '14438'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>After asking this question <a href="http://stackoverflow.com/questions/19649183/are-there-generalized-mathematical-operators-in-cs">on stackoverflow</a>, it has changed slightly.  Is there a way to represent a grammar as a basis for a vector space and represent a program as an object that lives in that vector space?</p>\n\n<p>I\'m interested in the parallels between mathematical operators (like the Hamiltonian, ladder operators, momentum operators, etc) and programming languages.  The operators that I\'m talking about can be thought of as transformation matrices that act on (potentially) infinite vectors.  It seems like a good place to start might be a tree algebra?</p>\n\n<p>An alternative would be to force the program into some sort of allowed bitwise representation, and then perform transforms on it.  Is such a thing possible?</p>\n', 'ViewCount': '50', 'Title': 'Generalized operators for programming languages', 'LastActivityDate': '2013-10-29T15:32:44.927', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16550', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11027', 'Tags': '<formal-grammars><linear-algebra><category-theory>', 'CreationDate': '2013-10-29T04:41:02.697', 'FavoriteCount': '0', 'Id': '16529'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been given a graph with n nodes. Now, I have to color every node of this graph by k colors, number from 0 to k-1. Now, there is a rule.</p>\n\n<p>For a node $x$ with adjacent nodes $y_1 , y_2, y_3, y_4,... y_m$, $color(x)=(color(y_1)+color(y_2)+color(y_3)+...+color(y_m)) \\pmod k $</p>\n\n<p>where $color(a)$ indicates a color number from 0 to k-1. I have to find number of ways I can color the whole graph.</p>\n\n<p>My approach to the problem was simple. I was constructing a $n*n$ matrix for n nodes in graph with equations like $col(x)-col(y_1)-col(y_2)-col(y_3)...-col(y_m)$. And trying to find number of all zero rows, which will provide us number of free variable. Is my approach correct?</p>\n', 'ViewCount': '56', 'Title': 'Solving a graph problem by Gaussian elimination', 'LastActivityDate': '2013-11-11T03:05:35.403', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '17903', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7890', 'Tags': '<graphs><linear-algebra><mathematical-programming>', 'CreationDate': '2013-11-10T03:10:23.457', 'Id': '17864'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the linear programs </p>\n\n<p>\\begin{array}{|ccc|}\n\\hline\nPrimal: &amp; A\\vec{x} \\leq \\vec{b} \\hspace{.5cm} &amp; \n\\max \\vec{c}^T\\vec{x} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|ccc|}\n\\hline\nDual: &amp; \\vec{c} \\leq \\vec{y}^TA \\hspace{.5cm} &amp;\n\\min \\vec{y}^T\\vec{b} \\\\\n\\hline\n\\end{array}</p>\n\n<p>The weak duality theorem states that \nif $\\vec{x}$ and $\\vec{y}$ satisfy the constraints then\n$\\vec{c}^T\\vec{x} \\leq \\vec{y}^T\\vec{b}$.\nIt has a short and slick proof using linear algebra:\n$\\vec{c}^T\\vec{x} \\leq  \\vec{y}^T A \\vec{x} \\leq \\vec{y}^T\\vec{b}$.</p>\n\n<p>The strong duality theorem states that if the $\\vec{x}$ is an optimal solution for the primal then there is $\\vec{y}$ which is a solution for the dual and \n$\\vec{c}^T\\vec{x} = \\vec{y}^T\\vec{b}$.</p>\n\n<p>Is there a similarly short and slick proof for the strong duality theorem?</p>\n', 'ViewCount': '213', 'Title': 'Short and slick proof of the strong duality theorem for linear programming', 'LastActivityDate': '2013-11-20T00:00:15.410', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<algorithms><reference-request><linear-programming><linear-algebra><duality>', 'CreationDate': '2013-11-19T09:39:14.263', 'Id': '18151'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we are given matrices $A_1, \\ldots, A_k$ which are $n \\times n$ matrices with rational entries and are asked to determine whether the polynomial ${\\rm det}(\\alpha_1 A_1 + \\alpha_2 A_2 + \\cdots + \\alpha_k A_k)$ is identically zero. How can we do this <strong>deterministically</strong> in polynomial time in $n$ and $k$?</p>\n\n<p>I'm aware that black-box polynomial identity testing is a difficult problem, but then this is not quite a black box. </p>\n", 'ViewCount': '150', 'Title': 'Testing whether a determinant polynomial is identically zero', 'LastEditorUserId': '12370', 'LastActivityDate': '2013-12-29T04:06:42.227', 'LastEditDate': '2013-12-29T04:06:42.227', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '12370', 'Tags': '<algorithms><linear-algebra><polynomials>', 'CreationDate': '2013-12-25T16:01:54.260', 'Id': '19278'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given integers $n,m$, I want to find a $m \\times n$ binary matrix $X$ such that there does not exist any non-zero vector $y \\in \\{-1,0,1\\}^n$ with $Xy=0$ (all operations performed over $\\mathbb{Z}$).  What algorithm could I use for this?</p>\n\n<hr>\n\n<p>In more detail: We are given parameters $n$ and $m$.  The problem is to determine if there exists $x$ such that $x_{i,j} \\in \\{0,1\\}$, and there does not exist $y\\ne (0,0,\\dots,0)$ where $y_j \\in \\{-1,0,1\\}$ for all $j$ and for all $1 \\leq i \\leq m$,</p>\n\n<p>$$\\sum_{1 \\leq j \\leq n} x_{i,j} y_j = 0.$$</p>\n\n<p>(Notice that we require that at least one of the  $y_j \\ne 0$ to avoid the trivial solution.)</p>\n\n<p>For example, consider $m=3,n=4$.  Then, expressing $x_{i,j}$ as a matrix $X$,</p>\n\n<p>$$\nX=\\begin{pmatrix}\n0 &amp; 1 &amp; 1 &amp; 0 \\\\\n1 &amp; 0 &amp; 1 &amp; 1 \\\\\n0 &amp; 1 &amp; 0 &amp; 1 \\\\\n\\end{pmatrix}\n$$</p>\n\n<p>is a valid solution for $m=3$ and $n=4$.</p>\n\n<p>What algorithm can I use to solve this problem?  Can I formulate this as an integer linear programming problem or maybe as a constraint programming problem?</p>\n', 'ViewCount': '205', 'Title': 'Find a binary matrix so that no vector from {-1,0,1}^n is in its kernel', 'LastEditorUserId': '8942', 'LastActivityDate': '2014-01-03T13:18:40.517', 'LastEditDate': '2014-01-03T13:13:21.270', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8942', 'Tags': '<complexity-theory><linear-programming><linear-algebra><constraint-programming><integer-programming>', 'CreationDate': '2013-12-27T20:07:50.993', 'FavoriteCount': '1', 'Id': '19333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a two-dimensional grid with wrap-around edges (a doughnut-shaped graph). I need to calculate the second-largest eigenvalue of the adjacency matrix. Is there a faster way of computing it for such a special graph than a general method such as the "eigs" function in MATLAB?</p>\n', 'ViewCount': '35', 'Title': 'Calculating eigenvalue gap of a torus graph', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-02T13:39:46.200', 'LastEditDate': '2014-02-02T13:39:46.200', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19801', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9802', 'Tags': '<graph-theory><linear-algebra>', 'CreationDate': '2014-01-17T23:06:50.043', 'Id': '19800'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Having large matrices, $W$ (the unknown) and $M$ (known), is it possible to solve for $W$ in this equation\n$$W \\cdot W^{T} = M,$$\nwhere $M$ can have negative entries.</p>\n', 'ViewCount': '26', 'Title': 'Solving for the matrix $W$ in an equation involving $W \\cdot W^{T}$', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-05T11:24:43.127', 'LastEditDate': '2014-03-05T11:24:43.127', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22302', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<linear-algebra><matrices><mathematical-programming>', 'CreationDate': '2014-03-05T10:37:02.450', 'Id': '22301'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've implemented a program for computing eigenvectors of some random, symmetric, $N$x$N$ matrix using the power method.  I have found difficulty in calculating all $N$ eigenvectors consistently, almost every time the algorithm fails to converge for all $N$.  The Wikipedia page on the power method tells me this algorithm is not guaranteed to converge for all $N$ eigenvectors, can someone suggest a way for me to encourage convergence, at least in a majority of the cases?  Is this possible?  If not, can someone suggest a better algorithm for computing eigenvectors?</p>\n", 'ViewCount': '44', 'Title': 'Power method to calculate eigenvectors', 'LastActivityDate': '2014-03-06T05:48:26.150', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15331', 'Tags': '<algorithms><linear-algebra><matrices>', 'CreationDate': '2014-03-06T05:02:27.497', 'Id': '22326'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to code a solver for nonsingular systems of $N$ linear equations in $N$ unknowns (say up to $N=100$) with an asymmetric Toeplitz matrix. I know that the Levinson algorithm can solve it in time $O(N^2)$ and I am looking for a solution with this complexity. I have seen mentions of alternative approaches such as Schur decomposition, LDU decomposition, Bareiss, Cholesky...</p>\n\n<p>The equations are established in a Galois field, so that stability is not at all an issue here.</p>\n\n<p>I am seeking advice for a good method to implement. My priorities are</p>\n\n<p>1) ease of implementation,</p>\n\n<p>2) low memory requirements.</p>\n\n<p>I am not specially looking for superfast methods ($o(N^2)$), unless they are appropriate for moderate $N$, and simple.</p>\n\n<p>What do you recommend ?</p>\n', 'ViewCount': '16', 'Title': 'Solution of a Toepltiz system of linear equations', 'LastActivityDate': '2014-03-23T17:43:45.287', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16034', 'Tags': '<algorithms><linear-algebra>', 'CreationDate': '2014-03-23T17:43:45.287', 'Id': '22976'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '79', 'Title': 'How to compute a curious inverse', 'LastEditDate': '2014-04-05T16:42:34.627', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16449', 'FavoriteCount': '1', 'Body': '<p>Let $M$ be a square matrix with entries that are $0$ or $1$ and let $v$ be a vector with values that are also $0$ or $1$.  If we are given $M$ and $y = Mv$, we can computer $v$ if $M$ is non-singular.  </p>\n\n<p>Now let us take the second bit (from the right) of the binary representation of each $y_i$ as another vector $z$. So $z$ also has entries which are $0$ or $1$. If $y_i$ has fewer than two bits we just let $z_i=0$.  </p>\n\n<blockquote>\n  <p>If we are given $z$ and $M$, how (and when) can you find a $v$ so that\n  $Mv$ would produce $z$ under this operation?</p>\n</blockquote>\n\n<p>Here is an example</p>\n\n<p>$$M = \\begin{pmatrix}\n  0 &amp; 0 &amp; 1 &amp; 0\\\\\n  1 &amp; 1 &amp; 0 &amp; 1\\\\\n  1 &amp; 1 &amp; 1 &amp; 0\\\\\n  0 &amp; 1 &amp; 1 &amp; 1\\\\\n\\end{pmatrix}\n, v = \\begin{pmatrix}\n  0 \\\\ \n  1 \\\\ \n  1 \\\\\n   1\\\\\n\\end{pmatrix}\n\\implies Mv=\\begin{pmatrix}\n  1 \\\\ \n  2 \\\\\n  2 \\\\ \n  3\\\\\n\\end{pmatrix}\n.$$</p>\n\n<p>So in this case </p>\n\n<p>$$z = \n\\begin{pmatrix}\n0 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n$$</p>\n\n<hr>\n\n<p>Is this problem in fact NP-hard?</p>\n', 'Tags': '<algorithms><np-hard><linear-algebra>', 'LastEditorUserId': '10359', 'LastActivityDate': '2014-04-05T16:42:34.627', 'CommentCount': '0', 'AcceptedAnswerId': '23429', 'CreationDate': '2014-04-04T17:13:48.063', 'Id': '23428'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<blockquote>\n  <p>How many arithmetic operations are required to find a <a href="http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse">Moore\u2013Penrose\n  pseudoinverse\n  matrix</a> of a arbitrary field?</p>\n</blockquote>\n\n<p>If the matrix is invertible and complex valued, then it\'s just the inverse. Finding the inverse takes $O(n^\\omega)$ time, where $\\omega$ is the matrix multiplication constant. It is Theorem 28.2 in Introduction to Algorithms 3rd Edition. </p>\n\n<p>If the matrix $A$ has linearly independent rows or columns and complex valued, then the pseudoinverse matrix can be computed with $A^*(A A^*)^{-1}$ or $(A A^*)^{-1}A^*$ respectively, where $A^*$ is the conjugate transpose of $A$. In particular, this implies an $O(n^\\omega)$ time for finding the pseudoinverse of $A$.</p>\n\n<p>For general matrix, the algorithms I have seen uses QR decomposition or SVD, which seems to take $O(n^3)$ arithmetic operations in the worst case. Is there algorithms that uses fewer operations? </p>\n', 'ViewCount': '115', 'Title': 'Complexity of finding the pseudoinverse matrix', 'LastActivityDate': '2014-04-23T22:00:05.707', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24065', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms><linear-algebra>', 'CreationDate': '2014-04-23T19:49:06.803', 'FavoriteCount': '1', 'Id': '24060'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $X$ be an $m\\times n$ ($m$: number of records, and $n$: number of attributes) dataset.  When the number of attributes $n$ is large and the dataset $X$ is noisy, classification gets more complicated and the classification accuracy decreases. One way to over come this problem is to use linear transformation, i.e., perform classification on $Y=XR$, where $R$ is an $n\\times p$ matrix, and $p\\leq n$. I was wondering how linear transformation simplifies classification? and why classification accuracy increases if we do classification on the transformed data $Y$ when $X$ is noisy?</p>\n', 'ViewCount': '57', 'Title': 'Why linear transformation can improve classification accuracy when the dimensionality of data is high?', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-28T22:51:22.710', 'LastEditDate': '2014-04-27T05:08:49.667', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '17110', 'Tags': '<machine-learning><data-mining><linear-algebra><matrices><classification>', 'CreationDate': '2014-04-27T01:46:05.837', 'FavoriteCount': '1', 'Id': '24147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can I write R code for finding the inverse of a matrix without using the built-in function, but using the determinant function instead?</p>\n', 'ViewCount': '4', 'ClosedDate': '2014-04-30T04:30:19.733', 'Title': 'R code for finding inverse of matrix using determinants', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-30T04:03:17.330', 'LastEditDate': '2014-04-30T04:03:17.330', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17200', 'Tags': '<linear-algebra>', 'CreationDate': '2014-04-30T02:50:56.243', 'Id': '24248'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>One approach for clustering a high dimensional dataset is to use linear transformation, and the most common approaches are PCA and random projection (where random projection arises from the Johnson-Lindenstrauss Lemma). I was wondering why we can't use other random transformation  s like when our transformation matrix $R$ was drawn from a uniform distribution?</p>\n", 'ViewCount': '16', 'Title': 'Subspace clustering with random transformation', 'LastActivityDate': '2014-04-30T04:26:59.197', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17110', 'Tags': '<data-mining><linear-algebra><classification><cluster>', 'CreationDate': '2014-04-30T03:43:25.603', 'Id': '24249'}},