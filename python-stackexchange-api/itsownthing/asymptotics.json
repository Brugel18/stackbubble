90_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '3042', 'Title': 'How does one know which notation of time complexity analysis to use?', 'LastEditDate': '2013-06-06T14:12:06.230', 'AnswerCount': '3', 'Score': '41', 'PostTypeId': '1', 'OwnerUserId': '110', 'FavoriteCount': '18', 'Body': '<p>In most introductory algorithm classes, notations like $O$ (Big O) and $\\Theta$ are introduced, and a student would typically learn to use one of these to find the time complexity.</p>\n\n<p>However, there are other notations, such as $o$, $\\Omega$ and $\\omega$. Are there any specific scenarios where one notation would be preferable to another?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2014-04-01T22:12:52.083', 'CommentCount': '1', 'AcceptedAnswerId': '61', 'CreationDate': '2012-03-07T01:42:10.933', 'Id': '57'},90_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I wrote</p>\n\n<p>$\\qquad \\displaystyle \\sum\\limits_{i=1}^n \\frac{1}{i} = \\sum\\limits_{i=1}^n \\cal{O}(1) = \\cal{O}(n)$</p>\n\n<p>but my friend says this is wrong. From the TCS cheat sheet I know that the sum is also called $H_n$ which has logarithmic growth in $n$. So my bound is not very sharp, but is sufficient for the analysis I needed it for.</p>\n\n<p>What did I do wrong?</p>\n\n<p><strong>Edit</strong>:\nMy friend says that with the same reasoning, we can prove that</p>\n\n<p>$\\qquad \\displaystyle \\sum\\limits_{i=1}^n i = \\sum\\limits_{i=1}^n \\cal{O}(1) = \\cal{O}(n)$</p>\n\n<p>Now this is obviously wrong! What is going on here?</p>\n', 'ViewCount': '219', 'Title': 'What goes wrong with sums of Landau terms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-16T21:32:29.560', 'LastEditDate': '2012-03-15T07:06:28.017', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '389', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-03-14T13:12:56.093', 'Id': '366'},90_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to understand what is wrong with the following proof of the following recurrence </p>\n\n<p>$$\r\nT(n) = 2\\,T\\!\\left(\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\right)+n \r\n$$\n$$\r\nT(n) \\leq 2\\left(c\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\right)+n \\leq cn+n = n(c+1) =O(n)\r\n$$</p>\n\n<p>The documentation says it's wrong because of the inductive hypothesis that\n$$\r\nT(n) \\leq cn\r\n$$\nWhat Am I missing?</p>\n", 'ViewCount': '418', 'Title': 'Error in the use of asymptotic notation', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-26T18:24:32.487', 'LastEditDate': '2012-03-26T18:24:32.487', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '777', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '736', 'Tags': '<algorithms><landau-notation><asymptotics><recurrence-relation>', 'CreationDate': '2012-03-25T21:16:46.657', 'Id': '772'},90_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '767', 'Title': 'Sorting functions by asymptotic growth', 'LastEditDate': '2013-06-06T16:01:40.233', 'AnswerCount': '5', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '776', 'FavoriteCount': '12', 'Body': '<p>Assume I have a list of functions, for example </p>\n\n<p>$\\qquad n^{\\log \\log(n)},  2^n, n!, n^3, n \\ln n, \\dots$</p>\n\n<p>How do I sort them asymptotically, i.e. after the relation defined by</p>\n\n<p>$\\qquad f \\leq_O g \\iff f \\in O(g)$,</p>\n\n<p>assuming they are indeed pairwise comparable (see also <a href="http://cs.stackexchange.com/questions/1780/are-the-functions-always-asymptotically-comparable">here</a>)? Using the definition of $O$ seems awkward, and it is often hard to prove the existence of suitable constants $c$ and $n_0$.</p>\n', 'Tags': '<asymptotics><landau-notation><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-11-19T13:21:17.813', 'CommentCount': '1', 'AcceptedAnswerId': '827', 'CreationDate': '2012-03-27T15:47:31.160', 'Id': '824'},90_4:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '354', 'Title': 'How to fool the plot inspection heuristic?', 'LastEditDate': '2014-01-26T03:54:58.673', 'AnswerCount': '4', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '6', 'Body': '<p>Over <a href="http://cs.stackexchange.com/a/825/98">here</a>, Dave Clarke proposed that in order to compare asymptotic growth you should plot the functions at hand. As a theoretically inclined computer scientist, I call(ed) this vodoo as a plot is never proof. On second thought, I have to agree that this is a very useful approach that is even sometimes underused; a plot is an efficient way to get first ideas, and sometimes that is all you need.</p>\n\n<p>When teaching TCS, there is always the student who asks: "What do I need formal proof for if I can just do X which always works?" It is up to his teacher(s) to point out and illustrate the fallacy. There is a brilliant set of examples of <a href="http://math.stackexchange.com/q/111440/3330">apparent patterns that eventually fail</a> over at math.SE, but those are fairly mathematical scenarios.</p>\n\n<p>So, how do you fool the plot inspection heuristic? There are some cases where differences are hard to tell appart, e.g.</p>\n\n<p><img src="http://i.stack.imgur.com/cXBip.png" alt="example">\n<img src="http://i.stack.imgur.com/tFr3Z.png" alt="example">\n<img src="http://i.stack.imgur.com/qzOHT.png" alt="example"><br>\n<sup>[<a href="https://github.com/akerbos/sesketches/blob/gh-pages/src/cs_857.gnuplot" rel="nofollow">source</a>]</sup></p>\n\n<p>Make a guess, and then check the source for the real functions. But those are not as spectacular as I would hope for, in particular because the real relations are easy to spot from the functions alone, even for a beginner.</p>\n\n<p>Are there examples of (relative) asymptotic growth where the truth is not obvious from the function definiton and plot inspection for reasonably large $n$ gives you a completely wrong idea? Mathematical functions and real data sets (e.g. runtime of a specific algorithm) are both welcome; please refrain from piecewise defined functions, though.</p>\n', 'Tags': '<asymptotics><didactics>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T03:54:58.673', 'CommentCount': '2', 'AcceptedAnswerId': '873', 'CreationDate': '2012-03-29T07:09:18.883', 'Id': '857'},90_5:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '360', 'Title': 'How to prove that  $n(\\log_3(n))^5 = O(n^{1.2})$?', 'LastEditDate': '2012-06-02T00:51:16.560', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '886', 'FavoriteCount': '1', 'Body': "<p>This a homework question from Udi Manber's book. Any hint would be nice :)</p>\n\n<p>I must show that:</p>\n\n<blockquote>\n  <p>$n(\\log_3(n))^5 = O(n^{1.2})$</p>\n</blockquote>\n\n<p>I tried using Theorem 3.1 of book:</p>\n\n<blockquote>\n  <p>$f(n)^c = O(a^{f(n)})$ (for $c &gt; 0$, $a &gt; 1$)</p>\n</blockquote>\n\n<p>Substituing:</p>\n\n<p>$(\\log_3(n))^5 = O(3^{\\log_3(n)}) = O(n) $</p>\n\n<p>but $n(\\log_3(n))^5 = O(n\\cdot n) = O(n^2) \\ne O(n^{1.2})$</p>\n\n<p>Thank you for any help.</p>\n", 'Tags': '<asymptotics><landau-notation><mathematical-analysis>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:24:44.133', 'CommentCount': '5', 'AcceptedAnswerId': '975', 'CreationDate': '2012-04-02T00:59:41.290', 'Id': '974'},90_6:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>An asymptotic lower bound such as exponential-hardness is generally thought to imply that a problem is "inherently difficult". Encryption that is "inherently difficult" to break is thought to be secure. </p>\n\n<p>However, an asymptotic lower bound does not rule out the possibility that a huge but finite class of problem instances are easy (eg. all instances with size less than $10^{1000}$).</p>\n\n<p>Is there any reason to think that cryptography being based on asymptotic lower bounds would confer any particular level of security? Do security experts consider such possibilities, or are they simply ignored? </p>\n\n<p>An example is the use of trap-door functions based on the decomposition of large numbers into their prime factors. This was at one point thought to be inherently difficult (I think that exponential was the conjecture) but now many believe that there may be a polynomial algorithm (as there is for primality testing). No one seems to care very much about the lack of an exponential lower bound.</p>\n\n<p>I believe that other trap door functions have been proposed that are thought to be NP-hard (see <a href="http://cs.stackexchange.com/q/356/98">related question</a>), and some may  even have a proven lower bound. My question is more fundamental: does it matter what the asymptotic lower bound is? If not, is the practical security of any cryptographic code at all related to any asymptotic complexity result?</p>\n', 'ViewCount': '199', 'Title': 'Are asymptotic lower bounds relevant to cryptography?', 'LastEditorUserId': '157', 'LastActivityDate': '2012-04-13T07:08:10.570', 'LastEditDate': '2012-04-13T07:08:10.570', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '1038', 'Tags': '<complexity-theory><cryptography><asymptotics>', 'CreationDate': '2012-04-11T16:44:36.490', 'FavoriteCount': '1', 'Id': '1226'},90_7:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is the recursive formula for which I\'m trying to find an asymptotic closed form by the <a href="http://en.wikipedia.org/wiki/Master_theorem" rel="nofollow">master theorem</a>:\n$$T(n)=9T(n/27)+(n \\cdot \\lg(n))^{1/2}$$</p>\n\n<p>I started with $a=9,b=27$ and $f(n)=(n\\cdot \\lg n)^{1/2}$  for using the master theorem by $n^{\\log_b(a)}$, and if so $n^{\\log_{27}(9)}=n^{2/3}$ but I don\'t understand how to play with the $(n\\cdot \\lg n)^{1/2}$. </p>\n\n<p>I think that the $(n\\cdot \\lg n)^{1/2}$ is bigger than $n^{2/3}$ but I\'m sure I skip here on something. </p>\n\n<p>I think it fits to the third case of the master theorem.</p>\n', 'ViewCount': '545', 'Title': 'Solve a recurrence using the master theorem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T13:23:52.493', 'LastEditDate': '2013-02-02T13:23:52.493', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '1297', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '747', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2012-04-16T01:19:14.927', 'Id': '1296'},90_8:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '237', 'Title': '$\\log^*(n)$ runtime analysis', 'LastEditDate': '2012-04-18T19:11:59.100', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1132', 'FavoriteCount': '1', 'Body': "<p>So I know that $\\log^*$ means iterated logarithm, so $\\log^*(3)$ = $(\\log\\log\\log\\log...)$ until $n \\leq 1$.</p>\n\n<p>I'm trying to solve the following:</p>\n\n<p>is </p>\n\n<blockquote>\n  <p>$\\log^*(2^{2^n})$</p>\n</blockquote>\n\n<p>little $o$, little $\\omega$, or $\\Theta$ of</p>\n\n<blockquote>\n  <p>${\\log^*(n)}^2$</p>\n</blockquote>\n\n<p>In terms of the interior functions, $\\log^*(2^{2^n})$ is much bigger than $\\log^*(n)$, but squaring the $\\log^*(n)$ is throwing me off. </p>\n\n<p>I know that $\\log(n)^2$ is $O(n)$, but I don't think that property holds for the iterative logarithm.</p>\n\n<p>I tried applying the master method, but I'm having trouble with the properties of a $\\log^*(n)$ function. I tried setting n to be max (i.e. $n = 5$), but this didn't really simplify the problem.</p>\n\n<p>Does anyone have any tips as to how I should approach this?</p>\n", 'Tags': '<asymptotics><landau-notation><mathematical-analysis>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-18T19:11:59.100', 'CommentCount': '0', 'AcceptedAnswerId': '1340', 'CreationDate': '2012-04-18T14:32:10.103', 'Id': '1339'},90_9:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I applied the Master theorem to a recurrence for a running time I encountered (this is a simplified version):</p>\n\n<p>$$T(n)=4T(n/2)+O(r)$$</p>\n\n<p>$r$ is independent of $n$. Case 1 of the Master theorem applies and tells us that $T(n)=O(n^2)$.</p>\n\n<p>However, this hides a constant dependent on $r$ in the big-oh notation: our recurrence has depth $O(\\log_2 n)$ so at the final level we have $O(4^{\\log_2 n})=O(n^2)$ subproblems, each of which takes $O(r)$ time to be handled. This means the actual running time is $O(n^2 r)$ (or worse: this analysis only talks about the lowest level).</p>\n\n<p>This is my actual recursion:</p>\n\n<p>$$T(n)=r^2T(n/r)+O(nr^2)$$</p>\n\n<p>Is there a method similar to the Master theorem for these kinds of recursions?</p>\n', 'ViewCount': '227', 'Title': 'Master theorem and constants independent of $n$', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T13:24:14.520', 'LastEditDate': '2013-02-02T13:24:14.520', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation><mathematical-analysis><master-theorem>', 'CreationDate': '2012-04-29T17:39:46.100', 'FavoriteCount': '2', 'Id': '1576'},90_10:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to find a $\\Theta$ bound for the following recurrence equation:</p>\n\n<p>$$ T(n) = 2 T(n/2) + T(n/3) + 2n^2+ 5n + 42 $$ </p>\n\n<p>I figure Master Theorem is inappropriate due to differing amount of subproblems and divisions. Also recursion trees do not work since there is no $T(1)$ or rather $T(0)$. </p>\n', 'ViewCount': '536', 'Title': 'Solving Recurrence Equations containing two Recursion Calls', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-09-28T12:36:36.840', 'LastEditDate': '2013-09-28T12:36:36.840', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1382', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2012-05-05T23:47:48.517', 'FavoriteCount': '1', 'Id': '1682'},90_11:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '234', 'Title': 'Recursion for runtime of divide and conquer algorithms', 'LastEditDate': '2012-05-10T16:31:21.383', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': "<p>A divide and conquer algorithm's work at a specific level can be simplified into the equation:</p>\n\n<p>$\\qquad \\displaystyle O\\left(n^d\\right) \\cdot \\left(\\frac{a}{b^d}\\right)^k$</p>\n\n<p>where $n$ is the size of the problem, $a$ is the number of sub problems, $b$ is the factor the size of the problem is broken down by at each recursion, $k$ is the level, and $d$ is the exponent for Big O notation (linear, exponential etc.).</p>\n\n<p>The book claims  if the ratio is greater than one the sum of work is given by the last term on the last level, but if it is less than one the sum of work is given by the first term of the first level. Could someone explain why this is true?</p>\n", 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><recursion><mathematical-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T16:31:21.383', 'CommentCount': '0', 'AcceptedAnswerId': '1746', 'CreationDate': '2012-05-09T03:13:56.910', 'Id': '1745'},90_12:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When we compare the complexity of two algorithms, it is usually the case that either $f(n) = O(g(n))$ or $g(n) = O(f(n))$ (possibly both), where $f$ and $g$ are the running times (for example) of the two algorithms.</p>\n\n<p>Is this always the case? That is, does at least one of the relationships $f(n) = O(g(n))$ and $g(n) = O(f(n))$ always hold, that is for general functions $f$,$g$? If not, which assumptions do we have to make, and (why) is it ok when we talk about algorithm running times?</p>\n', 'ViewCount': '378', 'Title': 'Are the functions always asymptotically comparable?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-16T05:42:38.933', 'LastEditDate': '2012-05-15T20:47:14.597', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '8', 'OwnerDisplayName': 'matrixx', 'PostTypeId': '1', 'Tags': '<asymptotics><mathematical-analysis>', 'CreationDate': '2012-05-10T15:29:47.797', 'FavoriteCount': '1', 'Id': '1780'},90_13:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '990', 'Title': 'Master theorem not applicable?', 'LastEditDate': '2013-02-02T13:24:28.657', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1561', 'FavoriteCount': '1', 'Body': '<p>Given the following recursive equation</p>\n\n<p>$$ T(n) = 2T\\left(\\frac{n}{2}\\right)+n\\log n$$ we want to apply the Master theorem and note that</p>\n\n<p>$$ n^{\\log_2(2)} = n.$$</p>\n\n<p>Now we check the first two cases for $\\varepsilon &gt; 0$, that is whether</p>\n\n<ul>\n<li>$n\\log n \\in O(n^{1-\\varepsilon})$ or</li>\n<li>$n\\log n \\in \\Theta(n)$.</li>\n</ul>\n\n<p>The two cases are not satisfied. So we have to check the third case, that is whether</p>\n\n<ul>\n<li>$n\\log n \\in \\Omega(n^{1+\\varepsilon})$ .</li>\n</ul>\n\n<p>I think the third condition is not satisfied either. But why? And what would be a good explanation for why the Master theorem cannot be applied in this case?</p>\n', 'Tags': '<proof-techniques><asymptotics><recurrence-relation><master-theorem>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-24T09:34:21.943', 'CommentCount': '5', 'AcceptedAnswerId': '2388', 'CreationDate': '2012-05-20T20:45:35.817', 'Id': '1957'},90_14:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1671', 'Title': 'Data structure with search, insert and delete in amortised time $O(1)$?', 'LastEditDate': '2012-05-22T07:32:22.800', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '1120', 'FavoriteCount': '7', 'Body': '<p>Is there a data structure to maintain an ordered list that supports the following operations in $O(1)$ amortized time? </p>\n\n<ul>\n<li><p><strong>GetElement(k)</strong>: Return the $k$th element of the list.</p></li>\n<li><p><strong>InsertAfter(x,y)</strong>: Insert the new element y into the list immediately after x.  </p></li>\n<li><p><strong>Delete(x)</strong>: Remove x from the list.</p></li>\n</ul>\n\n<p>For the last two operations, you can assume that x is given as a pointer directly into the data structure; InsertElement returns the corresponding pointer for y.  InsertAfter(NULL, y) inserts y at the beginning of the list.</p>\n\n<p>For example, starting with an empty data structure, the following operations update the ordered list as shown below:</p>\n\n<ul>\n<li>InsertAfter(NULL, a) $\\implies$ [a]</li>\n<li>InsertAfter(NULL, b) $\\implies$ [b, a]</li>\n<li>InsertAfter(b, c) $\\implies$ [b, c, a]</li>\n<li>InsertAfter(a, d) $\\implies$ [b, c, a, d]</li>\n<li>Delete(c) $\\implies$ [b, a, d]</li>\n</ul>\n\n<p>After these five updates, GetElement(2) should return d, and GetElement(3) should return an error.</p>\n', 'Tags': '<data-structures><time-complexity><asymptotics><amortized-analysis>', 'LastEditorUserId': '72', 'LastActivityDate': '2013-08-06T14:07:23.407', 'CommentCount': '8', 'AcceptedAnswerId': '7807', 'CreationDate': '2012-05-21T07:35:55.077', 'Id': '1970'},90_15:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '147', 'Title': 'Finding lambda of Master Theorem', 'LastEditDate': '2013-02-02T13:23:30.387', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Luc Peetersen', 'PostTypeId': '1', 'OwnerUserId': '1722', 'FavoriteCount': '1', 'Body': '<p>Suppose I have a recurrence like $T(n)=2T(n/4)+\\log(n)$ with $a=2, b=4$ and $f(n)=\\log(n)$.</p>\n\n<p>That should be <a href="http://en.wikipedia.org/wiki/Master_theorem#Case_1" rel="nofollow">case 1 of the Master theorem</a> because $n^{1/2}&gt;\\log(n)$. There is also a lambda in case 1: $f(n)=O(n^{(1/2)-\\lambda})$. Is this correct? And how can I find this lambda?</p>\n', 'Tags': '<proof-techniques><asymptotics><recurrence-relation><master-theorem>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T13:23:30.387', 'CommentCount': '1', 'AcceptedAnswerId': '2195', 'CreationDate': '2012-06-01T21:00:09.973', 'Id': '2192'},90_16:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The lookup time in perfect hash-tables is $O(1)$ in the worst case. Does that simply mean that the average should be $\\leq O(1)$?</p>\n', 'ViewCount': '163', 'Title': 'What is the average search complexity of perfect hashing?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-17T06:12:56.213', 'LastEditDate': '2012-07-17T06:12:56.213', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2067', 'Tags': '<complexity-theory><time-complexity><asymptotics><hash-tables>', 'CreationDate': '2012-07-04T14:36:16.523', 'Id': '2610'},90_17:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I have a high interest in priority-queues (E.g., see my answers on: <a href="http://cs.stackexchange.com/q/524">Does there exist a priority queue with $O(1)$ extracts?</a>), and was wondering if there is a priority-queue or similar data-structure where you can sort by multiple values?</p>\n\n<p>For example, if I wanted to sort by <code>numval</code> and sort by <code>strval</code>, and be able to get the highest (G\xf6del numbering for str) in $\\mathcal{O}(1)$.</p>\n\n\n\n<pre><code>struct Node {\n    int numval;\n    std::string strval;\n};\n</code></pre>\n\n<p>Easily I can think to just maintain two priority-queues, but this would require twice the memory.</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '214', 'Title': 'Queue that can sort by multiple priorities?', 'LastActivityDate': '2012-07-05T13:37:54.173', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<data-structures><asymptotics><efficiency><priority-queues><memory-management>', 'CreationDate': '2012-07-05T13:37:54.173', 'FavoriteCount': '2', 'Id': '2629'},90_18:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been tasked with solving some recurrence relations, and I've been running into trouble with so called 'chip &amp; conquer' relations.</p>\n\n<p>Here are some example problems:</p>\n\n<p>$$T(n) = T(n-5) + cn^2$$</p>\n\n<p>and</p>\n\n<p>$$T(n) = T(n-2) + \\log{n}$$</p>\n\n<p>I'm supposed to be giving an answer in $\\Theta$ notation. How do I go around and solving relations like these?</p>\n", 'ViewCount': '302', 'Title': "Solving Recurrence Relations 'Chip & Conquer'", 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-17T05:57:56.653', 'LastEditDate': '2012-07-17T05:57:56.653', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'OwnerDisplayName': 'user906153', 'PostTypeId': '1', 'Tags': '<asymptotics><recurrence-relation><mathematical-analysis>', 'CreationDate': '2012-04-25T02:16:52.133', 'Id': '2671'},90_19:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In computer science, we have often have to solve <a href="http://en.wikipedia.org/wiki/Recurrence_relation">recurrence relations</a>, that is find a <strong>closed form</strong> for a recursively defined sequence of numbers. When considering runtimes, we are often interested mainly in the sequence\'s <a href="http://en.wikipedia.org/wiki/Asymptotic_analysis"><strong>asymptotic</strong> growth</a>.</p>\n\n<p>Examples are </p>\n\n<ol>\n<li><p>The runtime of a tail-recursive function stepping downwards to $0$ from $n$ whose body takes time $f(n)$:</p>\n\n<p>$\\qquad \\begin{align}\n  T(0) &amp;= 0 \\\\\n  T(n+1) &amp;= T(n) + f(n)    \n \\end{align}$</p></li>\n<li><p>The <a href="http://en.wikipedia.org/wiki/Fibonacci_sequence">Fibonacci sequence</a>:</p>\n\n<p>$\\qquad \\begin{align}\n  F_0 &amp;= 0 \\\\\n  F_1 &amp;= 1 \\\\\n  F_{n+2} &amp;= F_n + F_{n+1}    \n \\end{align}$</p></li>\n<li><p>The number of <a href="http://en.wikipedia.org/wiki/Dyck_language">Dyck words</a> with $n$ parenthesis pairs:</p>\n\n<p>$\\qquad\\begin{align}\n    C_0 &amp;= 1 \\\\ \n    C_{n+1}&amp;=\\sum_{i=0}^{n}C_i\\,C_{n-i}\n  \\end{align}$</p></li>\n<li><p>The mergesort runtime recurrence on lists of length $n$:</p>\n\n<p>$\\qquad \\begin{align}\n  T(1) &amp;= T(0) = 0 \\\\\n  T(n) &amp;= T(\\lfloor n/2\\rfloor) + T(\\lceil n/2\\rceil) + n-1\n \\end{align}$</p></li>\n</ol>\n\n<p>What are methods to solve recurrence relations? We are looking for</p>\n\n<ul>\n<li>general methods and</li>\n<li>methods for a significant subclass</li>\n</ul>\n\n<p>as well as</p>\n\n<ul>\n<li>methods that yield precise solutions and</li>\n<li>methods that provide (bounds on) asymptotic growth.</li>\n</ul>\n\n<p><sup>This is supposed to become a reference question. Please post one answer per method and provide a general description as well as an illustrative example.</sup></p>\n', 'ViewCount': '1536', 'Title': 'Solving or approximating recurrence relations for sequences of numbers', 'LastEditorUserId': '6716', 'LastActivityDate': '2014-04-24T15:41:56.573', 'LastEditDate': '2013-06-06T16:02:38.220', 'AnswerCount': '8', 'CommentCount': '1', 'Score': '27', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<asymptotics><proof-techniques><combinatorics><recurrence-relation><reference-question>', 'CreationDate': '2012-07-17T18:31:38.607', 'FavoriteCount': '27', 'Id': '2789'},90_20:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I asked a (seed) question about sums of Landau terms <a href="http://cs.stackexchange.com/questions/366/what-goes-wrong-with-sums-of-landau-terms">before</a>, trying to gauge the dangers of abusing asymptotics notation in arithmetics, with mixed success.</p>\n\n<p>Now, <a href="http://cs.stackexchange.com/a/2803/98">over here</a> our recurrence guru <a href="http://cs.stackexchange.com/a/2803/98">JeffE</a> does essentially this:</p>\n\n<p>$\\qquad \\displaystyle \\sum_{i=1}^n \\Theta\\left(\\frac{1}{i}\\right) = \\Theta(H_n)$</p>\n\n<p>While the end result is correct, I think this is wrong. Why? If we add in all the existence of constants implied (only the upper bound), we have</p>\n\n<p>$\\qquad \\displaystyle \\sum_{i=1}^n c_i \\cdot \\frac{1}{i} \\leq c \\cdot H_n$.</p>\n\n<p>Now how do we compute $c$ from $c_1, \\dots, c_n$? The answer is, I believe, that we can not: $c$ has to bound for all $n$ but we get <em>more</em> $c_i$ as $n$ grows. We don\'t know anything about them; $c_i$ may very well depend on $i$, so we can not assume a bound: a finite $c$ may not exist.</p>\n\n<p>In addition, there is this subtle issue of which variable goes to infinity on the left-hand side -- $i$ or $n$? Both? If $n$ (for the sake of compatibility), what is the meaning of $\\Theta(1/i)$, knowing that $1 \\leq i \\leq n$? Does it not only mean $\\Theta(1)$? If so, we can\'t bound the sum better than $\\Theta(n)$.</p>\n\n<p>So, where does that leave us? It it a blatant mistake? A subtle one? Or is it just the usual abuse of notation and we should not look at $=$ signs like this one out of context? Can we formulate a (rigorously) correct rule to evalutate (certain) sums of Landau terms?</p>\n\n<p>I think that the main question is: what is $i$? If we consider it constant (as it <em>is</em> inside the scope of the sum) we can easily build counterexamples. If it is not constant, I have no idea how to read it.</p>\n', 'ViewCount': '243', 'Title': 'Sums of Landau terms revisited', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-01T14:11:11.147', 'LastEditDate': '2012-07-18T22:01:47.640', 'AnswerCount': '3', 'CommentCount': '19', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2012-07-18T15:35:15.237', 'FavoriteCount': '1', 'Id': '2814'},90_21:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '371', 'Title': 'Do non-computable functions grow asymptotically larger?', 'LastEditDate': '2012-07-29T18:23:35.940', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2157', 'FavoriteCount': '1', 'Body': "<p>I read about busy beaver numbers and how they grow asymptotically larger than any computable function. Why is this so? Is it because of the busy beaver function's non-computability? If so, then do all non-computable functions grow asymptotically larger than computable ones?</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>Great answers below but I would like to explain in plainer english what I understand of them.</p>\n\n<p>If there was a computable function f that grew faster than the busy beaver function, then this means that the busy beaver function is bounded by f. In other words, a turing machine would simply need to run for f(n) many steps to decide the halting problem. Since we know the halting problem is undecidable, our initial presupposition is wrong. Therefore, the busy beaver function grows faster than all computable functions.</p>\n", 'Tags': '<computability><asymptotics>', 'LastEditorUserId': '2157', 'LastActivityDate': '2012-07-29T18:23:35.940', 'CommentCount': '2', 'AcceptedAnswerId': '2940', 'CreationDate': '2012-07-28T13:58:25.233', 'Id': '2939'},90_22:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '461', 'Title': 'Solving the recurrence relation $T(n) = 2T(\\lfloor n/2 \\rfloor) + n$', 'LastEditDate': '2012-07-31T20:30:24.733', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '778', 'FavoriteCount': '1', 'Body': '<p>Solving the recurrence relation $T(n) = 2T(\\lfloor n/2 \\rfloor) + n$.<br>\nThe book from which this example is, falsely claims that $T(n) = O(n)$ by guessing $T(n) \\leq cn$ and then arguing  </p>\n\n<p>$\\qquad \\begin{align*} T(n) &amp; \\leq 2(c \\lfloor n/2 \\rfloor ) + n \\\\ &amp;\\leq cn +n \\\\ &amp;=O(n) \\quad \\quad \\quad \\longleftarrow \\text{ wrong!!} \\end{align*}$  </p>\n\n<p>since $c$ is constant.The error is that we have not proved the <em>exact</em> form of the inductive hypothesis.</p>\n\n<p>Above I have exactly quoted what the book says. Now my question is why cannot we write $cn+n=dn$ where $d=c+1$ and now we have $T(n) \\leq dn$ and hence $T(n) = O(n)$?</p>\n\n<p>Note: </p>\n\n<ol>\n<li>The correct answer is $T(n) =O(n \\log n).$  </li>\n<li>The book I am referring here is <em>Introduction to algorithms</em> by Cormen et al., page 86, 3rd edition.</li>\n</ol>\n', 'Tags': '<proof-techniques><asymptotics><recurrence-relation><landau-notation><induction>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-01T07:09:01.457', 'CommentCount': '1', 'AcceptedAnswerId': '2972', 'CreationDate': '2012-07-31T18:15:32.510', 'Id': '2971'},90_23:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '802', 'Title': 'What is the meaning of $O(m+n)$?', 'LastEditDate': '2012-08-13T22:22:38.217', 'AnswerCount': '2', 'Score': '26', 'OwnerDisplayName': 'Frank', 'PostTypeId': '1', 'OwnerUserId': '9667', 'FavoriteCount': '5', 'Body': '<p>This is a basic question, but I\'m thinking that $O(m+n)$ is the same as $O(\\max(m,n))$, since the larger term should dominate as we go to infinity? Also, that would be different from $O(\\min(m,n))$. Is that right? I keep seeing this notation, especially when discussing graph algorithms. For example, you routinely see: $O(|V| + |E|)$ (e.g. see <a href="http://algs4.cs.princeton.edu/41undirected/">here</a>).</p>\n', 'Tags': '<terminology><asymptotics><mathematical-analysis><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-13T22:22:38.217', 'CommentCount': '1', 'AcceptedAnswerId': '3150', 'CreationDate': '2012-08-13T15:29:47.517', 'Id': '3149'},90_24:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ be a continuous positive function, where $f(n)$ is integer for each integer $n$. Prove or disprove whether the following always holds:</p>\n\n<p>$\\qquad f(n+1) = \\Theta(f(n))$</p>\n', 'ViewCount': '301', 'Title': 'Asymptotic growth rate of $f(n)$ and $f(n+1)$', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-17T16:01:14.057', 'LastEditDate': '2012-08-17T09:40:13.643', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '5', 'OwnerDisplayName': 'Anirban Ghosh', 'PostTypeId': '1', 'OwnerUserId': '2556', 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'CreationDate': '2012-08-16T19:01:06.800', 'Id': '3231'},90_25:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So I have this question to prove a statement:</p>\n\n<p>$O(n)\\subset\\Theta(n)$...</p>\n\n<p>I don't need to know how to prove it, just that in my mind this makes no sense and I think it should rather be that $\\Theta(n)\\subset O(n)$. </p>\n\n<p>My understanding is that $O(n)$ is the set of all functions who do no worse than $n$ while $\\Theta(n)$ is the set of all functions that do no better and no worse than n. </p>\n\n<p>Using this, I can think of the example of a constant function say $g(n)=c$. This function will surely be an element of $O(n)$ as it will do no worse than $n$ as $n$ approaches a sufficiently large number. </p>\n\n<p>However, the same function $g$ would not be an element of $\\Theta(n)$ as g does do better than $n$ for large $n$... Then since $g  \\in  O(n)$ and $g \\not\\in \\Theta(n)$, then $O(n)\\not\\in\\Theta(n)$  </p>\n\n<p>So is the question perhaps wrong ? I've learnt it is dangerous to make that assumption and usually I have missed something, I just can't see what it might be in this case. </p>\n\n<p>Any thoughts ? \nThanks a lot.. </p>\n", 'ViewCount': '205', 'Title': 'Is $O$ contained in $\\Theta$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-18T06:21:28.647', 'LastEditDate': '2012-08-18T06:21:28.647', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '3247', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2562', 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'CreationDate': '2012-08-17T14:53:47.270', 'Id': '3240'},90_26:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>what is the complexity of below relation</p>\n\n<p>$ T(n) = 2*T(\\sqrt n) + \\log n$</p>\n\n<p>and $T(2) = 1$</p>\n\n<p>Is it $\\Theta (\\log n * \\log \\log n)$ ?</p>\n', 'ViewCount': '213', 'Title': 'what is the complexity of recurrence relation?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-25T17:33:28.563', 'LastEditDate': '2012-08-30T04:58:10.133', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '3363', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2667', 'Tags': '<asymptotics><recurrence-relation><mathematical-analysis>', 'CreationDate': '2012-08-29T11:06:38.823', 'Id': '3362'},90_27:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In Big Theta notation used for defining the running time of an algorithm, are the constants $c_1$ and $c_2$ <em>different</em> for every value of $n$?</p>\n\n<p>Definition:</p>\n\n<p>$\\qquad \\displaystyle \\Theta (g(n)) = \\{ f(n): \\exists\\, c_1,c_2,n_0&gt;0. \\forall n \\geq n_0.\\ \\ c_1g(n) \\leq f(n) \\leq c_2g(n) \\}.$</p>\n', 'ViewCount': '216', 'Title': 'Value of constants in Big Theta notation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-10T14:08:00.560', 'LastEditDate': '2012-09-08T21:39:00.347', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'sai', 'PostTypeId': '1', 'Tags': '<terminology><asymptotics><mathematical-analysis><landau-notation>', 'CreationDate': '2012-09-08T05:00:24.313', 'Id': '3470'},90_28:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '862', 'Title': 'Changing variables in recurrence relations', 'LastEditDate': '2012-09-09T23:16:22.610', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'goooser', 'PostTypeId': '1', 'OwnerUserId': '2768', 'FavoriteCount': '1', 'Body': '<p>Currently, I am self-studying Intro to Algorithms (CLRS) and there is one particular method they outline in the book to solve recurrence relations. </p>\n\n<p>The following method can be illustrated with this example. Suppose we have the recurrence</p>\n\n<p>$$T(n) = 2T(\\sqrt n) + \\log n$$</p>\n\n<p>Initially they make the substitution m = lg(n), and then plug it back in to the recurrence and get:</p>\n\n<p>$$T(2^m) = 2T(2^{\\frac{m}{2}}) + m$$</p>\n\n<p>Up to this point I understand perfectly. This next step is the one that\'s confusing to me.</p>\n\n<p>They now "rename" the recurrence $S(m)$ and let $S(m) = T(2^m)$, which apparently produces</p>\n\n<p>$$S(m) = 2S(m/2) + m$$</p>\n\n<p>For some reason it\'s not clear to me why this renaming works, and it just seems like cheating. Can anyone explain this better? </p>\n', 'Tags': '<asymptotics><recurrence-relation><mathematical-analysis><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-09T23:16:22.610', 'CommentCount': '0', 'CreationDate': '2012-09-09T20:03:08.237', 'Id': '3482'},90_29:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '253', 'Title': 'Complexity inversely propotional to $n$', 'LastEditDate': '2012-09-11T01:36:36.780', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1272', 'FavoriteCount': '1', 'Body': '<p>Is it possible an algorithm complexity decreases by input size? Simply $O(1/n)$ possible?</p>\n', 'Tags': '<algorithms><time-complexity><asymptotics><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-11T01:36:36.780', 'CommentCount': '2', 'AcceptedAnswerId': '3497', 'CreationDate': '2012-09-10T15:09:25.010', 'Id': '3495'},90_30:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '620', 'Title': 'Solving $T(n)= 3T(\\frac{n}{4}) + n\\cdot \\lg(n)$ using the master theorem', 'LastEditDate': '2013-09-28T12:36:33.403', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1758', 'FavoriteCount': '1', 'Body': '<p><em>Introduction to Algorithms</em>, 3rd edition (p.95) has an example of how to solve the recurrence</p>\n\n<p>$$\\displaystyle T(n)= 3T\\left(\\frac{n}{4}\\right) + n\\cdot \\log(n)$$</p>\n\n<p>by applying the Master Theorem.</p>\n\n<p>I am very confused by how it is done. So, $a=3, b=4, f(n) = n\\cdot \\log(n)$<br>\nFirst step is to compare $n^{\\log_b a} = n^{\\log_4 3}= O(n^{0.793})$ with $f(n)$.</p>\n\n<p>I have no clue on how they compared this. The book explains: </p>\n\n<blockquote>\n  <p>$f(n) = \\Omega (n^{\\log_4 3+\\epsilon })$, where $\\epsilon \\approx 0.2$, case 3 applies if we can show that the regularity condition holds for $f(n).$ </p>\n</blockquote>\n\n<p>Followed by: </p>\n\n<blockquote>\n  <p>For sufficiently large n, we have that: $af\\left(\\frac{n}{b}\\right) = 3\\left(\\frac{n}{4}\\right)\\log\\left(\\frac{n}{5}\\right) \\le\\left(\\frac{3}{4}\\right)n \\log n = cf(n)~ for~ c=\\frac{3}{4}.$</p>\n</blockquote>\n\n<p>Where did $3\\left(\\frac{n}{4}\\right)$ come from?</p>\n', 'Tags': '<asymptotics><recurrence-relation><landau-notation><mathematical-analysis><master-theorem>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-28T12:36:33.403', 'CommentCount': '0', 'AcceptedAnswerId': '3505', 'CreationDate': '2012-09-11T03:32:38.143', 'Id': '3504'},90_31:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1816', 'Title': 'n*log n and n/log n against polynomial running time', 'LastEditDate': '2012-09-16T06:11:06.343', 'AnswerCount': '6', 'Score': '9', 'OwnerDisplayName': 'mihsathe', 'PostTypeId': '1', 'OwnerUserId': '2957', 'FavoriteCount': '2', 'Body': '<p>I understand that $\\Theta(n)$ is faster than $\\Theta(n\\log n)$ and slower than $\\Theta(n/\\log n)$. What is difficult for me to understand is how to actually compare $\\Theta(n \\log n)$ and $\\Theta(n/\\log n)$ with $\\Theta(n^f)$ where $0 &lt; f &lt; 1$.</p>\n\n<p>For example, how do we decide $\\Theta(n/\\log n)$ vs. $\\Theta(n^{2/3})$ or $\\Theta(n^{1/3})$</p>\n\n<p>I would like to have some directions towards proceeding in such cases. Thank you.</p>\n', 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-16T06:11:06.343', 'CommentCount': '0', 'AcceptedAnswerId': '3564', 'CreationDate': '2012-09-08T03:33:33.247', 'Id': '3563'},90_32:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m confused to conclude the recursion tree method a guess for the next recurrence:\n$$T(n)=3T\\left (\\left\\lfloor \\frac{n}{2}\\right \\rfloor\\right) +n$$\nI write some costs for the levels of tree, you can see, but I\'m confusing in the final guess. I know for the master theorem that the answer for the guess is like that \n$$\\Theta(n^{\\log_2 3}) $$ but in the steps by tree and I don\'t belive, you can see my error (?). How can I finished or know the outcome for this method to calculate $T(n)$? \nthanks,\n<img src="http://i.stack.imgur.com/T7vlE.png" alt="enter image description here">\n<img src="http://i.stack.imgur.com/V00qK.png" alt="Recursion Tree by the my problem">\nAnd the final conclusion is (?)\n$$=2 n^{\\log_2 3}+\\Theta(n^{\\log_2 3})\\leq cn^{\\log_2 3} \\rightarrow \\ \\ T(n)\\in\\Theta(n^{\\log_2 3})$$ with $c=3$ ?</p>\n', 'ViewCount': '124', 'Title': 'Doubt with a problem of grown functions and recursion tree', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-22T15:42:55.307', 'LastEditDate': '2012-09-17T17:48:41.503', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2012-09-17T02:09:19.717', 'Id': '4583'},90_33:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Substation method fails to prove that $T(n)=\\Theta(n^2) $ for the recursion $T(n)=4T(n/2) + n^2$, since you end up with $T(n) &lt; cn^2 \\leq cn^2 + n^2$.</p>\n\n<p>I don't understand how to subtract off lower-order term to prove that substitution works.  </p>\n\n<p>Came up with: $T(n) \\leq cn^2 - bn^2$</p>\n\n<p>Assume it holds for $T(n/2) \\leq c(n/2)^2 - b(n/2)^2$  </p>\n\n<p>$T(n) \\leq 4(c(n/2)^2 - b(n/2)^2) + n^2 = cn^2- bn^2 + n^2 $ </p>\n\n<p>However, there is no way to solve $cn^2- bn^2 + n^2 \\leq cn^2 - bn^2 $ for $b$</p>\n", 'ViewCount': '455', 'Title': 'Subtracting lower-order term to prove subtitution method works', 'LastEditorUserId': '2499', 'LastActivityDate': '2012-09-26T03:13:49.490', 'LastEditDate': '2012-09-19T12:00:43.927', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '4733', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1758', 'Tags': '<asymptotics><recurrence-relation><landau-notation>', 'CreationDate': '2012-09-19T07:43:01.930', 'Id': '4612'},90_34:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m going through the MIT Online Course Videos on Intro. to Algorithms at <a href="http://video.google.com/videoplay?docid=-3860843648161712896" rel="nofollow">here</a> at around 38:00.</p>\n\n<p>So we have a recursion formula</p>\n\n<p>$\\qquad T(n) = T(n/10) + T(9n/10) + O(n)$</p>\n\n<p>If we build a recursion tree it looks like </p>\n\n<pre><code>                   T(n)                     -- Level 1       = c*n\n             /               \\\n       T(n/10)             T(9n/10)         -- Level 2       = c*n\n        /   \\             /         \\\n T(n/100)  T(9n/100) T(9n/100)  T(81n/100)  -- Level 3       = c*n\n\n   /                                  \\                      &lt;= c*n\n    .                                .\n    .                                .\n 0(1)                                 0(1)\n</code></pre>\n\n<p>where $c$ is a constant larger than $0$.</p>\n\n<p>Shortest path from the root to the leaf is $\\log_{10}(n)$.</p>\n\n<p>Longest path from the root to the leaf is $\\log_{10/9}(n)$</p>\n\n<p>Therefore, the cost could be calculated as Cost = Cost of each level * number of levels.</p>\n\n<p>With the shortest path cost, we get a lower bound of $cn\\log_{10}(n)$, and with the  longest path cost an upper bound of  $cn\\log_{10/9}(n)$.</p>\n\n<p>And now I have to add the costs of leaf nodes, which leads to my problem. In the video it says the total number of leaves is in $\\Theta(n)$. I have trouble figuring out how he got to $\\Theta(n)$.</p>\n\n<p>The video further says $T(n)$ is bounded by</p>\n\n<p>$\\qquad cn\\log_{10}(n) + O(n) \\leq T(n) \\leq cn\\log_{10/9}(n) + O(n)$</p>\n\n<p>Wouldn\'t it make more sense to say it\'s </p>\n\n<p>$\\qquad cn\\log_{10}(n) + O(n^{\\log_{10}(2)}) \\leq T(n) \\leq cn\\log_{10/9}(n) + O(n^{\\log_{10/9}(2)})$</p>\n\n<p>where $\\Theta(n^{log_{10}(2)})$ represents the leaves on the left and $\\Theta(n^{\\log_{10/9}(2)})$ represents the leaves on the right.</p>\n\n<p>Or is there a way to simplify these terms to $\\Theta(n)$? </p>\n', 'ViewCount': '381', 'Title': 'Finding the number of leaves in a imbalanced recursion tree', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-21T21:08:01.437', 'LastEditDate': '2012-09-21T21:05:43.703', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '4657', 'Score': '1', 'OwnerDisplayName': 'user1671022', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation>', 'CreationDate': '2012-09-14T10:17:19.653', 'Id': '4656'},90_35:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am learning algorithms. So, I came along with something very interesting.</p>\n\n<p>The asymptotic bound of linear function $an+b$ is $O(n^2)$ for all $a&gt;0$.</p>\n\n<p>This is same as for $an^2 + bn + c$. But shouldn't it be different?</p>\n", 'ViewCount': '82', 'Title': 'Why bound of linear function is same as that of quadratic equation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-29T22:12:00.863', 'LastEditDate': '2012-09-29T20:29:01.790', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3010', 'Tags': '<algorithms><asymptotics>', 'CreationDate': '2012-09-29T18:06:56.773', 'Id': '4796'},90_36:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What would the order of growth for this loop be:</p>\n\n<pre><code>int sum = 0;\nfor (int n = N; n &gt; 0; n /= 2)\n          for(int i = 0; i &lt; n; i++)\n             sum++;\n</code></pre>\n\n<p>The first loop seems to run for $\\log N + 1$ times and the second loop runs $n$ times.\nSo is the correct answer $O(n \\log n)$?</p>\n', 'ViewCount': '803', 'Title': 'The order of growth analysis of simple for loop', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-03T10:40:26.673', 'LastEditDate': '2012-10-02T17:33:25.097', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'Rick', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><asymptotics>', 'CreationDate': '2012-09-28T18:11:59.537', 'Id': '4800'},90_37:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '233', 'Title': 'Definition of $\\Theta$ for negative functions', 'LastEditDate': '2012-09-30T21:45:34.657', 'AnswerCount': '2', 'Score': '6', 'OwnerDisplayName': 'Ockham', 'PostTypeId': '1', 'OwnerUserId': '3017', 'FavoriteCount': '1', 'Body': "<p>I'm working out of the 3rd edition CLRS Algorithms textbook and in Chapter 3 a discussion begins about asymptotic notation which starts with $\\Theta$ notation. I understood the beginning definition of:</p>\n\n<p>$$\\Theta(g(n)) = \\{ f(n)\\,|\\, \\exists\\, c_1, c_2 &gt; 0, n_0 \\in \\mathbb{N}: 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n)\\ \\ \\forall n \\geq n_0\\}$$</p>\n\n<p>But then on the next page the text says that:</p>\n\n<blockquote>\n  <p>The definition of $\\Theta(g(n))$ requires that every member $f(n) \\in \\Theta(g(n))$ be asymptotically nonnegative, that is, that $f(n)$ be nonnegative whenever $n$ is sufficiently large. (An asymptotically positive function is one that is positive for all sufficiently large $n$.) Consequently, the function g(n) itself must be asymptotically nonnegative, or else the set $\\Theta(g(n))$ is empty.</p>\n</blockquote>\n\n<p>That last part about the how if the function is negative the set $\\Theta(g(n))$ is empty and the general requirement of a positive function is sort of confusing. Can anyone out there clarify this definition for me and what it means, possible with an example, it would be much appreciated.</p>\n", 'Tags': '<terminology><asymptotics>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-30T21:51:26.507', 'CommentCount': '2', 'AcceptedAnswerId': '4820', 'CreationDate': '2012-09-30T15:53:07.287', 'Id': '4818'},90_38:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to prove the following lemma:</p>\n\n<p>$c$ is a positive real number and $f, g$ are functions from natural numbers to non-negative real numbers. I'm trying to prove rigorously that:</p>\n\n<p>$\\Omega(cf(n))$ = $\\Omega(f(n))$.</p>\n\n<p>I know that it is obvious but I'm trying to construct a proof that is as complete as possible. My current approach is like this:</p>\n\n<p>This lemma is equivalent to saying that: $f(n) \\in \\Omega(cf(n))$ iff $cf(n) \\in \\Omega(f(n))$.</p>\n\n<p>We can also restate that as:</p>\n\n<ol>\n<li>If $t(n) \\in \\Omega(cf(n))$ then $t(n) \\in \\Omega(f(n))$.</li>\n<li>If $t(n) \\in \\Omega(f(n))$ then $t(n) \\in \\Omega(cf(n))$.</li>\n</ol>\n\n<p>For 1.,</p>\n\n<p>$(*)$ $\\exists d_1, d_2 \\gt 0, \\forall n \\gt n_0, n_1, \\forall n \\in N$:</p>\n\n<p>$t(n) \\ge d_1cf(n)$ and $t(n) \\ge d_2f(n)$</p>\n\n<p>Now let's fix $d_1, d_2$ and $n_0, n_1$ to be any constants that fulfils $(*)$, such that:\n$n'=max\\{n_0, n_1\\}$ and $d_1c \\ge d_2$, using this we can say that:</p>\n\n<p>$t(n) \\ge d_1cf(n) \\ge d_2f(n)$ and hence the 1. is satisfied because $t(n) \\in \\Omega(cf(n)), \\Omega(f(n))$. The proof of 2. is mutatis mutandis.</p>\n\n<p>Do I have a mistake in my proof, is there a better/more elegant way to prove this lemma? Shortly how can I improve this?</p>\n", 'ViewCount': '141', 'Title': 'Proving $\\Omega(cf) = \\Omega(f)$', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-03T11:34:57.393', 'LastEditDate': '2012-10-02T17:45:39.213', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2956', 'Tags': '<asymptotics><proof-techniques><check-my-proof>', 'CreationDate': '2012-10-02T12:45:11.833', 'Id': '4840'},90_39:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I have been reading <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow"><em>Introduction to Algorithms</em> by Cormen et al.</a> and I\'m reading <a href="http://books.google.co.uk/books?id=NLngYyWFl_YC&amp;pg=PA73" rel="nofollow">the statement of the Master theorem starting on page 73</a>. In case 3 there is also a regularity condition that needs to be satisfied to use the theorem:</p>\n\n<blockquote>\n  <p>... 3. If </p>\n  \n  <p>$\\qquad \\displaystyle f(n) = \\Omega(n^{\\log_b a + \\varepsilon})$</p>\n  \n  <p>for some constant $\\varepsilon &gt; 0$ and if</p>\n  \n  <p>$\\qquad \\displaystyle af(n/b) \\leq cf(n)$ \xa0\xa0\xa0\xa0\xa0[<strong>this is the regularity condition</strong>]</p>\n  \n  <p>for some constant $c &lt; 1$ and for all sufficiently large $n$, then .. </p>\n</blockquote>\n\n<p>Can someone tell me why the regularity condition is needed? How does the theorem fail if the condition is not satisfied?</p>\n', 'ViewCount': '2056', 'Title': 'Why is there the regularity condition in the master theorem?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-04T09:41:58.807', 'LastEditDate': '2012-10-04T09:41:58.807', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '7', 'OwnerDisplayName': 'GrowinMan', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><mathematical-analysis><landau-notation><master-theorem>', 'CreationDate': '2012-10-02T03:54:52.210', 'FavoriteCount': '2', 'Id': '4854'},90_40:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I face a problem with computing a complexity. \nI have this equality : $P(u) = (\\sqrt{u}+1)P(\\sqrt{u}) + \\theta(\\sqrt{u})$</p>\n\n<p>And I want to prove that $P(u) = O(u)$</p>\n\n<p>This is how I process :</p>\n\n<p>I put $m = \\lg\\lg u \\implies P(u) = P(2^{2^{m}}) = (2^{2^{m-1}}+1)P(2^{2^{m-1}}) + \\theta(2^{2^{m-1}})$</p>\n\n<p>Now, I consider $S(m)$ that is : $S(m) = P(2^{2^{m}}) = mS(m-1) + \\theta(m-1)$</p>\n\n<p>And here I have a problem. I obtain a factorial complexity and I don't know how to integrate $\\lg$ to prove the equality $P(u) = O(u)$</p>\n\n<p>Some advice ?</p>\n", 'ViewCount': '79', 'Title': 'Recursive complexity with change of variable', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-04T09:59:27.583', 'LastEditDate': '2012-10-04T09:59:27.583', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '4868', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3050', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2012-10-03T20:46:34.207', 'Id': '4865'},90_41:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let\'s say I have a graph $|G|$ with $|E|=O(V^2)$ edges. I want to run BFS on $G$ which has a running time of $O(V+E)$.</p>\n\n<p>It feels natural to write that the running time on this graph would be $O(O(V^2)+V)$ and then simplify to $O(V^2)$.</p>\n\n<p>Are there any pitfalls to using such a "remove-the-nested-O" shortcut (not just in this case, but more generally)?</p>\n', 'ViewCount': '329', 'Title': 'Nested Big O-notation', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-10-09T21:54:10.183', 'LastEditDate': '2012-10-07T14:31:31.387', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '4919', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2012-10-07T07:26:22.087', 'Id': '4913'},90_42:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Prove or disprove the following statements:</p>\n\n<ol>\n<li><p>$T\\left( n \\right) = 2T\\left( {\\frac{n}{2}} \\right) + f\\left( n \\right),f\\left( n \\right) = \\theta \\left( {{n^2}} \\right) $ then $ {\\rm{ }}T\\left( n \\right) = \\theta \\left( {f\\left( n \\right)} \\right) $ for all $ {\\rm{ n = }}{{\\rm{2}}^k}$</p></li>\n<li><p>$T\\left( n \\right) = 2T\\left( {\\frac{n}{2}} \\right) + f\\left( n \\right),f\\left( n \\right) = \\Omega \\left( {{n^2}} \\right) $ then $ {\\rm{ }}T\\left( n \\right) = O\\left( {f\\left( n \\right)} \\right)$ for all $ {\\rm{ n = }}{{\\rm{2}}^k}$</p></li>\n</ol>\n\n<p>I think I should use the third case of the master theorem to check these equations.</p>\n\n<p>But I have not been able to check this constraint for these inequations:</p>\n\n<p>$\\qquad af\\left( {\\frac{n}{b}} \\right) \\le cf\\left( n \\right)$</p>\n\n<p>How do I do that?</p>\n', 'ViewCount': '236', 'Title': 'Problems showing the constraint of master theorem case three holds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-06T10:31:29.743', 'LastEditDate': '2012-10-08T10:36:30.260', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '240', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2012-10-08T09:10:03.237', 'FavoriteCount': '2', 'Id': '4945'},90_43:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '299', 'Title': 'How to prove $(n+1)! = O(2^{(2^n)})$', 'LastEditDate': '2012-10-16T08:03:18.780', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3004', 'FavoriteCount': '2', 'Body': "<p>I am trying to prove $(n+1)! = O(2^{(2^n)})$. I am trying to use L'Hospital rule but I am stuck with infinite derivatives.</p>\n\n<p>Can anyone tell me how i can prove this?</p>\n", 'Tags': '<asymptotics><mathematical-analysis><landau-notation>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-28T07:23:20.603', 'CommentCount': '1', 'AcceptedAnswerId': '6080', 'CreationDate': '2012-10-14T22:13:27.907', 'Id': '6075'},90_44:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Here is my proof, but I am not sure whether it is correct.</p>\n\n<p>We know:</p>\n\n<p>$\\qquad \\begin{array}{l}\n \\forall {c_1},\\exists {n_1},0 \\le f\\left( n \\right) \\le {c_1}g\\left( n \\right),\\forall n \\ge {n_1} \\\\ \n \\forall {c_1},\\exists {n_2},0 \\le g\\left( n \\right) \\le {c_1}h\\left( n \\right),\\forall n \\ge {n_2} \\\\ \n \\end{array}$</p>\n\n<p>Hope to prove:</p>\n\n<p>$\\qquad \\begin{array}{l}\n \\forall c,\\exists {n_0},0 \\le f\\left( n \\right) \\le ch\\left( n \\right),\\forall n \\ge {n_0} \\\\ \n  \\Rightarrow 0 \\le f\\left( n \\right) \\le {c_1}g\\left( n \\right) \\le {c_1}\\left( {{c_1}h\\left( n \\right)} \\right),\\forall n \\ge \\max \\left\\{ {{n_1},{n_2}} \\right\\} \\\\ \n \\end{array}$</p>\n\n<p>Let $c = {c_1}^2$. Then,</p>\n\n<p>$\\qquad 0 \\le f\\left( n \\right) \\le {c^2}h\\left( n \\right),\\forall n \\ge {n_0} = \\max \\left\\{ {{n_1},{n_2}} \\right\\}$</p>\n\n<p>Is there any mistake?</p>\n', 'ViewCount': '240', 'Title': 'How to prove transitivity in small-o of asymptotic analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T11:12:16.707', 'LastEditDate': '2012-10-28T11:12:16.707', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '240', 'Tags': '<asymptotics><check-my-proof>', 'CreationDate': '2012-10-21T03:41:43.880', 'Id': '6207'},90_45:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have the following pseudo code:    </p>\n\n<pre><code>Multiply(y,z)    \n1. if (z==0) return 0     \n2. else if (z is odd)    \n3.    then return (Multiply(2y, floor(z/2)) + y )    \n4. else return (Multiply(2y, floor(z/2)))    \n</code></pre>\n\n<p>Towards analysing this procedure's runtime, this recurrence relation is given as answer:</p>\n\n<p>$\\qquad \\displaystyle T(z) = \\begin{cases} 0 &amp; z=0 \\\\ T(z/2)+1 &amp; z&gt;0\\end{cases}$ </p>\n\n<p>Why is $T(z)=0$ when $z=0$? Shouldn't it be $1$ for this case?        </p>\n\n<p>And, the $+1$ in $T(z/2)\\mathbf{+1}$ is because the worst case is \n<code>(multiply(2y, floor(z/2)) + y</code> (note the <code>+ y</code>). Am I correct?     </p>\n", 'ViewCount': '128', 'Title': 'How does this recurrence relation fit the algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-29T00:23:43.783', 'LastEditDate': '2012-10-28T11:02:22.307', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4379', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><recurrence-relation>', 'CreationDate': '2012-10-28T01:06:28.050', 'Id': '6345'},90_46:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '957', 'Title': 'Solving a recurrence relation with $\\sqrt{n}$ as parameter', 'LastEditDate': '2012-10-31T23:17:42.550', 'AnswerCount': '3', 'Score': '6', 'OwnerDisplayName': 'KodeSeeker', 'PostTypeId': '1', 'OwnerUserId': '1510', 'Body': "<p>Let $T(n) = \\sqrt{n} T(\\sqrt{n}) + c\\,n$ for $n \\gt 2$ and some positive constant $c$ and $T(2) = 1$.</p>\n\n<p>I know the Master theorem, but I'm not sure as to how we could solve this relation using it. Any insight would be helpful.</p>\n", 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-23T12:49:26.543', 'CommentCount': '5', 'AcceptedAnswerId': '6425', 'CreationDate': '2012-10-30T22:28:19.830', 'Id': '6410'},90_47:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '439', 'Title': 'How asymptotically bad is naive shuffling?', 'LastEditDate': '2012-11-06T20:49:40.330', 'AnswerCount': '2', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '242', 'FavoriteCount': '3', 'Body': "<p>It's well-known that this 'naive' algorithm for shuffling an array by swapping each item with another randomly-chosen one doesn't work correctly:</p>\n\n<pre><code>for (i=0..n-1)\n  swap(A[i], A[random(n)]);\n</code></pre>\n\n<p>Specifically, since at each of $n$ iterations, one of $n$ choices is made (with uniform probability), there are $n^n$ possible 'paths' through the computation; because the number of possible permutations $n!$ doesn't divide evenly into the number of paths $n^n$, it's impossible for this algorithm to produce each of the $n!$ permutations with equal probability.  (Instead, one should use the so-called <em>Fischer-Yates</em> shuffle, which essentially changes out the call to choose a random number from [0..n) with a call to choose a random number from [i..n); that's moot to my question, though.)</p>\n\n<p>What I'm wondering is, how 'bad' can the naive shuffle be?  More specifically, letting $P(n)$ be the set of all permutations and $C(\\rho)$ be the number of paths through the naive algorithm that produce the resulting permutation $\\rho\\in P(n)$, what is the asymptotic behavior of the functions </p>\n\n<p>$\\qquad \\displaystyle M(n) = \\frac{n!}{n^n}\\max_{\\rho\\in P(n)} C(\\rho)$ </p>\n\n<p>and </p>\n\n<p>$\\qquad \\displaystyle m(n) = \\frac{n!}{n^n}\\min_{\\rho\\in P(n)} C(\\rho)$?  </p>\n\n<p>The leading factor is to 'normalize' these values: if the naive shuffle is 'asymptotically good' then </p>\n\n<p>$\\qquad \\displaystyle \\lim_{n\\to\\infty}M(n) = \\lim_{n\\to\\infty}m(n) = 1$.  </p>\n\n<p>I suspect (based on some computer simulations I've seen) that the actual values are bounded away from 1, but is it even known if $\\lim M(n)$ is finite, or if $\\lim m(n)$ is bounded away from 0?  What's known about the behavior of these quantities?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics><probability-theory><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-10T04:07:42.843', 'CommentCount': '13', 'AcceptedAnswerId': '6596', 'CreationDate': '2012-11-06T19:22:56.410', 'Id': '6519'},90_48:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>While studying master method at recurrences topic I'm stacked at a point. It is written in the book as:</p>\n\n<blockquote>\n  <p>$T(n) = 3T(n/4) + n \\log n$,</p>\n  \n  <p>we have $a = 3, b = 4$, </p>\n  \n  <p>$f(n) = n \\log n$, and </p>\n  \n  <p>$n^{\\log_b(a)} = n^{\\log_4 3} = O(n^{0.793})$.</p>\n  \n  <p>Since  $f(n) = \\Omega(n^{\\log_4( 3)+\\varepsilon} )$, where  $\\varepsilon \\approx0.2$ ....</p>\n</blockquote>\n\n<p>The authors means that the $n\\log n = \\Omega(n)$. How will we know this? Is $n \\log n = \\Omega(n)$ true? Or \nsomething is wrong?</p>\n", 'ViewCount': '199', 'Title': 'Big Omega of $n \\log n$', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-09T08:28:06.703', 'LastEditDate': '2012-11-08T19:10:26.370', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user1308990', 'PostTypeId': '1', 'Tags': '<asymptotics><landau-notation><master-theorem>', 'CreationDate': '2012-10-22T10:26:01.877', 'Id': '6564'},90_49:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '390', 'Title': 'Confusion about big-O notation comparison of two functions', 'LastEditDate': '2012-11-15T13:23:16.190', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4601', 'FavoriteCount': '0', 'Body': '<p>On page 16 of this <a href="http://www.cs.berkeley.edu/~vazirani/algorithms/all.pdf" rel="nofollow">algorithms book</a>, it states:</p>\n\n<blockquote>\n  <p>For example, suppose we are choosing between two algorithms for a particular computational task. One takes $f_1(n) = n^2$ steps, while the other takes $f_2(n) = 2n + 20$ steps (Figure 0.2).</p>\n</blockquote>\n\n<p>He then goes on to say:</p>\n\n<blockquote>\n  <p>This superiority ... (of $f_2$ over $f_1$) ... is captured by the big-O notation: $f_2 = O(f_1)$, because ...</p>\n</blockquote>\n\n<p>Now my problem is that in the original quote, he said that $f_1(n) = n^2$ steps and $f_2(n) = 2n+20$ steps, so thus $f_1 = O(n^2)$ and $f_2 = O(n)$ (big-O is defined in Section 0.3). But the second quote above states $f_2 = O(f_1)$, which means $f_2 = O(n^2)$ and contradicts his definition of big-O notation. What have I missed?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-15T22:33:41.427', 'CommentCount': '2', 'AcceptedAnswerId': '6689', 'CreationDate': '2012-11-15T12:39:23.337', 'Id': '6676'},90_50:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A book I am reading demonstrates how $5n^3 + 2n^2 + 22n + 6 = O(n^3)$, which I believe is true. After all, there exists a value $c$ for which $cn^3$ is always greater than $5n^3 + 2n^2 + 22n + 6$ for all $n$ greater than or equal to some value $n_0$.</p>\n\n<p>However, the book then casually notes that $c = 5$ and $n_0 = 10$. Where did these values come from? What algebraic calculations were done (if any) to derive the $c$ and $n_0$ values?</p>\n', 'ViewCount': '78', 'Title': 'Finding $c$ and $n_0$ for a big-O bound', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-17T20:33:44.143', 'LastEditDate': '2012-11-17T08:26:08.783', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '6722', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-17T00:44:33.033', 'Id': '6700'},90_51:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Hopefully this is the right section. I need to prove that $2^{(log(n)^{1/2})}$ is $O(n^a)$. From the basic principle of Big-O notation, I know I need to find two numbers $c$ and $N$ so that $f(n) \\le c\\cdot g(n)$ for all values of $n \\ge N$. In this case, $f(n)$ is $2^{(log(n)^{1/2})}$, and $g(n)$ is $n^a$.</p>\n\n<p>The first problem in the set had me prove that $2^{n+a}$ is $O(2^n)$. For this problem, I had separated the $f(n)$ to $2^n \\cdot 2^a$. I was guided by another user to set $c = 2^a$. This way, with $g(n) = 2^n$, $cg(n) = 2^a2^n$, which is obviously $\\ge f(n)$, $2^{n+a}$, for all values of $n \\ge 0$. This problem, however, I don't find as easy.</p>\n\n<p>Could someone point me in the right direction? I've been trying to rearrange the problem and solve it for a while now.</p>\n", 'ViewCount': '83', 'Title': 'Prove that $2^{(log(n)^{1/2})}$ is $O(n^a)$', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-17T09:03:08.617', 'LastEditDate': '2012-11-17T09:03:08.617', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4622', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-17T03:57:13.407', 'FavoriteCount': '1', 'Id': '6702'},90_52:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '280', 'Title': 'When does $1.00001^n$ exceed $n^{100001}$?', 'LastEditDate': '2012-11-17T09:49:53.533', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '1', 'Body': '<p>I have been told than $n^{1000001} = O(1.000001^n)$. If that\'s the case, there must be some value $n$ at which $1.000001^n$ exceeds $n^{1000001}$.</p>\n\n<p>However, when I consult Wolfram Alpha, I get a negative value for when that occurs.\n<a href="http://www.wolframalpha.com/input/?i=1.000001%5Ex+%3D+x%5E1000001" rel="nofollow">http://www.wolframalpha.com/input/?i=1.000001%5Ex+%3D+x%5E1000001</a></p>\n\n<p>Why is that? Shouldn\'t this value be really big instead of negative?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-27T22:13:35.007', 'CommentCount': '2', 'AcceptedAnswerId': '6713', 'CreationDate': '2012-11-17T09:22:49.630', 'Id': '6711'},90_53:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm currently studying for an algorithms midterm in about 2 days and am reading from the beginning of the course, and stumbled upon this when I actually looked at the examples.</p>\n\n<p>The question equation: $f(n) = 6n^3 + n^2\\log n$</p>\n\n<p>The exact line written for the answer is: $f(n) \\leq 6n^3 + n^2 \\centerdot n \\text{, for all }n \\geq 1, \\text{since} \\log n \\leq n$</p>\n\n<p>First of all, I don't really see why the logarithm was removed or why it actually matters when the dominant piece is the $6n^3$. I also don't get why it's $n \\geq 1$ instead of $n \\geq 6$ (unless it's a continuation of the first one.</p>\n\n<p>Been staring at it for about 15 minutes and still not getting how it comes down to $n \\geq 1$. Would anybody be kind enough to give me a hint as to what's wrong?</p>\n", 'ViewCount': '179', 'Title': 'Why is this $f(n) \\leq 6n^3 + n^2 \\log n \\in O(n^3)$ for all $n \\geq 1$?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-22T08:08:46.570', 'LastEditDate': '2012-11-22T08:08:46.570', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '6826', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4681', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-21T18:47:36.573', 'Id': '6823'},90_54:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>$T(n)=2T(n/2) + n\\log^2(n)$.</p>\n\n<p>If I try to substitute $m = \\log(n)$ I end up with </p>\n\n<p>$T(2^m)=2 T(2^{m-1}) + 2^m\\log^{2}(2^m)$.</p>\n\n<p>Which isn't helpful to me. Any clues?</p>\n\n<p>PS. hope this isn't too localized. I specified that the problem was a squared logarithm which should make it possible to find for others wondering about the same thing.</p>\n", 'ViewCount': '187', 'Title': 'Solving recurrence with logarithm squared $T(n)=2T(n/2) + n \\log^2n$', 'LastEditorUserId': '472', 'LastActivityDate': '2012-11-22T14:51:07.147', 'LastEditDate': '2012-11-22T14:51:07.147', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '6834', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<asymptotics><proof-techniques><recurrence-relation>', 'CreationDate': '2012-11-22T09:18:57.203', 'Id': '6833'},90_55:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/2789/solving-or-approximating-recurrence-relations-for-sequences-of-numbers">Solving or approximating recurrence relations for sequences of numbers</a>  </p>\n</blockquote>\n\n\n\n<p>I know that the solution for $T(n) = 2 T(n/2) + O(n)$ is  $ T(n) = O(n \\log(n))$</p>\n\n<p>But how do you get to that point? I don\'t understand when it says put t into the equation repeatedly until it drops out...</p>\n\n<p>Any help?</p>\n', 'ViewCount': '880', 'ClosedDate': '2012-11-23T22:41:35.970', 'Title': 'Solving the big-Oh notation for $T(n) = 2 T(n/2) + O(n)$', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-23T22:17:04.403', 'LastEditDate': '2012-11-23T13:16:33.447', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'OwnerDisplayName': 'user1846486', 'PostTypeId': '1', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-11-23T09:08:32.463', 'Id': '6855'},90_56:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The title of the question expresses what I\'m looking for - this is to help me better understand the prerequisites for the <a href="http://en.wikipedia.org/wiki/Time_hierarchy_theorem#Non-deterministic_time_hierarchy_theorem">Non-Deterministic Time Hierarchy Theorem</a></p>\n\n<p>For instance, the Arora-Barak book explains the theorem using $g(n) = n$ and $G(n) = n^{1.5}$ - but, I can see that $n \\in o(n^{1.5})$ as well! So, I\'m trying to better understand what "extra" time is guaranteed by specifying that in order for $\\text{NTIME}(g(n))$ to be a proper subset of $\\text{NTIME}(G(n))$, $g(n + 1) = o(G(n))$, <strong>not</strong> $g(n) = o(G(n))$...  </p>\n', 'ViewCount': '127', 'Title': 'Two functions $g(n)$, $G(n)$ such that $g(n) = o(G(n))$ but $g(n+1) \\neq o(G(n))$', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-27T00:09:41.660', 'LastEditDate': '2012-11-26T22:05:53.567', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '6932', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '476', 'Tags': '<complexity-theory><time-complexity><asymptotics><landau-notation>', 'CreationDate': '2012-11-26T20:16:07.620', 'Id': '6929'},90_57:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '123', 'Title': 'How to guess the value of $j$ at the end of the loop?', 'LastEditDate': '2012-11-27T15:10:08.253', 'AnswerCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4732', 'FavoriteCount': '1', 'Body': '<pre><code>for ( i = n , j = 0 ; i &gt; 0 ; i = i / 2 , j = j + i ) ;\n</code></pre>\n\n<p>All variables are integers.(i.e. if decimal values occur, consider their floor value)</p>\n\n<p>Let $\\text{val}(j)$ denote the value of $j$, after the termination of the loop. Which of the following is true?</p>\n\n<p>(A)$\\quad \\text{val(j)} = \\Theta(\\log(n)) $ <br>\n(B)$\\quad \\text{val(j)} = \\Theta(\\sqrt n) $ <br>\n(C)$\\quad \\text{val(j)} = \\Theta(n) $ <br>\n(D)$\\quad \\text{val(j)} = \\Theta(\\log\\log n) $</p>\n\n<p>Please explain, is there any easy way to guess the value of $j$?</p>\n', 'Tags': '<algorithm-analysis><asymptotics><integers>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-28T00:56:50.520', 'CommentCount': '13', 'AcceptedAnswerId': '6963', 'CreationDate': '2012-11-27T11:02:16.417', 'Id': '6952'},90_58:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p><strong>Problem:</strong> Suppose $V$ is an <a href="http://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL tree</a> (a self-balancing binary search tree) of $n$\n  elements. After the insertion of $n^2$ elements, what would be its\n  height?</p>\n</blockquote>\n\n<p><strong>My idea:</strong> the height of an AVL tree is originally $O(\\log(n))$ where $n$ is the number of elements. After insertion of $n^2$ elements, its height will be:$$O(\\log(n+n^2))=O(\\log(n^2))=O(2\\log(n))=O(\\log(n))$$</p>\n\n<p>My answer would be $O(\\log(n))$ but I\'m having doubts.</p>\n\n<p>Why is the asymptotic complexity of the result the same despite the fact that there are more elements?</p>\n', 'ViewCount': '82', 'Title': 'Height of AVL after entries', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-29T07:36:40.940', 'LastEditDate': '2012-11-29T06:08:02.270', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7007', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<algorithms><asymptotics><binary-trees><search-trees>', 'CreationDate': '2012-11-28T15:22:50.797', 'Id': '6995'},90_59:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '428', 'LastEditorDisplayName': 'user742', 'Title': 'Why is $3^n = 2^{O(n)}$ true?', 'LastEditDate': '2012-12-03T08:39:00.760', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '1', 'Body': '<p>$3^n = 2^{O(n)}$ is apparently true. I thought that it was false though because $3^n$ grows faster than any exponential function with a base of 2.</p>\n\n<p>How is $3^n = 2^{O(n)}$ true?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastActivityDate': '2012-12-03T08:39:00.760', 'CommentCount': '3', 'AcceptedAnswerId': '7092', 'CreationDate': '2012-12-01T20:53:46.493', 'Id': '7091'},90_60:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Sorry if this question is very simplistic; I'm just starting out and I'm trying to wrap my head around all this asymptotic bound stuff. When trying to find the upper bound for the worst case of a function does it need to take into account what the meat of the code actually does? I have some code that would (in the worst case) iterate through a while loop n times, but when you consider what that code actually does, it would always make it so that the condition for the while loop becomes false on the next iteration.</p>\n\n<p>Some people say that it doesn't matter what is actually happening within the code; just that if it has the ability to iterate n times (even though it's virtually impossible because of the body of the loop) then that would be the worst case vs. however many steps the code ACTUALLY runs. </p>\n\n<p>If anyone has any insight it would be greatly appreciated! Thanks!</p>\n", 'ViewCount': '144', 'Title': 'Input to make worst case on big O not possible?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-06T06:06:04.333', 'LastEditDate': '2012-12-06T06:06:04.333', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4888', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2012-12-05T21:47:04.640', 'Id': '7198'},90_61:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The following is an excerpt from <a href="http://www.amazon.ca/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1355097851&amp;sr=1-1" rel="nofollow">CLRS</a>:</p>\n\n<blockquote>\n  <p>$\\Theta(g(n))= \\{ f(n) \\mid  \\text{ $\\exists c_1,c_2,n_0&gt;0$ such that $0 \\le c_1 g(n) \\le f(n) \\le c_2g(n)$ for all $n \\ge n_0$}\\}$.</p>\n</blockquote>\n\n<p>Assuming $n \\in \\mathbb{N}$, I was unable to find $f(n)$ and $g(n)$ such that the bound does not apply for all $n$.</p>\n\n<p><strong>Note:</strong> This question was asked with the flawed assumption that $f(n)$ and $g(n)$ necessarily have natural domains.</p>\n', 'ViewCount': '234', 'Title': 'If $f(n) = \\Theta(g(n))$, do both functions bound each other for all $n$ or only sufficiently large $n$?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-10T20:25:50.367', 'LastEditDate': '2012-12-10T12:17:23.270', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7282', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4267', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2012-12-10T00:31:06.167', 'Id': '7281'},90_62:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The following is an excerpt from <a href="http://www.amazon.ca/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844" rel="nofollow">CLRS</a>:</p>\n\n<blockquote>\n  <p>The definition of $g(n)$ requires that every member $f(n) \\in \\Theta(g(n))$ be asymptotically nonnegative, that is, that $f(n)$ be nonnegative whenever n is sufficiently large. (An asymptotically positive function is one that is positive for all sufficiently large $n$). Consequently, the function $g(n)$ itself must be asymptotically nonnegative, or else the set $g(n)$ is empty. </p>\n</blockquote>\n\n<p>Intuition suggests that having one function with a positive domain while the other with a negative one perverts the purpose of asymptotic analysis (a measure of the order of growth) as the positive function can be an upper asymptotic bound of the negative function simply on merit of it being positive, even in cases where the negative function grows faster.</p>\n\n<p>In cases where <em>both</em> functions have negative domains, asymptotic analysis would still be a valid measure of the order of growth, making the restriction of both functions having to be positive appear useless.</p>\n', 'ViewCount': '58', 'Title': 'Why does every member $f(n) \\in \\Theta(g(n))$, and $g(n)$ have to be asymptotically non-negative?', 'LastActivityDate': '2012-12-11T22:49:01.640', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7343', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4267', 'Tags': '<complexity-theory><asymptotics>', 'CreationDate': '2012-12-11T21:51:09.250', 'Id': '7340'},90_63:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How is asymptotic analysis (big o, little o, big theta, big theta etc.) defined for functions with multiple variables?</p>\n\n<p>I know that the Wikipedia article has a section on it, but it uses a lot of mathematical notation which I am unfamiliar with it. I also found the following paper: <a href="http://people.cis.ksu.edu/~rhowell/asymptotic.pdf">http://people.cis.ksu.edu/~rhowell/asymptotic.pdf</a> However the paper is very long and provides a complete analysis of asymptotic analysis rather than just giving a definition. Again the frequent usage of mathematical notation made it very hard to understand.</p>\n\n<p>Could someone provide a definition of asymptotic analysis <em>without</em> the complex mathematical notation?</p>\n', 'ViewCount': '198', 'Title': 'Asymptotic Analysis for two variables?', 'LastActivityDate': '2012-12-17T23:46:10.497', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7481', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '5105', 'Tags': '<complexity-theory><asymptotics>', 'CreationDate': '2012-12-17T23:13:43.127', 'FavoriteCount': '2', 'Id': '7480'},90_64:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '426', 'Title': 'Simplify complexity of n multichoose k', 'LastEditDate': '2013-01-05T23:36:04.090', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5271', 'FavoriteCount': '4', 'Body': "<p><strong>Edit:</strong>  In my case, $k$ may be greater than $n$ and they grow independently.</p>\n\n<p>I have a recursive algorithm with time complexity equivalent to choosing k elements from n with repetition, and I was wondering whether I could get a more simplified big-O expression.</p>\n\n<p>Specifically, I'd expect some explicit exponential expression.\nThe best I could find so far is that based on Stirling's approximation $O(n!) \\approx O((n/2)^n)$, so I can use that, but I wondered if I could get anything nicer.</p>\n\n<p>$$O\\left({{n+k-1}\\choose{k}}\\right) = O(?)$$</p>\n", 'Tags': '<asymptotics><combinatorics><runtime-analysis>', 'LastEditorUserId': '5271', 'LastActivityDate': '2013-01-05T23:36:04.090', 'CommentCount': '3', 'AcceptedAnswerId': '7694', 'CreationDate': '2013-01-02T10:10:10.627', 'Id': '7691'},90_65:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to understand the approximation ratio for the <a href="http://mor.journal.informs.org/content/25/4/645.full.pdf" rel="nofollow">Kenyon-Remila</a> algorithm for the 2D cutting stock problem.</p>\n\n<p>The ratio in question is $(1 + \\varepsilon) \\text{Opt}(L) + O(1/\\varepsilon^2)$.</p>\n\n<p>The first term is clear, but the second doesn\'t mean anything to me and I can\'t seem to figure it out.</p>\n', 'ViewCount': '235', 'Title': 'What does big O mean as a term of an approximation ratio?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-08T06:24:22.580', 'LastEditDate': '2013-01-08T06:24:22.580', 'AnswerCount': '5', 'CommentCount': '0', 'AcceptedAnswerId': '7722', 'Score': '2', 'OwnerDisplayName': 'Jacob Fogner', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><approximation>', 'CreationDate': '2013-01-01T23:33:43.330', 'Id': '7720'},90_66:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>How is an algorithm with complexity $O(n \\log n)$ also in $O(n^2)$? I'm not sure exactly what its saying here, I feel it may be something to do with the fact that big-oh is saying less than or equal to, but I am not fully sure. Any have any ideas? Thanks.</p>\n", 'ViewCount': '58', 'Title': 'How is this algorithm in these two complexities?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-12T16:37:28.597', 'LastEditDate': '2013-01-12T16:37:28.597', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7856', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3088', 'Tags': '<algorithms><asymptotics><landau-notation>', 'CreationDate': '2013-01-09T21:20:29.653', 'Id': '7855'},90_67:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm working exercises on solving recurrences, just using subsitution, master theorem is after this chapter. I'm sort of stuck on one of the exercises. It states that:</p>\n\n<p>The solution of $T(n) = 2T(\\lfloor n/2 \\rfloor) + n$ is $O(n \\lg n)$. Show that it's also $\\Omega(n \\lg n)$ and conclude that the solution is $\\Theta(n \\lg n)$.</p>\n\n<p>For showing that it's $O(n \\lg n)$, I've to show that $T(n) \\leq cn \\lg n$. This can be solved by choosing an $m &lt; n$, like $\\lfloor n/2\\rfloor$, and substituting.</p>\n\n<p>But if we arrive at the conclusion that $T(n) \\leq cn \\lg n$ for any appropriate $c &gt; 0$ and $n \\geq n_0$, than how can we say that it is also $\\Omega(n \\lg n)$ which implies that $T(n) \\geq cn \\lg n$?</p>\n\n<p>Some more clarification would be nice!</p>\n", 'ViewCount': '76', 'Title': 'Showing bounds of a recurrence', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-12T19:35:42.130', 'LastEditDate': '2013-01-12T16:31:47.647', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5391', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2013-01-12T10:57:49.360', 'Id': '7904'},90_68:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>From what I've read, Big O is the absolute worst ever amount of complexity an algorithm will be given an input. On the side, Big Omega is the best possible efficiency, i.e. lowest complexity.</p>\n\n<p>Can it be said then that every algorithm has a complexity of  $O(\\infty)$ since infinite complexity is the worst ever possible?\nBy the same token, can it be said that every algorithm is $\\Omega(1)$ since the absolute lowest complexity an algorithm can be is a constant?</p>\n", 'ViewCount': '208', 'Title': "Is every algorithm's complexity $\\Omega(1)$ and $O(\\infty)$?", 'LastEditorUserId': '55', 'LastActivityDate': '2013-01-18T05:27:12.653', 'LastEditDate': '2013-01-18T05:27:12.653', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '9000', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics>', 'CreationDate': '2013-01-17T18:16:04.023', 'Id': '8998'},90_69:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Concerning the Master Theorem. I have found the following equation as the base of analysis:  </p>\n\n<p>$\\quad T(n) = aT(n/b) + \\Theta(n^k)$  </p>\n\n<p>but I also found the following:  </p>\n\n<p>$\\quad T(n) = aT(n/b) + \\Theta(n^k\\cdot\\log_p n)$  </p>\n\n<p>where the base $p$ is a real number.  </p>\n\n<p>Can anyone explain the second equation? I understand the proof with the first equation but can not understand the second formula.</p>\n', 'ViewCount': '130', 'Title': 'Explanation of a specific recurrence with respect to Master Theorem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-01T03:32:14.113', 'LastEditDate': '2013-01-20T15:26:39.403', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6487', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2013-01-20T10:42:04.463', 'Id': '9055'},90_70:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am familiar with asymptotic notations like Big-O ,little-o. But while I am reading some papers people are using the notations like  $O(\\epsilon^{1/2^d})$, $O(d)^d$ etc. I couldn't understand these notations properly. Is there any way (Lecture notes or video lectures with examples) to understand these things clearly.\nThank You.  </p>\n", 'ViewCount': '162', 'Title': 'asymptotic notations with two exponents', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-31T07:03:28.657', 'LastEditDate': '2013-01-30T21:33:53.430', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-01-30T06:37:15.033', 'Id': '9299'},90_71:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm reading Cormen's Introduction to Algorithms 3rd edition, and in examples of Master Method recursion solving Cormen gives two examples</p>\n\n<ol>\n<li>$3T( \\frac{n}{4} ) + n\\log(n)$</li>\n<li>$2T( \\frac{n}{2} ) + n\\log(n)$</li>\n</ol>\n\n<p>For the first example we have $a=3$ and $b=4$  so $n^{\\log_4\n(3)}=n^{0.793}$ and Cormen says that if we choose $\\epsilon = 0.207$ then $f(n) = n\\log(n) = \\Omega(n^{\\log_4(3)  + \\epsilon})$</p>\n\n<p>How? As I understand it if $\\epsilon = 0.207$ then $\\Omega(n^{\\log_4(3)  + \\epsilon})= \\Omega(n)$ so we have $n\\log(n) = \\Omega(n)$  but it's not true; But he proves that $n\\log(n) = \\Omega( n^{\\log_4(3)  + \\epsilon} )$</p>\n\n<p>And then he proves that for the second case $n\\log(n)$ does not apply to masters method 3-rd case the same way as I prove above.</p>\n\n<p>So could somebody explain me in detail how  the third case of the master's theorem applies to  $3T( \\frac{n}{4} ) + n \\log(n)$  but not to $2T( \\frac{n}{2} ) + n\\log(n)$.</p>\n", 'ViewCount': '464', 'Title': 'How to the examples for using the master theorem in Cormen work?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T17:20:19.517', 'LastEditDate': '2013-02-02T13:24:39.077', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '9403', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6664', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2013-02-01T15:58:58.403', 'Id': '9390'},90_72:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '240', 'Title': "Infinite chain of big $O's$", 'LastEditDate': '2014-01-31T14:32:53.120', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2499', 'FavoriteCount': '1', 'Body': "<p>First, let me write the definition of big $O$ just to make things explicit.</p>\n\n<p>$f(n)\\in O(g(n))\\iff \\exists c, n_0\\gt 0$ such that $0\\le f(n)\\le cg(n), \\forall n\\ge n_0$</p>\n\n<p>Let's say we have a finite number of functions: $f_1,f_2,\\dots f_n$ satisftying:</p>\n\n<p>$O(f_1)\\subseteq O(f_2)\\dots \\subseteq O(f_n)$</p>\n\n<p>By transitivity of $O$, we have that: $O(f_1)\\subseteq O(f_n)$</p>\n\n<p>Does this hold if we have an infinite chain of $O's$? In other words, is $O(f_1) \\subseteq O(f_\\infty)$?</p>\n\n<p>I'm having trouble imagining what's going on.</p>\n", 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '2499', 'LastActivityDate': '2014-01-31T14:32:53.120', 'CommentCount': '3', 'AcceptedAnswerId': '9507', 'CreationDate': '2013-02-05T15:24:05.437', 'Id': '9506'},90_73:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If I have some function whose time complexity is O(<em>mn</em>), where <em>m</em> and <em>n</em> are the sizes of its two inputs, would we call its time complexity "linear" (since it\'s linear in both <em>m</em> and <em>n</em>) or "quadratic" (since it\'s a product of two sizes)? Or something else?</p>\n\n<p>I feel calling it "linear" is confusing because O(m + n) is also linear but much faster, but I feel like calling it "quadratic" is also weird because it\'s linear in each variable separately.</p>\n', 'ViewCount': '533', 'Title': 'Is O(mn) considered "linear" or "quadratic" growth?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-07T20:34:03.160', 'LastEditDate': '2013-02-06T07:41:49.433', 'AnswerCount': '5', 'CommentCount': '7', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-02-05T22:54:48.270', 'FavoriteCount': '1', 'Id': '9523'},90_74:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The Master theorem is a beautiful tool for <a href="http://cs.stackexchange.com/a/2823/98">solving certain kinds of recurrences</a>. However, we often gloss over an integral part when applying it. For example, during the analysis of Mergesort we happily go from</p>\n\n<p>$\\qquad T(n) = T\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right) + T\\left(\\left\\lceil \\frac{n}{2} \\right\\rceil\\right) + f(n)$</p>\n\n<p>to </p>\n\n<p>$\\qquad T\'(n) = 2 T\'\\left(\\frac{n}{2}\\right) + f(n)$</p>\n\n<p>considering only $n=2^k$. We assure ourselved that this step is valid -- that is, $T \\in \\Theta(T\')$ -- because $T$ behaves "nicely". In general, we assume $n=b^k$ for $b$ the common denominator.</p>\n\n<p>It is easy to construct recurrences which do not allow this simplification by using vicious $f$. For example, above recurrence for $T\\,$/$\\,T\'$ with</p>\n\n<p>$\\qquad f(n) = \\begin{cases}\n                 1 &amp;, n=2^k \\\\\n                 n &amp;, \\text{else}\n               \\end{cases}$</p>\n\n<p>will yield $\\Theta(n)$ by using the Master theorem in the usual way, but there is clearly a subsequence that grows like $\\Theta(n \\log n)$. See <a href="http://cs.stackexchange.com/a/4980/98">here</a> for another, more contrived example.</p>\n\n<p>How can we make this "nicely" rigorous? I am quite certain that monotonicity is sufficient, but not even the simple Mergesort recurrence is monotone; <a href="http://cs.stackexchange.com/a/2390/98">there is a periodic component</a> (which is dominated asymptotically). Is it enough to investigate $f$, and what are necessary and sufficient conditions on $f$ that ensure the Master theorem works?</p>\n', 'ViewCount': '252', 'Title': 'Rigorous proof for validity of assumption $n=b^k$ when using the Master theorem', 'LastActivityDate': '2013-02-11T00:43:01.050', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '9654', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<asymptotics><proof-techniques><recurrence-relation><master-theorem>', 'CreationDate': '2013-02-07T07:35:33.753', 'FavoriteCount': '3', 'Id': '9569'},90_75:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So I have the following pseudo-code:</p>\n\n<pre><code>Function(n):\nfor (i = 4 to n^2):\n    for (j = 5 to floor(3ilog(i))):\n        // Some math that executes in constant time\n</code></pre>\n\n<p>So far, I know that to find this i will be calculating</p>\n\n<p>$\\sum_{i=4}^{n^2}\\sum_{j=5}^{3i \\log_{2}i}C$ where $C$ is a constant, but I am completely lost as to how to proceed past the first summation which, if I'm not mistaken, will give me $3C \\cdot (\\sum_{i=4}^{n^2}(i \\log_{2}i) - 5(n^2 - 4))$, but from here I'm lost.  I don't need exact running time, but the asymptotic complexity.</p>\n\n<p>All help is greatly appreciated!  I realize that this might be a duplicate, but I haven't been able to find a nested for loop problem of this nature anywhere...</p>\n", 'ViewCount': '468', 'Title': 'Running time of a nested loop with $\\sum i \\log i$ term', 'LastEditorUserId': '4751', 'LastActivityDate': '2013-02-12T22:39:49.373', 'LastEditDate': '2013-02-12T22:39:49.373', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '9716', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6847', 'Tags': '<time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-02-12T19:55:39.130', 'Id': '9714'},90_76:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am trying to find out why $(\\log(n))^{99} = o(n^{\\frac{1}{99}})$. I tried to find the limit as this fraction goes to zero.</p>\n\n<p>$$\n\\lim_{n \\to \\infty} \\frac{ (\\log(n))^{99} }{n^{\\frac{1}{99}}}\n$$</p>\n\n<p>But I'm not sure how I can reduce this expression.</p>\n", 'ViewCount': '154', 'Title': 'Why is $(\\log(n))^{99} = o(n^{\\frac{1}{99}})$', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-19T00:20:34.973', 'LastEditDate': '2013-02-17T15:18:48.700', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '9854', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-02-17T03:52:56.133', 'Id': '9852'},90_77:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to analyze the running time of a bad implementation of generating the $n$th member of the fibonacci sequence (which requires generating the previous 2 values from the bottom up).</p>\n\n<p>Why does this algorithm have a time complexity of $\\Omega(2^{\\frac{n}{2}})$? Where does the exponent come from?</p>\n', 'ViewCount': '852', 'Title': 'Why does a recurrence of $T(n - 1) + T(n - 2)$ yield something in $\\Omega(2^{\\frac{n}{2}})$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-19T23:48:57.897', 'LastEditDate': '2013-02-19T06:22:05.190', 'AnswerCount': '4', 'CommentCount': '5', 'AcceptedAnswerId': '9908', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2013-02-18T19:02:47.990', 'Id': '9899'},90_78:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Many a times if the complexities are having constants such as 3n, we neglect this constant and say O(n) and not O(3n). I am unable to understand how can we neglect such three fold change? Some thing is varying 3 times more rapidly than other! Why do we neglect this fact?  </p>\n', 'ViewCount': '248', 'Title': 'Justification for neglecting constants in Big O', 'LastEditorUserId': '157', 'LastActivityDate': '2013-02-21T05:14:05.497', 'LastEditDate': '2013-02-21T05:14:05.497', 'AnswerCount': '5', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<complexity-theory><asymptotics><landau-notation>', 'CreationDate': '2013-02-20T07:12:59.983', 'FavoriteCount': '2', 'Id': '9957'},90_79:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In a recitation video for <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=P7frcB_-g4w">MIT OCW 6.006</a> at 43:30, </p>\n\n<p>Given an $m \\times n$ matrix $A$ with $m$ columns and $n$ rows, the 2-D peak finding algorithm, where a peak is any value greater than or equal to it\'s adjacent neighbors, was described as:</p>\n\n<p><em>Note: If there is confusion in describing columns via $n$, I apologize, but this is how the recitation video describes it and I tried to be consistent with the video.  It confused me very much.</em></p>\n\n<blockquote>\n  <ol>\n  <li><p>Pick the middle column $n/2$ // <em>Has complexity $\\Theta(1)$</em></p></li>\n  <li><p>Find the max value of column $n/2$ //<em>Has complexity  $\\Theta(m)$ because there are $m$ rows in a column</em></p></li>\n  <li><p>Check horiz. row neighbors of max value, if it is greater then a peak has been found, otherwise recurse with $T(n/2, m)$ //<em>Has complexity $T(n/2,m)$</em></p></li>\n  </ol>\n</blockquote>\n\n<p>Then to evaluate the recursion, the recitation instructor says</p>\n\n<blockquote>\n  <p>$T(1,m) =  \\Theta(m)$ because it finds the max value</p>\n  \n  <p>$$ T(n,m) =  \\Theta(1) +  \\Theta(m) + T(n/2, m) \\tag{E1}$$</p>\n</blockquote>\n\n<p>I understand the next part, at 52:09 in the video, where he says to treat $m$ like a constant, since the number of rows never changes.  But I don\'t understand how that leads to the following product:</p>\n\n<p>$$ T(n,m) = \\Theta(m) \\cdot \\Theta(\\log n) \\tag{E2}$$</p>\n\n<p>I think that, since $m$ is treated like a constant, it is thus treated like $\\Theta(1)$ and eliminated in $(E1)$ above.  But I\'m having a hard time making the jump to $(E2)$.  Is this because we are now considering the case of $T(n/2)$ with a constant $m$?</p>\n\n<p>I think can "see" the overall idea is that a $\\Theta(\\log n)$ operation is performed, at worst, for m number of rows.  What I\'m trying to figure out is how to describe the jump from $(E1)$ to $(E2)$ to someone else, i.e. gain real understanding.</p>\n', 'ViewCount': '168', 'Title': '2-D peak finding complexity (MIT OCW 6.006)', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-28T14:11:09.913', 'LastEditDate': '2013-02-27T22:09:59.223', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '7062', 'Tags': '<algorithms><algorithm-analysis><asymptotics><matrices>', 'CreationDate': '2013-02-27T18:22:56.113', 'FavoriteCount': '0', 'Id': '10141'},90_80:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to solve a recurrence by using substitution method. The recurrence relation is:\n$$T(n)=4T(n/2)+n^2$$\nMy guess is $T(n)$ is $\\Theta(n\\log n)$ (and I am sure about it because of master theorem), and to find an upper bound, I use induction. I tried to show that $T(n)\\le cn^2\\log n$ but that did not work, I got $T(n)\\le cn^2\\log n+n^2$.</p>\n\n<p>I then tried to show that, if $T(n)\\le c_1 n^2\\log n-c_2 n^2$, then it is also $\\mathcal O(n^2\\log n)$, but that also did not work and I got $T(n)\\le c_1n^2\\log(n/2)-c_2 n^2+n^2$.</p>\n\n<p>What trick can I use to show that? Thanks.</p>\n', 'ViewCount': '734', 'Title': 'Solving $T(n)=4T(n/2)+n^2$', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-15T22:16:54.140', 'LastEditDate': '2013-03-05T06:55:11.717', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7122', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2013-03-03T12:33:28.547', 'FavoriteCount': '0', 'Id': '10227'},90_81:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '230', 'Title': 'Solve a recurrence by drawing the recursion tree?', 'LastEditDate': '2013-03-05T07:00:38.817', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'imGreg', 'PostTypeId': '1', 'OwnerUserId': '7149', 'FavoriteCount': '1', 'Body': "<p>I'm studying for an entrance exam and I have sample questions. One of the questions is this</p>\n\n<blockquote>\n  <p>Prove that recurrence $T(n) = T(n/5) + T(4n/5)+n/2$\n  has a solution $T(n) = \\omega(n \\log n)$.   </p>\n  \n  <p>Solve by drawing the recursion tree.</p>\n</blockquote>\n\n<p>This is what I drew on my paper: </p>\n\n<pre><code>root:  n/2 =&gt; (4n/5)/2\n\n           =&gt; (n/5)/2\n\nright sub tree:  (4n/5)/2 =&gt; (16n/25)/2\n\n                          =&gt; (4n/25)/2\n\nleft sub tree:   (n/5)/2  =&gt; (4n/25)/2\n\n                          =&gt; (n/25)/2\n</code></pre>\n\n<p>From what I saw online when I was searching for a solution to this question I noticed people were drawing the trees and saying Big O something as an answer. I'm wondering how do they determine that Big O notation is the correct answer for this question or if my tree is correct?</p>\n", 'Tags': '<asymptotics><recurrence-relation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T16:51:48.820', 'CommentCount': '2', 'CreationDate': '2013-03-03T04:17:38.587', 'Id': '10242'},90_82:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I don't understand why $n \\log \\log n $ is not $\\Theta (n)$.</p>\n\n<p>Suppose we give $n$ a value of $10,000$. Then $n \\log \\log n$ is $6020.6$. So isn't $n \\log \\log n$ upper- and lower-bounded by $n$, as $n \\log \\log n \\geq Cn$?</p>\n", 'ViewCount': '265', 'Title': 'Why is $n \\log \\log n$ not tightly bounded by $n$?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-05T17:07:14.253', 'LastEditDate': '2013-03-05T17:07:14.253', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '10282', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4539', 'Tags': '<terminology><asymptotics>', 'CreationDate': '2013-03-05T03:48:03.413', 'Id': '10281'},90_83:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>A homework assignment asks me to state the complexity in Big-O notation of the function\n$$f(n) =  7n \u2013 3n \\log n + 100000 $$</p>\n\n<p>I graphed this function and decreases all the way down to zero nearly its entire lifespan. \nTherefore I concluded that the complexity is bounded by a constant and has the complexity $O(1)$.</p>\n\n<p>Is this correct?</p>\n\n<p>Also out of curiosity, what is the Big-Omega of this function is? The best it could ever run is also O(1). \nWhat about Big-Theta? I'm having trouble getting my head around these.</p>\n", 'ViewCount': '66', 'Title': 'Asymptotics of a function that decreases as n increases', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-06T07:20:34.140', 'LastEditDate': '2013-03-06T07:14:59.393', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<asymptotics>', 'CreationDate': '2013-03-06T04:22:22.430', 'Id': '10310'},90_84:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>One of my lectures makes the following statement:</p>\n\n<p>$$( f(n)=O(n) \\land f(n)\\neq o(n) )\\implies f(n)=\\Theta(n)$$</p>\n\n<p>Maybe I'm missing something in the definitions, but for example bubble sort is $O(n^2)$ and not $o(n^2)$ but it's also not $\\theta(n^2)$ since it's best case run time $\\Omega(n)$.</p>\n\n<p>What am I missing here?</p>\n", 'ViewCount': '106', 'Title': '$( f(n)=O(n) \\land f(n) \\neq o(n) ) \\implies f(n)=\\Theta(n)$', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T17:22:18.050', 'LastEditDate': '2013-03-07T17:22:18.050', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10353', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-03-07T07:47:57.967', 'Id': '10352'},90_85:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I can think of functions such as $n^2 \\sin^2 n$ that don't have asymptotically tight bounds,  but are there actually common algorithms in computer science that don't have asymptotically tight bounds on their worst case running times?</p>\n", 'ViewCount': '205', 'Title': 'Common Algorithms without Asymptotically Tight Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T11:27:03.257', 'LastEditDate': '2013-03-07T11:27:03.257', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '10355', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-07T08:39:31.110', 'Id': '10354'},90_86:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I find it fairly easy to generate an upper bound for nearly any iterative solution (e.g. look at the limits on each loop, etc.), and can oftentimes create an upper bound for normal recursive functions.</p>\n\n<p>However, I am now trying to determine a "Big-O" for a DP problem I\'ve memoized.  I don\'t really care about the answer to this specific problem, but am more interested in a method I can use for other programs I write, or a resource that I can read to learn how to analyze this type of program.</p>\n\n<p>In case a concrete example helps, the following is my program to solve this <a href="http://www.geeksforgeeks.org/dynamic-programming-set-21-box-stacking-problem/" rel="nofollow">box stacking problem</a>.  (I wrote my solution before looking at theirs, which appears to use bottom-up DP instead of top-down/memoization.  Thus, I don\'t think I can cross-apply their time complexity to my algorithm.)</p>\n\n<pre><code>import java.util.*;\n\npublic class BoxStack {\n  private static HashMap&lt;Base, Double&gt; memo; //My memo\n\n  //How many times do I call the reward subroutine *and* look something up?\n  private static int callCt = 0; \n\n  //Test data\n  private static double[] h = {4, 1, 4, 10}; //heights of boxes\n  private static double[] w = {6, 2, 5, 12}; //widths of boxes\n  private static double[] d = {7, 3, 6, 32}; //depths of boxes\n  private static final int N = 4;            //number of test cases\n\n  //My "r()" subroutine (short for reward).\n  //Given a box size (maximum width, maximum depth, and height of the box to be added)\n  //Return the maximum size tower I can place on top of that base\n  public static double r(double maxW, double maxD, double elementH) {\n    //I don\'t really need 3 max variables here, but it helped me to think \n    //about the maximum of each box with a given rotation\n    double max1 = 0;\n    double max2 = 0;\n    double max3 = 0;\n\n    //Return the memoized result if possible\n    Base testBase = new Base(maxW, maxD);\n    if (memo.get(testBase) != null) {\n      return memo.get(testBase);\n    }\n\n    //We\'re going to do some calculating, so increment call count\n    callCt++;\n\n    //Go through all the boxes...\n    for (int i = 0; i &lt; N; i++) {\n      //If you can stack it on top of the base in any orientation, do so!\n      if ((w[i] &lt; maxW &amp;&amp; d[i] &lt; maxD) || (w[i] &lt; maxD &amp;&amp; d[i] &lt; maxW)) {\n        max1 = Math.max(max1, r(w[i], d[i], h[i])+h[i]); //Recursive call!\n      }\n\n      if ((h[i] &lt; maxW &amp;&amp; d[i] &lt; maxD) || (h[i] &lt; maxD &amp;&amp; d[i] &lt; maxW)) {\n        max2 = Math.max(max2, r(h[i], d[i], w[i])+w[i]);\n      }\n\n      if ((h[i] &lt; maxW &amp;&amp; w[i] &lt; maxD) || (h[i] &lt; maxD &amp;&amp; w[i] &lt; maxW)) {\n        max3 = Math.max(max3, r(h[i], w[i], d[i])+d[i]);\n      }        \n    }\n\n    //Add to memo\n    memo.put(testBase, Math.max(max1, Math.max(max2, max3)));\n\n    return Math.max(max1, Math.max(max2, max3));\n  }\n\n  public static void main(String[] args) {\n    //Set up memo\n    memo = new HashMap&lt;Base, Double&gt;();\n\n    //Print max height\n    System.out.println(r(Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY, 0));\n\n    System.out.println(callCt);\n  }\n\n  //Something I can use to map my base sizes to my heights\n  private static class Base {\n    double width;\n    double height;\n\n   public Base (double width, double height) {\n      this.width = width;\n      this.height = height;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n      Base that = (Base) o;\n      return ((that.width == this.width &amp;&amp; that.height == this.height) || (that.height == this.width &amp;&amp; that.width == this.height));\n    }    \n\n    @Override\n    public int hashCode() {\n      return (this.toString()).hashCode();\n    }\n\n    public String toString() {\n      return ("(" + Math.min(width, height) + "," + Math.max(height, width) + ")");\n    }\n  }\n}\n</code></pre>\n', 'ViewCount': '328', 'Title': 'Time Complexity Upper Bound of Memoized DP Problems', 'LastActivityDate': '2013-03-18T12:57:14.393', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10600', 'Score': '1', 'OwnerDisplayName': 'anorton', 'PostTypeId': '1', 'OwnerUserId': '1592', 'Tags': '<asymptotics><dynamic-programming>', 'CreationDate': '2013-03-05T02:00:43.727', 'FavoriteCount': '0', 'Id': '10369'},90_87:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've learned that a while loop such as </p>\n\n<pre><code>int i = 100;\nwhile (i &gt;= 1){\n     ...\n     ///Stuff\n     i = i/2\n}\n</code></pre>\n\n<p>will run in logarithmic time, specifically, <code>O(logn)</code>, since it keeps dividing in half each time (like a binary search).</p>\n\n<p>However, what if my while loop looks like this</p>\n\n<pre><code> int i = 100;\n    while (i &gt;= 1){\n         ...\n         ///Stuff\n         i = i/3\n    }\n</code></pre>\n\n<p>Is the complexity still <code>O(logn)</code>?</p>\n\n<p>Can someone explain yes/no and why?</p>\n", 'ViewCount': '1248', 'Title': 'Complexity of a while loop that divides by parameter by three each iteration', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-08T20:48:01.123', 'LastEditDate': '2013-03-08T20:48:01.123', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '-3', 'OwnerDisplayName': 'Imray', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-08T19:15:47.000', 'Id': '10389'},90_88:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '743', 'Title': 'What is an Efficient Algorithm?', 'LastEditDate': '2013-03-12T14:10:14.620', 'AnswerCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6728', 'FavoriteCount': '1', 'Body': '<p>From the point of view of asymptotic behavior, what is considered an "efficient" algorithm?  What is the standard / reason for drawing the line at that point?  Personally, I would think that anything which is what I might naively call "sub-polynomial", such that $f(n) = o(n^2)$ such as $n^{1+\\epsilon}$ would be efficient and anything which is $\\Omega(n^2)$ would be "inefficient".  However, I\'ve heard anything which is of any polynomial order be called efficient.  What\'s the reasoning?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-12T17:39:15.060', 'CommentCount': '1', 'AcceptedAnswerId': '10477', 'CreationDate': '2013-03-12T10:11:19.533', 'Id': '10472'},90_89:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've found in many exercises where I'm asked to show that $f(n)=\\Theta(g(n))$ where the two functions are of the same order of magnitude I have difficulty finding a constant $c$ and a value $n_0$ for the lower bound.  I'm using Corman's definition of $\\Theta$: </p>\n\n<p>$$\\exists c_1,c_2&gt;0\\in\\mathbb{R}:\\forall n\\geq n_0: 0 \\leq c_1 g(n)\\leq f(n)\\leq c_2 g(n)$$</p>\n\n<p>Showing the upper bound usually doesn't give me too much trouble, but for the lower bound I allot of times find myself using limits.  And even though I'm getting the right answers, I'm a bit worried that my method isn't very rigorous and that maybe I'm doing a bit of hand waving in the process.</p>\n\n<p>For example, problem 2.17 from Skiena's Algorithm Design Manual:</p>\n\n<p>Show that for any $a,b\\in \\mathbb{R}: b&gt;0$ that $(n+a)^b = \\Theta(n^b)$</p>\n\n<p>In this case I used limits to help find both constants.  </p>\n\n<p>For the upper limit I decided to look for some $c$ such that $(n+a)^b \\leq c^bn^b$.  So taking the $b$th root of each side and dividing by $n$ I have $\\frac{n+a}{n}\\leq c$ which gives me $1 + \\frac{a}{n} \\leq c$.  For any $a\\in\\mathbb{R}$, $\\lim_{n\\to\\infty }1+\\frac{a}{n}=1$.  If I pick $n_0&gt;|a|$, then for $a&lt;0$ the expression approaches 1 from the left starting arbitrarily close to $0$.  If $a&gt;0$ then the expression approaches 1 from the right starting arbitrarily close to 2. So choosing $c=2$ will satisfy the inequality and we have $c_2=2^b$.</p>\n\n<p>Now for the lower bound.  I'm looking at the same expression except with the inequality pointing the other way.  In this case I'm trying to find $n_0$ and $c$ such that $c\\leq 1+\\frac{a}{n}$.  The value of $n_0$ has to be greater than $|a|$ because otherwise we would have $c\\leq 0$ which isn't allowed.  This puts us in the same range of values between $0$ and $2$ approaching 1 from each side.  So I choose any $c,n_0$ such that $n_0&gt;|a|$ and $0 &lt; c\\leq 1-|\\frac{a}{n_0}|$.  So I could choose $n_0=3|a|$ and $c=\\frac{2}{3}$.  </p>\n\n<p>Thus we have $0 &lt; (\\frac{2}{3})^bn^b \\leq (n+a)^b \\leq 2^bn^b$ for any $n \\geq 3|a|$.</p>\n\n<p><strong>Is there an easier way to do this?</strong>  </p>\n\n<p>Normally when looking for upper limit constants where the two functions are of the same magnitude I simply eliminate negative lower order terms and change positive ones into multiples of the highest order term such as : </p>\n\n<p>$$3n^2+15n-5\\leq 3n^2+15n^2=18n^2$$</p>\n\n<p>But when looking for the constant for the lower bound I find myself typically resorting to looking at limits.  <strong>Is there any kind of short cut to finding the lower bound constant like there is for the upper bound constant?</strong></p>\n", 'ViewCount': '216', 'Title': 'Methods for Finding Asymptotic Lower Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-15T00:40:06.203', 'LastEditDate': '2013-03-14T14:32:45.263', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10512', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<asymptotics><landau-notation><lower-bounds>', 'CreationDate': '2013-03-13T18:57:03.363', 'Id': '10511'},90_90:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>When dealing with Landau notation, $\\Theta, O,\\Omega,o,\\omega$, why do some texts choose the Corman style definitions, i.e.:</p>\n\n<p>$$o(g(n))=\\{ f(n): \\forall c&gt;0:\\exists n_0&gt;0:\\; 0\\leq f(n) &lt; cg(n): \\; \\forall n\\geq n_0 \\}$$</p>\n\n<p>and some texts use limit based definitions such as:</p>\n\n<p>$$\\lim_{n\\to\\infty}\\frac{f(n)}{g(n)}=0\\Rightarrow f(n)\\in o(g(n))$$</p>\n\n<p>Is there any inherent advantage to one definition or the other?  Or is it more a matter of the author's personal preference? </p>\n", 'ViewCount': '114', 'Title': "Landau Notation, Definitions: Limits vs. Corman's", 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-15T19:53:19.617', 'LastEditDate': '2013-03-15T07:46:31.157', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '10542', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-03-15T07:19:17.640', 'Id': '10531'},90_91:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '586', 'Title': 'Construct two functions $f$ and $g$ satisfying $f \\ne O(g), g \\ne O(f)$', 'LastEditDate': '2013-03-16T07:00:17.103', 'AnswerCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '7300', 'FavoriteCount': '1', 'Body': u'<p>Construct two functions $  f,g:   R^+ \u2192 R^+ $ satisfying:</p>\n\n<ol>\n<li>$f, g$ are continuous;</li>\n<li>$f, g$ are monotonically increasing;</li>\n<li>$f \\ne O(g)$ and $g \\ne O(f)$.</li>\n</ol>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '3011', 'LastActivityDate': '2014-03-05T13:09:32.820', 'CommentCount': '2', 'AcceptedAnswerId': '10549', 'CreationDate': '2013-03-16T06:54:52.737', 'Id': '10548'},90_92:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>This is a Data structures &amp; Algorithms question. For instance I have the following grades of functions: $O(1), O(2^n), O(n \\log n), O(e^n), O(n^3), O(n^{1/3})$ and $O(\\log \\log n)$  </p>\n\n<p>I need to show and proof to which of these function grades does the function: $n^7$ belong to. \nI didn't get a chance to ask my professor on this topic so I'm not sure how to solve this problem.</p>\n", 'ViewCount': '54', 'ClosedDate': '2013-03-18T10:24:46.367', 'Title': 'Show that a function belongs to grade of incline', 'LastEditorUserId': '157', 'LastActivityDate': '2013-03-18T10:14:45.007', 'LastEditDate': '2013-03-17T21:26:38.540', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'user1125177', 'PostTypeId': '1', 'Tags': '<complexity-theory><asymptotics><landau-notation>', 'CreationDate': '2013-03-17T20:11:18.047', 'Id': '10588'},90_93:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When dealing with the analysis of time and space complexity of algorithms, is it safe to assume that any function which has tight bounds ( i.e. $f(n)=\\Theta(g(n))$ is asymptotically positive and asymptotically monotonically increasing.  I mean that for all $n$ greater than or equal to some $n_0$ both those properties hold?  </p>\n', 'ViewCount': '313', 'Title': 'Asymptotic Properties of Functions in Complexity Analysis', 'LastActivityDate': '2013-03-20T21:07:13.017', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10666', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><asymptotics><landau-notation>', 'CreationDate': '2013-03-20T20:07:26.793', 'Id': '10664'},90_94:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>The worst case running time of insertion sort is $\\Theta(n^2)$, we don\u2019t write it as $O(n^2)$.</p>\n\n<p>$O$-notation is used to give upper bound on function. If we use it to bound a worst case running time of insertion sort, it implies that $O(n^2)$ is upper bound of algorithm no matter what type of input is, means it doesn\u2019t matter whether input is sorted, unsorted, reverse sorted, have same values, etc the upper bound will be same $O(n^2)$. But this is not the case of insertion sort. Insertion sort running time depends on type of input used. So when the input is already sorted, it runs in linear time and doesn\u2019t take more that $O(n)$ time.</p>\n\n<p>Therefore to write insertion sort running time as $O(n^2)$ is technically not good.</p>\n\n<p><strong>We use $\\Theta$-notation to write worst case running time of insertion sort. But I\u2019m not able to relate properties of $\\Theta$-notation with insertion sort, why $\\Theta$-notation is suitable to insertion sort.If $f(n)$ belong to $\\Theta(g(n))$ we write it as $f(n)= \\Theta(g(n))$, then $f(n)$ must satisfies the properties. And properties state that there exits constants $c_1$, $c_2$ and $n_0$ such that $0$$\\leq$$c_1\\cdot g(n)$$\\leq$$f(n)$$\\leq$$c_2\\cdot g(n)$ For all $n&gt;n_0$. How does the insertion sort function lies between the $c_1\\cdot n^2$ and $c_2\\cdot n^2$ for all $n&gt;n_0$.</strong></p>\n\n<p>Running time of insertion sort as $\\Theta(n^2)$ implies that it has upper bound $O(n^2)$ and lower bound $\\Omega(n^2)$. I\u2019m confused as to whether the lower bound on insertion-sort is $\\Omega(n^2)$ or $\\Omega(n)$.</p>\n', 'ViewCount': '477', 'Title': 'Why is $\\Theta$ notation suitable to insertion sort to describe its worst case running time?', 'LastEditorUserId': '7384', 'LastActivityDate': '2013-04-29T21:19:54.897', 'LastEditDate': '2013-04-29T16:22:00.450', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'siddstuff', 'PostTypeId': '1', 'OwnerUserId': '7384', 'Tags': '<algorithms><time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-03-25T06:59:09.077', 'Id': '10763'},90_95:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to determine the worst case runtime of this program:</p>\n\n<pre><code>while n &gt; 1\n  for i = 1,..,n\n    m = log(n)\n  n = n/2\n</code></pre>\n\n<p>Obviously the outer loop runs <code>log(n)</code> times, because n is halfed after each step.</p>\n\n<p>But the inner for loop is dependent on the decreasing n and I am not quite sure how to deal with that.</p>\n\n<p>A simple bound for it would be <code>n</code>, so the whole thing runs in at most <code>log(n)*n</code>, but I am sure the inner loop is faster than that.</p>\n\n<p>If I look at each step of the outer loop, I see that the inner loop runs <code>n,n/2,n/4,..,2</code> times. So I would say all together the inner loop runs at most <code>2n</code> times which is in <code>O(n)</code>.</p>\n\n<p>Can I just deduct from that fact that the overall runtime is <code>O(log(n)+n) = O(n)</code>?</p>\n\n<p>I am unsure because I am not multiplying anything, which I feel like I should when dealing with multiple loops.</p>\n', 'ViewCount': '161', 'ClosedDate': '2013-03-27T11:54:26.093', 'Title': 'Decreasing runs of inner loop in outer loop', 'LastActivityDate': '2013-03-26T19:40:41.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '594', 'Tags': '<time-complexity><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-26T19:04:39.817', 'Id': '10813'},90_96:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '180', 'Title': 'Time complexity based on two variables', 'LastEditDate': '2013-04-02T07:59:08.437', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'Naji', 'PostTypeId': '1', 'OwnerUserId': '7531', 'Body': "<p>Suppose we have a function based on two inputs of length $m,n$. Therefore the time complexity of the function is calculated by $T(m,n)$. Suppose that we have:</p>\n\n<ul>\n<li>$T(m,c)\\in O(m^2)$ for any constant $c$.</li>\n<li>$T(c',n)\\in O(n^2)$ for any constant $c'$.</li>\n</ul>\n\n<p>What can we say about $T(m,n)$?</p>\n", 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T07:59:08.437', 'CommentCount': '2', 'AcceptedAnswerId': '10956', 'CreationDate': '2013-04-01T03:44:10.490', 'Id': '10955'},90_97:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Following method is explained by my senior. I want to know whether I can use it in all cases or not. When I solve it manually, I come to same answer. </p>\n\n<p>$T(n)= 4T(n/2) + \\frac{n^2}{\\lg n}$</p>\n\n<p>In above recurrence master theorem fails. But he gave me this solution, when </p>\n\n<p>for $T(n) = aT(n/b) + \\Theta(n^d \\lg^kn)$ </p>\n\n<p>if $d = \\log_b a$ </p>\n\n<p>if $k\\geq0$ then $T(n)=\\Theta(n^d \\lg^{k+1})$</p>\n\n<p>if $k=-1$ then $T(n)=\\Theta(n^d\\lg\\lg n)$</p>\n\n<p>if $k&lt;-1$ then $T(n)=\\Theta(n^{\\log_ba})$</p>\n\n<p>using above formulae, the recurrence is solved to $\\Theta(n^2\\lg\\lg n)$. When I solved manually, I come up with same answer. If it is some standard method, what it is called ?</p>\n', 'ViewCount': '278', 'Title': 'Finding recurrence when Master Theorem fails', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T21:44:26.690', 'LastEditDate': '2013-04-02T19:09:34.620', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '10980', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2013-04-02T17:51:49.660', 'Id': '10977'},90_98:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a> has a good cheat sheet, but however it does not involve no. of comparisons or swaps. (though no. of swaps is usually decides its complexity). So I created the following. Is the following info is correct ? Please let me know if there is any error, I will correct it.</p>\n\n<p><strong>Insertion Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case : $\\Theta(n^2)$ ; happens when input is\nalready sorted in descending order</li>\n<li>Best Case : $\\Theta(n)$ ; when input is already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(n)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in Best case</li>\n</ul>\n\n<p><strong>Selection Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best Case: $\\Theta(n^2)$ </li>\n<li>No. of comparisons : $\\Theta(n^2)$</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Merge Sort :</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best case : $\\Theta(nlgn)$ ; doesn\'t matter at all whether the input is sorted or not</li>\n<li>No. of comparisons : $\\Theta(n+m)$ in worst case &amp; $\\Theta(n)$ in best case ; assuming we are merging two array of size n &amp; m where $n&lt;m$</li>\n<li>No. of swaps : No swaps ! [but requires extra memory, not in-place sort]</li>\n</ul>\n\n<p><strong>Quick Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$ ; happens input is already sorted</li>\n<li>Best Case : $\\Theta(nlogn)$ ; when pivot divides array in exactly half</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(nlogn)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Bubble Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$</li>\n<li>Best Case : $\\Theta(n)$ ; on already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Linear Search:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n)$ ; search key not present or last element</li>\n<li>Best Case : $\\Theta(1)$ ; first element</li>\n<li>No. of comparisons : $\\Theta(n)$ in worst case &amp; $1$ in best case</li>\n</ul>\n\n<p><strong>Binary Search:</strong></p>\n\n<ul>\n<li>Worst case/Average case : $\\Theta(logn)$</li>\n<li>Best Case : $\\Theta(1)$ ; when key is middle element</li>\n<li>No. of comparisons : $\\Theta(logn)$ in worst/average case &amp; $1$ in best case</li>\n</ul>\n\n<hr>\n\n<ol>\n<li>I have considered only basic searching &amp; sorting algorithms. </li>\n<li>It is assumed above that sorting algorithms produce output in ascending order</li>\n<li>Sources : The awesome <a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">CLRS</a> and this <a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a></li>\n</ol>\n', 'ViewCount': '9952', 'ClosedDate': '2014-02-09T15:34:28.957', 'Title': 'Complexities of basic operations of searching and sorting algorithms', 'LastActivityDate': '2014-02-09T07:07:24.577', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><sorting><searching>', 'CreationDate': '2013-04-03T13:09:39.333', 'FavoriteCount': '1', 'Id': '10991'},90_99:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was learning about algorithms with polynomial time complexity. I found the following algorithms interesting.</p>\n\n<ul>\n<li><p>Linear Search - with time complexity $O(n)$</p></li>\n<li><p>Matrix Addition - with time complexity $O(n^2)$</p></li>\n<li><p>Matrix Multiplication - with time complexity  $O(n^3)$</p></li>\n</ul>\n\n<p>Is there any algorithm with a higher complexity like $n^4$, $n^5$ etc? I would like to know about practical algorithms with polynomial time complexity only.</p>\n\n<p>(I am familiar with algorithms having exponential complexity and class NP algorithms. My doubt is not about them.)</p>\n', 'ViewCount': '568', 'Title': 'Algorithms with polynomial time complexity of higher order', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-03T21:05:45.970', 'LastEditDate': '2013-04-03T21:05:45.970', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '11002', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7363', 'Tags': '<algorithms><complexity-theory><asymptotics>', 'CreationDate': '2013-04-03T18:10:35.997', 'Id': '10997'},90_100:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say I have two functions $f(x) = O(1)$ and $g(x) = o(1)$. Let $h(x) = f(x)g(x)$. Is $h(x) = o(1)$?</p>\n\n<p>By definition of small-o $g(x)$ must approach 0 as $x \\rightarrow \\infty$, so I think yes. However, how can I formally prove it? Is my intuition correct?</p>\n', 'ViewCount': '35', 'Title': 'Does $O(1) * o(1)$ equal a $o(1)$ function?', 'LastActivityDate': '2013-04-05T07:31:21.967', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11042', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<asymptotics>', 'CreationDate': '2013-04-05T07:07:19.470', 'Id': '11041'},90_101:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For a regular language $L$, let $c_n(L)$ be the number of words in $L$ of length $n$. Using Jordan canonical form (applied to the unannotated transition matrix of some DFA for $L$), one can show that for large enough $n$,\n$$ c_n(L) = \\sum_{i=1}^k P_i(n) \\lambda_i^n, $$\nwhere $P_i$ are complex polynomials and $\\lambda_i$ are complex "eigenvalues". (For small $n$, we may have additional terms of the form $C_k[n=k]$, where $[n=k]$ is $1$ if $n=k$ and $0$ otherwise. These correspond to Jordan blocks of size at least $k+1$ with eigenvalue $0$.)</p>\n\n<p>This representation seems to imply that if $L$ is infinite then asymptotically, $c_n(L) \\sim C n^k \\lambda^n$ for some $C,\\lambda&gt;0$. However, this is patently false: for the language $L$ over $\\{0,1\\}$ of all words of even length, $c_{2n}(L) = 2^{2n}$ but $c_{2n+1}(L) = 0$. This suggests that for some $d$ and for all $a \\in \\{0,\\ldots,d-1\\}$, either $c_{dm+a}(L) = 0$ for large enough $m$ or $c_{dm+a} \\sim C_a (dm+a)^{k_a} \\lambda_a^{dm+a}$. This is proved in <a href="http://algo.inria.fr/flajolet/Publications/FlSe02.ps.gz">Flajolet &amp; Sedgewick</a> (Theorem V.3), who attribute the proof to Berstel.</p>\n\n<p>The proof provided by Flajolet and Sedgewick is somewhat technical; so technical, in fact, that they only sketch it. I attempted a more elementary proof using Perron-Frobenius theory. We can regard the transition graph of the DFA as a digraph. If the digraph is primitive then the result follows almost directly from the Perron-Frobenius theorem. If the digraph is irreducible but imprimitive with index $r$, then by considering the "$r$th power" of the DFA (each transition corresponds to $r$ symbols), we get the same result. The difficult case is when the digraph is reducible. We can reduce to the case of a path of strongly connected components, and then we get the result by estimating sums of the form\n$$ \\sum_{m_1+\\cdots+m_k=m} \\prod_{i=1}^k \\lambda_i^{m_i}. $$\n(Each such sum corresponds to a particular way of accepting a word, going through the different components in a certain way.) This sum, in turn, can be estimated by pinpointing the largest term, which corresponds to $m_i \\propto \\log \\lambda_i$. For every eigenvalue which is repeated $r$ times, we get an extra factor of $\\Theta(m^{r-1})$.</p>\n\n<p>The proof has its rough edges: in the reducible case, we need to pass from terms asymptotic to $C \\lambda_i^m$ to the sum mentioned above, and then we need to estimate the sum.</p>\n\n<p>The proof by Flajolet and Sedgewick is perhaps simpler, but less elementary. Its starting point is the rational generating function of $c_n(L)$, and it involves induction on the number of pole magnitudes (!). The basic idea is that all eigenvalues of maximal modulus are roots of unity (if normalized by their modulus), due to a (moderately easy) theorem of Berstel. Choosing an appropriate $d$ and looking at words of length $dm+a$, all these eigenvalues become real. Considering the partial fraction expansion, we get that if the eigenvalue of maximal modulus "survives", then it determines the asymptotics, which are of the form $Cn^k\\lambda^n$. Otherwise, we find a new rational generating function which corresponds just to words of this length (using an Hadamard product), and repeat the argument. The aforementioned quantity keeps decreasing, and so eventually we find the desired asymptotics; $d$ might have to grow in the process, to reflect everything that happens in the inductive steps.</p>\n\n<blockquote>\n  <p>Is there a simple and elementary proof for the asymptotic property of $c_n(L)$?</p>\n</blockquote>\n', 'ViewCount': '449', 'Title': 'Asymptotics of the number of words in a regular language of given length', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-27T16:50:23.943', 'LastEditDate': '2013-04-16T09:30:24.363', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '683', 'Tags': '<formal-languages><reference-request><regular-languages><asymptotics><combinatorics>', 'CreationDate': '2013-04-16T01:37:16.013', 'FavoriteCount': '6', 'Id': '11350'},90_102:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '77', 'Title': 'Resolving this recurrence equation', 'LastEditDate': '2013-04-29T12:38:07.347', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Federico Ponzi', 'PostTypeId': '1', 'OwnerUserId': '17193', 'Body': "<p>I have this recurrence equation:</p>\n\n<p>$T(n) = T(n/4) + T(3n/4) + \\mathcal{O}(n)$</p>\n\n<p>$T(1) = 1$</p>\n\n<p>I know that the result is $\\mathcal{O}(n \\log n)$ but i don't know how to proceed.</p>\n", 'ClosedDate': '2013-04-29T18:28:35.180', 'Tags': '<asymptotics><runtime-analysis><recurrence-relation><recursion>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-04-29T12:38:07.347', 'CommentCount': '6', 'AcceptedAnswerId': '11657', 'CreationDate': '2013-04-14T19:02:46.790', 'Id': '11656'},90_103:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm coding a few set comparisons and noting their big O's using different algorithms and set implementations. I got to one particular function and I decided that it is $O(max(n,m))$ runtime. Is that the proper way to express this?</p>\n", 'ViewCount': '131', 'Title': 'Big O notation of max?', 'LastEditorUserId': '8002', 'LastActivityDate': '2013-05-01T19:03:33.613', 'LastEditDate': '2013-05-01T19:03:33.613', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8002', 'Tags': '<asymptotics>', 'CreationDate': '2013-05-01T17:08:31.390', 'Id': '11702'},90_104:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How would I solve the following.</p>\n\n<p>An algorithm that is $O(n^2)$ takes 10 seconds to execute on a particular computer when n=100, how long would you expect to take it when n=500?</p>\n\n<p>Can anyone help me answer dis. </p>\n', 'ViewCount': '140', 'Title': 'Algorithm analysis question in growth of functions', 'LastEditorUserId': '6980', 'LastActivityDate': '2013-05-05T03:09:56.113', 'LastEditDate': '2013-05-05T03:09:56.113', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11782', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithms><time-complexity><asymptotics>', 'CreationDate': '2013-05-04T21:35:39.893', 'Id': '11781'},90_105:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Runtime for many programming languages is typically analyzed either assuming each operation takes a constant amount of time, or assuming each operation takes a logarithmic amount of time in the size the the data being manipulated. From a practical perspective, this is reasonable since modern computers have essentially constant time memory access (albeit for a fixed amount of memory).</p>\n\n<p>However, from a theoretical perspective, it seems to me that this is a bad assumption.</p>\n\n<p>Since bits can't be stored denser than the Planck scale (which we are fast approaching), the number of bits we can store in a certain volume of space can grow at most linearly as volume grows. Further, since the speed of memory retrieval and storage is bounded by the speed of light, it makes a lot of sense to say that as the size of the data we store grows, the cost for storing and accessing it should grow. For instance, one could assume that storing and accessing the $n$th bit of data requires $\\sqrt[3]{n}$ time. (Since space is 3-dimensional).</p>\n\n<p>Presumably this has been discussed in the literature somewhere. Could anyone point me to some references? Have algorithms been analyzed using this model?</p>\n", 'ViewCount': '93', 'Title': "Why don't we scale the cost of memory access when analyzing runtime of algorithms?", 'LastActivityDate': '2013-05-06T10:20:24.770', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8065', 'Tags': '<reference-request><time-complexity><asymptotics><runtime-analysis>', 'CreationDate': '2013-05-06T05:14:51.447', 'FavoriteCount': '1', 'Id': '11818'},90_106:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For every integer $t$, is there a problem whose solutions can be verified in $O(n^{s})$ time but cannot be found in $O(n^{st})$ time?</p>\n\n<p>By verifying, I mean that given a candidate solution $y$, we can judge whether $y$ is correct or not in time $O(n^s)$.</p>\n', 'ViewCount': '401', 'Title': 'A Problem on Time Complexity of Algorithms', 'LastEditorUserId': '72', 'LastActivityDate': '2013-05-09T22:53:07.350', 'LastEditDate': '2013-05-09T14:35:06.697', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8074', 'Tags': '<complexity-theory><time-complexity><asymptotics><complexity-classes><lower-bounds>', 'CreationDate': '2013-05-07T01:48:11.327', 'Id': '11844'},90_107:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>We wish to manufacture n distinct hardware items. Each item needs to go through 3 stages of processing. The first stage called design can only be performed by our master designer who works by starting work on an item, designing it through to the end, and only then starting on another item. The remaining two phases, called assembly and testing, are outsourced and for each of them there is an infinite supply of people who can perform the corresponding task as soon as it is assigned to them. Naturally, each item first needs to be designed, then assembled, and then tested.</p>\n\n<p>Each item a requires d\u2090 hours of design, a\u2090 hours of assembly, and t\u2090 hours of testing. We are interested in determining the order in which we should design the items so that we minimize the time by which all items will be ready. For example, if we only had two pieces and we first designed item 1 and then designed item 2, the time by which both items would be finished is</p>\n\n<p>max{d\u2081 + a\u2081 + t\u2081, d\u2081 + d\u2082 + a\u2082 + t\u2082}.</p>\n\n<p>If, alternatively, we first design item 2 and then item 1, the time by which both items would</p>\n\n<p>be finished is</p>\n\n<p>max{d\u2082 + a\u2082 + t\u2082, d\u2082 + d\u2081 + a\u2081 + t\u2081}.</p>\n\n<p>Give a O(n log n) algorithm which takes as input n triples (d, a, t) and determines the optimal design order.</p>\n\n<p>Thanks for your help guys!</p>\n', 'ViewCount': '80', 'Title': 'Interval Scheduling Optimization type of Problem, optimal order of manufacture', 'LastActivityDate': '2013-05-07T06:43:02.403', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8075', 'Tags': '<algorithms><asymptotics><optimization><scheduling>', 'CreationDate': '2013-05-07T06:43:02.403', 'Id': '11847'},90_108:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Some authors define $\\Omega$  in a slightly different way: let\u2019s use\n$ \\overset{\\infty}{\\Omega}$\n(read \u201comega infinity\u201d) for this alternative definition. We say that $f(n) = \\overset{\\infty}{\\Omega}(g(n))$\nif there exists a positive constant $c$ such that $f(n) \\geq c\\cdot g(n) \\geq   0$ for infinitely many integers $n$, whereas the usual $\\Omega$ requires that this holds for all integers greater than a certain $n_0$. </p>\n\n<p>Show that for any two functions $f(n)$ and $g(n)$ that are asymptotically nonnegative,\neither $f(n) = O(g(n))$ or $f(n)= \\overset{\\infty}{\\Omega}(g(n))$ or both, whereas this is not true if we use $\\Omega$ in place of $\\overset{\\infty}{\\Omega}$. </p>\n\n<p>I am trying learn Algorithms. But I am unable to prove this. Can the experts help me ?</p>\n', 'ViewCount': '328', 'Title': 'Variations of Omega and Omega infinity', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-05-21T22:54:15.767', 'LastEditDate': '2013-05-21T21:34:08.090', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8263', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-05-20T03:58:41.007', 'Id': '12147'},90_109:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>$$T(n) = 2T(n/2) + \\Theta(n), n &gt; 1$$\n$$T(n) = \\Theta (1), n \\le 1$$</p>\n\n<p>$$G(n) = G(\\lfloor n/2 \\rfloor) + G (\\lceil n/2 \\rceil) + \\Theta(n), n &gt; 1$$\n$$G(n) = \\Theta (1), n \\le 1$$</p>\n\n<p>Prove $T(n)$ and $G(n)$ have the same asymtotic running time.</p>\n\n<p>I tried using the sandwich theorem but got nowhere. Please be really explicit and show all your steps in your answer. Thank you.</p>\n', 'ViewCount': '76', 'Title': 'Compare speed of two algorithms?', 'LastActivityDate': '2013-05-21T13:44:05.233', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8269', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2013-05-20T16:22:48.897', 'Id': '12160'},90_110:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been recently studying Monte-Carlo and other randomized methods for a lot of applications, and one that popped into my mind was making an (approximate) convex hull by examining random points, and try to get them inside the convex hull. I would like to know if there are algorithms for convex hulls that can improve the $O(n \\log n)$ bound of comparison based algorithms, and the $O(n\\cdot h)$ bound for Jarvis march and related to $O(n)$, either by building an approximate convex hull in $O(n)$ (with or without some approximation criteria) or by building an exact convex hull in expected linear time.</p>\n", 'ViewCount': '43', 'Title': 'Randomized convex hull', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-21T23:02:42.530', 'LastEditDate': '2013-05-21T19:34:43.807', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12208', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2138', 'Tags': '<algorithms><asymptotics><computational-geometry><randomized-algorithms>', 'CreationDate': '2013-05-21T19:30:47.060', 'Id': '12199'},90_111:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>\u0398 - Tightly bound</p>\n\n<p>O - Upper bound</p>\n\n<p>\u03a9 - Lower bound</p>\n\n<p>I understand that for addition, the max asymptotic value is taken, for example...</p>\n\n<p>if \u01921(n) = O(n) and \u01922(n) = O(n log n), then \u01921 + \u01922 = O(max(\u01921, \u01922) = O(n log n)</p>\n\n<p>and for multiplication, the multiplied asymptotic value is taken, for example...</p>\n\n<p>if \u01921(n) = O(n) and \u01922(n) = O(n log n), then \u01921 * \u01922 = O(\u01921 * \u01922) = O((n^2)log n)</p>\n\n<p>My question is, what happens to the formulas for adding and multiplying functions of different growth rates when other bounds besides upper bounds (such as \u03a9 - Lower bound and \u0398 - Tightly bound) are used?</p>\n', 'ViewCount': '45', 'Title': 'Behavior of different multiplied and added time complexities', 'LastActivityDate': '2013-06-15T09:10:03.637', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12686', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8681', 'Tags': '<time-complexity><asymptotics>', 'CreationDate': '2013-06-15T08:24:43.060', 'Id': '12685'},90_112:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I met the problem </p>\n\n<blockquote>\n  <p>Show that the solution to $T(n) = 2T(\\lfloor n/2\\rfloor + 17) + n$ is $O(n \\log n)$</p>\n</blockquote>\n\n<p>while reading Introduction to Algorithm. It's a question about the substitution method for solving recurrences.</p>\n\n<p>Should I just assume $T(n/2 + 17) \\leq O((n/2 + 17) \\log(n/2 + 17))$ and then prove $T(n) \\leq O(n \\log n)$? Is this inductive assumption enough to solve it?</p>\n", 'ViewCount': '122', 'ClosedDate': '2013-09-16T08:55:38.767', 'Title': 'How to prove the asymptotic upper bound for $T(n) = 2T(\\lfloor n/2\\rfloor + 17) + n$ is $O(n \\log n)$?', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-06-27T18:45:56.293', 'LastEditDate': '2013-06-27T18:45:56.293', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8838', 'Tags': '<asymptotics><recurrence-relation><induction><divide-and-conquer>', 'CreationDate': '2013-06-24T14:43:28.910', 'Id': '12865'},90_113:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am a bit confused about calculating complexities.</p>\n\n<p>Above is a C++ program converting a char array into an int, incrementing the value, parsing it back to char array.</p>\n\n<pre><code>#include &lt;iostream&gt;\n\nint main() {\n    char number[] = {'4', '3', '1'};    \n    int num = 0;\n    //char to int conversion\n    for (int i = 0; i &lt; (int)sizeof(number); i++) {\n        num += number[i] - '0';\n        num*=10;\n    }\n    num/=10;\n\n    //incrementation\n    num++;\n\n    //int to char conversion\n    for (int i = (int)sizeof(number) -1; i &gt;= 0; i--) {\n        number[i] = '0' + num % 10;\n        num/=10;\n    }\n\n    //printing the result\n    std::cout &lt;&lt; number &lt;&lt; endl;\n    return 0;\n}\n</code></pre>\n\n<p>Now let's say array size(3) is n. In that case I would say that the complexity is O(n+n) which is O(2n). However I've heard that O(2n) is actually O(n) for some reason but I could not find any actual source about it. What is the time complexity of this program?</p>\n", 'ViewCount': '72', 'Title': 'Complexity of a particular algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-25T08:26:42.210', 'LastEditDate': '2013-06-25T08:26:42.210', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-06-24T18:53:48.697', 'Id': '12873'},90_114:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose that we have a directed graph $G = (V, E)$ represented as an adjacency list.  Suppose that we want to list all of the edges incident to some node $v \\in V$.  We can do this by iterating over all the elements in the adjacency list.</p>\n\n<p>My question concerns the runtime of this operation.  Is it proper to say that the runtime is $\\Theta(deg^+(v))$ (since the runtime is a linear function of the outdegree of $v$)?  Or should it be $\\Theta(1 + deg^+(v))$, since even if $deg^+(v)$ is zero, there is some amount of work that's still done?  Or are both of these terminologies incorrect because there is no underlying variable $n$ for which we could apply the formal definition of $\\Theta$?</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '46', 'LastEditorDisplayName': 'user742', 'Title': 'What is the proper runtime for visiting all outgoing edges in an adjacency list?', 'LastActivityDate': '2013-09-20T09:28:16.213', 'LastEditDate': '2013-09-20T09:28:16.213', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<terminology><graph-theory><asymptotics>', 'CreationDate': '2013-06-29T00:58:31.787', 'Id': '12959'},90_115:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How do you go about analysing coin change recursive solution. i.e,\n<code>T(N,K) = T(N,K-1) + T(N-1,K)</code> for K denominations that add up to amount N.</p>\n\n<p>You can find the problem description and pseudo code here - <a href="http://www.algorithmist.com/index.php/Coin_Change#Recursive_Formulation" rel="nofollow">http://www.algorithmist.com/index.php/Coin_Change#Recursive_Formulation</a></p>\n', 'ViewCount': '177', 'Title': 'Complexity of recursive solution to coin change', 'LastActivityDate': '2013-07-02T01:56:38.320', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'Raghu', 'PostTypeId': '1', 'OwnerUserId': '8298', 'Tags': '<time-complexity><recursion><asymptotics>', 'CreationDate': '2013-06-30T21:17:49.447', 'Id': '13017'},90_116:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Within the field of cache-oblivious algorithms the ideal cache model is used for determining the cache complexity of an algorithm.  One of the assumptions of the ideal cache model is that it models a "tall cache".  This is given by the statement $Z = \\Omega(L^2)$.  Where $Z$ is the size of the cache and $L$ is the size of the cache line.  What does $\\Omega$ represent?</p>\n', 'ViewCount': '69', 'Title': 'In the "tall cache assumption" what does $\\Omega$ represent?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-03T13:21:34.633', 'LastEditDate': '2013-07-03T13:21:34.633', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13053', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8986', 'Tags': '<asymptotics><landau-notation><computation-models><cpu-cache>', 'CreationDate': '2013-07-03T08:32:00.390', 'Id': '13052'},90_117:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's say I have a problem which depends on two variables, $m$ and $n$.  I also have two algorithms for solving the problem.  How do I decide which algorithm to use?</p>\n\n<p>For example, say I have an array of unique numbers $A$, not necessarily sorted, and a second sorted array $B$.  I want to create a third array $C$ which contains how many times each number in $A$ appears in $B$.  </p>\n\n<p>Algorithm 1 runs in time $O(m \\lg n)$ and algorithm 2 in $O(m \\lg m + n)$.</p>\n\n<p>It seems to me I would need to divide into three cases:</p>\n\n<ol>\n<li>$n=\\Theta(m)$</li>\n<li>$n\\gg m$</li>\n<li>$m \\gg n$</li>\n</ol>\n\n<p>How would I generally proceed from here?</p>\n", 'ViewCount': '81', 'Title': 'Deciding between two algorithms with similar runtime in two parameters', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-07T17:51:42.857', 'LastEditDate': '2013-07-07T17:51:42.857', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '13134', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><time-complexity><asymptotics>', 'CreationDate': '2013-07-07T13:25:17.193', 'Id': '13129'},90_118:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can I solve $\\mathcal{O}$-notations without using Java or any other programming language?</p>\n\n<p>I only want to use pen and paper.</p>\n', 'ViewCount': '325', 'ClosedDate': '2013-07-17T09:17:35.527', 'Title': "How to do Big 'O' notations", 'LastEditorUserId': '16189', 'LastActivityDate': '2014-03-27T17:04:35.730', 'LastEditDate': '2014-03-27T17:04:35.730', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '9210', 'Tags': '<algorithms><formal-languages><terminology><regular-languages><asymptotics>', 'CreationDate': '2013-07-17T05:51:43.213', 'Id': '13305'},90_119:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>$f(n) = 3n+3$ ;<br>\n$f(n) = O(n)$<br>\nBy definition :<br>\n$3n+3 \\leq c_1.n$<br>\nBy dividing both side by $n$<br>\n$3+\\frac{3}{n} \\leq c_1$<br>\nmeans we are getting constant range for $c_1$ for any $n$. Again it shows $c_1$'s value must be   greater than $3$ at any cost.<br>\ne.g. if we take $c_1$'s value 3.5 so $n$'s value will be $6$.<br>\nNow if we plot graph ( Because I want to learn this concept by understanding graph )<br>\n$c_1.g(n)$ graph goes below of $f(n)$ graph. I have taken following values for both functions :<br>\n$f(n)=3n+3$  </p>\n\n<p>$\n\\begin{matrix}\nn &amp; f(n)\\\\\n1 &amp; 6\\\\\n2 &amp; 9\\\\\n3 &amp; 12\\\\\n-2 &amp; -3\n\\end{matrix}\n$</p>\n\n<p>for $g(n) = 3.5n$<br>\n$\n\\begin{matrix}\nn &amp; g(n)\\\\\n1 &amp; 3.5\\\\\n2 &amp; 7\\\\\n3 &amp; 10.5\\\\\n-2 &amp; -7\n\\end{matrix}\n$ </p>\n\n<p>If we plot graph by these values it doesn't bind $f(n)$ i.e. $3n+3$ above by the value of $g(n)$ i.e. $3.5n$<br>\nCan anyone explain me this concept by graph ?</p>\n", 'ViewCount': '130', 'Title': 'Want to understand Big O by graph', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-29T08:04:13.840', 'LastEditDate': '2013-07-29T08:04:13.840', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<asymptotics>', 'CreationDate': '2013-07-25T12:40:19.470', 'Id': '13433'},90_120:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Some authors define $\\Omega$ in a slightly different way: let\u2019s use $\\Omega_\\infty$ (read \u201comega infinity\u201d) for this alternative definition. We say that $f(n)=\\Omega_\\infty(g(n))$ if there exists a positive constant $c$ such that $f(n)\\ge c \\cdot g(n)\\ge0$ for infinitely many integers $n$, whereas the usual $\\Omega$ requires that this holds for all integers greater than a certain $n_0$.</p>\n\n<p>Show that for any two functions $f(n)$ and $g(n)$ that are asymptotically nonnegative, either $f(n)=\\mathcal O(g(n))$ or $f(n)=\\Omega_\\infty(g(n))$ or both, whereas this is not true if we use $\\Omega$ in place of $\\Omega_\\infty$.</p>\n\n<p>Now my doubt is that how can I have both $f(n)=\\mathcal O(g(n))$ and $f(n)=\\Omega_\\infty(g(n))$\nas one requires $f$ to be lesser than $g$ and other requires it to be greater than $g$.</p>\n', 'ViewCount': '29', 'ClosedDate': '2013-07-28T20:56:55.487', 'Title': 'Order of Growth', 'LastEditorUserId': '3094', 'LastActivityDate': '2013-07-27T20:30:19.250', 'LastEditDate': '2013-07-27T20:30:19.250', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9376', 'Tags': '<asymptotics>', 'CreationDate': '2013-07-27T15:19:27.510', 'Id': '13466'},90_121:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What is the worst case running time to search for an element in a balanced binary search tree with $n 2^n$ elements?</p>\n\n<p>The answer is $\\Theta(n)$.</p>\n\n<p>My answer:</p>\n\n<p>To search an element in BST is $\\log (n)$ so\n$$\n\\begin{align*}\n\\log(n 2^n ) &amp;=  \\log(n) + \\log(2^n) \\\\\n            &amp;=  \\log(n) + n\\log 2 &amp; \\text{(base is 2)} \\\\  \n            &amp;=  \\log(n) + n  \n\\end{align*}\n$$</p>\n\n<blockquote>\n  <p>Why have they used $\\Theta$ in the answer?<br>\n  And why only $n$?   </p>\n</blockquote>\n', 'ViewCount': '819', 'Title': 'Worst case running time to search for an element in a balanced BST with $n2^n$ elements', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-19T12:00:56.800', 'LastEditDate': '2013-07-29T06:33:34.763', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<asymptotics>', 'CreationDate': '2013-07-28T11:18:13.813', 'Id': '13474'},90_122:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $W(n)$ and $A(n)$ denote, respectively, the worst case and average case running time of an<br>\n algorithm executed on an input of size $n$.<br>\n Which of the following is <em>always true</em>?<br>\n(A) $A(n) = \\Omega(W(n))$<br>\n(B) $A(n) = \\Theta(W(n))$<br>\n(C) $A(n) = O(W(n))$<br>\n(D) $A(n) = o(W(n))$</p>\n\n<p>Answer: (C).</p>\n\n<blockquote>\n  <p>What do all these options "mean"?\n  I find it hard to reconcile the graphs exemplifying asymptotic notation and abstract situations such as this one.</p>\n</blockquote>\n', 'ViewCount': '147', 'ClosedDate': '2013-07-29T08:01:33.390', 'Title': 'Making sense of asymptotic notation', 'LastEditorUserId': '683', 'LastActivityDate': '2013-07-28T14:39:43.147', 'LastEditDate': '2013-07-28T14:39:43.147', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<asymptotics>', 'CreationDate': '2013-07-28T13:15:59.730', 'Id': '13476'},90_123:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the following functions  </p>\n\n<ul>\n<li><p>$f(n) = 3n^{(n^{1/2})}  $</p></li>\n<li><p>$g(n) = 2^{(n^{1/2}) \\log n }$  // here base of $\\log n$ is 2   </p></li>\n<li><p>$h(n) = n !$</p></li>\n</ul>\n\n<p>Which of the following is true?</p>\n\n<ol>\n<li><p>$ h (n)$ is $O (f (n) )  $</p></li>\n<li><p>$h (n)$ is $O (g (n)) $ </p></li>\n<li><p>$g (n)$ is not $O (f (n) )  $</p></li>\n<li><p>$ f(n)$ is $O(g (n))$</p></li>\n</ol>\n\n<p>I tried this way :  </p>\n\n<p>To check which function is large :      </p>\n\n<p>$3n^{(n^{1/2})}$ = taking log   </p>\n\n<p>$n^{1/2} \\log 3n $  </p>\n\n<p>$g(n) = 2^{(n^{1/2}) \\log n }$ </p>\n\n<p>by taking log :   </p>\n\n<p>$((n^{1/2})\\log n )\\log 2$ => $0.3 (n^{1/2})\\log n  $</p>\n\n<p>$h(n) = n !$ => $n^n  $ </p>\n\n<p>by taking log :   </p>\n\n<p>$n \\log n $   </p>\n\n<p>So now comparing all these 3 functions I got following equality :  </p>\n\n<p>$f(n) &lt; g(n) &lt; h(n)  $ \nSo answer should be (4) but actually answer is (1).<br>\nCould anyone explain me where I am wrong ? </p>\n', 'ViewCount': '89', 'Title': 'Relations of some exponential functions -- which is correct?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-01T09:14:11.867', 'LastEditDate': '2013-08-01T09:14:11.867', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<asymptotics>', 'CreationDate': '2013-07-31T17:01:00.383', 'Id': '13545'},90_124:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<pre><code>f(n) = 3n+3 ;  \nf(n) = O(n)   \n</code></pre>\n\n<p>By definition :   </p>\n\n<pre><code>$3n+3 \\le c_1.n$   \n</code></pre>\n\n<p>By dividing both side by n<br>\n$3+(3/n) \\le c_1$<br>\nmeans we r getting constant range for $c_1$ for any $n$.\nAgain it shows c's value must be greater than 3 at any cost.<br>\ne.g. if we take c's value 3.5 so n's value will be 6.<br>\nNow if we plot graph ( Bcoz I want to learn this concept by understanding graph )<br>\n$cg(n)$ graph goes below of $f(n)$ graph. \nI have taken following values for both functions :  </p>\n\n<pre><code>f(n) : 3n+3   \nn  f(n) \n\n1   6  \n2   9  \n3   12   \n-2  -3 \n</code></pre>\n\n<p>for </p>\n\n<pre><code>g(n) = 3.5n   \nn   g(n)  \n1   3.5  \n2   7  \n3   10.5  \n-2  -7  \n</code></pre>\n\n<p>If we plot graph by these values it doesn't bind $f(n)$ i.e. $3n+3$ above by the value of $g(n)$ i.e. $3.5\\cdot n$<br>\nCan anyone explain me this concept by graph ?</p>\n", 'ViewCount': '102', 'LastEditorDisplayName': 'user742', 'Title': 'Want to understand Big O by graph', 'LastActivityDate': '2013-08-02T09:43:05.630', 'LastEditDate': '2013-08-02T09:43:05.630', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-2', 'OwnerDisplayName': 'user1745866', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<asymptotics>', 'CreationDate': '2013-07-25T12:04:59.710', 'Id': '13565'},90_125:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $f(n)$ and $g(n)$ be complexity functions. Are the following statements true?</p>\n\n<p>$$\n\\begin{equation}\n\\frac {f(n)}{g(n)}= O\\left(\\frac{f(n)}{ g(n)}\\right) \\\\\nf(n) \\times g(n) = O(\\max(f^2(n),g^2(n)))\n\\end{equation}\n$$</p>\n', 'ViewCount': '50', 'Title': 'Maximum complexity of a product and of a quotient', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-21T07:41:40.623', 'LastEditDate': '2013-08-21T07:41:40.623', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'CreationDate': '2013-08-21T03:24:32.137', 'Id': '13850'},90_126:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '838', 'Title': "What's Big O of $\\log(n+7)^{\\log(n)}$?", 'LastEditDate': '2013-08-25T11:38:43.547', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9815', 'FavoriteCount': '1', 'Body': "<p>As part of my continual improvement plans, I'm reviewing algorithm analysis and I've hit a roadblock. I cannot figure out how to determine the Big O complexity of $\\log(n+7)^{\\log(n)}$. I've spent the last few days rolling this over in my head and I still haven't made any progress in solving it. Could someone tell me what the Big O of this algorithm is, and explain how you solved it?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-03T17:16:56.050', 'CommentCount': '2', 'AcceptedAnswerId': '13913', 'CreationDate': '2013-08-24T17:35:13.647', 'Id': '13912'},90_127:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '116', 'Title': 'Is the DPLL algorithm complexity in terms of # of clauses or # of variables?', 'LastEditDate': '2013-08-29T06:21:14.013', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Roak', 'PostTypeId': '1', 'OwnerUserId': '9878', 'Body': '<p>I\'m a bit confused how worst case complexity is estimated for the <a href="http://en.wikipedia.org/wiki/DPLL_algorithm" rel="nofollow">DPLL algorithm</a>. Is it in terms of number of clauses, number of variables, or something else?</p>\n', 'Tags': '<time-complexity><asymptotics>', 'LastEditorUserId': '9878', 'LastActivityDate': '2013-08-30T21:42:43.867', 'CommentCount': '7', 'AcceptedAnswerId': '14048', 'CreationDate': '2013-08-28T21:02:28.893', 'Id': '14004'},90_128:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to prove (or disprove) the statement $n^{2} - n = O(n^3)$.</p>\n\n<p>As far as I understand, to prove this I need to find some $c, n_0$ such that $c &gt; 0 , n_0 &gt; 0$ and $0 \\leq f(n) \\leq cg(n)$. So I set $f(n) = n^2 - n$, and $g(n) = n^3$.</p>\n\n<p>This is where I get lost. I could plot both functions out on my calculator, and show that $f(n)$ always is below $cg(n)$ after a certain point, but not sure how to use that as a proof.</p>\n', 'ViewCount': '1106', 'ClosedDate': '2013-09-09T06:41:58.613', 'Title': 'Proving/disproving Big O notation statements', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-08T23:33:52.683', 'LastEditDate': '2013-09-08T17:29:27.463', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10043', 'Tags': '<asymptotics>', 'CreationDate': '2013-09-08T17:25:03.183', 'Id': '14213'},90_129:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '70', 'Title': 'Tight asymptotic bound for recursive algorithm', 'LastEditDate': '2013-09-11T11:39:35.377', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5066', 'FavoriteCount': '1', 'Body': "<p>I have this algorithm where:</p>\n\n<p>$$\nT(n) =\n\\begin{cases}\n  1 &amp; \\text{if}\\; n \\le 1 \\\\\n  T(n/2) + 1 &amp; \\text{otherwise} \\\\\n\\end{cases}\n$$</p>\n\n<p>So, evaluating for $T(0), T(1), T(2), T(3), \\ldots, T(n)$, I'm getting values like:\n$$ 1, 1, 2, 2, 3, 3, \\ldots, n, n $$</p>\n\n<p>I assume this is twice the sum of $1$ to $n$, that would be the same as $n (n+1)$ or $n^2+2$.</p>\n\n<p>Is my assumption ok?</p>\n", 'Tags': '<algorithm-analysis><asymptotics><recursion>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-11T13:19:20.067', 'CommentCount': '3', 'AcceptedAnswerId': '14269', 'CreationDate': '2013-09-11T05:05:47.037', 'Id': '14264'},90_130:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am new to Advanced Algorithms and I have studied various samples on Google and StackExchange. What I understand is:</p>\n\n<ol>\n<li><p>We use $O(\\log n)$ complexity when  there is division of any $n$ number on each recursion (especially in divide and conquer).</p></li>\n<li><p>I know that for binary search, we have time complexity $O(n \\log n)$, I understood $\\log n$ is because each time it halves the full $n$ size number list in a recursive manner until it finds the required element. But why is it multiplied with $n$ even we just traverse half of the $n$ size element for each execution so why we multiply $\\log n$ with $n$?</p></li>\n<li><p>Please give me any example explaining the complexity $O(n^2 \\log n)$. I hope this will help me in understanding much better the above two questions.</p></li>\n</ol>\n', 'ViewCount': '218', 'Title': 'Confusion regarding several time complexities including the logarithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:56:44.833', 'LastEditDate': '2013-09-16T08:56:44.833', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10129', 'Tags': '<algorithms><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2013-09-13T23:31:05.913', 'Id': '14299'},90_131:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So I'm reading Introductions to Algorithms and sometimes I wish it would be a little bit more friendly with how it explains topics. One of these topics is proving big-theta. </p>\n\n<p>I understand that the definition of $\\Theta$ is as follows: </p>\n\n<blockquote>\n  <p>$\\Theta(g(n)) = f(n)$ if there exists positive constants $c_1$, $c_2$, and $n_0$ such that $0 \\le c_1 g(n) \\le f(n) \\le c_2 g(n)$ for all $n \\ge n_0$.</p>\n</blockquote>\n\n<p>In other words, for $f(n)$ and $g(n)$, $f(n)$ can be bounded by $c_2 g(n)$ from above and bounded below by $c_1 g(n)$. </p>\n\n<p>So the example in the book goes: </p>\n\n<blockquote>\n  <p>Show that $\\frac{1}{2} n^2 - 3 n = \\Theta(n^2)$.</p>\n</blockquote>\n\n<p>From the definition of big theta: </p>\n\n<p>$$ c_1 n^2 \\le \\frac{1}{2} n^2 - 3 n \\le c_2 n^2$$</p>\n\n<p>CLRS begins by dividing by the largest order term of $n$ which is $n^2$ to get </p>\n\n<p>$$ c_1 \\le \\frac{1}{2} - \\frac{3}{n} \\le c_2 $$</p>\n\n<p>From here we split the problem into two parts, the right-hand inequality and the left-hand inequality. </p>\n\n<p>On the right hand side: CLRS chooses $c_2 \\ge \\frac{1}{2}$ because for $n \\gt 1$, $\\frac{1}{2} - \\frac{3}{n}$ can never be less than $\\frac{1}{2}$ since $\\frac{3}{n}$ goes to $0$ as $n$ goes to infinity. (I'm assuming they choose $n \\gt 1$ here because if $n=0$ then we would be dividing by $0$.)</p>\n\n<p><em>Now this is where I start to get lost.</em> </p>\n\n<p>On the left hand side: CLRS chooses  $c_1 = \\frac{1}{14}$ (not sure why) and $n \\ge 7$. I'm not sure what the significance of these choices is. At $n \\le 6$, $\\frac{1}{2}-\\frac{3}{n}$ becomes negative. But why $\\frac{1}{14}$ for $c_2$? I'm just not sure how they arrived at solving the left hand side and the book doesn't really explain it well for me.   </p>\n", 'ViewCount': '145', 'Title': 'Trouble understanding how to pick constants to prove big theta', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-15T10:08:13.853', 'LastEditDate': '2013-09-15T10:04:36.650', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14327', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10152', 'Tags': '<complexity-theory><asymptotics>', 'CreationDate': '2013-09-15T08:14:47.893', 'Id': '14326'},90_132:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '120', 'Title': 'Is $\\Theta$ symmetric?', 'LastEditDate': '2013-09-16T08:36:54.397', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'user65165', 'PostTypeId': '1', 'OwnerUserId': '7307', 'Body': '<p>For example if \n$$ f(x)= \\Theta (g(x)) $$</p>\n\n<p>from the definition of the theta notation, there exist c1 and c2 constants such that</p>\n\n<p>$$c_1 g(x) \\le f(x) \\le c_2 g(x)$$</p>\n\n<p>then if only we took the constants $1/c_1$ and $1/c_2$ we could say from the definition that </p>\n\n<p>$$ g(x)= \\Theta (f(x)) $$</p>\n\n<p>Right?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:36:54.397', 'CommentCount': '1', 'AcceptedAnswerId': '14341', 'CreationDate': '2013-09-15T18:58:05.713', 'Id': '14339'},90_133:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>This is not homework. I have the solution but it's not what I'm getting. I know there are multiple solutions to the problem but I want to make sure that I'm not missing anything. </p>\n\n<p>The question is as follows:</p>\n\n<blockquote>\n  <p>Prove that 2$n^2$\n   - 4n + 7 = \u0398 ($n^2$). give the values of the constants and show your work.</p>\n</blockquote>\n\n<p>Here is how I approached the problem: </p>\n\n<p>From the definition of \u0398(g(n)): </p>\n\n<p>0 \u2264 C<sub>1</sub>$n^2$ \u2264 2$n^2$ - 4n + 7 \u2264 C<sub>2</sub>$n^2$</p>\n\n<p>Divide the inequality by the largest order n-term. (This is the only way I know how to solve these equations.) </p>\n\n<blockquote>\n  <p>0 \u2264 C<sub>1</sub> \u2264 2 - (4/n - 7/$n^2$) \u2264 C<sub>2</sub></p>\n</blockquote>\n\n<p>Split the problem into two parts: LHS and RHS. </p>\n\n<p>We start with the RHS:</p>\n\n<p>Find constant C<sub>2</sub> that will satisfy</p>\n\n<blockquote>\n  <p>0 \u2264 2 - (4/n - 7/$n^2$) \u2264 C<sub>2</sub></p>\n</blockquote>\n\n<p>n=1, (2 - (4/1 - 7/$1^2$)) = 5</p>\n\n<p>n=2, (2 - (4/2 - 7/$2^2$)) = 7/4 </p>\n\n<p>n=3, (2 - (4/3 - 7/9)) =  13/9   </p>\n\n<p>We choose C<sub>2</sub> to be 2, n\u22652 to satisfy the RHS. </p>\n\n<p>LHS: we try to find a constant that will satisfy</p>\n\n<blockquote>\n  <p>0 \u2264 C<sub>1</sub> \u2264 2 - (4/n - 7/$n^2$)</p>\n</blockquote>\n\n<p>From above, we know that after n=2, the equation approaches 2 as n grows larger, so if we pick a constant that is less than 2 then it should satisfy the LHS. </p>\n\n<p>We choose C<sub>1</sub> to be 1.\nFor n, choosing 1 would satisfy the left hand side, but since the RHS needs n\u22652, we stick with it. </p>\n\n<p>So the constants that prove 2$n^2$ - 4n + 7 = \u0398 ($n^2$) are </p>\n\n<blockquote>\n  <p>C<sub>1</sub> = 1 , C<sub>2</sub> = 2 , n\u22652</p>\n</blockquote>\n\n<p>The given solution to this problem chooses n\u22654, but I'm not sure why. It seems that n\u22652 would work fine. Am I wrong somewhere? </p>\n\n<p>If I'm not wrong, if I would have picked C<sub>1</sub> to also be 2, wouldn't that also satisfy the left hand side since the inequality allows it to be \u2264? </p>\n", 'ViewCount': '818', 'Title': 'Big Theta Proof on polynomial function', 'LastEditorUserId': '10152', 'LastActivityDate': '2013-09-16T00:21:52.253', 'LastEditDate': '2013-09-16T00:13:51.397', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14346', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10152', 'Tags': '<asymptotics>', 'CreationDate': '2013-09-16T00:08:14.077', 'Id': '14344'},90_134:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I already have a solution for this problem but it's just not making sense to me. </p>\n\n<p>Here is the problem (It's from Introduction to Algorithms by CLRS found in CH.4): </p>\n\n<blockquote>\n  <p>Show $T(n) = 2T(\\lfloor n/2 \\rfloor +17)+n$  is $O(n \\log n)$</p>\n</blockquote>\n\n<p>This is what I have so far: </p>\n\n<p>So Assume $T(k) \\leq cn\\lg n$, for $k&lt;n$.</p>\n\n<p>$\\qquad \\begin{align*}\n  T(n) &amp;= 2T(\\lfloor n/2 \\rfloor +17)+n \\\\\n       &amp;\\leq 2c(\\lfloor n/2 \\rfloor +17)\\lg(\\lfloor n/2 \\rfloor + 17) +n \\\\\n       &amp;\\leq 2c(n/2 + 17) \\lg (n/2 + 17) + n \\\\\n       &amp;= c(n + 34) \\lg((n+34)/2)+ n\n\\end{align*}$</p>\n\n<p>And this is where I stop understanding what is going on. Looking at the solution to this problem tells me: </p>\n\n<blockquote>\n  <p>Note that $(n + 34)/2 \\leq (3n)/4$ for $n \\geq 68$\n  so that $\\lg((n + 34)/2) \\leq \\lg((3n)/4) = \\lg(n) + \\lg(3/4)$ for $n \\geq 68$.</p>\n</blockquote>\n\n<p>But it fails to tell me why/how we know that $(n+34)/2 \\leq 3n/4$ for $n \\geq 68$. Where did this number come from and how would I arrive at this if I did not know the solution beforehand? </p>\n", 'ViewCount': '1319', 'Title': 'Solving recurrences using substitution method', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-30T00:18:19.787', 'LastEditDate': '2013-09-16T08:49:52.240', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '14352', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10152', 'Tags': '<asymptotics><proof-techniques><recurrence-relation>', 'CreationDate': '2013-09-16T05:33:59.237', 'Id': '14347'},90_135:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>If the running time of an algorithm scales linearly with the size of its input, we say it has $O(N)$ complexity, where we understand <code>N</code> to represent input size.</p>\n\n<p>If the running time does not vary with input size, we say it\'s $O(1)$, which is essentially saying it varies proportionally to 1; i.e., doesn\'t vary at all (because 1 is constant).</p>\n\n<p>Of course, 1 is not the only constant. <em>Any</em> number could have been used there, right? (Incidentally, I think this is related to the common mistake many CS students make, thinking "$O(2N)$" is any different from $O(N)$.)</p>\n\n<p>It seems to me that 1 was a sensible choice. Still, I\'m curious if there is more to the etymology there\u2014why not $O(0)$, for example, or $O(C)$ where $C$ stands for "constant"? Is there a story there, or was it just an arbitrary choice that has never really been questioned?</p>\n', 'ViewCount': '225', 'Title': 'Why is it O(1) (and not, say, O(2))?', 'LastEditorUserId': '9574', 'LastActivityDate': '2013-09-19T17:17:20.410', 'LastEditDate': '2013-09-18T20:36:25.443', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9574', 'Tags': '<terminology><time-complexity><asymptotics><landau-notation><history>', 'CreationDate': '2013-09-18T16:39:15.317', 'FavoriteCount': '2', 'Id': '14416'},90_136:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Problem taken from here (page 3): <a href="http://cse.unl.edu/~choueiry/S06-235/files/MasterTheorem-Handout.pdf" rel="nofollow">http://cse.unl.edu/~choueiry/S06-235/files/MasterTheorem-Handout.pdf</a></p>\n\n<p>$T(n) = 3T(\\frac{n}{2}) + \\frac{3}{4}n + 1$</p>\n\n<p>$f(n) = \\frac{3}{4}n + 1$</p>\n\n<p>It says we cannot use the traditional Master Theorem because $f(n)$ is not a polynomial. How is $\\frac{3}{4}n + 1$ not a polynomial? It\'s a polynomial of degree one with a fractional coefficient.</p>\n', 'ViewCount': '165', 'Title': 'Solving a recurrence with the Master Theorem', 'LastActivityDate': '2013-09-20T23:36:30.247', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10092', 'Tags': '<asymptotics><master-theorem>', 'CreationDate': '2013-09-20T20:56:02.193', 'Id': '14482'},90_137:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have a program p that has time complexity $O(n)$ and a second program q that calls program p $m$ times. If we know the input size of p will be the same every one of those times (n), we can say that the complexity of q is $O(m \\times n)$. This case sounds pretty simple to me.</p>\n\n<p>However, suppose the input size of p varies over these $m$ times, from $0$ to $m-1$.</p>\n\n<p>If we knew that p ran precisely $f(n) = a \\times n$ instructions for an input of size $n$, it would be simple: q would run $a \\times 0 + a \\times 1 + \\ldots + a \\times (m-1) = am(m-1)/2$ instructions exactly, which is $O(m^2)$.</p>\n\n<p>However, in this case, all we know is that $f(n)$ is below $a \\times n$ for all $n$ greater than some $n_0$, by the definition of big-O. We can't say for sure that $f(0) + f(1) + ... + f(m-1)$ is below $a \\times 0 + a \\times 1 + \\ldots + a \\times (m-1)$, because we don't know that $0, 1, \\ldots, m-1$ are greater than this $n_0$. For this reason, I don't think we can say that q runs in $O(m^2)$.</p>\n\n<p>Is there a way, in this case, when all we know is the big-O behaviour of p, to analyze q?</p>\n\n<p>What if we replace all the big-O's with big-Theta's?</p>\n", 'ViewCount': '86', 'Title': 'Big O and program calls with varied input sizes', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-17T16:01:47.387', 'LastEditDate': '2013-11-17T16:01:47.387', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14535', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10283', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-09-22T23:26:38.763', 'Id': '14532'},90_138:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '83', 'Title': 'How to solve for recurrence with substitution or other methods?', 'LastEditDate': '2013-09-23T03:11:57.373', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10289', 'Body': "<p>Forgive me if I am new, I am trying to learn how to solve recurrences.</p>\n\n<p>I have the following recurrence:</p>\n\n<p>$$T(n) = 2 T(\\lfloor\\frac{n}{3}\\rfloor) + \\frac{1}{2} T(\\lfloor\\frac{2n}{3}\\rfloor) + n^2 \\text{ if } n&gt;0$$</p>\n\n<p>Now from my understanding is that I am not able to use the following methods.</p>\n\n<ol>\n<li>Master Thereom (because it's not in the form $aT(\\frac{n}{b}) + f(n)$)</li>\n<li>Tree Method (because of the $\\frac{1}{2} T$, you cannot have a node of $\\frac{1}{2}$), please correct me if I am wrong.</li>\n</ol>\n\n<p>So which leaves me with the following method:</p>\n\n<p>Only the substitution method, but what I don't understand that is the fact that for the substitution method, you have to guess for the $f(n)$ to substitute into the recurrence.</p>\n\n<p>So my question is, how does one select the correct $f(n)$ to substitute?</p>\n", 'ClosedDate': '2013-09-23T07:27:35.290', 'Tags': '<asymptotics><recurrence-relation>', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-09-23T06:47:52.253', 'CommentCount': '5', 'AcceptedAnswerId': '14547', 'CreationDate': '2013-09-23T02:28:36.440', 'Id': '14543'},90_139:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>For analyzing the running time of an algorithm , I'm stuck with this recursive equation :\n$$\nT(n) = \\log(n) \\cdot T(\\log n) + n\n$$\nObviously this can't be handled with the use of the Master Theorem, so I was wondering if anybody has any ideas for solving this recursive equation.</p>\n\n<p>I'm pretty sure that it should be solved with a change in the parameters, like considering $n$ to be $2^m$, but I couldn't manage to find any good fix.</p>\n", 'ViewCount': '371', 'Title': 'Recursive equation for complexity: T(n) = log(n) * T(log(n)) + n', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-03T11:25:54.990', 'LastEditDate': '2013-10-03T11:25:54.990', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14777', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10478', 'Tags': '<asymptotics><runtime-analysis><recursion><master-theorem>', 'CreationDate': '2013-10-03T09:53:19.790', 'Id': '14775'},90_140:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Big-O notation hides constant factors, so some $O(n)$ algorithms exist that are infeasible for any reasonable input size because the coefficient on the $n$ term is so huge.</p>\n\n<p>Are there any known algorithms whose runtime is $O(f(n))$ but with some low-order $o(f(n))$ term that is so huge that for reasonable input sizes it completely dominates the runtime?  I'd like to use an algorithm like this an an example in an algorithms course, as it gives a good reason why big-O notation isn't everthing.</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '180', 'Title': 'Example of an algorithm where a low-order term dominates the runtime for any practical input?', 'LastActivityDate': '2013-12-04T17:31:23.557', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><asymptotics>', 'CreationDate': '2013-10-05T07:50:06.317', 'FavoriteCount': '2', 'Id': '14823'},90_141:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>So I\'m studying for a midterm and my professor put out a sample exam with the answers, and I\'m stuck on one of the questions.</p>\n\n<p><img src="http://i.stack.imgur.com/lexzm.png" alt="enter image description here"></p>\n\n<p>The answer is <code>Big-O(n^2 log n)</code></p>\n\n<p>Could someone show me the steps necessary to go from regular time complexity to asymptotic time complexity?</p>\n', 'ViewCount': '117', 'Title': 'Finding asymptotic time complexity', 'LastEditorUserId': '10424', 'LastActivityDate': '2013-10-06T21:35:47.470', 'LastEditDate': '2013-10-06T21:35:47.470', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14846', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10424', 'Tags': '<time-complexity><asymptotics>', 'CreationDate': '2013-10-06T02:04:33.010', 'Id': '14845'},90_142:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m taking the MIT Open Courseware for Introduction to Algorithms and I\'m having trouble understanding the first homework problem/solution.</p>\n\n<p>We are supposed to compare the asymptotic complexity (big-O) for different functions:</p>\n\n<p>$f_1(n) = n^{0.999999}\\log(n)$<br>\n$f_2(n) = 10000000n$</p>\n\n<p>$f_2(n)$ is obviously O(n), but the big-O given for $f_1(n)$ confuses me and I don\'t follow the given solution.</p>\n\n<p>The solution says $f_1(n)$ has less Big-O complexity than $f_2(n)$:</p>\n\n<p>"recall that for any $c &gt; 0$, $log(n)$ is $O(n^c)$.</p>\n\n<p>Therefore we have: $f(n) = n^{0.999999}log(n) = O(n^{0.999999}n^{0.000001}) = O(n) = O(f_2(n))$"</p>\n\n<p>I do not understand the logic underlying the solution. I may be forgetting something simple? Can someone break it down for me? Thanks!</p>\n', 'ViewCount': '131', 'Title': 'Big O Notation of $n^{0.999999}\\log(n)$', 'LastActivityDate': '2013-10-06T23:38:39.473', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10536', 'Tags': '<algorithms><asymptotics>', 'CreationDate': '2013-10-06T23:12:56.490', 'FavoriteCount': '1', 'Id': '14864'},90_143:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm just wondering what the correct notation is when referring to an average case complexity of an algorithm that was calculated by doing empirical analysis.</p>\n\n<p>For example, I have tested my algorithm and fitted the results to the curve $f(n)=2.65\\times 10^{-15}\\cdot(2.17^{n})$ and in my report right now I'm saying something like: </p>\n\n<blockquote>\n  <p><em>the average case complexity was found to be $\\approx 2.65\\times 10^{-15}\\cdot(2.17^{n})$</em>, </p>\n</blockquote>\n\n<p>but I would rather say something like </p>\n\n<blockquote>\n  <p><em>the average case complexity is $\\in \\Theta(2.17^{n})$</em>. </p>\n</blockquote>\n\n<p>But I'm not sure if this is technically correct because the result hasn't been theoretically proven, only empirically tested and fitted to the curve.</p>\n", 'ViewCount': '69', 'Title': 'Notation for average case complexity of an algorithm', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-09T17:53:03.743', 'LastEditDate': '2013-10-09T17:26:34.530', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14961', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2562', 'Tags': '<asymptotics><notation>', 'CreationDate': '2013-10-09T17:12:05.483', 'Id': '14960'},90_144:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $O(n)$ be "Big-O" of $n$ and $o(n)$ be "Small-O" of $n$.</p>\n\n<p>It is a well-known fact that $O(n \\log{n}) \\subset O(n^{1 + \\epsilon})$ for any $\\epsilon &gt; 0$. Can we omit the $\\epsilon$, and just type $O(n \\log{n}) \\subset O(n^{1 + o(1)})$?</p>\n', 'ViewCount': '106', 'Title': 'Is $\\log{n}$ bounded from above by $n^{o(1)}$?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-19T16:34:55.637', 'LastEditDate': '2013-10-18T21:20:54.777', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '16211', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8887', 'Tags': '<asymptotics><landau-notation>', 'CreationDate': '2013-10-18T20:53:30.907', 'Id': '16210'},90_145:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>What notation is used to discuss the coefficients of functions in big-O notation?</p>\n\n<p>I have two functions:</p>\n\n<ul>\n<li>$f(x) = 7x^2 + 4x +2$</li>\n<li>$g(x) = 3x^2 + 5x +4$</li>\n</ul>\n\n<p>Obviously, both functions are $O(x^2)$, indeed $\\Theta(x^2)$, but that doesn't allow a comparison further than that.  How do I discuss the the coefficients 7 and 3. Reducing the coefficient to 3 doesn't change the asymptotic complexity but it still makes a significant difference to runtime/memory usage.<br/></p>\n\n<p>Is it <b>wrong</b> to say that $f$ is $O(7x^2)$ and $g$ is $O(3x^2)$ ?\nIs there other notation that does take coefficients into consideration? Or what would be the best way to discuss this?</p>\n", 'ViewCount': '111', 'Title': 'How to discuss coefficients in big-O notation', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-28T07:34:55.270', 'LastEditDate': '2013-10-28T07:28:08.003', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '16347', 'Score': '9', 'OwnerDisplayName': 'El Bee', 'PostTypeId': '1', 'Tags': '<terminology><asymptotics><landau-notation>', 'CreationDate': '2013-10-07T23:11:30.400', 'Id': '16346'},90_146:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am unclear about finding the memory complexity of an algorithm.</p>\n\n<p>Some places refer memory complexity as what container would be carrying for instance:</p>\n\n<pre><code>for i = 1 to n-1\n     if d[i] == d[i + 1]\n           d[i] = (d[i] + 5) mod 13\n</code></pre>\n\n<p>Is considered as having $\\theta(N)$ memory complexity.</p>\n\n<p>At some other places how much data we write to a container is a complexity for instance:</p>\n\n<pre><code>reverse_list(n)\n\n    Stack res\n    while (n != NULL)\n         res push n\n         n = n-&gt;next\n    while (res != null) \n         a = pop res\n         print a\n</code></pre>\n\n<p>Is considered as having a memory complexity of $\\theta(N)$ too. Moreover:</p>\n\n<p>Such thing is considered having $\\theta(1)$ memory complexity</p>\n\n<pre><code>reverse_list(head)\n    last = NULL;\n    while(last != head)\n        current = head\n        while(current-&gt;next != last)\n            current = current-&gt;next\n        print current\n        last = current\n</code></pre>\n\n<p>I know how these algorithms work and what they do, but I don't understand how are we meant to be analysing their memory complexity. Could someone explain that please?</p>\n", 'ViewCount': '472', 'Title': 'Memory complexity?', 'LastEditorUserId': '8849', 'LastActivityDate': '2013-10-28T15:14:42.793', 'LastEditDate': '2013-10-28T15:14:42.793', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '16464', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics><space-complexity>', 'CreationDate': '2013-10-27T04:26:35.907', 'Id': '16461'},90_147:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I need to prove by definition (show that for every $C$ there exists a $n_0$) that: $$5n^2+3n= o(n^3-4n)$$</p>\n\n<p>By trying to simplify the expression I get to the point where I should prove that: $$ \\frac{5n^2+3n}{n^3-4n} &lt; C$$.</p>\n\n<p>Now, showing that $\\lim\\limits_{x\\to\\infty} \\frac{5n^2+3n}{n^3-4n} = 0$ isn't a tough mission, but how do I prove it by definition? How do I point out specific $n_0$ for every $C$ that comes up? I'm asked to provide a tight answer and not let $n_0 = C^C$ so that the $n_0$ for sure will be large enough but to point out $n_0$ that is reasonably small.</p>\n", 'ViewCount': '82', 'Title': 'Proving Little-Oh notation by definition', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-28T07:40:45.093', 'LastEditDate': '2013-10-28T07:40:45.093', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10998', 'Tags': '<asymptotics>', 'CreationDate': '2013-10-27T13:12:26.477', 'Id': '16470'},90_148:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I cannot really find a source that does not use the same examples provided by CLRS. I need a simpler example than <code>MULTI-POP</code> example. Could someone provide an example and explain me:</p>\n\n<p>a) What is the difference between worst-case analysis and amortised analysis?</p>\n\n<p>b) What is the difference between average-case analysis and amortised analysis?</p>\n\n<p>c) In plain English(with using minimal notations), how can we provide a complexity(especially I am interested in potential method)</p>\n', 'ViewCount': '89', 'Title': 'Basics of Amortised Analysis', 'LastActivityDate': '2013-10-28T16:55:38.243', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithm-analysis><asymptotics><amortized-analysis><average-case>', 'CreationDate': '2013-10-28T15:14:12.807', 'FavoriteCount': '1', 'Id': '16502'},90_149:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '71', 'Title': 'Clarification about big Oh calculation', 'LastEditDate': '2013-10-30T10:36:42.927', 'AnswerCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9743', 'FavoriteCount': '1', 'Body': '<p>Say you have a loop that iterates over an array, </p>\n\n<pre><code>for i in someArray:\n    //some code\n</code></pre>\n\n<p>This basic example would have a running time of $O(n)$. Say that you added a nested loop with equal number of operations, this then would be $O(n^2)$. My question is, is it safe to do this kind of simplication in general? For example, </p>\n\n<p>Say your outer loop had worst case complexity of $O(n^2)$ and your inner loop has worst case complexity of $O(\\log n)$. Can the total time complexity be said as $O(n^2\\log n)$? </p>\n', 'Tags': '<asymptotics>', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-30T17:02:21.843', 'CommentCount': '3', 'AcceptedAnswerId': '16567', 'CreationDate': '2013-10-30T03:53:19.790', 'Id': '16564'},90_150:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>How would you determine big O notation for $\\log^b x$? I don't think you can simply say $O(\\log^b x)$, can you?</p>\n\n<p>If you can, then here is a better question: $x^3 + \\log^b x$. How would you know if it's $O(x^3)$ or something else depending on the $b$ value?</p>\n", 'ViewCount': '88', 'Title': 'How to find the big O notation of $\\log^b x$', 'LastEditorUserId': '683', 'LastActivityDate': '2013-11-12T03:29:57.240', 'LastEditDate': '2013-11-07T04:51:48.520', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '16782', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11229', 'Tags': '<asymptotics>', 'CreationDate': '2013-11-06T23:22:09.930', 'Id': '16781'},90_151:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to estimate with big O the following expressions:</p>\n\n<p>$$\n\\begin{align*}\nf_1(x) &amp;= \\frac{x^4 + x^2 + 1}{x^4 + 1}, \\\\\nf_2(x) &amp;= \\frac{x^3 + 5 \\log x}{x^4 + 1}.\n\\end{align*}\n$$</p>\n\n<p>How do you eliminate the constant on the bottom of the fraction? How would you work to solve these types of problems?</p>\n', 'ViewCount': '156', 'Title': 'How to solve this big O notation of a fraction', 'LastEditorUserId': '683', 'LastActivityDate': '2013-11-08T14:05:28.327', 'LastEditDate': '2013-11-07T00:28:54.797', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11229', 'Tags': '<asymptotics>', 'CreationDate': '2013-11-06T23:48:24.650', 'Id': '16783'},90_152:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Following up on <a href="http://cs.stackexchange.com/a/10373/98">vonbrand\'s answer</a> I want to write a small document about stronger master theorems for our students, one of which is the Akra-Bazzi theorem. I have copied the theorem from their paper [1] and found -- besides a small notational confusion\xb2 -- the following problem.</p>\n\n<p>The authors require (emphasis mine):</p>\n\n<blockquote>\n  <p>$g(x)$ is defined for real values $x$, and is <strong>bounded</strong>, positive and nondecreasing function\n  $\\forall x \\geq 0$</p>\n</blockquote>\n\n<p>Here, $g$ is the toll function, that is the recurrence has the form</p>\n\n<p>$\\qquad\\displaystyle T(n) = g(n) + \\sum_{i=1}^k a_i T\\bigl(\\lfloor n b_i^{-1} \\rfloor\\bigr)$.</p>\n\n<p>Now, at the end of their paper (p209) they give multiple examples for applying their result and they use functions in $\\Omega(n)$ which are clearly <em>not</em> bounded.</p>\n\n<p>From skimming the proof they mainly seem to require that integrals of the form</p>\n\n<p>$\\qquad\\displaystyle \\int_a^b \\frac{g(x)}{x^{p+1}} dx$</p>\n\n<p>have finite values. So, requiring $g$ to be bounded <em>on every compact interval</em> might be sufficient; I did not work through the proof in detail. Is it possible they mean that?</p>\n\n<p>My question is: How should the Akra-Bazzi theorem be stated so that it is consistent with proof <em>and</em> examples?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1023/A%3A1018373005182">On the solution of linear recurrence equations</a> by M. Akra and L. Bazzi (1998)</li>\n<li>They require $a_i \\in R^{*+}$. Is this some notation I don\'t know, or a typo? I assume the intended meaning is $(0,\\infty) \\subseteq \\mathbb{R}$.</li>\n</ol>\n', 'ViewCount': '69', 'Title': 'Why does Akra-Bazzi need that toll-function g is bounded?', 'LastActivityDate': '2014-01-11T00:22:27.687', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2013-11-11T14:29:28.533', 'Id': '17916'},90_153:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>While thinking about <a href="http://cs.stackexchange.com/questions/14343/evaluating-recurrence-relation">this question</a> on a recurrence I checked out some stronger master theorems.\nUnfortunately, they do not seem to apply because terms</p>\n\n<p>$\\qquad\\displaystyle T(n) = \\dots + T(n-1) + \\dots$</p>\n\n<p>are not covered.</p>\n\n<ul>\n<li><p>Roura [1] investigates (in the discrete case) recurrences of the form</p>\n\n<p>$\\qquad\\displaystyle T(n) = t_n + \\sum_{1 \\leq d \\leq D} (w_d + r_{d,n}) \\cdot T(z_d n + s_{d,n})$</p>\n\n<p>where $\\sum |r_{d,n}|$ and $\\sum |s_{d,n}|/n$ have to vanish for $n \\to \\infty$ and, sadly, $0 &lt; z_d &lt; 1$. I did not go through the proof, but an application to <a href="http://cs.stackexchange.com/questions/14343/evaluating-recurrence-relation">the example at hand</a> yielded a quite clearly wrong result of $T \\in \\Theta(g)$, so this one does not carry over as is.</p></li>\n<li><p>Akra and Bazzi [2] consider only recurrences of the form</p>\n\n<p>$\\qquad\\displaystyle T(n) = g(n) + \\sum_{i=1}^k a_i T(\\lfloor n/b_i \\rfloor)$</p>\n\n<p>with $b_i \\geq 2$, so above parameter choice is clearly not covered.</p>\n\n<p>Leighton [3] introduces a more general form of Akra-Bazzi for recurrences of the form</p>\n\n<p>$\\qquad\\displaystyle T(x) = g(x) + \\sum_{i=1}^k a_iT(b_ix + h_i(x))$,</p>\n\n<p>but similarly requires that $b_i \\in (0,1)$. It seems to me that existence of $p$ as solution of</p>\n\n<p>$\\qquad\\displaystyle \\sum_{i=1}^k a_i b_i^p = 1$</p>\n\n<p>in Theorem 2 can no longer be guaranteed if there is one $b_i=1$ since all $a_i$ are positive.</p></li>\n<li><p>Drmota and Szpankowski [4] look at recurrences of the form</p>\n\n<p>$\\qquad\\displaystyle T(n) = a_n + \\sum_{j=1}^m b_jT(\\lfloor p_j n + \\delta_j \\rfloor)$</p>\n\n<p>with $0 &lt; p_j &lt; 1$. The proof does not explicitly use $p_j \\neq 1$. I am not too sure about the properties of several Dirichlet series (cf section 4.1) if some $p_j=1$, though.</p></li>\n</ul>\n\n<p>Does any of the theorems hold if there are recursive parameters of the form $T(n-c)$? Can one be adapted to cover such cases, or are there other, more general results that do so?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1007/3-540-63165-8_201" rel="nofollow">An improved master theorem for divide-and-conquer recurrences</a> by S. Roura (1997)</li>\n<li><a href="http://dx.doi.org/10.1023/A%3A1018373005182" rel="nofollow">On the solution of linear recurrence equations</a> by M. Akra and L. Bazzi (1998)</li>\n<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.1636&amp;rep=rep1&amp;type=pdf" rel="nofollow">Notes on Better Master Theorems for Divide-and-Conquer Recurrences</a> by T. Leighton (1996)</li>\n<li><a href="http://dl.acm.org/citation.cfm?id=2133036.2133064" rel="nofollow">A master theorem for discrete divide and conquer recurrences</a> by  M. Drmota and W. Szpankowski. (2011)</li>\n</ol>\n', 'ViewCount': '49', 'Title': 'Are there master theorems that deal with parameters of the form $n-c$?', 'LastActivityDate': '2013-11-12T14:27:37.343', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2013-11-12T14:27:37.343', 'Id': '17960'},90_154:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I encountered these few recursive relations:</p>\n\n<p><code>T(n) = (n/2)T(n/2) + O(n^2)</code></p>\n\n<p><code>T(n) = T(n/2) + T(n/2-1) + ... + T(1) + O(n^2)</code></p>\n\n<p>I do notice that these recursions can be solved faster by DP. I'm just curious as to:</p>\n\n<ol>\n<li>What's the big-O of the running time of these relations, if we follow the resursive formula</li>\n<li>How do you analysis recursive relations like these.</li>\n<li>Does the trailing term (in this case O(n^2)) matter?</li>\n</ol>\n\n<p>Any help is much appreciated.</p>\n", 'ViewCount': '60', 'ClosedDate': '2013-11-13T21:56:00.057', 'Title': 'Running time of this recursive relation: T(n) = (n/2)T(n/2) + O(n^2)', 'LastActivityDate': '2013-11-13T18:49:11.920', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '17993', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11356', 'Tags': '<algorithm-analysis><asymptotics>', 'CreationDate': '2013-11-13T18:21:34.540', 'Id': '17992'},90_155:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have to estimate the computational complexity of some algorithm that does $\\psi_q(m)$ iterations. Assume that all inputs $m$ are coprime to $q$.</p>\n\n<p>So I need to know what growth the $\\psi(m)$ has.</p>\n', 'ViewCount': '44', 'Title': u'What is growth of $\\psi_q(m) = \\min \\{ p: m \u2223 (q^p\u22121) \\}$ for fixed $q$?', 'LastEditorUserId': '8887', 'LastActivityDate': '2013-11-14T23:39:56.287', 'LastEditDate': '2013-11-14T23:11:45.677', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '18035', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8887', 'Tags': '<asymptotics><number-theory>', 'CreationDate': '2013-11-14T22:44:13.930', 'Id': '18033'},90_156:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a question about simplifying O-notation. Specifically, why is $O(t(n)b^{t(n)}) = 2^{O(t(n))}$ where $t(n)$ is the running time of an algorithm?</p>\n', 'ViewCount': '41', 'Title': 'why is $O(t(n)b^{t(n)}) = 2^{O(t(n))}$', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-19T04:36:55.950', 'LastEditDate': '2013-11-18T21:04:26.033', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Tags': '<asymptotics>', 'CreationDate': '2013-11-17T00:19:28.753', 'Id': '18085'},90_157:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '63', 'Title': 'Understanding Big O', 'LastEditDate': '2013-11-18T04:16:15.633', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11306', 'Body': '<p>I\'m just trying to get my understanding of big O down. I know the concept and the basics but I\'m a bit confused about what it means to be <em>equal</em> to big O of something.</p>\n\n<p>For example, is $2^{2n} = O(2^{100n})$? From my understanding it is, since $2^{2n}$ is "faster" than $2^{100n}$, and so completes within the time $2^{100n}$. Is this correct? And if this is true, does it mean that $n$ could have any coefficient greater than $2$ (in this case) such that $2^{2n}$ is equal to $O(2^{mn})$, for all $m &gt; 2$?</p>\n', 'ClosedDate': '2013-11-25T09:43:03.317', 'Tags': '<asymptotics>', 'LastEditorUserId': '683', 'LastActivityDate': '2013-11-18T04:16:15.633', 'CommentCount': '3', 'AcceptedAnswerId': '18108', 'CreationDate': '2013-11-18T01:23:19.263', 'Id': '18107'},90_158:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When can an algorithm be said to have $O(1)$ complexity? My doubt is if $n$ is specified to be a large number but constant and we cannot implement it in reality without a loop even then can we call it to have $O(1)$ time complexity? Consider the following examples.</p>\n\n<ol>\n<li><p>Algorithm to add first 1000 natural numbers (that is I mean to say if n is specified directly). Then can we say this has $O(1)$ time complexity?</p></li>\n<li><p>Finding the $7$th smallest element in a min heap. This element is present in anywhere in the first 6 levels of the heap (considering root at level 0). So to find the element we need to check $2^7 - 1$ elements. Then can we say this has $O(1)$ time complexity?</p></li>\n</ol>\n', 'ViewCount': '223', 'Title': 'When does an algorithm run in $O(1)$ time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-02T12:21:30.010', 'LastEditDate': '2013-12-02T12:21:30.010', 'AnswerCount': '4', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8533', 'Tags': '<algorithms><asymptotics><runtime-analysis>', 'CreationDate': '2013-11-28T19:12:48.417', 'Id': '18451'},90_159:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have the expression:\n\\begin{equation}\n  |Q|f(n)|\\Gamma|^{f(n)}\n\\end{equation}</p>\n\n<p>Here is my solution to convert the above into an asymptotic expression:\n$|Q| = 2^l$ for some $l\\in\\mathbb{R}$\n$|\\Gamma| = 2^k$ for some $k\\in\\mathbb{R}$</p>\n\n<p>Therefore we have $2^lf(n)(2^k)^{f(n)} = f(n)2^{kf(n)+l} = f(n)2^{O(f(n))}$</p>\n\n<ol>\n<li>Is this correct? I just want to verify my understanding of a discussion in Sipser's Theory of Computation text.</li>\n</ol>\n", 'ViewCount': '25', 'ClosedDate': '2013-12-14T20:04:00.230', 'Title': 'Correctness of asymptotic expression from exact expression', 'LastActivityDate': '2013-12-08T21:43:13.917', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Tags': '<asymptotics>', 'CreationDate': '2013-12-08T20:38:10.507', 'Id': '18759'},90_160:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>There's an <code>f(n)</code> such that <code>f(n) != O(f(n/2))</code></p>\n\n<p>so by the definition of big O notation:<br>\nfor <code>f(n) = n^2</code> the statement is false, because there is a constant c such that <code>n^2 = c*(n^2/2)</code></p>\n\n<p>Which f(n) will work?<br>\nMy guess is f(n) = 2. is that correct?  </p>\n", 'ViewCount': '49', 'Title': 'Big O Notation - Find a Function That Represents the Statement', 'LastActivityDate': '2013-12-09T22:33:40.627', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18795', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '11957', 'Tags': '<complexity-theory><asymptotics>', 'CreationDate': '2013-12-09T22:12:00.927', 'Id': '18794'},90_161:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose we need to find a tight asymptotic bound on the worst case run time of the following program</p>\n\n<pre><code>t = 0;\nfor i = 0 to n-1 do\n  for j = i+1 to n-1 do\n    for k = j+1 to n-1 do\n      t++;\n</code></pre>\n\n<p>I can't for the life of me figure out what the run-time would be. I have a feeling it would have n! in there somewhere, but I can't seem to wrap my head around it. All help is appreciated</p>\n", 'ViewCount': '144', 'Title': 'Worst run-time for 3 nested loop', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-11T11:29:27.627', 'LastEditDate': '2013-12-11T11:16:01.677', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12007', 'Tags': '<asymptotics><runtime-analysis><lower-bounds>', 'CreationDate': '2013-12-11T02:10:13.197', 'FavoriteCount': '1', 'Id': '18857'},90_162:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is a practice problem I\'ve come up with in order to study for an exam I have in a couple of hours.</p>\n\n<p>Again, here\'s the problem: Show T(n) = 2T(n-1) + k is O(2^n) where k is some positive constant.</p>\n\n<p>First of all, 2^n is indeed the upperbound, right? Since I\'ve made this problem up myself, I\'m not 100% sure, but from what I have seen, this should be O(2^n).</p>\n\n<p>If so, here\'s my progress:</p>\n\n<p>We want to show P(n) is true for all n > n0, where P(n) is the statement "T(n) &lt;= c * 2^n" (for some fixed c, to be determined)</p>\n\n<p>For now, let\'s fix c at T(1).</p>\n\n<p>Base case: n = 1</p>\n\n<p>We want to show T(1) &lt;= c * 2. Since c = T(1), our equation becomes T(1) &lt;= T(1) * 2, which is trivially true.</p>\n\n<p>Inductive step: Assume P(n - 1) is true for n > 1. We want to show that P(n) holds.</p>\n\n<pre><code>T(n) = 2 T(n - 1) + k, by definition\n    &gt;= 2 (c * 2^(n-1) ) + k = c * 2^n + k\n</code></pre>\n\n<p>Here\'s where I get stuck. How can we possibly show that c * 2^n + k &lt;= c * 2^n for any value of c? Did I make a mistake somewhere?</p>\n', 'ViewCount': '74', 'Title': 'How do I show T(n) = 2T(n-1) + k is O(2^n)?', 'LastActivityDate': '2013-12-12T08:07:34.127', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '18901', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12042', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2013-12-11T21:55:26.153', 'Id': '18900'},90_163:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to prove:\n$(n^4 + n^2 -1) \\notin \\Omega (n^5 -n^3 + n)$</p>\n\n<p>I have $g(n) = (n^4 + n^2 -1)$ and $f(n) = (n^5 -n^3 + n)$. I know that I have to show \n$g(n) &lt; c f(n)$. </p>\n\n<p>So far, I have:</p>\n\n<ul>\n<li>$n^4 + n^2 -1 &lt; n^4 + n^2 = n^2(n^2 + 1)$</li>\n<li>$n^5 -n^3 + n &gt; n^5 - n^3 = n^2(n^3  - n)$</li>\n<li>From these two statements, I have been trying to show that $(n^2 + 1) &lt; (n^3  - n)$</li>\n</ul>\n\n<p>But I am stuck there, can somebody please help!</p>\n\n<p>Also, let me know if there is another approach I can take!</p>\n', 'ViewCount': '37', 'Title': 'disproving big omega problem', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-14T20:41:45.700', 'LastEditDate': '2013-12-14T20:26:22.123', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12058', 'Tags': '<asymptotics>', 'CreationDate': '2013-12-12T12:23:03.357', 'Id': '18922'},90_164:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From Introduction to Algorithms(pg 47-49), I need help in understanding the following paragraph:</p>\n\n<p>The number of anonymous functions in an expression is understood to be equal to the number of times the asymptotic notation appears. For example, in the expression:\n$$ \\sum_{i=1}^{n} O(i)$$\nthere is only a single anonymous function(a function of $i$). This expression is thus not the same as $O(1) + O(1) + O(1) + ... + O(n)$.</p>\n\n<p>Does this make sense? What are we summing exactly? How can we talk about a function of $i$ here?</p>\n', 'ViewCount': '16', 'ClosedDate': '2013-12-13T07:37:14.310', 'Title': 'Big O notation in a summation', 'LastActivityDate': '2013-12-13T06:49:58.667', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12083', 'Tags': '<asymptotics><mathematical-analysis>', 'CreationDate': '2013-12-13T06:49:58.667', 'Id': '18947'},90_165:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'m learning about asymptotic analysis, and have seen some exotic looking complexities living between other common ones. For instance "log log n" is strictly between 1 and log n. It makes me wonder if one can always find complexities between any other two.</p>\n\n<p>Specifically, for any functions f and g with O(f) \u2282 O(g) does there always exist an h such that O(f) \u2282 O(h) \u2282 O(g)?</p>\n\n<p>This isn\'t homework or anything. I\'m just curious if anyone knows.</p>\n', 'ViewCount': '106', 'Title': 'Is there always a Big Oh complexity strictly between any two others?', 'LastActivityDate': '2013-12-14T20:58:16.143', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18994', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '12119', 'Tags': '<complexity-theory><time-complexity><asymptotics>', 'CreationDate': '2013-12-14T20:02:02.963', 'FavoriteCount': '1', 'Id': '18993'},90_166:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to understand why the sorting algorithm Selection Sort has a time complexity of O(n^2).</p>\n\n<p>Looking at the math, the time complexity is</p>\n\n<p>T(n) = (n-1) + (n-2) + ... + 2 + 1</p>\n\n<p>And this is stated to be equal to</p>\n\n<p>O(n^2)</p>\n\n<p>However I just don't understand the intuition. I have tried several practical experiments for n=10 up to n=5000, and all point to that the time complexity of e.g. 5000 can never be greater T(5000) = 12.497.500 -- not T(5000) = 5000^2 = 25.000.000.</p>\n\n<p>Now, I know that 5000 is not the same as infinity, but I just don't understand the intuition behind</p>\n\n<p>(n-1) + (n-2) + ... + 2 + 1 = O(n^2)</p>\n\n<p>Does someone have a great pedagogical explanation that my dim-witted mind can understand?</p>\n", 'ViewCount': '1122', 'Title': 'Selection Sort Time Complexity using Big O notation', 'LastActivityDate': '2013-12-18T16:54:33.630', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19096', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12219', 'Tags': '<algorithm-analysis><asymptotics><sorting>', 'CreationDate': '2013-12-18T16:43:12.773', 'Id': '19094'},90_167:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From what I have learned asymptotically tight bound means that it is bound from above and below as in theta notation.\nBut what does asymptotically tight upper bound mean for Big-O notation?</p>\n', 'ViewCount': '319', 'Title': 'What is an asymptotically tight upper bound?', 'LastActivityDate': '2013-12-20T12:47:13.920', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '19147', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11131', 'Tags': '<asymptotics>', 'CreationDate': '2013-12-20T04:38:58.967', 'Id': '19141'},90_168:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>The cube root of a natural number n is defined as the largest natural number m such that m^3\u2264n. The complexity of computing the cube root of n (n is represented in binary notation) is</p>\n\n<p>(A) O(n) but not O(n^0.5)</p>\n\n<p>(B) O(n^0.5) but not O((log n)^k) for any constant k > 0</p>\n\n<p>(C) O((log n)^k) for some constant k > 0, but not O((log log n)^m) for any constant m > 0</p>\n\n<p>(D) O((log log n)^k) for some constant k > 0.5, but not O((log log n)^0.5)</p>\n\n<p>I am lost solving this previous year problem .Can any one help me to understand this question</p>\n', 'ViewCount': '118', 'ClosedDate': '2014-01-05T17:25:55.000', 'Title': 'Complexity to find cube root of n', 'LastActivityDate': '2013-12-30T20:34:33.803', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'avi', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<asymptotics><time-complexity>', 'CreationDate': '2013-12-24T23:20:17.427', 'Id': '19361'},90_169:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>For example let's say that we know that the worst-case running time is o(n) and the best-case is o(1). how can i get the average-case running time using the given big Ohs?</p>\n", 'ViewCount': '91', 'Title': 'how to calculate Average-Case complexity time by using worst-case and best-case complexity time?', 'LastActivityDate': '2013-12-30T18:36:07.227', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12207', 'Tags': '<data-structures><asymptotics>', 'CreationDate': '2013-12-30T18:28:15.073', 'Id': '19381'},90_170:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I believe this question should be extremely easy but I am having a (embarrassing) hard time figuring out why its true if there exist OWF (computable in polynomial time) then there exits a OWF that is computed in $O(n^2)$.</p>\n\n<p>This is what I have/tried.</p>\n\n<p>Let $ \\ f(x)$ be a OWF that can be computed in $k^c$. Then we can construct a OWF:</p>\n\n<p>$$\nf\'(x\'||\\ x\'\') = f(x\') || \\ x\'\'\n$$\nwhere:\n$$\n|x\'| = k \\\\\n|x\'\'| = k^c\n$$</p>\n\n<p>Notice the size of the input for $f\'$ is $n = k^c + k$.</p>\n\n<p>Its intuitively "obvious" f\' is a OWF since f is OWF (or you can go ahead and prove it by contradiction if you want to be pedantic). But how come it takes $O(n^2)$ to compute the OWF f\'? Does this depend on the Turing Machine model being used to compute f\'?</p>\n\n<p>It seems to me you can just parse the input $x\'||x\'\'$ (separate it so that you can feed the appropriate thing to the original f) in O(n) and then compute $f(x\')$ in $k^c = O(n)$\nand then concatenate it to $x\'\'$ and print f(x\')|x\'\' (printing takes at most $O(n)$). It seems to me it takes $O(n)$ and that the bound $O(n^2)$ is unnecessarily un-tight (I know $cn + d = O(n) = O(n^2)$). Or maybe the parsing algorithm is "harder" than I expect it... even if you just append the lengths at the beginning just for parsing purposes , isn\'t the time to compute $f\'$ just $O(n)$?</p>\n\n<p>Does someone understands why my O(n) argument is wrong?</p>\n', 'ViewCount': '44', 'Title': 'If a one-way functions (OWF) exist, then there exits a OWF that is computable in quadratic running time by a padding argument', 'LastEditorUserId': '12623', 'LastActivityDate': '2014-01-08T06:44:06.893', 'LastEditDate': '2014-01-07T00:25:10.693', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19532', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '12623', 'Tags': '<asymptotics><cryptography><one-way-functions>', 'CreationDate': '2014-01-06T04:48:25.020', 'Id': '19527'},90_171:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a simple directed graph $G(V,E)$ that has a source $s$ and sink $t$. Each edge $e$ of $G$ has positive integer capacity $c(e)$ and positive integer cost $a(e)$. I am trying to find the minimum cost maximum flow from $s$ to $t$ using the <a href="http://wwwhome.math.utwente.nl/~uetzm/do/DO_Lecture4.pdf" rel="nofollow">well-known Dijkstra potential method for finding augmenting paths</a>. It goes something like this:</p>\n\n<pre><code>Initialize all edge flows to 0.\nInitialize all potentials pi[v] to 0.\nWhile there exists an augmenting path in G_f (the residual network):\n    Set the costs of all edges e = uv to be:\n        b(e) = a(e) + pi[u] - pi[v], if e exists in G or\n        b(e) = -a(e_reverse) + pi[u] - pi[v], where e_reverse = vu otherwise\n    # We are now assured all edges have nonnegative costs\n    Using Dijkstra method with costs b(e) in G_f:\n        Find the cheapest augmenting path from s to t\n        Calculate dist(v), the cost of cheapest path from s to v\n    Augment the cheapest path to t to current flow\n    Set pi[v] = pi[v] + dist(v) for all vertices v\nThe current flow gives the minimum cost maximum flow.\n</code></pre>\n\n<p>Obviously, if all costs $a(e) \\le a_{max}$ and all capacities $c(e) \\le c_{max}$, then there is a loose bound $|E|c_{max}a_{max}$ for cost of minimum cost maximum flow. However, the bound on the potentials $\\pi(v)$ and Dijkstra distances $dist(v)$ is not so obvious. In fact, judging by how it adds $dist(v)$ to $\\pi(v)$ each iteration, $\\pi(v)$ can possibly be multiplied by $|V|$ each iteration!</p>\n\n<p><strong>My question is</strong>, is there a way to calculate a non-exponential bound for $\\pi(v)$? If not, say all capacities and costs are at most $10^4$, $|V| = 200$, $|E| = 5000$. The minimum cost of the maximum flow is at most $5000 \\times 10^4 \\times 10^4 = 5 \\times 10^{11}$. But is it possible that $\\pi(v)$ and $dist(v)$ exceeds 64-bit integers? How do so many implementations not use Big Integers?</p>\n', 'ViewCount': '104', 'Title': 'Potential values of minimum cost maximum flow algorithm', 'LastEditorUserId': '7137', 'LastActivityDate': '2014-01-15T14:37:32.410', 'LastEditDate': '2014-01-15T14:37:32.410', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7137', 'Tags': '<algorithms><graph-theory><algorithm-analysis><asymptotics><network-flow>', 'CreationDate': '2014-01-11T04:25:09.883', 'Id': '19645'},90_172:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's have an array where first half are of value 50 and the second half 100. What would be the asymptotic performance when sorting using Quicksort.</p>\n\n<p>I think it it should be $O(n^2)$ as for array of same elements the complexity is $O(n^2)$ and this particular problem could be rewritten as sorting the first half + sorting the second hald $O(2*(\\frac{n}{2})^2 + n)$ which is still $O(n^2)$.</p>\n\n<p>But my schoolmates claim it should be $O(n log(n))$.. so which one is correct?</p>\n", 'ViewCount': '87', 'LastEditorDisplayName': 'user12779', 'Title': "Quicksort's asymptotic performance for array of [50,...,50,100,...100]", 'LastActivityDate': '2014-01-29T21:37:14.940', 'LastEditDate': '2014-01-12T18:30:54.990', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'user12779', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><performance><quicksort>', 'CreationDate': '2014-01-12T18:18:39.200', 'Id': '19675'},90_173:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to solve the following recurrency: $T(n) = 2T(\\sqrt{n}) + O(1)$. It\'s for a simple undergrad problem that a student asked me, but I really couldn\'t solve it. Since it is for an undergrad question, it would be nice to solve it only with algebraic manipulation and reduction to a well known recurrence, or in the end use the master theorem or a recursion tree. Looking at <a href="http://cs.stackexchange.com/questions/6410/solving-a-recurrence-relation-with-sqrtn-as-parameter">this</a> question, I tried to make $m=\\log(n)$, but I got lost at squeezing the $\\log$ inside the term $T(\\sqrt{n})$. Is it like $T(\\log(\\sqrt{n}))$ or $T(\\sqrt{\\log(n)})$? Or is this the wrong approach?</p>\n', 'ViewCount': '100', 'ClosedDate': '2014-01-16T13:40:10.247', 'Title': 'Solving the recurrency $T(n) = 2T(\\sqrt{n}) + O(1)$', 'LastActivityDate': '2014-01-14T19:10:33.447', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '19720', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2138', 'Tags': '<asymptotics>', 'CreationDate': '2014-01-14T16:27:52.287', 'Id': '19717'},90_174:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>As the title states, I am asking for how the big-O in asymptotic analysis is used in theoretical computer science. It would be helpful if an example would be given.</p>\n', 'ViewCount': '34', 'ClosedDate': '2014-01-24T16:13:28.263', 'Title': 'Big-O in computer Science', 'LastActivityDate': '2014-01-24T03:27:35.117', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19929', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13078', 'Tags': '<asymptotics>', 'CreationDate': '2014-01-24T03:14:31.133', 'Id': '19928'},90_175:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a recursive algorithm defined by the following recursion.</p>\n\n<p>$$T(n) = T(n/f(n)) + O(\\log f(n)).$$</p>\n\n<p>I want to find the function $f$ that minimizes $T(n)$. If $f$ is a constant then $T(n) = \\Theta(\\log n)$. If $f = O(n)$ then $T(n) = \\Theta(\\log n)$. Is this true for all functions $f$ that $T(n) = \\Omega(\\log n)$ or can I get asymptotically better behaviour for some $f$? The reason I think there might be is that you can look at an extended binary search where you split your domain into $f(n)$ sections and then examine each section to see if your value is in that section. This recursion can be presented as follows:</p>\n\n<p>$$T(n) = T(n/f(n)) + O(f(n)).$$</p>\n\n<p>Binary search clearly runs in logarithmic time or worse for all values of $f$. This is the same as my recursion except that it is linear in $f$ instead of logarithmic. So you might expect better behaviour. </p>\n', 'ViewCount': '28', 'Title': 'Pick parameter function that minimises whole function', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T02:10:02.690', 'LastEditDate': '2014-01-26T02:10:02.690', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19978', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13133', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2014-01-25T23:10:07.513', 'Id': '19975'},90_176:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p>The algorithm task is to find an integer (range is not known). the function <code>guess(num)</code> returns one of three chars: \'>\',\'&lt;\' or \'=\'.<br>\n  Find the secret number with <code>O(logS)</code> guesses, where <code>S</code> is the secret number.  You can use <code>find_secret(N1, N2)</code> which operate with <code>O(log(N2-N1))</code>.. What it does is simply a binary search.   </p>\n</blockquote>\n\n<p>So, the algorithm implemented (with Python) as follows:  </p>\n\n<pre><code>def find_secret2():\n    low = 1\n    high = 2\n    answer = guess(high)\n    while answer == "&gt;":\n        low *= 2\n        high *= 2\n        answer = guess(high)\n\n    if answer == "=":\n        return high\n    return find_secret(low, high)\n</code></pre>\n\n<p>my thoughts about the complexity of this algorithm:  </p>\n\n<p>it takes <code>O(logS)</code> to reach the range where <code>low &lt; secret &lt; high</code>.<br>\nthen, it takes <code>O(log(high-low))</code> - because we\'re using <code>find_secret(N1, N2)</code> method.</p>\n\n<p>I\'ll be glad if you could help me explain why the algorithm\'s complexity is <code>O(logS)</code> in a mathematical/rigorous way using the O-notation.<br>\nThanks!</p>\n', 'ViewCount': '55', 'Title': 'Runtime analysis of a "find the secret number" algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T20:39:11.993', 'LastEditDate': '2014-01-29T20:39:11.993', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20080', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13229', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-01-29T19:33:20.053', 'Id': '20078'},90_177:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So I have this code:</p>\n\n<pre><code>  done &lt;- false                                     [1]\n  n &lt;- 0                                            [1]\n  while (n &lt; a) and (done = false)                  [(n+1)(1+1+1)]\n    done &lt;- true                                    [n]\n    for m &lt;- (a- 1) downto n                        [n(1+1+1+1)]\n       if list[m] &lt; list[m - 1] then                [n]\n         tmp &lt;- list[m]                             [n]\n         list[m] &lt;- list[m-1]                       [n]\n         list[m - 1] &lt;- tmp                         [n]\n         done &lt;- false                              [n]\n       n &lt;- n + 1                                   [1]\n  return list                                       [1]\n</code></pre>\n\n<p>Am I doing this right? My conclusions are that the inne for-loop runs (n^2 + n) / 2 times and the outher while-loop runs n+1 times. I don't know how to properly argue for that the bubble sort has the complexity O(n^2) </p>\n", 'ViewCount': '207', 'Title': 'Bubble sort complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T16:00:25.457', 'LastEditDate': '2014-01-31T14:19:33.393', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13277', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><loops>', 'CreationDate': '2014-01-31T13:41:59.570', 'Id': '20155'},90_178:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For example, say I want to analyze $T(n)=3T(\\lfloor n/3 \\rfloor )+2n$ for $n&gt;2$, and $T(n)=1$ otherwise. This is clearly $O(n\\log n)$; however it seems that with induction you can prove it is $O(n)$:</p>\n\n<p>Base case: $T(1)$ is $O(1)$, $T(2)$ is $O(2)$</p>\n\n<p>Inductive step: $T(n) = 3\\times O(n/3) + 2n \\in O(n)$</p>\n\n<p>Where does this go wrong? Why can I do this? Induction clearly is a valid technique, but what subtlety of $O$ causes me to be able to prove a wrong bound?</p>\n', 'ViewCount': '47', 'ClosedDate': '2014-01-31T17:19:34.023', 'Title': 'Why does induction with big-O lead to paradoxes?', 'LastActivityDate': '2014-01-31T17:19:51.087', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6778', 'Tags': '<asymptotics>', 'CreationDate': '2014-01-31T15:58:11.113', 'Id': '20160'},90_179:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have an algorithm and I determined the asymptotic worst-case runtime, represented by Landau notation. Let's say $T(n) = O(n^2)$; this is measured in number of operations.</p>\n\n<p>But this is the worst case, how about in average? I tried to run my algorithm 1000 times for each $n$ from $1$ to $1000$.I get another graph which is the average running time against $n$ but measured in real seconds.</p>\n\n<p>Is there any possible way to compare these figures?</p>\n", 'ViewCount': '56', 'Title': 'Compare asymptotic WC runtime with measured AC runtime', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-02T14:37:42.050', 'LastEditDate': '2014-02-02T14:22:32.553', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11506', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><average-case>', 'CreationDate': '2014-02-01T13:58:46.173', 'FavoriteCount': '1', 'Id': '20186'},90_180:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am a little confused on how to prove/disprove Big O.</p>\n\n<p>For the problem, $2^{n+3}= O(2^n)$, I did the following:</p>\n\n<p>$$2^{n+3} \\leq K \\times 2^n$$</p>\n\n<p>Set $K = 1$</p>\n\n<p>$$2^{n+3} \\leq 2^n$$</p>\n\n<p>Test for large values of n (so I plugged in n = 100)</p>\n\n<p>$2^{103} \\leq 2^{100}$  --- which is false therefore Big O is disproven</p>\n\n<p>Is this process correct?</p>\n', 'ViewCount': '62', 'Title': 'How to prove that $2^{n+3} = O(2^n)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-04T09:51:59.883', 'LastEditDate': '2014-02-04T09:51:59.883', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13371', 'Tags': '<asymptotics>', 'CreationDate': '2014-02-03T23:20:57.327', 'FavoriteCount': '1', 'Id': '20267'},90_181:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>How would you prove/disprove that $e^n = O(2n^2)$? It's unclear to me which function grows faster.</p>\n", 'ViewCount': '82', 'ClosedDate': '2014-02-04T10:02:41.813', 'Title': 'Which of $e^n$ and $2n^2$ grows faster?', 'LastEditorUserId': '683', 'LastActivityDate': '2014-02-12T04:26:21.643', 'LastEditDate': '2014-02-04T00:54:49.120', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13373', 'Tags': '<asymptotics>', 'CreationDate': '2014-02-04T00:25:04.967', 'Id': '20271'},90_182:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I want to prove that any polynomial of degree $k$ is in $\\Theta(n^k)$. The coefficient of $n^k$, $a_{k}$, is positive.</p>\n\n<p>I know I need $0 \\leq c_{1}n^k \\leq a_{k}n^k + ... + a_{0} \\leq c_{2}n^k$ for all $n \\geq n_0$.</p>\n\n<p>The upper limit is easy to prove by taking $c_{2} = \\sum\\limits_{i=0}^k |a_i|$</p>\n\n<p>I don't know how to prove the lower limit. Any hints?</p>\n", 'ViewCount': '70', 'Title': 'How to prove any polynomial of degree $k$ is in $\\Theta(n^k)$?', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-05T16:34:32.797', 'LastEditDate': '2014-02-05T00:14:02.530', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '21320', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11913', 'Tags': '<asymptotics>', 'CreationDate': '2014-02-04T21:03:49.117', 'Id': '21298'},90_183:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '24', 'Title': "Blum's speedup theorem in big-O format?", 'LastEditDate': '2014-02-06T08:36:39.950', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '1', 'Body': '<p>Is there a way to state <a href="http://en.wikipedia.org/wiki/Blum%27s_speedup_theorem" rel="nofollow">Blum\'s speedup theorem</a> in terms of <a href="http://en.wikipedia.org/wiki/Landau_notation" rel="nofollow">Big-O (Landau) notation</a>?</p>\n', 'Tags': '<complexity-theory><asymptotics>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-06T08:36:39.950', 'CommentCount': '1', 'AcceptedAnswerId': '21343', 'CreationDate': '2014-02-06T01:45:42.293', 'Id': '21342'},90_184:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider this example: a problem of dimension $n$ and $m$ ($m,n$: any given integers).\nhas a search space of size $O(n^n * m^n)$. \nIt is clear that this problem is exponential in $n$,\nwhatsoever $m$ may be.\nMy question: is this same problem polynomial in $m$? \nwhat are the assumptions if we can say that?\nis this way of complexity analysis correct? </p>\n', 'ViewCount': '26', 'Title': 'How to analyse the complexity of a problem with two or more size measures', 'LastEditorUserId': '13107', 'LastActivityDate': '2014-02-07T23:26:10.240', 'LastEditDate': '2014-02-07T23:26:10.240', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14490', 'Tags': '<complexity-theory><time-complexity><asymptotics>', 'CreationDate': '2014-02-07T22:47:41.883', 'FavoriteCount': '1', 'Id': '21435'},90_185:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Is \n$$\nf(n) \\in \\Theta(g(n)) \\Leftrightarrow f(n) = g(n) \\cdot (1+o(1))\n$$</p>\n\n<p>true?</p>\n\n<p>For clarity, here are the definitions I use:\n$$\nf(n) \\in o(g(n)) \\Leftrightarrow \\forall \\epsilon &gt; 0 \\exists n_o\\forall n&gt; n_0: |f(n)|\\leq \\epsilon \\cdot |g(n)|\n$$\n$$\nf(n) \\in O(g(n)) \\Leftrightarrow \\exists c &gt; 0 \\exists n_o\\forall n&gt; n_0: |f(n)|\\leq c \\cdot |g(n)|\n$$\n$$\nf(n) \\in \\Omega(g(n)) \\Leftrightarrow \\exists c &gt; 0 \\exists n_o\\forall n&gt; n_0: c \\cdot |g(n)| \\leq |f(n)|\n$$\n$$\nf(n) \\in \\Theta(g(n)) \\Leftrightarrow f(n) \\in \\Omega(g(n)) \\land f(n)\\in O(g(n))\n$$</p>\n\n<p>What I have so far:</p>\n\n<p>The $\\Leftarrow$ direction is quite easy:</p>\n\n<p>$$\nf(n) \\in o(g(n)) \\Rightarrow f(n) \\in O(g(n)) \\land g(n)+o(1)\\cdot g(n) \\in \\Omega(g(n))\n$$</p>\n\n<p>However, I am unsure about the other direction. I suspect that it's not, but can't proof it.</p>\n\n<p>PS: This is not a homework assignment, but homework related: I am working on a seminar paper and want to understand this.</p>\n", 'ViewCount': '61', 'Title': 'Relationshop between Theta and little-o (Big-O notation)', 'LastActivityDate': '2014-02-11T15:55:34.430', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6658', 'Tags': '<asymptotics>', 'CreationDate': '2014-02-11T15:55:34.430', 'Id': '21530'},90_186:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to figure out which is better asymptotic complexity, $O(\\log{\\frac{n}{p}})$ or $O\\left(\\frac{\\log{n}}{\\log{p}}\\right)$. $p$ is the amount of parallelism (i.e. number of cores), and $n$ is the problem size. When I plot them in Grapher.app with any constant value for $p$, the first looks better:</p>\n\n<p><img src="http://i.stack.imgur.com/1wHSD.png" alt="enter image description here"></p>\n\n<p>But when I try to work out the math it doesn\'t seem right:</p>\n\n<p>$$\nO(\\log{\\frac{n}{p}}) = O(\\log{n} - \\log{p})\n$$</p>\n\n<p>$O\\left(\\frac{\\log{n}}{\\log{p}}\\right)$ seems like a better bound than the above since it divides by $\\log{p}$ instead of just subtracting it. What am I missing?</p>\n', 'ViewCount': '170', 'Title': 'Why is log(n/p) asymptotically less than log(n)/log(p)', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-13T11:32:52.523', 'LastEditDate': '2014-02-13T11:32:52.523', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '986', 'Tags': '<asymptotics>', 'CreationDate': '2014-02-12T16:43:50.170', 'FavoriteCount': '1', 'Id': '21570'},90_187:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '194', 'Title': u'What is the result of multiplying O(n) and \u03a9(n)?', 'LastEditDate': '2014-02-12T20:07:43.373', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11837', 'FavoriteCount': '2', 'Body': '<p>If $f(x) = \\Omega(n)$ and  $g(x)= O(n)$, what would be the order of growth of $f(x) \\cdot g(x)$ ?</p>\n\n<p>First I figured it should $\\Theta(n)$ , as two extremes would cancel each other and the order of growth will be same as $n$</p>\n\n<p>But, where I came across this question, the answer given was $\\Omega(n)$, and no proof was mentioned. Well, I didn\'t understand why, but intuitively I convinced myself as "you can\'t know for sure the upper limit of growth for $f(x) \\cdot g(x)$ so you can\'t say it\'s $O(n)$, but you can be sure that it won\'t be lower than $\\Omega(n)$"</p>\n\n<p>Can someone help me in understanding this, in a more believable way?</p>\n', 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-13T10:06:23.037', 'CommentCount': '1', 'AcceptedAnswerId': '21576', 'CreationDate': '2014-02-12T19:20:56.130', 'Id': '21575'},90_188:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am asked to find two functions that are not big-oh of each other.\nIs it correct if I pick say $f(n)=2sin (n)$ and $g(n)=1$? That way, $f$ will never always be greater than $g$.</p>\n', 'ViewCount': '52', 'ClosedDate': '2014-02-17T05:09:22.817', 'Title': 'Big-Oh question', 'LastActivityDate': '2014-02-16T18:30:24.227', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14677', 'Tags': '<asymptotics>', 'CreationDate': '2014-02-14T17:46:16.293', 'Id': '21641'},90_189:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I know that:</p>\n\n<p>If $f(n) = O(g(n))$ , then there are constants $M$ and $x_0$ , such that </p>\n\n<p>$f(n) &lt;= M*g(n), \\forall n &gt; n_0$</p>\n\n<p>The other, plain English way of defining it is,</p>\n\n<p>If $f(n)=O(g(n))$ then for large $n$ , $f(n)$ would <em>grow</em> as fast as $g(n)$.</p>\n\n<p>I got confused when comparing $2^n$ with $2^{2n}$. Here , $f(n) = 2^n$ and $g(n) = 2^{2n}$. Clearly , $f(n)$ is smaller than $g(n)$ by a factor of $2^n$. So there will be constants $A$ and $x_0$ such that the first definition above is met.</p>\n\n<p>However, for large $n$ , $2^{2n}$ would grow much faster than $2^n$, leaving $2^n$ far behind. That is $2^{2n}$  won't be an asymptotic/tight bound for $2^n$ .</p>\n\n<p>So, is $2^n = O(2^{2n})$ or not? (or did I just create a confusing situation out of nothing)</p>\n", 'ViewCount': '69', 'Title': 'Big O relation between $2^n$ and $2^{2n}$', 'LastActivityDate': '2014-02-16T03:48:30.417', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '21688', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11837', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics><landau-notation>', 'CreationDate': '2014-02-15T17:49:54.007', 'Id': '21675'},90_190:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '135', 'Title': 'How to deal with questions having two or more asymptotic notations', 'LastEditDate': '2014-02-17T09:34:55.493', 'AnswerCount': '2', 'Score': '2', 'OwnerDisplayName': 'geek90', 'PostTypeId': '1', 'OwnerUserId': '14749', 'Body': "<p>The following was asked as part of a homework assignment and I am not asking for the solution to these but rather tips or resources on how to solve this and similar questions, </p>\n\n<p>Let $f(n)$ and $g(n)$ be two functions from $\\mathbb{N}^+$ to $\\mathbb{R}^+$. Prove or disprove the following assertions. To disprove, you only need to give a counter example for functions $f(n)$ and/or $g(n)$ which make the assertion false. Consider the following.</p>\n\n<p>$$\\Omega(\\Theta(f(n))) = \\Omega(f(n))$$</p>\n\n<p>According to me the answer is false in this case, my reasoning is that whenever we say $f(n)=\\Theta(g(n))$ we are saying that $f(n)$ will always lie between the limits of $g(n)$.\nHere, let's say $x(n) = \\Theta(f(n))$ so it will always lie between the function $f(n)$ then we take $\\Omega(x(n))$ which would mean that $y(n)$ will always be greater than $x(n)$, and on the right hand side we compare it to some $a(n)$ which is equal $\\Omega(f(n))$ meaning that $a(n)$ will always be greater than $f(n)$. Hence we cannot say correctly if the relation will hold, it may or may not be true.</p>\n\n<p>Is this the correct way of looking at the problem, is there a better way to solve these questions?</p>\n", 'Tags': '<asymptotics><landau-notation>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-22T16:46:45.470', 'CommentCount': '1', 'AcceptedAnswerId': '21725', 'CreationDate': '2014-02-17T06:15:22.287', 'Id': '21724'},90_191:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '37', 'Title': 'Recurrence relation in 2 variables', 'LastEditDate': '2014-02-20T13:39:25.047', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1100', 'FavoriteCount': '1', 'Body': '<p>When analyzing an algorithm, the following recurrence relation popped up:</p>\n\n<p>$T(n,d)=2T(n/2,d)+T(n,d-1)+O(dn)$</p>\n\n<p>where $T(n,1)=O(n \\log{n})$ and $T(1,d)=O(d)$.</p>\n\n<p>By applying the Master Theorem inductively, for any particular $d$, it holds that $T(n,d)=O(n (\\log{n})^d)$. However, it does not necessarily hold that $T(n,d)=O(n (\\log{n})^d)$ because the constant hidden by the $O$-notation depends on the value of $d$.</p>\n\n<p>I was hoping that the technically incorrect bound given by repeated application of the master theorem would be good enough. It turns out that this is actually a terrible, terrible bound. The actual values of $T(n,d)$ are orders of magnitude lower from what the asymptotic bound would predict. Does anyone know how to get a better bound?</p>\n', 'Tags': '<time-complexity><asymptotics><recurrence-relation>', 'LastEditorUserId': '14720', 'LastActivityDate': '2014-02-20T13:39:25.047', 'CommentCount': '3', 'AcceptedAnswerId': '21817', 'CreationDate': '2014-02-19T14:17:23.563', 'Id': '21812'},90_192:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can I find the cost of pseudocode with a nested loop and a nested if statement?</p>\n\n<p><img src="http://i.stack.imgur.com/bk2g6.png" alt="Exercise"></p>\n\n<p>On the left hand side is an example from a textbook I am following. On the right hand side is pseudo code that I found. I took a guess, but I don\'t know what the time would be for the inner code fragments.</p>\n\n<p>I am especially unsure what the code segments inside the if statement would be because the if statement doesn\'t always occur.</p>\n', 'ViewCount': '86', 'Title': 'How to find the cost of pseudocode with a nested loop and a nested if statement?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-20T21:50:41.273', 'LastEditDate': '2014-02-20T11:11:16.393', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '21831', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14864', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-02-19T23:42:49.297', 'Id': '21829'},90_193:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '44', 'Title': 'How to solve recurrences involving log?', 'LastEditDate': '2014-02-23T20:30:26.173', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'Happy', 'PostTypeId': '1', 'OwnerUserId': '14963', 'FavoriteCount': '1', 'Body': '<p>For example, </p>\n\n<p>$T(n) = \\log {n} \\cdot T(\\frac{n}{\\log{n}}) + \\Theta(n)$</p>\n\n<p>I tried using the substitution method with $ n = 2^m $, but that got me nowhere, since it still ends up with a $m$ <em>and</em> $2^m$. The recurrence tree method also becomes rather complicated. Is there a general approach to follow to solve such recurrences?</p>\n', 'Tags': '<asymptotics><recurrence-relation>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-23T20:30:26.173', 'CommentCount': '2', 'AcceptedAnswerId': '21953', 'CreationDate': '2014-02-11T11:51:01.843', 'Id': '21947'},90_194:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am a teaching assistant on a course for computer science students where we recently talked about big-O notation. For this course I would like to teach the students a general method for  finding the constants $C$ and $k$ in the definition of big-O\n$$\n|f(x)| \\leq C|g(x)|, x &gt; k,\n$$\nwhen the function $f(x)$ is a fraction of polynomials.</p>\n\n<p>I have tried to concoct my own method, but I am unsure of its correctness. I was inspired by the easy method of finding $C$ and $k$ for a polynomial where for example we can show that $x^3+2x+3$ is $O(x^3)$ by\n$$\nx^3+2x+3 \\leq x^3+2x^3+3x^3 = 6x^3\n$$\nto find $C = 6$ and $k = 1$.</p>\n\n<p>Now, for a fraction of polynomials I am unsure what to do with the denominator. My attempt at a general method is as follows. I would like to show that $\\frac{x^4+x^2x1}{x^3+1}$ is $O(x)$. First I divide by the highest term in the denominator to get a $1$ in the denominator:\n$$\n\\frac{(x^4+x^2+1)/x^3}{(x^3+1)/x^3} = \\frac{x+\\frac{1}{x}+\\frac{1}{x^2}}{1+\\frac{1}{x^3}}\n$$\nNow I argue, somewhat analogously to the previous example, that the fractions in the numerator must be less than (when x > 0) x, and since a smaller denominator makes the expression smaller, setting all the terms in denominator except the $1$ to $0$, I obtain the inequality\n$$\n\\frac{x+\\frac{1}{x}+\\frac{1}{x^2}}{1+\\frac{1}{x^3}} \\leq \\frac{x+x+x}{1+0} = 3x\n$$\nand I find $C = 3$ and $k = 1$.</p>\n\n<p>Now my question is, does this homebrewed method actually work or is it complete nonsense? And if it is nonsense, does anybody know of another general method for finding $C$ and $k$ in instances like this?</p>\n\n<p>Note that I need to find the constants $C$ and $k$, not just show that a given function is big-O of some other function, and the students have had no course in calculus, so I can use no concepts from there, such as limits.</p>\n', 'ViewCount': '99', 'Title': 'Finding constants C and k for big-O of fraction of polynomials', 'LastActivityDate': '2014-03-02T21:10:23.610', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '22141', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '15140', 'Tags': '<asymptotics><polynomials>', 'CreationDate': '2014-02-28T19:28:09.083', 'Id': '22137'},90_195:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have two functions $S$ and $T$ which are interrelated and I want to find the asymptotic worst case runtime. The fact that they are interrelated is stumping me...</p>\n\n<p>How would I find the asymptotic runtime $S(n)$ and $T(n)$?</p>\n\n<p>$$\n\\begin{align*}\nS(n) &amp;= 2S(n/4) +  T(n/4) \\\\\nT(n) &amp;=  S(n/2) + 2T(n/2)\n\\end{align*}\n$$</p>\n', 'ViewCount': '55', 'Title': 'Asymptotic Runtime of Interrelated Functions', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-01T23:51:07.287', 'LastEditDate': '2014-03-01T23:51:07.287', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11204', 'Tags': '<algorithms><algorithm-analysis><asymptotics><search-algorithms><master-theorem>', 'CreationDate': '2014-03-01T02:34:54.417', 'Id': '22149'},90_196:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '482', 'Title': 'How is this problem related to the study of algorithms and big O notation?', 'LastEditDate': '2014-03-02T18:06:17.933', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '15202', 'FavoriteCount': '2', 'Body': "<p>I'm taking a graduate computer science course on algorithms and analysis. The current subject is big O notation and recursion. How is the following problem related to the study of algorithms, recursion, and big O notation? I know and understand the solution to the problem, but I just don't see how this is relevant to the subject matter.</p>\n\n<p><em>Given an $x$, show that $x^{62}$ can be computed with only eight multiplications (A general algorithm can not accomplish it).</em></p>\n", 'Tags': '<algorithms><asymptotics>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-03T10:57:00.337', 'CommentCount': '1', 'AcceptedAnswerId': '22201', 'CreationDate': '2014-03-02T17:56:54.607', 'Id': '22200'},90_197:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I had a debate with my friend. He argued that $o(1)\\subseteq O(1)$, so if a function converges to 0, then it belongs to both $o(1)$ and $O(1)$. However I imagine that $O(1)$ represents a constant time, in essence, a non-zero constant time. Is there a broad acceptance that a function converging to zero belongs to $o(1)$ and not to $O(1)$?  </p>\n', 'ViewCount': '136', 'Title': 'Should O(1) necessarily stand for a non-zero constant?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-12T22:18:36.997', 'LastEditDate': '2014-03-10T09:02:49.010', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15418', 'Tags': '<terminology><asymptotics>', 'CreationDate': '2014-03-08T15:17:20.457', 'Id': '22398'},90_198:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given an extendable heap with $n$ elements and an array size of $A$, I'm trying to use the accounting method to find the amortized cost of a delete. We want a load factor of $\\frac{1}{4}$.    </p>\n\n<p>So, the minimum number of deletes before we need to shrink the array is $\\frac{4n-A+3}{4}$.    </p>\n\n<p>The cost of shrinking at that point would be $n$</p>\n\n<p>So, the amortized cost would be $n/\\frac{4n - A + 3}{4}$</p>\n\n<p>Is my calculation correct?</p>\n", 'ViewCount': '46', 'Title': 'Amortized Cost of a delete from an extendable heap', 'LastEditorUserId': '8639', 'LastActivityDate': '2014-03-14T13:48:57.807', 'LastEditDate': '2014-03-14T13:48:57.807', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<data-structures><asymptotics>', 'CreationDate': '2014-03-10T03:28:46.547', 'Id': '22452'},90_199:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '45', 'Title': 'Examples of algorithms that have runtime O(N + M) resp O(NM)', 'LastEditDate': '2014-03-14T09:40:03.310', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '544', 'FavoriteCount': '1', 'Body': "<p>I'm looking for examples of loops that have running time $O(nm)$, $O(n+m)$ and $O(n\\log m)$ to help me understand these concepts.  Could anybody give some examples and explain why they have the given running time?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T09:40:03.310', 'CommentCount': '4', 'AcceptedAnswerId': '22612', 'CreationDate': '2014-03-14T05:44:15.117', 'Id': '22611'},90_200:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '179', 'Title': 'Why does merge sort run in $O(n^2)$ time?', 'LastEditDate': '2014-03-16T15:08:35.860', 'AnswerCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12448', 'FavoriteCount': '1', 'Body': '<p>I have been learning about Big O, Big Omega, and Big Theta. I have been reading many SO questions and answers to get a better understanding of the notations. From my understanding, it seems that <strong>Big O is the upper bound</strong> running time/space of the algorithm, <strong>Big Omega is the lower bound</strong> running time/space of the algorithm and <strong>Big Theta is like the in between of the two</strong>.</p>\n\n<p>This particular <a href="http://stackoverflow.com/questions/10376740/big-theta-notation-what-exactly-does-big-theta-represent?lq=1">answer</a> on SO stumbled me with the following statement </p>\n\n<blockquote>\n  <p>For example, merge sort worst case is both ${\\cal O}(n\\log n$) and $\\Omega(n\\log n)$ -\n  and thus is also $\\Theta(n\\log n)$, but it is also ${\\cal O}(n^2)$, since $n^2$ is\n  asymptotically "bigger" than it. However, it is NOT $\\Theta(n^2)$, Since\n  the algorithm is not $\\Omega(n^2)$</p>\n</blockquote>\n\n<p>I thought merge sort is ${\\cal O}(n\\log n)$ but it seems it is also ${\\cal O}(n^2)$ because $n^2$ is asymptotically bigger than it. Can someone explain this to me?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-16T15:08:35.860', 'CommentCount': '5', 'AcceptedAnswerId': '22674', 'CreationDate': '2014-03-16T00:41:55.560', 'Id': '22662'},90_201:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've recently learned about the Big Oh notation and heard that the following aren't true:</p>\n\n<ol>\n<li>$f(n)\\in O(f(n)^2)$.</li>\n<li>Either $f(n)\\in O(g(n))$ or $f(n)\\in\\Omega(g(n))$ or both.</li>\n<li>$f(n)\\in\\omega(g(n))$ implies $\\log(f(n)) = \\omega(\\log(g(n))$.</li>\n</ol>\n\n<p>I'm trying to find a good counter-example for each but it's proving to be a bit challenging. What would be a good one for these?</p>\n", 'ViewCount': '36', 'ClosedDate': '2014-03-19T08:13:40.697', 'Title': 'Big Oh notation', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-19T02:47:44.470', 'LastEditDate': '2014-03-19T02:47:44.470', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15826', 'Tags': '<asymptotics>', 'CreationDate': '2014-03-19T02:21:28.153', 'Id': '22788'},90_202:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am trying to find the worst case $\u0398$ bound for the following recurrence equation:\n$$\nT(n)=\\sum_{i=1}^kT(a_i)+n+\\lg k\\sum_{i=1}^ka_i\\quad\nwhere\\quad n=1+\\sum_{i=0}^ka_i\\quad and\\quad\na_0\\ge a_1, a_2, \\dots,a_k\\ge 1\n$$\nBy master theorem, with $k=a_0=a_1=n/3$ and $a_2=a_3=\\dots =a_k=1$, $T(n)=\u0398(n\\lg n)$. Now my freind and I guessed that the worst case of $T(n)$ is also $\u0398(n\\lg n)$, but we are not able to prove it.</p>\n\n<p>My question is, what is the worst case bound of $T(n)$ and how to prove it?</p>\n\n<p><strong>Edit</strong>: By worst case I mean that $T(n)$ to be the <em>maximum</em> of the expression I wrote over all $k\\ge1$ and $a_0\\ge a_1,a_2,\\dots,a_k\\ge1$ such that $n=1+a_0+\\dots+a_k$.</p>\n', 'ViewCount': '56', 'Title': 'Solve the worst case of this recurrence equation', 'LastEditorUserId': '9249', 'LastActivityDate': '2014-03-22T03:43:09.767', 'LastEditDate': '2014-03-22T02:01:10.610', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9249', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2014-03-21T10:35:07.460', 'FavoriteCount': '1', 'Id': '22898'},90_203:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '55', 'Title': 'O(f) vs O(f(n))', 'LastEditDate': '2014-03-21T20:54:17.343', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10901', 'FavoriteCount': '0', 'Body': '<p>I first learned about the Big O notation in an intro to Algorithms class. He showed us that function $g \\in O(f(n))$ <br>\nAfterwords in Discrete Math another Professor, without knowing of the first, told us that we should never do that and that it should be done as $g \\in O(f)$ where g and f are functions. \nThe question is which one of these is right, why, and if they both are, what is the difference?  </p>\n', 'Tags': '<terminology><asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-21T21:04:42.580', 'CommentCount': '3', 'AcceptedAnswerId': '22914', 'CreationDate': '2014-03-21T17:58:00.083', 'Id': '22908'},90_204:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can I find the tightest possible asymptotic bounds on the recurrence T(n) = 3T(n/2)+cn, where <em>c</em> is a positive constant. Must use iteration method.</p>\n', 'ViewCount': '16', 'ClosedDate': '2014-03-26T08:44:43.887', 'Title': 'Iteration method - recurrence', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T08:45:12.010', 'LastEditDate': '2014-03-26T08:45:12.010', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'user16139', 'PostTypeId': '1', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2014-03-26T05:02:45.943', 'Id': '23063'},90_205:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '94', 'Title': u'How do O and \u03a9 relate to worst and best case?', 'LastEditDate': '2014-03-26T15:50:22.980', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12756', 'FavoriteCount': '1', 'Body': '<p>Today we discussed in a lecture a very simple algorithm for finding an element in a sorted array using <a href="http://en.wikipedia.org/wiki/Binary_search_algorithm" rel="nofollow">binary search</a>. We were asked to determine its asymptotic compelxity for an array of $n$ elements.</p>\n\n<p>My idea was, that it is obvisously $O(\\log n)$, or $O(\\log_2 n)$ to be more specific because $\\log_2 n$ is the number of operations in the worst case. But I can do better, for example if I hit the searched element the first time - then the lower bound is $\\Omega(1)$.</p>\n\n<p>The lecturer presented the solution as $\\Theta(\\log n)$ since we usually consider only worst case inputs for algorithms.</p>\n\n<p>But when considering only worst cases, whats the point of having $O$ and $\\Omega$-notation when all worst cases of the given problem have the same complexity ($\\Theta$ would be all we need, right?).</p>\n\n<p>What am I missing here?</p>\n', 'Tags': '<algorithm-analysis><asymptotics>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-26T18:04:51.830', 'CommentCount': '4', 'AcceptedAnswerId': '23082', 'CreationDate': '2014-03-26T11:10:34.473', 'Id': '23068'},90_206:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I believe I understand the concepts of algorithm analysis. However, I'm not fully confident in applying those concepts. I'd appreciate help in bridging the gap between concept and application. </p>\n\n<p>I understand that if $f(n) \\in O(g(n))$ and $f(n) \\in \\Omega(g(n))$ then $f(n) \\in \\Theta(g(n))$</p>\n\n<p>Essentially, when Big-O and Big-Omega form the upper and lower bounds of algorithmic performance, Big-Theta represents the optimal solution. But how does one determine those bounds and thus optimal performance? </p>\n\n<p>How does/should one determine <em>Big-O, Big-Omega, Big-Theta</em> for the following algorithm?</p>\n\n<pre><code>int summation(int[] values)\n{\n  int sum = 0;\n  for(int i = 0; i &lt; values.length; i++)\n  {\n    sum += values[i];\n  }\n}\n</code></pre>\n", 'ViewCount': '35', 'ClosedDate': '2014-03-29T11:47:10.477', 'Title': 'Analysis of Algorithms: Applying Concepts', 'LastEditorUserId': '16252', 'LastActivityDate': '2014-03-28T21:55:52.333', 'LastEditDate': '2014-03-28T21:55:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16252', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-03-28T21:14:52.010', 'Id': '23192'},90_207:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '45', 'Title': u'find function which is in o(log^k(n)) for fixed value of k and in \u03c9(1)', 'LastEditDate': '2014-04-06T23:17:21.140', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16507', 'Body': '<p>I need to find a function $f$ which is in $o(\\log^{k} n)$ for fixed value of $k$ with $f = \\omega(1)$. I know that for little $o$ the function should be strictly less than $c\\log^k n$ for all $c$ and large enough $n$; and for little $\\omega$ it should be strictly greater than $c\\cdot 1$ for all $c$ and large enough $n$, but I am stuck here. How does one usually solve such type of problems?</p>\n', 'ClosedDate': '2014-04-07T06:13:44.060', 'Tags': '<asymptotics>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-06T23:17:21.140', 'CommentCount': '5', 'AcceptedAnswerId': '23491', 'CreationDate': '2014-04-06T21:24:34.527', 'Id': '23485'},90_208:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '28', 'Title': 'asymptotic growth of n^log log n', 'LastEditDate': '2014-04-07T06:18:37.547', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16515', 'Body': "<p>I'm ordering functions by their asymptotic growth for an assignment and I have verified I have the correct order by using limits, but I'm trying to understand why $n^{log\\ log\\ n}$ is between $n^3$ and $2^n$. It seems like it should be closer to the order of $n^n$. Can anyone provide some insight on this.</p>\n", 'ClosedDate': '2014-04-07T06:18:03.597', 'Tags': '<asymptotics>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-07T06:18:37.547', 'CommentCount': '0', 'AcceptedAnswerId': '23498', 'CreationDate': '2014-04-07T03:43:05.123', 'Id': '23496'},90_209:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I met a recurrence equation for my algorithm \n$$ f(n) = 2\\cdot \\left( f(n-1) + f(\\frac{n}{2}) \\right)$$\nwith $f(1)=1$, $f(2)=4$, $f(3)=10$. </p>\n\n<p>I guess it is $\\Theta((2+\\epsilon)^n)$, where $\\epsilon$ ban be arbitrarily close to 0. \nI want to have an asymptotic formula of $f(n)$. I do not know how to prove or disprove my guess. </p>\n', 'ViewCount': '37', 'Title': 'Bounding the recurrence $f(n)=2f(n-1)+2f(n/2)$', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-09T20:13:22.223', 'LastEditDate': '2014-04-09T20:13:22.223', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '23608', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '646', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2014-04-09T19:27:31.053', 'Id': '23607'},90_210:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have an algorithm that has a performance of $O(n + 2)$. Here if n gets really large the 2 becomes insignificant. In this case it's perfectly clear the real performance is $O(n)$.</p>\n\n<p>However, say another algorithm has a performance of $O(n^2/2)$. Here if n gets really large then $n^2/2$ is exactly half of $n^2$, which is not significantly smaller than $n^2$. So why we drop 1/2 from $O(n^2/2)$ and it becomes $O(n^2)$?</p>\n", 'ViewCount': '392', 'Title': 'Why is constant always dropped from big O analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T02:45:53.917', 'LastEditDate': '2014-04-12T15:22:48.923', 'AnswerCount': '3', 'CommentCount': '5', 'AcceptedAnswerId': '23704', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16713', 'Tags': '<terminology><algorithm-analysis><asymptotics><landau-notation>', 'CreationDate': '2014-04-12T14:54:50.047', 'Id': '23703'},90_211:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have the following simple algorithm to find duplicate characters in a string:</p>\n\n<pre><code>for i = 1 -&gt; n\n    for j = i + 1 -&gt; n\n        if A[i] == A[j] return true\nreturn false \n</code></pre>\n\n<p>Why is the running time of this algorithm $\\mathcal{O}(n^2)$?\nIf the first iteration is $n$ steps then, $n-1, n-2,n-3,..,1$ it seems to me that \nadding all these would never be $n^2$ or am I wrong?</p>\n', 'ViewCount': '48', 'Title': 'Confusion with the Running Time of an algorithm that finds duplicate character', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T07:47:17.597', 'LastEditDate': '2014-04-14T07:47:17.597', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '23749', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16747', 'Tags': '<terminology><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2014-04-13T19:40:14.433', 'Id': '23748'},90_212:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say that $f(n) = \\cal O(n^2)$ and $g(n) = \\cal O(n)$.</p>\n\n<p>If $h(n)=f(n)/g(n)$,  is it true that $h(n) =\\cal O(n)$?</p>\n\n<p>Is it mathematically correct to say that $h(n) = \\cal O(n^2)/ O(n) = O(n)$? if not, what would be the correct way to show this?</p>\n', 'ViewCount': '69', 'Title': 'What order of growth does a ratio of Bigh-Ohs have?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T10:37:02.780', 'LastEditDate': '2014-04-14T07:54:23.860', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16742', 'Tags': '<asymptotics>', 'CreationDate': '2014-04-13T21:16:02.837', 'Id': '23750'},90_213:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>The following recurrence relation,</p>\n\n<p>$$T(n)=16T(\\frac{n}{4}) + n^2$$</p>\n\n<p>has been given to me to be solved via the Master Theorem. I'm pretty sure this is a case 2 situation, since</p>\n\n<p>$$\\log_4{16} = 2$$ and </p>\n\n<p>$$\\log_n{n^2} = 2$$ </p>\n\n<p>Where $b = 4$, $a = 16$, and $c = 2$, such that $f(n) = n^c$ </p>\n\n<p>What I'm <em>not</em> sure about is the end runtime complexity. </p>\n\n<p>What I've come up with so far is</p>\n\n<p>$$T(n) = \\theta(n^2log_2{n})$$</p>\n\n<p>...since the Master Theorem states that, for case 2 situations, $f(n)$ must be within the set of $O(n^clog^kn)$, where $k$ is some constant in which $k \\ge 0$.</p>\n\n<p>My reasoning was to substitute 0 for $k$, which led to $\\log_2^1{n}$ in the result, as the result requires that $T(n) = \\theta(n^clog_2^{k+1}{n})$.</p>\n\n<p>In a nutshell, while I know the first portion of the analysis is correct, and that this <em>is</em> a case 2 situation, am I correct in the reasoning behind the logarithmic portion in the ending runtime analysis?</p>\n", 'ViewCount': '32', 'Title': 'Is my analysis of this recurrence relation correct?', 'LastActivityDate': '2014-04-24T06:24:28.710', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17028', 'Tags': '<asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2014-04-23T23:29:21.213', 'Id': '24066'},90_214:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>It\'s been a while since I had to solve a recurrence and I wanted to make sure I understood the iterative method of solving these problems. Given:</p>\n\n<p>$$T(n) = 3T(n-2)$$</p>\n\n<p>My first step was to iteratively substitute terms to arrive at a general form:</p>\n\n<p>$$T(n-2) = 3T(n-2 -2) = 3T(n-4)$$\n$$T(n) = 3 *3T(n-4)$$</p>\n\n<p>leading to the general form:</p>\n\n<p>$$ T(n) = 3^k T(n-2k) $$</p>\n\n<p>Now I solve $n-2k = 1$ for $k$, which is the point where the recurrence stops (where $T(1)$) and insert that value ($n/2 - 1/2 = k$) into the general form:</p>\n\n<p>$$T(n) = 3^{n/2-1/2}$$\n$$T(n) = O(3^n)$$</p>\n\n<p>I\'m not sure about that last step:</p>\n\n<p>I would just "argue" that as $n \\to \\infty$ one can ignore $-1/2$ and $n/2 \\to n$ ?\nIs that assumption correct?</p>\n', 'ViewCount': '309', 'Title': 'Solving the recurrence T(n) = 3T(n-2) with iterative method', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-02T22:03:03.363', 'LastEditDate': '2014-05-02T22:03:03.363', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '24291', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '17242', 'Tags': '<asymptotics><recurrence-relation>', 'CreationDate': '2014-05-01T08:17:13.333', 'Id': '24290'},90_215:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a task to perform Big O analysis on the insertion/deletion/search for B- Trees,multilevel indexing and hashing.</p>\n\n<p>Could some one provide reading links for the above topic ,so that I could get a head start.</p>\n\n<p>Thanks</p>\n', 'ViewCount': '11', 'ClosedDate': '2014-05-03T23:47:46.737', 'Title': 'Big O analysis of B-Trees', 'LastActivityDate': '2014-05-03T18:05:35.010', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17301', 'Tags': '<asymptotics><hash>', 'CreationDate': '2014-05-03T18:05:35.010', 'Id': '24360'}