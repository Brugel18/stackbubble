{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In computer vision, scales are important when we carry out a scene analysis. Choosing different scales affect the result of the analysis. For example, if a face is relatively small in the scene, then the details including nose, eyes will be omitted. On the other hand, details on larger faces become relatively more salient.</p>\n\n<p>I know both Gaussian Blur with different sigmas and Down Sampling on the image can generate different scales. Which is more reasonable on a cognitive sense?</p>\n', 'ViewCount': '95', 'Title': 'Which Is a Better Way of Obtaining Scales, Gaussian Blur or Down Sampling?', 'LastActivityDate': '2012-08-24T18:29:39.153', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3319', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<machine-learning><computer-vision><image-processing>', 'CreationDate': '2012-08-24T08:40:02.273', 'Id': '3310'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am doing a project on face detection in C#. I want to find the skin area by using skin color segmentation. For that purpose, I have to extract the skin area.</p>\n\n<p>I know I can use <a href="http://en.wikipedia.org/wiki/HSI_color_space" rel="nofollow">HSI</a> or <a href="http://en.wikipedia.org/wiki/YCbCr" rel="nofollow">YCbCr</a>, but what is the exact difference between these?</p>\n', 'ViewCount': '276', 'Title': 'Difference between HSI and YCbCr for pattern recognition by color', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-31T04:32:18.547', 'LastEditDate': '2012-09-22T20:51:08.260', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2893', 'Tags': '<pattern-recognition><image-processing>', 'CreationDate': '2012-09-22T19:49:55.650', 'Id': '4673'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Shannon's entropy [plog(1/p)] for an image is a probabilistic method for comparing two pixels or a group of pixels.Suppose an image with a  matrix of 3x3 has pixel intensity values</p>\n\n<pre><code>1 1 2\n2 3 3\n4 4 5\n</code></pre>\n\n<p>and another image with 3x3 matrix has group of pixels having intensity values</p>\n\n<pre><code>5 5 6\n6 7 7\n8 8 9\n</code></pre>\n\n<p>Then shannon's entropy for the images would be the same.So in this case the entropy values would point out that the images are same though in actual they are different.So image matching using this technique doesn't help.On basis of supervised classification where I classify an image based on trained databases of shannon's entropy ,we use the concept of entropy to find similarity between two images.Is there any method or research paper where this entropy can be used or modified for image matching for the above case..?</p>\n", 'ViewCount': '2300', 'Title': "Shannon's entropy for an image", 'LastEditorUserId': '157', 'LastActivityDate': '2013-10-11T21:42:02.737', 'LastEditDate': '2012-10-23T02:37:47.963', 'AnswerCount': '3', 'CommentCount': '7', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '3093', 'Tags': '<pattern-recognition><image-processing><entropy><computer-vision>', 'CreationDate': '2012-10-07T20:51:48.457', 'Id': '4935'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When dealing with thresholding in digital image processing, what do the following terms mean?</p>\n\n<ul>\n<li>contrasting object</li>\n<li>contrasting background</li>\n</ul>\n', 'ViewCount': '37', 'Title': 'Contrasting object and background', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T12:18:43.807', 'LastEditDate': '2012-10-11T12:18:43.807', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3153', 'Tags': '<graphics><image-processing>', 'CreationDate': '2012-10-11T09:37:56.967', 'Id': '5017'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2786', 'Title': 'DIfferences between computer vision and image processing', 'LastEditDate': '2012-12-01T14:05:16.613', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4813', 'FavoriteCount': '1', 'Body': '<p>I am basically an Electronics and Communication Engineering student. My relevant coursework include Probability Theory and Stochastic Processes, Engineering Mathematics, Signals and Systems, Digital Signal Processing (this semester).</p>\n\n<p>Now, this area Computer Vision - Image Processing - Object recognition - etc caught my eye and I am thinking to study this subject and write a nice research paper in one of the areas later on.</p>\n\n<p>So, could anyone tell me what the differences between Computer Vision and Image Processing are? Say, we consider Object recognition - what are the roles of vision and image processing?</p>\n', 'Tags': '<education><image-processing><computer-vision>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-05T14:59:59.047', 'CommentCount': '7', 'AcceptedAnswerId': '7087', 'CreationDate': '2012-11-30T15:25:15.030', 'Id': '7050'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\u2019m trying to make a photo mosaic web application. I split the main picture in to a number of squares. I split this square up in to 9 squares (<a href="http://i49.tinypic.com/imryf6.jpg" rel="nofollow">See picture</a>) Then I calculate the mean color (RGB) of the entire square and the mean color of the 9 little squares. So I will get the following vector.</p>\n\n<p>Vector: Mean R; mean G; mean B; cell 1 R; cell 1 G; cell 1 B; cell 2 R; cell 2 G; cell 2 B; cell 3 R; cell 3 G; cell 3 B; cell 4 R; cell 4 G; cell 4 B; cell 5 R; cell 5 G; cell 5 B; cell 6 R; cell 6 G; cell 6 B; cell 7 R; cell 7 G; cell 7 B; cell 8 R; cell 8 G; cell 8 B; cell 9 R; cell 9 G; cell 9 B</p>\n\n<p>This will all be number between 0 and 255. And the mean RGB is more important than the other elements.</p>\n\n<p>I make the same vector for all pictures in my DB. So now I want to compare a square of the original too my DB and want to find the 10 closest pictures.</p>\n\n<p>My question is how do I compare these vectors??</p>\n', 'ViewCount': '116', 'Title': 'Compare vectors, photo mosaic', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-02T19:18:14.603', 'LastEditDate': '2013-01-02T19:18:14.603', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5274', 'Tags': '<algorithms><image-processing>', 'CreationDate': '2013-01-02T14:47:48.197', 'Id': '7699'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>As part of my homework I need to prove or disprove that changing the contrast and than changing the brightness is, or isn't the same as changing the brightness and after that changing the contrast.</p>\n\n<p>let's assume that we are using gray-scale.</p>\n\n<p>brightness - is the mean of the the gray levels.<br>\nchanging brightness - adding or decreasing a const from every pixel.<br>\ncontrast - max - min of the gray-scale levels.<br>\nchanging the contrast - stretching the gray levels around the mean.</p>\n\n<p>I understand that you need to disprove it. but I don't know how to do it in a formal way.</p>\n", 'ViewCount': '67', 'Title': 'Contrast vs. Brightness question', 'LastActivityDate': '2013-01-30T19:04:23.757', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9322', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6632', 'Tags': '<image-processing>', 'CreationDate': '2013-01-30T17:54:18.340', 'Id': '9320'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Operate on an image by performing Median Filtering in a 3x3 window.<br>\nOperate on the resulting image by performing, again, Median Filtering in a 3x3 window.<br>\nCan the resulting image be obtained from a single Median filtering?</p>\n\n<p>my initial thought is that it can be done with the right mask. maybe a median next to a median.\nbut i'm not sure.</p>\n", 'ViewCount': '250', 'Title': 'median filter one after the other', 'LastActivityDate': '2013-01-31T19:42:23.147', 'AnswerCount': '2', 'CommentCount': '8', 'AcceptedAnswerId': '9368', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6632', 'Tags': '<image-processing>', 'CreationDate': '2013-01-30T22:20:36.623', 'Id': '9336'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>let us assume that:<br>\n <strong>f</strong> is a grayscale image of size NxN, and <strong>F</strong> is the Fourier transform of <strong>f</strong>.\n<br>\n<strong>G</strong> is a 2Nx2N transform obtained by inserting 0 between every value in <strong>F</strong>:\n<img src="http://i.stack.imgur.com/iE9ET.jpg" alt="enter image description here"></p>\n\n<p>What is the inverse transform of <strong>G</strong>?</p>\n\n<p>solution:\nso i have programmed it, and the picture will be 4 times bigger with the picture 4 times in it. g= [f f;f f]\ncan you please explain why this is the answer? </p>\n', 'ViewCount': '33', 'Title': 'Transform inverse result', 'LastActivityDate': '2013-02-03T18:48:22.943', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9450', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6632', 'Tags': '<image-processing>', 'CreationDate': '2013-02-03T16:05:42.553', 'Id': '9449'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am facing a problem with processing some frontal facial images.</p>\n\n<p>I need to adjust the centers of the eyes (irises) to be at certain specific pixels in the image. In other words, i need to transform the image until the centers of the eyes are at specific pixels.</p>\n\n<p>Can anyone help me with this problem, an algorithm, technique, ...etc. that might solve this problem ?</p>\n\n<p>Thanks,</p>\n', 'ViewCount': '63', 'Title': 'How center the eyes of human face at specific pixels?', 'LastActivityDate': '2013-10-29T18:15:09.450', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7031', 'Tags': '<image-processing><computer-vision>', 'CreationDate': '2013-02-25T19:58:04.990', 'Id': '10090'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How do those matrices work? Do I need to multiple every single pixel? How about the upperleft, upperright, bottomleft and bottomleft pixels where there\'s no surrounding pixel? And does the matrix work from left to right and from up to bottom or from up to bottom first and then left to right?</p>\n\n<p>Why does this kernel (Edge enhance):</p>\n\n<p><img src="http://i.stack.imgur.com/M8Uel.png" alt="http://i.stack.imgur.com/d755G.png"></p>\n\n<p>turn into this image:</p>\n\n<p><img src="http://i.stack.imgur.com/gtuBM.jpg" alt="http://i.stack.imgur.com/NRdkK.jpg"></p>\n', 'ViewCount': '266', 'Title': 'How do convolution matrices work?', 'LastEditorUserId': '31', 'LastActivityDate': '2013-04-11T11:02:43.563', 'LastEditDate': '2013-03-12T09:20:48.573', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<matrices><image-processing>', 'CreationDate': '2013-03-12T07:59:11.783', 'Id': '10470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The problem: Only pattern-like images are supposed to be uploaded and be used as backgrounds to a site, photos should not. How would you detect if an image looks enough like itself to be regarded as a pattern?</p>\n', 'ViewCount': '168', 'Title': 'Image pattern detection - Finding similarities in same image', 'LastActivityDate': '2013-03-16T11:13:41.120', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7299', 'Tags': '<image-processing>', 'CreationDate': '2013-03-16T05:10:04.240', 'Id': '10545'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m trying to figure out what are currently the two most efficent algorithms that permit, starting from a Left/Right pair of stereo images created using a traditional camera (so affected by some epipolar lines misalignment), to produce a pair of adjusted images plus their depth information by looking at their disparity.</p>\n\n<p>Actually I\'ve found lots of papers about these two methods, like:</p>\n\n<ul>\n<li>"Computing Rectifying Homographies for Stereo Vision" (Zhang - seems one of the best for rectification only)</li>\n<li>"Three-step image recti\ufb01cation" (Monasse)</li>\n<li>"Rectification and Disparity" (slideshow by Navab)</li>\n<li>"A fast area-based stereo matching algorithm" (Di Stefano - seems a bit inaccurate)</li>\n<li>"Computing Visual Correspondence with Occlusions via Graph Cuts" (Kolmogorov - this one produces a very good disparity map, with also occlusion informations, but is it efficient?)</li>\n<li>"Dense Disparity Map Estimation Respecting Image Discontinuities" (Alvarez - too long for a first review)</li>\n</ul>\n\n<p>Could someone please give me some advice for diving into this wide topic? </p>\n\n<p>What kind of algorithm/method should I treat first, considering that I\'ll work on a very simple input: a pair of left and right images and nothing else, no more information (some papers are based on additional, pre-obtained, calibration information)?</p>\n\n<p>Speaking about working implementations, the only interesting results I\'ve seen so far belongs to <a href="http://stereo.jpn.org/eng/stphmkr/index.html" rel="nofollow">this</a> piece of software, but only for automatic rectification, not disparity.</p>\n\n<p>I tried the "auto-adjustment" feature and it seems really effective. Too bad there is no source code.</p>\n', 'ViewCount': '120', 'Title': 'Stereo images rectification and disparity: which algorithms?', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-17T18:19:58.393', 'LastEditDate': '2013-03-17T18:19:58.393', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7314', 'Tags': '<algorithms><reference-request><image-processing>', 'CreationDate': '2013-03-17T16:33:52.137', 'FavoriteCount': '1', 'Id': '10584'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would you algorithmically detect for any given photo whether the sun was shining when the picture was taken?</p>\n\n<p><strong>Examples</strong></p>\n\n<p>A sample from <a href="http://www.saentisbahn.ch/wetter-am-saentis.html" rel="nofollow">this webcam</a> at a mountain top: </p>\n\n<p><img src="http://frightanic.com/misc/saentis-20130406-110012-links.jpg" alt="sunshine example"></p>\n\n<p>Clearly the sun is shining.</p>\n\n<p>In this other sample it\'s far less obvious:</p>\n\n<p><img src="http://frightanic.com/misc/bettmeralp-20130407-135203-cam7.jpg" alt="cloudy example"></p>\n\n<p>One could probably detect fairly easy whether it\'s foggy by trying to identify the tiny church spire on the chapel in the center. However, knowing very little about image processing I\'d be surprised if there was a (combination of) algorithm that could reliably tell if there\'s sunshine or not. </p>\n', 'ViewCount': '137', 'Title': 'How to detect sunshine on a photo', 'LastEditorUserId': '7599', 'LastActivityDate': '2013-04-07T12:23:05.503', 'LastEditDate': '2013-04-07T12:23:05.503', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7599', 'Tags': '<algorithms><image-processing>', 'CreationDate': '2013-04-05T14:58:06.533', 'FavoriteCount': '1', 'Id': '11060'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to represent the calculation in this image mathematically?  <img src="http://i.stack.imgur.com/bNJ3V.gif" alt="http://i.stack.imgur.com/vpbuy.gif"></p>\n\n<p>For example: With the discrete convolution\n$(f * g)[n]\\ \\stackrel{\\mathrm{}}{=}\\ \\sum_{m=-\\infty}^\\infty f[m]\\, g[n - m]$\nand Fourier transform.</p>\n', 'ViewCount': '97', 'Title': 'Mathmatical way to represent an image kernel?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-04-07T19:37:32.717', 'LastEditDate': '2013-04-06T19:49:47.300', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<image-processing>', 'CreationDate': '2013-04-06T19:19:39.033', 'Id': '11078'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Why is the laplace transform not popular for image processing convolution? Most textbooks only conver the Fourier transforms.</p>\n', 'ViewCount': '694', 'Title': 'Why is the laplace transform not popular for image processing convolution?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-31T10:49:19.287', 'LastEditDate': '2013-04-11T07:35:40.143', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<history><image-processing>', 'CreationDate': '2013-04-11T02:44:03.157', 'Id': '11216'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Note that I had asked this question in <a href="http://gis.stackexchange.com/questions/41724/space-filling-between-random-2d-lines">GIS forum</a>, although it\n  has gotten many up-votes, still has not received any answer. Hope you can\n  break the silence, some collaboration :)</p>\n</blockquote>\n\n<p>Consider a region (2D) filled with lines randomly (following Figure). We are interested in filling the empty spaces between lines including four boundary edges in a way:</p>\n\n<p><strong>0-</strong> maximizing the size of parcels;<br>\n<strong>1-</strong> shape of filling parcels is square aligned horizontally or vertically;<br>\n<strong>2-</strong> shape of filling parcels is square, <em>i.e., relaxed alignment</em>;<br>\n<strong>3-</strong>  <strong>shape of filling parcels is any quadrangle.</strong> <em>our original question</em>  </p>\n\n<p>So for now there are three different scenarios.<br>\n<strong><em>Note</strong> that the lines are of the form <code>[x1,y1,x2,y2]</code> point set, real numbers.</em></p>\n\n<p>[* * *] <strong><em>Ideas of possible solutions/algorithms/code snippets/etc are more than welcome.</em></strong></p>\n\n<p><img src="http://i.stack.imgur.com/K0gTM.png" alt="enter image description here"></p>\n\n<hr>\n\n<p>For the first case i.e., horizontally/vertically aligned squares, our proposal as a solution is:<br>\n<strong>1-</strong> <em>rasterising input lines into bitmap (matrix) using e.g., <a href="http://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm" rel="nofollow">Bresenham\'s algorithm</a></em><br>\n<strong>2-</strong> <em>searching nearby cells for each cell of desired colour (i.e., the same colour) with an objective function to maximize the associated area i.e., the number of pixels.</em>  </p>\n\n<p><img src="http://i.stack.imgur.com/Pb8pZ.png" alt="enter image description here">  </p>\n\n<p>We assumed that the reader is familiar with the concept of space-filling-tiling. You may follow <a href="http://paulbourke.net/texture_colour/randomtile/" rel="nofollow">this link</a> for inspiration. However, note that our problem is different. As we do not fill the empty space randomly and we do not choose the size of filler shapes randomly. The solution should be iterative, starting from largest to smallest permitted. The size boundary can be defined, for example, according to our explanation below for a particular study (our interest). For all the cases, there is no limit on the number of parcel being fitted. Indeed, it is up to user to limit the iteration number, by choosing a minimum area for parcels, for example. This is obvious in the example given above in which we discretised lines into pixels with specified size. That is, the procedure should run until entire empty area is filled respecting the criterion e.g., the maximum area of parcels.</p>\n\n<p><strong>A mathematical view for the problem can be stated as follows:</strong><br>\n<strong>2D:</strong> Find all rectangles that could be extracted from a given 2D region with some lines optimized for larger rectangle size as possible.<br>\n<strong>3D:</strong> Find all rectangular cubes that could be extracted from a given 3D region with some sub-planes (better: polygons) optimized for larger block size as possible. </p>\n\n<p><strong>- Application:</strong><br>\nOne application is to find out the distribution of extractable intact \'rock\' blocks in a heavily fractured \'mine\'. This could be very helpful for many aspects including drilling design, financial evaluation and so on.  </p>\n\n<p><em>-- Details:</em><br>\nFor a mine of decorative rock (stone) the products which are the blocks of intact rocks cut as rectangular cubes the price is closely dependent to the size of the block.  Extraction of a block from a suitable area i.e., with no major fracture will be desired if the amount of remaining parts is small as possible. Usually, the small pieces of rocks have no economic value relatively and are considered so as waste.<br>\nThe question in this post investigates solutions for this kind of problem.  </p>\n\n<p><strong>Size Constrains:</strong><br>\nYou may put some restrictions on the solution for the ultimate question, although, we believe it is always possible to add more later.  For example, follow these:\n{2D case}<br>\nThe best size of a block (economically optimum rectangle) to be extracted under the conditions mentioned above, is <code>1x1 m</code> given <code>10x10 m</code> for the region in the example. This is one constraint defined based on economical value. The minimum workable size for cutting etc, let be <code>0.15x0.15 m</code>; so this the second size limit.<br>\n<img src="http://i.stack.imgur.com/akj12.jpg" alt="enter image description here"><br>\nThe figure above shows the economic value function depending to the block size. So for this particular case every rock piece smaller than <code>0.15x0.15 m</code> is just waste. There will be no block size larger than <code>1.7x1.7 m</code> due to operation limits.</p>\n', 'ViewCount': '121', 'Title': 'Space filling between random 2D lines', 'LastEditorUserId': '7712', 'LastActivityDate': '2013-04-14T10:49:45.397', 'LastEditDate': '2013-04-14T10:49:45.397', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7712', 'Tags': '<algorithms><computational-geometry><image-processing>', 'CreationDate': '2013-04-13T12:03:42.897', 'FavoriteCount': '2', 'Id': '11282'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p><em>First off, I am not sure if this is the correct stackexchange site to ask this question on, so moderators can feel free to move it.</em></p>\n\n<p>I am working on an application that identifies an object in an image.  For example, let's say that the object is an apple, and the apple can be green, red, or both.</p>\n\n<p>My plan is to create an array the same size as the image, then scan through the image, pixel by pixel, and if the current pixel is within my range of colors (green-ish or red-ish), add a 1 to the corresponding location in the array, else add a 0.</p>\n\n<p>After this scan through the array, I will have an array of 1s and 0s, which will hopefully contain a concentration of 1s, representing the apple.</p>\n\n<hr>\n\n<p>I am a computer science student, but have yet to take an AI class.  So my question is:  is this a good way to go about doing this?  Or are there more established AI methods (or algorithms) for identifying objects in an image based on color?</p>\n\n<p>EDIT:  I should clarify that the problem isn't identifying if the image contains an apple, the problem is identifying where the apple is in the image.</p>\n", 'ViewCount': '84', 'Title': 'Identifying an object in an image based on color (AI ?)', 'LastActivityDate': '2013-04-22T20:24:16.660', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '11499', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7838', 'Tags': '<algorithms><artificial-intelligence><image-processing>', 'CreationDate': '2013-04-22T15:04:35.920', 'Id': '11489'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Why do we need to flip the kernel in 2D convolution in the first place?\nWhat\'s the benefit of this?\nSo, why can\'t we leave it unflipped?\n<a href="http://www.songho.ca/dsp/convolution/convolution2d_example.html" rel="nofollow">http://www.songho.ca/dsp/convolution/convolution2d_example.html</a></p>\n\n<p><img src="http://i.stack.imgur.com/J3yXY.gif" alt="input"> \ninput</p>\n\n<p><img src="http://i.stack.imgur.com/Be7My.gif" alt="kernel">\nkernel</p>\n\n<p><img src="http://i.stack.imgur.com/a1K5z.gif" alt="output">\noutput </p>\n\n<p>"First, flip the kernel, which is the shaded box, in both horizontal and vertical direction"\n<img src="http://i.stack.imgur.com/z3qT0.gif" alt="http://www.songho.ca/dsp/convolution/files/conv_img16.gif"></p>\n', 'ViewCount': '67', 'Title': '2D convolution: Flipping the kernel?', 'LastActivityDate': '2013-04-26T22:35:36.173', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<image-processing>', 'CreationDate': '2013-04-26T22:35:36.173', 'Id': '11591'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>This is my first question, so please, be soft on me.</p>\n\n<p>I have a following problem:</p>\n\n<ul>\n<li>I'm a programmer not a mathematician, I don't often understand pure mathematical language and marks or symbols, I need to write a program using Java/C++</li>\n<li>I have a histogram (table[vector] of size 255 filled with integers)</li>\n<li>I\u2019ve got to write an algorithm that will approximate it</li>\n<li><p>I just need to find a proper polynomial : let's assume that degree is given (for example 4), my job is to find best fitted adverbials for every degree</p>\n\n<p>Can anyone help me to write it or send links to a proper step-by-step algorithm?</p>\n\n<p>I am aware it's not an easy task, any help would be appreciated.</p></li>\n</ul>\n", 'ViewCount': '69', 'Title': 'Aproximation algorithm for histogram', 'LastActivityDate': '2013-05-31T11:17:08.347', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8461', 'Tags': '<algorithms><approximation><image-processing>', 'CreationDate': '2013-05-31T11:17:08.347', 'Id': '12395'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider this small picture of a sunflower, and its histogram:</p>\n\n<p><img src="http://i.stack.imgur.com/Bbh3z.jpg" alt="Picture of a sunflower"> <img src="http://i.stack.imgur.com/yZ9fs.jpg" alt="Histogram for the sunflower picture"></p>\n\n<blockquote>\n  <p>What would the Fourier transform of the first picture look like?  Is there any relationship between the histogram and the Fourier transform?</p>\n</blockquote>\n', 'ViewCount': '204', 'Title': 'What the difference between the Fourier Transform of an image and an image histogram?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-06T21:03:53.853', 'LastEditDate': '2013-06-06T21:03:53.853', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8005', 'Tags': '<image-processing><graphics><fourier-transform>', 'CreationDate': '2013-06-06T18:36:09.453', 'Id': '12497'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Take the image and spectrum below. \nIf I look at the spectrum, it just look like noise....\nHow to make sense of it intuitively?</p>\n\n<p>Image:</p>\n\n<p><img src="http://i.stack.imgur.com/YwVP7.jpg" alt="original image"></p>\n\n<p>Frequency spectrum of image (using Fourier Transform):</p>\n\n<p><img src="http://i.stack.imgur.com/LQEMw.png" alt="frequency spectrum via fourier transformation"></p>\n', 'ViewCount': '42', 'Title': 'How can you see which points in the spectrum is from which pixel in the original image?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-09T14:39:59.397', 'LastEditDate': '2013-06-09T13:47:03.173', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'user8005', 'PostTypeId': '1', 'OwnerUserId': '8005', 'Tags': '<image-processing><fourier-transform>', 'CreationDate': '2013-06-09T02:22:10.053', 'Id': '12561'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it correct to say that the upperleft value \'0\' is has the (x,y)-coordinate of (-1,-1) and the bottomright \'0\' hast the (x,y)-coordinate of (1,1) and the center (x,y)-coordinate (0,0) has the value of \'5\'?</p>\n\n<p><img src="http://i.stack.imgur.com/bfUO8.png" alt="enter image description here"></p>\n\n<p><a href="http://en.wikipedia.org/wiki/Kernel_(image_processing)#Details" rel="nofollow">http://en.wikipedia.org/wiki/Kernel_(image_processing)#Details</a></p>\n', 'ViewCount': '19', 'Title': 'Cartesian coordinate system for an image kernel?', 'LastActivityDate': '2013-06-19T13:44:24.000', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8005', 'Tags': '<image-processing>', 'CreationDate': '2013-06-19T13:44:24.000', 'Id': '12761'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was admiring this rendition of the Mona Lisa from <a href="http://www.flickr.com/photos/quasimondo/4652406419/" rel="nofollow">quasimondo</a>\'s Flickr account.  He says:</p>\n\n<blockquote>\n  <p>Combining circle packing with data visualization. The pie charts show\n  the distribution of the dominant colors under the circle area.</p>\n  \n  <p>The circle packing technique used here is a combination of an image\n  segmentation with a distance transform and the first one who came up\n  with it is John Balestrieri: www.flickr.com/photos/tinrocket/</p>\n</blockquote>\n\n<p>I have traced it to an app called <a href="http://www.percolatorapp.com/blog/" rel="nofollow">Percolator</a></p>\n\n<p>How are such circle packings calculated? How are the pie charts calculated from the image?</p>\n\n<p><img src="http://farm5.staticflickr.com/4004/4652406419_f071593885.jpg"></p>\n', 'ViewCount': '350', 'Title': 'circle packing algorithm used by Percolator', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-27T15:34:34.887', 'LastEditDate': '2013-06-27T14:19:40.753', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12930', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><computational-geometry><discrete-mathematics><image-processing>', 'CreationDate': '2013-06-27T12:06:27.987', 'Id': '12925'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '615', 'Title': 'Where can a student learn image processing?', 'LastEditDate': '2013-07-01T05:37:46.307', 'AnswerCount': '5', 'Score': '3', 'OwnerDisplayName': 'Little Child', 'PostTypeId': '1', 'OwnerUserId': '8418', 'FavoriteCount': '1', 'Body': '<p>I am a student of computer science with interest in image processing. I have learned how to apply a few effects to images like making them grayscale, sketching them out of lines, etc.  </p>\n\n<p>I would like to learn more about the algorithmic techniques  behind creative manipulation of images like making them sepia-tone, smudging them, etc.  </p>\n\n<p><strong>Can someone please point me in the right direction?</strong> \nHow do I learn the fundamentals of these algorithms? </p>\n', 'Tags': '<reference-request><image-processing>', 'LastEditorUserId': '31', 'LastActivityDate': '2013-07-03T22:38:40.973', 'CommentCount': '14', 'AcceptedAnswerId': '13005', 'CreationDate': '2013-06-30T10:09:46.973', 'Id': '12990'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Many sites give the <a href="http://en.wikipedia.org/wiki/Sobel_operator" rel="nofollow">Sobel operators</a> as the convolution mask for smoothing an image. However, I haven\'t found a single site that describes how you can derive the operators from partial first derivatives. If anyone can explain the derivation, I would highly appreciate it.</p>\n', 'ViewCount': '212', 'Title': 'Deriving the Sobel equations from derivatives', 'LastEditorUserId': '7459', 'LastActivityDate': '2013-09-10T14:07:50.747', 'LastEditDate': '2013-09-10T02:17:44.400', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14245', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10052', 'Tags': '<image-processing><computer-vision>', 'CreationDate': '2013-09-09T19:12:38.067', 'Id': '14239'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have in mind a particular 3D object.  Given an image taken by a camera, I want to check whether that image contains an instance of my object.</p>\n\n<p>For instance, let's say that the object is a bathroom sink.  There are many kinds of bathroom sinks, but they tend to share some common elements (e.g., shape, size, color, function).  There can also be significant variation in lighting and pose.  Given an image, I want to know whether the image contains a bathroom sink.</p>\n\n<p>How do I do that?  What technique/algorithm would be appropriate?  Is there research on this topic?</p>\n\n<p>Of course, it is easy to use Google Images to obtain many example images that are known to contain a bathroom sink (or whatever the object I'm looking for might be), which could be used for training some sort of machine learning algorithm.  This suggests to me that maybe some combination of computer vision plus machine learning might be a promising approach, but I'm not sure exactly what the specifics might look like.</p>\n", 'ViewCount': '100', 'Title': 'Object recognition - given an image, does it contain a particular 3D object of interest?', 'LastActivityDate': '2013-10-29T18:12:04.513', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '14719', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<machine-learning><image-processing><computer-vision><pattern-recognition>', 'CreationDate': '2013-09-30T23:58:18.703', 'Id': '14717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '53', 'Title': 'Job sites for applied/interdisciplinary mathematics related to computer science?', 'LastEditDate': '2013-11-27T13:43:45.137', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11167', 'FavoriteCount': '0', 'Body': "<p>I'm looking for job sites in applied/interdisciplinary mathematics, more specially, say postdocs or higher positions in mathematics and medical imaging, mathematics and computer vision. I'm aware of mostly all the popular job sites, mathjobs, euro math jobs, jobs.ac.uk, nordic math jobs etc etc, but most of the jobs there are of 'pure' nature, with very few for applied/interdisciplinary.</p>\n\n<p>I'm trying to find postdoctoral position in mathematical imaging problems, which would use significant amount of conformal/quasiconformal mappings, Riemann surfaces, differential geometry etc. Looking into individual group's webpage is too much work. But if there's an webpage containing all the information, that'll be much better! </p>\n\n<p>So, if you know any such website for the above (for Europe(preferable) and US), I'd appreciate if you could pass them onto me. Thanks! </p>\n", 'ClosedDate': '2013-11-27T19:41:43.750', 'Tags': '<computational-geometry><image-processing><computer-vision>', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-11-27T13:43:45.137', 'CommentCount': '1', 'CreationDate': '2013-11-04T09:22:05.450', 'Id': '16697'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '109', 'Title': 'Which algorithms are usable for heatmaps and what are their pros and cons', 'LastEditDate': '2013-11-28T08:26:17.133', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11126', 'FavoriteCount': '1', 'Body': '<p>This is a <a href="http://stackoverflow.com/questions/19765076/which-algorithms-are-usable-for-heatmaps-and-what-are-their-pros-and-cons">cross post from Stack Overflow,</a> and <a href="http://dsp.stackexchange.com/questions/11463/which-algorithms-are-usable-for-heatmaps-and-what-are-their-pros-and-cons">DSP at Stackexchange</a> since I cannot really decide which part of Stackexchange is most fitting. If this is the wrong place please tell me and I\'ll remove the question.</p>\n\n<p>I have a matrix with numerical data. The matrix contains values from 0 to an arbitrary integer value.</p>\n\n<p>Each element of the matrix is equivalent to a coordinate on a map.</p>\n\n<p>I want to display that data as a heatmap overlayed the original map.</p>\n\n<p>The three approaches I have found so far are </p>\n\n<ol>\n<li><p>Linear interpolation. I guess the interpolation is don from the original datapoint to some set distance away from it in each direction. </p></li>\n<li><p>Average of surrounding cells. Each empty cell gets the average value of the eight adjacent cells. </p></li>\n<li><p>Gaussian blur as suggested on the SO thread.</p></li>\n<li><p>Box blur with 1..n passes.</p></li>\n</ol>\n\n<p>Are there any more methods? What are the pros and cons of the different approaches? What is a good source, online or print, for a discussion on heatmaps or similar problems?</p>\n', 'Tags': '<algorithms><image-processing><matrices><signal-processing>', 'LastEditorUserId': '11126', 'LastActivityDate': '2013-11-28T08:26:17.133', 'CommentCount': '6', 'AcceptedAnswerId': '18422', 'CreationDate': '2013-11-04T09:51:03.010', 'Id': '16699'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question about preparing the dataset of positive samples for a cascaded classifier that will be used for object detection.</p>\n\n<p>As positive samples, I have been given 3 sets of images:</p>\n\n<ol>\n<li>a set of <strong>colored</strong> images in full size (about 1200x600) with a <strong>white background</strong> and with the object displayed at a different angles in each image</li>\n<li>another set with the same images in grayscale and with a <strong>white background</strong>, scaled down to the detection window size (60x60)</li>\n<li>another set with the same images in grayscale and with a <strong>black background</strong>, scaled down to the detection window size (60x60)</li>\n</ol>\n\n<p>My question is that in Set 1, should the background really be white? Should it not instead be an <strong>environment</strong> that the object is likely to be found in in the testing dataset? Or should I have a fourth set where the images are in their natural environments? How does environment figure into the training samples?</p>\n', 'ViewCount': '36', 'Title': 'Environment requirement in training image dataset for classifier', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-27T12:25:13.433', 'LastEditDate': '2013-11-12T16:52:26.290', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<machine-learning><image-processing><computer-vision><data-sets><classification>', 'CreationDate': '2013-11-12T09:44:33.823', 'Id': '17949'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My aim is to classify types of cars (Sedans,SUV,Hatchbacks) and earlier I was using corner features for classification but it didn\'t work out very well so now I am trying Gabor features.<br/></p>\n\n<p><a href="http://www.mathworks.in/matlabcentral/fileexchange/38844-gabor-image-features" rel="nofollow">code from here</a><br/></p>\n\n<p>Now the features are extracted and suppose when I give an image as input then for 5 scales and 8 orientations I get 2 [1x40] matrices.</p>\n\n<p><strong>1. 40 columns of squared Energy.</strong></p>\n\n<p><strong>2. 40 colums of mean Amplitude.</strong></p>\n\n<p>Problem is I want to use these two matrices for classification and I have about 230 images of 3 classes (SUV,sedan,hatchback).</p>\n\n<p>I do not know how to create a [N x 230] matrix which can be taken as vInputs by the neural netowrk in matlab.(where N be the total features of one image).</p>\n\n<p>My question:</p>\n\n<ol>\n<li><p>How to create a one dimensional image vector from the 2 [1x40] matrices for one image.(should I append the mean Amplitude to square energy matrix to get a [1x80] matrix or something else?)</p></li>\n<li><p>Should I be using these gabor features for my purpose of classification in first place? if not then what?</p></li>\n</ol>\n\n<p>Thanks in advance</p>\n', 'ViewCount': '53', 'Title': 'Making feature vector from Gabor filters for classification', 'LastActivityDate': '2013-11-16T13:58:45.183', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18073', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11370', 'Tags': '<machine-learning><neural-networks><image-processing><classification>', 'CreationDate': '2013-11-14T06:23:16.160', 'Id': '18009'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm searching for computer vision or machine learning algorithms/methods that are used to classify or differentiate two outdoor environments. Given an image with vehicles, I need to be able to detect whether the vehicles are in a natural open landscape (desert, in particular), or whether they're in the city. </p>\n\n<p>I've searched but can't seem to find relevant work on this. Perhaps because I'm new at computer vision, I'm using the wrong search terms.</p>\n\n<p>Any ideas? Is there any work (or related) available in this direction?</p>\n", 'ViewCount': '70', 'Title': 'Environment detection - How to detect "city" versus "landscape" background environment in computer vision?', 'LastActivityDate': '2013-12-19T18:52:08.227', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<image-processing><computer-vision><classification>', 'CreationDate': '2013-11-28T06:55:27.333', 'Id': '18437'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let\'s assume that I\'ve implemented a method for the extraction of the principal lines of the palmprint. For instance, let\'s say that I\'m able to do a transformation like the following one (image from <a href="http://kst.buu.ac.th/proceedings/KST2010/docs/en08.pdf" rel="nofollow">Patprapa Tunkpien, Sasipa Panduwadeethorn and Suphakant Phimoltares</a>):</p>\n\n<p><img src="http://i.stack.imgur.com/vc7YJ.png" alt="Example of extraction of principal lines of the palmprint"></p>\n\n<p>Now, I want to make an evaluation of the accuracy level of my algorithm. In order to do this, I have only the algorithm, and a big set of palmprint images (on which I can execute the extraction procedure to obtain the corresponding set of binary images of principal lines of the palmprint).</p>\n\n<p>How can I evaluate the accuracy level of my algorithm, without having any kind of reference extraction method?</p>\n', 'ViewCount': '34', 'Title': 'How to evaluate the accuracy of a method for the extraction of the principal lines of the palmprint', 'LastEditorUserId': '12301', 'LastActivityDate': '2013-12-22T20:05:22.463', 'LastEditDate': '2013-12-22T16:43:22.697', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19197', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12301', 'Tags': '<algorithms><algorithm-analysis><image-processing>', 'CreationDate': '2013-12-22T16:35:11.107', 'Id': '19192'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a image similar to below. In that I need to find all red circles and count them.\nSo, I am thinking to use MATLAB R2011a with Image Processing Toolkit for it. How could I possibly extract them?</p>\n\n<p><img src="http://i.stack.imgur.com/vSK87.jpg" alt="enter image description here"></p>\n', 'ViewCount': '66', 'Title': 'How to count near circular objects in image', 'LastActivityDate': '2013-12-24T22:28:05.727', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12333', 'Tags': '<image-processing>', 'CreationDate': '2013-12-24T05:10:54.997', 'Id': '19232'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m studying about Local Binary Pattern and I\'m having trouble understanding the following part about the number of output labels for binary patterns from <em>Computer Vision using Local Binary Patterns</em>, by Pietik\xe4inen et al. (2011):</p>\n\n<blockquote>\n  <p>Another extension to the original operator uses so called <em>uniform patterns</em>&nbsp;[53]. For this, a uniformity measure of a pattern is used: $U$ ("pattern") is the number of bitwise transitions from 0 to&nbsp;1 or vice versa when the bit pattern is considered circular. A local binary pattern is called uniform if its uniformity measure is at most&nbsp;2. For example, the patterns 00000000 (0&nbsp;transitions), 01110000 (2&nbsp;transitions) and 11001111 (2&nbsp;transitions) are uniform whereas the patterns 11001001 (4&nbsp;transitions) and 01010011 (6&nbsp;transitions) are not. In uniform LBP mapping there is a separate output label for each uniform pattern and all the non-uniform patterns are assigned to a single label. Thus, the number of different output labels for mapping for patterns of $P$&nbsp;bits is $P(P-1)+3$. For instance, the uniform mapping produces 59 output labels for neighborhoods of 8 sampling points and 243 labels for neighborhoods of 16 sampling points.</p>\n</blockquote>\n\n<p>I don\'t understand why the number of different LBP output labels is $P(P-1) + 3$. Could someone explain why?....</p>\n\n<p>Thnx for any help =) </p>\n\n<p>Update: I think I have some idea already :) I included an example from my book:</p>\n\n<p><img src="http://i.stack.imgur.com/AMLGu.png" alt="LBP example"></p>\n', 'ViewCount': '54', 'Title': 'Number of different output labels in Local Binary Pattern', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-01-20T12:40:12.363', 'LastEditDate': '2014-01-20T12:16:14.070', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19849', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12977', 'Tags': '<image-processing><binary-arithmetic>', 'CreationDate': '2014-01-20T07:10:14.723', 'Id': '19844'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><img src="http://i.stack.imgur.com/bNQ68.png" alt="enter image description here"></p>\n\n<p>I\'m working on semi automatic texture segmentation using level set and gabor feature vector described by Manjunath und Ma [1]. I consider their feature representation is better than others since I can get good result on several images but somehow performs really bad on other images data. So I guess there\'s something wrong with my understanding.</p>\n\n<p>They describe that the feature vectors consist of $\\mu_{mn}$ (mean) and $\\sigma_{mn}$ (standard deviation). I use scale $S = 4$ and orientation $K = 6$, thus my feature vector will be 48 dimension (4 * 6 * 2 = 48):</p>\n\n<p>$f = \\{  \\mu_{00}, \\sigma_{00}, \\mu_{01}, \\sigma_{01}, \\mu_{02}...\\mu_{35}, \\sigma_{35}  \\} \\tag1$</p>\n\n<p>I am confused with their equation:</p>\n\n<p>$W(x, y) = \\int I(x_1,y_1)g_{mn} * (x - x_1, y - y_1) dx_1 dy_1 \\tag2$</p>\n\n<p>$\\mu_{mn} = \\int \\int |W_{mn}(x,y)|dxdy \\tag3$</p>\n\n<p>$\\sigma_{mn} = \\sqrt{ \\int \\int (|W_{mn}(x,y) - \\mu_{mn}|)^2 dxdy } \\tag4$</p>\n\n<p>So my first question is: <strong>are $\\mu$ and $\\sigma$ simply mean and average?</strong> So far, I just average every pixel value of the magnitude to obtain $\\mu$ and compute the standard deviation to obtain $\\sigma$.</p>\n\n<p>After I compute the feature vector, I need to do distance measure between two vectors $f_i$ and $f_j$</p>\n\n<p>$d(i, j) = \\sum_{}^{m}\\sum_{}^{n} d_{mn}(i, j) \\tag5$</p>\n\n<p>$d_{mn}(i, j) = |\\frac{\\mu_{mn}^i - \\mu_{mn}^j}{\\alpha(\\mu_{mn})}|  + |\\frac{\\sigma_{mn}^i - \\sigma_{mn}^j}{\\alpha(\\sigma_{mn})}| \\tag6$</p>\n\n<p>I\'m really puzzled with $\\alpha(\\mu_{mn})$ and $\\alpha(\\sigma_{mn})$ ? <strong>What are they?</strong> I have tried to calculate 48 standard deviations from $ (1)$</p>\n\n<p>$\\alpha = \\{ \\alpha(\\mu_{00}), \\alpha(\\sigma_{00}), \\alpha(\\mu_{01}), \\alpha(\\sigma_{01}), \\alpha(\\mu_{02})...\\alpha(\\mu_{35}), \\alpha(\\sigma_{35}) )\\} \\tag7$</p>\n\n<p>and use it to calculate the distance function but I\'m not convinced with my calculation since <strong>it seems not better than simple euclidean distance.</strong></p>\n\n<hr>\n\n<ol>\n<li><a href="http://vision.ece.ucsb.edu/publications/96PAMITrans.pdf" rel="nofollow">Texture Features for Browsing and Retrieval of Image Data</a> by B.S. Manjunath and W.Y. Ma (1996)</li>\n</ol>\n', 'ViewCount': '32', 'Title': 'How to compute Gabor feature vector?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-20T18:13:21.637', 'LastEditDate': '2014-01-20T11:42:09.483', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12978', 'Tags': '<algorithms><image-processing><computer-vision>', 'CreationDate': '2014-01-20T10:59:27.953', 'FavoriteCount': '1', 'Id': '19847'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '21', 'Title': 'How do I apply patch sized features to larger images?', 'LastEditDate': '2014-01-24T17:23:21.613', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13101', 'FavoriteCount': '0', 'Body': "<p>I've been trying to teach myself some machine learning, and I wanted to ask what seems a simple question, but I've not been able to find any resources that explain the next step.</p>\n\n<p>Let's say I am doing semi-supervised learning, and have a few hundred sparsely distributed feature extractors, trained on random 32x32 regions (the unsupervised part of the process). </p>\n\n<p>I now want to take the larger images in my training set, and do some supervised learning based on the feature extractors I now have. In this case, multi-label classification.</p>\n\n<p>The bit I'm not clear on is what I do with the full sized image from my training set:</p>\n\n<ul>\n<li>Take random samples from it? -- seems like it would be pot luck if it picks an area needed to identify appropriate labels</li>\n<li>Take overlapping tiles with a sliding window? -- seems like I'd end up with absurd dimensionality, since for each tile, I get a whole vector of features</li>\n<li>Take adjacent tiles? -- dimensionality still nonsensical, and probably translation sensitive as well</li>\n</ul>\n\n<p>It's a hypothetical example, but let's say my inputs are 800x600 photographs, i.e. the input is about 100-1000 times bigger than the samples I used in the unsupervised learning stage.</p>\n", 'Tags': '<machine-learning><image-processing><pattern-recognition>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-25T12:26:53.737', 'CommentCount': '0', 'AcceptedAnswerId': '19962', 'CreationDate': '2014-01-24T15:46:33.130', 'Id': '19944'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to recognize waterways from aerial photographs (say from Google Maps). Local governments often have GIS data that says where the waterways (and roads, buildings etc) are, but the water data in them is often somewhat inaccurate, and we might be able to improve them using aerial imagery. So we have some data already that isn\'t always to be trusted.</p>\n\n<p>I know how to do some basic image processing on the data (unfortunately I don\'t have sample images to show here yet, I\'m trying to imagine how I could do this, no working code yet):</p>\n\n<ul>\n<li><p>I can collect some set of color values using bits of waterway in images, and figure out which pixels are closest to these colors, possibly also for other types of feature (grass, roads, buildings etc). If I set a threshold on which pixels are "close enough", I get a set of pixels that are probably waterways (but there will be a lot of noise).</p></li>\n<li><p>I can turn the image into grayscale and use a standard edge detection algorithm to figure out where the edges are. Again, this gives me a set of pixels of like boundaries, but there will be noise and edges will be too think and/or have gaps.</p></li>\n</ul>\n\n<p>What I want to have as output is a set of <em>polygons</em> that outline the probable waterways.</p>\n\n<p>Intuitively I\'d like to use the detected edges to create polygons, and the colour information to decide which of them are water, possibly making use of the government data we already have.</p>\n\n<p>Is there a known way to get from the result of an edge detection algorithm to a nice set of closed polygons? Or any other tips on how to attack this problem, if there\'s a better way?</p>\n', 'ViewCount': '22', 'Title': 'Recognizing waterways in an aerial photo -- polygons from edge detection images', 'LastActivityDate': '2014-02-24T18:17:26.403', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '7', 'OwnerDisplayName': 'RemcoGerlich', 'PostTypeId': '1', 'OwnerUserId': '13099', 'Tags': '<image-processing>', 'CreationDate': '2014-02-17T11:39:13.220', 'Id': '21983'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If you resize an 1000x1000 raster image to 10x10 and resize it back to 1000x1000 is this considered to be an example of <a href="https://en.wikipedia.org/wiki/Aliasing" rel="nofollow">aliasing</a>?</p>\n', 'ViewCount': '32', 'ClosedDate': '2014-03-30T02:33:04.923', 'Title': 'Resizing and aliasing in computer science', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-28T12:48:00.597', 'LastEditDate': '2014-03-28T12:48:00.597', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16228', 'Tags': '<terminology><image-processing><graphics>', 'CreationDate': '2014-03-28T08:24:19.157', 'Id': '23167'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I thought the Gibbs phenomenom is the result of Fourier analysis estimation (but was it Fourier Series estimation or can it also be Fourier Transform estimation?)</p>\n\n<p><img src="http://i.stack.imgur.com/A1Vt2.png" alt="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Gibbs_phenomenon_50.svg/285px-Gibbs_phenomenon_50.svg.png"></p>\n\n<p>JPEG uses the Discrete cosines Transform. A DCT is similar to a Fourier transform in the sense that it produces a kind of spatial frequency spectrum.</p>\n\n<p><img src="http://i.stack.imgur.com/H2AuR.jpg" alt="JPEG example JPG RIP 001.jpg  Lowest quality"> </p>\n\n<p>But what are the differences between the Gibbs phenomenom artefacts from Fourier and the artefacts from the Discrete Cosine?</p>\n', 'ViewCount': '32', 'Title': 'What are all the differences between the Gibbs phenomenon artefects and JPEG artefacts?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T02:14:05.370', 'LastEditDate': '2014-03-28T12:46:38.770', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16228', 'Tags': '<image-processing><data-compression><graphics><signal-processing>', 'CreationDate': '2014-03-28T08:56:20.763', 'Id': '23169'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My research is in the area of Document Image Analysis. To be specific, the topic of my thesis is to automatically recognize and index characters in a set of hand-drawn objects, e.g. given a volume of comics --> how to localize, recognize then index the characters in it (e.g. return "Lucky Luke" position in some random volume).</p>\n\n<p>First thing first, before I can work on the indexing part, I must have some reliable system to recognize the interested characters. After some months working on it, I find the recognition is extremely hard, because the drawings are some time very "abstract".</p>\n\n<p>For normal face recognition, there are a lot of researches on it, thus we have Haar-like features, biometrics features, etc. For normal photograph objects, we may use e.g. SIFT/SURF, or other strong features for the purpose of object recognition.</p>\n\n<p>But in cartoons or comics, we may extract so few or zero feature(s) following the traditional approaches. Human visual interpretation system is amazingly effective in recognize such abstract objects (e.g. only with 4 lines and 2 curves, we still recognize the familiar character).</p>\n\n<p>So I think the solution must be somewhere in the understanding how human can interpret such a set of line strokes as a meaningful object. Once we understand it we can try to "teach" the computer the same way.</p>\n\n<p>Thus are there some good approaches in the terms of recognition of abstract objects such as drawn comic objects? (I searched for some but they\'re not so relevant). Thanks.</p>\n', 'ViewCount': '81', 'Title': 'Method to recognize abstract objects such as hand-drawn objects?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T05:03:02.323', 'LastEditDate': '2014-04-10T22:29:54.210', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'Jim Raynor', 'PostTypeId': '1', 'OwnerUserId': '16662', 'Tags': '<machine-learning><image-processing><pattern-recognition>', 'CreationDate': '2014-04-09T21:53:22.637', 'Id': '23657'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking into different attendance taking options for a particularly large seminar class (600ish students) and I was thinking - what if a picture was taken of the seats, maybe with a visual cue on them such as reflective tape. Another picture was taken during a class with assigned seats and if the reflective tape is covered, it could be a good indication of whether the seat is occupied or not.</p>\n\n<p>What kind of software should I be looking at to achieve this effect. The main things are being able to detect an object (maybe some kind of material which is obvious to the computer) and detecting if its gone</p>\n\n<p>Thank you for your help - let me know if I should post this somewhere else</p>\n", 'ViewCount': '13', 'ClosedDate': '2014-04-17T10:59:58.220', 'Title': 'Using image processing to infer if a seat is occupied', 'LastActivityDate': '2014-04-17T10:59:38.567', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16825', 'Tags': '<education><image-processing>', 'CreationDate': '2014-04-16T11:33:00.040', 'Id': '23852'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I wrote matlab code for face detection.In my code it is detecting face for first 100 frames and it crop faces from each frame and saves it in database folder.Problems iam facing</p>\n\n<p>1.Detecting frame by frame is very slow.Is there any idea to run faster since i have to work on 4000 frames.</p>\n\n<p>2.In my database folder it has to show 1 to 100 face images but it is not showing 11th and 12th face images directly it showing 13th face image after 10th image.23rd face image is blurr.Likewise so many images are missing and some are blurr.Last image number it is showing as 216.But total 106 face images are there in database folder.In that 12 images are blurr.Remaining are correct images.</p>\n\n<pre><code>clc;\nclear all;\n\nobj=vision.VideoFileReader('basu.avi');\n\nfor k=0:99\n\nvideoFrame      = step(obj);\n%using viola-jones algorithm\n      FaceDetect = vision.CascadeObjectDetector;\n\n     %FaceDetect\n      BB = step(FaceDetect,videoFrame);\n      %BB\n       figure(2),imshow(videoFrame);\n\nfor i = 1:size(BB,1)\n      rectangle('Position',BB(i,:),'LineWidth',3,'LineStyle','-','EdgeColor','r');\nend\n\n%crop faces and convert it to gray\nfor i = 1:size(BB,1)\nJ= imcrop(videoFrame,BB(i,:));\nI=rgb2gray(imresize(J,[292,376]));\n\n%save cropped faces in database folder\nfilename = ['G:\\matlab_installed\\bin\\database\\' num2str(i+k*(size(BB,1))) '.jpg'];\n    imwrite(I,filename);\nend\nend\n</code></pre>\n", 'ViewCount': '15', 'ClosedDate': '2014-04-27T11:07:02.613', 'Title': 'Detecting face from video using vision.CascadeObjectDetector and cropped faces are missing from database in matlab', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-27T09:58:32.427', 'LastEditDate': '2014-04-27T09:58:32.427', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17114', 'Tags': '<image-processing><video>', 'CreationDate': '2014-04-27T04:34:09.173', 'Id': '24148'}},