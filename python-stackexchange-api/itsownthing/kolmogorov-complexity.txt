{'ViewCount': '154', 'Title': 'Equivalence of Kolmogorov-Complexity definitions', 'LastEditDate': '2012-03-09T03:10:00.007', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '157', 'FavoriteCount': '1', 'Body': '<p>There are many ways to define the <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov-Complexity</a>, and usually, all these definitions they are equivalent up to an additive constant. That is if $K_1$ and $K_2$ are kolmogorov complexity functions (defined via different languages or models), then there exists a constant $c$ such that for every string $x$, $|K_1(x) - K_2(x)| &lt; c$. I believe this is because for every Kolmogorov complexity function $K$ and for every $x$ it holds that $K(x) \\le |x| +c$, for some constant $c$.</p>\n\n<p>I\'m interested in the following definitions for $K$, based on Turing-machines</p>\n\n<ol>\n<li><strong>number of states</strong>: Define $K_1(x)$ to be the minimal number $q$ such that a TM with $q$ states outputs $x$ on the empty string.</li>\n<li><strong>Length of Program</strong>: Define $K_2(x)$ to be the shortest "program" that outputs $x$. Namely, fix a way to encode TMs into binary strings; for a machine $M$ denote its (binary) encoding as $\\langle M \\rangle$.  $K_2(x) = \\min |\\langle M \\rangle|$ where the minimum is over all $M$\'s that output $x$ on empty input.</li>\n</ol>\n\n<p>Are $K_1$ and $K_2$ equivalent? What is the relation between them, and which one grasps better the concept of Kolmogorov complexity, if they are not equivalent.</p>\n\n<p>What especially bugs me is the rate $K_2$ increase with $x$, which seems not to be super-linear (or at least linear with constant $C&gt;1$ such that $K_2 &lt; C|x|$, rather than $|x|+c$).\nConsider the most simple TM that outputs $x$ - the one that just encodes $x$ as part of its states and transitions function. it is immediate to see that\n$K_1(x) \\le |x|+1$. However the encoding of the same machine is much larger, and the trivial bound I get is $K_2(x) \\le |x|\\log |x|$. </p>\n', 'Tags': '<computability><kolmogorov-complexity>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-15T01:02:24.947', 'CommentCount': '4', 'AcceptedAnswerId': '402', 'CreationDate': '2012-03-09T01:46:42.533', 'Id': '146'}{'ViewCount': '290', 'Title': 'Difference between "information" and "useful information" in algorithmic information theory', 'LastEditDate': '2012-04-22T15:52:04.393', 'AnswerCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '877', 'FavoriteCount': '3', 'Body': '<p>According to <a href="http://en.wikipedia.org/wiki/Algorithmic_information_theory#Overview">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>Informally, from the point of view of algorithmic information theory, the information content of a string is equivalent to the length of the shortest possible self-contained representation of that string.</p>\n</blockquote>\n\n<p>What is the analogous informal rigorous definition of "useful information"? Why is "useful information" not taken as the more natural or more fundamental concept; naively it seems a purely random string must by definition contain zero information, so I\'m trying to get my head around the fact that it is considered to have maximal information by the standard definition.</p>\n', 'Tags': '<information-theory><terminology><kolmogorov-complexity>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-28T19:54:34.903', 'CommentCount': '1', 'AcceptedAnswerId': '946', 'CreationDate': '2012-04-01T14:16:41.417', 'Id': '945'}{'Body': '<p>In <em>Algorithmic Randomness and Complexity</em> from Downey and Hirschfeldt, it is stated on page 129 that </p>\n\n<p>$\\qquad \\displaystyle \\sum_{K(\\sigma)\\downarrow} 2^{-K(\\sigma)} \\leq 1$, </p>\n\n<p>where $K(\\sigma)\\downarrow$ means that $K$ halts on $\\sigma$, $\\sigma$ being a binary string. $K$ denotes the prefix-free Kolmogorov complexity.</p>\n\n<p>When does $K$ halt? I think it only halts on a finite number of inputs, since the classical proof on non-computability of the Kolmogorov complexity gives an upper bound on the domain of $K$. But then, the finite set of inputs on which $K$ halts can be chosen arbitrary (one just needs to store the finite number of complexities in the source code).</p>\n\n<p>So is this sum well-defined? In other words, is the domain of $K$ well defined?</p>\n', 'ViewCount': '145', 'Title': 'When does the function mapping a string to its prefix-free Kolmogorov complexity halt?', 'LastEditorUserId': '2069', 'LastActivityDate': '2012-07-08T09:51:02.807', 'LastEditDate': '2012-07-05T09:06:51.267', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '2625', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2069', 'Tags': '<computability><terminology><kolmogorov-complexity><descriptive-complexity>', 'CreationDate': '2012-07-04T15:34:08.397', 'Id': '2614'}{'Body': '<p>What\'s the definition of Kolmogorov complexity for a decision problem? For example, how to define the length of the shortest program that solves the 3SAT problem? Is it the "smallest" Turing machine that recognizes the 3SAT langauge?</p>\n', 'ViewCount': '76', 'Title': 'Kolmogorov complexity of a decision problem', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:33:05.280', 'LastEditDate': '2012-07-18T01:33:05.280', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '2779', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1986', 'Tags': '<computability><terminology><decision-problem><kolmogorov-complexity><3-sat>', 'CreationDate': '2012-07-16T18:48:03.180', 'Id': '2770'}{'ViewCount': '212', 'Title': 'Approximating the Kolmogorov complexity', 'LastEditDate': '2013-12-10T11:06:26.913', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '702', 'FavoriteCount': '3', 'Body': '<p>I\'ve studied something about the <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">Kolmogorov Complexity</a>, read some articles and books from <a href="http://homepages.cwi.nl/~paulv/kolmogorov.html" rel="nofollow">Vitanyi and Li</a> and used the concept of <a href="http://complearn.org/ncd.html" rel="nofollow">Normalized Compression Distance</a> to verify the stilometry of authors (identify how each author writes some text and group documents by their similarity).</p>\n\n<p>In that case, data compressors were used to approximate the Kolmogorov complexity, since the data compressor could be used as a Turing Machine.</p>\n\n<p>Besides data compression and programming languages (in which you would write some kind of compressor), what else could be used to approximate the Kolmogorov complexity? Are there any other approaches that could be used?</p>\n', 'Tags': '<computability><approximation><data-compression><kolmogorov-complexity>', 'LastEditorUserId': '702', 'LastActivityDate': '2013-12-10T21:28:05.640', 'CommentCount': '4', 'AcceptedAnswerId': '3531', 'CreationDate': '2012-09-11T00:02:12.553', 'Id': '3501'}{'ViewCount': '168', 'Title': 'Kolmogorov complexity of string concatenation', 'LastEditDate': '2012-09-16T06:11:49.433', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '140', 'FavoriteCount': '1', 'Body': '<p>If $K(s)$ is the <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">Kolmogorov complexity</a> of the string $s \\in \\{0,1\\}^*$,</p>\n\n<blockquote>\nCan we prove (or disprove) the following statement:\n<br><br>\n"Every string $s$ is a prefix of an incompressible string; i.e. for every string $s$ there exists a string $r$ such that $K(sr) \\geq |sr|$" ?\n</blockquote>\n\n<p>In a very informal (and perhaps not too meaningful) way: we know that $K(r) \\leq |r| + O(1)$; if we pick a large enough incompressible string $r$, can we "use" the $O(1)$ to "mask" the compressibility of the given string $s$ ?</p>\n\n<p>A similar (but different) result is that for any $c$, we can find $s$ and $r$ such that: $K(sr) &gt; K(s) + K(r) + c$</p>\n', 'Tags': '<computability><kolmogorov-complexity>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-16T18:30:20.183', 'CommentCount': '2', 'AcceptedAnswerId': '4576', 'CreationDate': '2012-09-15T22:20:59.723', 'Id': '4570'}{'Body': '<p>Any string generated from a PRNG clearly has a very short description.  You need the code for the random number generator, the seed, and then the number of times to iterate.  So, it seems that all such strings have low KC complexity.</p>\n\n<p>If the above it true, then what is an example of a "complex" string, in the Kolmogorov-Chatin sense?</p>\n', 'ViewCount': '96', 'Title': 'What is an example of complex random string, in the Kolmogorov-Chatin sense?', 'LastActivityDate': '2013-02-13T08:53:31.840', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'OwnerDisplayName': 'Travis M.', 'PostTypeId': '1', 'Tags': '<kolmogorov-complexity>', 'CreationDate': '2013-02-11T04:11:16.233', 'Id': '9721'}{'Body': '<p>The Kolmogorov complexity of a string $x$ is the size of the smallest Turing machine $M$ that started on empty tape produces $x$. To make it computable, we can add a bound on the time used by $M$ to produce $x$: </p>\n\n<p>$C^{t}(x) = \\min \\{|M| : U(M) = x$ in less than $t(n)$ steps $ n = |x| \\}$ </p>\n\n<p>And for a nice function $f(n) &lt; n$ we can define:</p>\n\n<p>$C[f(n),t(n)] = \\{x : C^t(x) \\leq f(n), n = |x| \\}$</p>\n\n<p>i.e. the set of compressible strings $x$ (whose compressed program has size less than $f(n)$) and that can be generated in time $t(n)$.</p>\n\n<p>For example, for unbounded $f$, we have $C[f(n),n^k] \\subseteq C[f(n),n^{k+1}] \\subset C[f(n),\\infty]$</p>\n\n<blockquote>\n<ul><li>Is the first inclusion tight?</li>\n<li>What is known about the *size* of $C[f(n), n^{k+1}] \\setminus C[f(n), n^{k}]$ ?</li>\n<li>Are there known results for particular classes like $C[n/2,n^k]$?</li>\n</ul>\n</blockquote>\n', 'ViewCount': '51', 'Title': 'Interval density of time bounded Kolmogorov complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-11T07:34:56.750', 'LastEditDate': '2013-04-11T07:34:56.750', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '140', 'Tags': '<complexity-theory><reference-request><descriptive-complexity><kolmogorov-complexity>', 'CreationDate': '2013-04-10T22:52:05.313', 'Id': '11214'}{'Body': "<p>Fix a universal Turing machine $M$.</p>\n\n<p>Let $A^*=\\{0,1\\}^n$ be the set of all binary string of length $n$.</p>\n\n<p>Determine the Kolmorogov complexity $K(x)$ of each $x\\in A$, w.r.t. $M$.</p>\n\n<p>Just for a matter of simplicity assume that $K(x)'=\\min(|x|,K(x))$.</p>\n\n<p>Let $B^*=\\{0,1\\}^n$ be the set of all programs $y\\in B$ of $T$: $x= M(y)$.</p>\n\n<p>Now, suppose a parallel machine $M_p$ that can run each program $y\\in B$ for each element in $A$, in parallel. So we can get the length of all the programs $y$ whose machine ended with the right string $x$, and take the minimum as $K(x)'$.</p>\n\n<p>So, should I assume that the non-computability is due to the fact that some machines that has not ended jet whose inputs $y$ are less than the current minimal program  will eventually halt? What's the probability of this?</p>\n\n<p>Also, maybe a measure of complexity should include a minimal length with a probabilistic confidence? Say $P(K(x)\\geq l)\\geq p$?</p>\n", 'ViewCount': '86', 'Title': 'Computability of Kolmogorov Complexity', 'LastActivityDate': '2013-05-09T10:59:55.187', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11852', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6655', 'Tags': '<computability><kolmogorov-complexity>', 'CreationDate': '2013-05-07T08:38:31.827', 'Id': '11850'}{'Body': '<p>I wonder how it\'s possible that:</p>\n\n<blockquote>\n  <p>it can be shown that all reasonable choices of programming\n  languages lead to quantification of the amount of absolute information in\n  individual objects that is invariant up to an additive constant.   </p>\n</blockquote>\n\n<p>from the <a href="http://books.google.co.uk/books?id=25fue3UYDN0C&amp;pg=PR7&amp;lpg=PR7&amp;dq=it+can+be+shown+that+all+reasonable+choices+of+programming+languages+lead+to+quantification&amp;source=bl&amp;ots=U58ndePaej&amp;sig=ZTCWNL8-tKStxBgoEaL3bPiYAsQ&amp;hl=en&amp;sa=X&amp;ei=zj-GUazlNYaY0AXy34CwDw&amp;ved=0CDgQ6AEwAg#v=onepage&amp;q=it%20can%20be%20shown%20that%20all%20reasonable%20choices%20of%20programming%20languages%20lead%20to%20quantification&amp;f=false" rel="nofollow">Preface to the First Edition, An Introduction to Kolmogorov Complexity and Its Applications</a> by Li &amp; Vitanyi.</p>\n\n<p>I don\'t know how they defined "<em>reasonable choice of programming language</em>", but in my sense, it seems natural that the choice of programming language could mix the rank of the complexity. If the programming language fits the object, the program would be shorter, and if not, it would be longer.</p>\n\n<p>But, if I understood it all right, the paragraph above says that no matter which programming language you choose for the strings(objects), the rank of the length of the programs would not change!</p>\n\n<p>Is it really possible? and could someone explain how it\'s possible? </p>\n\n<p>It is well known that either AND and NOT or OR and NOT can be a primitive function for a computer,</p>\n\n<p>and if I want to implement AND with (OR and NOT), it goes like</p>\n\n<p>NOT(NOT a OR NOT b) instead of just AND!!!</p>\n', 'ViewCount': '72', 'Title': 'The choice of programming language and the length of a program', 'LastActivityDate': '2013-05-16T16:04:59.000', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'user1924418', 'PostTypeId': '1', 'Tags': '<computability><kolmogorov-complexity>', 'CreationDate': '2013-05-06T08:30:57.243', 'Id': '12074'}{'Body': u'<p>I am reading Kolmogorov Complexity by Li and Vit\xe1nyi:</p>\n\n<p>"Let $x$ be a finite binary string. We write \'$x$ is random\' if the shortest binary description of $x$ with respect to the optimal specification method $D_0$ has length at least $x$."\nBy length $x$ I understand the natural number that the binary string maps to canonically.</p>\n\n<p>[proof which I do not understand follows]</p>\n\n<p>"This shows that although most strings are random, it is impossible to effectively prove them random."</p>\n\n<p>However, I am able to produce a counterexample and can find a proof that $x$ is random effectively (there is an algorithm). Iterate over all the words of size up to $x-1$ of a description language. If you find a description $\\alpha_x$ such that $D_0(\\alpha_x)=x$ ($\\alpha_x$ describes $x$) then terminate with verdict that $x$ is not random. If you exhaust all the words of length $&lt;x$(there are finitely many since $x$ is finite so the program halts) and none of them describes $x$ and then terminate with result that $x$ is random.</p>\n\n<p>What is wrong in my counterexample?</p>\n', 'ViewCount': '88', 'Title': 'Proving a string is random', 'LastEditorUserId': '683', 'LastActivityDate': '2013-07-01T05:05:12.860', 'LastEditDate': '2013-07-01T05:05:12.860', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2598', 'Tags': '<computability><kolmogorov-complexity>', 'CreationDate': '2013-07-01T03:36:35.217', 'Id': '13001'}{'Body': '<p>I was reading <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity">Wikipedia\'s entry on Kolmogorov Complexity</a> (<a href="http://cs.stackexchange.com/questions/3501/approximating-the-kolmogorov-complexity">thanks to this question</a>), which states:</p>\n\n<blockquote>\n  <p>It can be shown that the Kolmogorov complexity of any string cannot be more than a few bytes larger than the length of the string itself.</p>\n</blockquote>\n\n<p>Why would you ever need anything more than the string itself to describe it?</p>\n', 'ViewCount': '370', 'Title': 'Kolmogorov Complexity: Why would you need more bytes than the string itself?', 'LastActivityDate': '2013-12-10T21:56:14.743', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18844', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '12000', 'Tags': '<kolmogorov-complexity>', 'CreationDate': '2013-12-10T21:47:38.200', 'FavoriteCount': '1', 'Id': '18842'}{'Body': "<p>I'm aware some ints have higher or lower Kolmogorov Complexities. For example, the number <code>5.41126806512</code> has a very low complexity as it can be expressed by <code>17/pi</code>. I'm also aware that, though the KC varies depending on the expression language, it is always the same up to a given constant. So, I ask: is there a way to calculate an <strong>approximation</strong> of the KC for the first N ints?</p>\n", 'ViewCount': '87', 'Title': 'What is an estimation of the Kolmogorov Complexity for the first N integers?', 'LastActivityDate': '2014-01-12T23:58:08.803', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<complexity-theory><kolmogorov-complexity>', 'CreationDate': '2014-01-10T03:59:57.223', 'Id': '19615'}{'Body': '<p>a few days ago I asked a question about the limits of compression:\n<a href="http://cs.stackexchange.com/questions/23010/can-prngs-be-used-to-magically-compress-stuff">Can PRNGs be used to magically compress-stuff?</a></p>\n\n<p>The idea common to all the answers was that if you consider all programs\nof length &lt; k (Let\'s call this set $C_k$), then there is only a finite number of possible outputs generated by programs in $C_k$ (let\'s call that set $S_k$).</p>\n\n<p>In the comments, I asked a followup question roughly equivalent to  "what determines which messages are possible outputs?" and have been thinking about this ever since.</p>\n\n<p>It seemes clear that what determines that is the program that executes\nthe program. If we ignore TM\'s and think in terms of source code, then \nthe output of a piece of code depends only on the syntax and semantics \nof the programming language. So the language design determines\nthe contents of $S_k$. There are some interesting corollaries:</p>\n\n<ul>\n<li>All program transformations that do no affect the AST nor enlarge the program beyond k bits will produce programs that generate the same output.</li>\n<li>All refactorings, which might alter the AST but preserve all input/output\nrelationships, will do the same if they do not inflate the size of the source code.</li>\n<li>a PL that provides multiple constructs ("There\'s more then one way to do it")\nfor achieving the same result (control flow constructs, for example) will , generally speaking, have more programs of length &lt; k which generate the same output and so a smaller $S_k$ then a similar but "slimmer" PL.</li>\n</ul>\n\n<p>This brings me to the following question: If I consider all programs\nin $C_k$ that generate the same output (it\'s an equivalence relation)\nand then proceed to compress them, what would be the results? Would\nthe equivalent programs compress down to roughly the same size or not?\nDoes the compressability depend on the meaning of the program (which\nis identical) or the bit sequence (which is not)?  </p>\n\n<p>A statement given in the answers to the previous question is\nthat the entropy of a bit sequence determines how compressible it is.\nIs that entropy determined by the input/output relationships encoded\nin the program, or by it\'s bit-representation in language X?\nWhat\'s the right way to think of this?</p>\n\n<p><strong>update</strong>\nSince a program describes a computational process\nrather then merely output, the notion of equivalence\nsuggested really doesn\'t indicate me much about\nthe relative complexity of programs which are equivalent \nby that definition.</p>\n', 'ViewCount': '76', 'LastEditorDisplayName': 'user15782', 'Title': "What determines the entropy of a program's source code?", 'LastActivityDate': '2014-03-30T11:05:16.263', 'LastEditDate': '2014-03-30T11:05:16.263', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23242', 'Score': '3', 'OwnerDisplayName': 'user15782', 'PostTypeId': '1', 'Tags': '<data-compression><kolmogorov-complexity>', 'CreationDate': '2014-03-30T02:17:13.703', 'Id': '23238'}