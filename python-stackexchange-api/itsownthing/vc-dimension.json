{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My goal is to solve the following problem, which I have described by its input and output:</p>\n\n<p><strong>Input:</strong></p>\n\n<p>A directed acyclic graph $G$ with $m$ nodes, $n$ sources, and $1$ sink ($m > n \\geq 1$).</p>\n\n<p><strong>Output:</strong></p>\n\n<p>The <a href="https://en.wikipedia.org/wiki/Vc_dimension">VC-dimension</a> (or an approximation of it) for the neural network with topology $G$.</p>\n\n<p><strong>More specifics</strong>: </p>\n\n<ul>\n<li>Each node in $G$ is a sigmoid neuron. The topology is fixed, but the weights on the edges can be varied by the learning algorithm.</li>\n<li>The learning algorithm is fixed (say backward-propagation).</li>\n<li>The $n$ source nodes are the input neurons and can only take strings from $\\{-1,1\\}^n$ as input.</li>\n<li>The sink node is the output unit. It outputs a real value from $[-1,1]$ that we round up to $1$ or down to $-1$ if it is more than a certain fixed threshold $\\delta$ away from $0$. </li>\n</ul>\n\n<p>The naive approach is simply to try to break more and more points, by attempting to train the network on them. However, this sort of simulation approach is not efficient.</p>\n\n<hr>\n\n<h3>Question</h3>\n\n<p>Is there an efficient way (i.e. in $\\mathsf{P}$ when changed to the decision-problem: is VC-dimension less than input parameter $k$?) to compute this function? If not, are there hardness results?</p>\n\n<p>Is there a works-well-in-practice way to compute or approximate this function? If it is an approximation, are there any guarantees on its accuracy?</p>\n\n<h3>Notes</h3>\n\n<p>I asked a <a href="http://stats.stackexchange.com/q/25952/4872">similar question</a> on stats.SE but it generated no interest.</p>\n', 'ViewCount': '204', 'Title': 'Efficiently computing or approximating the VC-dimension of a neural network', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T02:12:14.187', 'LastEditDate': '2012-04-25T16:58:07.127', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><complexity-theory><machine-learning><neural-networks><vc-dimension>', 'CreationDate': '2012-04-25T15:21:46.690', 'FavoriteCount': '1', 'Id': '1504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So I\'m reading <em>"Introduction to Machine Learning"</em> 2nd edition, by Bishop, et. all.  On page 27 they discuss the Vapnik-Chervonenkis Dimension which is,</p>\n\n<blockquote>\n  <p><em>"The maximum number of points that can be shattered by H [the hypothesis class] is called the Vapnik-Chervonenkis (VC) Dimension of H, is denoted VC(H) and measures the capacity of H."</em></p>\n</blockquote>\n\n<p>Whereas "shatters" indicates a hypothesis $h \\in H$ for a set of N data points such that it separates the positive examples from the negative.  In such an example it is said that "H shatters N points".</p>\n\n<p>So far I think I understand this.  However, the authors lose me with the following:</p>\n\n<blockquote>\n  <p><em>"For example, four points on a line cannot be shattered by rectangles."</em></p>\n</blockquote>\n\n<p>There must be some concept here I\'m not fully understanding, because I cannot understand why this is the case.  Can anyone explain this to me?  </p>\n', 'ViewCount': '159', 'Title': 'Vapnik-Chervonenkis Dimension: why cannot four points on a line be shattered by rectangles?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-19T19:46:25.100', 'LastEditDate': '2012-05-19T08:23:32.217', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '1932', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '603', 'Tags': '<machine-learning><vc-dimension>', 'CreationDate': '2012-05-18T20:22:02.690', 'Id': '1917'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am confused about the Vapnik-Chervonenkis dimension of a linear separator in 3 dimensions. </p>\n\n<p>In three dimensions, a linear separator would be a plane, and the classification model would be "everything on one side of a plane." </p>\n\n<p>It\'s apparently proved that the VC dimension of linear separators is d+1, so in 3D, its VC dimension is four. That means it should be able to put any set of 1, 2, 3, or 4 points on one side of a plane. </p>\n\n<p>But, what about this case: four coplanar points on a square with opposite corners same adjacent corners different?</p>\n\n<pre>\n+1    -1\n\n-1    +1\n</pre>\n\n<p>This is the case that a line (2-dimensional linear separator) cannot handle, but the 3-dimensional linear separator is supposed to be able to shatter this. But, I can\'t see how you could put two corners on "one side of a plane" because all four points are coplanar. </p>\n\n<p>Could someone explain how a 3-d linear separator can shatter the four points I just described? </p>\n', 'ViewCount': '90', 'Title': 'VC dimension of linear separator in 3D', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T07:43:51.953', 'LastEditDate': '2013-04-25T07:43:51.953', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7884', 'Tags': '<statistics><learning-theory><vc-dimension><classification>', 'CreationDate': '2013-04-25T03:25:36.933', 'Id': '11548'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My aim is to prove a VC-dimension $d$ for different problems. All the problems I have do not have a target function (or concept) explicitly stated. This unlike most of the examples I came through. For example in the interval problem, the target function $h^*$ is: if point $x\\in [a,b]$ then $x=+$ and $-$ otherwise. I do not know where $[a,b]$ resides in $R$ but at least I know its an interval. Therefore, three points of $(+,-,+)$ cannot be shattered by any concept.   </p>\n\n<p>I am given an infinite input space $X$ and $H$ is the class of all finite languages over $X$ and asked to prove the VC-dimension for this problem. I have no clue what the target function looks like.  </p>\n\n<p>Assume I got two points $x_1,x_2\\in X$, there are $2^2$ possibilities $(-,-),(-,+),(+,-),(+,+)$. I am stucking here since I don't know what is <em>shattered</em> (i.e. realisable) and what is <em>not</em> without knowing the target function $h^*$. Should I assume such function exists and then workout based on its behaviour? am I missing something? </p>\n", 'ViewCount': '84', 'Title': 'How to find the shattered set size for unknown hypothesis target', 'LastActivityDate': '2013-10-12T22:09:56.733', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<machine-learning><vc-dimension>', 'CreationDate': '2013-10-12T22:09:56.733', 'Id': '15025'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '63', 'Title': 'How to determine the size of training data using VC dimension?', 'LastEditDate': '2013-11-25T21:24:28.870', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11576', 'FavoriteCount': '0', 'Body': u'<p>I want to determine the size of training data ($m$) when I know the parameters $VC(H)$, $\u03b4$ and $e$. As I know the $VC$ bound satisfy this equation:</p>\n\n<p>$$ \\mathrm{error}_{\\mathrm{true}}(h) \\le \\mathrm{error}_{\\mathrm{train}}(h) + \\sqrt\\frac{VC(H) \\times \\ln\\left(\\frac{2m}{VC(H)} + 1\\right) + \\ln(4\u03b4)}m\n$$</p>\n\n<p>but how can I determine the size of training data ($m$) if I know the others?</p>\n', 'Tags': '<machine-learning><learning-theory><vc-dimension>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-26T13:09:12.277', 'CommentCount': '0', 'AcceptedAnswerId': '18383', 'CreationDate': '2013-11-23T14:40:08.877', 'Id': '18276'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question regarding optimal mistake bound for learning algorithm</p>\n\n<p>There is a famous fact that $VC(C) \\leq Opt(C)$, </p>\n\n<p>where $C$ - set of learning concepts,</p>\n\n<p><a href="http://en.wikipedia.org/wiki/VC_dimension" rel="nofollow">VC(C)</a> - VC dimension of C,</p>\n\n<p>$Opt(C)$ - the smallest mistake bound (of the best learning algorithm) on the hardest learning concept $c \\in C$ .</p>\n\n<p>I don\'t understand why $VC(C) \\leq Opt(C)$, in my opinion the notion of best algorithm $A$ is so vague that you cannot for sure say that $VC(C)$ is not more than $Opt(C)$</p>\n\n<p>For example,  $VC(line\\ on\\ the\\ plane)$=3 , it means that $Opt(C) \\geq 3$ in words it means for the hardest concept $c \\in C$ (represented as a line on the plane) the number of mistakes of the best learning algorithm is more than 3.</p>\n\n<p>Why the above fact is so strong?</p>\n', 'ViewCount': '77', 'Title': 'VC dimension and optimal mistake bound', 'LastActivityDate': '2013-12-19T19:49:50.850', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Tags': '<machine-learning><vc-dimension>', 'CreationDate': '2013-12-19T07:37:09.793', 'Id': '19113'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am new to statistical learning. I have a structure $X$ where I showed its hypothesis class $H$ has VC dimension $d$. All I know now is that I can bound the number of examples by $m\\geq \\frac{1}{\\epsilon}ln \\frac{d}{\\delta}$ and with probability at least $1-\\delta$ I will get a hypothesis with error at most $\\epsilon$. </p>\n\n<p>My question concerns what is usually the next step(s),with regard to the big picture of learning a structure $X$, after showing its VCD? </p>\n\n<p>I thought about studying other complexity measures for $X$ but wish to hear others suggestions. </p>\n', 'ViewCount': '90', 'Title': 'What is usually the next step after showing the VC dimension?', 'LastActivityDate': '2014-01-23T00:00:54.697', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<machine-learning><learning-theory><vc-dimension>', 'CreationDate': '2014-01-22T04:40:35.833', 'FavoriteCount': '1', 'Id': '19887'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $C\\subseteq 2^X$ be a concept class over $X$ and let $\\bar{C}:=\\{X\\setminus c\\mid c\\in C\\}$ be the complement. Show that $VCdim(C)=VCdim(\\bar{C})$.</p>\n\n<p>Proof:</p>\n\n<p>Let $d:=VC_{dim}(C)$, then there exists $S\\subseteq X$, $|S|=d$, s.t. $S$ is shattered by $C$.</p>\n\n<p>Let $d':=VC_{dim}(\\bar{C})$, then there exists $S'\\subseteq X$, $|S'|=d'$, s.t. $S'$ is shattered by $C$.</p>\n\n<p>Show that $d\\leq d'$ and $d' \\leq d$. I know that a set $S$ is shattered by $C$ iff $\\Pi_C(S):=\\{c\\cap S\\mid c\\in C\\}=2^S$, but I have no clue how to show the two sides. Can someone help me with that?</p>\n", 'ViewCount': '55', 'Title': 'VC dimension of complement', 'LastActivityDate': '2014-02-12T17:27:50.553', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '21572', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14600', 'Tags': '<machine-learning><vc-dimension>', 'CreationDate': '2014-02-12T08:52:04.493', 'Id': '21563'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We know VC dimension of 1-NN classifier is infinite for continuous metric space. Is there any proof of VC dimension of 1-NN classifier if the metric space is discrete?</p>\n', 'ViewCount': '7', 'Title': 'VC dimension of 1-NN classifier for discrete metric space?', 'LastActivityDate': '2014-02-13T03:59:37.423', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '215', 'Tags': '<vc-dimension>', 'CreationDate': '2014-02-13T03:59:37.423', 'Id': '21592'}},