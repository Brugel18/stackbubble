98_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Minimum bandwidth problem is to a find an ordering of graph nodes on integer line that minimizes the largest distance between any two adjacent nodes. </p>\n\n<p>The decision problem is NP-complete even for binary trees. <a href="http://www.jstor.org/stable/10.2307/2100947" rel="nofollow">Complexity Results for Bandwidth Minimization. Garey, Graham, Johnson and Knuth, SIAM J. Appl. Math., Vol. 34, No.3, 1978</a>.</p>\n\n<p>What is the best known efficient approximability result for computing minimum bandwidth on binary trees? What is best known conditional hardness of approximation result? </p>\n', 'ViewCount': '174', 'Title': 'Approximation of minimum bandwidth on binary trees', 'LastEditorUserId': '472', 'LastActivityDate': '2012-05-24T21:14:57.037', 'LastEditDate': '2012-04-02T11:58:13.790', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '988', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '96', 'Tags': '<complexity-theory><np-complete><reference-request><approximation>', 'CreationDate': '2012-03-15T14:56:56.453', 'Id': '416'},98_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '754', 'Title': 'Decision problems vs "real" problems that aren\'t yes-or-no', 'LastEditDate': '2012-04-02T21:58:54.973', 'AnswerCount': '3', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '157', 'FavoriteCount': '6', 'Body': '<p>I read in many places that some problems are difficult to approximate (it is  <a href="https://en.wikipedia.org/wiki/Hardness_of_approximation"><strong>NP-hard</strong> to approximate</a>  them). But approximation is not a decision problem: the answer is a real number and not Yes or No. Also for each desired approximation factor, there are many answers that are correct and many that are wrong, and this changes with the desired approximation factor!</p>\n\n<p>So how can one say that this problem is NP-hard?</p>\n\n<p><em>(inspired by the second bullet in <a href="http://cs.stackexchange.com/q/423/157">How hard is counting the number of simple paths between two nodes in a directed graph?</a>)</em></p>\n', 'Tags': '<complexity-theory><time-complexity><np-hard><approximation>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-29T16:31:13.863', 'CommentCount': '0', 'AcceptedAnswerId': '476', 'CreationDate': '2012-03-17T18:28:41.347', 'Id': '473'},98_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '201', 'Title': 'What is the name of this logistic variant of TSP?', 'LastEditDate': '2012-04-23T14:25:32.307', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '1', 'Body': '<p>I have a logistic problem that can be seen as a variant of $\\text{TSP}$. It is so natural, I\'m sure it has been studied in Operations research or something similar. Here\'s one way of looking at the problem.</p>\n\n<p>I have $P$ warehouses on the Cartesian plane. There\'s a path from a warehouse to every other warehouse and the distance metric used is the Euclidean distance. In addition, there are $n$ different items. Each item $1 \\leq i \\leq n$ can be present in any number of warehouses. We have a collector and we are given a starting point $s$ for it, say the origin $(0,0)$. The collector is given an order, so a list of items. Here, we can assume that the list only contains distinct items and only one of each. We must determine the shortest tour starting at $s$ visiting some number of warehouses so that the we pick up every item on the order.</p>\n\n<p>Here\'s a visualization of a randomly generated instance with $P = 35$. Warehouses are represented with circles. Red ones contain item $1$, blue ones item $2$ and green ones item $3$. Given some starting point $s$ and the order ($1,2,3$), we must pick one red, one blue and one green warehouse so the order can be completed. By accident, there are no multi-colored warehouses in this example so they all contain exactly one item. This particular instance is a case of <a href="http://en.wikipedia.org/wiki/Set_TSP_problem" rel="nofollow">set-TSP</a>.</p>\n\n<p><img src="http://i.stack.imgur.com/5kKsj.png" alt="An instance of the problem."></p>\n\n<p>I can show that the problem is indeed $\\mathcal{NP}$-hard. Consider an instance where each item $i$ is located in a different warehouse $P_i$. The order is such that it contains every item. Now we must visit every warehouse $P_i$ and find the shortest tour doing so. This is equivalent of solving an instance of $\\text{TSP}$.</p>\n\n<p>Being so obviously useful at least in the context of logistic, routing and planning, I\'m sure this has been studied before. I have two questions:</p>\n\n<ol>\n<li>What is the name of the problem?</li>\n<li>How well can one hope to approximate the problem (assuming $\\mathcal{P} \\neq \\mathcal{NP}$)? </li>\n</ol>\n\n<p>I\'m quite happy with the name and/or reference(s) to the problem. Maybe the answer to the second point follows easily or I can find out that myself.</p>\n', 'Tags': '<algorithms><optimization><reference-request><approximation>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-23T18:22:16.660', 'CommentCount': '4', 'AcceptedAnswerId': '1464', 'CreationDate': '2012-04-22T15:35:56.930', 'Id': '1440'},98_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '299', 'Title': 'Approximation algorithm for TSP variant, fixed start and end anywhere but starting point + multiple visits at each vertex ALLOWED', 'LastEditDate': '2012-04-28T09:30:21.653', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1282', 'FavoriteCount': '1', 'Body': '<p>NOTE: Due to the fact that the trip does not end at the same place it started and also the fact that every point can be visited more than once as long as I still visit all of them, this is not really a TSP variant, but I put it due to lack of a better definition of the problem.</p>\n\n<p>This problem was originally posted on StackOverflow, but I was told that this would be a better place. I got one pointer, which converted the problem from non-metric to a metric one.</p>\n\n<p>So..</p>\n\n<p>Suppose I am going on a hiking trip with n points of interest. These points are all connected by hiking trails. I have a map showing all trails with their distances, giving me a directed graph.</p>\n\n<p>My problem is how to approximate a tour that starts at a point A and visits all n points of interest, while ending the tour anywhere but the point where I started and I want the tour to be as short as possible.</p>\n\n<p>Due to the nature of hiking, I figured this would sadly not be a symmetric problem (or can I convert my asymmetric graph to a symmetric one?), since going from high to low altitude is obviously easier than the other way around.</p>\n\n<p>Since there are no restrictions regarding how many times I visit each point, as long as I visit all of them, it does not matter if the shortest path from a to d goes through b and c. Is this enough to say that triangle inequality holds and thus I have a metric problem?</p>\n\n<p>I believe my problem is easier than TSP, so those algorithms do not fit this problem. I thought about using a minimum spanning tree, but I have a hard time applying it to this problem, which under the circumstances, should be a metric asymmetric directed graph?</p>\n\n<p>What I really want are some pointers as to how I can come up with an approximation algorithm that will find a near optimal tour through all n points</p>\n', 'Tags': '<algorithms><complexity-theory><graphs><graph-theory><approximation>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-28T21:59:47.610', 'CommentCount': '5', 'AcceptedAnswerId': '1551', 'CreationDate': '2012-04-28T07:45:11.773', 'Id': '1542'},98_4:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say I have a weighted undirected complete graph $G = (V, E)$. Each edge $e = (u, v, w)$ is assigned with a positive weight $w$. I want to calculate the minimum-weighted $(d, h)$-tree-decomposition. By $(d, h)$-tree-decomposition, I mean to divide the vertices $V$ into $k$ trees, such that the height of each tree is $h$, and each non-leaf node has $d$ children. </p>\n\n<p>I know it is definitely $\\text{NP}$-Hard, since minimum $(1, |V|-1)$-tree-decomposition is the minimum Hamilton path. But are there any good approximation algorithms?</p>\n', 'ViewCount': '214', 'Title': 'Approximate minimum-weighted tree decomposition on complete graphs', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T11:59:46.367', 'LastEditDate': '2012-05-10T11:59:46.367', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '11', 'OwnerDisplayName': 'Geni', 'PostTypeId': '1', 'OwnerUserId': '1354', 'Tags': '<algorithms><complexity-theory><graphs><graph-theory><approximation>', 'CreationDate': '2012-05-02T21:38:35.253', 'Id': '1640'},98_5:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If the weights of the weighted 3-DIMENSIONAL-MATCHING problem are restricted to let\'s say, 1 and 2, is there a possibility to reduce this case to the unweighted 3-DIMENSIONAL-MATCHING problem?\n(Because for the unweighted version, there is a (1.5+$\\epsilon$)-approximation<sup>1</sup> algorithm, for the weighted version, there is only a 2-approx<sup>2,3</sup> algorithm)</p>\n\n<hr>\n\n<p>References:</p>\n\n<ol>\n<li><p><a href="http://www.nada.kth.se/~viggo/wwwcompendium/node275.html#HurSch89" rel="nofollow">unweighted ($1.5+\\epsilon$-approx)</a> </p></li>\n<li><p><a href="http://www.nada.kth.se/~viggo/wwwcompendium/node275.html#ArkHas97" rel="nofollow">weighted ($2+\\epsilon$-approx)</a> </p></li>\n<li><p><a href="http://www.cs.umd.edu/~yhchan/thesis.pdf" rel="nofollow">weighted ($2$-approx)</a> by CHAN, Yuk Hei, 2009</p></li>\n</ol>\n', 'ViewCount': '256', 'Title': 'Weighted Maximum 3-DIMENSIONAL-MATCHING with restricted weights (Approx Algo)', 'LastEditorUserId': '1464', 'LastActivityDate': '2012-05-14T19:27:33.897', 'LastEditDate': '2012-05-14T19:27:33.897', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1464', 'Tags': '<algorithms><approximation>', 'CreationDate': '2012-05-12T13:43:09.090', 'Id': '1806'},98_6:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Solving the <a href="https://en.wikipedia.org/wiki/Maximum_flow_problem" rel="nofollow">maximum flow problem</a> yields one qualified minimal cut. But I want several (maybe hundreds) small cuts as candidates. The cuts don\'t have to be minimum cuts, as long as they are small (in weight). How do I do that?</p>\n', 'ViewCount': '154', 'Title': 'In s-t directed graph, how to find many small cuts?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-02T17:30:29.300', 'LastEditDate': '2012-05-25T11:18:35.083', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1620', 'Tags': '<algorithms><graphs><graph-theory><optimization><approximation>', 'CreationDate': '2012-05-24T20:19:25.250', 'FavoriteCount': '2', 'Id': '2052'},98_7:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a run-time implementation question regarding the 3-dimensional (unweighted 2-)approximation algorithm below:\nHow can I construct the maximum matching M_r in S_r in linear time in line 8?</p>\n\n<p>$X, Y, Z $ are disjoint sets; a matching $M$ is a subset of $S$ s.t. no two triples in $M$ have the same coordinate at any dimension.</p>\n\n<p>$\n\\text{Algorithm: unweighted 3-dimensional matching (2-approximation)} \\\\\n\\text{Input: a set $S\\subseteq X \\times Y \\times Z$ of triples} \\\\\n\\text{Output: a matching M in S}\n$</p>\n\n<pre><code> 1) construct maximal matching M in S;  \n 2) change = TRUE;  \n 3) while (change) {  \n 4)   change = FALSE;  \n 5)   for each triple (a,b,c) in M {  \n 6)     M = M - {(a,b,c)};  \n 7)     let S_r be the set of triples in S not contradicting M;  \n 8)     construct a maximum matching M_r in S_r;  \n 9)     if (M_r contains more than one triple) {  \n10)       M = M \\cup M_r;  \n11)       change = TRUE;  \n12)     } else {  \n13)       M = M \\union {(a,b,c)};  \n14)     }  \n15) }  \n</code></pre>\n\n<hr>\n\n<p>[1] <a href="http://faculty.cse.tamu.edu/chen/courses/cpsc669/2011/notes/ch9.pdf" rel="nofollow">http://faculty.cse.tamu.edu/chen/courses/cpsc669/2011/notes/ch9.pdf</a>, p. 326</p>\n', 'ViewCount': '459', 'Title': '3-dimensional matching approximation algorithm (implementation details)', 'LastEditorUserId': '157', 'LastActivityDate': '2013-01-13T19:29:47.290', 'LastEditDate': '2013-01-13T19:29:47.290', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2574', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2037', 'Tags': '<algorithms><graphs><approximation><matching>', 'CreationDate': '2012-07-01T17:27:37.793', 'Id': '2571'},98_8:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m having a very hard time trying to figure out how to solve this problem efficiently. Let me describe how it goes:</p>\n\n<p>"A hard working mom bought several fruits with different nutritional values for her 3 kids, Amelia, Jessica and Bruno. Both girls are overweight, and they are very vicious and always leave poor Bruno with nothing, so their mother decided to share the food in the following manner:</p>\n\n<ul>\n<li><p>Amelia being the heaviest one gets the most amount of Nutritional Value.</p></li>\n<li><p>Jessica gets an amount equal or less than Amelia</p></li>\n<li><p>Bruno gets an amount equal or less than Jessica, but you need to find a way to give him the highest possible nutritional value while respecting the rule ( $A \\geq J \\geq B$ )"</p></li>\n</ul>\n\n<p>One of the test cases given by my teacher is the following:</p>\n\n<pre><code>The fruit list has the following values { 4, 2, 1, 8, 11, 5, 1\n\nInput:\n7   -----&gt; Number of Fruits\n4 2 1 8 11 5 1 ----&gt; Fruits Nutritional Values\n\nOutput:\n1 11  ----&gt;  One fruit, their nutritional values sum for Amelia\n5     ----&gt;  Position of the fruit in the list\n3 11  ----&gt;  Three fruits, their nutritional values sum for Jessica\n1 2 6 ----&gt;  Position of the fruits in the list\n3 10  ----&gt;  Three fruits, their nutritional values sum for Bruno\n3 4 7 ----&gt;  Position of the fruits in the list\n</code></pre>\n\n<p>Note: I am aware that there are several ways of diving the fruits among the kids, but it doesn\'t really matter as long as it follows the rule $A \\geq J \\geq B$.</p>\n\n<p>I\'m trying to make a program in C# that solves this kind of problems but I need an efficient formula to make this work. Generating all the subsets is out of the question because it is very consuming task. The list of fruits can have up to $50$ elements, $2^{50}$ is a huge number.</p>\n', 'ViewCount': '739', 'Title': 'Partition of a set of integer into 3 subsets of approximately equal sum', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-02T20:17:28.170', 'LastEditDate': '2012-07-02T19:42:50.433', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'Julian J. Tejera', 'PostTypeId': '1', 'Tags': '<algorithms><integers><approximation><linear-programming>', 'CreationDate': '2012-07-02T06:06:44.417', 'Id': '2580'},98_9:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>If $L$ is an APX-hard language, doesn't the existence of a PTAS for $L$ trivially imply $\\mathsf{P} = \\mathsf{NP}$?</p>\n\n<p>Since for example metric-TSP is in APX, but it is not approximable within 220/219 of OPT [1] unless $\\mathsf{P} = \\mathsf{NP}$. Thus if there was a PTAS for $L$ we could reduce metric-TSP using a PTAS reduction to $L$ and thus can approximate OPT within arbitrary precision.</p>\n\n<p>Is my argument correct?</p>\n\n<hr>\n\n<p>[1] Christos H. Papadimitriou and Santosh Vempala. On the approximability Of the traveling salesman problem. Combinatorica, 26(1):101\u2013120, Feb. 2006. </p>\n", 'ViewCount': '206', 'Title': '$L$ APX-hard thus PTAS for $L$ implies $\\mathsf{P} = \\mathsf{NP}$', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-22T17:52:57.157', 'LastEditDate': '2012-08-20T09:00:14.813', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3259', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2568', 'Tags': '<complexity-theory><np-complete><approximation>', 'CreationDate': '2012-08-17T21:15:08.610', 'Id': '3245'},98_10:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '212', 'Title': 'Approximating the Kolmogorov complexity', 'LastEditDate': '2013-12-10T11:06:26.913', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '702', 'FavoriteCount': '3', 'Body': '<p>I\'ve studied something about the <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">Kolmogorov Complexity</a>, read some articles and books from <a href="http://homepages.cwi.nl/~paulv/kolmogorov.html" rel="nofollow">Vitanyi and Li</a> and used the concept of <a href="http://complearn.org/ncd.html" rel="nofollow">Normalized Compression Distance</a> to verify the stilometry of authors (identify how each author writes some text and group documents by their similarity).</p>\n\n<p>In that case, data compressors were used to approximate the Kolmogorov complexity, since the data compressor could be used as a Turing Machine.</p>\n\n<p>Besides data compression and programming languages (in which you would write some kind of compressor), what else could be used to approximate the Kolmogorov complexity? Are there any other approaches that could be used?</p>\n', 'Tags': '<computability><approximation><data-compression><kolmogorov-complexity>', 'LastEditorUserId': '702', 'LastActivityDate': '2013-12-10T21:28:05.640', 'CommentCount': '4', 'AcceptedAnswerId': '3531', 'CreationDate': '2012-09-11T00:02:12.553', 'Id': '3501'},98_11:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's say I found  a 2-approximation algorithm for a certain problem and I want to show that the analysis is tight. </p>\n\n<p>Do I now need to come up with an example of generic size $n$ or does it suffice to show that I   have an example of size $10$ for which the algorithm yields $2OPT$?</p>\n", 'ViewCount': '398', 'Title': 'Providing Tight Example in Approximation Algorithm Analysis', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-18T18:33:07.623', 'LastEditDate': '2012-10-18T06:35:18.780', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4239', 'Tags': '<algorithms><proof-techniques><approximation>', 'CreationDate': '2012-10-17T23:07:10.890', 'FavoriteCount': '2', 'Id': '6140'},98_12:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><strong>Inputs.</strong> I am given a finite set $S$ of symbols.  I know there should exist some total order $&lt;$ on $S$, but I\'m not given this ordering and it could be anything.</p>\n\n<p>I am also given a collection of assertions.  Each assertion takes the form $s_1&lt;s_2&lt;\\cdots&lt;s_m$, where $s_1,\\dots,s_m$ form a subset of the symbols of $S$.  The assertion probably won\'t mention all of the symbols of $S$, just a subset.  Each assertion will probably cover a different subset.</p>\n\n<p><strong>Warmup problem.</strong> The starter problem is: Given $n$ assertions, identify whether they are all internally self-consistent, i.e., whether there exists a total order on $S$ that is consistent with all of the assertions, and if so, output an example of such a total order.</p>\n\n<p><strong>The real problem.</strong> In practice, a few assertions might be faulty.  Almost all of them should be correct, though.  So, the real problem is: if the assertions are not all internally self-consistent, find a minimal subset of assertions to label as "probably-erroneous", such that if you remove the probably-erroneous assertions, the remainder are all self-consistent.</p>\n\n<p><strong>What I know.</strong> I know how to solve the warmup problem (just compute the transitive closure of the union of the partial orders given by each assertion, and check that the result is antisymmetric; or, in other words, create a graph with $S$ as vertex set and an edge $s\\to t$ if $s&lt;t$ appears in any assertion, then check for cycles).  However, I don\'t know how to solve the real problem.  Any ideas?</p>\n\n<p><strong>Real-world parameters.</strong> In the application domain where I\'ve run into this, $S$ might have up to a few hundred symbols, and I might have up to a few thousand assertions, with each assertion typically mentioning dozens of symbols.</p>\n', 'ViewCount': '112', 'Title': 'Given many partial orders, check them for consistency and report any that are not consistent', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-19T22:21:54.297', 'LastEditDate': '2012-10-19T18:36:26.050', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><graphs><approximation><finite-sets><partial-order>', 'CreationDate': '2012-10-19T17:25:00.620', 'FavoriteCount': '1', 'Id': '6173'},98_13:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Intro:</p>\n\n<p>I'm working with huge data set that i need to plot in browser, and since there may be up to 1M points my idea was to create different representations for different zoom levels</p>\n\n<p>lets say i have 100k points, i would average two-by-two until i get 50k, then i would repeat that until i get below 500 points (my arbitrary threshold)</p>\n\n<p>so on the most zoomed-out level i would draw all 500 points, or part of it, depending of the chart size, and as i zoom in, i would switch to next zoom level (and stream data if user drags selection l/r), and ultimately if user wants to see fine grain details he can zoom to 0 zoom level and see all the fine details.</p>\n\n<p>I actually created this prototype, and its working quite well, except for one thing: side-effect of this is, as you can imagine, that peaks are lost in those iterations of averaging.</p>\n\n<p>I did some research and find about Douglas-Peucker algorithm, and how it can perserve peaks, i did some tests, and it works quite well, but the problem with that is that if it encounters a series of data (y values) [1,1,1,1,5,6,1,1,1,1,1,1] it will smooth that to something like [1,6,1,1] which doesn't work for me since i need to keep ratio of zoom levels like this</p>\n\n<p>n (length of original data) > n/2 > n/4 > n/8 > .....</p>\n\n<p>I read quite few papers on line smoothing, but all algorithms that i found are accepting distance threshold, that they use for smoothing as a parameter, and none of those can accept number of desired output elements, and also, since their goal is to smooth the line, they will transform sequence like this (y values) [1,1,1,1,1,1,1,1,1,1,1] into [1,1]</p>\n\n<p>So, finally, my question:</p>\n\n<p>Is there an algoritm that:</p>\n\n<ul>\n<li>instead of usual distance threshold accepts the desired number of output elements</li>\n<li>tries to perserve peaks (as Douglas-Peucker does)</li>\n<li>will smooth data uniformly, so even if it gets (y values) [1,1,1,1,1,1] and i say i want 3 outputs, event if it IS in theory correct to smooth as [1,1] i would need to get [1,1,1] instead</li>\n</ul>\n\n<p>Also, please don't be confused by lack of X axis information because it is irrelevant since all data are measured from 1 to n in steps of 1, so there are no N/A values, or blank spots, or values like [1.3,1.4,3]</p>\n\n<p>x is always [1,2,3....n]</p>\n", 'ViewCount': '366', 'Title': 'Line smoothing algorithm that perserve data uniformity', 'LastEditorUserId': '4366', 'LastActivityDate': '2012-10-30T18:22:35.023', 'LastEditDate': '2012-10-26T14:23:52.147', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4366', 'Tags': '<computational-geometry><approximation><graphics>', 'CreationDate': '2012-10-26T04:04:35.900', 'FavoriteCount': '2', 'Id': '6321'},98_14:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a container with a certain dimension. A number of small boxes that may be different in size is to be packed into the container. How to arrange the small boxes such that the container contains as many as possible?</p>\n\n<ul>\n<li>No rotation is allowed.</li>\n<li>The heavier boxes must not be on the top of the lighter ones.</li>\n<li>Approximation is allowed.</li>\n</ul>\n\n<p>I am looking for the algorithm so I can implement it in a software.</p>\n', 'ViewCount': '213', 'Title': 'Algorithm to pack any small boxes into a big box', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:25:11.543', 'LastEditDate': '2013-05-24T03:25:11.543', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4542', 'Tags': '<algorithms><combinatorics><efficiency><approximation><knapsack-problems>', 'CreationDate': '2012-11-10T19:18:01.317', 'Id': '6606'},98_15:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '164', 'Title': 'Randomized Rounding of Solutions to Linear Programs', 'LastEditDate': '2012-11-30T04:10:17.983', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/Linear_programming#Integral_linear_programs" rel="nofollow">Integer linear programming</a> (ILP) is an incredibly powerful tool in combinatorial optimization. If we can formulate some problem as an instance of an ILP then solvers are guaranteed to find the global optimum. However, enforcing integral solutions has runtime that is exponential in the worst case. To cope with this barrier, several approximation methods related to ILPs can be used,</p>\n\n<ul>\n<li>Primal-Dual Schema</li>\n<li>Randomized Rounding</li>\n</ul>\n\n<p>The Primal-Dual Schema is a versatile method that gives us a "packaged" way to come up with a greedy algorithm and prove its approximation bounds using the relaxed dual LP. Resulting combinatorial algorithms tend to be very fast and perform quite well in practice. However its relation to linear programming is closer tied to the analysis. Further because of this analysis, we can easily show that constraints are not violated.</p>\n\n<p>Randomized rounding takes a different approach and solves the relaxed LP (using interior-point or ellipsoid methods) and rounds variables according to some probability distribution. If approximation bounds can be proven this method, like the Primal-Dual schema, is quite useful. However, one portion is not quite clear to me:</p>\n\n<blockquote>\n  <p>How do randomized rounding schemes show that constraints are not violated?</p>\n</blockquote>\n\n<p>It would appear that naively flipping a coin, while resulting in a 0-1 solution, could violate constraints! Any help illuminating this issue would be appreciated. Thank you.</p>\n', 'Tags': '<optimization><randomized-algorithms><linear-programming><approximation>', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-30T04:10:17.983', 'CommentCount': '4', 'AcceptedAnswerId': '6949', 'CreationDate': '2012-11-27T05:05:34.050', 'Id': '6941'},98_16:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If an optimization problem is known to be inapproximable up to some precision, does this automatically imply that the problem is apx-hard?</p>\n', 'ViewCount': '103', 'Title': 'Inapproximability result implies apx-hardness?', 'LastActivityDate': '2012-11-30T00:08:10.550', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7030', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4532', 'Tags': '<complexity-theory><approximation>', 'CreationDate': '2012-11-29T23:12:55.417', 'Id': '7029'},98_17:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the multiway cut problem, the input is an undirected graph $G= (V, E)$ and set of terminal nodes $s_1, s_2,\\ldots s_k$ are in $V$. The goal is to find a minimum\nset of edges in $E$ whose removal leaves all terminals in different components.</p>\n\n<ol>\n<li><p>How do we show that this problem can be solved exactly in polynomial time when\n$k= 2$?</p></li>\n<li><p>How do we get an approximation algorithm with ratio at most 2 for the case when $k \\geq 3$?</p></li>\n</ol>\n', 'ViewCount': '353', 'Title': '2OPT Approximation Algorithm for Multiway Cut Problem', 'LastEditorUserId': '19', 'LastActivityDate': '2013-04-02T15:47:46.743', 'LastEditDate': '2013-04-02T14:17:26.360', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'lam lae', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><approximation><network-flow>', 'CreationDate': '2012-12-02T22:34:14.450', 'Id': '7205'},98_18:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For which values $A,B$ is the problem $\\mathsf{gap\\mathord-VC}\\mathord-[A,B]$ NP-hard? VC is the <a href="http://en.wikipedia.org/wiki/Vertex_cover" rel="nofollow">vertex cover</a> problem. I am given three options: $B=\\frac{3}{4},A=\\frac{1}{2}$ or  $B=\\frac{3}{4},A=\\frac{1}{4}$ or none.</p>\n\n<p>I would to review what I think that I need to do, I\'m not sure that the way I think of it is correct. This is what I think: I need to decide if it NP-hard to approximate the VC to $\\frac{1}{2}$, i.e., can I build an NP Turing machine that would return Yes iff for a given graph, it can guarantee that it has less than $\\frac{1}{4}V$ vertices that cover the whole graph? Maybe even for $\\frac{1}{2}V$ vertices? </p>\n\n<p><sub> This is a question from a past midterm that I\'m solving now in order to prepare myself for my own midterm in a "Computational Complexity Theory" course. </sub></p>\n', 'ViewCount': '67', 'Title': 'For what values of A and B is the gap-VC-[A,B] problem NP-HARD?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-12T20:39:47.017', 'LastEditDate': '2012-12-11T21:29:00.627', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7365', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<complexity-theory><graph-theory><approximation>', 'CreationDate': '2012-12-11T19:46:56.387', 'Id': '7332'},98_19:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><strong>Facts:</strong> n points in the plane, each has one of k colors, all k colors are represented.</p>\n\n<p><strong>Problem:</strong> You wish to select k points, one of each color, such that the perimeter of the convex hull is as small as possible.</p>\n\n<p><strong>Greedy algorithm:</strong> For each point p, for each color c not equal to p, select the point of color c closest to p. In the end, choose the point set that has a convex hull with the smallest diameter (diameter is the distance between the two points furthest apart.)</p>\n\n<p>Why is the approximation ratio $\\pi/2$?</p>\n\n<p>This was an exercise on my graduate level algorithms exam. We were only given a few lines to answer so it should be simple enough, but I do not know where to start.</p>\n', 'ViewCount': '61', 'Title': 'Show that approximation ratio for a convex hull algorithm is $\\pi/2$', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-12-11T20:16:15.093', 'LastEditDate': '2012-12-11T20:05:41.483', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><approximation><greedy-algorithms>', 'CreationDate': '2012-12-11T19:59:06.100', 'Id': '7333'},98_20:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let $L_\\epsilon$ be the language of all $2$-CNF formulas $\\varphi$, such that at least $(\\frac{1}{2}+\\epsilon)$ of $\\varphi$'s clauses can be satisfied.</p>\n\n<p>I need to prove that there exists $\\epsilon'$ s.t $L_\\epsilon$ is $\\mathsf{NP}$-hard for any $\\epsilon&lt;\\epsilon'$.</p>\n\n<p>We know that $\\text{Max}2\\text{Sat}$ can be approximate to $\\frac{55}{56}$ precent of the clauses from a $\\text{Max}3\\text{Sat}$ reduction. How should I solve this one?</p>\n", 'ViewCount': '124', 'Title': "Find $\\epsilon'$ s.t $L_\\epsilon$ is $\\mathsf{NP}$-hard for any $\\epsilon<\\epsilon'$", 'LastEditorUserId': '41', 'LastActivityDate': '2012-12-23T09:24:26.913', 'LastEditDate': '2012-12-23T09:24:26.913', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7528', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2329', 'Tags': '<complexity-theory><satisfiability><approximation>', 'CreationDate': '2012-12-20T13:11:14.170', 'Id': '7523'},98_21:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to understand the approximation ratio for the <a href="http://mor.journal.informs.org/content/25/4/645.full.pdf" rel="nofollow">Kenyon-Remila</a> algorithm for the 2D cutting stock problem.</p>\n\n<p>The ratio in question is $(1 + \\varepsilon) \\text{Opt}(L) + O(1/\\varepsilon^2)$.</p>\n\n<p>The first term is clear, but the second doesn\'t mean anything to me and I can\'t seem to figure it out.</p>\n', 'ViewCount': '235', 'Title': 'What does big O mean as a term of an approximation ratio?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-08T06:24:22.580', 'LastEditDate': '2013-01-08T06:24:22.580', 'AnswerCount': '5', 'CommentCount': '0', 'AcceptedAnswerId': '7722', 'Score': '2', 'OwnerDisplayName': 'Jacob Fogner', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><approximation>', 'CreationDate': '2013-01-01T23:33:43.330', 'Id': '7720'},98_22:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have been posed with a question whereby a computer truncates numbers to x number of digits. Due to this, if this computer is trying to store a decimal number which has a binary equivalent greater than x, it truncates the remaining digits producing a different binary number. However, this binary number is still an 'approximation' of what it should be, but of course is the equivalent of a different decimal number (close to what we were trying to store initially). </p>\n\n<p>What problems can occur due to this incorrect storage of data?</p>\n", 'ViewCount': '111', 'Title': 'Implications of truncation of numbers when converted into binary', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-28T00:18:41.680', 'LastEditDate': '2013-01-28T00:18:41.680', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6571', 'Tags': '<approximation><numerical-analysis><floating-point>', 'CreationDate': '2013-01-26T19:54:34.997', 'Id': '9177'},98_23:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '90', 'Title': 'Hardness of Approximating 0-1 Integer Programs', 'LastEditDate': '2013-02-18T05:51:25.817', 'AnswerCount': '1', 'Score': '8', 'OwnerDisplayName': 'Jonas Anderson', 'PostTypeId': '1', 'OwnerUserId': '1439', 'Body': '<p>Given a $0,1$ (binary) integer program of the form:\n$$\n\\begin{array}{lll}\n\\text{min} &amp; f(x) &amp; \\\\\n\\text{s.t.} &amp;A\\vec{x} = \\vec{b} &amp; \\quad \\forall i\\\\\n &amp;x_i\\ge 0 &amp; \\quad \\forall i\\\\\n&amp;x_i \\in \\{0,1\\} &amp; \\quad \\forall i\n\\end{array}\n$$</p>\n\n<p>Note: the size of $A$ is not fixed in either dimension.</p>\n\n<p>I believe this problem has been shown to be hard to approximate (strongly ${\\sf NP}$-Complete) <a href="http://dl.acm.org/citation.cfm?id=322090">Garey &amp; Johnson</a>. </p>\n\n<p>If so, is this still the case when $A$, $\\vec{b}$ have binary entries and $f(x)$ is a linear function ( $f(x) = \\sum_i c_i x_i$ )?</p>\n', 'Tags': '<complexity-theory><np-complete><approximation><integer-programming>', 'LastEditorUserId': '683', 'LastActivityDate': '2013-02-18T05:51:25.817', 'CommentCount': '2', 'AcceptedAnswerId': '9887', 'CreationDate': '2013-02-14T01:13:24.667', 'Id': '9810'},98_24:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>i have a problem regarding the following situation. </p>\n\n<p>I have two arrays of numbers like this:</p>\n\n<pre><code>index/pos     0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 \nArray 1(i):   1   2   3   4   7   5   4   3   7   6   5   1   2   3   4   2\nArray 2(j):   4   4   8  10  10   7   7  10  10  11   7   4   7   7   4\n</code></pre>\n\n<p>now suppose the second array is very hard to compute but I have noticed that if I add  </p>\n\n<p>A[i] + A[i+1]</p>\n\n<p>in the array 1 I get the number very close to the number A[j] in the array 2. </p>\n\n<ol>\n<li><p>Is my solution a heuristic or approximation?</p></li>\n<li><p>If I had a reason to believe that I will never overshoot the value of A[j] by +-x with this algorithm and can prove it, would then  my solution be a heuristic or approximation?</p></li>\n</ol>\n\n<p>Is there any literature that deals with heuristic vs. approximation questions for P class problems where the solution can be achieved in polynomial time but the input is just too big for a poly time algorithm to be practical.</p>\n\n<p>thank you </p>\n', 'ViewCount': '960', 'Title': 'Difference between heuristic and approximation algorithm?', 'LastActivityDate': '2013-03-01T23:54:21.747', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6697', 'Tags': '<approximation><heuristics>', 'CreationDate': '2013-03-01T18:37:47.320', 'FavoriteCount': '1', 'Id': '10182'},98_25:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am currently confused by the following situation: </p>\n\n<p>1) The metric $k$-center problem is inapproximable in polynomial time within $2-\\epsilon$ unless $P=NP$. <br>\n2) The metric $k$-center problem can approximated within $1+\\epsilon$ in time $O(k^{O(k/ \\epsilon)})$</p>\n\n<p>Did I just win a million dollars or why isn\'t this a contradiction?\nI guess my confusion comes from the unprecise statement  "in polynomial time" in 1).</p>\n', 'ViewCount': '151', 'Title': '$1+\\epsilon$ approximation for inapproximable problems', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-05-10T12:15:42.897', 'LastEditDate': '2013-05-10T12:15:42.897', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '10302', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7159', 'Tags': '<complexity-theory><np-complete><approximation><p-vs-np><parametrized-complexity>', 'CreationDate': '2013-03-05T19:27:39.220', 'Id': '10300'},98_26:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I have to implement a limitation algorithm in order to avoid to reach a throughput limit imposed by the service I'm interacting with.</p>\n\n<p>The limit is specified as \xabN request over 1 day\xbb where N is of the order of magnitude of 10^6.</p>\n\n<p>I have a distributed system of clients interacting with the service so they should share the measure.</p>\n\n<p>An exact solution should involve to record all the events and than computing the limit \xabwhen\xbb the event of calling the service occur: of course this approach is too expensive and so I'm looking for an approximate solution.</p>\n\n<p>The first one I devised imply to discretize the detection of the events: for example maintaing 24 counters at most and recording the number of requests occurred within an hour.</p>\n\n<p>Acceptable.</p>\n\n<p>But I feel that a more elegant, even if leaded by different \xabforces\xbb, is to declinate the approach to the continuum.</p>\n\n<p>Let's say recording the last N events I could easily infer the \xabcurrent\xbb throughput. Of course this algorithm suffer for missing consideration of the past events occurred the hours before. I could improve with with an aging algorithm but\u2026 and here follow my question:</p>\n\n<p>Q: \xabThere's an elegant approximate solution to the problem of estimating the throughput of a service?\xbb</p>\n", 'ViewCount': '26', 'Title': 'Throughput measure', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T11:58:18.983', 'LastEditDate': '2013-03-28T11:58:18.983', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7177', 'Tags': '<optimization><distributed-systems><approximation>', 'CreationDate': '2013-03-28T10:33:22.480', 'FavoriteCount': '1', 'Id': '10864'},98_27:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '85', 'Title': 'Prize collecting steiner tree', 'LastEditDate': '2013-04-07T11:49:54.243', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Armin Meisterhirn', 'PostTypeId': '1', 'OwnerUserId': '7866', 'Body': '<p>I\'m reading about the <strong>prize collecting steiner tree</strong> problem and an approximation algorithm that uses randomization to set a lower bound on the optimal solution (see Chapter 5.7 in <a href="http://www.designofapproxalgs.com/book.pdf" rel="nofollow"> The Design of Approximation Algorithms </a> by Williamson and Shmoys). I don\'t understand the second line in the proof for Lemma 5.16: <img src="http://i.stack.imgur.com/9uldl.png" alt="Lemma 5.16">.</p>\n\n<p>It seems to me that $V-V(T)$ is a much larger set than $U$. So, how can the total penalty for this set be upper bounded by the total penalty of a set that is much smaller?</p>\n', 'Tags': '<algorithm-analysis><optimization><approximation><trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T11:49:54.243', 'CommentCount': '0', 'AcceptedAnswerId': '11070', 'CreationDate': '2013-04-04T00:13:44.200', 'Id': '11069'},98_28:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '216', 'Title': 'Average length of s-t (simple) paths in a directed graph', 'LastEditDate': '2013-04-08T19:10:43.587', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '7644', 'FavoriteCount': '3', 'Body': '<p>Given the fact that $s$-$t$ path enumeration is a #P-complete problem, could there be efficient methods that compute (or at least approximate) the average length of $s$-$t$ path without enumerating them? <strike>What if paths are allowed to revisit vertices?</strike> </p>\n\n<p>Relevant results on special graphs could also be helpful.</p>\n', 'Tags': '<algorithms><complexity-theory><graphs><approximation><enumeration>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T04:41:12.817', 'CommentCount': '5', 'AcceptedAnswerId': '11184', 'CreationDate': '2013-04-08T18:28:42.923', 'Id': '11146'},98_29:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given an undirected graph $G=(V,E)$ could we build a tree $T$ that approximates the distances from given vertex $r$ and the total weight, i.e. $\\forall x \\in V, d_G(r,x) \\le d_T(r,x) \\le 3 \\cdot d_G(r,x)$ and $w(T) \\le 3\\cdot w(\\text{MST}(G))$, where $\\text{MST}$ is the minimum spanning tree and $w(\\cdot)$ is the weight function i.e. $w:\\Bbb E \\to \\Bbb R^+$. $d_G(v,u)$ denotes the shortest path distance between $v$ and $u$ in $G$, and $d_T(v,u)$ is the shortest path distance between $v$ and $u$ in $T$.</p>\n\n<p>Could any one help me to understand how to build this tree and if there is any material that would help?</p>\n', 'ViewCount': '85', 'Title': 'Finding a tree that approximates the distances and total weights', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-28T14:06:38.913', 'LastEditDate': '2013-04-28T14:06:38.913', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '11617', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><approximation>', 'CreationDate': '2013-04-27T18:36:55.457', 'Id': '11607'},98_30:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I was working on proving this one and I've solve one direction as follows :<br>\nto prove that $P \\subseteq PCP(0,logn)$ I said :<br>\nlet $M$ be deterministic polynomial TM that accepts $L \\in P$ ,we want to show that we can there exists a proof system which consists of prover and deterministic polynomial verifier that have access to an oracle and make at most $O(log(n))$ queries and $\\forall x \\in L$ $\\exists \\pi _x $suchthat the verifier always(i.e. probability =1) accepts and $\\forall x \\notin L$ always reject what ever the proof was.<br>\nthe prover and the verifier will be the machine $M$ (since no randomness required) and $\\pi _x$ will be: run M and obtain solution denoted by : $ANS$ send it to the verifier then the verifier checks if $ANS$ is the same and rejects or accepts accordingly ... this proves that: $P \\subseteq PCP(0,O(1)) \\subseteq PCP(0,O(log(n)) $ </p>\n", 'ViewCount': '135', 'Title': 'proving $P \\subseteq PCP(0,O(log(n))$', 'LastActivityDate': '2013-05-01T20:31:12.117', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11691', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<complexity-theory><approximation>', 'CreationDate': '2013-05-01T00:18:29.293', 'Id': '11683'},98_31:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to reduce $MAX3SAT$ to $MAX2SAT$ ...<br>\nMAX-n-SAT : given $\\phi $ n-CNF formula and number k does $\\phi$ has an assignment that satisfy k clauses? </p>\n', 'ViewCount': '62', 'Title': 'reducing Max3SAT to Max2sat', 'LastActivityDate': '2013-05-02T21:51:27.307', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11742', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><np-complete><reductions><approximation>', 'CreationDate': '2013-05-02T20:59:20.690', 'Id': '11739'},98_32:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>If we have polynomial algorithm that $c$-approximation, $c&lt;\\frac{4}{3}$ for graphs that their chromatic number $\\geq k$ then $NP=P$, how to prove such statements?</p>\n\n<p>I also have some sort of explanation of this statement: It's NP-hard to separate between graphs that have chromatic number $k$ and chromatic number $c \\cdot k$ when $c&lt;\\frac{4}{3} \\quad \\forall k\\geq 3$ </p>\n", 'ViewCount': '80', 'Title': 'Hardness of approximation of the 3 colorability problem', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-09T01:58:07.277', 'LastEditDate': '2013-05-09T01:58:07.277', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11769', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><np-hard><approximation>', 'CreationDate': '2013-05-03T22:19:10.063', 'Id': '11766'},98_33:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am looking for the paper : "B. Awerbuch, A. Baratz, and D. Peleg, E\ufb03cient broadcast and light-weight spanners, Manuscript, (1991)."<br>\nIt claims that we can build $(\\alpha ,1+\\frac{4}{\\alpha -1})-LAST$ where LAST hold for "Light Approximation Shortest path Tree"<br>\nif not available could any one explain the algorithms used there ?  </p>\n', 'ViewCount': '50', 'Title': 'Light approximation for shortest path tree', 'LastEditorUserId': '139', 'LastActivityDate': '2013-05-07T07:25:47.910', 'LastEditDate': '2013-05-07T07:25:47.910', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><reference-request><approximation>', 'CreationDate': '2013-05-04T22:40:46.600', 'Id': '11786'},98_34:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let $N(G)$ be the null graph. What's the number of vertex cover for this graph? I wanted to modify the reduction from SAT to vertex cover by adding vertices that are not connect to any vertices.</p>\n", 'ViewCount': '118', 'Title': "What's the vertex cover of the null graph?", 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-08T19:06:28.147', 'LastEditDate': '2013-05-08T17:50:41.690', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11873', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><approximation>', 'CreationDate': '2013-05-07T21:28:32.327', 'Id': '11865'},98_35:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given $n$ points in $\\mathbf{R}^2$, define the optimal Euclidean Steiner tree to be a minimum (Euclidean) length tree containing all $n$ points and any other subset of points from $\\mathbf{R}^2$.\nProve that each of the additional points must have degree 3, with all three angles being $120^\\circ$.</p>\n', 'ViewCount': '113', 'Title': 'Euclidean Steiner Tree Question in Approximation Algorithms', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-05-08T07:18:28.980', 'LastEditDate': '2013-05-08T07:18:28.980', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11881', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7300', 'Tags': '<algorithms><algorithm-analysis><computational-geometry><approximation><trees>', 'CreationDate': '2013-05-08T06:34:08.107', 'Id': '11880'},98_36:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a question about the structure of the complexity class $APX$. Obviously, unless $P=NP$, no problem in the class $PTAS$ can be $APX$-complete (under the AP-reduction). However, what about the rest of problems in $APX$? Are there any problems known that are in $APX$, do not have a $PTAS$ (unless $P=NP$) and at the same time are provably not $APX$-complete (unless $P=NP$)?</p>\n\n<p>For the class $NP$, Ladner's Theorem guarantees the existence of problems in $NP - P$ that are not $NP$-complete (unless $P=NP$) - the so-called $NP$-intermediate problems. I am curious if any similar result has been proved for $APX - PTAS$ with respect to approximation preserving reductions.</p>\n\n<p>It is possible that the answer to this question is trivial - to be honest, the only $APX$-complete problem I know is MAX-3-SAT. However, I wonder how hard it is with respect to other problems in $APX - PTAS$.</p>\n", 'ViewCount': '92', 'Title': 'Are there any problems in $APX - PTAS$ that are not $APX$-complete?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T15:01:57.763', 'LastEditDate': '2013-05-19T15:01:57.763', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12132', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2091', 'Tags': '<complexity-theory><approximation><complexity-classes>', 'CreationDate': '2013-05-18T13:53:24.377', 'Id': '12112'},98_37:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Starting to use <a href="https://code.google.com/p/nanoflann/" rel="nofollow">nanoflann</a> to do some point cloud nearest neighbor searching and it got me thinking about just how "approximate" ANN methods are.</p>\n\n<p>If I have a (more or less) randomly distributed point cloud what is the likelihood that I get the exact nearest neighbor given a target point within the clouds bounding box?  I know that it is dataset dependent... but does anyone have a good numerical study somewhere that shows trends?</p>\n', 'ViewCount': '94', 'Title': 'How approximate are "approximate" nearest neighbor (ANN) search algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T17:44:30.770', 'LastEditDate': '2013-05-28T06:46:45.047', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8395', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><approximation><heuristics>', 'CreationDate': '2013-05-27T20:13:40.783', 'FavoriteCount': '1', 'Id': '12310'},98_38:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>This is my first question, so please, be soft on me.</p>\n\n<p>I have a following problem:</p>\n\n<ul>\n<li>I'm a programmer not a mathematician, I don't often understand pure mathematical language and marks or symbols, I need to write a program using Java/C++</li>\n<li>I have a histogram (table[vector] of size 255 filled with integers)</li>\n<li>I\u2019ve got to write an algorithm that will approximate it</li>\n<li><p>I just need to find a proper polynomial : let's assume that degree is given (for example 4), my job is to find best fitted adverbials for every degree</p>\n\n<p>Can anyone help me to write it or send links to a proper step-by-step algorithm?</p>\n\n<p>I am aware it's not an easy task, any help would be appreciated.</p></li>\n</ul>\n", 'ViewCount': '69', 'Title': 'Aproximation algorithm for histogram', 'LastActivityDate': '2013-05-31T11:17:08.347', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8461', 'Tags': '<algorithms><approximation><image-processing>', 'CreationDate': '2013-05-31T11:17:08.347', 'Id': '12395'},98_39:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In Graph-Theory there are many ways for efficient approximation-algorithms to solve the Metric TSP. The best solution seems to be the Christofides Heuristic with a factor of 1.5 to the optimal solution. My Teacher said, there would be the so called $\\frac{4}{3}$-conjecture, which states: there might be a approximation solution for the metric tsp, that has only a $\\frac{4}{3}$-factor.</p>\n\n<p>But i cannot find any literature or further information about this assumption. Maybe you can?</p>\n', 'ViewCount': '54', 'Title': 'Where can i find literature about the $\\frac{4}{3}$-conjecture for approximation of the Metric TSP?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-10T14:49:29.627', 'LastEditDate': '2013-06-10T14:49:29.627', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12594', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6828', 'Tags': '<complexity-theory><reference-request><np-complete><approximation><traveling-salesman>', 'CreationDate': '2013-06-10T10:33:26.633', 'Id': '12593'},98_40:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '406', 'Title': 'About metric TSP instances', 'LastEditDate': '2013-07-03T21:59:48.323', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8997', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/Christofides_algorithm" rel="nofollow">Christofides\' 1.5-approximation</a> considers complete graphs as inputs, and as I understand this is essential. If the input graph is not complete, how can I add new edges with suitable weights such that the resulting complete graph still satisfies the triangle inequality, and, of course, the TSP solution for the complete graph only uses original edges? Thank you.  </p>\n', 'Tags': '<graphs><approximation><traveling-salesman>', 'LastEditorUserId': '8997', 'LastActivityDate': '2013-07-04T00:20:24.543', 'CommentCount': '0', 'AcceptedAnswerId': '13077', 'CreationDate': '2013-07-03T21:27:28.363', 'Id': '13070'},98_41:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider we have a finite set $S$ with $n$ distinct elements. We want to find a subset $\\{a_1, a_2, \\dotsc, a_k\\}\\subseteq S$ ($k\\ll n$) such that a function $f(a_1,a_2,\\dotsc,a_k)$ is maximized. Consider $f$ to be a <a href="http://en.wikipedia.org/wiki/Symmetric_function" rel="nofollow">symmetric function</a> that takes $k$ arguments.</p>\n\n<hr>\n\n<p>More specifically, we are given $n = 120$ items, each item being associated with three positive numbers $(A_i, B_i,C_i)$, and we want to choose $k=12$ items within this set such that</p>\n\n<p>$$ \\frac{\\sum_{k=1}^{12} A_{i_k} \\times \\left\\lceil\\frac{\\sum_{k=1}^{12} B_{i_k}}{10000}\\right\\rceil}{\\sum_{k=1}^{12} C_{i_k}} $$</p>\n\n<p>is maximal.</p>\n\n<hr>\n\n<p>If we solve it by exhaustive search it requires $\\binom{120}{12} \\approx 10^{16}$ operations. Is there faster method to this problem? Approximate solution is also fine.</p>\n', 'ViewCount': '107', 'Title': 'Subset optimization problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-07T07:40:27.017', 'LastEditDate': '2013-08-07T07:40:27.017', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9021', 'Tags': '<algorithms><optimization><approximation>', 'CreationDate': '2013-07-04T15:56:53.990', 'Id': '13088'},98_42:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>During my involvement in a course on dealing with NP-hard problems I have encountered the PCP theorem, stating</p>\n\n<p>$\\qquad\\displaystyle \\mathsf{NP} = \\mathsf{PCP}(\\log n, 1)$. </p>\n\n<p>I understand the technical definition of a PCP verifier, so I know in principle what kind of algorithm has to exist for every NP problem: a randomised algorithm that checks $O(1)$ bits of the given certificate for the given input using $O(\\log n)$ random bits, so that this algorithm is essentially a one-sided error Monte-Carlo verifier.</p>\n\n<p>However, I have trouble imagining how such an algorithm can deal with an NP-complete problem. Short of reading the proof of the PCP theorem, are there concrete examples for such algorithms?</p>\n\n<p>I skimmed the relevant sections of <a href="http://www.cs.princeton.edu/theory/complexity/" rel="nofollow">Computational Complexity: A Modern Approach</a> by Arora and Barak (2009) but did not find any.</p>\n\n<p>An example using a $\\mathsf{PCP}(\\_,\\ll n)$ algorithm would be fine.</p>\n', 'ViewCount': '220', 'Title': 'Example for a non-trivial PCP verifier for an NP-complete problem', 'LastActivityDate': '2013-07-19T09:24:04.750', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><complexity-theory><np-complete><approximation><randomized-algorithms>', 'CreationDate': '2013-07-12T11:10:36.380', 'Id': '13246'},98_43:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '162', 'LastEditorDisplayName': 'user742', 'Title': 'Why are all problems in FPTAS also in FPT?', 'LastEditDate': '2013-09-08T10:00:18.023', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2131', 'FavoriteCount': '1', 'Body': '<p>According to <a href="https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme" rel="nofollow">the Wikipedia article on polynomial-time approximation schemes</a>:</p>\n\n<blockquote>\n  <p>All problems in FPTAS are fixed-parameter tractable.</p>\n</blockquote>\n\n<p>This result surprises me - these classes seem to be totally different from one another.  FPTAS characterizes problems by how easy they are to approximate, while FPT characterizes problems by their difficulty relative to some parameter.  Unfortunately, Wikipedia (as of the time I\'m asking this question) doesn\'t provide a citation for this.</p>\n\n<p>Is there a standard proof of this result?  Or is there a source I could consult to learn more about this connection?</p>\n', 'Tags': '<complexity-theory><reference-request><approximation><parametrized-complexity>', 'LastActivityDate': '2013-09-08T10:00:18.023', 'CommentCount': '5', 'AcceptedAnswerId': '13681', 'CreationDate': '2013-08-08T21:07:15.470', 'Id': '13679'},98_44:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the <em>edge-disjoint paths problem</em> (EDP), we are given a (possibly directed) graph $G=(V,E)$, and a set of distinct source-sink pairs $\\{ (s_i,t_i) \\mid 1 \\leq i \\leq k \\}$, and we want to maximize the number of pairs that can be simultaneously connected in an edge-disjoint manner. When we add the constraint that the paths need to be <em>shortest paths</em>, we get the <em>edge-disjoint shortest paths</em> (EDSP) problem.</p>\n\n<p>According to [1], the EDSP problem is hard for a graph with unit edge lengths, even when the graph is planar. Furthermore, it claims this is so for both directed and undirected graphs.</p>\n\n<blockquote>\n  <p>What is known about the approximability of the EDSP problem?</p>\n</blockquote>\n\n<p>I\'m especially interested in results for undirected graphs. In [2], the authors seem to only consider variants of the EDP problem, but not the EDSP problem. Further following the references, it seems like the EDP problem has been studied extensively.</p>\n\n<hr>\n\n<p>[1] <a href="http://www.sciencedirect.com/science/article/pii/S0166218X97001212" rel="nofollow">Eilam-Tzoreff, Tali. "The disjoint shortest paths problem." Discrete applied mathematics 85.2 (1998): 113-138.</a></p>\n\n<p>[2] <a href="http://www.sciencedirect.com/science/article/pii/S0022000003000667" rel="nofollow">Guruswami, Venkatesan, et al. "Near-optimal hardness results and approximation algorithms for edge-disjoint paths and related problems." Journal of Computer and System Sciences 67.3 (2003): 473-496.</a></p>\n', 'ViewCount': '131', 'Title': 'Approximability of the edge-disjoint shortest paths problem', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-01T11:44:40.417', 'LastEditDate': '2013-09-01T11:44:40.417', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><reference-request><shortest-path><approximation>', 'CreationDate': '2013-08-29T16:50:41.277', 'Id': '14023'},98_45:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What is the difference between approximation schemes and approximation algorithms?<br>\nWhy do we study approximation schemes?</p>\n', 'ViewCount': '121', 'Title': 'Difference between approximation scheme and approximation algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-06T16:07:46.940', 'LastEditDate': '2013-09-06T16:07:46.940', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '10000', 'Tags': '<algorithms><terminology><approximation>', 'CreationDate': '2013-09-06T02:45:46.770', 'Id': '14162'},98_46:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '114', 'LastEditorDisplayName': 'user742', 'Title': 'Approximation algorithm for Feedback Arc Set', 'LastEditDate': '2013-11-17T16:03:32.293', 'AnswerCount': '1', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '10198', 'FavoriteCount': '1', 'Body': '<p>Given a directed graph $G = (V,A)$, a feedback arc set is a set of arcs whose removal leaves an acyclic graph.  The problem is to find the minimum cardinality such set.</p>\n\n<p>I want to find out about is there some approximation algorithm around this problem.</p>\n', 'Tags': '<algorithms><graph-theory><approximation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-17T16:03:32.293', 'CommentCount': '6', 'AcceptedAnswerId': '14432', 'CreationDate': '2013-09-18T09:21:04.343', 'Id': '14410'},98_47:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Consider the following random process.  We have a $10\\times 10$ grid.  At each time step, we pick a random empty grid cell (selected uniformly at random from among all empty cells) and place a marker in that grid cell.  As soon as we have five contiguous markers in a line (in a row, column, or diagonal), we stop.</p>\n\n<p>I'm given a grid containing some markers in some positions, and I'd like to estimate how long until the process stops if we start from that configuration (i.e., the number of additional time steps until five-in-a-line occurs).  I would be happy with any reasonable metric for that: e.g., the expected time until it stops, or the value $t$ such that there's a probability $0.5$ that the process will stop in $\\le t$ time steps.  I'd be happy with an estimate of any such metric.</p>\n\n<p>Is there any efficient algorithm to estimate this metric, given a configuration where some markers have already been placed?  I'm hoping for something faster than random simulation (repeatedly simulating the process and computing an estimate based upon the resulting empirical distribution).</p>\n", 'ViewCount': '129', 'Title': 'Estimating the time until we obtain five-in-a-row?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-25T20:05:32.690', 'LastEditDate': '2013-10-04T06:45:50.277', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><combinatorics><probability-theory><approximation>', 'CreationDate': '2013-10-02T04:18:07.603', 'FavoriteCount': '1', 'Id': '14745'},98_48:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <em>MAXDICUT</em> problem on a directed graph $G$ is where one is given integer $t$ and a graph $G$ and one has to decide if there is a subset $S \\subset \\mathcal{V}(G)$ such that $$|\\{(i\\rightarrow j)\\in\\mathcal{E}(G)\\mid i\\in S \\text{ and } j\\cap S=\\emptyset\\}|\\geq t$$</p>\n\n<p>Consider a $(c^a,d^a)$-regular directed hypergraph $H$ (meaning every hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(H)$ has $c^a$ vertices with  $head(i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a})=i_1$ and every vertex is on $d^a$ hyperedges with each vertex being a head on only one hyperedge) on $n^a$ vertices with fixed $n\\geq 4$ and fixed $c,d\\geq 2$ and variable parameter $a\\geq 1$. Consider the problem of given an integer $t$ and a hypergraph $H$ with above parameters, one has to decide if there is a subset $S \\subset \\mathcal{V}(H)$ such that $$|\\{\\mathcal{I}=\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(H)\\mid i_1\\in S \\text{ and } \\mathcal{I}\\setminus i_1\\cap S=\\emptyset\\}|\\geq t$$</p>\n\n<p>Call this problem <em>REGULAR-HYPERGRAPH-MAXDICUT</em>. Clearly this problem is NP-complete. Note that a directed $d$-regular graph on $n$ vertices with one outgoing edge per vertex is a $(2,d)$-regular hypergraph on $n$ vertices. So <em>REGULAR-HYPERGRAPH-MAXDICUT</em> $\\leq$ <em>MAXDICUT</em>.</p>\n\n<p>Now consider a $(c^a,d^a)$-regular directed hypergraph $H$ on $n^a$ vertices with fixed $n\\geq 4$ and fixed $c,d\\geq 2$ and variable parameter $a\\geq 1$. Given this graph, form a sequence of $d^a$-regular directed graphs $G_j$ on $n^a$ vertices for $j=1\\rightarrow c^a-1$ by projecting the hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(H)$ to the edge $\\{i_1 \\rightarrow i_{j+1}\\}$ and dropping the remaining possibles projections. Conversely given such $G_i$s, we can glue each set of edges from $i_1$ to get back the hypergraph $H$. Note each vertex of $G_i$ lies on $d^a$ edges.</p>\n\n<p>Given this graph, form a sequence of $r^m$-regular directed graphs $\\mathcal{G}_j$ on $N^m$ vertices for $j=1\\rightarrow r^m-1$ by projecting the hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(\\mathcal{H}([G,m]))$ to the edge $\\{i_1 \\rightarrow i_{j+1}\\}$ and dropping the remaining possibles projections. Conversely given such $\\mathcal{G}_j$ graphs, we can glue each set of edges from $i_1$ to get back the hypergraph $\\mathcal{H}[G,m]$. Note each vertex of $G_i$ lies on $r^m$ edges. </p>\n\n<p>Now from the $G_j$ construct the graph sum $\\mathcal{G}$ which is the directed graph with the hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(\\mathcal{H}([G,m]))$ replaced by a sequence of edges $\\{i_1\\rightarrow i_2,i_1\\rightarrow i_3,i_1\\rightarrow i_4,\\dots,i_1\\rightarrow i_{c^a}\\}\\in\\mathcal{E}(\\mathcal{G}$. It is clear \\emph{MAXDICUT} problem on $\\mathcal{G}$ solves the <em>HYPERGRAPH-MAXDICUT</em> problem in $\\mathcal{H}([G,m])$.</p>\n\n<p>Can the above give <em>MAXDICUT</em> $\\leq$ <em>REGULAR-HYPERGRAPH-MAXDICUT</em>? (THE ISSUE IS WHEN WE CHOOSE AN HYPEREDGE WE CHOOSE ALL ITS VERTICES. THIS IS NOT THE CASE WHEN WE REPLACE THE HYPERGRAPH WITH A DIRECTED GRAPH).</p>\n\n<p>Is <em>REGULAR-HYPERGRAPH-MAXDICUT</em> $\\leq_{APX}$ <em>MAXDICUT</em> and <em>MAXDICUT</em> $\\leq_{APX}$ <em>REGULAR-HYPERGRAPH-MAXDICUT</em>? This will give a constant factor approximation to <em>REGULAR-HYPERGRAPH-MAXDICUT</em> where the constant factor is independent of $a$.</p>\n\n<p>Can we generalize to a possible version of <em>IRREGULAR-HYPERGRAPH-MAXDICUT</em>? Would this yield constant factor approximations to <em>IRREGULAR-HYPERGRAPH-MAXDICUT</em> as well?</p>\n', 'ViewCount': '39', 'Title': 'Constant factor approximation of hypergraph maxdicut', 'LastEditorUserId': '9753', 'LastActivityDate': '2013-10-02T19:15:11.187', 'LastEditDate': '2013-10-02T19:15:11.187', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9753', 'Tags': '<reductions><approximation-algorithms><max-cut>', 'CreationDate': '2013-10-02T08:29:17.627', 'Id': '14751'},98_49:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '192', 'Title': 'How can you bound the error of an approximation without knowing the optimal solution?', 'LastEditDate': '2013-10-11T07:28:50.080', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '10572', 'FavoriteCount': '1', 'Body': '<p>I been looking at this <a href="http://www.math.uwaterloo.ca/tsp/world/countries.html">site</a> and it says that people found solutions for TSP tours that are just 0.031% higher than the optimal tour is. Without finding the optimal tour how does they know what length it is supposed to be?</p>\n', 'Tags': '<complexity-theory><approximation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-17T15:01:09.993', 'CommentCount': '3', 'AcceptedAnswerId': '15003', 'CreationDate': '2013-10-09T12:03:45.390', 'Id': '14945'},98_50:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In real word problems, the influence of multiple not perfectly known factors results in using heuristics instead of mathemacial solutions that calculates a perfect value from only precisly defined input data. Consequently, any method that does not supply the mathematical maximum or minimum is not an optimisation but an improvement.</p>\n\n<p>Somehow my opinion on this topic differs from the use of the term <code>optimisation</code> in many papers. Are the people just not precise in their language or is my understanding of the term wrong?</p>\n\n<p><code>Improvement</code> doesn't sound as facy as <code>optimisation</code>, but is there maybe some facy word that allows people to still be precise?</p>\n", 'ViewCount': '40', 'Title': 'Is a non-perfect improvement and optimisation?', 'LastActivityDate': '2013-10-14T13:53:07.487', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10634', 'Tags': '<terminology><optimization><approximation><applied-theory><approximation-algorithms>', 'CreationDate': '2013-10-14T13:28:29.267', 'Id': '16071'},98_51:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the text book, Introduction to Algorithm, 3rd Edition.</p>\n\n<p>In the chapter, <strong>Approximation Algorithms</strong> and for the problem <strong>Travelling Salesman Problem</strong>, the author says: </p>\n\n<p><img src="http://i.stack.imgur.com/99jYL.png" alt="enter image description here"></p>\n\n<p>I am wondering how triangle inequality gives rise to this assertion? It seems that this property is not that important, as I searched through the rest of this section, it does not appear.</p>\n', 'ViewCount': '143', 'Title': 'without triangle inequality, finding good approximate tours for TSP in polynomial time is impossible unless P=NP?', 'LastActivityDate': '2013-10-18T00:46:42.290', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><complexity-theory><traveling-salesman><approximation-algorithms>', 'CreationDate': '2013-10-16T17:31:49.740', 'Id': '16143'},98_52:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the chapter, <em>Approximation Algorithms</em> of <em>Introduction to Algorithm, 3rd Edition</em>, for the approximation problem <strong>Travelling Salesman Problem</strong>, the author proposes a approximation method that first constructs a minimum spanning tree.</p>\n\n<p><img src="http://i.stack.imgur.com/G9Bvx.png" alt="enter image description here"></p>\n\n<p>In order to prove this algorithm is a 2-approximation algorithm, the author claims that:</p>\n\n<p>The weight of the minimum spanning tree $T$ is less than the cost of the optimal tour.</p>\n\n<p>I am wondering if the minimum spanning tree(which is <strong>acyclic</strong>) of $G$ ensures that its weight is <strong>necessarily</strong> smaller than any tour(which is <strong>cyclic</strong>) of the same graph $G$</p>\n\n<p>PS: </p>\n\n<p>The original claim is:</p>\n\n<p><img src="http://i.stack.imgur.com/MGgcu.png" alt="enter image description here"></p>\n', 'ViewCount': '134', 'Title': 'Approximated TSP: weight of minimum spanning tree less than cost of the optimal tour?', 'LastActivityDate': '2013-10-16T17:49:55.960', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><graph-theory><traveling-salesman><approximation-algorithms>', 'CreationDate': '2013-10-16T17:45:18.160', 'FavoriteCount': '1', 'Id': '16144'},98_53:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a problem in understanding how to prove the following question.</p>\n\n<p>Let $Q = \\langle\\max,f,L\\rangle$ be an NPO-Problem, where $f$ only supports integers. \nDefine $$L_Q^* =\\{(x_0,1^k) : \\exists x . L(x_0,x) \\land f(x_0,x) \\geq k\\}.$$\nThe instance of $x_0$ is binary coded, while the numerical parameter $k$ is unary coded. Show that if $L_Q^*$ is NP-complete, then there is no FPTAS for $Q$.\nIt can be assumed that $P \\neq NP$.</p>\n\n<p>Normally I have some ideas, but this time I am really stumped. My only idea was to use the fact that if $L_Q^*$ has an approximation scheme, then $f$ must run in time polynomial in $|x_0|+|x|$.</p>\n', 'ViewCount': '111', 'Title': 'NP-hardness and FPTAS', 'LastEditorUserId': '10940', 'LastActivityDate': '2013-10-30T09:53:21.487', 'LastEditDate': '2013-10-24T10:36:46.610', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '16378', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10940', 'Tags': '<np-hard><approximation>', 'CreationDate': '2013-10-23T21:18:59.300', 'Id': '16373'},98_54:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Here is a variation of a job-scheduling Problem.\nLet $J = \\{j_1,...j_n\\}$ be a set of Jobs for $1 \\leq i \\leq n$. Given Job length $|j_i|\\in \\mathbb{N}$, deadline $f_i \\in \\mathbb{N}$, profit $p_i \\ge 0$ and starting-time $s_i  \\in \\mathbb{N}$. I am looking for a greedy approximation factor given that the Job length may only be distinguished by factor k. </p>\n\n<p>$$max_i|j_i| \\leq k \\cdot min_i|j_i|$$</p>\n\n<p>The Greedy algorithm of this Problem is fairly stupid. Greedy takes a job with the biggest profit.  I created an example (3-Job-Scheduling):</p>\n\n<p>Let $J = \\{j_1,j_2,j_3\\}$ with $|j_1| = 2, j_2 = j_3 = 1$ and </p>\n\n<p>$s_1 = 0; s_2 = 0; s_3 = 1$,</p>\n\n<p>$f_1 = 2;f_2 = 1; f_3 = 2$</p>\n\n<p>$p_1 = w; p_2 = p_3 = (w-1)$</p>\n\n<p>What I want to show is that Greedy gives us w while 2(w-1) is the optimal solution. </p>\n\n<p>My question: Is this valid for n-Job-Scheduling (the general case). Is this the worst-case? </p>\n\n<p>I can't think of anything worse. So I figured since the problem is a k-Matroid (is this a common term?) there will be a an approximation factor $\\frac{1}{k-\\epsilon}$ for  any $\\epsilon &gt; 0.$ I know this is not exactly a proof yet, but am I on the right way?</p>\n\n<p>Thanks for your help!</p>\n", 'ViewCount': '118', 'Title': 'Single machine job scheduling (Greedy heuristic)', 'LastActivityDate': '2013-11-08T13:06:23.300', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10940', 'Tags': '<approximation><scheduling><greedy-algorithms><check-my-proof>', 'CreationDate': '2013-11-08T13:06:23.300', 'Id': '16821'},98_55:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm given this problem:</p>\n\n<p>Consider the following closest-point heuristic for building an approximate traveling-salesman  tour. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex u that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest u is vertex v. Extend the cycle to include u by inserting u just after v. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour. </p>\n\n<p>This is the same as Prim's algorithm.  Unless I'm missing something, this is not an approximate traveling salesman tour since the traveling salesman requires a Hamiltonian path where we don't revisit any nodes, but on many graphs this algorithm seems to require revisiting nodes to get back to the source node.  Am I wrong or is this problem unclearly worded?</p>\n", 'ViewCount': '120', 'Title': "Traveling Salesman's Tour Approx Algorithm: is this really a Hamiltonian Path?", 'LastActivityDate': '2013-11-19T10:58:35.200', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '18152', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11474', 'Tags': '<traveling-salesman><spanning-trees><approximation-algorithms>', 'CreationDate': '2013-11-19T06:43:05.060', 'Id': '18147'},98_56:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>Below is a homework problem where we have been asked to alter a greedy algorithm to return n element instance of a set problem. The original algorithm is also below. I was thinking that I could alter line 3 so that it would run until the size of C was equal to n, and change the logic in line 4 so that it would pick and remove vertices until the size of n. A vertex would be removed when the size of C doesn't equal to n but the cover is complete. I can't really think of any other way to do it. The real problem is that I'm not entirely sure how to make the algorithm run in exponential time like they are asking. </p>\n\n<blockquote>\n  <p>GREEDY-SET-COVER can return a number of different solutions, depending on\n  how we break ties in line 4. Give a procedure BAD-SET-COVER-INSTANCE.n/\n  that returns an n-element instance of the set-covering problem for which, depending\n  on how we break ties in line 4, GREEDY-SET-COVER can return a number of\n  different solutions that is exponential in n.</p>\n  \n  <p>$X$ \u2014 some finite set<br>\n  $F$ \u2014 a family of subsets of $X$<br>\n  $C$ \u2014 cover being constructed    </p>\n  \n  <p>GREEDY-SET-COVER($n$)<br>\n  1    let $U = X$<br>\n  2    let $C = \\varnothing$<br>\n  3    while $U \\ne \\varnothing$<br>\n  3a            select an $S \\in F$ that maximizes $\\left|S \\cap U\\right|$<br>\n  3b            set $U = U \\setminus S$<br>\n  3c            set $C = C \\cup \\{S\\}$<br>\n  4    return $C$   </p>\n</blockquote>\n\n<p>Could it be said that since the number of subsets a set has is $2^n$ and that in the worst case this algorithm will end up finding all of those subsets before settling on an n-instance set to return?</p>\n", 'ViewCount': '112', 'Title': 'Finding an instance of an n-element set cover', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-28T21:30:30.133', 'LastEditDate': '2013-11-28T21:30:30.133', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11580', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms><set-cover><approximation-algorithms>', 'CreationDate': '2013-11-23T23:48:31.680', 'Id': '18287'},98_57:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given two NP NP-hard functional problems, A and B, one can find a reduction of A to B. Is it possible to find a reduction that would honour approximations? That is, if you have an approximation algorithm for B that yield approximate solutions within accuracy $\\delta$, is it possible to reduce A to B in such a way that one would be able to derive an approximate solution of A within accuracy $\\epsilon = \\epsilon(\\delta)$?</p>\n', 'ViewCount': '85', 'Title': 'Approximation algorithms for NP-complete problems', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-19T05:48:21.523', 'LastEditDate': '2013-12-16T20:06:37.607', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10447', 'Tags': '<complexity-theory><reductions><approximation-algorithms>', 'CreationDate': '2013-12-16T18:26:00.343', 'Id': '19050'},98_58:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Does anyone know a good algorithm for quickly finding an approximate solution to the following problem?</p>\n\n<p>Given two square matrices $A$ and $B$, minimize $\\| P A P^\\top - B \\|$ over all permutation matrices $P$.</p>\n\n<p>I have heard that there are several types of algorithms for these kinds of problems, like iterative improvement, simulated annealing, tabu search, genetic algorithms, evolution strategies, ant algorithms, and scatter search. I am looking for existing software.</p>\n', 'ViewCount': '34', 'Title': 'Quadratic programming problem involving permutation matrices', 'LastActivityDate': '2013-12-26T14:22:58.533', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12383', 'Tags': '<algorithms><optimization><permutations><approximation-algorithms>', 'CreationDate': '2013-12-26T14:22:58.533', 'Id': '19303'},98_59:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '127', 'Title': 'NP-complete decision problems - how close can we come to a solution?', 'LastEditDate': '2014-01-02T15:28:30.023', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '2', 'Body': '<p>After we prove that a certain <strong>optimization</strong> problem is NP-hard, the natural next step is to look for a polynomial algorithm that comes close to the optimal solution - preferrably with a constant approximation factor.</p>\n\n<p>After we prove that a certain <strong>decision</strong> problem is NP-complete, what is the natural next step? Obviously we cannot "approximate" a boolean value...</p>\n\n<p>My guess is that, the next step is to look for a randomized algorithm that returns the correct solution with a high probability. Is this correct?</p>\n\n<p>If so, what probability of being correct can we expect to get from such a randomized algorithm?</p>\n\n<p>As far as I understand from Wikipedia, <a href="https://en.wikipedia.org/wiki/PP_%28complexity%29" rel="nofollow">PP contains NP</a>. This means that, if the problem is in NP, it should be easy to write an algorithm that is correct more than $0.5$ of the times.</p>\n\n<p>However, <a href="https://en.wikipedia.org/wiki/Bounded-error_probabilistic_polynomial" rel="nofollow">it is not known whether BPP contains NP</a>. This means that, it may be difficult (if not impossible) to write an algorithm that is correct more than $0.5+\\epsilon$ of the times, for every positive $\\epsilon$ independent of the size of input.</p>\n\n<p>Did I understand correctly?</p>\n', 'Tags': '<np-complete><approximation><randomized-algorithms>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-01-03T01:18:29.533', 'CommentCount': '2', 'AcceptedAnswerId': '19419', 'CreationDate': '2013-12-31T16:03:52.760', 'Id': '19412'},98_60:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Many problems in computer science come in two flavors:</p>\n\n<ul>\n<li>Optimization problem: "Find an object with the largest size".</li>\n<li>Threshold problem: "Given $n$, find an object with a size of at least $n$, or reply that such an object does not exist".</li>\n</ul>\n\n<p>Given a solution for the optimization problem, we can solve the threshold problem simply by running the optimization solution and checking if the result has a size of at least $n$. But this doesn\'t help us if the optimization problem is NP-complete.</p>\n\n<p>MY QUESTION IS: If we have a constant-factor approximation algorithm for the optimization problem, how can we use it for the threshold problem?</p>\n\n<p>An obvious answer is: Given $n$, run the approximation algorithm. If the result has a size of at least $n$, return it. If the result has a size of less than $n/c$ (where c is the approximation constant), return that an object of size $n$ does not exist. Otherwise, return "I don\'t know".</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '48', 'Title': 'Using approximations to optimization problems for threshold problems', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-01-31T16:18:31.590', 'LastEditDate': '2014-01-01T14:23:41.853', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<np-complete><reductions><optimization><approximation>', 'CreationDate': '2014-01-01T13:25:36.750', 'Id': '19433'},98_61:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have question on understanding the following neighborhood relation within a local-search approximation scheme. \nLet $M$ be a legal matching on any bipartite graph. \nLet $U_k$ be the neighborhood defined as follows:\n$$U_k := \\{M' : |(M' \\backslash M) \\cup (M \\backslash M')| \\leq k\\}$$</p>\n\n<p>Can somebody give me an example or explain this to me? </p>\n\n<p>If i choose a small k-value, the cardinality of $M'$ will be small as well, but how does an algorithm decide which matching pair of nodes to take?</p>\n\n<p>If we define node-values and make it a weighted matching,let say we define a weight function $w_e \\in \\mathbb{R}$ for any edge e in our graph, now the algorithm may use greedy method and take the best possible pair of nodes (with greatest weight). </p>\n\n<p>But I still don't understand the exact set definition of our neighborhood.</p>\n\n<p>I would be grateful for an example, because I'm stumped on this one. </p>\n", 'ViewCount': '59', 'Title': 'Local search: Problem with neighborhood definition', 'LastActivityDate': '2014-01-10T12:36:41.790', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19626', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12731', 'Tags': '<optimization><approximation><heuristics><bipartite-matching>', 'CreationDate': '2014-01-10T11:48:35.183', 'Id': '19625'},98_62:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say I have variables $w_1, \\dots w_n, h_1, \\dots h_m \\in \\mathbb R$, constants $W, H$, functions $f_1, \\dots f_k : \\mathbb R\\times\\mathbb R\\to\\mathbb R$ from some family $F$ and for each function $f_i$, a pair of intervals $x_i \\subseteq [1, n]$, $y_i \\subseteq [1, m]$. All quantities $\\geq 0$.</p>\n\n<p>I want to find the $w_i$ and $h_i$ to minimize $\\sum f_i \\Big(\\sum _{j\\in x_i} w_j, \\sum _{j\\in y_i} h_j \\Big)$ with constraints $\\sum w_i = W$ and $\\sum h_i = H$. Approximations are perfectly fine.</p>\n\n<p>Informally, this is a grid with column widths $w_i$ and row heights $h_i$ and cells that may span multiple rows and columns, that have costs $f_i$.</p>\n\n<p>My question is, for what families $F$ does this problem have reasonably efficient solutions? The set of affine functions should work. What about step functions? Piecewise linear? Smooth monotonic? Smooth functions in general?</p>\n', 'ViewCount': '41', 'Title': 'What functions are easy to optimize?', 'LastActivityDate': '2014-01-26T12:56:04.577', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5167', 'Tags': '<optimization><approximation>', 'CreationDate': '2014-01-26T12:56:04.577', 'FavoriteCount': '1', 'Id': '19994'},98_63:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been wondering lately how GPUs compute sines and cosines, and Google hasn't helped me finding a precise answer.</p>\n\n<p>Initially, I was thinking that in order to make the computations as fast as possible, the GPU would use some kind of lookup table. But then I realized, storing all sin values in a table of the range of doubles between [0, 2 * pi] would be a massive one, and thus not be a valid option.</p>\n\n<p>The table could possible be shrunk down in resolution, and the missing values for a lookup can then be lerped. This however, introduces a possible error which can propagate to bigger and unacceptable errors when performing the computation multiple times.</p>\n\n<p>My last idea is then that they could be using a Taylor approximation, but that would involve quite some arithmetic, which may be too slow for a GPU. So the question is, what do GPUs use to calculate the sines? Are it lookup tables, approximations, or a hybrid of both? And possible, do they use the same method for other computations like sqrt()?</p>\n", 'ViewCount': '111', 'Title': 'How do GPUs compute sines?', 'LastActivityDate': '2014-02-02T19:37:31.007', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20227', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '13308', 'Tags': '<approximation><arithmetic>', 'CreationDate': '2014-02-01T15:48:44.533', 'Id': '20189'},98_64:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In order to plot a curve a set of points is usually calculated based on some formula. The function FPLOT in MATLAB also supports plotting with some error tolerance. Its help says the following about this functionality:</p>\n\n<pre><code>The FPLOT function begins with a minimum step of size (XMAX-XMIN)*TOL.\nThe step size is subsequently doubled whenever the relative error\nbetween the linearly predicted value and the actual function value is\nless than TOL.\n</code></pre>\n\n<p>So, if I plot a curve based on some expression and with some predefined tolerance TOL, is the error of the line segment approximation between any two calculated points always lower than TOL? Unfortunately, this is not obvious for me.</p>\n\n<p>If not, how could one achieve a guaranteed (small) error of a piecewise-linear curve approximation (compared with the "exact" curve).</p>\n', 'ViewCount': '14', 'Title': 'Error estimates of piecewise-linear curve approximations', 'LastActivityDate': '2014-02-06T14:32:19.993', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '21379', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13186', 'Tags': '<approximation><approximation-algorithms><error-estimation>', 'CreationDate': '2014-02-06T14:20:58.570', 'Id': '21376'},98_65:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I believe the following question in summary is: How to approximate Euclidean distance in a digital plane?</p>\n\n<blockquote>\n  <p>When a pebble falls on a calm surface of water a circular wave propagates. I want to color the pixels with time to show this effect. So I discretize the time and in each time step, starting  from the center, I color one pixel away in all directions. But this gives a square wave. I guess what is wrong is that I have approximated the Euclidean distance with the infinity norm.</p>\n</blockquote>\n\n<p>How do I approximate Euclidean distance to get the a circular wave on the pixels? I don't want to measure the distance from each pixel to the center in each time slot. That will be very heavy. I am looking for a simple algorithm like coloring the next pixel adjacent to last colored pixel.</p>\n", 'ViewCount': '38', 'Title': 'Wave propagation in digital image', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-13T16:56:21.790', 'LastEditDate': '2014-02-13T16:56:21.790', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21589', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12321', 'Tags': '<algorithms><computational-geometry><approximation>', 'CreationDate': '2014-02-13T02:10:38.453', 'Id': '21585'},98_66:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have this programming problem, but I really cant figure out what it wants me to do. Heres what it is:</p>\n\n<blockquote>\n  <p>The cube root of a number can be found based on the observation that, if $t$ is an approximation of the cube root of $a$, then $\\tfrac13\\left(\\tfrac{a}{t^2} + 2t\\right)$ is a better approximation. </p>\n  \n  <p>Create a method <code>double betterCubeRoot(double a, double t)</code> that will find the cube root of $a$ accurate enough so that the difference between $t^3$ and $a$ is less than 0.0001. Use recursion.</p>\n  \n  <p>Then write the method <code>double cubeRoot(double a)</code> that makes use of the method with 1 as the initial value of $t$. Write a program that will test values both positive and negative.</p>\n</blockquote>\n\n<p>Can someone please explain what this means. I dont understand how I am supposed to find the cube root of 'a' using the given equation. Thanks.</p>\n\n<p>Here is what i have done:</p>\n\n<pre><code>public static double betterCubeRoot (double a, double t)\n{\n    double tCubed = Math.pow (t, 3);\n    double dif = Math.abs (tCubed - a);\n\n    double eq = ((a / (t * t)) + 2 * t) / 3;\n\n    if (dif &lt; 0.001)\n    {\n        return eq;\n    }\n    else\n    {            \n        return betterCubeRoot (eq, t) ;\n    }\n}\n</code></pre>\n", 'ViewCount': '37', 'Title': 'Can someone interpret what this is asking for', 'LastEditorUserId': '15182', 'LastActivityDate': '2014-03-02T02:44:26.557', 'LastEditDate': '2014-03-02T02:44:26.557', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15182', 'Tags': '<approximation><numerical-analysis>', 'CreationDate': '2014-03-02T01:24:38.247', 'Id': '22177'},98_67:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '72', 'Title': 'Approximation algorithms for Euclidean Traveling Salesman', 'LastEditDate': '2014-04-13T09:51:49.107', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15429', 'FavoriteCount': '1', 'Body': "<p>I am trying to find a way to solve Euclidean TSP in a polynomial time. I looked at some papers but I couldn't decide which one is better. What is the general approximation algorithm for solving this problem in polynomial time?</p>\n", 'ClosedDate': '2014-04-13T09:54:22.080', 'Tags': '<algorithms><reference-request><approximation><traveling-salesman>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-13T09:51:49.107', 'CommentCount': '5', 'CreationDate': '2014-03-09T18:25:51.543', 'Id': '22438'},98_68:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If we have a FPRAS for approximating the quantity c, can we get another FPRAS for estimating (c-1) using the estimation of c?</p>\n', 'ViewCount': '17', 'Title': 'Estimating (c-1) from approximation of c', 'LastActivityDate': '2014-03-10T16:49:39.253', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15499', 'Tags': '<approximation-algorithms>', 'CreationDate': '2014-03-10T16:00:59.913', 'Id': '22473'},98_69:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '27', 'Title': 'What is the Unique Games Conjecture?', 'LastEditDate': '2014-04-07T11:58:09.610', 'AnswerCount': '0', 'Score': '-1', 'OwnerDisplayName': 'zighalo', 'PostTypeId': '1', 'OwnerUserId': '16535', 'Body': '<p>What is the unique game conjecture in relatively simple words? What are the consequences of proving it or disproving it? Does it has any relation to game theory? Why is there "game" in the name?</p>\n', 'ClosedDate': '2014-04-07T14:07:00.723', 'Tags': '<complexity-theory><np-hard><approximation>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-04-07T11:58:09.610', 'CommentCount': '4', 'CreationDate': '2014-04-04T21:43:52.677', 'Id': '23508'},98_70:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given a set of $N$ $n \\times n$ matrices $A_1,\\ldots,A_N$, and two vectors $x,y$, the problem is to find a product of up to $K$ matrices $A = A_{j_1}A_{j_2}\\cdots A_{j_k}$ so that $Ax$ is as close to $y$ as possible in terms of euclidean distance. The problem can be shown to be NP-hard e.g. by reduction from subset sum. However the problem seems so hard that I have a hard time believing there is even a polynomial time strong approximation algorithm. Also there is the issue of how to define the quality of the approximation. The problem is invariant under scaling $x$ and $y$ so probably we should assume $x$ has length $1$ and an approximation gets within distance $\\epsilon ||y||$ of the optimal solution, where $y \\neq 0$ and $\\epsilon &gt; 0$ can be chosen arbitrarily small. </p>\n\n<p>Anyway like I said, I believe there is no polynomial time strong approximation algorithm but I'm having a hard time thinking of how a proof might go. If someone could help me resolve this question about whether a polynomial time strong approximation algorithm exists, that'd be great.</p>\n", 'ViewCount': '14', 'Title': 'Approximation scheme for finding best product of matrices that minimizes $||Ax - y||$ for given $x,y$', 'LastActivityDate': '2014-04-26T18:15:06.397', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><approximation><matrices>', 'CreationDate': '2014-04-26T18:15:06.397', 'Id': '24136'},98_71:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the <a href="http://www.nada.kth.se/~viggo/wwwcompendium/node149.html" rel="nofollow">hitting set problem</a> with $n$ elements and $m$ sets. I gather from the linked page as well as <a href="http://people.csail.mit.edu/dmoshkov/papers/set-cover/set-cover-full.pdf" rel="nofollow">this</a> that </p>\n\n<p>1) it is NP-hard to approximate the cost of the optimal solution to a multiplicative factor of $c \\log n$ for some $c&gt;0$. </p>\n\n<p>2) it is NP-hard to approximate the cost of the optimal solution to a multiplicative factor of $c \\log m$ for some $c&gt;0$. </p>\n\n<p>3) it is NP-hard to approximate the cost of the optimal solution to a multiplicative factor of $c \\log \\max(n,m)$ for some $c&gt;0$. </p>\n\n<p>Is my understanding correct? This seems to be a straightforward consequence of what is on the internet, but some of the notation in these sources is mysterious to me and I want to make sure I\'m not misunderstanding. </p>\n', 'ViewCount': '25', 'Title': 'Hardness of approximating hitting set', 'LastEditorUserId': '17154', 'LastActivityDate': '2014-04-28T21:58:39.947', 'LastEditDate': '2014-04-28T20:57:46.510', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '24201', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '17154', 'Tags': '<np-hard><approximation>', 'CreationDate': '2014-04-28T20:51:30.080', 'Id': '24199'}