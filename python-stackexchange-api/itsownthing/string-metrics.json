{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have two strings. Call them $A$ and $B$. Neither string has any repeated characters.</p>\n\n<p>How can I find the shortest sequence of insert, move, and delete operation that turns $A$ into $B$, where:</p>\n\n<ul>\n<li><code>insert(char, offset)</code> inserts <code>char</code> at the given <code>offset</code> in the string</li>\n<li><code>move(from_offset, to_offset)</code> moves the character currently at offset <code>from_offset</code> to a new position so that it has offset <code>to_offset</code></li>\n<li><code>delete(offset)</code> deletes the character at <code>offset</code></li>\n</ul>\n\n<p>Example application: You do a database query and show the results on your website. Later, you rerun the database query and discover that the results have changed. You want to change what is on the page to match what is currently in the database using the minimum number of DOM operations. There are two reasons why you'd want the shortest sequence of operations. First, efficiency. When only a few records change, you want to make sure that you do $\\mathcal{O}(1)$ rather than $\\mathcal{O}(n)$ DOM operations, since they are expensive. Second, correctness. If an item moved from one position to another, you want to move the associated DOM nodes in a single operation, without destroying and recreating them. Otherwise you will lose focus state, the content of <code>&lt;input&gt;</code> elements, and so forth.</p>\n", 'ViewCount': '238', 'Title': 'Expressing an arbitrary permutation as a sequence of (insert, move, delete) operations', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:22:27.077', 'LastEditDate': '2012-05-15T20:22:27.077', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '330', 'Tags': '<algorithms><combinatorics><string-metrics>', 'CreationDate': '2012-03-21T03:25:44.267', 'Id': '576'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to write a spell-checker which should work with a pretty large dictionary. I really want an efficient way to index my dictionary data to be used using a <a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance">Damerau-Levenshtein</a> distance to determine which words are closest to the misspelled word.</p>\n\n<p>I\'m looking for a data structure who would give me the best compromise between space complexity and runtime complexity.</p>\n\n<p>Based on what I found on the internet, I have a few leads regarding what type of data structure to use:</p>\n\n<h2>Trie</h2>\n\n<p><img src="http://i.stack.imgur.com/KhvoF.png" alt="trie-500px"></p>\n\n<p>This is my first thought and looks pretty easy to implement and should provide fast lookup/insertion. Approximate search using Damerau-Levenshtein should be simple to implement here as well. But it doesn\'t look very efficient in terms of space complexity since you most likely have a lot of overhead with pointers storage.</p>\n\n<h2>Patricia Trie</h2>\n\n<p><img src="http://i.stack.imgur.com/EJYB0.png" alt="trie-500px"></p>\n\n<p>This seems to consume less space than a regular Trie since you\'re basically avoiding the cost of storing the pointers, but I\'m a bit worried about data fragmentation in case of very large dictionaries like what I have.</p>\n\n<h2>Suffix Tree</h2>\n\n<p><img src="http://i.stack.imgur.com/uXH1b.png" alt="suffix-500px"></p>\n\n<p>I\'m not sure about this one, it seems like some people do find it useful in text mining, but I\'m not really sure what it would give in terms of performance for a spell checker.</p>\n\n<h2>Ternary Search Tree</h2>\n\n<p><img src="http://i.stack.imgur.com/X8hPY.png" alt="tst"></p>\n\n<p>These look pretty nice and in terms of complexity should be close (better?) to Patricia Tries, but I\'m not sure regarding fragmentation if it would be better of worse than Patricia Tries.</p>\n\n<h2>Burst Tree</h2>\n\n<p><img src="http://i.stack.imgur.com/9jn1m.png" alt="burst"></p>\n\n<p>This seems kind of hybrid and I\'m not sure what advantage it would have over Tries and the like, but I\'ve read several times that it\'s very efficient for text mining.</p>\n\n<hr>\n\n<p>I would like to get some feedback as to which data structure would be best to use in this context and what makes it better than the other ones. If I\'m missing some data structures who would be even more appropriate for a spell-checker, I\'m very interested as well.</p>\n', 'ViewCount': '1908', 'Title': 'Efficient data structures for building a fast spell checker', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-04T14:00:33.120', 'LastEditDate': '2012-05-15T20:22:56.733', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '23', 'PostTypeId': '1', 'OwnerUserId': '1307', 'Tags': '<data-structures><strings><string-metrics>', 'CreationDate': '2012-05-02T03:07:23.057', 'FavoriteCount': '5', 'Id': '1626'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>This question has been prompted by <a href="http://cs.stackexchange.com/questions/1626/efficient-data-structures-for-building-a-fast-spell-checker">Efficient data structures for building a fast spell checker</a>.</p>\n\n<p>Given two strings $u,v$, we say they are <em>$k$-close</em> if their <a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance" rel="nofollow">Damerau\u2013Levenshtein distance</a>\xb9 is small, i.e. $\\operatorname{LD}(u,v) \\geq k$ for a fixed $k \\in \\mathbb{N}$. Informally, $\\operatorname{LD}(u,v)$ is the minimum number of deletion, insertion, substitution and (neighbour) swap operations needed to transform $u$ into $v$. It can be computed in $\\Theta(|u|\\cdot|v|)$ by dynamic programming. Note that $\\operatorname{LD}$ is a <a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29" rel="nofollow">metric</a>, that is in particular symmetric.</p>\n\n<p>The question of interest is:</p>\n\n<blockquote>\n  <p>Given a set $S$ of $n$ strings over $\\Sigma$ with lengths at most $m$, what is the cardinality of </p>\n  \n  <p>$\\qquad \\displaystyle S_k := \\{ w \\in \\Sigma^* \\mid \\exists v \\in S.\\ \\operatorname{LD}(v,w) \\leq k \\}$?</p>\n</blockquote>\n\n<p>As even two strings of the same length have different numbers of $k$-close strings\xb2 a general formula/approach may be hard (impossible?) to find. Therefore, we might have to compute the number explicitly for every given $S$, leading us to the main question:</p>\n\n<blockquote>\n  <p>What is the (time) complexity of finding the cardinality of the set $\\{w\\}_k$ for (arbitrary) $w \\in \\Sigma^*$?</p>\n</blockquote>\n\n<p>Note that the desired quantity is exponential in $|w|$, so explicit enumeration is not desirable. An efficient algorithm would be great.</p>\n\n<p>If it helps, it can be assumed that we have indeed a (large) set $S$ of strings, that is we solve the first highlighted question.</p>\n\n<hr>\n\n<ol>\n<li>Possible variants include using the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> instead.</li>\n<li>Consider $aa$ and $ab$. The sets of $1$-close strings over $\\{a,b\\}$ are $\\{ a, aa,ab,ba,aaa,baa,aba,aab \\}$ (8 words) and $\\{a,b,aa,bb,ab,ba,aab,bab,abb,aba\\}$ (10 words), respectively .</li>\n</ol>\n', 'ViewCount': '268', 'Title': 'How many strings are close to a given set of strings?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:21:01.853', 'LastEditDate': '2012-05-15T20:21:01.853', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><time-complexity><strings><word-combinatorics><string-metrics>', 'CreationDate': '2012-05-09T15:48:12.173', 'FavoriteCount': '1', 'Id': '1758'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to turn one string into another with only single letter substitions.  What is a good way to do this, passing through only valid words in between (<a href="http://www.wuzzlesandpuzzles.com/wordchange/" rel="nofollow">this</a> website has some examples)?</p>\n\n<p>Valid here means "a word in English" as this is the domain I consider.</p>\n\n<p>My current idea is that I could use a shortest path algorithm with the Hamming distance for edge weights. The problem is that it will take a long time to build the graph, and even then the weight is not so precise in terms of distance (though it will never underestimate it) unless the weight is one, so I would probably have to find a to build a graph that only had weights of one.</p>\n\n<p>What would be the easiest way to build the graph? Am I taking entirely the wrong approach?</p>\n', 'ViewCount': '320', 'Title': 'Turn one string into another with single letter substitions', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:21:41.307', 'LastEditDate': '2012-05-15T20:21:41.307', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '863', 'Tags': '<algorithms><strings><string-metrics>', 'CreationDate': '2012-05-10T23:30:48.357', 'Id': '1785'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given two strings, $r$ and $s$, where $n = |r|$, $m = |s|$ and $m \\ll n$, find the minimum edit distance between $s$ for each beginning position in $r$ efficiently.</p>\n\n<p>That is, for each suffix of $r$ beginning at position $k$, $r_k$, find the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> of $r_k$ and $s$ for each $k \\in [0, |r|-1]$.  In other words, I would like an array of scores, $A$, such that each position, $A[k]$, corresponds to the score of $r_k$ and $s$.</p>\n\n<p>The obvious solution is to use the standard dynamic programming solution for each $r_k$ against $s$ considered separately, but this has the abysmal running time of $O(n m^2)$ (or $O(n d^2)$, where $d$ is the maximum edit distance).  It seems like you should be able to re-use the information that you\'ve computed for $r_0$ against $s$ for the comparison with $s$ and $r_1$.</p>\n\n<p>I\'ve thought of constructing a prefix tree and then trying to do dynamic programming algorithm on $s$ against the trie, but this still has worst case $O(n d^2)$ (where $d$ is the maximum edit distance) as the trie is only optimized for efficient lookup.</p>\n\n<p>Ideally I would like something that has worst case running time of $O(n d)$ though I would settle for good average case running time.  Does anyone have any suggestions?  Is $O(n d^2)$ the best you can do, in general?</p>\n\n<p>Here are some links that might be relevant though I can\'t see how they would apply to the above problem as most of them are optimized for lookup only:</p>\n\n<ul>\n<li><a href="http://stevehanov.ca/blog/index.php?id=114" rel="nofollow">Fast and Easy Levensthein distance using a Trie</a></li>\n<li><a href="http://stackoverflow.com/questions/3183149/most-efficient-way-to-calculate-levenshtein-distance">SO: Most efficient way to calculate Levenshtein distance</a></li>\n<li><a href="http://stackoverflow.com/questions/4057513/levenshtein-distance-algorithm-better-than-onm?rq=1">SO: Levenshtein Distance Algoirthm better than $O(n m)$</a></li>\n<li><a href="http://www.berghel.net/publications/asm/asm.php" rel="nofollow">An extension of Ukkonen\'s enhanced dynamic programming ASM algorithm</a></li>\n<li><a href="http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata" rel="nofollow">Damn Cool Algorithms: Levenshtein Automata</a></li>\n</ul>\n\n<p>I\'ve also heard some talk about using some type of distance metric to optimize search (such as a <a href="http://en.wikipedia.org/wiki/BK-tree" rel="nofollow">BK-tree</a>?) but I know little about this area and how it applies to this problem.</p>\n', 'ViewCount': '836', 'Title': 'Efficiently calculating minimum edit distance of a smaller string at each position in a larger one', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-28T15:40:06.263', 'LastEditDate': '2012-06-28T15:40:06.263', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '2526', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '67', 'Tags': '<algorithms><runtime-analysis><strings><dynamic-programming><string-metrics>', 'CreationDate': '2012-06-27T20:48:29.300', 'Id': '2519'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '139', 'Title': 'Find string that minimizes the sum of the edit distances to all other strings in set', 'LastEditDate': '2012-06-30T10:54:21.683', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'jmvidal', 'PostTypeId': '1', 'OwnerUserId': '2015', 'FavoriteCount': '1', 'Body': '<p>I have a set of strings $S$ and I am using the edit-distance (<a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein</a>) to measure the distance between all pairs.</p>\n\n<p>Is there an algorithm for finding the string $x$ which minimizes the sum of the distances to all strings in $S$, that is</p>\n\n<p>$\\arg_x \\min \\sum_{s \\in S} \\text{edit-distance}(x,s)$</p>\n\n<p>It seems like there should, but I can\'t find the right reference.</p>\n', 'Tags': '<algorithms><reference-request><strings><string-metrics>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-30T10:54:21.683', 'CommentCount': '0', 'AcceptedAnswerId': '2551', 'CreationDate': '2012-06-29T15:25:33.753', 'Id': '2546'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '106', 'Title': 'Determining how similar a given string is to a collection of strings', 'LastEditDate': '2012-07-22T09:43:50.247', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2185', 'FavoriteCount': '2', 'Body': '<p>I\'m not sure if this question belongs here and I apologize if not.  What I am looking to do is to develop a programmatic way in which I can probabilistically determine whether a given string "belongs" in a bag of strings.  For example, if I have bag of 10,000 US city names, and then I have the string "Philadelphia", I would like some quantitative measure of how likely \'Philadelphia\' is a US city name based on the US city names I already know.  While I know I won\'t be able to separate real city names from fake city names in this context, I would at least expect to have strings such as "123.75" and "The quick red fox jumped over the lazy brown dogs" excluded given some threshold.</p>\n\n<p>To get started, I\'ve looked at Levenshtein Distance and poked around a bit on how that\'s been applied to problems at least somewhat similar to the one I\'m trying to solve.  One interesting application I found was plagiarism detection, with one paper describing how Levenshtein distance was used with a modified Smith-Waterman algorithm to score papers based on how likely they were a plagarized version of a given base paper.  My question is if anyone could point me in the right direction with other established algorithms or methodologies that might help me.  I get the feeling that this may be a problem someone in the past has tried to solve but so far my Google-fu has failed me.</p>\n', 'Tags': '<algorithms><reference-request><string-metrics>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-22T09:43:50.247', 'CommentCount': '3', 'AcceptedAnswerId': '2780', 'CreationDate': '2012-07-16T21:29:07.047', 'Id': '2777'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a very specific question about semantic clustering.</p>\n\n<p>I have a list of words/phrases. I want to run an intelligent semantic clustering algorithm on this list. Please let me know what the available options are. Definitely I am looking for NLP based algorithms.</p>\n\n<p>Simple, open-source, easy-to-use solutions will be highly appreciated. The semantic part is extremely important here.</p>\n', 'ViewCount': '523', 'Title': 'Semantic clustering', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-27T18:49:17.373', 'LastEditDate': '2012-07-27T14:31:08.407', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2288', 'Tags': '<algorithms><strings><string-metrics><natural-lang-processing><ontologies>', 'CreationDate': '2012-07-27T10:56:33.110', 'FavoriteCount': '1', 'Id': '2922'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On <a href="https://en.wikipedia.org/wiki/Levenshtein_distance#Computing_Levenshtein_distance">Wikipedia</a>, an implementation for the bottom-up dynamic programming scheme for the edit distance is given. It does not follow the definition completely; inner cells are computed thus:</p>\n\n<pre><code>if s[i] = t[j] then  \n  d[i, j] := d[i-1, j-1]       // no operation required\nelse\n  d[i, j] := minimum\n             (\n               d[i-1, j] + 1,  // a deletion\n               d[i, j-1] + 1,  // an insertion\n               d[i-1, j-1] + 1 // a substitution\n             )\n}\n</code></pre>\n\n<p>As you can see, the algorithm <em>always</em> chooses the value from the upper-left neighbour if there is a match, saving some memory accesses, ALU operations and comparisons. </p>\n\n<p>However, deletion (or insertion) may result in a <em>smaller</em> value, thus the algorithm is locally incorrect, i.e. it breaks with the optimality criterion. But maybe the mistake does not change the end result -- it might be cancelled out.</p>\n\n<p>Is this micro-optimisation valid, and why (not)?</p>\n', 'ViewCount': '306', 'Title': 'Micro-optimisation for edit distance computation: is it valid?', 'LastActivityDate': '2012-08-02T07:32:35.867', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2997', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><dynamic-programming><string-metrics><correctness-proof><program-optimization>', 'CreationDate': '2012-08-01T15:41:33.670', 'Id': '2985'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1105', 'Title': 'Fast k mismatch string matching algorithm', 'LastEditDate': '2012-09-29T20:37:46.053', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '3011', 'FavoriteCount': '2', 'Body': '<p>I am looking for a fast k-mismatch string matching algorithm. Given a pattern string P of length m, and a text string T of length n, I need a fast (linear time) algorithm to find all positions where P matches a substring of T with at most k mismatches. This is different from the k-differences problem (edit distance). A mismatch implies the substring and the pattern have a different letter in at most k positions. I really only require k=1 (at most 1 mismatch), so a fast algorithm for the specific case of k=1 will also suffice. The alphabet size is 26 (case-insensitive english text), so space requirement should not grow too fast with the size of the alphabet (eg., the FAAST algorithm, I believe, takes space exponential in the size of the alphabet, and so is suitable only for protein and gene sequences).</p>\n\n<p>A dynamic programming based approach will tend to be O(mn) in the worst case, which will be too slow. I believe there are modifications of the Boyer-Moore algorithm for this, but I am not able to get my hands on such papers. I do not have subscription to access academic journals or publications, so any references will have to be in the public domain.</p>\n\n<p>I would greatly appreciate any pointers, or references to freely available documents, or the algorithm itself for this problem. </p>\n', 'Tags': '<algorithms><reference-request><strings><string-metrics><substrings>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-04T09:58:32.560', 'CommentCount': '3', 'AcceptedAnswerId': '4855', 'CreationDate': '2012-09-29T19:47:41.390', 'Id': '4797'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have two strings, where one is a permutation of the other. I was wondering if there is an alternative to <a href="http://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a> where instead of finding the minimum number of substitutions required, it would find the minimum number of translocations required to go from string a to string b.</p>\n\n<p>My strings are always of the same size and I know there are no errors/substitutions.</p>\n\n<p>Example:</p>\n\n<pre><code>1 2 3 4 5\n3 2 5 4 1\n</code></pre>\n\n<p>This would give me two:</p>\n\n<pre><code>3 2 5 4 1 (start)\n-&gt; 3 2 1 4 5 \n-&gt; -&gt; 1 2 3 4 5\n</code></pre>\n\n<p>If this is already implemented in R that would be even better.</p>\n', 'ViewCount': '419', 'Title': 'Alternative to Hamming distance for permutations', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-13T13:08:30.077', 'LastEditDate': '2012-10-12T19:35:10.880', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '6', 'OwnerDisplayName': 'user1357015', 'PostTypeId': '1', 'OwnerUserId': '3174', 'Tags': '<terminology><string-metrics><permutations><edit-distance>', 'CreationDate': '2012-10-12T16:31:30.713', 'Id': '5036'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have two datasets that I am trying to match to one another. One dataset's contents is a subset of the other, but contains typographical errors of varying types and magnitude.</p>\n\n<p>I am applying the following logic to the data linkage process:</p>\n\n<p>Apply regular expressions in an iterative manner with increasing flexibility until a match is found (for example, in one iteration, leave vowels as optional). If two matches are found for one record within one iteration, categorize match as tie and leave unmatched. Apply Python's fuzzy regex to handle scenarios which a rule-based regex can't handle, namely character insertions, deletions, and substitutions within an edit distance of one.</p>\n\n<p>I'm having to discuss this process in detail for specs. However, I'm having trouble deciding how to categorize the process. Would this be considered a deterministic process?</p>\n\n<p>Your help would be appreciated. I do not have a CS background, so I apologize if my question is fairly rudimentary.</p>\n", 'ViewCount': '119', 'Title': 'Distinguishing probabilistic, deterministic, and fuzzy matching methods', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-06T06:58:03.727', 'LastEditDate': '2013-03-06T06:58:03.727', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7160', 'Tags': '<regular-expressions><strings><string-metrics>', 'CreationDate': '2013-03-05T19:59:45.920', 'Id': '10301'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is a sentence with N words.</p>\n\n<p>The words are randomly shuffled.</p>\n\n<p>I have a heuristic algorithm that tries to restore the original order.</p>\n\n<p>I want to evaluate my algorithm on a dataset of several hundred long sentences from the internet. </p>\n\n<p>The main goal of the algorithm is that humans looking at the restored sentence will be able to easily know what the original sentence was. So I can evaluate my algorithm by asking humans to grade the quality of the restored order. However, this is very expensive.</p>\n\n<p>Another option is to count the number of words that are in their correct position in each sentence, and divide by the total number of words in all sentences. However, this is not a good measurement of the quality, because, for example, if the only mistake of my algorithm is that it put the last word first, the resulting sentence will get a score of 0, although it is quite similar to the gold standard, and a human will easily notice this.</p>\n\n<p>A third option is to find the minimum number of word moves that need to be done on the algorithm output in order to achieve the gold standard. However, this seems like a non-trivial task in itself.</p>\n\n<p>Can you suggest a measurement that will be both meaningful and easy to implement?</p>\n', 'ViewCount': '66', 'Title': 'Evaluation metric for an ordering algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-21T01:35:12.557', 'LastEditDate': '2013-03-19T10:25:00.073', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '10674', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<natural-lang-processing><string-metrics>', 'CreationDate': '2013-03-19T08:17:04.963', 'Id': '10613'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If you have a long string of length $n$ and a shorter string of length $m$, what is a suitable recurrence to let you compute all $n-m+1$ <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levevenshtein distances</a> between the shorter string and all substrings of the longer string of length $m$?</p>\n\n<p>Can it in fact be done in $O(nm)$ time?</p>\n', 'ViewCount': '132', 'Title': 'Semi-local Levenshtein distance', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T09:52:43.530', 'LastEditDate': '2013-09-02T09:52:43.530', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8938', 'Tags': '<recurrence-relation><dynamic-programming><strings><string-metrics><edit-distance>', 'CreationDate': '2013-06-30T11:00:35.020', 'Id': '12986'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have many vectors in my database. They are in high dimensions such as:</p>\n\n<ul>\n<li>$v_1$ : $\\langle 23, 23, 1, 33, 103, 219, \\dots \\rangle$</li>\n<li>$v_2$ : $\\langle 92, 83, 1, 33, 239, 192, \\dots \\rangle$</li>\n<li>...</li>\n</ul>\n\n<p>I will use Hamming distance to calculate their difference: The\ndifference between $v_1$ and $v_2$ is $4$ because elements $3$\nand $4$ are the same and others are difference.</p>\n\n<p>Now, I want to use Locality Sensitive Hashing (LSH) to put those\nvectors into different bins.</p>\n\n<blockquote>\n  <p>What kind of hash function can I use for this case?</p>\n</blockquote>\n\n<p>I have read some article about universal hash function, but I am\nnot sure can I use it and how to ensure that the probability for\nthe similar vectors going to the same bin is higher than those\nnon-similar one.</p>\n\n<p>Here is the way that I think how should I use the universal hash\nfunction for my task.</p>\n\n<p>I will first divide those high dimensions vectors into sub-vectors:\n$$x : 23, 23 \\; | \\; 1, 33 \\;  | \\; 103, 219 \\; | \\; \\dots$$</p>\n\n<p>sub1-x : 23 23<br>\nsub2-x : 1 33<br>\nsub3-x : 103 219<br></p>\n\n<p>The following function will be used for each sub-vector:\n$$sum_{i=0}^{r} a_{i}x_{i} \\mod m$$</p>\n\n<p>Basically this is a dot product, a = {a_1, a_2, ... a_i}, x =\n{23, 23, 1, 33, 103, 219 ...}, m is a prime.</p>\n\n<p><ul>\n<li>Different combination of {a} will form a different hash table, one hash table is used for one sub-vector.</li>\n<li>I can now hash the data into bins, <strong>but the question is</strong></p>\n\n<blockquote>\n  <p>Is this an LSH method?  I don't know that two similar vectors  will go into the same bin with a high probability.</li>\n  </ul></p>\n</blockquote>\n", 'ViewCount': '159', 'Title': 'Find similar vector by Locality Sensitive Hashing', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-07-30T08:00:31.173', 'LastEditDate': '2013-07-30T08:00:31.173', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13352', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9238', 'Tags': '<hash><databases><hash-tables><string-metrics>', 'CreationDate': '2013-07-18T14:41:29.760', 'Id': '13334'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How do you find the minimum hamming distance of a code?<br>\nA naive way is computing the distance of each pair of codewords in our code.  </p>\n\n<p>It becomes hard when the code is sufficiently large. Is there a formula for minimum hamming distance? </p>\n', 'ViewCount': '53', 'Title': 'Compute minimum hamming distance of a code', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-30T15:19:23.487', 'LastEditDate': '2014-01-30T15:19:23.487', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '20110', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13229', 'Tags': '<string-metrics><coding-theory>', 'CreationDate': '2014-01-30T13:45:36.770', 'Id': '20105'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Title says it all pretty much. I do realize that often edit distance is defined as the minimum number of operations needed to transform one string to another, but I want something to point to that\'s even more general than that. </p>\n\n<p>I recently saw a brain teaser that claimed that some string was connected in a network to another as "friends" given an edit distance of one. It then went on to claim that friends of friends, friends of friends of friends, etc. (with no given upper bound on the degree of separation), were also in the network of that original string. I can\'t see how this definition of network doesn\'t include every possible string, and so I think the brain teaser is ill-formed. Given sufficient edits, a string can transform to any other string, right? -- but is there a name for that observation? I think this is so fundamental as to be near-impossible to Google, but there is always the distinct possibility of me being an idiot.</p>\n', 'ViewCount': '100', 'Title': 'Is there a basic proof that there exists some edit distance between two strings?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-03T07:54:13.697', 'LastEditDate': '2014-04-03T06:40:20.177', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '23385', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16402', 'Tags': '<strings><string-metrics><edit-distance>', 'CreationDate': '2014-04-03T04:06:27.853', 'Id': '23382'}}