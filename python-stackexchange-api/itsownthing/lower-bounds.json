{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '660', 'Title': 'How to use adversary arguments for selection and insertion sort?', 'LastEditDate': '2012-04-23T06:41:39.370', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '947', 'FavoriteCount': '1', 'Body': '<p>I was asked to find the adversary arguments necessary for finding the lower bounds for selection and insertion sort. I could not find a reference to it anywhere.</p>\n\n<p>I have some doubts regarding this. I understand that adversary arguments are usually used for finding lower bounds for certain "problems" rather than "algorithms".</p>\n\n<p>I understand the merging problem. But how could I write one for selection and insertion sort?</p>\n', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-23T17:27:14.653', 'CommentCount': '4', 'AcceptedAnswerId': '1465', 'CreationDate': '2012-04-23T03:40:40.260', 'Id': '1455'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '175', 'Title': 'Bound on space for selection algorithm?', 'LastEditDate': '2012-07-24T08:41:54.167', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '67', 'FavoriteCount': '2', 'Body': '<p>There is a well known worst case $O(n)$ <a href="http://en.wikipedia.org/wiki/Selection_algorithm" rel="nofollow">selection algorithm</a> to find the $k$\'th largest element in an array of integers.  It uses a <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Properties_of_pivot" rel="nofollow">median-of-medians</a> approach to find a good enough pivot, partitions the input array in place and then recursively continues in it\'s search for the $k$\'th largest element.</p>\n\n<p>What if we weren\'t allowed to touch the input array, how much extra space would be needed in order to find the $k$\'th largest element in $O(n)$ time?  Could we find the $k$\'th largest element in $O(1)$ extra space and still keep the runtime $O(n)$?  For example, finding the maximum or minimum element takes $O(n)$ time and $O(1)$ space.  </p>\n\n<p>Intuitively, I cannot imagine that we could do better than $O(n)$ space but is there a proof of this?</p>\n\n<p>Can someone point to a reference or come up with an argument why the $\\lfloor n/2 \\rfloor$\'th element would require $O(n)$ space to be found in $O(n)$ time?</p>\n', 'Tags': '<algorithms><algorithm-analysis><space-complexity><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-26T18:36:48.300', 'CommentCount': '1', 'AcceptedAnswerId': '2896', 'CreationDate': '2012-07-24T03:19:59.790', 'Id': '2893'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Over at <a href="http://cs.stackexchange.com/questions/3200/counting-inversion-pairs">this question about inversion counting</a>, I <a href="http://cs.stackexchange.com/questions/3200/counting-inversion-pairs#comment8724_3200">found a paper</a> that proves a lower bound on space complexity for all (exact) <a href="https://en.wikipedia.org/wiki/Streaming_algorithm">streaming algorithms</a>. I have claimed that this bound extends to all linear time algorithms. This is a bit bold as in general, a linear time algorithm can jump around at will (random access) which a streaming algorithm can not; it has to investigate the elements in order. I may perform multiple passes, but only constantly many (for linear runtime).</p>\n\n<p>Therefore my question:</p>\n\n<blockquote>\n  <p>Can every linear-time algorithm be expressed as a streaming algorithm with constantly many passes?</p>\n</blockquote>\n\n<p>Random access seems to prevent a (simple) construction proving a positive answer, but I have not been able to come up with a counter example either.</p>\n\n<p>Depending on the machine model, random access may not even be an issue, runtime-wise. I would be interested in answers for these models:</p>\n\n<ul>\n<li>Turing machine, flat input</li>\n<li>RAM, input as array</li>\n<li>RAM, input as linked list</li>\n</ul>\n', 'ViewCount': '469', 'Title': 'Is every linear-time algorithm a streaming algorithm?', 'LastActivityDate': '2012-08-16T18:15:02.147', 'AnswerCount': '4', 'CommentCount': '7', 'AcceptedAnswerId': '3221', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><streaming-algorithm><simulation><lower-bounds>', 'CreationDate': '2012-08-16T08:26:31.460', 'FavoriteCount': '1', 'Id': '3214'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>First, consider this simple problem --- design a data structure of <em>comparable</em> elements that behaves just like a stack (in particular, push(), pop() and top() take constant time), but can also return its min value in $O(1)$ time, without removing it from the stack. This is easy by maintaining a second stack of min values.</p>\n\n<p>Now, consider the same problem, where the stack is replaced by a queue. This seems impossible because one would need to keep track of $\\Theta(n^2)$ values (min values between elements $i$ and $j$ in the queue). True or false ?</p>\n\n<p>Update: $O(1)$ amortized time is quite straightforward as explained in one of the answers (using two min-stacks). A colleague pointed out to me that one can de-amortize such data structures by performing maintenance operations proactively. This is a little tricky, but seems to work.</p>\n', 'ViewCount': '378', 'Title': 'Lower bounds: queues that return their min elements in $O(1)$ time', 'LastEditorUserId': '5189', 'LastActivityDate': '2013-01-04T15:12:38.933', 'LastEditDate': '2012-12-26T18:06:43.007', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '6467', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5189', 'Tags': '<data-structures><priority-queues><lower-bounds>', 'CreationDate': '2012-10-18T05:16:22.060', 'Id': '6146'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>By making use of the fact that sorting $n$ numbers requires \n$\\Omega(n \\log n)$ steps for any optimal algorithm (which uses \'comparison\' for sorting), how can I prove that finding the <a href="http://en.wikipedia.org/wiki/Convex_hull" rel="nofollow">convex-hull</a> of $n$ points is bounded by $\\Omega (n \\log n)$ steps?</p>\n', 'ViewCount': '254', 'Title': 'Lower bound for Convex hull', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-29T18:56:12.077', 'LastEditDate': '2012-10-29T18:56:12.077', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4190', 'Tags': '<complexity-theory><computational-geometry><lower-bounds>', 'CreationDate': '2012-10-29T07:20:12.997', 'Id': '6369'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's start with the comparison sorting lower bound proof, which I'll summarize as follows:</p>\n\n<ol>\n<li>For $n$ distinct numbers, there are $n!$ possible orderings.</li>\n<li>There is only one correct sorted sequence of the $n$ numbers.</li>\n<li>We are given that comparison ($&lt;$) is the only operation we have that can narrow down the $n!$ possible orderings, and each comparison has only two possible outcomes.</li>\n<li>So $\\log_2(n!)$ comparisons are required.</li>\n</ol>\n\n<p>Now consider the following generalization of the argument above:</p>\n\n<ol>\n<li>Define a space of possibilities and its size (in the case of sorting, $n!$ orderings)</li>\n<li>Define the goal state and its size (in this case, only one correctly sorted answer)</li>\n<li>Define the amount of information that is gained at each step of the computation (in this case, one bit since there are only two possible outcomes per comparison)</li>\n<li>Calculate the information difference between the space of possibilities (step 1) and the goal space (step 2) and divide by the information gain per step (step 3) to yield the lower bound on the number of steps (in this case, $(\\log_2(n!)$ - $\\log_2(1))$ / 1 = $\\log_2(n!)$).</li>\n</ol>\n\n<p>Please answer all the following questions. I'm less concerned with the correctness of the particulars of step 4 than I am with the correctness of steps 1 - 3.</p>\n\n<ol>\n<li>Are there any problems with the generalized argument?</li>\n<li>If the problems can be fixed, what are the fixes?</li>\n<li>If the problems can't be fixed, please point out the fatal ones and provide directions to sources which describe these lower-bounds proofs and their pitfalls.</li>\n</ol>\n", 'ViewCount': '274', 'Title': 'Generalizing the Comparison Sorting Lower Bound Proof', 'LastEditorUserId': '19', 'LastActivityDate': '2012-12-23T05:23:41.497', 'LastEditDate': '2012-11-08T16:36:29.557', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1295', 'Tags': '<proof-techniques><sorting><information-theory><check-my-proof><lower-bounds>', 'CreationDate': '2012-11-08T15:48:38.323', 'FavoriteCount': '1', 'Id': '6562'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In  the theory of distributed algorithms, there are problems with lower bounds, as $\\Omega(n^2)$, that are "big" (I mean, bigger than $\\Omega(n\\log n)$), and nontrivial.\nI wonder if are there problems with similar bound in the theory of serial algorithm, I mean of order much greater than $\\Omega(n\\log n)$.</p>\n\n<p>With trivial, I mean "obtained just considering that we must read the whole input" and similarly.</p>\n', 'ViewCount': '172', 'Title': 'Is there any nontrivial problem in the theory of serial algorithms with a nontrivial polynomial lower bound of $\\Omega(n^2)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-24T08:01:48.460', 'LastEditDate': '2013-05-24T08:01:48.460', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1665', 'Tags': '<algorithms><complexity-theory><lower-bounds>', 'CreationDate': '2012-11-18T01:28:50.303', 'FavoriteCount': '1', 'Id': '6732'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the argument made in <a href="http://cs.stackexchange.com/questions/6562/generalizing-the-comparison-sorting-lower-bound-proof">this question</a> based on the comparison sorting lower-bounds proof, which runs as follows. </p>\n\n<p>First, the comparison sorting lower-bounds proof was recited:</p>\n\n<ol>\n<li>For $n$ distinct numbers, there are $n!$ possible orderings.</li>\n<li>There is only one correct sorted sequence of the $n$ numbers.</li>\n<li>We are given that comparison ($&lt;$) is the only operation we have that can narrow down the $n!$ possible orderings, and each comparison has only two possible outcomes.</li>\n<li>So $\\log_2(n!)$ comparisons are required.</li>\n</ol>\n\n<p>Then, the argument was generalized:</p>\n\n<ol>\n<li>Define a space of input possibilities</li>\n<li>Define the goal space, a subset of the input space</li>\n<li>Define the amount of information that is gained at each step of the computation </li>\n<li>Calculate the information difference between the space of possibilities and the goal space and divide by the information gain per step to yield the lower bound on the number of steps.</li>\n</ol>\n\n<p>Both arguments above rely on information arguments to prove a lower bound on the number of steps.</p>\n\n<p>Question: are there proofs and/or proof strategies like the above which: </p>\n\n<ol>\n<li>prove lower bounds,</li>\n<li>define an input space,</li>\n<li>define a goal space which is a subset of the input space, and </li>\n<li>do not rely on information arguments?</li>\n</ol>\n', 'ViewCount': '72', 'Title': 'Proofs based on narrowing down sets of possibilities', 'LastActivityDate': '2012-12-12T08:10:05.657', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7352', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1295', 'Tags': '<proof-techniques><lower-bounds>', 'CreationDate': '2012-12-12T03:46:21.073', 'Id': '7349'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Show that for $l(n) = \\log \\log n$, it holds that $\\text{DSPACE}(o(l)) = \\text{DSPACE}(O(1))$.</p>\n\n<p>It's well known fact in Space Complexity, but how to show it explicitly?</p>\n", 'ViewCount': '285', 'Title': 'Space complexity below $\\log\\log$', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-13T15:09:01.203', 'LastEditDate': '2012-12-13T15:09:01.203', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7378', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4799', 'Tags': '<complexity-theory><regular-languages><space-complexity><lower-bounds>', 'CreationDate': '2012-12-13T10:57:13.230', 'Id': '7372'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a list of unbounded integers, written in binary, and we want to write a (formal) proof that the list is sorted in ascending order.</p>\n\n<p>Such a proof might look (informally) like: "2 &lt; 3, and 3 &lt; 5, and ... and 71 &lt; 79, so the list is sorted."</p>\n\n<p>It would seem such a proof must have length $\\Omega(n)$ where $n$ is the length of the list of integers, as you could use the same kind of argument that is used to show that comparison sorting is $\\Omega(n \\log n)$; roughly, if the proof was any shorter than $n$, it would have missed one of the integers on the list.</p>\n\n<p>Is this the case, or have I missed something?  Is there a clever way to construct an asymptotically shorter proof?</p>\n\n<p><strong>Edit</strong>: As Gilles and Yuval Filmas point out below, a specific answer can only be given if we have some constraints on the language in which the formal proof is written.  For the purposes of this question, suppose that no matter what particular proof language is used, <em>the proof must be written such that it can be verified in time at most polynomial in the length of the proof</em>.  This excludes proofs of the form "for all elements of the list, ..."  (I realize this constraint may make the question more difficult than if a particular proof language was chosen, but it really is closest to what I was thinking when I asked the question.)</p>\n', 'ViewCount': '83', 'Title': 'Lower bound on size of proof that a list of integers is sorted', 'LastEditorUserId': '5049', 'LastActivityDate': '2012-12-23T04:12:52.203', 'LastEditDate': '2012-12-15T09:43:11.183', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5049', 'Tags': '<proof-techniques><lower-bounds><descriptive-complexity>', 'CreationDate': '2012-12-14T18:05:51.623', 'FavoriteCount': '1', 'Id': '7396'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Continuing in the vein of two prior questions <a href="http://cs.stackexchange.com/questions/6562/generalizing-the-comparison-sorting-lower-bound-proof">(1)</a> and <a href="http://cs.stackexchange.com/questions/7349/proofs-based-on-narrowing-down-sets-of-possibilities">(2)</a>, we started with sorting, where we had</p>\n\n<ol>\n<li>a set of $n!$ input possibilities</li>\n<li>a goal space of only one element consisting of the one correct sorted sequence</li>\n</ol>\n\n<p>We then arrived at a generalized argument which I\'ll alter slightly and call <em>A</em>:</p>\n\n<ol>\n<li>Define a space of input possibilities</li>\n<li>Define the goal space, a subset of the input space</li>\n<li>Reason about constraints in order to set a lower bound</li>\n</ol>\n\n<p>Three Questions: given that there are quadratic algorithms for sorting, </p>\n\n<ol>\n<li>Can we conclude for the quadratic sorting algorithms, where we have an input space of $n!$ input possibilities and roughly $n^2$ steps, that there are $O(n!/n^2)$ possibilities removed from the search space per step on average?</li>\n<li>Let\'s say we\'re given a problem formalizable in the form of argument <em>A</em> above where the input space is exponential in terms of the size of the input, $O(2^n)$, and the problem is solvable in polynomial time, $O(n^c)$, for some constant $c$. Can we conclude there are $O(2^n/n^c)$ possibilities removed from the search space per step on average?</li>\n<li>Can we at least conclude that a $O(2^n)$ problem cannot be solved in polynomial time by removing one element from the input space per step?</li>\n</ol>\n', 'ViewCount': '58', 'Title': 'Progress of algorithms in problem spaces', 'LastActivityDate': '2013-01-04T10:38:39.383', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7753', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1295', 'Tags': '<proof-techniques><lower-bounds>', 'CreationDate': '2013-01-04T05:21:31.617', 'Id': '7747'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was studying binomial heaps and its time analysis. Are there any inputs that cause DELETE-MIN, DECREASE-KEY, and DELETE to run in $\\Omega(\\log n)$ time for a binomial heap rather than $O(\\log n)$?</p>\n', 'ViewCount': '243', 'Title': 'Input that causes an operation on a binomial heap to run in $\\Omega(\\log n)$ time?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-01-16T08:30:16.393', 'LastEditDate': '2013-01-16T08:30:16.393', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6438', 'Tags': '<algorithms><data-structures><lower-bounds><heaps>', 'CreationDate': '2013-01-16T02:01:40.263', 'Id': '8955'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question follows on previous questions <a href="http://cs.stackexchange.com/questions/6562/generalizing-the-comparison-sorting-lower-bound-proof">(1)</a>, <a href="http://cs.stackexchange.com/questions/7349/proofs-based-on-narrowing-down-sets-of-possibilities">(2)</a>, where we define an initial space of possibilities and reason about how a solution is chosen from that.</p>\n\n<p>Consider a problem P where we are given:</p>\n\n<ol>\n<li>the initial space of possibilities is exponential in the size of the input,</li>\n<li>the search space decreases monotonically as we read the input, </li>\n<li>for the algorithm that correctly computes P and has <a href="http://courses.csail.mit.edu/6.841/spring09/scribe/lect16.pdf" rel="nofollow">the minimum worst-case performance</a>, the search space is still exponential in the size of the input even after the final input is read.</li>\n</ol>\n\n<p>Can we conclude that P requires exponential time? If not, what would a polynomial-time counterexample look like?</p>\n', 'ViewCount': '110', 'Title': 'Search spaces and computation time', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-18T15:13:27.137', 'LastEditDate': '2013-01-18T15:13:27.137', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1295', 'Tags': '<complexity-theory><proof-techniques><lower-bounds>', 'CreationDate': '2013-01-18T06:25:20.673', 'Id': '9013'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is known that $Parity \\notin AC^0$ (nonuniform), but the proof is rather involved and combinatorial. Are there simpler, but weaker lower bounds, say for $NP \\not \\subseteq AC^0$ or $NEXP \\not \\subseteq AC^0$?</p>\n\n<p>For example, can nontrivial simplifications be obtained in the proof of $NEXP \\not \\subseteq ACC^0$ to deal only with the special case of $AC^0$?</p>\n', 'ViewCount': '110', 'Title': 'Simple lower bounds against AC0', 'LastEditorUserId': '41', 'LastActivityDate': '2013-02-14T22:05:44.687', 'LastEditDate': '2013-02-14T21:44:12.463', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '9786', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '667', 'Tags': '<complexity-theory><lower-bounds><circuits>', 'CreationDate': '2013-02-14T21:39:13.577', 'Id': '9783'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<h3>Background:</h3>\n\n<p>In this question we care only about worst-case running-time.</p>\n\n<p>Array and (doubly) linked lists can be used to keep a list of items and implement the <strong><a href="http://en.wikipedia.org/wiki/Vector_data_structure#Efficiency_comparison_with_other_data_structures">vector</a></strong> abstract data type. Consider the following three operations:</p>\n\n<ul>\n<li>$Location(i)$: returns a pointer to the $i$th item in the list of items in the array.</li>\n<li>$Insert(k,x)$: insert the item $k$ in the list after the item pointed to by $x$.</li>\n<li>$Delete(x)$: remove the item in the list pointed to by $x$.</li>\n</ul>\n\n<p>The main operation that an <strong>array</strong> provides is location which can be computed in constant time. However delete and insert are inefficient.</p>\n\n<p>On the other hand, in a <strong>doubly linked list</strong>, it is easy to perform insert and delete in constant time, but location is inefficient.</p>\n\n<h3>Questions:</h3>\n\n<p>Can there be a data structure to store a list of items where all three operations are $O(1)$? If not, what is the best worst-case running-time that we can achieve for all operations simultaneously? </p>\n\n<p>Note that a balanced binary search tree like red-black trees augmented with size of subtrees would give $O(\\lg n)$, is it possible to do better? Do we know a non-trivial lower-bound for this problem?</p>\n', 'ViewCount': '136', 'ClosedDate': '2013-03-01T18:52:59.140', 'Title': 'Is there a data-structure which is more efficient than both arrays and linked lists?', 'LastActivityDate': '2013-02-26T01:06:49.737', 'AnswerCount': '0', 'CommentCount': '17', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<data-structures><arrays><lower-bounds><linked-lists>', 'CreationDate': '2013-02-26T01:06:49.737', 'Id': '10111'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've found in many exercises where I'm asked to show that $f(n)=\\Theta(g(n))$ where the two functions are of the same order of magnitude I have difficulty finding a constant $c$ and a value $n_0$ for the lower bound.  I'm using Corman's definition of $\\Theta$: </p>\n\n<p>$$\\exists c_1,c_2&gt;0\\in\\mathbb{R}:\\forall n\\geq n_0: 0 \\leq c_1 g(n)\\leq f(n)\\leq c_2 g(n)$$</p>\n\n<p>Showing the upper bound usually doesn't give me too much trouble, but for the lower bound I allot of times find myself using limits.  And even though I'm getting the right answers, I'm a bit worried that my method isn't very rigorous and that maybe I'm doing a bit of hand waving in the process.</p>\n\n<p>For example, problem 2.17 from Skiena's Algorithm Design Manual:</p>\n\n<p>Show that for any $a,b\\in \\mathbb{R}: b&gt;0$ that $(n+a)^b = \\Theta(n^b)$</p>\n\n<p>In this case I used limits to help find both constants.  </p>\n\n<p>For the upper limit I decided to look for some $c$ such that $(n+a)^b \\leq c^bn^b$.  So taking the $b$th root of each side and dividing by $n$ I have $\\frac{n+a}{n}\\leq c$ which gives me $1 + \\frac{a}{n} \\leq c$.  For any $a\\in\\mathbb{R}$, $\\lim_{n\\to\\infty }1+\\frac{a}{n}=1$.  If I pick $n_0&gt;|a|$, then for $a&lt;0$ the expression approaches 1 from the left starting arbitrarily close to $0$.  If $a&gt;0$ then the expression approaches 1 from the right starting arbitrarily close to 2. So choosing $c=2$ will satisfy the inequality and we have $c_2=2^b$.</p>\n\n<p>Now for the lower bound.  I'm looking at the same expression except with the inequality pointing the other way.  In this case I'm trying to find $n_0$ and $c$ such that $c\\leq 1+\\frac{a}{n}$.  The value of $n_0$ has to be greater than $|a|$ because otherwise we would have $c\\leq 0$ which isn't allowed.  This puts us in the same range of values between $0$ and $2$ approaching 1 from each side.  So I choose any $c,n_0$ such that $n_0&gt;|a|$ and $0 &lt; c\\leq 1-|\\frac{a}{n_0}|$.  So I could choose $n_0=3|a|$ and $c=\\frac{2}{3}$.  </p>\n\n<p>Thus we have $0 &lt; (\\frac{2}{3})^bn^b \\leq (n+a)^b \\leq 2^bn^b$ for any $n \\geq 3|a|$.</p>\n\n<p><strong>Is there an easier way to do this?</strong>  </p>\n\n<p>Normally when looking for upper limit constants where the two functions are of the same magnitude I simply eliminate negative lower order terms and change positive ones into multiples of the highest order term such as : </p>\n\n<p>$$3n^2+15n-5\\leq 3n^2+15n^2=18n^2$$</p>\n\n<p>But when looking for the constant for the lower bound I find myself typically resorting to looking at limits.  <strong>Is there any kind of short cut to finding the lower bound constant like there is for the upper bound constant?</strong></p>\n", 'ViewCount': '216', 'Title': 'Methods for Finding Asymptotic Lower Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-15T00:40:06.203', 'LastEditDate': '2013-03-14T14:32:45.263', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10512', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<asymptotics><landau-notation><lower-bounds>', 'CreationDate': '2013-03-13T18:57:03.363', 'Id': '10511'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It seems that (in a broad sense) two approaches can be utilized to produce an algorithm for solving various optimization problems: </p>\n\n<ol>\n<li>Start with a feasible solution and expand search until constraints are tight and solution is maximal (or minimal).  </li>\n<li>Begin with violated constraints and search for maximal (or minimal) feasible approach.</li>\n</ol>\n\n<p>For the Max-Flow problem Ford-Fulkerson satisfies condition (1), while Push-Relabel satisfies condition (2). An interesting point is that Push-Relabel is a more efficient algorithm than Ford-Fulkerson. My question is this:</p>\n\n<blockquote>\n  <p>What other examples are there where (2)-based approaches outperform their (1)-based counterparts?</p>\n</blockquote>\n\n<p>A follow up is:</p>\n\n<blockquote>\n  <p>Do there exist meta-theorems regarding approaches based on condition (2)? </p>\n</blockquote>\n', 'ViewCount': '106', 'Title': 'Constraint violation and efficiency in search', 'LastEditorUserId': '19', 'LastActivityDate': '2013-03-19T00:26:53.147', 'LastEditDate': '2013-03-15T19:22:46.253', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10530', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '19', 'Tags': '<algorithms><optimization><lower-bounds>', 'CreationDate': '2013-03-15T02:46:32.423', 'Id': '10528'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given [[1,4,7],[2,5,8],[3,6,9]] which is a list of the column vectors of matrix</p>\n\n<pre><code>|1, 2, 3|\n|4, 5, 6|\n|7, 8, 9|\n</code></pre>\n\n<p>is $ \\Omega(n^2) $ a lower bound for transposing? Assume the matrix is not always square. I have to touch each element at least once, because going from 2 x 5 to 5 x 2 matrix for example, will mean going from a list of 5 lists to a list of 2 lists, so I can't really do any tricks with the array indices, right?</p>\n\n<p>Is there a faster way to transpose matrices?</p>\n", 'ViewCount': '88', 'Title': 'Complexity of transposing matrices represented as list of row or column vectors', 'LastEditorUserId': '7362', 'LastActivityDate': '2013-03-22T21:57:06.330', 'LastEditDate': '2013-03-22T21:57:06.330', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '10672', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7341', 'Tags': '<algorithms><lower-bounds><matrices>', 'CreationDate': '2013-03-20T05:13:01.430', 'Id': '10636'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '218', 'Title': 'Is detecting "doubly" arithmetic progressions 3SUM-hard?', 'LastEditDate': '2013-03-21T10:36:09.113', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '7362', 'FavoriteCount': '2', 'Body': '<p>This is inspired by an <a href="http://www.careercup.com/question?id=15877680">interview question</a>.</p>\n\n<p>We are given an array of integers $a_1, \\dots, a_n$ and have to determine if there are distinct $i \\lt j \\lt k$ such that</p>\n\n<ul>\n<li>$a_k - a_j = a_j - a_i$</li>\n<li>$k - j = j - i$</li>\n</ul>\n\n<p>i.e, the sequences $\\{a_i, a_j, a_k\\}$ and $\\{i,j,k\\}$ are both in arithmetic progression.</p>\n\n<p>There is an easy $O(n^2)$ algorithm for this, but finding a sub-quadratic algorithm seems elusive.</p>\n\n<p>Is this a known problem? Can we prove 3SUM-hardness of this? (or maybe provide a sub-quadratic algorithm?)</p>\n\n<p>If you like, you can assume $0 \\lt a_1 \\lt a_2 \\lt ... \\lt a_n$ and that $a_{r+1} - a_{r} \\le K$ for some known constant $K &gt; 2$. (In the interview problem, $K = 9$).</p>\n', 'Tags': '<algorithms><complexity-theory><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-24T03:39:07.830', 'CommentCount': '0', 'AcceptedAnswerId': '10725', 'CreationDate': '2013-03-21T07:56:00.003', 'Id': '10681'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given $n$ arrays of size $k$ each, we want to show that at least $\\Omega(nk \\log k)$ comparisons are needed to sort all arrays (indepentent of each other). </p>\n\n<p>My proof is a simple modification of the decision tree argument used to obtain the lower bound for comparison-based sorting of one array. More specifically, I argue that there are in total $k!^n$ possible permutations for the entries in all given arrays, and that a binary tree with that number of leaves is of height $h \\in \\Omega(nk \\log k)$. Is that argument correct? </p>\n\n<p>Furthermore, I was told that merely observing that one needs $\\Omega(k \\log k)$ comparisons for each of the arrays and we need to sort $n$ times in total (for $n$ arrays) is <em>not</em> a sufficient argument. Why is that? My answer would be that this is just <em>one</em> possible approach to this problem, and not a general argument excluding each and every other potential comparison-based algorithm for solving the given task with less than $\\Omega(nk \\log k)$ comparisons. \nHowever, this is not particularly concise and I would consider a rather technical argument (which I don't see) as more appropriate. What would that be?</p>\n", 'ViewCount': '132', 'Title': 'Lower bound for sorting n arrays of size k each', 'LastEditorUserId': '7486', 'LastActivityDate': '2013-03-29T13:24:00.140', 'LastEditDate': '2013-03-29T13:24:00.140', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10893', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7486', 'Tags': '<algorithms><sorting><arrays><lower-bounds><check-my-proof>', 'CreationDate': '2013-03-29T12:07:19.733', 'Id': '10890'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For every integer $t$, is there a problem whose solutions can be verified in $O(n^{s})$ time but cannot be found in $O(n^{st})$ time?</p>\n\n<p>By verifying, I mean that given a candidate solution $y$, we can judge whether $y$ is correct or not in time $O(n^s)$.</p>\n', 'ViewCount': '401', 'Title': 'A Problem on Time Complexity of Algorithms', 'LastEditorUserId': '72', 'LastActivityDate': '2013-05-09T22:53:07.350', 'LastEditDate': '2013-05-09T14:35:06.697', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8074', 'Tags': '<complexity-theory><time-complexity><asymptotics><complexity-classes><lower-bounds>', 'CreationDate': '2013-05-07T01:48:11.327', 'Id': '11844'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Firstly, let's suppose there exists an algorithm where $i$ iterates from $1$ to $n$, spending $\\frac{n^2}{i}$ time in each iteration.</p>\n\n<p>Thanks to the well known $O(\\log n)$ upper bound for the Harmonic series, the big $O$ notation of this algorithm comes to $O(n^2 \\log n)$.</p>\n\n<p>Now the actual algorithm that I am working on iterates from $k$ to $n$, where $1 &lt; k &lt; n$, spending $\\frac{n^2}{k}$ in the first iteration, then $\\frac{n^2}{k+1}$, $\\frac{n^2}{k+2}$, $\\frac{n^2}{k+3}$, and so on.</p>\n\n<p>I've been trying to deduce the upper bound of this new algorithm, but I keep getting the same $O(n^2 \\log n)$ time bound, which to me, is counter intuitive and a bit paradoxical, especially considering that the first terms of the harmonic series are the bigger and more significant terms. The time bound that I was actually expecting to get was $O(\\frac{n^2}{k} \\cdot \\log n)$.</p>\n\n<p>Could anyone please be able to shed some light onto this?</p>\n\n<p>Thanks in advance for any help.</p>\n", 'ViewCount': '997', 'Title': 'Time complexity in Big O notation for Harmonic series with first k terms missing', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-09T02:03:16.153', 'LastEditDate': '2013-05-08T23:25:51.343', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'wookie919', 'PostTypeId': '1', 'OwnerUserId': '6850', 'Tags': '<time-complexity><lower-bounds>', 'CreationDate': '2013-05-08T01:11:39.233', 'Id': '11895'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm facing some problems that deal with finding common elements between unsorted arrays and I'd like to know whether there are well-known lower-bounds for the <em>worst-case</em> and, eventually, what are these lower-bounds.</p>\n\n<p>The problems are pretty simple:</p>\n\n<blockquote>\n  <p>Given two unsorted arrays of distinct integers, $A$ and $B$, of size\n  $m$ and $n$, determine <em>all</em> the common elements between the two\n  arrays(the output must be sorted)</p>\n</blockquote>\n\n<p>Which I believe has a lower-bound complexity of $\\Omega((m+n)\\log(\\min(m,n)))$\n(and $O(1)$ space complexity, if we exclude the input arrays),\neven though I cannot find any proof of this fact. [I found an algorithm with this complexity, so I(hopefully) am not underestimating the complexity.]</p>\n\n<p>The other problem adds some restrictions(and hence I'd expect to be able to lower the complexity):</p>\n\n<blockquote>\n  <p>Given two unsorted arrays of distinct integers, $A$ and $B$, of size\n  $m$ and $n$; knowing that common elements between the arrays have the\n  same relative order in both arrays and that for every couple of\n  consecutive common elements, their distance is at most $k$(constant),\n  determine all the common elements between the two arrays maintaining\n  their relative order and using at most $O(k)$ memory in addition to\n  the input arrays.</p>\n</blockquote>\n\n<p>I've tried to think about this second problem but I cannot see how the restrictions change the complexities. What puzzles me is that I believe that finding a single couple of elements between two unsorted arrays is $\\Theta((n+m)\\log(\\min(n,m)))$, and since this is a special instance of this second problem then the restrictions do not add anything to the problem itself.</p>\n\n<p>Are my guesses correct, and if so where can I find proofs for these lower-bounds? Do the restrictions change anything at all or the solution for the first problem is the best we can achieve in both cases?</p>\n\n<hr>\n\n<p>Probably my questions can be summarized by the following:</p>\n\n<blockquote>\n  <p>Given two arrays of distinct integers $A$ and $B$ of size $m$ and $n$,\n  what is the lower-bound complexity for finding <em>one</em> common element?</p>\n</blockquote>\n\n<p>Because, once a common element is found, the second problem can be solved in linear time.</p>\n\n<hr>\n\n<p><strong>Edit</strong></p>\n\n<p>The algorithm(s) that I thought are pretty simple:</p>\n\n<p>For problem 1: Build two heaps for $A$ and $B$(which takes linear time and can be done in-place), then compare the minimum of the heaps, if they match print it and remove both, otherwise remove the smallest one and continue until the heaps are empty(this clearly takes $O((m\\log(m) + n\\log(n)) = O(\\max(m,n)\\log(\\max(m,n)))$).</p>\n\n<p>An other solution is to sort one array in-place, scan the other array and use bisection search to find matches. If we sort the smallest array (with size $m$) then the complexity is $O(m\\log(m) + n\\log(m)) = O(\\max(m,n)\\log(\\min(m,n)))$. This technique yields the values in the other in which their are found in the biggest array, and it can be used to solve the second problem.</p>\n\n<p>But, as you can see, I'm not using the extra restrictions at all and both algorithms use $O(1)$ space instead of $O(k)$(yes, it's still constant but $O(k)$ should give a bit more freedom).</p>\n", 'ViewCount': '325', 'Title': 'Lower-bound complexities for finding common elements between two unsorted arrays', 'LastEditorUserId': '7246', 'LastActivityDate': '2013-05-22T15:04:04.710', 'LastEditDate': '2013-05-22T15:04:04.710', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12210', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7246', 'Tags': '<time-complexity><arrays><lower-bounds>', 'CreationDate': '2013-05-21T12:03:27.047', 'Id': '12182'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following problem:</p>\n\n<blockquote>\n  <p>Give an algorithm to find the $1^{st}, 2^{nd}, 3^{th}$ fastest horses from 25 horses. In each round, at most 5 horses can race and you can get the exact position of these horses. Analyze the lower bound of this problem using <em>adversary argument</em>. One race is considered as one critical operation.</p>\n</blockquote>\n\n<p>I can figure out a solution using 7 races:</p>\n\n<blockquote>\n  <ol>\n  <li>Divide 25 horses into 5 groups with 5 horses for each: $A: a_1, a_2, a_3, a_4, a_5$; $B: b_1, b_2, b_3, b_4, b_5$; $C: c_1, c_2, c_3, c_4, c_5$; $D: d_1, d_2, d_3, d_4, d_5$; and $E: e_1, e_2, e_3, e_4, e_5$.</li>\n  <li>One race within each group. Suppose that the position of each horse in each group is consistent with its index: e.g., $A: a_1 &gt; a_2 &gt; a_3 &gt; a_4 &gt; a_5$.</li>\n  <li>One race for $a_1, b_1, c_1, d_1, e_1$ and get $a_1 &gt; b_1 &gt; c_1 &gt; d_1 &gt; e_1$. Thus, $a_1$ is the fastest horse.</li>\n  <li>The second and third ones are among $a_2, a_3, b_1, b_2, c_1$. So one more race is enough.</li>\n  </ol>\n</blockquote>\n\n<p>However I have difficulty in analyzing its lower bound using adversary argument.\nSo my problem is:</p>\n\n<blockquote>\n  <p>How to analyze its lower bound using <em>adversary argument</em>? What is the adversary strategy?</p>\n</blockquote>\n', 'ViewCount': '219', 'Title': 'How to analyze the lower bound of the horse racing problem using adversary argument?', 'LastActivityDate': '2013-10-25T07:05:30.847', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16420', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithms><algorithm-analysis><lower-bounds>', 'CreationDate': '2013-10-25T06:05:44.517', 'Id': '16417'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Cutting problems are problems where a certain large object should be cut to several small objects. For example, imagine you have a factory that works with large boards of raw glass, of width W and length L. There are several buyers, each of which wants an unbounded number of small glass boards. Buyer $i$ wants boards of length $l_i$ and width $w_i$. Your goal is to cut small boards from the large board, such that the total used is maximized and the waste is minimized (There are also <a href="http://www.citeulike.org/user/erelsegal-halevi/article/1695945">other types of cutting and packing problems</a>).</p>\n\n<p>One common restriction in cutting problems is, that the cuts must be <strong>guillotine cuts</strong>, i.e., each existing rectangle can be cut only to two smaller rectangles; it is impossible to make L-shapes etc. Obviously, the maximum used area with guillotine cuts might be smaller than the maximum used area without restriction.</p>\n\n<p>My question is: <strong>Are there upper and lower bounds on the ratio between the optimal guillotine cut and the optimal general cut?</strong></p>\n\n<p>Related work: <a href="http://www.citeulike.org/user/erelsegal-halevi/article/7153709">Song et al. (2009)</a> describe an algorithm that uses a restricted type of guillotine cuts - <em>twice-guillotine cuts</em>. They prove, using geometric constraints, that the ratio between the maximum twice-guillotine cut to the maximum guillotine cut is bounded by <strong>$\\frac{6}{7}$</strong>. I am looking for a comparable result about the ratio between the maximum guillotine cut to the maximum general cut.</p>\n', 'ViewCount': '61', 'Title': 'guillotine cuts versus general cuts', 'LastEditorUserId': '1342', 'LastActivityDate': '2013-10-29T07:48:31.090', 'LastEditDate': '2013-10-29T07:48:31.090', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<reference-request><computational-geometry><lower-bounds><packing>', 'CreationDate': '2013-10-29T07:40:13.090', 'Id': '16533'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we need to find a tight asymptotic bound on the worst case run time of the following program</p>\n\n<pre><code>t = 0;\nfor i = 0 to n-1 do\n  for j = i+1 to n-1 do\n    for k = j+1 to n-1 do\n      t++;\n</code></pre>\n\n<p>I can't for the life of me figure out what the run-time would be. I have a feeling it would have n! in there somewhere, but I can't seem to wrap my head around it. All help is appreciated</p>\n", 'ViewCount': '144', 'Title': 'Worst run-time for 3 nested loop', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-11T11:29:27.627', 'LastEditDate': '2013-12-11T11:16:01.677', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12007', 'Tags': '<asymptotics><runtime-analysis><lower-bounds>', 'CreationDate': '2013-12-11T02:10:13.197', 'FavoriteCount': '1', 'Id': '18857'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>An adversary gives you a set of items whose total size is $x$ (he gets to choose how $x$ is distributed. e.g. there may be $k-1$ items of size $\\frac{x}{k}$ and 2 items of size $\\frac{x}{2k}$).</p>\n\n<p>The item are now randomly distributed into $2x$ bins (you may assume $2x\\in \\mathbb{N}$).</p>\n\n<p>What\'s the probability (i.e. what can we guarantee to achieve for any adversarial choice)  no bin contains items with total size > 1?</p>\n\n<hr>\n\n<p>For example, if the adversary chose all items (except for the last one) to be of size $\\frac{1}{2} + \\epsilon$ , then we have $2x-1$ items that no two of them can fit in a single bin. The last item can fit everywhere. hence the probability is bounded by (relaxing to $2x-1$ bins):</p>\n\n<p>$\\frac{(2x-1)!}{(2x-1)^{2x-1}} &lt; e^{-(2x-1)}$ .</p>\n\n<p>On the other hand, all I know for a general item set is that a "good" coloring exist (easy to see using the first fit algorithm).</p>\n\n<p>Any ideas?</p>\n', 'ViewCount': '27', 'Title': 'Adversarial bin packing', 'LastEditorUserId': '12486', 'LastActivityDate': '2013-12-31T09:37:08.750', 'LastEditDate': '2013-12-31T09:37:08.750', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12486', 'Tags': '<combinatorics><probability-theory><lower-bounds><knapsack-problems><packing>', 'CreationDate': '2013-12-31T09:24:16.570', 'Id': '19398'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there a lower bound on the running time for solving 3-SAT if P = NP.  For instance, is it known that 3-SAT can't be solved in linear time?  What about quadratic?</p>\n", 'ViewCount': '112', 'Title': 'Lower bound on running time for solving 3-SAT if P = NP', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-16T13:38:40.363', 'LastEditDate': '2014-01-15T23:06:52.207', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19759', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8526', 'Tags': '<complexity-theory><satisfiability><lower-bounds>', 'CreationDate': '2014-01-15T21:59:30.103', 'Id': '19756'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If we were to intuitively construct a lower bound for searching an element in a list $A$ containing $n$ integers, it would be in $\\Omega(n)$.</p>\n\n<p>But with the decision tree model, the number of leafs is $n$, so we conclude that the lower bound is $\\Omega(\\log{n})$.</p>\n\n<p>This is the same as finding the maximum element in a list. Intuitively, it is in $\\Omega(n)$, but with the decision tree model it is $\\Omega(\\log{n})$.</p>\n\n<p>Can someone help me understand this discrepancy ?</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '17', 'Title': 'Doubt in the correctness of decision tree models for constructing a lower bound', 'LastActivityDate': '2014-02-27T19:13:00.927', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22104', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><search-algorithms><decision-problem><trees><lower-bounds>', 'CreationDate': '2014-02-27T17:39:15.747', 'Id': '22099'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In the words of (<a href="http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf" rel="nofollow">http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf</a>, section 17.2), "Each $f(x)$ can be interpreted as de\ufb01ning a hyperplane in $R^n$. Thus, tracing a path through the tree computes the intersection of the half-planes de\ufb01ned by the nodes touched by the path."</p>\n\n<p>I fail to visualize how path tracing is done? I would be glad to see it explained through the presentation of the path in a 2-dimensional space.</p>\n\n<p>I do understand that $x_1,x_2,...,x_n$ is a point in the $R^n$ dimensional space. But I don\'t get how Figure 17.1 in (<a href="http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf" rel="nofollow">http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf</a>) helped in proving the lower bounds of Element Uniqueness as $\\Omega(n\\log n)$. I also don\'t get the implication of $\\#F$ being connected components; why can\'t they simply be called solutions?</p>\n\n<p>Unfortunately, reading online resources did not help me much understand the aforementioned concepts.</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '34', 'Title': 'Help in geometrically understanding "Linear Decision Trees"', 'LastEditorUserId': '15072', 'LastActivityDate': '2014-02-27T22:05:37.933', 'LastEditDate': '2014-02-27T19:22:01.350', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '22113', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><search-trees><lower-bounds>', 'CreationDate': '2014-02-27T19:09:05.800', 'Id': '22102'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For a class project we're developing a software that solves a common optimisation problem. After some research we've found out that our problem is called QAP (Quadratic Asssignment Problem) and the algorithm that is commonly used is Branch and Bound. I understand the basics of the problem and I see the need of a lower bound to compute the sollution. I came up with a trivial bound example but our teacher told us lower bounds were no trivial matter and we should do some research. After a while we've found out that the Gilmore-Lawler bound is a good one to solve our problem (or at least good enough for learning purposes). </p>\n\n<p>Although I have read a couple of papers I can't get the grasp of it. The idea seems to be to convert the QAP into an LAP combining the two matrices of the original problem. I've got completely lost after that. How is the number I'm supposed to find as the lower bound calculated? </p>\n\n<p>Also, I'm aware that the lower bound has to be calculated for partial solutions, but how do I do that? The lower bound, as I understood, it's calculated from the program's matrices, which are a parameter for the branch and bound algorithm and are fixed from the beginning aren't they? I'd also need an explanation for that.</p>\n", 'ViewCount': '110', 'Title': 'Trying to understand the Gilmore-Lawler lower bound', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-28T12:34:17.467', 'LastEditDate': '2014-03-28T12:34:17.467', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12801', 'Tags': '<algorithms><optimization><heuristics><lower-bounds><branch-and-bound>', 'CreationDate': '2014-03-27T17:56:59.077', 'Id': '23146'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is a basic result in circuit complexity that says:</p>\n\n<blockquote>\n  <p>There exists a language that cannot be solved with circuits of size $o(\\frac{2^n}{n})$.</p>\n</blockquote>\n\n<p>The argument is a simple counting argument on the number of boolean functions and the number of distinct circuits.  See, for example, <a href="http://theory.stanford.edu/~trevisan/cs254-14/lecture03.pdf">these lecture notes</a>.</p>\n\n<p>I believe it is unknown whether or not this bound is tight.  That is, we don\'t know if the following statement is true:</p>\n\n<blockquote>\n  <p>Every language can be solved with circuits of size $O(\\frac{2^n}{n})$.</p>\n</blockquote>\n\n<p>If this statement were true, would it have any interesting implications for complexity theory?</p>\n', 'ViewCount': '33', 'Title': 'Implications of the $\\Omega(\\frac{2^n}{n})$ circuit lower bound being tight', 'LastActivityDate': '2014-04-02T21:09:54.020', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23369', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '16172', 'Tags': '<complexity-theory><lower-bounds><circuits>', 'CreationDate': '2014-04-02T20:16:50.513', 'FavoriteCount': '1', 'Id': '23365'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let me preface this question by giving some helpful <a href="http://lcm.csa.iisc.ernet.in/dsa/node187.html" rel="nofollow">background material</a>. </p>\n\n<p>I\'m trying to solve the traveling salesman problem using branch and bound. Concretely, for a partial solution, I\'m using the solution the algorithm would attain by making greedy choices as an upper bound, and comparing this bound to the current best solution. If the bound attained using this heuristic is better than the cost of the best known solution, we continue exploring the partial solution. Otherwise, we prune and discard. Note that the greedy solution attained is always valid.</p>\n\n<p>The above link mentions a method for finding a global lower bound. This is fairly intuitive: for each node, we add to the bound the sum of the two minimum cost edges to other nodes (representing the cheapest in/out edges, if you will) and divide by two to avoid double counting. The solution implied by this global bound is almost never a valid solution but instead gives a hard limit on the lowest cost we could ever hope to attain. </p>\n\n<p>Here\'s how I\'m currently trying to calculate a local lower bound (a lower bound for a partial solution) with cost $C(S)$.</p>\n\n<ul>\n<li>Set <code>bound</code> to the current accumulated cost, $C(S)$</li>\n<li>For each unvisited node, add to <code>bound</code> the sum of the two least cost edges, making sure that the edges selected don\'t lead to a node already visited</li>\n<li>return <code>bound/2</code></li>\n</ul>\n\n<p>Here\'s my question: How is the above solution different from the greedy bound? Maybe a better lower bound would be to add to <code>bound</code> the sum of the two least cost edges, paying <strong>no attention to whether these edges lead to nodes already visited</strong>. Also, how do I actually use this bound to prune the search space? Do I compare the local lower bound computed for a partial solution with the global lower bound, pruning if $L(S) &lt; GL$ or if $L(S) &gt; C(B)$, or if $L(S) &gt; L(B)$? (here $GL$ corresponds to the global lower bound; $L(S)$ computes the local lower bound on a partial solution; $C(B)$ represents the trip cost on the best known solution; $L(B)$ is the lower bound on the best solution). Also, how will $L(S)$ ever be less than the global lower bound (i.e. an infeasible solution) if we only ever use (valid) edges to cities we haven\'t yet visited? </p>\n\n<p>Comments and suggestions on using this lower bound as well as its efficacy would be very much appreciated. Thank you. </p>\n', 'ViewCount': '70', 'Title': 'Traveling Salesman: how to use a lower bound?', 'LastEditorUserId': '16389', 'LastActivityDate': '2014-04-08T08:07:23.937', 'LastEditDate': '2014-04-08T03:01:07.293', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16389', 'Tags': '<complexity-theory><optimization><lower-bounds><traveling-salesman><branch-and-bound>', 'CreationDate': '2014-04-07T17:34:38.440', 'Id': '23520'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we have an $N \\times N \\times N$ 3-d sorted array meaning that every row,column, and file is in sorted order. Searching for an element in this structure can be done using $O(N^2)$ comparisons. However are $\\Omega(N^2)$ comparisons needed in the worst-case? For an $N \\times N$ 2-d sorted array I recall a proof that $\\Omega(N)$ comparisons are needed; I'm having trouble seeing how to extend to the 3-d case though</p>\n", 'ViewCount': '24', 'Title': 'Lower bound on number of comparisons needed to search for a number in a sorted 3-d array', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-14T18:45:49.407', 'LastEditDate': '2014-04-14T18:28:24.510', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23793', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><arrays><lower-bounds>', 'CreationDate': '2014-04-14T17:58:35.807', 'Id': '23791'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The 1-D <strong>distinct</strong> closest pair of points problem is as follows: Given a set of n <strong>distinct integer</strong> points on real line, find a pair of points with the smallest distance between them, here the distance between two points p_i and p_j is absolute difference of their values, i.e, |p_i-p_j|. </p>\n\n<p>A naive way to solve this problem is to sort the points and check distance of each consecutive points in sorted order and take the minimum of such distances and this takes O(nlog n) time.</p>\n\n<p>My question is can we do better than that? Can we solve it in o(n log n)(note the little-oh) time? If not, then how to show an omega(nlog n) time bound for this problem?</p>\n\n<p>Note that, if the <strong>distinctness</strong> criteria was not there we could have shown a lower bound using Element Distinctness Problem.</p>\n', 'ViewCount': '20', 'Title': 'Linearithmic lower bound for 1-D "distinct" closest pair of points problem', 'LastActivityDate': '2014-04-25T20:06:05.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24116', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '15697', 'Tags': '<algorithms><data-structures><sorting><lower-bounds>', 'CreationDate': '2014-04-25T19:47:38.497', 'Id': '24115'}}