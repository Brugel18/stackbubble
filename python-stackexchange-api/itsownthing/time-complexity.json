{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '4932', 'Title': 'Evaluating the average time complexity of a given bubblesort algorithm.', 'LastEditDate': '2012-03-07T18:26:03.053', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '78', 'FavoriteCount': '1', 'Body': '<p>Considering this pseudo-code of a bubblesort:</p>\n\n<pre><code>FOR i := 0 TO arraylength(list) STEP 1  \n    switched := false\n    FOR j := 0 TO arraylength(list)-(i+1) STEP 1\n        IF list[j] &gt; list[j + 1] THEN\n            switch(list,j,j+1)\n            switched := true\n        ENDIF\n    NEXT\n    IF switched = false THEN\n        break\n    ENDIF\nNEXT\n</code></pre>\n\n<p>What would be the basic ideas I would have to keep in mind to evaluate the average time-complexity? I already accomplished calculating the worst and best cases, but I am stuck  deliberating how to evaluate the average complexity of the inner loop, to form the equation. </p>\n\n<p>The worst case equation is:</p>\n\n<p>$$\r\n\\sum_{i=0}^n \\left(\\sum_{j=0}^{n -(i+1)}O(1) + O(1)\\right) = O(\\frac{n^2}{2} + \\frac{n}{2}) = O(n^2)\r\n$$</p>\n\n<p>in which the inner sigma represents the inner loop, and the outer sigma represents the outer loop. I think that I need to change both sigmas due to the "if-then-break"-clause, which might affect the outer sigma but also due to the if-clause in the inner loop, which will affect the actions done during a loop (4 actions + 1 comparison if true, else just 1 comparison).</p>\n\n<p>For clarification on the term average-time: This sorting algorithm will need different time on different lists (of the same length), as the algorithm might need more or less steps through/within the loops until the list is completely in order. I try to find a mathematical (non statistical way) of evaluating the average of those rounds needed. </p>\n\n<p>For this I expect any order to be of the same possibility.</p>\n', 'Tags': '<algorithms><time-complexity><average-case>', 'LastEditorUserId': '12', 'LastActivityDate': '2013-01-30T17:00:13.023', 'CommentCount': '16', 'AcceptedAnswerId': '26', 'CreationDate': '2012-03-06T20:51:24.880', 'Id': '20'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '318', 'Title': 'Clever memory management with constant time operations?', 'LastEditDate': '2012-03-06T22:50:41.743', 'AnswerCount': '3', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '68', 'FavoriteCount': '1', 'Body': u"<p>Let's consider a memory segment (whose size can grow or shrink, like a file, when needed) on which you can perform two basic memory allocation operations involving fixed size blocks:</p>\n\n<ul>\n<li>allocation of one block</li>\n<li>freeing a previously  allocated block which is not used anymore.</li>\n</ul>\n\n<p>Also, as a requirement, the memory management system is not allowed to move around currently allocated blocks: their index/address must remain unchanged.</p>\n\n<p>The most naive memory management algorithm would increment a global counter (with initial value 0) and use its new value as an address for the next allocation.\nHowever this will never allow to shorten the segment when only a few allocated blocks remain.</p>\n\n<p>Better approach: Keep the counter, but maintain a list of deallocated blocks (which can be done in constant time) and use it as a source for new allocations as long as it's not empty.</p>\n\n<p>What next? Is there something clever that can be done, still with constraints of constant time allocation and deallocation, that would keep the memory segment as short as possible?</p>\n\n<p>(A goal could be to track the currently non-allocated block with the smallest address, but it doesn't seem to be feasible in constant time\u2026)</p>\n", 'Tags': '<time-complexity><memory-allocation><operating-systems>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-07T03:21:22.457', 'CommentCount': '9', 'AcceptedAnswerId': '69', 'CreationDate': '2012-03-06T21:46:27.713', 'Id': '27'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3710', 'Title': 'The time complexity of finding the diameter of a graph', 'LastEditDate': '2012-08-08T14:16:53.283', 'AnswerCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '51', 'FavoriteCount': '2', 'Body': '<blockquote>\n  <p>What is the time complexity of finding the diameter of a graph\n  $G=(V,E)$?</p>\n  \n  <ul>\n  <li>${O}(|V|^2)$</li>\n  <li>${O}(|V|^2+|V| \\cdot |E|)$</li>\n  <li>${O}(|V|^2\\cdot |E|)$</li>\n  <li>${O}(|V|\\cdot |E|^2)$</li>\n  </ul>\n</blockquote>\n\n<p>The diameter of a graph $G$ is the longest distance between two vertices in graph.</p>\n\n<p>I have no idea what to do about it, I need a complete analysis on how to solve a problem like this.</p>\n', 'Tags': '<algorithms><time-complexity><graph-theory>', 'LastEditorUserId': '72', 'LastActivityDate': '2013-03-23T22:43:49.053', 'CommentCount': '4', 'AcceptedAnswerId': '213', 'CreationDate': '2012-03-10T12:24:48.097', 'Id': '194'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Let's take as an example the 3d \u2192 2d reduction: What's the cost of simulating a 3d cellular automaton by a 2d cellular automaton?</p>\n\n<p>Here is a bunch of more specific questions:</p>\n\n<ol>\n<li><p>What kind of algorithms will have their time complexity changed, by how much?</p></li>\n<li><p>What would be the basic idea for the encoding; how is a 3d grid efficiently (or not efficiently\u2026) mapped to a 2d grid? (The challenge seems to achieve communication between two cells that where originally neighbors on the 3d grid, but are not neighbors anymore on the 2d grid). </p></li>\n<li><p>In particular, I'm interested in the complexity drift for exponential complexity algorithms (which I guess remains exponential whatever the dimension, is it the case?)</p></li>\n</ol>\n\n<p>Note: I'm not interested in low complexity classes for which the chosen I/O method has an influence on complexities. (Maybe the best is to assume that the I/O method is dimensionless: done locally on one specific cell during a variable amount of time steps.) </p>\n\n<hr>\n\n<p><em>Some context: I'm interested in parallel local graph rewriting, but those graphs are closer to 3d (or maybe \u03c9d\u2026) grids than to 2d grids, I'd like to know what to expect of a hardware implementation on a 2-dimentional silicon chip.</em></p>\n", 'ViewCount': '84', 'Title': 'Influence of the dimension of cellular automata on complexity classes', 'LastEditorUserId': '31', 'LastActivityDate': '2012-03-12T20:44:04.593', 'LastEditDate': '2012-03-12T17:34:06.217', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '254', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '68', 'Tags': '<complexity-theory><time-complexity><cellular-automata>', 'CreationDate': '2012-03-12T16:49:47.177', 'Id': '245'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is often said that hash table lookup operates in constant time: you compute the hash value, which gives you an index for an array lookup. Yet this ignores collisions; in the worst case, every item happens to land in the same bucket and the lookup time becomes linear ($\\Theta(n)$).</p>\n\n<p>Are there conditions on the data that can make hash table lookup truly $O(1)$? Is that only on average, or can a hash table have $O(1)$ worst case lookup?</p>\n\n<p><em>Note: I\'m coming from a programmer\'s perspective here; when I store data in a hash table, it\'s almost always strings or some composite data structures, and the data changes during the lifetime of the hash table. So while I appreciate answers about perfect hashes, they\'re cute but anecdotal and not practical from my point of view.</em></p>\n\n<p>P.S. Follow-up: <a href="http://cs.stackexchange.com/questions/477/for-what-kind-of-data-are-hash-table-operations-o1">For what kind of data are hash table operations O(1)?</a></p>\n', 'ViewCount': '7425', 'Title': '(When) is hash table lookup O(1)?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-05T22:31:57.543', 'LastEditDate': '2012-03-17T21:56:32.630', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '32', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<time-complexity><data-structures><hash-tables>', 'CreationDate': '2012-03-12T19:01:07.577', 'FavoriteCount': '12', 'Id': '249'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '754', 'Title': 'Decision problems vs "real" problems that aren\'t yes-or-no', 'LastEditDate': '2012-04-02T21:58:54.973', 'AnswerCount': '3', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '157', 'FavoriteCount': '6', 'Body': '<p>I read in many places that some problems are difficult to approximate (it is  <a href="https://en.wikipedia.org/wiki/Hardness_of_approximation"><strong>NP-hard</strong> to approximate</a>  them). But approximation is not a decision problem: the answer is a real number and not Yes or No. Also for each desired approximation factor, there are many answers that are correct and many that are wrong, and this changes with the desired approximation factor!</p>\n\n<p>So how can one say that this problem is NP-hard?</p>\n\n<p><em>(inspired by the second bullet in <a href="http://cs.stackexchange.com/q/423/157">How hard is counting the number of simple paths between two nodes in a directed graph?</a>)</em></p>\n', 'Tags': '<complexity-theory><time-complexity><np-hard><approximation>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-29T16:31:13.863', 'CommentCount': '0', 'AcceptedAnswerId': '476', 'CreationDate': '2012-03-17T18:28:41.347', 'Id': '473'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From the answers to <a href="http://cs.stackexchange.com/questions/249/when-is-hash-table-lookup-o1">(When) is hash table lookup O(1)?</a>, I gather that hash tables have $O(1)$ worst-case behavior, at least amortized, when the data satisfies certain statistical conditions, and there are techniques to help make these conditions broad.</p>\n\n<p>However, from a programmer\'s perspective, I don\'t know in advance what my data will be: it often comes from some external source. And I rarely have all the data at once: often insertions and deletions happen at a rate that\'s not far below the rate of lookups, so preprocessing the data to fine-tune the hash function is out.</p>\n\n<p>So, taking a step out: given some knowledge about data source, how can I determine whether a hash table has a chance of having $O(1)$ operations, and possibly which techniques to use on my hash function?</p>\n', 'ViewCount': '658', 'Title': 'For what kind of data are hash table operations O(1)?', 'LastActivityDate': '2013-05-06T13:07:46.187', 'AnswerCount': '4', 'CommentCount': '10', 'AcceptedAnswerId': '1351', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<data-structures><time-complexity><hash-tables>', 'CreationDate': '2012-03-17T21:55:43.600', 'FavoriteCount': '2', 'Id': '477'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a balanced binary tree, which represents a recursive partitioning of a set of $N$ points into nested subsets.  Each node of the tree represents a subset, with the following properties: subsets represented by two children nodes of the same parent are disjoint, and their union is equal to the subset represented by the parent.  The root represents the full set of points, and each leaf represents a single distinct point.  So there are $\\log N$ levels to the tree, and each level of the tree represents a partitioning of the points into increasingly fine levels of granularity.</p>\n\n<p>Now suppose we have two algorithms, each of which operates on all of the subsets of the tree.  The first does $O(D^2)$ operations at each node, where $D$ is the size of the subset represented by the node.  The second does $O(D \\log D)$ operations at each node.  What is the worst case runtime of these two algorithms?</p>\n\n<p>We can easily bound the first algorithm as $O(N^2 \\log N)$, because it does $O(N^2)$ work at each of $\\log N$ levels of the tree.  Similarly, we can bound the second algorithm as $O(N \\log ^2 N)$, by similar reasoning.</p>\n\n<p>The question is, are these bounds tight, or can we do better?  How do we prove it?</p>\n', 'ViewCount': '394', 'Title': 'What is the complexity of these tree-based algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-21T15:26:40.690', 'LastEditDate': '2012-03-21T06:35:12.333', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '579', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '363', 'Tags': '<algorithms><time-complexity><binary-trees>', 'CreationDate': '2012-03-21T04:43:41.727', 'Id': '578'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2045', 'Title': 'Intuition for logarithmic complexity', 'LastEditDate': '2012-06-18T13:16:23.193', 'AnswerCount': '7', 'Score': '44', 'PostTypeId': '1', 'OwnerUserId': '385', 'FavoriteCount': '16', 'Body': "<p>I believe I have a reasonable grasp of complexities like $\\mathcal{O}(1)$, $\\Theta(n)$ and $\\Theta(n^2)$.</p>\n\n<p>In terms of a list, $\\mathcal{O}(1)$ is a constant lookup, so it's just getting the head of the list.\n$\\Theta(n)$ is where I'd walk the entire list, and $\\Theta(n^2)$ is walking the list once for each element in the list.</p>\n\n<p>Is there a similar intuitive way to grasp $\\Theta(\\log n)$ other than just knowing it lies somewhere between $\\mathcal{O}(1)$ and $\\Theta(n)$?</p>\n", 'Tags': '<algorithms><complexity-theory><time-complexity><intuition>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-18T13:16:23.193', 'CommentCount': '6', 'AcceptedAnswerId': '582', 'CreationDate': '2012-03-21T05:51:41.653', 'Id': '581'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m looking for a data structure that stores a set of strings over a character set $\\Sigma$, capable of performing the following operations. We denote $\\mathcal{D}(S)$ as the data structure storing the set of strings $S$.</p>\n\n<ul>\n<li><code>Add-Prefix-Set</code> on $\\mathcal{D}(S)$: given some set $T$ of (possibly empty) strings, whose size is bounded by a constant and whose string lengths are bounded by a constant, return $\\mathcal{D}( \\{ t s\\ |\\ t \\in T, s \\in S\\} )$. Both these bounding constants are global: they are the same for all inputs $T$.</li>\n<li><code>Get-Prefixes</code> on $\\mathcal{D}(S)$: return $\\{ a \\ | \\ as \\in S, a \\in \\Sigma \\}$. Note that I don\'t really mind what structure is used for this set, as long as I can enumerate its contents in $O(|\\Sigma|)$ time.</li>\n<li><code>Remove-Prefixes</code> on $\\mathcal{D}(S)$: return $\\mathcal{D}( \\{ s \\ | \\ as \\in S, a \\in \\Sigma  \\} )$.</li>\n<li><code>Merge</code>: given $\\mathcal{D}(S)$ and $\\mathcal{D}(T)$, return $\\mathcal{D}(S \\cup T)$.</li>\n</ul>\n\n<p>Now, I\'d really like to do all these operations in $O(1)$ time, but I\'m fine with a structure that does all these operations in $o(n)$ time, where $n$ is the length of the longest string in the structure. In the case of the merge, I\'d like a $o(n_1+n_2)$ running time, where $n_1$ is $n$ for the first and $n_2$ the $n$ for the second structure.</p>\n\n<p>An additional requirement is that the structure is immutable, or at least that the above operations return \'new\' structures such that pointers to the old ones still function as before.</p>\n\n<p>A note about amortization: that is fine, but you have to watch out for persistence. As I re-use old structures all the time, I\'ll be in trouble if I hit a worst case with some particular set of operations on the same structure (so ignoring the new structures it creates).</p>\n\n<p>I\'d like to use such a structure in a parsing algorithm I\'m working on; the above structure would hold the lookahead I need for the algorithm.</p>\n\n<p>I\'ve already considered using a <a href="http://en.wikipedia.org/wiki/Trie">trie</a>, but the main problem is that I don\'t know how to merge tries efficiently. If the set of strings for <code>Add-Prefix-Set</code> consists of only single-character strings, then you could store these sets in a stack, which would give you $O(1)$ running times for the first three operations. However, this approach doesn\'t work for merging either.</p>\n\n<p>Finally, note that I\'m not interested in factors $|\\Sigma|$: this is constant for all I care.</p>\n', 'ViewCount': '657', 'Title': "Is there a 'string stack' data structure that supports these string operations?", 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-01T01:47:57.227', 'LastEditDate': '2012-04-01T01:47:57.227', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<data-structures><time-complexity><strings><stack>', 'CreationDate': '2012-03-22T17:49:11.333', 'FavoriteCount': '3', 'Id': '666'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2198', 'Title': 'Complexity of Towers of Hanoi', 'LastEditDate': '2012-04-04T19:15:22.400', 'AnswerCount': '1', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '898', 'FavoriteCount': '2', 'Body': '<p>I ran into the following doubts on the complexity of <a href="http://en.wikipedia.org/wiki/Tower_of_Hanoi">Towers of Hanoi</a>, on which I would like your comments.</p>\n\n<ul>\n<li><p><b>Is it in NP? </b> \nAttempted answer: Suppose Peggy (prover) solves the problem &amp; submits it to Victor (verifier). Victor can easily see that the final state of the solution is right (in linear time) but he\'ll have no option but to go through each of Peggy\'s moves to make sure she didn\'t make an illegal move. Since Peggy has to make a minimum of 2^|disks|  - 1  moves (provable), Victor too has to follow suit. Thus Victor has no polynomial time verification (the definition of NP), and hence can\'t be in NP.</p></li>\n<li><p><b> Is it in PSPACE </b> ? Seems so, but I can\'t think of how to extend the above reasoning. </p></li>\n<li><p><b> Is it PSPACE-complete? </b> Seems not, but I have only a vague idea. Automated Planning , of which ToH is a specific instance,  is PSPACE-complete. I think that Planning has far more hard instances than ToH. </p></li>\n</ul>\n\n<p><b> Updated </b>: Input = $n$, the number of disks; Output = disk configuration at each step. After updating this, I realized that this input/output format doesn\'t fit a decision problem. I\'m not sure about the right formalization to capture the notions of NP,PSPACE, etc. for this kind of problem.</p>\n\n<p><b> Update #2 </b>: After Kaveh\'s and Jeff\'s comments, I\'m forced to make the problem more precise: </p>\n\n<blockquote>\n  <p>Let the input be the pair of ints $(n,i)$ where $n$ is the number of disks. If the sequence of moves taken by the disks is written down in the format (disk-number,from-peg,to-peg)(disk-number, from-peg, to-peg)... from the first move to the last, and encoded in binary, output the $i$th bit.</p>\n</blockquote>\n\n<p>Let me know if I need to be more specific about the encoding. I suppose Kaveh\'s comment applies in this case?   </p>\n', 'Tags': '<complexity-theory><time-complexity>', 'LastEditorUserId': '898', 'LastActivityDate': '2012-04-04T19:58:27.353', 'CommentCount': '9', 'AcceptedAnswerId': '1043', 'CreationDate': '2012-04-02T21:49:28.057', 'Id': '999'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '371', 'LastEditorDisplayName': 'user742', 'Title': 'Optimal algorithm for finding the girth of a sparse graph?', 'LastEditDate': '2012-04-05T14:18:34.090', 'AnswerCount': '1', 'Score': '16', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': '<p>I wonder how to find the <a href="http://en.wikipedia.org/wiki/Girth_%28graph_theory%29">girth</a> of a sparse undirected graph. By sparse I mean $|E|=O(|V|)$. By optimum I mean the lowest time complexity.</p>\n\n<p>I thought about some modification on <a href="http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm">Tarjan\'s algorithm</a> for undirected graphs, but I didn\'t find good results. Actually I thought that if I could find a 2-connected components in $O(|V|)$, then I can find the girth, by some sort of induction which can be achieved from the first part. I may be on the wrong track, though. Any algorithm asymptotically better than $\\Theta(|V|^2)$ (i.e. $o(|V|^2)$) is welcome.</p>\n', 'Tags': '<algorithms><time-complexity><graph-theory>', 'LastActivityDate': '2013-04-15T08:22:17.407', 'CommentCount': '9', 'AcceptedAnswerId': '1087', 'CreationDate': '2012-04-02T23:30:30.770', 'Id': '1001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Two things (this may be naive):</p>\n\n<ol>\n<li><p>Does anyone believe there is a sub-exponential time algorithm for the <a href="http://en.wikipedia.org/wiki/Subset_sum_problem" rel="nofollow">Subset-sum problem</a>? It seems obvious to me that you would have to look through all possible subsets to prove (the negation of) an existential statement. It seems obvious in the same way that if somebody asked you "Is $x$ in this list of $n$ numbers?", it\'s obvious that you would have to look through all $n$ numbers.</p></li>\n<li><p>If Subset-sum can be polynomial time reduced to <a href="http://en.wikipedia.org/wiki/K-SAT#3-satisfiability" rel="nofollow">3SAT</a> and we agree on (1), then doesn\'t that mean $NP \\neq P$?</p></li>\n</ol>\n', 'ViewCount': '503', 'Title': 'Subset-sum and 3SAT', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-08T16:41:26.820', 'LastEditDate': '2012-04-08T06:35:06.860', 'AnswerCount': '3', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '959', 'Tags': '<complexity-theory><time-complexity><np-complete><reductions>', 'CreationDate': '2012-04-07T21:28:36.573', 'Id': '1118'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>I want to establish that this is part of my homework for a course I am currently taking.  I am looking for some assistance in proceeding, NOT AN ANSWER.</strong></p>\n\n<p>This is the question in question:</p>\n\n<blockquote>\n  <p>A 5-pointed-star in an undirected graph is a 5-clique. Show that\n  5-POINTED-STAR $\\in P$, where 5-POINTED-STAR = $\\{ &lt;G&gt;$ $: G$ contains a\n  5-pointed-star as a subgraph $\\}$.</p>\n</blockquote>\n\n<p>Where a clique is CLIQUE = $\\{(G, k) : G$ is an undirected graph $G$ with a $k$-clique $\\}$.</p>\n\n<p>Now my problem is that this appears to be solving the CLIQUE problem, determining whether a graph contains a clique with the additional constraint of having to determine that the CLIQUE forms a 5-pointed star.  This seems to involve some geometric calculation based on knowledge of a <a href="http://www.ehow.com/about_4606571_geometry-fivepoint-star.html">5-pointed star</a>.  However, in Michael Sipser\'s <em>Theory of Computation</em>, pg 268, there is a proof showing that CLIQUE is in $NP$ and on page 270 notes that,</p>\n\n<blockquote>\n  <p><em>We have presented examples of languages, such as HAMPATH and CLIQUE,\n  <strong>that are members of NP but that are not known to be in $P$.</em></strong> [emphasis added]</p>\n</blockquote>\n\n<p>If CLIQUE is not in $P$, why five pointed star be in $P$?  Is there something I\'m not seeing?\n<strong>Remember, this is a HOMEWORK PROBLEM and A DIRECT ANSWER WOULD NOT BE APPRECIATED.</strong>  Thanks!</p>\n', 'ViewCount': '369', 'Title': 'Finding a 5-Pointed Star in polynomial time', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-10T16:14:56.977', 'LastEditDate': '2012-04-08T21:20:42.257', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1144', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '603', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-04-08T20:17:21.780', 'Id': '1143'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am interested in calculating the $n$'th power of a $n\\times n$ matrix $A$. Suppose we have an algorithm for matrix multiplication which runs in $\\mathcal{O}(M(n))$ time. Then, one can easily calculate $A^n$ in $\\mathcal{O}(M(n)\\log(n))$ time. Is it possible to solve this problem in lesser time complexity?</p>\n\n<p>Matrix entries can, in general, be from a semiring but you can assume additional structure if it helps.</p>\n\n<p>Note: I understand that in general computing $A^m$ in $o(M(n)\\log(m))$ time would give a $o(\\log m)$ algorithm for exponentiation. But, a number of interesting problems reduce to the special case of matrix exponentiation where m=$\\mathcal O(n)$, and I was not able to prove the same about this simpler problem.</p>\n", 'ViewCount': '975', 'Title': 'Complexity of computing matrix powers', 'LastEditorUserId': '984', 'LastActivityDate': '2012-04-10T09:49:46.710', 'LastEditDate': '2012-04-10T09:49:46.710', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '984', 'Tags': '<algorithms><complexity-theory><time-complexity><computer-algebra>', 'CreationDate': '2012-04-09T00:05:00.413', 'FavoriteCount': '2', 'Id': '1147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '702', 'Title': 'Complexity of finding the largest $m$ numbers in an array of size $n$', 'LastEditDate': '2012-04-24T20:40:13.420', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '1', 'Body': "<p>What follows is my algorithm for doing this in what I believe to be $O(n)$ time, and my proof for that. My professor disagrees that it runs in $O(n)$ and instead thinks that it runs in $\\Omega(n^2)$ time. Any comments regarding the proof itself, or the style (i.e. my ideas may be clear but the presentation not).</p>\n\n<p>The original question:</p>\n\n<blockquote>\n  <p>Given $n$ numbers, find the largest $m \\leq n$ among them in time $o(n \\log n)$. You may not assume anything else about $m$.</p>\n</blockquote>\n\n<p>My answer:</p>\n\n<ol>\n<li>Sort the first $m$ elements of the array. This takes $O(1)$ time, as this is totally dependent on $m$, not $n$.</li>\n<li>Store them in a linked list (maintaining the sorted order). This also takes $O(1)$ time, for the same reason as above.</li>\n<li>For every other element in the array, test if it is greater than the least element of the linked list. This takes $O(n)$ time as $n$ comparisons must be done.</li>\n<li>If the number is in fact greater, then delete the first element of the linked list (the lowest one) and insert the new number in the location that would keep the list in sorted order. This takes $O(1)$ time because it is bounded by a constant ($m$) above as the list does not grow.</li>\n<li>Therefore, the total complexity for the algorithm is $O(n)$.</li>\n</ol>\n\n<p>I am aware that using a red-black tree as opposed to linked list is more efficient in constant terms (as the constant upper bound is $O(m\\cdot \\log_2(m))$ as opposed to $m$ and the problem of keeping a pointer to the lowest element of the tree (to facilitate the comparisons) is eminently doable, it just didn't occur to me at the time.</p>\n\n<p>What is my proof missing? Is there a more standard way of presenting it (even if it is incorrect)?</p>\n", 'Tags': '<algorithms><time-complexity><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-24T03:35:40.420', 'CommentCount': '9', 'AcceptedAnswerId': '1489', 'CreationDate': '2012-04-24T18:03:35.983', 'Id': '1485'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider the following version of the Clique problem where the input is of size $n$ and we're asked to find a clique of size $k$. The restriction is that the decision procedure cannot change the input graph into any other representation and cannot use any other representation to compute its answer, other than $\\log(n^k)$ extra bits beyond the input graph. The extra bits can be used for example in the brute-force algorithm to keep track of the status of the exhaustive search for a clique, but the decision procedure is welcome to use them in any other way that still decides the problem.</p>\n\n<p>Is anything known at this point about the complexity of this? Has any work been done on other restrictions of Clique, and if so, could you direct me to such work?</p>\n", 'ViewCount': '210', 'Title': 'Restricted version of the Clique problem?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-05-02T14:51:57.830', 'LastEditDate': '2012-05-01T13:50:43.373', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '1612', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1295', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-05-01T03:08:47.587', 'Id': '1606'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1389', 'Title': 'How can we assume that basic operations on numbers take constant time?', 'LastEditDate': '2013-09-10T22:18:05.507', 'AnswerCount': '6', 'Score': '31', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'FavoriteCount': '15', 'Body': "<p>Normally in algorithms we do not care about comparison, addition, or subtraction of numbers -- we assume they run in time $O(1)$.  For example, we assume this when we say that comparison-based sorting is $O(n\\log n)$, but when numbers are too big to fit into registers, we normally represent them as arrays so basic operations require extra calculations per element.</p>\n\n<p>Is there a proof showing that comparison of two numbers (or other primitive arithmetic functions) can be done in $O(1)$? If not why are we saying that comparison based sorting is $O(n\\log n)$?</p>\n\n<hr>\n\n<p><em>I encountered this problem when I answered a SO question and I realized that my algorithm is not $O(n)$ because sooner or later I should deal with big-int, also it wasn't pseudo polynomial time algorithm, it was $P$.</em></p>\n", 'Tags': '<algorithms><complexity-theory><algorithm-analysis><time-complexity><reference-question>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T22:18:05.507', 'CommentCount': '2', 'AcceptedAnswerId': '1661', 'CreationDate': '2012-05-03T00:06:31.453', 'Id': '1643'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '116', 'Title': 'Connection between castability and convexity', 'LastEditDate': '2012-05-10T13:53:55.323', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1379', 'FavoriteCount': '1', 'Body': "<p>I am wondering if there are any connection between convex polygon and castable object? What can we say about castability of the object if we know that the object is convex polygon and vice versa.</p>\n\n<p>Let's gather together few basic things that we have to know.</p>\n\n<blockquote>\n  <p>The object is castable if it can removed from the mold.</p>\n  \n  <p>The polyhedron P can be removed from its mold by a translation in direction $\\vec{d}$  if and only if $\\vec{d}$ makes an angle of at  least $90^{\\circ}$ with the outward normal of all ordinary facets of P.</p>\n</blockquote>\n\n<p>For a arbitrary object testing for castability has time complexity $O(n^2)$. In my opinion, for a convex polygon if could be improved to linear time, because for every new top facet we should test that the vector $\\vec{d}$ makes an angle at least $90^{\\circ}$ with outward normal not of all but only of two adjacent ordinary facets of P. </p>\n\n<p>If this is true at least we have improvement in testing for castability in case of convex polygon.</p>\n\n<p>We else can we state about castability and convexity. Especially interesting to know, if castability tells us something about convexity.</p>\n", 'Tags': '<complexity-theory><time-complexity><computational-geometry>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-10T18:39:41.280', 'CommentCount': '5', 'AcceptedAnswerId': '1677', 'CreationDate': '2012-05-05T14:52:23.943', 'Id': '1671'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am struggling with hashing and binary search tree material.\nAnd I read that instead of using lists for storing entries with the same hash values, it is also possible to use binary search trees. And I try to understand what the worst-case and average-case running time for the operations</p>\n\n<ol>\n<li><code>insert</code>, </li>\n<li><code>find</code> and</li>\n<li><code>delete</code></li>\n</ol>\n\n<p>is in worth- resp. average case. Do they improve with respect to lists?</p>\n', 'ViewCount': '1265', 'Title': 'Hashing using search trees instead of lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T15:37:46.957', 'LastEditDate': '2012-05-10T15:37:46.957', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<data-structures><time-complexity><runtime-analysis><search-trees><hash-tables>', 'CreationDate': '2012-05-08T21:46:33.920', 'Id': '1739'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>This question has been prompted by <a href="http://cs.stackexchange.com/questions/1626/efficient-data-structures-for-building-a-fast-spell-checker">Efficient data structures for building a fast spell checker</a>.</p>\n\n<p>Given two strings $u,v$, we say they are <em>$k$-close</em> if their <a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance" rel="nofollow">Damerau\u2013Levenshtein distance</a>\xb9 is small, i.e. $\\operatorname{LD}(u,v) \\geq k$ for a fixed $k \\in \\mathbb{N}$. Informally, $\\operatorname{LD}(u,v)$ is the minimum number of deletion, insertion, substitution and (neighbour) swap operations needed to transform $u$ into $v$. It can be computed in $\\Theta(|u|\\cdot|v|)$ by dynamic programming. Note that $\\operatorname{LD}$ is a <a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29" rel="nofollow">metric</a>, that is in particular symmetric.</p>\n\n<p>The question of interest is:</p>\n\n<blockquote>\n  <p>Given a set $S$ of $n$ strings over $\\Sigma$ with lengths at most $m$, what is the cardinality of </p>\n  \n  <p>$\\qquad \\displaystyle S_k := \\{ w \\in \\Sigma^* \\mid \\exists v \\in S.\\ \\operatorname{LD}(v,w) \\leq k \\}$?</p>\n</blockquote>\n\n<p>As even two strings of the same length have different numbers of $k$-close strings\xb2 a general formula/approach may be hard (impossible?) to find. Therefore, we might have to compute the number explicitly for every given $S$, leading us to the main question:</p>\n\n<blockquote>\n  <p>What is the (time) complexity of finding the cardinality of the set $\\{w\\}_k$ for (arbitrary) $w \\in \\Sigma^*$?</p>\n</blockquote>\n\n<p>Note that the desired quantity is exponential in $|w|$, so explicit enumeration is not desirable. An efficient algorithm would be great.</p>\n\n<p>If it helps, it can be assumed that we have indeed a (large) set $S$ of strings, that is we solve the first highlighted question.</p>\n\n<hr>\n\n<ol>\n<li>Possible variants include using the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> instead.</li>\n<li>Consider $aa$ and $ab$. The sets of $1$-close strings over $\\{a,b\\}$ are $\\{ a, aa,ab,ba,aaa,baa,aba,aab \\}$ (8 words) and $\\{a,b,aa,bb,ab,ba,aab,bab,abb,aba\\}$ (10 words), respectively .</li>\n</ol>\n', 'ViewCount': '268', 'Title': 'How many strings are close to a given set of strings?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:21:01.853', 'LastEditDate': '2012-05-15T20:21:01.853', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><time-complexity><strings><word-combinatorics><string-metrics>', 'CreationDate': '2012-05-09T15:48:12.173', 'FavoriteCount': '1', 'Id': '1758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '8101', 'Title': 'To Find the median of an unsorted array', 'LastEditDate': '2012-05-18T17:53:24.993', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1545', 'FavoriteCount': '2', 'Body': '<p>To fine the median of an unsorted array, we can make a min-heap in $O(n\\log n)$ time for $n$ elements, and then we can extract one by one $n/2$ elements to get the median. But this approach would take $O(n \\log n)$ time.</p>\n\n<p>Can we do the same by some method in $O(n)$ time? If we can, then please tell.</p>\n', 'Tags': '<algorithms><time-complexity>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-20T00:23:13.460', 'CommentCount': '4', 'AcceptedAnswerId': '1938', 'CreationDate': '2012-05-18T17:40:24.557', 'Id': '1914'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '370', 'Title': 'How to go from a recurrence relation to a final complexity', 'LastEditDate': '2012-05-22T17:52:22.850', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1562', 'FavoriteCount': '1', 'Body': u'<p>I have an algorithm, shown below, that I need to analyze. Because it\'s recursive in nature I set up a recurrence relation.  </p>\n\n<pre><code>//Input: Adjacency matrix A[1..n, 1..n]) of an undirected graph G  \n//Output: 1 (true) if G is complete and 0 (false) otherwise  \nGraphComplete(A[1..n, 1..n]) {\n  if ( n = 1 )\n    return 1 //one-vertex graph is complete by definition  \n  else  \n    if not GraphComplete(A[0..n \u2212 1, 0..n \u2212 1]) \n      return 0  \n    else \n      for ( j \u2190 1 to n \u2212 1 ) do  \n        if ( A[n, j] = 0 ) \n          return 0  \n      end\n      return 1\n}\n</code></pre>\n\n<p>Here is what I believe is a valid and correct recurrence relation:  </p>\n\n<p>$\\qquad \\begin{align}\r\n  T(1) &amp;= 0 \\\\\r\n  T(n) &amp;= T(n-1) + n - 1 \\quad \\text{for } n \\geq 2\r\n\\end{align}$</p>\n\n<p>The "$n - 1$" is how many times the body of the for loop, specifically the "if A[n,j]=0" check, is executed.</p>\n\n<p>The problem is, where do I go from here? How do I convert the above into something that actually shows what the resulting complexity is?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-22T17:52:22.850', 'CommentCount': '1', 'AcceptedAnswerId': '1960', 'CreationDate': '2012-05-20T21:24:29.637', 'Id': '1959'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1671', 'Title': 'Data structure with search, insert and delete in amortised time $O(1)$?', 'LastEditDate': '2012-05-22T07:32:22.800', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '1120', 'FavoriteCount': '7', 'Body': '<p>Is there a data structure to maintain an ordered list that supports the following operations in $O(1)$ amortized time? </p>\n\n<ul>\n<li><p><strong>GetElement(k)</strong>: Return the $k$th element of the list.</p></li>\n<li><p><strong>InsertAfter(x,y)</strong>: Insert the new element y into the list immediately after x.  </p></li>\n<li><p><strong>Delete(x)</strong>: Remove x from the list.</p></li>\n</ul>\n\n<p>For the last two operations, you can assume that x is given as a pointer directly into the data structure; InsertElement returns the corresponding pointer for y.  InsertAfter(NULL, y) inserts y at the beginning of the list.</p>\n\n<p>For example, starting with an empty data structure, the following operations update the ordered list as shown below:</p>\n\n<ul>\n<li>InsertAfter(NULL, a) $\\implies$ [a]</li>\n<li>InsertAfter(NULL, b) $\\implies$ [b, a]</li>\n<li>InsertAfter(b, c) $\\implies$ [b, c, a]</li>\n<li>InsertAfter(a, d) $\\implies$ [b, c, a, d]</li>\n<li>Delete(c) $\\implies$ [b, a, d]</li>\n</ul>\n\n<p>After these five updates, GetElement(2) should return d, and GetElement(3) should return an error.</p>\n', 'Tags': '<data-structures><time-complexity><asymptotics><amortized-analysis>', 'LastEditorUserId': '72', 'LastActivityDate': '2013-08-06T14:07:23.407', 'CommentCount': '8', 'AcceptedAnswerId': '7807', 'CreationDate': '2012-05-21T07:35:55.077', 'Id': '1970'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there any decision problem that is in a complexity class properly included in DLOGTIME? (except $O(1)$, of course)</p>\n\n<p>If there is, can we create complete problems for DLOGTIME? So, can there be reduction by $O(\\log(\\log n))$ or smaller?</p>\n', 'ViewCount': '66', 'Title': 'Complexity class that properly included in DLOGTIME', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-06-28T11:02:54.960', 'LastEditDate': '2013-06-28T11:02:54.960', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1412', 'Tags': '<complexity-theory><time-complexity><complexity-classes>', 'CreationDate': '2012-06-04T08:35:54.763', 'FavoriteCount': '1', 'Id': '2221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '204', 'Title': 'Can joins be parallelized?', 'LastEditDate': '2012-06-07T14:49:58.440', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1590', 'FavoriteCount': '1', 'Body': '<p>Suppose we want to join two relations on a predicate. Is this in NC?</p>\n\n<p>I realize that a proof of it not being in NC would amount to a proof that $P\\not=NC$, so I\'d accept evidence of it being an open problem as an answer.</p>\n\n<p>I\'m interested in the general case as well as specific cases (e.g. perhaps with some specific data structure it can be parallelized). </p>\n\n<p>EDIT: to bring some clarifications from the comments into this post:</p>\n\n<ul>\n<li>We could consider an equijoin $A.x = B.y$. On a single processor, a hash-based algorithm runs in $O(|A|+|B|)$ and this is the best we can do since we have to read each set</li>\n<li>If the predicate is a "black box" where we have to check each pair, there are $|A|\\cdot|B|$ pairs, and each one could be in or not, so $2^{ab}$ possibilities. Checking each pair divides the possibilities in half, so the best we can do is $O(ab)$.</li>\n</ul>\n\n<p>Could either of these (or some third type of join) be improved to $\\log^k n$ on multiple processors?</p>\n', 'Tags': '<complexity-theory><time-complexity><parallel-computing><database-theory><descriptive-complexity>', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-07-07T01:35:55.073', 'CommentCount': '9', 'AcceptedAnswerId': '2410', 'CreationDate': '2012-06-05T17:38:32.240', 'Id': '2235'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>As the title says, has anyone found a polynomial time algorithm for checking whether two graphs having a Hamiltonian cycle are isomorphic? Is this problem NP-complete?</p>\n', 'ViewCount': '306', 'Title': 'Has anyone found polynomial algorithm on Hamiltonian cycle isomorphism?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:06:12.337', 'LastEditDate': '2013-05-24T03:06:12.337', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1866', 'Tags': '<complexity-theory><graph-theory><time-complexity>', 'CreationDate': '2012-06-15T15:28:43.047', 'FavoriteCount': '0', 'Id': '2383'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the book <a href="http://www.cs.uu.nl/geobook/">"Computational Geometry: Algorithms and Applications"</a> by Mark de Berg et al., there is a very simple brute force algorithm for computing Delaunay triangulations. The algorithm uses the notion of <em>illegal edges</em> -- edges that may not appear in a valid Delaunay triangulation and have to be replaced by some other edges. On each step, the algorithm just finds these illegal edges and performs required displacements (called <em>edge flips</em>) till there are no illegal edges.</p>\n\n<blockquote>\n  <p>Algorithm <strong>LegalTriangulation</strong>($T$)</p>\n  \n  <p><em>Input</em>. Some triangulation $T$ of a point set $P$.<br>\n  <em>Output</em>. A legal triangulation of $P$.</p>\n  \n  <p><strong>while</strong> $T$ contains an illegal edge $p_ip_j$<br>\n  <strong>do</strong><br>\n  $\\quad$ Let $p_i p_j p_k$ and $p_i p_j p_l$ be the two triangles adjacent to $p_ip_j$.<br>\n  $\\quad$ Remove $p_ip_j$ from $T$, and add $p_kp_l$ instead.<br/>\n  <strong>return</strong> $T$.</p>\n</blockquote>\n\n<p>I\'ve heard that this algorithm runs in $O(n^2)$ time in worst case; however, it is not clear to me whether this statement is correct or not. If yes, how can one prove this upper bound?</p>\n', 'ViewCount': '769', 'Title': 'Brute force Delaunay triangulation algorithm complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-21T13:44:11.817', 'LastEditDate': '2012-06-17T13:28:32.350', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><time-complexity><algorithm-analysis><computational-geometry><runtime-analysis>', 'CreationDate': '2012-06-16T22:01:33.543', 'FavoriteCount': '2', 'Id': '2400'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1074', 'Title': 'Time complexity of a triple nested loop with squared indices', 'LastEditDate': '2012-06-19T15:26:25.123', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'null', 'PostTypeId': '1', 'OwnerUserId': '1895', 'FavoriteCount': '3', 'Body': '<p>I have seen this function in past year exam paper.</p>\n\n<pre><code>public static void run(int n){\n    for(int i = 1 ; i * i &lt; n ; i++){\n        for(int j = i ; j * j &lt; n ; j++){\n            for(int k = j ; k * k &lt; n ; k++){\n\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>After give some example, I guess it is a function that with time complexity in following formula</p>\n\n<p><em><strong>let make m = n^(1/2)</em></strong></p>\n\n<p><em><strong>[m+(m-1)+(m-2)+...+3+2+1] + [(m-1)+(m-2)+...+3+2+1] + ...... + (3+2+1) + (2+1) + 1</em></strong></p>\n\n<p>*Edit: I have asked this math question <a href="http://math.stackexchange.com/a/159142/33103">here</a>, the answer is <strong>m(m+1)(m+2)/6</strong></p>\n\n<p>Is this correct, if no, what is wrong, if yes, how would you translate to big O notation.\nThe question that I want to ask is not <strong>only</strong> about this specific example; but also how would you evaluate an algorithm, let\'s say, I can only give some example to watch the pattern it appears. But some algorithm are not that easy to evaluate, what is your way to evaluate using this example.</p>\n\n<p><strong>Edit:\n@LuchianGrigore\n@AleksG</strong></p>\n\n<pre><code>public static void run(int n){\n        for(int i = 1 ; i * i &lt; n ; i++){\n            for(int j = 1 ; j * j &lt; n ; j++){\n                for(int k = 1 ; k * k &lt; n ; k++){\n\n                }\n            }\n        }\n    }\n</code></pre>\n\n<p>This is an example that in my lecture notes, each loop is with time complexity of <strong>n</strong> to the power of <strong>1/2</strong>, for each loop there is another n^(1/2) inside, the total are n^(1/2) * n^(1/2) * n^(1/2) = n^(3/2).\nIs the first example the same? It is less than the second example, right?</p>\n\n<p><strong>Edit,Add:</strong></p>\n\n<p>How about this one? Is it <strong>log(n)*n^(1/2)*log(n^2)</strong></p>\n\n<pre><code>for (int i = 1; i &lt; n; i *= 2)\n            for (int j = i; j * j &lt; n; j++)\n                for (int m = j; j &lt; n * n; j *= 2)\n</code></pre>\n', 'Tags': '<time-complexity>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-06-19T15:31:00.670', 'CommentCount': '1', 'AcceptedAnswerId': '2408', 'CreationDate': '2012-06-19T08:24:22.770', 'Id': '2407'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '168', 'Title': 'Attempt to write a function with cubed log runtime complexity $O(\\log^3 n)$', 'LastEditDate': '2012-06-19T15:24:47.480', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'null', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Body': '<p>I\'m learning Data Structures and Algorithms now, I have a practical question that asked to write a function with O(log<sup>3</sup>n), which means log(n)*log(n)*log(n).</p>\n\n<pre><code>public void run(int n) {\n    for (int i = 1; i &lt; n; i *= 2) {\n        for (int j = 1; j &lt; n; j *= 2) {\n            for (int k = 1; k &lt; n; k *= 2) {\n                System.out.println("hi");\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>I have come with this solution, but it seems not correct. Please help me out.</p>\n', 'Tags': '<time-complexity>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-06-20T07:32:02.837', 'CommentCount': '5', 'AcceptedAnswerId': '2412', 'CreationDate': '2012-06-15T16:18:08.207', 'Id': '2411'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '138', 'Title': 'Semi-decidable problems with linear bound', 'LastEditDate': '2012-06-21T20:32:07.537', 'AnswerCount': '2', 'Score': '5', 'OwnerDisplayName': 'Joachim Breitner', 'PostTypeId': '1', 'OwnerUserId': '1913', 'Body': u'<p>Take a semi-decidable problem and an algorithm that finds the positive answer in finite time. The run-time of the algorithm, restricted to inputs with a positive answer, cannot be bounded by a computable function. (Otherwise we\u2019d know how long to wait for a positive answer. If the algorithm runs longer than that we know that the answer is no and the problem would be solvable.)</p>\n\n<p>My question is now: Can such an algorithm still have a, say, a run-time bound linear (polynomial, constant,...) in the input size, but with an uncomputable constant? Or would that still allow me to decide the problem? Are there example?</p>\n', 'Tags': '<computability><time-complexity><undecidability>', 'LastEditorUserId': '1728', 'LastActivityDate': '2012-06-21T20:32:07.537', 'CommentCount': '2', 'AcceptedAnswerId': '2426', 'CreationDate': '2012-06-20T07:52:13.637', 'Id': '2425'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is a well-known problem.</p>\n\n<p>Given an array $A[1\\dots n]$ of positive integers, output the smallest positive integer not in the array.</p>\n\n<p>The problem can be solved in $O(n)$ space and time: read the array, keep track in $O(n)$ space whether $1,2,\\dots,n+1$ occured, scan for smallest element.</p>\n\n<p>I noticed you can trade space for time. If you have $O(\\frac{n}{k})$ memory only, you can do it in $k$ rounds and get time $O(k n)$. In a special case, there is obviously constant-space quadratic-time algorithm.</p>\n\n<p>My question is:</p>\n\n<blockquote>\n  <p>Is this the optimal tradeoff, i.e. does $\\operatorname{time} \\cdot \\operatorname{space} = \\Omega(n^2)$?\n  In general, how does one prove such type of bounds?</p>\n</blockquote>\n\n<p>Assume RAM model, with bounded arithmetic and random access to arrays in O(1).</p>\n\n<p>Inspiration for this problem: time-space tradeoff for palindromes in one-tape model (see for example, <a href="http://www.cs.uiuc.edu/class/fa05/cs475/Lectures/new/lec24.pdf" rel="nofollow">here</a>).</p>\n', 'ViewCount': '291', 'Title': 'Time-space tradeoff for missing element problem', 'LastEditorUserId': '72', 'LastActivityDate': '2012-06-29T22:18:45.900', 'LastEditDate': '2012-06-29T22:18:45.900', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '667', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2012-06-23T15:32:31.797', 'FavoriteCount': '1', 'Id': '2464'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is an example in my lecture notes.\nIs this function with time complexity $O(n \\log n)$?.\nBecause the worst case is the funtion goes into <code>else</code> branch, and 2 nested loops with time complexity of $\\log n$ and $n$, so it is $O(n \\log n)$. Am I right?</p>\n\n<pre><code>int j = 3;\nint k = j * n / 345;\nif(k &gt; 100){\n    System.out.println("k: " + k);\n}else{\n    for(int i=1; i&lt;n; i*=2){\n        for(int j=0; j&lt;i; j++){\n            k++;\n        }\n    }\n}\n</code></pre>\n', 'ViewCount': '557', 'Title': 'What is the time complexity of this function?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-26T23:13:31.913', 'LastEditDate': '2012-06-25T17:21:54.770', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '2497', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Tags': '<complexity-theory><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-06-25T14:48:54.900', 'Id': '2487'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a directed graph $G=(V,E)$ and two nodes $A$ and $B$.\nI would like to know if there are already algorithms for calculating the following decision problem: </p>\n\n<blockquote>\n  <p>Are there at least two paths between $A$ and $B$ of the same length?</p>\n</blockquote>\n\n<p>How about the complexity? Can I solve it in polynomial time?</p>\n\n<hr>\n\n<p>I would like to add a new constrain on the graph, maybe the problem is more solvable.\nOn adjacency matrix, every column is not empty. So, every node has at least one arrow on input and there is also at least one node connected to itself. So if the node is the $i$-th node, then $(i,i)$ is an edge in the graph.</p>\n', 'ViewCount': '363', 'Title': 'Finding at least two paths of same length in a directed graph', 'LastEditorUserId': '1974', 'LastActivityDate': '2012-07-03T20:59:07.823', 'LastEditDate': '2012-06-27T16:49:56.487', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '1974', 'Tags': '<complexity-theory><graph-theory><time-complexity><graphs>', 'CreationDate': '2012-06-26T12:12:27.963', 'FavoriteCount': '6', 'Id': '2498'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been studying the <a href="http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="nofollow">Spearman\'s rank correlation coefficient</a></p>\n\n<p>$\\qquad \\displaystyle \\rho = \\frac{\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_i (x_i-\\bar{x})^2 \\sum_i(y_i-\\bar{y})^2}}$.</p>\n\n<p>for two lists $x_1, \\dots, x_n$ and $y_1, \\dots, y_n$. What\'s the <em>complexity</em> of the algorithm?</p>\n\n<p>Since the algorithm should just compute $n$ subtractions, is it possible to be $O(n)$ ?</p>\n', 'ViewCount': '204', 'Title': "What's the complexity of Spearman's rank correlation coefficient computation?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-05T14:00:32.050', 'LastEditDate': '2012-07-04T10:24:10.760', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2064', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2012-07-04T07:45:52.857', 'Id': '2604'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The lookup time in perfect hash-tables is $O(1)$ in the worst case. Does that simply mean that the average should be $\\leq O(1)$?</p>\n', 'ViewCount': '163', 'Title': 'What is the average search complexity of perfect hashing?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-17T06:12:56.213', 'LastEditDate': '2012-07-17T06:12:56.213', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2067', 'Tags': '<complexity-theory><time-complexity><asymptotics><hash-tables>', 'CreationDate': '2012-07-04T14:36:16.523', 'Id': '2610'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say instead of using a linked list as buckets for a hash table of size $m$, we use another hash table of size $p$ as buckets this time. What would be the average case for this problem?</p>\n\n<p>I looked up perfect hashing and I got a very close algorithm, and it is $O(1)$. Can someone please clarify?</p>\n', 'ViewCount': '239', 'Title': 'Using hash tables instead of lists for buckets in hash tables', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-05T07:18:20.110', 'LastEditDate': '2012-07-05T07:18:20.110', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2067', 'Tags': '<data-structures><time-complexity><hash-tables>', 'CreationDate': '2012-07-04T14:54:28.810', 'Id': '2613'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '604', 'Title': 'How hard is finding the discrete logarithm?', 'LastEditDate': '2012-07-09T23:57:38.123', 'AnswerCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1667', 'FavoriteCount': '2', 'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Discrete_logarithm">discrete logarithm</a> is the same as finding $b$ in $a^b=c \\bmod N$, given $a$, $c$, and $N$.</p>\n\n<p>I wonder what complexity groups (e.g. for classical and quantum computers) this is in, and what approaches (i.e. algorithms) are the best for accomplishing this task.</p>\n\n<p>The wikipedia link above doesn\'t really give very concrete runtimes.  I\'m hoping for something more like what the best known methods are for finding such.</p>\n', 'Tags': '<algorithms><complexity-theory><time-complexity><discrete-mathematics>', 'LastEditorUserId': '1667', 'LastActivityDate': '2012-07-17T13:43:38.477', 'CommentCount': '1', 'AcceptedAnswerId': '2765', 'CreationDate': '2012-07-09T17:33:10.047', 'Id': '2658'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'ve been reading a bit of the literature lately, and have found some rather interesting data-structures.</p>\n\n<p>I have researched various different methods of getting update times down to $\\mathcal{O}(1)$ worst-case update time [1-7].</p>\n\n<p>Recently I begun looking into lock-free data-structures, to support efficient concurrent access.</p>\n\n<p><strong>Have any of these worst-case $\\mathcal{O}(1)$ update-time techniques been used in the implementation of lock-free data structures?</strong></p>\n\n<p>I ask because; to me, they seem like the obvious practical extension of this "theoretical enhancement".</p>\n\n<hr>\n\n<ol>\n<li><p><a href="http://www.sciencedirect.com/science/article/pii/0020019083900996">Tarjan, Robert Endre. \u201cUpdating a Balanced Search Tree in O(1) Rotations.\u201d Information Processing Letters 16, no. 5 (1983): 253 \u2013 257.</a></p></li>\n<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4630">Driscoll, J R, N Sarnak, D D Sleator, and R E Tarjan. \u201cMaking Data Structures Persistent.\u201d In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, 109\u2013121. STOC  \u201986. New York, NY, USA: ACM, 1986.</a></p></li>\n<li><p><a href="http://dx.doi.org/10.1007/BF00299635">Levcopoulos, C., and Mark H. Overmars. \u201cA Balanced Search Tree with O(1) Worst Case Update Time.\u201d Acta Inf. 26, no. 3 (November 1988): 269\u2013277.</a></p></li>\n<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.9433">Fleischer, Rudolf. A Simple Balanced Search Tree With O(1) Worst-Case Update Time</a></p></li>\n<li><p><a href="http://dx.doi.org/10.1016/0020-0190%2894%2900115-4">Dietz, Paul F, and Rajeev Raman. \u201cA Constant Update Time Finger Search Tree.\u201d Information Processing Letters 52, no. 3 (1994): 147 \u2013 154.</a></p></li>\n<li><p><a href="http://dl.acm.org/citation.cfm?id=998223.998229">Lagogiannis, George, Christos Makris, Yannis Panagis, Spyros Sioutas, and Kostas Tsichlas. \u201cNew Dynamic Balanced Search Trees with Worst-case Constant Update Time.\u201d J. Autom. Lang. Comb. 8, no. 4 (July 2003): 607\u2013632.</a></p></li>\n<li><p><a href="http://www.cs.au.dk/~gerth/papers/stoc02.pdf">Brodal, Gerth St\xf8lting, George Lagogiannis, Christos Makris, Athanasios Tsakalidis, and Kostas Tsichlas. \u201cOptimal Finger Search Trees in the Pointer Machine.\u201d J. Comput. Syst. Sci. 67, no. 2 (September 2003): 381\u2013418.</a></p></li>\n</ol>\n', 'ViewCount': '506', 'Title': 'Lock-free, constant update-time concurrent tree data-structures?', 'LastEditorUserId': '1120', 'LastActivityDate': '2013-05-14T16:27:44.497', 'LastEditDate': '2012-07-18T05:21:06.267', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<reference-request><data-structures><time-complexity><concurrency><search-trees>', 'CreationDate': '2012-07-10T20:04:39.177', 'FavoriteCount': '5', 'Id': '2680'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In terms of asymptotic space and time complexity, what is the most efficient priority-queue?\nSpecifically I am looking for priority queues which minimize the complexity of inserts, it\'s ok if deletes are a little slower.</p>\n\n<p><sub> If you\'re looking for a survey of priority-queues which minimises complexity of deletes over inserts, see: <a href="http://cs.stackexchange.com/q/524">Does there exist a priority queue with $O(1)$ extracts?</a>. </sub></p>\n', 'ViewCount': '816', 'Title': 'Most efficient known priority queue for inserts', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-05T08:59:24.467', 'LastEditDate': '2012-09-25T21:29:08.723', 'AnswerCount': '5', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<data-structures><time-complexity><priority-queues>', 'CreationDate': '2012-07-19T15:49:47.360', 'Id': '2824'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We know that $\\# \\textbf{P}$ is closed under polynomial sums, i.e., sum of polynomially many $\\# \\textbf{P}$ functions is still in $\\# \\textbf{P}$.</p>\n\n<p>Functions in $\\textbf{P}^{\\# \\textbf{P}}$ are those which make polynomially many (non-parallel) queries to an oracle for a $\\# \\textbf{P}$-Complete problem. So, the computational complexity of such problems must be a sum of polynomially many $\\# \\textbf{P}$ functions. Due to the closure property, this sum will be in $\\# \\textbf{P}$.</p>\n\n<p>If the above is right, then:\nWhat is the difference between the classes $\\# \\textbf{P}$-Complete and $P^{\\# \\textbf{P}}$ ?</p>\n\n<p>Thanks.</p>\n', 'ViewCount': '75', 'Title': 'distinction between $\\textbf{P}^{\\# \\textbf{P}}$ and $\\# \\textbf{P}$-Complete', 'LastEditorUserId': '2250', 'LastActivityDate': '2012-07-23T20:14:48.037', 'LastEditDate': '2012-07-23T19:21:53.127', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2250', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-07-23T19:15:54.727', 'FavoriteCount': '1', 'Id': '2880'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Sipser\'s textbook "Introduction to the Theory of Computation, Second Edition," he defines nondeterministic time complexity as follows:</p>\n\n<blockquote>\n  <p>Let $N$ be a nondeterministic Turing machine that is a decider.  The <strong>running time</strong> of $N$ is the function $f : \\mathbb{N} \\rightarrow \\mathbb{N}$, where $f(n)$ is the maximum number of steps that $N$ uses on any branch of its computation on any input of length $n$ [...].</p>\n</blockquote>\n\n<p>Part of this definition says that the running time of the machine $N$ is the maximum number of steps taken by that machine on any branch.  Is there a reason that all branches are considered?  It seems like the length of the shortest accepting computation would be a better measure (assuming, of course, that the machine halts), since you would never need to run the machine any longer than this before you could conclude whether the machine was going to accept or not.</p>\n', 'ViewCount': '129', 'Title': 'Why does NTIME consider the length of the longest computation?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-24T08:48:55.467', 'LastEditDate': '2012-07-24T08:48:55.467', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '2892', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<complexity-theory><time-complexity><terminology><turing-machines><nondeterminism>', 'CreationDate': '2012-07-23T22:49:08.547', 'Id': '2887'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve just begun this stage 2 Compsci paper on algorithms, and stuff like this is not my strong point. I\'ve come across this in my lecture slides.</p>\n\n<pre><code>int length = input.length();\nfor (int i = 0; i &lt; length - 1; i++) {\n    for (int j = i + 1; j &lt; length; j++) {\n        System.out.println(input.substring(i,j));\n    }\n}\n</code></pre>\n\n<p>"In each iteration, the outer loop executes $\\frac{n^{2}-(2i-1)n-i+i^{2}}{2}$ operations from the inner loop for $i = 0, \\ldots, n-1$."</p>\n\n<p>Can someone please explain this to me step by step?</p>\n\n<p>I believe the formula above was obtained by using Gauss\' formula for adding numbers... I think...</p>\n', 'ViewCount': '1969', 'Title': 'Time complexity formula of nested loops', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-02T08:36:54.223', 'LastEditDate': '2012-08-02T08:36:54.223', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '2996', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-02T04:56:03.523', 'Id': '2994'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In <em>"Introduction to Algorithms: 3rd Edition"</em> there is Theorem 34.2, which states</p>\n\n<blockquote>\n  <p>$P = \\{ L \\mid L \\text{ is accepted by a polynomial-time algorithm} \\}$</p>\n</blockquote>\n\n<p><em>"Accepted in polynomial-time"</em> is defined by:</p>\n\n<blockquote>\n  <p>$L$ is accepted in polynomial time by an algorithm $A$ if it is accepted by $A$\n      and if in addition there exists a constant $k$ such that for any length-n string $x\\in L$, \n      algorithm $A$ accepts $x$ in time $O(n^k)$.</p>\n</blockquote>\n\n<p><em>"Accepted"</em> is defined by:</p>\n\n<blockquote>\n  <p>The language accepted by an algorithm $A$ is the set of strings\n          $L = \\{ x \\in \\{0,1\\}^* \\mid A(x) = 1 \\}$,\n      that is, the set of strings that the algorithm accepts.</p>\n</blockquote>\n\n<p>But what if we take $k = 0$, and algorithm $A(\\cdot) = 1$, which just returns 1 for everything?\nWouldn\'t that mean, that $P$ is just class of all languages?</p>\n', 'ViewCount': '92', 'Title': 'Problem with the definition of P', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-02T08:33:12.717', 'LastEditDate': '2012-08-02T08:33:12.717', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '3000', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1552', 'Tags': '<complexity-theory><terminology><time-complexity><polynomial-time>', 'CreationDate': '2012-08-02T07:57:36.103', 'Id': '2999'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What's the complexity of Conflict-Driven Clause Learning SAT solvers, compared to DPLL solvers? Was it proven that CDCL is faster in general? Are there instances of SAT that are hard for CDCL but easy for DPLL?</p>\n", 'ViewCount': '138', 'Title': 'Running time of CDCL compared to DPLL', 'LastEditorUserId': '472', 'LastActivityDate': '2012-11-25T21:52:35.260', 'LastEditDate': '2012-11-25T21:52:35.260', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3015', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2345', 'Tags': '<complexity-theory><time-complexity><efficiency><satisfiability><sat-solvers>', 'CreationDate': '2012-08-03T07:31:02.560', 'Id': '3014'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $M$ denote a set of $n$ positive integers, each less than $n^c$.</p>\n\n<p>What is the runtime of computing $\\prod_{m \\in M} m$ with a deterministic Turing machine?</p>\n', 'ViewCount': '74', 'Title': 'Run time of product of polynomially bounded numbers', 'LastEditorUserId': '2376', 'LastActivityDate': '2012-08-07T01:54:25.730', 'LastEditDate': '2012-08-07T01:54:25.730', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3047', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2372', 'Tags': '<algorithms><time-complexity><integers>', 'CreationDate': '2012-08-05T02:23:25.833', 'Id': '3039'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><em>Below, assume we\'re working with an infinite-tape Turing machine.</em></p>\n\n<p>When explaining the notion of time complexity to someone, and why it is measured relative to the input size of an instance, I stumbled across the following claim:</p>\n\n<blockquote>\n  <p>[..] For example, it\'s natural that you\'d need more steps to multiply two integers with 100000 bits, than, say multiplying two integers with 3 bits.</p>\n</blockquote>\n\n<p>The claim is convincing, but somehow hand-waving. In all algorithms I came across, the larger the input size, the more steps you need. In more precise words, the time complexity is a <a href="http://mathworld.wolfram.com/IncreasingFunction.html">monotonically increasing function</a> of the input size.</p>\n\n<blockquote>\n  <p>Is it the case that time complexity is <em>always</em> an increasing function in the input size? If so, why is it the case? Is there a <em>proof</em> for that beyond hand-waving?</p>\n</blockquote>\n', 'ViewCount': '222', 'Title': 'Why larger input sizes imply harder instances?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-14T13:24:07.873', 'LastEditDate': '2012-08-14T02:13:38.747', 'AnswerCount': '3', 'CommentCount': '10', 'AcceptedAnswerId': '3181', 'Score': '11', 'OwnerDisplayName': 'user20', 'PostTypeId': '1', 'Tags': '<complexity-theory><time-complexity><intuition>', 'CreationDate': '2012-08-13T23:09:10.920', 'Id': '3161'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Updated Algorithm:</strong> There was a major flaw in my original presentation of the algorithm which could have impacted the results. I apologize for the same. The correction has been posted underneath.\n<hr></p>\n\n<p>The original algorithm posted had a major flaw in its working. I tried my best but could not get the desired accuracy in presenting the algorithm in pseudo code and/or Set Theory notation. I am thus posting python code, which has been tested and produces the desired results.</p>\n\n<p>Note that my question, however, remains the same: What is the time complexity of the algorithm (assuming that powersets are already generated)?</p>\n\n<pre><code># mps is a set of powersets; below is a sample (test case)\nmps = [\n        [       [], [1], [2], [1,2]     ],\n        [       [], [3], [4], [3,4]     ],\n        [       [], [5], [6], [5,6]     ]\n      ]\n\n\n# Core algorithm\n# enumerate(mps) may not be required in languages like C which support indexed loops\nlen = mps.__len__()\nfor idx, ps in enumerate(mps):\n    if idx &gt; len - 2:\n            break;\n    mps[idx + 1] = merge(mps[idx], mps[idx+1])   # merge is defined below\n\n\n# Takes two powersets and merges them\ndef merge (psa, psb):\n    fs = []\n    for a in psa:\n            for b in psb:\n                    fs.append(list(set(a) | set(b)))\n    return fs\n</code></pre>\n\n<p><strong>Output</strong>: <code>mps[-1]   #Last item of the list</code></p>\n\n<p>Running the above example will result in listing out the powerset of $\\{1,2,3,4,5,6\\}$.</p>\n', 'ViewCount': '154', 'Title': 'What is the complexity of this subset merge algorithm?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-23T09:30:36.730', 'LastEditDate': '2012-08-23T09:30:36.730', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2597', 'Tags': '<algorithms><time-complexity><algorithm-analysis><check-my-algorithm>', 'CreationDate': '2012-08-21T15:00:26.797', 'Id': '3273'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm now doing exam revision, and from some past year exam papers, I noticed some questions that ask to write a recursive method with signature like</p>\n\n<pre><code>public void run(int n)\n</code></pre>\n\n<p>that must have a time complexity of like : $O(n^2), O(n^3), O(n^7), O(n^2!), O(2^n), O(9^n)$.</p>\n\n<p>Can anyone give some idea on how to solve this kind of recursion questions.</p>\n", 'ViewCount': '600', 'Title': 'How to write a recursive function that with certain time complexity', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-27T10:01:42.290', 'LastEditDate': '2012-08-23T09:12:36.930', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Tags': '<algorithms><time-complexity><recursion>', 'CreationDate': '2012-08-23T06:21:25.700', 'Id': '3297'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Please consider the following triple-nested loop:</p>\n\n<pre><code>for (int i = 1; i &lt;= n; ++i)\n    for (int j = i; j &lt;= n; ++j)\n        for (int k = j; k &lt;= n; ++k)\n            // statement\n</code></pre>\n\n<p>The statement here is executed exactly $n(n+1)(n+2)\\over6$ times. Could someone please explain how this formula was obtained? Thank you.</p>\n', 'ViewCount': '3201', 'Title': 'Time complexity of a triple-nested loop', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-24T22:01:32.450', 'LastEditDate': '2012-08-24T14:24:26.183', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2620', 'Tags': '<algorithms><time-complexity><algorithm-analysis>', 'CreationDate': '2012-08-24T02:42:49.667', 'FavoriteCount': '5', 'Id': '3306'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/1643/how-can-we-assume-comparison-addition-between-numbers-is-o1">How can we assume comparison, addition, &hellip; between numbers is $O(1)$</a>  </p>\n</blockquote>\n\n\n\n<p>When we calculate the time-complexity of some algorithm we make many simplifications (or assumptions) on our calculation. For instance we say that assume basic arithmetic operations are all constant time and we assume reading/writing to a memory location is constant time.</p>\n\n<p>I\'ve worked on some algorithms where these assumptions don\'t hold. I\'m looking for the background I need to properly approach/describe these algorithms. In particular I wish to know:</p>\n\n<ol>\n<li>What is the generic name for such a simplification/assumption?</li>\n<li>Is there a name for the basic model of simplifications we\'re using? (Assuming there is a common model)</li>\n</ol>\n\n<p><em>Note: I\'ve done work with cache-dependent/cache-oblivious algorithms. I understand how to calculate in that domain, but I\'m lacking the terminology/background to properly compare/contrast such algorithm analyis with that which don\'t consider the cache.</em></p>\n', 'ViewCount': '82', 'ClosedDate': '2012-08-30T13:10:54.383', 'Title': 'Complexity calculations, assumptions on basic costs', 'LastActivityDate': '2012-08-25T12:56:07.623', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3333', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1642', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-08-25T07:37:10.563', 'FavoriteCount': '1', 'Id': '3327'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If you have a quick-sort algorithm, and you always select the smallest (or largest) element as your pivot; am I right in assuming that if you provide an already sorted data set, you will always get worst-case performance regardless of whether your 'already sorted' list is in ascending or descending order? </p>\n\n<p>My thinking is that, if you always choose the smallest element for your pivot, then whether your 'already-sorted' input is sorted by ascending or descending doesn't matter because the subset chosen to be sorted relative to your pivot will always be the same size?</p>\n", 'ViewCount': '433', 'Title': 'Does Quicksort always have quadratic runtime if you choose a maximum element as pivot?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-27T12:58:30.307', 'LastEditDate': '2012-08-31T07:28:12.087', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '3379', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-08-31T04:24:50.107', 'Id': '3377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '253', 'Title': 'Complexity inversely propotional to $n$', 'LastEditDate': '2012-09-11T01:36:36.780', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1272', 'FavoriteCount': '1', 'Body': '<p>Is it possible an algorithm complexity decreases by input size? Simply $O(1/n)$ possible?</p>\n', 'Tags': '<algorithms><time-complexity><asymptotics><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-11T01:36:36.780', 'CommentCount': '2', 'AcceptedAnswerId': '3497', 'CreationDate': '2012-09-10T15:09:25.010', 'Id': '3495'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I saw a RAM model diagram that displayed an input tape, output tape, the program (read-only), the instruction pointer, and the memory registers. However, when I look at questions of time complexity, it is relevant how much time the program needs to spend on one action. Say you want to read one integer symbol from the read tape, add it to an integer from a memory register, and then squish the result into the write tape cell, and then move the read head one to the right and the write head one to the left. How much time or how many moves did I just waste?</p>\n', 'ViewCount': '82', 'Title': 'What constitutes one operation/cycle/move in the RAM model?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-16T21:23:57.300', 'LastEditDate': '2012-09-15T13:06:08.960', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '4574', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2835', 'Tags': '<algorithms><time-complexity><algorithm-analysis><machine-models>', 'CreationDate': '2012-09-15T04:38:07.633', 'Id': '3557'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '292', 'Title': 'complexity of decision problems vs computing functions', 'LastEditDate': '2012-09-22T17:24:26.267', 'AnswerCount': '2', 'Score': '-8', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '1', 'Body': u'<p>this is an area that admittedly Ive always found subtle about CS and occasionally trips me up, and clearly others. recently on tcs.se a user asked an apparently innocuous question about <a href="http://cstheory.stackexchange.com/questions/12682/is-the-n-queens-problem-np-hard">N-Queens being NP hard</a>, but it got downvoted there &amp; goes unanswered because the audience felt it was poorly phrased. </p>\n\n<p>the issue seems to be that some questions in CS have to be phrased as decision problems to measure their complexity, and it is not always obvious how to do that. also other questions are studied from the pov of computing a function with integer or string inputs and outputs, in which case eg their time or space complexity can be studied. for some NP complete problems eg SAT its proven that the decision problem (eg determining whether an answer exists) is roughly equivalent in time/space complexity to solving the function problem (finding a satisfying assignment), but of course thats not always the case.</p>\n\n<p>moreover much of CS theory is focused on NP complete problems [esp that presented to undergraduates in textbooks], which are nec. decision problems, which might lead to a mistaken impression that other types of problem formats are not as central to the theory. also NP complete is sometimes shown in a hierarchy that includes other complexity classes such as PSpace which can be used to measure <em>function</em> complexity in addition to <em>decision</em> complexity. </p>\n\n<p>think that the N-Queens example question &amp; related comments also shows that a bunch of related questions about the same problem in this case N-Queens can vary widely/dramatically in their complexity depending on slight variations of the phrasing. eg in this case:</p>\n\n<ul>\n<li>is there a solution to the N-Queens problem on a $n \\times n$ chessboard? this problem turns out to have O(1) time complexity\u2014 the answer is always Y.</li>\n<li>what is the complexity of finding a solution to the N-Queens problem given $n$ as the input referring to a square $n \\times n$ chessboard? as that question notes in comments by Peter Shor, an advanced and subtle thm from CS called "mahaney\'s" thm applies because it might refer to a "sparse input". also JeffE found a paper that says its in P.</li>\n<li>there is an NP complete or NP hard version of this problem if the squares are <em>irregularly specified</em> as the input. ie some kind of input that specifies the square allowed/unallowed locations (havent looked up the details).</li>\n</ul>\n\n<p><hr>\nwhile advanced theorists may find all this verging on trivial, feel its a legitimate area of focus which sometimes gets glossed over in theoretical treatments. my question</p>\n\n<blockquote>\n  <p>is there a reference somewhere that points/sorts out these kinds of subtleties/difficulties in formulating CS problems?</p>\n</blockquote>\n\n<p>it would be helpful if it also talks about how the issue relates to the complexity hierarchy. know that this is covered in some textbooks, but even then havent seen a nice concise discussion of that and wonder if anyone has a favorite ref for this type of issue. (suppose Garey and Johnson might have some discussion of this although dont have a copy close to check.)</p>\n\n<p>am not specifically focused on the N-Queens problem with this question, but an answer that sketches out the distinctions wrt N-Queens might be helpful. [eg an expanded explanation of Shor\'s comment how Mahaneys thm applies, the irregular board construction input format, etc]</p>\n\n<p>fyi here are two other example problems Ive noticed that can vary widely in complexity depending on various restrictions on the input</p>\n\n<p>[1] <a href="http://cs.stackexchange.com/questions/701/decidable-restrictions-of-the-post-correspondence-problem/4638">Post correspondence problem.</a> it can go from undecidable to NP complete or even in P depending on various restrictions on the solution.</p>\n\n<p>[2] Finding whether a regular expression is equivalent to all strings over the alphabet. with squaring this was shown to be in ExpSpace by Stockmeyer/Meyer, but restrictions on length lead to it to being in NP complete or P. see eg Chee Yap, <a href="http://cs.nyu.edu/yap/book/complexity/" rel="nofollow">Intro to Complexity classes</a>, ch5 on complete problems.</p>\n', 'ClosedDate': '2014-04-29T11:56:36.853', 'Tags': '<algorithms><complexity-theory><reference-request><time-complexity><np-complete>', 'LastEditorUserId': '699', 'LastActivityDate': '2012-10-01T00:23:22.727', 'CommentCount': '7', 'AcceptedAnswerId': '4674', 'CreationDate': '2012-09-22T16:57:55.790', 'Id': '4667'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have this little exercise:</p>\n\n<pre><code>for ( i = 0; i &lt; 2 * n; i += 2 )\n  for ( j = 1; j &lt;= n; j &lt;&lt;= 1 )\n    if ( j &amp; i )\n      foo ();\n</code></pre>\n\n<p>(<code>j &lt;&lt;= 1</code> means <code>j = (j &lt;&lt; 1)</code>, where <code>&lt;&lt;</code> is the bitwise left shift operator. <code>&amp;</code> is the bitwise and operator.)</p>\n\n<p>I am supposed to determine how many times will the <em>foo</em> function be called for some <em>n</em>. The result should be both an exact number (or the most accurate approximation possible) and asymptotic (like O(n)).</p>\n', 'ViewCount': '195', 'Title': 'Double-nested loop with bitwise operation', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-13T02:07:02.597', 'LastEditDate': '2012-09-22T23:09:47.477', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'OwnerDisplayName': 'Machta', 'PostTypeId': '1', 'OwnerUserId': '12141', 'Tags': '<time-complexity><imperative-programming><binary-arithmetic>', 'CreationDate': '2012-01-12T17:25:30.387', 'Id': '4681'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p><strong>Can I always increase the complexity of a problem via polynomial reduction?</strong> (in which case '<em>reduction</em>' is really a misnomer) For example, if I have a classic P problem (say, finding the smallest element in an array, by iterating through and making comparisons with the smallest value found thus far), what would be a corresponding problem in NP-complete (obtained via polynomial reduction)? Or, would it be possible to polynomially reduce a constant time (worst case) algorithm into a polynomial time algorithm (basically performing unnecessary executions for all n of the input for nothing)? </p>\n\n<p>As a side request, I'm looking for a basic reference that deals with how one problem (in P or NP-complete) can be reduced (in polynomial time) to a problem in (P or NP-complete). Most of the things I found online are beyond scope or vague. I'm looking for an easy way to figure out whether a certain Problem A (in P or NP-complete) can be polynomially reduced to a Problem B (in P or NP-complete).</p>\n", 'ViewCount': '326', 'Title': 'Clarifications on polynomial reducibility for problems in P and NP-complete', 'LastEditorUserId': '2835', 'LastActivityDate': '2012-10-08T22:46:53.090', 'LastEditDate': '2012-10-08T21:52:57.107', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '4961', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2835', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-10-08T21:45:59.520', 'Id': '4960'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '815', 'Title': 'What is the relationship between NP/NP-Complete/NP-Hard to time complexity?', 'LastEditDate': '2012-10-11T07:36:39.490', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2835', 'FavoriteCount': '3', 'Body': "<p>I'm familiar with a few problems of each class and even though the definitions are based on sets and polynomial reducibility, I see a pattern with time complexity. NP problems appear to be $O(2^n)$ (minus the P problems of course), and NP-hard problems seem to be worse: $n^n$ or $n!$ (Chess, TSP). Is this a misleading interpretation?</p>\n", 'Tags': '<complexity-theory><terminology><time-complexity><np-hard>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-12T04:51:51.627', 'CommentCount': '2', 'AcceptedAnswerId': '5011', 'CreationDate': '2012-10-11T05:29:06.593', 'Id': '5009'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3565', 'Title': 'A d-ary heap problem from CLRS', 'LastEditDate': '2012-10-17T19:50:03.620', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'lucasKoFromTW', 'PostTypeId': '1', 'OwnerUserId': '4195', 'Body': u'<p>I got confused while solving the following problem (questions 1\u20133).</p>\n\n<h3>Question</h3>\n\n<blockquote>\n  <p>A <em>d</em>-ary heap is like a binary heap, but(with one possible exception) non-leaf nodes have <em>d</em> children instead of 2 children.</p>\n  \n  <ol>\n  <li><p>How would you represent a <em>d</em>-ary heap in an array?</p></li>\n  <li><p>What is the height of a <em>d</em>-ary heap of <em>n</em> elements in terms of <em>n</em> and <em>d</em>?</p></li>\n  <li><p>Give an efficient implementation of EXTRACT-MAX in a <em>d</em>-ary max-heap. Analyze its running time in terms of <em>d</em> and <em>n</em>.</p></li>\n  <li><p><sub> Give an efficient implementation of INSERT in a <em>d</em>-ary max-heap. Analyze its running time in terms of <em>d</em> and <em>n</em>. </sub></p></li>\n  <li><p><sub> Give an efficient implementation of INCREASE-KEY(<em>A</em>, <em>i</em>, <em>k</em>), which flags an error if <em>k</em> &lt; A[i] = k and then updates the <em>d</em>-ary matrix heap structure appropriately. Analyze its running time in terms of <em>d</em> and <em>n</em>. </sub></p></li>\n  </ol>\n</blockquote>\n\n<h3>My Solution</h3>\n\n<ol>\n<li><p>Give an array $A[a_1 .. a_n]$</p>\n\n<p>$\\qquad \\begin{align}\n \\text{root} &amp;: a_1\\\\\n \\text{level 1} &amp;: a_{2} \\dots a_{2+d-1}\\\\\n \\text{level 2} &amp;: a_{2+d} \\dots a_{2+d+d^2-1}\\\\\n &amp;\\vdots\\\\\n \\text{level k} &amp;: a_{2+\\sum\\limits_{i=1}^{k-1}d^i} \\dots a_{2+\\sum\\limits_{i=1}^{k}d^i-1}\n \\end{align}$</p>\n\n<p>\u2192 <strong>My notation seems a bit sophisticated. Is there any other simpler one?</strong></p></li>\n<li><p>Let <em>h</em> denotes the height of the <em>d</em>-ary heap.</p>\n\n<p>Suppose that the heap is a complete <em>d</em>-ary tree\n$$\n 1+d+d^2+..+d^h=n\\\\\n \\dfrac{d^{h+1}-1}{d-1}=n\\\\\n h=log_d[n{d-1}+1] - 1\n $$</p></li>\n<li><p>This is my implementation:</p>\n\n<pre><code>EXTRACT-MAX(A)\n1  if A.heapsize &lt; 1\n2      error "heap underflow"\n3  max = A[1]\n4  A[1] = A[A.heapsize]\n5  A.heap-size = A.heap-size - 1\n6  MAX-HEAPIFY(A, 1)\n7  return max\n\nMAX-HEAPIFY(A, i)\n1  assign depthk-children to AUX[1..d]\n2  for k=1 to d\n3      compare A[i] with AUX[k]\n4      if A[i] &lt;= AUX[k]\n5          exchange A[i] with AUX[k]\n6          k = largest\n7  assign AUX[1..d] back to A[depthk-children]\n8  if largest != i\n9      MAX-HEAPIFY(A, (2+(1+d+d^2+..+d^{k-1})+(largest-1) )\n</code></pre>\n\n<ul>\n<li><p>The running time of MAX-HEAPIFY:</p>\n\n<p>$$T_M = d(c_8*d + (c_9+..+c_13)*d +c_14*d)$$\nwhere $c_i$ denotes the cost of <em>i</em>-th line above.</p></li>\n<li><p>EXTRACT-MAX:\n$$\n       T_E = (c_1+..+c_7) + T_M \\leq C*d*h\\\\\n       = C*d*(log_d[n(d-1)+1] - 1)\\\\\n       = O(dlog_d[n(d-1)])\n       $$</p></li>\n</ul>\n\n<p>\u2192 <strong>Is this an efficient solution? Or there is something wrong within my solution?</strong></p></li>\n</ol>\n', 'Tags': '<data-structures><time-complexity><runtime-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-12T17:23:56.180', 'CommentCount': '0', 'AcceptedAnswerId': '6097', 'CreationDate': '2012-10-15T03:45:12.463', 'Id': '6078'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Does there exist a procedure that determines if a polytime machine prints a certain string, and does so in less time than the machine itself takes to run?</p>\n\n<p>Define a machine $a$ that analyzes another machine $b$, input $i$, and string $r$:</p>\n\n<p>$a(b,i,r) = b(i) \\text{ prints } r$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $[T_b = O(n^k), k &gt;= 2]$</p>\n\n<p>Is there an $a$ with run-time $T_a \\in o(T_b)$ on all $b,i,r$?</p>\n', 'ViewCount': '89', 'Title': 'determine if a machine prints a certain string in less time than it takes to run the machine itself?', 'LastEditorUserId': '4223', 'LastActivityDate': '2012-11-29T23:05:04.600', 'LastEditDate': '2012-10-17T23:41:16.120', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4223', 'Tags': '<complexity-theory><time-complexity><turing-machines>', 'CreationDate': '2012-10-17T04:15:29.833', 'Id': '6117'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have two pieces of code in a function which I'm trying to calculate the asymptotic running time for:</p>\n\n<pre><code>for (int x = 0; x &lt; y; x++) {\n    total  +=  total;\n    total  +=  x;\n}\n</code></pre>\n\n<p>and:</p>\n\n<pre><code>while (y &gt; 0) {\n    total  -=  y;\n    y  =  y/2;\n}\n</code></pre>\n\n<p>Combining those two pieces of code, what is the run time of that function and how do I calculate it?</p>\n", 'ViewCount': '258', 'Title': 'Asymptotic time complexity of a two-loop program', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-17T11:59:24.930', 'LastEditDate': '2012-10-17T17:47:31.143', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'OwnerDisplayName': 'Lost', 'PostTypeId': '1', 'Tags': '<time-complexity><imperative-programming><loops>', 'CreationDate': '2012-10-16T16:19:38.743', 'Id': '6126'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '340', 'Title': 'Finding big O notation of function with two parameters', 'LastEditDate': '2012-10-19T23:55:51.233', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'mdjnewman', 'PostTypeId': '1', 'OwnerUserId': '4280', 'Body': "<p>I'm looking to work out the big-O notation for the following:</p>\n\n<p>$$\\frac{n^{s + 1} - 1}{n - 1} - 1$$</p>\n\n<p>I have a feeling the result is $O\\left( n^s \\right)$ but I'm not sure how to prove it.</p>\n\n<p>Any help greatly appreciated! :)</p>\n", 'Tags': '<time-complexity>', 'LastEditorUserId': '4280', 'LastActivityDate': '2012-10-19T23:55:51.233', 'CommentCount': '4', 'AcceptedAnswerId': '6177', 'CreationDate': '2012-10-19T07:39:03.893', 'Id': '6176'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>This is a homework problem for my introduction to algorithms course.</p>\n\n<blockquote>\n  <p>Recall the scheduling problem from Section 4.2 in which we sought to\n  minimize the maximum lateness. There are $n$ jobs, each with a deadline\n  $d_i$ and a required processing time $t_i$, and all jobs are available to be\n  scheduled starting at time $s$. For a job $i$ to be done, it needs to be assigned\n  a period from $s_i \\geq s$ to $f_i$ = $s_i + t_i$, and different jobs should be assigned\n  nonoverlapping intervals. As usual, such an assignment of times will be\n  called a schedule.</p>\n  \n  <p>In this problem, we consider the same setup, but want to optimize a\n  different objective. In particular, we consider the case in which each job\n  must either be done by its deadline or not at all. We\u2019ll say that a subset $J$ of\n  the jobs is schedulable if there is a schedule for the jobs in $J$ so that each\n  of them finishes by its deadline. Your problem is to select a schedulable\n  subset of maximum possible size and give a schedule for this subset that\n  allows each job to finish by its deadline.</p>\n  \n  <p>(a) Prove that there is an optimal solution $J$ (i.e., a schedulable set of\n  maximum size) in which the jobs in $J$ are scheduled in increasing\n  order of their deadlines.</p>\n  \n  <p>(b) Assume that all deadlines $d_i$ and required times $t_i$ are integers. Give\n  an algorithm to find an optimal solution. Your algorithm should\n  run in time polynomial in the number of jobs $n$, and the maximum\n  deadline $D = \\max_i d_i$.</p>\n</blockquote>\n\n<p>I've solved the problem as worded with the recurrence </p>\n\n<p>$Opt(i, d) = \\max\\left \\{ \n\\begin{array}\n \\\\ Opt(i-1, d-t_i) + 1 \\hspace{20 mm} d\\leq d_i\n \\\\ Opt(i-1, d) \n\\end{array}\n\\right \\}$</p>\n\n<p>but our instructor added a new requirement that our algorithm must not be dependent on D. This recurrence seems like it would produce an $O(nD)$ running time if implemented with dynamic programming.</p>\n\n<p>I can't figure out how to reduce its running time from $O(nD)$ to $O(n^k)$. To me it seems like it's a variation on the knapsack problem with all values equal to 1. In which case it seems like this is the best that can be done.</p>\n\n<p>If I'm doing something wrong could someone point me in the right direction, or if I've done everything right so far, could someone at least give me a hint as to how I can make an $O(n^k)$ recurrence or algorithm.</p>\n", 'ViewCount': '560', 'Title': 'Maximum Schedulable Set Zero-Lateness Deadline Scheduling', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-30T18:55:05.237', 'LastEditDate': '2012-10-31T09:59:21.227', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4294', 'Tags': '<algorithms><time-complexity><dynamic-programming><efficiency><scheduling>', 'CreationDate': '2012-10-20T23:13:27.230', 'FavoriteCount': '2', 'Id': '6202'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for a $\\Theta$ approximation of\n$$T(n) = T(n-1) + cn^{2}$$</p>\n\n<p>This is what I have so far:</p>\n\n<p>$$\n\\begin{align*}\nT(n-1)&amp; = T(n-2) + c(n-1)^2\\\\\nT(n) &amp;= T(n-2) + c(n-1) + cn^2\\\\[1ex]\nT(n-2) &amp;= T(n-3) + c(n-2)^2\\\\\nT(n) &amp; = T(n-3) + c(n-2)^2 + c(n-1)^2 + cn^2 \\\\[1ex]\nT(n-3) &amp;= T(n-4) + c(n-3)^2 \\\\\nT(n) &amp;= T(n-4) + c(n-3)^2 + c(n-2)^2 + c(n-1)^2 + cn^2\n\\end{align*}\n$$</p>\n\n<p>So, at this point I was going to generalize and substitute $k$ into the equation.</p>\n\n<p>$$T(n)= T(n-k) + (n-(k-1))^2 + c(k-1)^2$$</p>\n\n<p>Now, I start to bring the base case of 1 into the picture. On a couple of previous, more simple problems, I was able to set my generalized k equation equal to 1 and then solve for $k$. Then put $k$ back into the equation to get my ultimate answer.</p>\n\n<p>But I am totally stuck on the $(n-k+1)^2$ part. I mean, should I actually foil all this out? I did it and got $k^2-2kn-2k+n^2 +2n +1 = 1$. At this point I'm thinking I totally must have done something wrong since I've never see this in previous problems.</p>\n\n<p>Could anyone offer me some help with how to solve this one? I would greatly appreciate it. I also tried another approach where I tried to set $n-k = 0$ from the last part of the equation and got that $k = n$. I plugged n back into the equation towards the end and ultimately got $n^2$ as an answer. I have no clue if this is right or not.</p>\n\n<p>I am in an algorithms analysis class and we started doing recurrence relations and I'm not 100% sure if I am doing this problem correct. I get to a point where I am just stuck and don't know what to do. Maybe I'm doing this wrong, who knows. The question doesn't care about upper or lower bounds, it just wants a theta.</p>\n", 'ViewCount': '1572', 'Title': 'Recurrence relation for time complexity $T(n) = T(n-1) + n^2$', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-28T00:27:27.063', 'LastEditDate': '2012-10-25T22:53:46.943', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '6275', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4336', 'Tags': '<time-complexity><algorithm-analysis><proof-techniques><recurrence-relation>', 'CreationDate': '2012-10-24T00:56:53.863', 'Id': '6274'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the time complexity of computing $\\frac{1}{2^n} {{n}\\choose{(n+2)/2}}$?</p>\n\n<p>$$\\frac{1}{2^n} {{n}\\choose{(n+2)/2}} = \\frac{1}{2^n} \\frac{n(n-1)\\cdots ((n-2)/2)}{((n+2)/2) (n/2) \\cdots 1}$$</p>\n\n<p>The numerator and denominator in $\\frac{n(n-1)\\cdots ((n-2)/2)}{((n+2)/2) (n/2) \\cdots 1}$ will take $O(n)$ multiplications.</p>\n\n<p>$2^n$ will take $n$ multiplications.</p>\n\n<p>So in total, there will be $O(n)$ multiplications. Is it the correct time complexity?</p>\n', 'ViewCount': '124', 'Title': 'What is the time complexity of computing $\\frac{1}{2^n} {{n}\\choose{(n+2)/2}}$', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-25T11:00:25.887', 'LastEditDate': '2012-10-25T11:00:25.887', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><time-complexity><discrete-mathematics>', 'CreationDate': '2012-10-24T18:37:04.533', 'FavoriteCount': '0', 'Id': '6295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have the following pseudo code:    </p>\n\n<pre><code>Multiply(y,z)    \n1. if (z==0) return 0     \n2. else if (z is odd)    \n3.    then return (Multiply(2y, floor(z/2)) + y )    \n4. else return (Multiply(2y, floor(z/2)))    \n</code></pre>\n\n<p>Towards analysing this procedure's runtime, this recurrence relation is given as answer:</p>\n\n<p>$\\qquad \\displaystyle T(z) = \\begin{cases} 0 &amp; z=0 \\\\ T(z/2)+1 &amp; z&gt;0\\end{cases}$ </p>\n\n<p>Why is $T(z)=0$ when $z=0$? Shouldn't it be $1$ for this case?        </p>\n\n<p>And, the $+1$ in $T(z/2)\\mathbf{+1}$ is because the worst case is \n<code>(multiply(2y, floor(z/2)) + y</code> (note the <code>+ y</code>). Am I correct?     </p>\n", 'ViewCount': '128', 'Title': 'How does this recurrence relation fit the algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-29T00:23:43.783', 'LastEditDate': '2012-10-28T11:02:22.307', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4379', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><recurrence-relation>', 'CreationDate': '2012-10-28T01:06:28.050', 'Id': '6345'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '376', 'Title': 'Multitape Turing machines against single tape Turing machines', 'LastEditDate': '2012-10-31T19:13:05.493', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'user7760', 'PostTypeId': '1', 'OwnerUserId': '4409', 'Body': '<p><em>Introduction</em>: I recently learned that a multi-tape Turing Machine $\\text{TM}_k$  is no more "powerful" than a single tape Turing machine $\\text{TM}$. The proof that  $\\text{TM}_k \\equiv \\text{TM}$ is based on the idea that a $\\text{TM}$ can simulate a $\\text{TM}_k$ by using a unique character to separate the respective areas of each of the $k$ tapes.</p>\n\n<p>Given this idea, how would we prove that a process taking $t(n)$ time on a $\\text{TM}_k$ can be simulated by a 2-tape Turing machine $\\text{TM}_2$ with $ O(t(n))\\log(t(n))$ time?</p>\n', 'Tags': '<time-complexity><turing-machines><simulation><tape-complexity>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-10-31T19:13:05.493', 'CommentCount': '0', 'AcceptedAnswerId': '6387', 'CreationDate': '2012-10-29T21:06:01.913', 'Id': '6385'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '985', 'Title': "Base of logarithm in runtime of Prim's and Kruskal's algorithms", 'LastEditDate': '2012-11-01T22:48:50.390', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'CodeKingPlusPlus', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Body': "<p>For Prim's and Kruskal's Algorithm there are many implementations which will give different running times. However suppose our implementation of Prim's algorithm has runtime $O(|E| + |V|\\cdot \\log(|V|))$ and Kruskals's algorithm has runtime $O(|E|\\cdot \\log(|V|))$.</p>\n\n<p>What is the base of the $\\log$?</p>\n", 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T22:48:50.390', 'CommentCount': '3', 'AcceptedAnswerId': '6436', 'CreationDate': '2012-10-27T20:48:47.810', 'Id': '6435'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am currently working on a project where I\'m using an implementation of Hoffman and Pavley\'s "<a href="http://dl.acm.org/citation.cfm?doid=320998.321004" rel="nofollow">Method for the Solution of the Nth Best Path Problem</a>" to find n-th best path through a directed graph. The implementation is based on <a href="http://quickgraph.codeplex.com/wikipage?title=Ranked%20Shortest%20Path" rel="nofollow">QuickGraph\'s Ranked Shortest Path implementation</a>.</p>\n\n<p>I have been trying to determine the complexity of Hoffman and Pavley\'s algorithm as well as QuickGraph\'s implementation, but without any luck -- so basically my question is if someone knows the complexity of the original method proposed by Hoffman and Pavley as well as the complexity of QuickGraph\'s implementation?</p>\n', 'ViewCount': '175', 'Title': "What is the complexity of Hoffman and Pavley's Nth best path algorithm?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-02T10:09:43.120', 'LastEditDate': '2012-11-02T10:09:43.120', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4433', 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><shortest-path>', 'CreationDate': '2012-11-02T08:43:34.007', 'FavoriteCount': '1', 'Id': '6444'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would I solve these problems involving time complexity:</p>\n\n<ol>\n<li><p>Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$, insertion sort runs in $8n^2$ steps, while merge sort runs in $64n \\log_2 n$ steps. For which values of $n$ does insertion sort beat merge sort?</p></li>\n<li><p>What is the smallest value of n such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine?</p></li>\n<li><p>For each function $f(n)$ and time $t$ , determine the largest size $n$ of a problem that can be solved in time $t$, assuming that the algorithm to solve the problem takes $f(n)$ microseconds. </p>\n\n<p>(a) $n! = 1$ second </p>\n\n<p>(b) $n \\log_2 n$ = 1 second</p></li>\n</ol>\n', 'ViewCount': '492', 'Title': 'Comparing Time complexity?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-04T06:34:16.437', 'LastEditDate': '2012-11-03T10:04:20.713', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-5', 'PostTypeId': '1', 'OwnerUserId': '4442', 'Tags': '<time-complexity><algorithm-analysis>', 'CreationDate': '2012-11-03T03:42:34.377', 'Id': '6459'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '600', 'Title': 'Iterative binary search analysis', 'LastEditDate': '2012-11-04T15:41:46.187', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Hasan Tahsin', 'PostTypeId': '1', 'OwnerUserId': '4610', 'Body': '<p>I\'m a little bit confused about the analysis of <a href="http://en.wikipedia.org/wiki/Binary_search" rel="nofollow">binary search</a>.\nIn almost every paper, the writer assumes that the array size $n$ is always $2^k$.\nWell I truly understand that the time complexity becomes $\\log(n)$ (worst case) under this assumption. But what if $n \\neq 2^k$?</p>\n\n<p>For example if $n=24$, then we have\n5 iterations for 24<br>\n4 i. for 12<br>\n3 i. for 6<br>\n2 i. for 3<br>\n1 i. for 1</p>\n\n<p>So how do we get the result $k=\\log n$ in this example (I mean of course every similar example whereby $n\\neq2^k$)?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-04T15:41:46.187', 'CommentCount': '4', 'AcceptedAnswerId': '6471', 'CreationDate': '2012-11-03T10:15:40.587', 'Id': '6470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For a list of integers, of size n, where n is exponential, will merge-sort(n), run in poly-time or psuedo poly-time?</p>\n', 'ViewCount': '146', 'Title': 'Exponential input and poly-time algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-27T23:40:33.147', 'LastEditDate': '2012-11-05T08:06:37.387', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4365', 'Tags': '<algorithms><terminology><time-complexity><polynomial-time>', 'CreationDate': '2012-11-04T20:13:32.497', 'FavoriteCount': '1', 'Id': '6478'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a group of n sets for which I need to calculate a sort of "uniqueness" or "similarity" value.  I\'ve settled on the <a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a> as a suitable metric.  Unfortunately, the Jaccard index only operates on two sets at a time.  In order to calculate the similarity between all $n$ sets, it will require in the order of $n^2$ Jaccard calculations.</p>\n\n<p>(If it helps, $n$ is usually between 10 and 10000, and each set contains on average 500 elements.  Also, in the end, I don\'t care how similar any two specific sets are - rather, I only care what the internal similarity of the whole group of sets is. (In other words, the mean (or at least a sufficiently accurate approximation of the mean) of all Jaccard indexes in the group))</p>\n\n<p>Two questions:</p>\n\n<ol>\n<li>Is there a way to still use the Jaccard index without the $n^2$ complexity?</li>\n<li>Is there a better way to calculate set similarity/uniqueness across a group of sets than the way I\'ve suggested above?</li>\n</ol>\n', 'ViewCount': '477', 'Title': 'Set Similarity - Calculate Jaccard index without quadratic complexity', 'LastActivityDate': '2013-01-24T12:02:54.673', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '7', 'OwnerDisplayName': 'rinogo', 'PostTypeId': '1', 'OwnerUserId': '4498', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2012-11-06T16:12:54.127', 'Id': '6526'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am following <strong>"Introduction to the theory of computation" by Sipser</strong>.</p>\n\n<p>My question is about relationship of different classes which is present in <strong>Chapter 8.2. The Class PSPACE</strong>.</p>\n\n<p>$P \\subseteq NP \\subseteq PSPACE = NPSPACE \\subseteq EXPTIME$</p>\n\n<p>I am trying to understand why the the following part is true $NPSPACE \\subseteq EXPTIME$.</p>\n\n<p>The explanation from the textbook is following: </p>\n\n<p><em>"For $f(x)\\geq n$, a TM that  uses $f(x)$ space can have at most $f(n)2^{O(f(n))}$ different configurations, by a simple generalization of the proof of the Lemma 5.8 on page 194. A TM computation that halts may not repeat a configuration. Therefore a TM that uses space $f(n)$ must run in time $f(n)2^{O(f(n))}$, so $NPSPACE \\subseteq EXPTIME$"</em></p>\n\n<p>I am trying to understand why it\'s true, why TM that uses $f(n)$ space must run in time $f(n)2^{O(f(n))}$. Let\'s try to reverseengeneer the formula: $n$ is the length of the input, 2 is the size of alphabet, $f(n)$ is the space that TM use on the second tape (operational tape) and $f(n) \\geq n$, but how to explain what $O(f(n))$ means. Apparently, $2^{O(f(n))}$ expreses a configuration, so $O(f(n))$ must express union of transition function and alphabet, but actually it seems like I get it wrong. The most intriguing question why, in the end,  $f(n)2^{O(f(n))}$ expressed in the terms of time, the transition from space to time is very vague for me.</p>\n\n<p>I will very appreciate if someone could explain me this relationship.</p>\n', 'ViewCount': '153', 'Title': 'Proving that NPSPACE $\\subseteq$ EXPTIME', 'LastActivityDate': '2012-11-13T19:29:32.513', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6651', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1170', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2012-11-13T18:43:57.293', 'Id': '6649'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to ask you some clarification on the following question:\nknow that ${\\sf NP}$ is a subset of ${\\sf IP}$ \nand also ${\\sf coNP}$ it is a subset of ${\\sf IP}$.\nSo ${\\sf IP}$ is a biggest class, but how much it is big?</p>\n\n<p>May i say that ${\\sf PSPACE}$ is a subset of ${\\sf IP}$? or that they may intersect?</p>\n\n<p>Can you give me a easy clarification about it?</p>\n', 'ViewCount': '160', 'Title': 'Relation between interactive proof systems (IP), NP, coNP, PSPACE', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-16T21:07:21.567', 'LastEditDate': '2012-11-16T21:07:21.567', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4488', 'Tags': '<complexity-theory><time-complexity><space-complexity><complexity-classes><interactive-proof-systems>', 'CreationDate': '2012-11-15T13:47:04.607', 'Id': '6679'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '271', 'Title': 'Proving that if $\\mathrm{NTime}(n^{100}) \\subseteq \\mathrm{DTime}(n^{1000})$ then $\\mathrm{P}=\\mathrm{NP}$', 'LastEditDate': '2012-12-13T02:35:05.343', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2329', 'FavoriteCount': '3', 'Body': "<p>I'd really like your help with proving the following.</p>\n\n<p>If $\\mathrm{NTime}(n^{100}) \\subseteq \\mathrm{DTime}(n^{1000})$ then $\\mathrm{P}=\\mathrm{NP}$.</p>\n\n<p>Here, $\\mathrm{NTime}(n^{100})$ is the class of all languages which can be decided by nondeterministic Turing machine in polynomial time of $O(n^{100})$ and $\\mathrm{DTime}(n^{1000})$  is the class of all languages which can be decided by a deterministic Turing machine in polynomial time of $O(n^{1000})$.</p>\n\n<p>Any help/suggestions?</p>\n", 'Tags': '<time-complexity><complexity-classes><p-vs-np>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-13T02:35:05.343', 'CommentCount': '2', 'AcceptedAnswerId': '6730', 'CreationDate': '2012-11-16T12:57:46.333', 'Id': '6695'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '158', 'Title': 'How can repeated addition/multiplication be done in polynomial time?', 'LastEditDate': '2012-11-22T23:53:23.740', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '1', 'Body': '<p>I can see how adding 2 unsigned four-bit values is $O(n)$. We just go from the rightmost digits to the leftmost digits and add the digits up sequentially. We can also perform multiplication in polynomial time ($O(n^2)$) via the algorithm we all learned in grade school.</p>\n\n<p>However, how can we add up or multiply say $i$ numbers together in polynomial time? After we add up 2 numbers together, we get a bigger number that will require more bits to represent. Same with multiplication.</p>\n\n<p>How can we ensure that these extra bits do not produce exponential blowup?</p>\n', 'Tags': '<time-complexity><arithmetic>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-23T00:03:19.090', 'CommentCount': '1', 'AcceptedAnswerId': '6845', 'CreationDate': '2012-11-22T23:18:09.637', 'Id': '6843'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/3495/complexity-inversely-propotional-to-n">Complexity inversely propotional to $n$</a>  </p>\n</blockquote>\n\n\n\n<p>I\'m curious if anyone\'s come up with a problem or method as n => infinity t => 0. Are there any sort of cases found in quantum computing?</p>\n', 'ViewCount': '96', 'ClosedDate': '2012-11-23T09:32:28.617', 'Title': 'is there an example of an algorithm that has O(1/n)?', 'LastActivityDate': '2012-11-23T00:19:24.717', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6849', 'Score': '0', 'OwnerDisplayName': 'zcaudate', 'PostTypeId': '1', 'OwnerUserId': '1800', 'Tags': '<time-complexity>', 'CreationDate': '2012-11-22T20:56:21.603', 'Id': '6846'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I understood what amortized analysis does, but can anyone tell me what is the main purpose of this kind of analysis?</p>\n\n<p>What I understood:</p>\n\n<p>Let say we have 3 three operations a,b,c used 1,2 and 3 times to achieve d. Based on aggregate analysis a,b and c are used 2 times each. Is this correct?</p>\n\n<p>I am trying to understand the advantages of this in CLRS but I am completely lost. For example in dynamic programming we save the answers to sub problems in tables which helps us reduce the running time(lets say from exponential to polynomial). But I am unable to get a complete picture of amortized analysis.</p>\n', 'ViewCount': '356', 'Title': 'Advantages of amortized analysis', 'LastEditorUserId': '2755', 'LastActivityDate': '2014-05-03T23:17:14.713', 'LastEditDate': '2012-11-25T07:34:07.733', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '6873', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<time-complexity><algorithm-analysis><amortized-analysis>', 'CreationDate': '2012-11-24T07:49:27.967', 'Id': '6865'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am new to "Computational Complexity" and therefore I have enough problems with some exercises like the following one:</p>\n\n<blockquote>\n  <p>Remember: $\\text{PH} := \\bigcup_{i} \\Sigma_i$</p>\n  \n  <p>Show:</p>\n  \n  <p>$\\bullet \\bigcup_{i}(\\Sigma_i \\cup \\Pi_i \\cup \\Delta_i) = \\bigcup_{i}\\Sigma_i = \\bigcup_{i}\\Pi_i = \\bigcup_{i}\\Delta_i$</p>\n  \n  <p>$\\bullet \\forall k \\in \\mathbb{N} (\\Sigma_k = \\Pi_k \\Rightarrow \\text{PH} = \\Sigma_k)$</p>\n</blockquote>\n\n<p>I have tried to make myself familiar with <a href="http://en.wikipedia.org/wiki/Polynomial_hierarchy" rel="nofollow">Polynomial hierarchy</a>, but on the one hand I don\'t understand the meaning of the symbols $\\Delta, \\Sigma, \\Pi$ and on the other hand I don\'t know how to solve the actual exercise.</p>\n\n<p>Can somebody give me some help, please?</p>\n', 'ViewCount': '151', 'Title': 'Notations around the polynomial hierarchy', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-28T05:10:48.793', 'LastEditDate': '2012-11-24T19:10:26.167', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4713', 'Tags': '<complexity-theory><terminology><time-complexity>', 'CreationDate': '2012-11-24T11:26:59.523', 'FavoriteCount': '1', 'Id': '6870'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I think the following exercise is to "warm up", but nevertheless it\'s quite difficult for me:</p>\n\n<blockquote>\n  <p>Let $k \\in \\mathbb{N}$ and let $L \\in \\Sigma_k$. Show that also $L^{*} \\in \\Sigma_k$.</p>\n</blockquote>\n\n<p>The following details from my lecture notes seem to be useful:</p>\n\n<p>Notation. Let $n \\in \\mathbb{N}$.</p>\n\n<p>We write $\\exists_n y. \\varphi(y)$ for $\\exists y \\in \\Sigma^{*}.|y| \\le n \\wedge \\varphi(y)$.</p>\n\n<p>We write $\\forall_n y. \\varphi(y)$ for $\\forall y \\in \\Sigma^{*}.|y| \\le n \\Rightarrow \\varphi(y)$.</p>\n\n<p>Theorem.</p>\n\n<p>$L \\in \\Sigma^P_i \\Leftrightarrow$ there is a language $A \\in P$ and a polynomial $p$ so that: $x \\in L \\Leftrightarrow \\exists_{p(|x|)}y_1 \\forall_{p(|x|)}y_2 \\exists_{p(|x|)}y_3 .../\\forall_{p(|x|)}y_i (x,y_1,y_2,...,y_i) \\in A$</p>\n\n<p>Unfortunately I don\'t see the solution of the "puzzle". Can somebody please help me a little bit (despite the fact that it\'s weekend)?</p>\n', 'ViewCount': '76', 'Title': 'Show that a language belongs to the polynomial hierarchy', 'LastActivityDate': '2012-11-24T18:25:10.387', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6875', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4713', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-11-24T17:22:31.363', 'Id': '6874'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The following exercise gives me headaches:</p>\n\n<blockquote>\n  <p>Show: If the polynomial hierarchy is strict (i.e. $\\forall k \\in \\mathbb{N}. \\Sigma_k \\neq \\Sigma_{k+1}$), then there is no $\\text{PH}$-complete problem for polynomial-time reductions (i.e. there is no problem $\\text{P} \\in \\text{PH}$ such that each problem in $\\text{PH}$ can be reduced to $\\text{P}$ through a function $f \\in \\text{FP}$).</p>\n</blockquote>\n\n<p>The exercise describes in detail what has to be shown, but my issue is that I don't know how to show this. Can somebody please help me?</p>\n", 'ViewCount': '66', 'Title': 'Strict polynomial hierarchy and reduction', 'LastActivityDate': '2012-11-26T17:57:15.720', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '6926', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4713', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-11-25T13:47:45.003', 'Id': '6885'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The title of the question expresses what I\'m looking for - this is to help me better understand the prerequisites for the <a href="http://en.wikipedia.org/wiki/Time_hierarchy_theorem#Non-deterministic_time_hierarchy_theorem">Non-Deterministic Time Hierarchy Theorem</a></p>\n\n<p>For instance, the Arora-Barak book explains the theorem using $g(n) = n$ and $G(n) = n^{1.5}$ - but, I can see that $n \\in o(n^{1.5})$ as well! So, I\'m trying to better understand what "extra" time is guaranteed by specifying that in order for $\\text{NTIME}(g(n))$ to be a proper subset of $\\text{NTIME}(G(n))$, $g(n + 1) = o(G(n))$, <strong>not</strong> $g(n) = o(G(n))$...  </p>\n', 'ViewCount': '127', 'Title': 'Two functions $g(n)$, $G(n)$ such that $g(n) = o(G(n))$ but $g(n+1) \\neq o(G(n))$', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-27T00:09:41.660', 'LastEditDate': '2012-11-26T22:05:53.567', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '6932', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '476', 'Tags': '<complexity-theory><time-complexity><asymptotics><landau-notation>', 'CreationDate': '2012-11-26T20:16:07.620', 'Id': '6929'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Unfortunately I have no idea how to show this:</p>\n\n<blockquote>\n  <p>Show that the set of ${\\sf P}$-complete languages is not closed under intersection.</p>\n</blockquote>\n\n<p>As far as I understand my lecture notes, ${\\sf P}$-completeness is defined as follows:</p>\n\n<ul>\n<li>$A \\subset \\Sigma^{*}$ is complete for ${\\sf P}$ iff $A \\in \\text{P}$ and $\\forall B \\in {\\sf P}, B \\le_L A$</li>\n<li>$\\le_L$ is ${\\sf LOGSPACE}$-reduction: for $A,B \\subset \\Sigma^{*}$, the relation $A \\le_{L} B$ is defined by\n$$A \\le_{L} B \\quad\\text{iff}\\quad \\exists f \\in {\\sf FLOGSPACE}, (x \\in A \\Leftrightarrow f(x) \\in B)$$</li>\n</ul>\n', 'ViewCount': '224', 'Title': 'Proof for P-complete is not closed under intersection', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-04T12:04:27.640', 'LastEditDate': '2012-11-29T20:54:49.207', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4713', 'Tags': '<complexity-theory><time-complexity><reductions>', 'CreationDate': '2012-11-29T12:48:39.203', 'Id': '7014'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In famous Structure and Interretation of Computer Programs, there is an exercise (<a href="http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#%_thm_1.14" rel="nofollow">1.14</a>), that asks for the time complexity of the following algorithm - in Scheme - for counting change (the problem statement suggests drawing the tree for <code>(cc 11 5)</code> - which looks <a href="http://telegraphics.com.au/~toby/sicp/ex1-14.svg" rel="nofollow">like this</a>):</p>\n\n<pre><code> ; count change\n (define (count-change amount)\n   (define (cc amount kinds-of-coins)\n     (cond ((= amount 0) 1)\n           ((or (&lt; amount 0) (= kinds-of-coins 0)) 0)\n           (else (+ (cc (- amount\n                           (first-denomination kinds-of-coins))\n                        kinds-of-coins)\n                    (cc amount\n                        (- kinds-of-coins 1))))))\n   (define (first-denomination kinds-of-coins)\n     (cond ((= kinds-of-coins 1) 1)\n           ((= kinds-of-coins 2) 5)\n           ((= kinds-of-coins 3) 10)\n           ((= kinds-of-coins 4) 25)\n           ((= kinds-of-coins 5) 50)))\n   (cc amount 5))\n</code></pre>\n\n<p>Now... there are sites with solutions to the SICP problems, but I couldn\'t find any easy to understand proof for the <em>time</em> complexity of the algorithm - there is a mention somewhere that it\'s polynomial <code>O(n^5)</code></p>\n', 'ViewCount': '120', 'Title': 'Time complexity for count-change procedure in SICP', 'LastActivityDate': '2012-12-02T18:19:47.847', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7106', 'Score': '0', 'OwnerDisplayName': 'NeuronQ', 'PostTypeId': '1', 'OwnerUserId': '4844', 'Tags': '<algorithms><time-complexity><space-complexity>', 'CreationDate': '2012-11-22T20:09:19.883', 'Id': '7105'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the best known approximation for the computational complexity of the clique problem? Is it accurate to consider it $O(2^n)$?</p>\n', 'ViewCount': '116', 'Title': 'Computational complexity of the clique problem', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-03T08:10:55.423', 'LastEditDate': '2012-12-03T07:57:07.407', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7113', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4846', 'Tags': '<complexity-theory><time-complexity><algorithm-analysis>', 'CreationDate': '2012-12-03T00:18:28.903', 'Id': '7112'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I had the choice of choosing one out of the following two optimization problems which I could use to solve my problem. Which choice is the fastest? How much of a trade-off would it be?  Is the improvement in speed by many factors!?</p>\n\n<ol>\n<li><p>Minimizing a convex function $L(X)$ in one matrix variable with orthogonality constraints over the matrix-essentially in my case this ends up to solving an eigen-decomposition.</p></li>\n<li><p>Minimizing the same convex function $L(X)$ with linear constraints in $X$.</p></li>\n</ol>\n\n<p>I know that 2.) should be faster. But what is the direction of work I need to do- to compare the improvement in speed-especially in terms of using the fastest available eigen solver for 1.)-what would be the corresponding fastest approach to solve 2.)?</p>\n', 'ViewCount': '61', 'Title': 'Time - Complexity Convex Optimization and Eigen Decomposition', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T00:32:56.540', 'LastEditDate': '2012-12-06T10:17:22.360', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'VSPC', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity><optimization><efficiency><linear-algebra>', 'CreationDate': '2012-10-16T18:17:29.447', 'Id': '7206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to show that reasonable advice can really speed up computation.</p>\n\n<p>Show, that every time-constructible function $t$, there exists a set $S$ in time $\\text{DTIME}(t^2) \\setminus \\text{DTIME}(t)$ that can be decided in linear time using an advice of linear length, $S \\in \\text{DTIME}(l) /  l$ (definition in analogy to $P / \\text{poly}$), where $l(n)=O(n)$.</p>\n\n<p>The problem is that I cannot come up with the structure of set.</p>\n', 'ViewCount': '53', 'Title': 'Advice speeds up computations', 'LastEditorUserId': '683', 'LastActivityDate': '2012-12-10T18:11:09.897', 'LastEditDate': '2012-12-10T18:06:46.360', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7310', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2012-12-10T17:51:28.267', 'Id': '7308'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a deterministic Turing machine with an input tape and a work tape. The work tape is restricted to $\\log_2 n+100$ cells ($n$ represents the input length) and its tape alphabet is of size $2006$. Moreover, the Turing machine has $27$  states.</p>\n\n<p>I wonder how come the running time of such machine is $O(n^{1+\\log_2 2006}\\cdot \\log_2n)$ (The answer is the correct choice for this multiple choices question out of an exam I practise)</p>\n', 'ViewCount': '68', 'Title': 'How come this turing machine running time is $O(n^{1+\\log_2 2006}\\cdot \\log_2n)$', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-14T22:12:10.200', 'LastEditDate': '2012-12-14T22:12:10.200', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7399', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<complexity-theory><time-complexity><turing-machines>', 'CreationDate': '2012-12-14T18:55:50.397', 'Id': '7398'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Summation of $O(n)$ from $1\\le k\\le n$.</p>\n\n<p>I think it should be $O(n)$ only. Because we are addition $O(k)$ and maximum order will be $O(n)$. But answer is given as $O(n^2)$.</p>\n\n<p>Correct me if I'm wrong.</p>\n", 'ViewCount': '126', 'Title': 'Summation of $O(n)$ from $1\\le k\\le n$', 'LastEditorUserId': '3016', 'LastActivityDate': '2012-12-19T03:47:23.047', 'LastEditDate': '2012-12-18T22:44:33.537', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '7500', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4763', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2012-12-17T22:29:28.933', 'Id': '7479'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In [1] <em>strongly-polynomial</em> is defined as either:</p>\n\n<ul>\n<li>The algorithm runs in strongly polynomial time if the algorithm is a polynmomial space algorithm and performs a number of elementary arithmetic operations which is bounded by a polynomial in the number of input numbers.</li>\n<li>A polynomial algorithm is a polynomial space algorithm (in our standard Turing machine model) and polynomial time algorithm in the arithmetic model (see this question for a clarification).</li>\n</ul>\n\n<p>Why do they restrict to polynomial TM space as opposed to polynomial TM time? (this came up <a href="http://cs.stackexchange.com/a/7542/5106">here</a>)</p>\n\n<p>It seems strange for an algorithm that takes a number of TM steps unboundable by a polynomial to still be considered strongly-polynomial (provided it takes polynomial space and a number of arithmetic operations polynomial in the number of numbers in the input). Can it be shown that such an algorithm does not exist? Perhaps based on this argument: the number of arithmetic operations would not be polynomial, since under the arithmetic model every operation is an arithmetic operation (?).</p>\n\n<p>[1] Gr\xf6tschel, Martin; L\xe1szl\xf3 Lov\xe1sz, Alexander Schrijver (1988). "Complexity, Oracles, and Numerical Computation". Geometric Algorithms and Combinatorial Optimization. Springer. ISBN 0-387-13624-X.</p>\n', 'ViewCount': '203', 'Title': 'Are there strongly-polynomial algorithms that take more than polynomial time?', 'LastActivityDate': '2012-12-22T11:18:34.597', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5106', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2012-12-22T02:53:58.370', 'Id': '7543'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to solve an exercise in <strong>Distributed Algorithm</strong> described as follow.</p>\n\n<p>Let\'s define a <em>label assignment problem</em> as follow. There is a anonymous network (vertices don\'t have unique ID\'s , nodes don\'t have any clue about $n$ - number of vertices nor about network topology ). The labels assigned to vertices must be distinct and there is no limitation on the length of the label. <em>The initiator is given.</em> Show the algorithm for solving the problem in asynchronous model with time complexity $\\text{O}(D)$ and using $\\text{O}(m)$ messages (where $D$ is the diameter of graph and $m$ number of edges), prove that the labels are indeed distinct. Show the changes in complexities if the algorithm is executed on the synchronous network.</p>\n\n<p>Let\'s say that the initiator is the node A, wlog assign it $A_{id} = 1$, broadcast $A_{id} = 1$ to all children of $A$ by <a href="http://en.wikipedia.org/wiki/Flooding_%28computer_networking%29" rel="nofollow">flooding algorithm</a>, any node $B$ by receiving message from it\'s parent for the first time will assign $B_{id} = PARENT_{id}+1$ and send $B_{id}$ to it\'s children, if $B_{id}$ is already defined, drop the message. $T=\\text{O}(D)$ - time complexity, $M=\\text{O}(m)$ - message complexity for broadcast.</p>\n\n<p>It seems like on synchronous network the complexities will be the same, just becasue <strong>broadcast</strong> and <strong>convergecast</strong> on synchronous and asynchronous network run in $T=\\text{O(D)}$ and $M=\\text{O}(m)$.</p>\n\n<p>If there are any change in complexities on the synchronous network? Does the above algorithm look good?</p>\n', 'ViewCount': '142', 'Title': 'Label Assignment Problem', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-04-26T09:08:40.590', 'LastEditDate': '2012-12-27T07:22:32.760', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><distributed-systems>', 'CreationDate': '2012-12-26T09:48:16.627', 'Id': '7603'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '138', 'Title': 'Computing time complexity', 'LastEditDate': '2012-12-26T14:53:25.097', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Rastegar', 'PostTypeId': '1', 'OwnerUserId': '5202', 'Body': '<p>If we have an algorithm such that its complexity is $\\Theta(m + n^2)$ and we know that $0 &lt; m &lt; n^2$, then its complexity becomes $\\Theta(n^2)$. But if we had an algorithm such that its complexity was $\\Theta(m\\log n)$ and $0 &lt; m &lt; n^2$, could we conclude that its complexity is $\\Theta(n^2\\log n)$?</p>\n', 'Tags': '<time-complexity><landau-notation>', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-26T18:06:57.817', 'CommentCount': '4', 'AcceptedAnswerId': '7606', 'CreationDate': '2012-12-26T11:39:49.503', 'Id': '7604'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '206', 'Title': 'randomized algorithm for checking the satisfiability of s-formulas, that outputs the correct answer with probability at least $\\frac{2}{3}$', 'LastEditDate': '2013-01-05T20:18:01.783', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '1', 'Body': "<p>I'm trying to practice myself with random algorithms.</p>\n\n<p>Lets call a CNF formula over n variables s-formula if it is either unsatisable or it has at\nleast $\\frac{2^n}{n^{10}}$ satisfying assignments.</p>\n\n<p>I would like your help with show a randomized algorithm for checking the\nsatisfiability of s-formulas, that outputs the correct answer with probability at\nleast $\\frac{2}{3}$.</p>\n\n<p>I'm not really sure how to prove it. First thing that comes to my head is this thing- let's accept with probability $\\frac{2}{3}$ every input. Then if the input in the language, it was accepted whether in the initial toss($\\frac{2}{3}$) or it was not and then the probability to accept it is $\\frac{1}{3}\\cdot proability -to-accept$ which is bigger than $\\frac{2}{3}$. Is this the way to do that or should I use somehow Chernoff inequality which I'm not sure how.</p>\n", 'Tags': '<complexity-theory><time-complexity><randomized-algorithms>', 'LastEditorUserId': '1589', 'LastActivityDate': '2013-01-05T20:18:01.783', 'CommentCount': '0', 'AcceptedAnswerId': '7748', 'CreationDate': '2012-12-29T09:38:45.680', 'Id': '7641'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How do you determine the Time Complexity of a Genetic Algorithm(in general)? If possible.</p>\n\n<p>I have been thinking about this a lot, and all of the teaching I have had is related to determining the Time Complexity of problems that are much less stochastic in nature.</p>\n', 'ViewCount': '437', 'Title': 'Time Complexity of Genetic Algorithms', 'LastEditorUserId': '55', 'LastActivityDate': '2013-01-06T21:35:42.127', 'LastEditDate': '2013-01-06T02:35:03.520', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '110', 'Tags': '<time-complexity><evolutionary-computing>', 'CreationDate': '2013-01-05T22:20:22.927', 'FavoriteCount': '2', 'Id': '7793'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '298', 'Title': 'Prove or refute: BPP(0.90,0.95) = BPP', 'LastEditDate': '2013-01-07T22:38:45.893', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '3', 'Body': '<p>I\'d really like your help with the proving or refuting the following claim: $BPP(0.90,0.95)=BPP$. In computational complexity theory, BPP, which stands for <a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29">bounded-error probabilistic polynomial time</a> is the class of decision problems solvable by a probabilistic Turing machine in polynomial time, with an error probability of at most $\\frac{1}{3}$ for all instances. $BPP=BPP(\\frac{1}{3},\\frac{2}{3})$.</p>\n\n<p>It is not immediate that any of the sets are subset of the other, since if the probability for an error is smaller than $0.9$ it doesn\'t have to be smaller than $\\frac{1}{3}$ and if it is bigger than $\\frac{2}{3}$ it doesn\'t have to be bigger than $0.905$. </p>\n\n<p>I\'m trying to use Chernoff\'s inequality for proving the claim, I\'m not sure exactly how.\nI\'d really like your help. Is there a general claim regarding these relations that I can use?</p>\n', 'Tags': '<complexity-theory><time-complexity><probabilistic-algorithms>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-08T19:20:38.810', 'CommentCount': '4', 'AcceptedAnswerId': '7829', 'CreationDate': '2013-01-07T21:40:43.267', 'Id': '7820'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm thinking about the optimal algorithm for the following problem:</p>\n\n<p>Input data:</p>\n\n<ul>\n<li>a <strong>text</strong>, say it's an article about 5-50 pages.</li>\n<li>a set of <strong>ngrams</strong> (ngram strings, n>2), of arbitrary length, could be more than 20k n-grams.</li>\n</ul>\n\n<p>The algorithm should output the following:</p>\n\n<ul>\n<li>a dictionary of all ngrams that were found in the text with the corresponding quantities, it should also take into account that ngrams could partially intersect or consist of each other (like <em>'probability density', 'probability density function', 'probability density distribution'</em>)</li>\n</ul>\n\n<p>So <strong>the question is</strong> what would be the most time-efficient algorithm to compute this?</p>\n\n<p>Both all words in a text and all words in ngrams are reduced to the canonical forms.</p>\n", 'ViewCount': '169', 'Title': 'Optimal algorithm for finding all ngrams from a pre-defined set in a text', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-27T01:11:03.780', 'LastEditDate': '2013-01-16T21:10:34.413', 'AnswerCount': '4', 'CommentCount': '0', 'AcceptedAnswerId': '8976', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2066', 'Tags': '<algorithms><time-complexity><strings><natural-lang-processing>', 'CreationDate': '2013-01-16T16:38:56.730', 'Id': '8972'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '150', 'Title': 'Where is the mistake in this apparently-O(n lg n) multiplication algorithm?', 'LastEditDate': '2013-01-18T23:31:19.787', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '535', 'FavoriteCount': '1', 'Body': u'<p>A recent <a href="http://pratikpoddarcse.blogspot.ca/2013/01/evenly-spaced-ones-in-binary-string.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3a%20pratikpoddarcse%20%28CSE%20Blog%29">puzzle blog post</a> about finding three evenly spaced ones lead me to a <a href="http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string">stackoverflow question</a> with a top answer that claims to do it in O(n lg n) time. The interesting part is that the solution involves squaring a polynomial, referencing a <a href="http://www.cs.iastate.edu/~cs577/handouts/polymultiply.pdf">paper that describes how to do it in O(n lg n) time</a>.</p>\n\n<p>Now, multiplying polynomials is practically the same as multiplying numbers. The only real difference is the lack of carries. But... the carries can also be done in O(n lg n) time. For example:</p>\n\n<pre><code>    var value = 100; // = 0b1100100\n\n    var inputBitCount = value.BitCount(); // 7 (because 2^7 &gt; 100 &gt;= 2^6)\n    var n = inputBitCount * 2; // 14\n    var lgn = n.BitCount(); // 4 (because 2^4 &gt; 14 =&gt; 2^3)\n    var c = lgn + 1; //5; enough space for 2n carries without overflowing\n\n    // do apparently O(n log n) polynomial multiplication\n    var p = ToPolynomialWhereBitsAreCoefficients(value); // x^6 + x^5 + x^2\n    var p2 = SquarePolynomialInNLogNUsingFFT(p); // x^12 + 2x^11 + 2x^10 + x^8 + 2x^7 + x^4\n    var s = CoefficientsOfPolynomial(p2); // [0,0,0,0,1,0,0,2,1,0,2,2,1]\n    // note: s takes O(n lg n) space to store (each value requires at most c-1 bits)\n\n    // propagate carries in O(n c) = O(n lg n) time\n    for (var i = 0; i &lt; n; i++)\n        for (var j = 1; j &lt; c; j++)\n            if (s[i].Bit(j))\n                s[i + j].IncrementInPlace();\n\n    // extract bits of result (in little endian order)\n    var r = new bool[n];\n    for (var i = 0; i &lt; n; i++)\n        r[i] = s[i].Bit(0);\n\n    // r encodes 0b10011100010000 = 10000\n</code></pre>\n\n<p>So my question is this: where\'s the mistake, here? Multiplying numbers in O(n lg n) is a gigantic open problem in computer science, and I really really doubt the answer would be this simple.</p>\n\n<ul>\n<li>Is the carrying wrong, or not O(n lg n)? I\'ve worked out that lg n + 1 bits per value is enough to track the carries, and the algorithm is so simple I\'d be surprised if it was wrong. Note that, although an individual increment can take O(lg n) time, the aggregate cost for x increments is O(x).</li>\n<li>Is the polynomial multiplication algorithm from the paper wrong, or have conditions that I\'m violating? The paper uses a fast fourier transform instead of a number theoretic transform, which could be an issue.</li>\n<li>Have a lot of smart people missed an obvious variant of the <a href="http://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm">Sch\xf6nhage\u2013Strassen algorithm</a> for 40 years? This seems by far the least likely.</li>\n</ul>\n\n<p>I\'ve actually written code to implement this, except for the efficient polynomial multiplication (I don\'t understand the number theoretic transform well enough yet). Random testing appears to confirm the algorithm being correct, so the issue is likely in the time complexity analysis.</p>\n', 'Tags': '<algorithms><time-complexity>', 'LastEditorUserId': '535', 'LastActivityDate': '2013-01-19T05:18:55.963', 'CommentCount': '2', 'AcceptedAnswerId': '9035', 'CreationDate': '2013-01-18T23:26:18.080', 'Id': '9034'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'d like your help with the following question:</p>\n\n<p>Assume we proved that $\\mbox{BPP}\\subseteq \\Pi_2$ -What conclusions can you make?</p>\n\n<p><a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29" rel="nofollow">BPP </a> is  the class of decision problems solvable by a probabilistic Turing machine in polynomial time, with an error probability of at most 1/3 for all instances, </p>\n\n<p>$\\Pi_2$ is the class of all languages $L$ such that there\'s a polynomial algorithm $M$ and a polynom $p$ so that $\\forall x.x\\in L\\Leftrightarrow \\forall u\\in \\{ 0,1 \\}^*.\\exists v \\in \\{ 0,1 \\}^*.M(x,u,v)=1$.</p>\n\n<p>We already know that   $\\mbox{BPP}\\subseteq \\Sigma_2$, so $\\mbox{BPP}\\subseteq \\Pi_2\\cap \\Sigma_2$.</p>\n', 'ViewCount': '78', 'Title': 'Assuming $\\mbox{BPP}\\subseteq \\Pi_2$ -What conclusions can we make?', 'LastActivityDate': '2013-01-20T17:11:01.780', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2329', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-01-19T19:40:47.960', 'Id': '9045'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that $\\sf BPP[2/3,1/3]= BPP[\\alpha,\\beta]$ when $\\alpha\\lt\\beta$, but I read something on Wikipedia which got me confused:</p>\n\n<blockquote>\n  <p>In practice, an error probability of $1/3$ might not be acceptable, however, the choice of $1/3$ in the definition is arbitrary. It can be any constant between $0$ and $1/2$ (exclusive) and the set $\\mathsf{BPP}$ will be unchanged.</p>\n</blockquote>\n\n<p>The reason for my question is the this question that I'm trying to answer:</p>\n\n<p>We define the class $PP_{\\frac{7}{8}}$: $L \\in PP_{\\frac{7}{8}}$. There's a probabilistic Turing machine that  for $x \\in L$ accepts $x$ with probability $&gt;$ than $\\frac{7}{8}$ and for $x \\notin L$ it accepts $x$ with probabilty $\\leq \\frac{7}{8}$.</p>\n\n<p>So by the $\\alpha, \\beta$ first definition I can conclude  that $PP_{\\frac{7}{8}}$ which  equals to $\\sf BPP[7/8,7/8+\\epsilon]$ also equals to $\\sf BPP[2/3,1/3]$ but the I am asked to prove that $\\sf NP \\subseteq  BPP$ which we don't know yet.</p>\n", 'ViewCount': '52', 'Title': 'BPP clarification', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-21T21:48:07.393', 'LastEditDate': '2013-01-21T21:48:07.393', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<complexity-theory><time-complexity><probabilistic-algorithms>', 'CreationDate': '2013-01-21T21:06:27.130', 'Id': '9077'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here's a simple question which made me confused.</p>\n\n<p>Please tell me if my analysis of the problem and my conclusions are correct.</p>\n\n<p>For an optimization problem $\\sf Gap-A[a,b] \\in L$ for $b&gt;a$. I want to discuss what I can know about $\\sf Gap-A[a/2,b/2]$. </p>\n\n<p>$\\sf(i)$ Is it correct to say that $\\sf Gap-A[a/2,b/2]$ is not in $\\sf NP-Hard$ and it is easy only because the realtion $\\frac{a/2}{b/2}=\\frac{a}{b}$ and we already know that it is easy to approximate $A$ with  factor of $a/b$? </p>\n\n<p>$\\sf (ii)$ Is it true that I can't say though that $\\sf Gap-A[a/2,b/2]$ is in $\\sf L$ since there is no intersection between the gaps? In order that gap $B$ would be in the exact class that gap $A$ is in, we have to have $A \\subseteq B$?</p>\n", 'ViewCount': '69', 'Title': 'Gap problems clarifation', 'LastEditorUserId': '1183', 'LastActivityDate': '2013-01-23T06:08:01.450', 'LastEditDate': '2013-01-22T22:25:03.647', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-01-22T11:53:58.257', 'Id': '9090'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>obviously the "Finding an $n/2$ size clique in a directed/ non directed graph" is not $\\sf NL$ but I\'d really like to understand where does it fall in terms of the definition of the $\\sf NL$ definition.</p>\n\n<p>I have to have an input and witness tapes, both of them can be polynomial and a logarithmic size of work stripe. So why can\'t I just guess an $n/2$ clique, each time put 2 vertices on the work tape, check that they share an edge, and count them. The witness tape provides me each time two vertices from left to write, in total $O({\\frac{n}{2}}^2)$. In order to count that I need $O(\\log ({\\frac{n}{2}}^2))$ which is logarithmic. so what\'s lacking or wrong with this attempt?</p>\n', 'ViewCount': '90', 'Title': 'How come finding an $n/2$ size clique in a directed/ non directed graph is not in $\\sf NL$?', 'LastActivityDate': '2013-01-30T03:19:57.180', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2329', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-01-27T07:19:51.583', 'Id': '9198'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some recurrence relationships I use to compute some coefficients of a series. I want to know what the time complexity of computing these is. Suppose I know the coefficients $a_0,...,a_n$, and I want to compute $c_0,...,c_n$ as follows</p>\n\n<p>$ c_{n}=\\begin{cases}\ne^{b_{0}}, &amp; n=0\\\\\n\\frac{1}{n}\\sum_{k=1}^{n}kb_{k}c_{n-k}, &amp; n\\geq1\n\\end{cases}$</p>\n\n<p>where </p>\n\n<p>$b_{n}=\\begin{cases}\n\\frac{1}{a_{0}}, &amp; n=0\\\\\n-\\frac{1}{a_{0}}\\sum_{i=1}^{n}a_{i}b_{n-i}, &amp; n\\geq1\n\\end{cases}$</p>\n\n<p>So far this is what I have. If we know $b_0,...,b_{k-1}$ then $b_k$ can be computed in $O(k)$ time, so to compute $b_0,...,b_{n}$ takes $\\sum_{k=0}^{n}O(k)=O(n^2)$ time. </p>\n\n<p>Now suppose that we know $b_0,...,b_{k}$ and $c_0,...,c_{k-1}$ then it takes  $O(k)$ time to compute $c_k$. Thus to compute $c_0,...,c_{n}$ it takes $\\sum_{k=0}^{n}O(k)=O(n^2)$. Ofcourse we have to take into account the time it takes to compute  $b_0,...,b_{n}$ which is $O(n^2)$.</p>\n\n<p>So can I conclude that the time it takes to compute $c_0,...,c_{n}$ is </p>\n\n<p>$O(n^2)+O(n^2)=O(n^2)$??</p>\n', 'ViewCount': '37', 'Title': 'Computational complexity of coefficients', 'LastEditorUserId': '6580', 'LastActivityDate': '2013-01-27T19:24:29.233', 'LastEditDate': '2013-01-27T16:42:50.047', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9218', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6580', 'Tags': '<time-complexity><recurrence-relation>', 'CreationDate': '2013-01-27T16:30:50.947', 'Id': '9206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We say that the language $J \\subseteq \\Sigma^{*}$ is <em>dense</em> if there exists a polynomial $p$ such that $$ |J^c \\cap \\Sigma^n| \\leq p(n)$$ for all $n \\in \\mathbb{N}.$ In other words, for any given lenght $n$ there exist only polynomially many words of length $n$ that are not in $J.$</p>\n\n<p>The problem I am currently studying asks to show the following</p>\n\n<blockquote>\n  <p>If there exist a dense $NP$-complete language then $P = NP$</p>\n</blockquote>\n\n<p>What the text suggest is to consider the polynomial reduction to $3$-$SAT$ and then construct an algorithm that tries to satisfy the given $CNF$ formula while also generating elements in $J^c.$</p>\n\n<p>What I am wondering is</p>\n\n<blockquote>\n  <p>Is there a more direct proof? Is this notion known in a more general setting?</p>\n</blockquote>\n', 'ViewCount': '262', 'Title': 'A dense NP complete language implies P=NP', 'LastActivityDate': '2013-02-05T23:51:20.210', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '9529', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '3092', 'Tags': '<complexity-theory><time-complexity><np-complete><satisfiability>', 'CreationDate': '2013-01-30T20:07:13.607', 'FavoriteCount': '1', 'Id': '9327'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that I have a turing machine that receives as input the string $1^{n\\times n}$ (unary input), what is the time complexity of writing $p_1,...,p_n$ on the output tape, where $p_i$ is the i-th prime number (written in binary)? And writing the first $n$ primes  followed by their product (all written in binary)?</p>\n', 'ViewCount': '130', 'Title': 'Time complexity of generating the first n primes and their product', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-19T05:24:50.567', 'LastEditDate': '2013-01-30T20:59:19.290', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '9922', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6638', 'Tags': '<time-complexity><turing-machines>', 'CreationDate': '2013-01-30T20:31:30.967', 'Id': '9330'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am interested in precise time complexity of distributed algorithm for finding MIS (Maximum Independent Set) of a given graph $G$.</p>\n\n<p>I investigate the Slow MIS distributed algorithm (from <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">these lecture notes</a>, page 2).</p>\n\n<p>Following is the more detailed version than in lecture notes.</p>\n\n<ul>\n<li>Every node sends its UID to it\'s neighbors.</li>\n<li>Run procedure join</li>\n</ul>\n\n<p><strong>on event</strong>: getting a message <em>decide(1)</em>  from a neighbor $w$ do</p>\n\n<ul>\n<li><p>Set $b = 0$ - flag that node terminates and not participating in further phases.</p></li>\n<li><p>Send <em>decide(0)</em> to all neighbors</p></li>\n</ul>\n\n<p><strong>on event</strong>: getting a message <em>decide(0)</em> from a neighbor $w$ do:</p>\n\n<ul>\n<li>Invoke procedure <strong>join</strong>.</li>\n</ul>\n\n<p>Procedure <strong>Join</strong></p>\n\n<p>if every neighbor $w$ of $v$ with a larger identifier has decided $b(w) = 0$, then do</p>\n\n<ul>\n<li>Set $b = 1$.</li>\n<li>Send <em>decided(1)</em> to all neighbors.</li>\n</ul>\n\n<p>The question is what\'s time complexity of the algorithm $\\Theta(n)$ or $\\Theta(D)$, where $D$ is a diameter of $G$.</p>\n\n<p>In the lecture notes linked above, they say that time complexity is $O(n)$. I  think that in our case it can be expressed as $\\Theta(D)$ (simultaneously $O(D)$ and $\\Omega(D)$) for special cases.</p>\n\n<p>The problem is how to prove that that time complexity in general is $O(D)$ and there are special cases when time complexity is $\\Omega(D)$ if it\'s right at all.</p>\n\n<p>Let\'s take a look at the example I have in mind.</p>\n\n<p><img src="http://i.stack.imgur.com/Q3dGs.png" alt="enter image description here"></p>\n\n<p>$D=1$ and $n=4$, and as I understood the algorithm every vertex will decide to join MIS on the first round.</p>\n\n<p>If you have an idea how to show that, please, share it with us. </p>\n', 'ViewCount': '131', 'Title': 'Slow MIS Distributed Algorithm', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-02-03T17:10:12.623', 'LastEditDate': '2013-02-02T19:53:16.413', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><distributed-systems>', 'CreationDate': '2013-02-01T06:56:49.720', 'FavoriteCount': '1', 'Id': '9379'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1342', 'Title': 'Why is push_back in C++ vectors constant amortized?', 'LastEditDate': '2013-02-01T22:45:15.410', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '2', 'Body': '<p>I am learning C++ and noticed that the running time for the push_back function for vectors is constant "amortized." The documentation further notes that "If a reallocation happens, the reallocation is itself up to linear in the entire size."</p>\n\n<p>Shouldn\'t this mean the push_back function is $O(n)$, where $n$ is the length of the vector? After all, we are interested in worst case analysis, right?</p>\n\n<p>I guess, crucially, I don\'t understand how the adjective "amortized" changes the running time.</p>\n', 'Tags': '<algorithms><time-complexity><amortized-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-02-01T22:45:15.410', 'CommentCount': '1', 'AcceptedAnswerId': '9382', 'CreationDate': '2013-02-01T07:17:05.997', 'Id': '9380'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some difficulties in understanding distributed algorithm for tree 6 - coloring in $O(\\log^*n)$ time.</p>\n\n<p>The full description can be found in following paper: <a href="http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1613&amp;context=cstech" rel="nofollow">Parallel Symmetry-Breaking in Sparse Graphs. Goldberg, Plotkin, Shannon</a>.</p>\n\n<p>In short, the idea is ...</p>\n\n<p>Starting from the valid coloring given by the processor ID\'s, the procedure iteratively reduces the number of bits in the color descriptions by recoloring each nonroot node $v$ with the color obtained by concatenating the index of a bit in which $C_v$ differs from $C_{parent}(v)$ and the value of this bit. The root $r$  concatenates $0$ and $C_r[0]$ to form its new color.</p>\n\n<p><strong>The algorithm terminates after $O(\\log^*n)$ iterations.</strong></p>\n\n<p>I don\' have the intuitive understanding why it\'s actually terminates in $O(\\log^*n)$ iterations. As it\'s mentioned in the paper on the final iteration there is the smallest index where two bit string differs is at most 3. So 0th bit and 1th bit could be the same and $2^2=4$, so this two bit will give us 4 colors + another 2 colors for different 3th bit, and in total 8 colors and not 6 like in the paper, and why we cannot proceed further with 2 bits, it\'s still possible to find different bits and separate them.</p>\n\n<p>I would appreciate a little bit deeper analysis of the algorithm than in the paper.</p>\n', 'ViewCount': '101', 'Title': '6-coloring of a tree in a distributed manner', 'LastEditorUserId': '31', 'LastActivityDate': '2014-05-03T20:56:08.843', 'LastEditDate': '2013-02-06T13:18:13.150', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><colorings>', 'CreationDate': '2013-02-06T10:58:35.073', 'Id': '9539'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question if I have my $k=300$\nand my loop is like this :</p>\n\n<pre><code>for( int x = 0 ; x&lt;n ; x--){\n    for(int y=0 ; y&lt;k; y++){\n        ...\n    }\n}\n</code></pre>\n\n<p>Is this still $O(n^2)$? If no, why?\nThank you :)</p>\n', 'ViewCount': '104', 'Title': 'Big O time complexity', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-02-09T09:43:54.210', 'LastEditDate': '2013-02-09T09:43:54.210', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '9614', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6786', 'Tags': '<time-complexity><landau-notation>', 'CreationDate': '2013-02-09T03:53:59.947', 'Id': '9613'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So I have the following pseudo-code:</p>\n\n<pre><code>Function(n):\nfor (i = 4 to n^2):\n    for (j = 5 to floor(3ilog(i))):\n        // Some math that executes in constant time\n</code></pre>\n\n<p>So far, I know that to find this i will be calculating</p>\n\n<p>$\\sum_{i=4}^{n^2}\\sum_{j=5}^{3i \\log_{2}i}C$ where $C$ is a constant, but I am completely lost as to how to proceed past the first summation which, if I'm not mistaken, will give me $3C \\cdot (\\sum_{i=4}^{n^2}(i \\log_{2}i) - 5(n^2 - 4))$, but from here I'm lost.  I don't need exact running time, but the asymptotic complexity.</p>\n\n<p>All help is greatly appreciated!  I realize that this might be a duplicate, but I haven't been able to find a nested for loop problem of this nature anywhere...</p>\n", 'ViewCount': '468', 'Title': 'Running time of a nested loop with $\\sum i \\log i$ term', 'LastEditorUserId': '4751', 'LastActivityDate': '2013-02-12T22:39:49.373', 'LastEditDate': '2013-02-12T22:39:49.373', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '9716', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6847', 'Tags': '<time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-02-12T19:55:39.130', 'Id': '9714'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>That is, can the running time of every algorithm $A$ be written as $O(f_A(n))$ and $\\Omega(f_A(n))$, for the same function $f_A$?</p>\n', 'ViewCount': '484', 'Title': "Can every algorithm's running time be expressed as $\\Theta(f(n))$?", 'LastActivityDate': '2013-02-14T02:39:23.590', 'AnswerCount': '6', 'CommentCount': '3', 'Score': '4', 'OwnerDisplayName': 'user13731', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-02-13T01:21:43.743', 'FavoriteCount': '0', 'Id': '9728'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I read about the time complexity for modular arithmetic in many books. There is one thing that I don't understand. I read in some books the following:</p>\n\n<p>For any $a \\mod N$, $a$ has a multiplicative inverse modulo $N$ if and only if it is relatively prime to $N$. When this inverse exists, it can be found in time $O(n^3)$ (where  $n$ denotes the number of bits in the binary representation of $N$) by running the extended Euclid algorithm. My question revolves around extended Euclid algorithm having $O(n^3)$ complexity.</p>\n\n<p>When I write in Java or C#, a line like this:</p>\n\n<pre><code>A = B.modInverse(N) // Java syntax\n</code></pre>\n\n<p>Can I usually say that this line has time complexity $O(n^3)$?\nOr is it necessary to write the code for the extended Euclid algorithm?</p>\n\n<p>Secondly, does extended Euclid algorithm depend on the compiler or the computer architecture?</p>\n", 'ViewCount': '43', 'Title': 'Time complexity for modular arithmatic', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-02-16T13:26:28.003', 'LastEditDate': '2013-02-16T13:26:28.003', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6898', 'Tags': '<time-complexity><compilers>', 'CreationDate': '2013-02-15T23:57:35.800', 'Id': '9821'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have an algorithm where the number of items in my set decrease by $\\sigma/(1+\\sigma)$ on each iteration until all items are exhausted.</p>\n\n<p>$$\n\\begin{align*}\nS_0 &amp;= S \\\\\nS_{k+1} &amp;= S_k - S_k \\frac{\\sigma}{1+\\sigma}\n\\end{align*}\n$$</p>\n\n<p>Here $\\sigma$ is a small value.</p>\n\n<p>How can I find number of iterations? I know it is a geometric series but can't seem to simplify for number of iterations.</p>\n", 'ViewCount': '140', 'Title': 'Finding the number of iterations to a recurrence', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-18T21:56:51.273', 'LastEditDate': '2013-02-18T21:56:51.273', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '9886', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5399', 'Tags': '<time-complexity><recurrence-relation><discrete-mathematics>', 'CreationDate': '2013-02-18T05:29:52.657', 'Id': '9885'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The sum divider game for $n$ starts with the set $M_0 = \\{1,\\dots,n\\}$. Player A chooses a number $m_1$ from $M_0 \\setminus \\{1\\}$ and B has to choose a divider $m_2$ of $m_1$ from $M_1 = M_0 \\setminus \\{m_1\\}$. The players continue to choose a number $m_i$ from $M_{i-1} = M_{i-2} \\setminus \\{m_{i-1}\\}$ alternatingly, where every $m_i$ has to divide $\\sum_{k=1}^{i-1} m_k$. A player wins, if the other player is unable to do so and $M_{i-1} \\neq \\emptyset$, $M_{i-1} = \\emptyset$ is considered a tie.</p>\n\n<p>My questions:</p>\n\n<ul>\n<li>Is there an $n &gt; 2$, for which A has no winning strategy?</li>\n<li>Given some $n$ (in <strike>binary</strike> unary representation), how hard is it to decide whether there is a winning strategy for A\n<ul>\n<li>where A wins in at most $k$ steps ?</li>\n<li>where A chooses no prime numbers ?</li>\n</ul></li>\n</ul>\n', 'ViewCount': '230', 'Title': 'Complexity of deciding whether there is a winning strategy in the following game', 'LastEditorUserId': '41', 'LastActivityDate': '2013-07-20T09:17:09.133', 'LastEditDate': '2013-07-20T09:17:09.133', 'AnswerCount': '0', 'CommentCount': '18', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '6716', 'Tags': '<complexity-theory><time-complexity><game-theory><number-theory>', 'CreationDate': '2013-02-21T14:10:55.227', 'FavoriteCount': '2', 'Id': '10011'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need a good book which starts from quite beginner to learn about calculating the complexity of Algorithm??</p>\n', 'ViewCount': '480', 'Title': 'Book to learn Algorithm Complexity', 'LastActivityDate': '2013-02-24T10:59:41.620', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '6669', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2013-02-24T06:37:31.737', 'Id': '10042'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '852', 'Title': 'What is the complexity of this matrix transposition?', 'LastEditDate': '2013-02-26T02:44:32.997', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5391', 'FavoriteCount': '1', 'Body': "<p>I'm working on some exercises regarding graph theory and complexity. \nNow I'm asked to give an algorithm that computes a transposed graph of $G$, $G^T$ given the adjacency matrix of $G$. So basically I just have to give an algorithm to transpose an $N \\times N$ matrix.</p>\n\n<p>My first thought was to loop through all rows and columns and simply swapping values in each of the $M[i,j]$ place. Giving a complexity of $O(n^2)$ But I immediately realized there's no need to swap more than once, so I can skip a column every time e.g. when I've iterated over row i, there's no need to start iteration of the next row at column i, but rather at column i + 1.</p>\n\n<p>This is all well and good, but how do I determine the complexity of this. When I think about a concrete example, for instance a 6x6 matrix this leads to 6 + 5 + 4 + 3 + 2 + 1 swaps (disregarding the fact that position [i,i] is always in the right position if you want to transpose a $N \\times N$ matrix, so we could skip that as well).\nThis looks alot like the well-known arithmetic series which simplifies to $n^2$, which leads me to think this is also $O(n^2)$. There are actually $n^2/2$ swaps needed, but by convention the leading constants may be ignored, so this still leads to $O(n^2)$. Skipping the i,i swaps leads to $n^2/2 - n$ swaps, which still is $O(n^2)$, but with less work still..</p>\n\n<p>Some clarification would be awesome :)</p>\n", 'Tags': '<graph-theory><time-complexity><algorithm-analysis><linear-algebra><adjacency-matrix>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-25T22:31:32.400', 'CommentCount': '3', 'AcceptedAnswerId': '10082', 'CreationDate': '2013-02-25T13:54:53.913', 'Id': '10081'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2134', 'Title': 'What are the characteristics of an $O(n \\log n)$ time complexity algorithm?', 'LastEditDate': '2013-02-26T07:32:50.337', 'AnswerCount': '6', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '7007', 'FavoriteCount': '3', 'Body': '<p>Sometimes it\'s easy to identify the time complexity of an algorithm my examining it carefully. Algorithms with two nested loops of $N$ are obviously $N^2$. Algorithms that explore all the possible combinations of $N$ groups of two values are obviously $2^N$.</p>\n\n<p>However I don\'t know how to "spot" an algorithm with $O(N \\log N)$ complexity. A recursive mergesort implementation, for example, is one. What are the common characteristics of mergesort or other $O(N \\log N)$ algorithms that would give me a clue if I was analyzing one?</p>\n\n<p>I\'m sure there is more than one way an algorithm can be of $O(N \\log N)$ complexity, so any and all answers are appreciated. BTW I\'m seeking general characteristics and tips, not rigorous proofs.</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><intuition>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-26T19:08:10.450', 'CommentCount': '5', 'AcceptedAnswerId': '10102', 'CreationDate': '2013-02-25T21:02:00.667', 'Id': '10091'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '141', 'Title': 'How to show that the complement of a language in $\\mathsf P$ is also in $\\mathsf P$?', 'LastEditDate': '2013-03-04T13:30:31.070', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7140', 'Body': u'<p>If $L$ is a binary language (that is, $L \\subseteq \\Sigma = \\{0,1\\}^\u2217$) and $\\overline{L}$ is the complement of $L$:</p>\n\n<p>How can I show that if $L \\in \\mathsf P$, then $\\overline{L} \\in \\mathsf P$ as well?</p>\n', 'ClosedDate': '2013-03-04T19:39:55.200', 'Tags': '<complexity-theory><formal-languages><time-complexity><complexity-classes>', 'LastEditorUserId': '2152', 'LastActivityDate': '2013-03-04T17:31:28.363', 'CommentCount': '2', 'AcceptedAnswerId': '10270', 'CreationDate': '2013-03-04T12:32:28.090', 'Id': '10257'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I asked a question on Rabin-Karp Searching algorithm <a href="http://cs.stackexchange.com/questions/10173/rabin-karp-searching-algorithm">here</a>, which I am reading from the book "<a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">Introduction to Algorithms" 3rd edition Cormen et al.</a>. </p>\n\n<p>After reading few para of the section on Rabin-Karp, I got some more confusions:</p>\n\n<p>In the third paragraph the authors say that the if we could find <strong>p</strong>  (decimal value of pattern P[1....m] )  in  time O(m) <em>and all the <strong>ts</strong> values</em> (i.e decimal value of length-m sub-string T[s+1....s+m], s=0,1,2,,,,n-m) in time O(n-m+1),  then we could determine all valid shifts s in time O(m) + O(n-m+1) by comparing <strong>p</strong> with each of the <strong>ts</strong> values. </p>\n\n<p>How is this possible? O(m) is for finding p, O(n-m+1) is for finding all ts, so total pre-processing time so far is O(m) + O(n-m+1). This is the total pre-processing time; the comparison has yet to start, I have to spend some extra $ for doing comparison of a decimal p with each of the (n-m+1)-ts values. </p>\n\n<p>1-Then why the authors say in the first para that the pre-processing time is O(m)? Why it is not O(m) + O(n-m+1) which include processing time of p and all ts values? </p>\n\n<p>2- Now if we talk about worst case matching time, what should be that? So in the worst my decimal number p (already calculated ) will be compared with <em>each</em> of the another (m-n+1) decimal numbers, which are the values of ts (already calculated, no extra cash needed for doing this job now ). The worst case is when I am most unlucky and I have to compare every value of ts with p. Right? </p>\n\n<p>Based on my understanding,(if I am right) the worst case matching time should be O(m-n+1) and not O((m-n+1)m) as claimed by the authors in the first para. For example let us say my Pattern is P[1...m]=226 and Text is T[1....n]=224225226. so  my p is decimal 226, and ts is decimal value of T[s+1, s+2, s+3], for s=0,1,2...6 as n=9, and m=3. The ts values will be as follows: </p>\n\n<blockquote>\n  <p>s=0 => T[224]=> ts=224     </p>\n  \n  <p>s=1 => T[242]=> ts=242 </p>\n  \n  <p>s=2 => T[422]=> ts=422 </p>\n  \n  <p>s=3 => T[225]=> ts=225 </p>\n  \n  <p>s=4 => T[252]=> ts=252 </p>\n  \n  <p>s=5 => T[522]=> ts=522 </p>\n  \n  <p>s=6 => T[226]=> ts=226</p>\n</blockquote>\n\n<p>Now you will be comparing p=226 with all these values. So are you not making n-m+1=7 comparisons to achieve search for 226 in T, and not (n-m+1)m =7 x3=21? So the worst case time should be O(n-m+1) and not O((n-m+1)m). </p>\n\n<p>In short I understand that:</p>\n\n<blockquote>\n  <p>Total pre-processing time = O(m) + O(n-m+1) (including for both p and\n  all the ts values)</p>\n  \n  <p>Total matching time in worst case = O(n-m+1)</p>\n</blockquote>\n\n<p>Where I am making mistake? </p>\n', 'ViewCount': '877', 'Title': 'Time Complexity of Rabin-Karp matching algorithm', 'LastEditorUserId': '6466', 'LastActivityDate': '2013-03-07T13:53:20.220', 'LastEditDate': '2013-03-04T12:43:50.803', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><complexity-theory><time-complexity><search-algorithms>', 'CreationDate': '2013-03-04T12:37:40.083', 'FavoriteCount': '0', 'Id': '10258'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For an open-addressing hash table, what is the average time complexity to find an item with a given key:</p>\n\n<ol>\n<li>if the hash table uses linear probing for collision resolution?</li>\n<li>if the hash table uses double probing for collision resolution?</li>\n</ol>\n\n<p>From my understanding my answer to the first question would be $O(n)$\nbut I'm not sure about the double probing question.</p>\n", 'ViewCount': '294', 'Title': 'Hash table collision probability', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-03-07T14:55:19.393', 'LastEditDate': '2013-03-07T14:55:19.393', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7149', 'Tags': '<time-complexity><hash-tables>', 'CreationDate': '2013-03-04T21:20:01.743', 'FavoriteCount': '1', 'Id': '10273'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I can think of functions such as $n^2 \\sin^2 n$ that don't have asymptotically tight bounds,  but are there actually common algorithms in computer science that don't have asymptotically tight bounds on their worst case running times?</p>\n", 'ViewCount': '205', 'Title': 'Common Algorithms without Asymptotically Tight Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T11:27:03.257', 'LastEditDate': '2013-03-07T11:27:03.257', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '10355', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-07T08:39:31.110', 'Id': '10354'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a recurrence relation and trying to use master theorem to solve it. The recurrence relation is:</p>\n\n<p>$T(n) = 3T(n/5) + n^{0.5}$</p>\n\n<p>Can I use the master theorem in that relation? If so, can I say that $T(n)$ is $\u0398(n^{0.5})$?</p>\n', 'ViewCount': '24', 'ClosedDate': '2013-03-07T11:23:42.837', 'Title': 'The use of master theorem appriopriately', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-09-28T12:35:50.493', 'LastEditDate': '2013-09-28T12:35:50.493', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7175', 'Tags': '<algorithms><time-complexity><recurrence-relation><recursion><master-theorem>', 'CreationDate': '2013-03-07T09:48:45.607', 'Id': '10356'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a recurrence relation, it is like the following:</p>\n\n<p>$T(e^n) = 2(T(e^{n-1})) + e^n$, where $e$ is the base of the natural logarithm.</p>\n\n<p>To solve this and find a $\\Theta$ bound, I tried the following: I put $k=e^n$, and the equation transforms into:</p>\n\n<p>$$T(k)=2T(k/e)+k$$</p>\n\n<p>Then, I try to use the Master Theorem. According to Master Theorem, $a=2$, $b=e\\gt 2$ and $f(k)=k$. So, we have the case where $f(k)=\\Omega(n^{\\log_b a+\u03b5})$ for some $\u03b5\\gt 0$, thus we have $T(k)=\\Theta(f(k))=\\Theta(k)$. Then put $k=n$, we have $T(n)=\\Theta(n)$. Does my solution have any mistakes?</p>\n', 'ViewCount': '81', 'Title': 'Not sure if my solution to following recurrence is correct', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-09-28T12:36:02.527', 'LastEditDate': '2013-09-28T12:36:02.527', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7175', 'Tags': '<algorithms><time-complexity><recurrence-relation><master-theorem><check-my-answer>', 'CreationDate': '2013-03-07T11:52:11.320', 'FavoriteCount': '1', 'Id': '10359'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1073', 'Title': 'Implement queue with a linked list; why would it be bad to insert at the head and remove at the tail?', 'LastEditDate': '2013-03-10T19:51:55.500', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4348', 'FavoriteCount': '1', 'Body': '<p>In my textbook, Data Structures and Algorithms in Java, the author says when implementing a queue using a linked list you choose the front of the queue to be at the head of the list, and the rear of the queue to be at the tail of the list. In this way, you remove from the head and insert at the tail.</p>\n\n<p>The author then asks cryptically, "Why would it be bad to insert at the head and remove at the tail?" without providing an answer.</p>\n\n<p>I can\'t see what the difference really is. In effect, "Head" and "Tail" are just arbitrary names we define. What would be so bad if to enqueue() we add a head and create a reference to the old head, and to dequeue() we take from the tail and move the tail over?</p>\n\n<p>What is the answer to the author\'s question?</p>\n', 'Tags': '<time-complexity><linked-lists><abstract-data-types>', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-03-10T19:51:55.500', 'CommentCount': '0', 'AcceptedAnswerId': '10435', 'CreationDate': '2013-03-10T19:10:25.620', 'Id': '10434'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I want to prove that $E \\subsetneq EXP$ and i would like to do so using the Time Hierarchy Theorem </p>\n\n<p>I need to choose $f(n)$, i think $2^{cn}$ is a good choice, so here is my Proof:</p>\n\n<ul>\n<li>$E\\subseteq TIME(2^{cn})$</li>\n<li>$TIME(2^{cn}) \\subsetneq TIME(n^2 \\cdot (2^{cn})^2)$ Time Hierarchy Theorem</li>\n<li>$E \\subsetneq EXP$</li>\n</ul>\n\n<p>Is this correct ?</p>\n\n<hr>\n\n<p>I have done something similar with $P\\subsetneq EXP$: <br> <strong>PROOF IDEA</strong>:</p>\n\n<ul>\n<li>$P\\subseteq TIME(2^n)$</li>\n<li>$TIME(2^n)\\subsetneq TIME(n^2\\cdot (2^n)^2) \\ \\text{Time Hierarchy Theorem}$</li>\n<li>$TIME(n^2\\cdot (2^n)^2)\\subseteq EXP$</li>\n<li>$P\\subsetneq EXP$</li>\n</ul>\n\n<hr>\n\n<blockquote>\n  <p><strong>Complexity class E:</strong> $E=\\bigcup_{c\\ge 0}TIME(2^{cn})$ <br>\n  <strong>Complexity Class EXPTIME:</strong> $EXP=\\bigcup_{c\\ge 0}TIME(2^{n^c})$ <br>\n  <strong>Time Hierarchy Theorem:</strong> $TIME(f(n)) \\subsetneq TIME(n\xb2\\cdot (fn)\xb2)$</p>\n</blockquote>\n\n<p>The <strong><em>Time Hierarchy Theorem</em></strong> shows that allowing Turing Machines more computation time strictly increases the class of languages that they can decide. Recall that a function $f : N \u2192 N$ is a time-constructible function if there is a Turing machine that, given the input $1^n$ , writes down $1^{f(n)}$ on its\ntape in $O(f (n))$ time. </p>\n', 'ViewCount': '115', 'Title': 'How to Prove E $\\subsetneq$ EXP?', 'LastEditorUserId': '6672', 'LastActivityDate': '2013-03-17T13:20:00.520', 'LastEditDate': '2013-03-16T16:16:54.643', 'AnswerCount': '2', 'CommentCount': '10', 'AcceptedAnswerId': '10560', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6672', 'Tags': '<complexity-theory><time-complexity><complexity-classes>', 'CreationDate': '2013-03-16T14:04:12.593', 'Id': '10551'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Does the difficulty of a strongly NP-hard or NP-complete problem (as e.g. defined <a href="http://en.wikipedia.org/wiki/Strongly_NP-complete" rel="nofollow">here</a>) change when its input is unary instead of binary encoded?</p>\n\n<p>What difference does it make if the input of a strongly NP-hard problem is unary encoded? I mean, if I take for instance the weakly NP-complete Knapsack problem, it is NP-complete when binary encoded but can be solved in polynomial time by dynamic programming when unary encoded. Maybe it has some implications for hardness of higher levels of the polynomial time heirarchy?</p>\n\n<p>Does the notion of strongly ...-hard also hold for other complexity classes, e.g. higher classes of the polynomial time hierarchy?</p>\n\n<p>I previously asked this <a href="http://stackoverflow.com/q/15454532/1708806">question at stackoverflow.com</a> but it was pointed out that it is more appropriate here. </p>\n', 'ViewCount': '233', 'Title': 'Does the complexity of strongly NP-hard or -complete problems change when their input is unary encoded?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-17T18:18:32.653', 'LastEditDate': '2013-03-17T18:12:21.403', 'AnswerCount': '4', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '7309', 'Tags': '<complexity-theory><time-complexity><np-complete>', 'CreationDate': '2013-03-16T22:59:58.783', 'Id': '10563'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When dealing with the analysis of time and space complexity of algorithms, is it safe to assume that any function which has tight bounds ( i.e. $f(n)=\\Theta(g(n))$ is asymptotically positive and asymptotically monotonically increasing.  I mean that for all $n$ greater than or equal to some $n_0$ both those properties hold?  </p>\n', 'ViewCount': '313', 'Title': 'Asymptotic Properties of Functions in Complexity Analysis', 'LastActivityDate': '2013-03-20T21:07:13.017', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10666', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><asymptotics><landau-notation>', 'CreationDate': '2013-03-20T20:07:26.793', 'Id': '10664'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The worst case running time of insertion sort is $\\Theta(n^2)$, we don\u2019t write it as $O(n^2)$.</p>\n\n<p>$O$-notation is used to give upper bound on function. If we use it to bound a worst case running time of insertion sort, it implies that $O(n^2)$ is upper bound of algorithm no matter what type of input is, means it doesn\u2019t matter whether input is sorted, unsorted, reverse sorted, have same values, etc the upper bound will be same $O(n^2)$. But this is not the case of insertion sort. Insertion sort running time depends on type of input used. So when the input is already sorted, it runs in linear time and doesn\u2019t take more that $O(n)$ time.</p>\n\n<p>Therefore to write insertion sort running time as $O(n^2)$ is technically not good.</p>\n\n<p><strong>We use $\\Theta$-notation to write worst case running time of insertion sort. But I\u2019m not able to relate properties of $\\Theta$-notation with insertion sort, why $\\Theta$-notation is suitable to insertion sort.If $f(n)$ belong to $\\Theta(g(n))$ we write it as $f(n)= \\Theta(g(n))$, then $f(n)$ must satisfies the properties. And properties state that there exits constants $c_1$, $c_2$ and $n_0$ such that $0$$\\leq$$c_1\\cdot g(n)$$\\leq$$f(n)$$\\leq$$c_2\\cdot g(n)$ For all $n&gt;n_0$. How does the insertion sort function lies between the $c_1\\cdot n^2$ and $c_2\\cdot n^2$ for all $n&gt;n_0$.</strong></p>\n\n<p>Running time of insertion sort as $\\Theta(n^2)$ implies that it has upper bound $O(n^2)$ and lower bound $\\Omega(n^2)$. I\u2019m confused as to whether the lower bound on insertion-sort is $\\Omega(n^2)$ or $\\Omega(n)$.</p>\n', 'ViewCount': '477', 'Title': 'Why is $\\Theta$ notation suitable to insertion sort to describe its worst case running time?', 'LastEditorUserId': '7384', 'LastActivityDate': '2013-04-29T21:19:54.897', 'LastEditDate': '2013-04-29T16:22:00.450', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'siddstuff', 'PostTypeId': '1', 'OwnerUserId': '7384', 'Tags': '<algorithms><time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-03-25T06:59:09.077', 'Id': '10763'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to determine the worst case runtime of this program:</p>\n\n<pre><code>while n &gt; 1\n  for i = 1,..,n\n    m = log(n)\n  n = n/2\n</code></pre>\n\n<p>Obviously the outer loop runs <code>log(n)</code> times, because n is halfed after each step.</p>\n\n<p>But the inner for loop is dependent on the decreasing n and I am not quite sure how to deal with that.</p>\n\n<p>A simple bound for it would be <code>n</code>, so the whole thing runs in at most <code>log(n)*n</code>, but I am sure the inner loop is faster than that.</p>\n\n<p>If I look at each step of the outer loop, I see that the inner loop runs <code>n,n/2,n/4,..,2</code> times. So I would say all together the inner loop runs at most <code>2n</code> times which is in <code>O(n)</code>.</p>\n\n<p>Can I just deduct from that fact that the overall runtime is <code>O(log(n)+n) = O(n)</code>?</p>\n\n<p>I am unsure because I am not multiplying anything, which I feel like I should when dealing with multiple loops.</p>\n', 'ViewCount': '161', 'ClosedDate': '2013-03-27T11:54:26.093', 'Title': 'Decreasing runs of inner loop in outer loop', 'LastActivityDate': '2013-03-26T19:40:41.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '594', 'Tags': '<time-complexity><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-26T19:04:39.817', 'Id': '10813'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Note: This is a part of a homework question</p>\n\n<p>Were asked to construct a multi-tape Turing Machine for language {$a^n b^n c^n \\mid n \\geq 0$}</p>\n\n<p>Then it says "Discuss how much time your machines saves over a one-tape DTM using the same algorithm"\nAny hint?</p>\n\n<p>Here\'s my algorithm:</p>\n\n<p>(1) Cut-and-paste c\'s to tape 3</p>\n\n<p>(2) Cut-and-paste b\'s to tape 2</p>\n\n<p>(3) Cross out each triplets, accept if last round cuts all three, reject if there\'s leftover</p>\n\n<p>Which seems like it has a time complexity of $O(n+n+3n)=O(5n)$\nThen how do we determine the time complexity for the one-tape version?</p>\n', 'ViewCount': '197', 'Title': 'How do we determine how much time a multi-tape DTM saves over a one-tape DTM?', 'LastEditorUserId': '6980', 'LastActivityDate': '2013-05-01T11:05:47.477', 'LastEditDate': '2013-05-01T11:05:47.477', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11671', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6980', 'Tags': '<time-complexity><turing-machines><efficiency>', 'CreationDate': '2013-04-05T11:39:26.017', 'Id': '11053'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '115', 'Title': 'Why does a polynomial-time language have a polynomial-sized circuit?', 'LastEditDate': '2013-04-08T14:41:49.757', 'AnswerCount': '2', 'Score': '2', 'OwnerDisplayName': 'John Smith', 'PostTypeId': '1', 'OwnerUserId': '4631', 'Body': '<p>I wish to understand why P is a subset of PSCPACE, that is why a polynomial-time langauge does have a polynomial-sized circuit. I read many proofs like <a href="http://www.stanford.edu/~rrwill/week3.pdf" rel="nofollow">this one here on page 2-3</a>, but all the proofs use the same technique used in the Cook-Levin theorem to convert the computation of M on an n-bit input x to a polynomial sized circuit. </p>\n\n<p>What I don\'t understand is that the resulting circuit is dependent on the input x, because what is being converted into a circuit is the computation of M on the specific input x. By definition of PSIZE, the same circuit must work for all the inputs in a fixed length, and thus is not dependent on one specific input. </p>\n\n<p>So how is the process of creating a poly-sized circuit family for a poly-time deterministic Turing machine works exactly?</p>\n', 'Tags': '<complexity-theory><time-complexity><space-complexity><complexity-classes><polynomial-time>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:41:49.757', 'CommentCount': '1', 'AcceptedAnswerId': '11121', 'CreationDate': '2013-04-07T21:22:54.587', 'Id': '11117'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In the paper <a href="http://link.springer.com/content/pdf/10.1007/BF01300131">Complexity of the Frobenius Problem</a> by Ram\xedrez-Alfons\xedn, a problem was proved to be NP-complete using Turing reductions.\nIs that possible? How exactly? I thought this was only possible by a polynomial time many one reduction. Are there any references about this?</p>\n\n<p>Are there two different notions of NP-hardness, even NP-completeness? But then I am confused, because from a practical viewpoint, if I want to show that my problem is NP-hard, which do I use?</p>\n\n<p>They started the description as follows:</p>\n\n<blockquote>\n  <p>A  polynomial  time  Turing  reduction from  a problem $P_1$  to  another problem $P_2$  is  an  algorithm  A  which  solves  $P_1$  by  using  a  hypothetical  subroutine A\'  for  solving  $P_2$  such  that,  if  A\' were  a  polynomial  time  algorithm  for  $P_2$  then  A would  be  a  polynomial  time  algorithm  for  $P_1$.  We  say  that  $P_1$  can  be  Turing  reduced to  $P_2$. </p>\n  \n  <p>A  problem  $P_1$  is  called  (Turing)  NP-hard  if  there  is  an  NP-complete  decision \n  problem $P_2$  such  that  $P_2$  can  be  Turing  reduced  to  $P_1$.</p>\n</blockquote>\n\n<p>And then they use such a Turing reduction from an NP-complete problem to show NP-completeness of some other problem.</p>\n', 'ViewCount': '355', 'Title': 'Can one show NP-hardness by Turing reductions?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:48:09.817', 'LastEditDate': '2013-04-08T14:48:09.817', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '7309', 'Tags': '<complexity-theory><time-complexity><np-complete><reductions>', 'CreationDate': '2013-04-08T00:05:59.537', 'FavoriteCount': '1', 'Id': '11120'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '135', 'Title': 'How is a witness found in a proof of $\\mathsf{NP} \\subseteq \\mathsf{P}/\\log \\implies \\mathsf{P} = \\mathsf{NP}$?', 'LastEditDate': '2013-04-11T20:23:08.153', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'Caleb', 'PostTypeId': '1', 'OwnerUserId': '1765', 'Body': '<p>I\'m having a hard time understanding the actual proof of this proposition:</p>\n\n<p>$\\qquad \\mathsf{NP} \\subseteq \\mathsf{P}/\\log \\implies \\mathsf{P} = \\mathsf{NP}$</p>\n\n<p>The sketch of the proof is on slides 6-8 of <a href="http://www.cs.uiuc.edu/class/sp08/cs579/slides/CC-S08-Lect10.pdf" rel="nofollow">this PDF</a>.</p>\n\n<p>So I let $L \\in \\mathsf{NP}$. That means that there\'s a deterministic polynomial-time TM $M^1_L$ s.t. $x \\in L \\iff \\exists w. M^1_L(x,w) = 1$. I now need to construct a deterministic poly-time TM $M^2_L$ s.t. $x \\in L \\iff M^2_L(x) = 1$.</p>\n\n<p>Looking at the proof in the slides above, I realize that given $x\\in\\{0,1\\}^n$, $M^2_L$ needs to somehow find a witness $w$ for $x$ and then simply return whatever $M^1_L(x,w)$ does. But how do I find that witness?</p>\n', 'Tags': '<complexity-classes><time-complexity>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-12T08:35:31.197', 'CommentCount': '5', 'AcceptedAnswerId': '11253', 'CreationDate': '2013-04-07T23:07:59.247', 'Id': '11223'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is the complexity class <a href="https://en.wikipedia.org/wiki/Elementary_recursive" rel="nofollow">ELEMENTARY</a> that captures all problems that can be solved by using elementary recursive function only. So if algorithms for solving problems in some complexity class (e.g. NP or P) are converted to elementary recursive function form, would they retain time complexity of the complexity class? </p>\n\n<p>For example, in complexity class P, we know that problems take deterministic polynomial time to solve. Would an elementary recursive form of a solving algorithm retain this complexity?</p>\n\n<p>By converting into elementary recursive form, I mean:</p>\n\n<p>Yes, it is true that NP is in elementary, that is there is an elementary recursive algorithm that can solve NP problems, but what I ask is "will such algorithm retain its time complexity?" For example, complexity P has problems that can be solved in polynomial time complexity; however, it is not clear whether it will retain polynomial time complexity if the algorithm has to be in elementary recursive form.</p>\n\n<p>By my understanding, elementary recursive algorithm would be the one that does not necessarily use "if and else".</p>\n\n<p>Modification to the question: Let us say that for all decision problems we consider, there exist function problems that have same time complexity as their decision problem counterparts. For example, for 3-SAT problem with some input $x$, one satisfying assignment to the variables is treated as output. The reason why some people think this question is not valuable may be because for all decision problems, output is always either zero or one. So let us consider the function version of decision problems (that keeps time complexity).</p>\n', 'ViewCount': '122', 'ClosedDate': '2013-04-20T20:27:22.187', 'Title': 'Does converting algorithms into elementary recursive form preserve runtime bounds?', 'LastEditorUserId': '7743', 'LastActivityDate': '2013-04-18T08:30:59.960', 'LastEditDate': '2013-04-16T15:12:54.647', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7720', 'Tags': '<complexity-theory><time-complexity><arithmetic>', 'CreationDate': '2013-04-14T06:57:39.190', 'Id': '11302'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '361', 'Title': 'FFT-less $O(n\\log n)$ algorithm for pairwise sums', 'LastEditDate': '2013-04-20T15:02:37.317', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '5', 'Body': '<p>Suppose we are given $n$ distinct integers $a_1, a_2, \\dots, a_n$, such that $0 \\le a_i \\le kn$ for some constant $k \\gt 0$, and for all $i$.</p>\n\n<p>We are interested in finding the counts of all the possible pairwise sums $S_{ij} = a_i + a_j$. ($i = j$ is allowed).</p>\n\n<p>One algorithm is to construct the polynomial $P(x) = \\sum_{j=1}^{n} x^{a_j}$ of degree $\\le kn$, and compute its square using the Fourier transform method and read off the powers with their coefficients in the resulting polynomial. This is an $O(n \\log n)$ time algorithm.</p>\n\n<p>I have two questions:</p>\n\n<ul>\n<li><p>Is there an $O(n \\log n)$ algorithm which does not use FFT? </p></li>\n<li><p>Are better algorithms known (i.e $o(n \\log n)$)? (FFT allowed).</p></li>\n</ul>\n', 'Tags': '<algorithms><time-complexity><fourier-transform>', 'LastEditorUserId': '139', 'LastActivityDate': '2013-11-04T01:13:21.093', 'CommentCount': '4', 'AcceptedAnswerId': '16667', 'CreationDate': '2013-04-20T00:51:54.663', 'Id': '11418'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '98', 'Title': 'Time Complexity of a selection problem', 'LastEditDate': '2013-04-21T14:42:44.753', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2449', 'FavoriteCount': '1', 'Body': "<p>I wonder what's the time complexity of the following selection problem I found while thinking of a string-matching problem.</p>\n\n<p>[Assuming operations on integers take $O(1)$ time]</p>\n\n<p>We are Given $m$ sets, with $n$ integer numbers each. We want to select exactly one integer from each set, to make a set S, such that $~ l = \\max(S) - \\min(S)~$ is minimized.</p>\n\n<p>For example, n = 4, m = 3:</p>\n\n<p>$S_1 = \\{1, 43, 71, 101\\}$</p>\n\n<p>$S_2 = \\{18, 53, 80, 107\\}$</p>\n\n<p>$S_3 = \\{3, 16, 51, 208\\}$</p>\n\n<p>Now</p>\n\n<p>$~S = \\{43, 53, 51\\}$</p>\n\n<p>has one number from each set and </p>\n\n<p>$~l = \\max(S) - \\min(S) = 53 - 43 = 10 ~$ </p>\n\n<p>wich is the minimum possible value of $l$ (I think).</p>\n\n<p>First thing I tried was a reduction to the set cover problem, but I wasn't able to find one.</p>\n", 'Tags': '<complexity-theory><time-complexity><set-cover>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T14:42:44.753', 'CommentCount': '0', 'AcceptedAnswerId': '11430', 'CreationDate': '2013-04-20T12:51:15.197', 'Id': '11424'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was looking over <a href="http://stackoverflow.com/q/352203/2327689">this question</a> requesting an algorithm to generate all permutations of a given string. A comment in the answer caught my eye:</p>\n\n<p><em>It might seem that it can take O(n) time per permutation, but if you think about it more carefully, you can prove that it takes only O(n log n) time for all permutations in total, so only O(1) -- constant time -- per permutation.</em></p>\n\n<p>This seemed strange to me because the best method I was aware of to generate all permutations of a string is in O(2^n) time. Looking through the other results, I came across a <a href="http://stackoverflow.com/a/7140205/2327689">response to a similar question</a> which states: <em>While it technically produces the desired output, you\'re solving something that could be O(n lg n) in O(n^n)</em></p>\n\n<p>I am aware of an algorithm to unrank permutations in O(n log n) time, but these responses seem to imply that all permutations in total can be generated in time O(n log n). Am I misunderstanding these responses?</p>\n', 'ViewCount': '1009', 'Title': 'Can all permutations of a set or string be generated in O(n log n) time?', 'LastActivityDate': '2013-04-28T05:47:47.137', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '11626', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7938', 'Tags': '<algorithms><complexity-theory><time-complexity>', 'CreationDate': '2013-04-27T20:47:12.143', 'Id': '11611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There is an infinite wall with a hole somewhere, you are placed on that wall at an unknown position. Let the distance between your initial position &amp; the hole be $x$. Find the average distance traveled in terms of $x$ until you find the hole. What's the complexity of this problem in terms of $x$ and how does an algorithm look like that solves it?</p>\n", 'ViewCount': '164', 'Title': 'Find a hole while travelling along an infinite wall', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-12-22T00:29:47.893', 'LastEditDate': '2013-06-03T22:05:07.963', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'OwnerDisplayName': 'bludger', 'PostTypeId': '1', 'OwnerUserId': '7946', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-04-26T15:39:13.763', 'Id': '11624'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that the CVAL problem is P-complete.\nIn the CVAL problem the input is a Boolean circuit together with an input to this circuit, and the answer is the evaluation of the given circuit on the given input.</p>\n\n<p>I wish to know if the problem of evaluating a Boolean formula on a given assignment is also P-complete. From one hand, it seems that a Boolean circuit and a Boolean formula are very similar objects. Also, the proof of that CVAL is P-complete results from the Cook-Levin problem, the same theorem that actually shows that SAT is NP-Complete, so I don't see a reason why this problem won't be P-Complete.\nFrom the other hand, it seems pretty easy to evaluate a Boolean formula in logarithmic space, so if this problem is indeed P-complete I think it would imply L = P which is unknown. </p>\n\n<p>So I think I'm missing something.. any ideas people? </p>\n", 'ViewCount': '89', 'Title': 'Is the problem of evaluating a boolean formula on a given assignment P-complete?', 'LastActivityDate': '2013-05-01T17:23:20.530', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11705', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4631', 'Tags': '<complexity-theory><time-complexity><complexity-classes>', 'CreationDate': '2013-05-01T15:23:31.873', 'Id': '11697'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would I solve the following.</p>\n\n<p>An algorithm that is $O(n^2)$ takes 10 seconds to execute on a particular computer when n=100, how long would you expect to take it when n=500?</p>\n\n<p>Can anyone help me answer dis. </p>\n', 'ViewCount': '140', 'Title': 'Algorithm analysis question in growth of functions', 'LastEditorUserId': '6980', 'LastActivityDate': '2013-05-05T03:09:56.113', 'LastEditDate': '2013-05-05T03:09:56.113', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11782', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithms><time-complexity><asymptotics>', 'CreationDate': '2013-05-04T21:35:39.893', 'Id': '11781'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am trying to understand an algorithm  presented in <a href="http://www.cc.gatech.edu/dimacs10/papers/%5B13%5D-dimacs_stable_community.pdf" rel="nofollow">Using Stable Communities for Maximizing Modularity</a> by S. Srinivasan and S. Bhowmick, <strong>along with its complexity results</strong>. (The complete algorithm is presented on page 7, in pseudocode.)</p>\n\n<p>As I understand it, the idea of the algorithm is to identify densely connected communities (basically, subgraphs) in a graph, such that any subset of vertices in the community has more connections (that is, edges pointing to another vertex) inside the community than to any other community in the graph.</p>\n\n<p>Below I\'ve cited the part of the algorithm I\'m having trouble with:</p>\n\n<pre><code>For all vertices v_i in network N that are not in any stable community\n    Create a subset S of v_i and its neighbors  \n        For each neighbor n_j\n            ...\n            Identify y_j, the subset of external neighbors \n            which are all within distance k of each other\n</code></pre>\n\n<p>I take "external neighbors" to mean the neighbours of $n_j$ that aren\'t in $S$, and where the distance between them is also never measured through $S$ (as otherwise, they would never be more than two hops apart\u2014through $n_j$).</p>\n\n<p>The authors state the following about the complexity of this part:</p>\n\n<blockquote>\n  <p>...the average degree of a vertex is $d$...</p>\n  \n  <p>...the complexity for computing $y_j$ for all possible values of $k$ is $O(d^3)$.</p>\n</blockquote>\n\n<p>On the next page, the authors make some more statements about the complexity of this part, including that the values of the shortest paths between external vertices <em>can be reused</em> for many neighbours and that <em>setting $k$ to small values (2\u20144) is sufficient in most cases</em>. I\'m not sure if these considerations are already factored in, however I\'m a bit stuck on "all possible values of $k$" in the above quotation.</p>\n\n<p>My question is: what assumptions are necessary for the complexity result $O(d^3)$ to hold, with $d$ being the average degree of a vertex? How is the subset found in this case?</p>\n', 'ViewCount': '72', 'Title': 'Complexity of finding a subset of vertices within distance k of each other, given a set of vertices', 'LastActivityDate': '2013-05-05T12:58:25.270', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8057', 'Tags': '<graph-theory><time-complexity>', 'CreationDate': '2013-05-05T12:58:25.270', 'Id': '11803'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Runtime for many programming languages is typically analyzed either assuming each operation takes a constant amount of time, or assuming each operation takes a logarithmic amount of time in the size the the data being manipulated. From a practical perspective, this is reasonable since modern computers have essentially constant time memory access (albeit for a fixed amount of memory).</p>\n\n<p>However, from a theoretical perspective, it seems to me that this is a bad assumption.</p>\n\n<p>Since bits can't be stored denser than the Planck scale (which we are fast approaching), the number of bits we can store in a certain volume of space can grow at most linearly as volume grows. Further, since the speed of memory retrieval and storage is bounded by the speed of light, it makes a lot of sense to say that as the size of the data we store grows, the cost for storing and accessing it should grow. For instance, one could assume that storing and accessing the $n$th bit of data requires $\\sqrt[3]{n}$ time. (Since space is 3-dimensional).</p>\n\n<p>Presumably this has been discussed in the literature somewhere. Could anyone point me to some references? Have algorithms been analyzed using this model?</p>\n", 'ViewCount': '93', 'Title': "Why don't we scale the cost of memory access when analyzing runtime of algorithms?", 'LastActivityDate': '2013-05-06T10:20:24.770', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8065', 'Tags': '<reference-request><time-complexity><asymptotics><runtime-analysis>', 'CreationDate': '2013-05-06T05:14:51.447', 'FavoriteCount': '1', 'Id': '11818'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For every integer $t$, is there a problem whose solutions can be verified in $O(n^{s})$ time but cannot be found in $O(n^{st})$ time?</p>\n\n<p>By verifying, I mean that given a candidate solution $y$, we can judge whether $y$ is correct or not in time $O(n^s)$.</p>\n', 'ViewCount': '401', 'Title': 'A Problem on Time Complexity of Algorithms', 'LastEditorUserId': '72', 'LastActivityDate': '2013-05-09T22:53:07.350', 'LastEditDate': '2013-05-09T14:35:06.697', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8074', 'Tags': '<complexity-theory><time-complexity><asymptotics><complexity-classes><lower-bounds>', 'CreationDate': '2013-05-07T01:48:11.327', 'Id': '11844'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Firstly, let's suppose there exists an algorithm where $i$ iterates from $1$ to $n$, spending $\\frac{n^2}{i}$ time in each iteration.</p>\n\n<p>Thanks to the well known $O(\\log n)$ upper bound for the Harmonic series, the big $O$ notation of this algorithm comes to $O(n^2 \\log n)$.</p>\n\n<p>Now the actual algorithm that I am working on iterates from $k$ to $n$, where $1 &lt; k &lt; n$, spending $\\frac{n^2}{k}$ in the first iteration, then $\\frac{n^2}{k+1}$, $\\frac{n^2}{k+2}$, $\\frac{n^2}{k+3}$, and so on.</p>\n\n<p>I've been trying to deduce the upper bound of this new algorithm, but I keep getting the same $O(n^2 \\log n)$ time bound, which to me, is counter intuitive and a bit paradoxical, especially considering that the first terms of the harmonic series are the bigger and more significant terms. The time bound that I was actually expecting to get was $O(\\frac{n^2}{k} \\cdot \\log n)$.</p>\n\n<p>Could anyone please be able to shed some light onto this?</p>\n\n<p>Thanks in advance for any help.</p>\n", 'ViewCount': '997', 'Title': 'Time complexity in Big O notation for Harmonic series with first k terms missing', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-09T02:03:16.153', 'LastEditDate': '2013-05-08T23:25:51.343', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'wookie919', 'PostTypeId': '1', 'OwnerUserId': '6850', 'Tags': '<time-complexity><lower-bounds>', 'CreationDate': '2013-05-08T01:11:39.233', 'Id': '11895'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have this confusion related to the time complexity of the algorithm solving the knapsack problem using dynamic programming</p>\n\n<p><img src="http://i.stack.imgur.com/CvvMV.png" alt="enter image description here"></p>\n\n<p>I didn\'t get how the time complexity of the algorithm came out to be $O(nV^*)$</p>\n', 'ViewCount': '914', 'Title': 'Confusion related to time complexity of dynamic programming algorithm for knapsack problem', 'LastActivityDate': '2013-05-13T02:10:27.163', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6999', 'Tags': '<computability><time-complexity><dynamic-programming><knapsack-problems>', 'CreationDate': '2013-05-12T20:43:16.567', 'Id': '11976'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '189', 'Title': 'Modeling the problem of finding all stable sets of an argumentation framework as SAT', 'LastEditDate': '2013-05-20T14:48:10.623', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7443', 'FavoriteCount': '2', 'Body': '<p>As a continuation of my previous <a href="http://cs.stackexchange.com/questions/12087/converting-math-problems-to-sat-instances?noredirect=1#comment25370_12087">question</a> i will try to explain my problem and how i am trying to convert my algorithm to a problem that can be expressed in a CNF form.</p>\n\n<p>Problem: Find all stable sets of an <a href="http://en.wikipedia.org/wiki/Argumentation_framework" rel="nofollow">argumentation framework</a> according to <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.4129" rel="nofollow">Dung\'s proposed framework</a>.</p>\n\n<p>Brief theory: Having an argumentation framework AF, with A the set of all arguments and R the set of the relations, a stable set is a set which attacks all arguments not in their set and there is no attack relation between arguments in the stable set.\nExample:</p>\n\n<p>Let\'s say we have an argumentation framework AF ,A={1,2,3,4}(arguments of AF) and attack relations R{1,3} and R{2,4}.\nIt\'s obvious that the set {1,2} is a stable extension of the framework because:</p>\n\n<p>a)it attacks all arguments not in their set (3 and 4)</p>\n\n<p>b)it\'s conflict free(no attacks between arguments in the set) because argument 1 does not attack argument 2 and the opposite </p>\n\n<p>My exhaustive abstract algorithm:</p>\n\n<pre><code>argnum=number of arguments;\n\nAi[argnum-1]=relation "attacks" ,where 1&lt;=i&lt;=argnum\n\nP[2^argnum-1]=all possible relations that can be generated from all the arguments\n\nS[2^argnum-1]=empty; where S are all the stable sets\n\nj=0; //counter for while\nk=1; //counter for counting stable sets\nwhile j&lt;2^argnum-1\n    if P[j] attacks all arguments not in P[j](check using Ai[])\n        if all arguments in P[j] are conlfict-free\n            S[k++]=P[j];\n        end if\n    end if \n    j++;\nend while\n</code></pre>\n\n<p>I want to solve the above problem either by transforming the above algorithm to CNF or by using a different algorithm and finally use a SAT Solver(or anything similar if exists) give CNF as input and get stable sets as output.</p>\n\n<p>I wonder if someone can give me any feedback of how i can transform any algorithm like the above to CNF in order to be used into a SAT Solver.</p>\n\n<p>I decided to use <a href="http://fmv.jku.at/precosat/" rel="nofollow">precosat</a>.</p>\n', 'Tags': '<algorithms><complexity-theory><time-complexity><np-complete><satisfiability>', 'LastEditorUserId': '7443', 'LastActivityDate': '2013-05-26T00:33:46.807', 'CommentCount': '7', 'AcceptedAnswerId': '12176', 'CreationDate': '2013-05-19T15:50:58.177', 'Id': '12135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm facing some problems that deal with finding common elements between unsorted arrays and I'd like to know whether there are well-known lower-bounds for the <em>worst-case</em> and, eventually, what are these lower-bounds.</p>\n\n<p>The problems are pretty simple:</p>\n\n<blockquote>\n  <p>Given two unsorted arrays of distinct integers, $A$ and $B$, of size\n  $m$ and $n$, determine <em>all</em> the common elements between the two\n  arrays(the output must be sorted)</p>\n</blockquote>\n\n<p>Which I believe has a lower-bound complexity of $\\Omega((m+n)\\log(\\min(m,n)))$\n(and $O(1)$ space complexity, if we exclude the input arrays),\neven though I cannot find any proof of this fact. [I found an algorithm with this complexity, so I(hopefully) am not underestimating the complexity.]</p>\n\n<p>The other problem adds some restrictions(and hence I'd expect to be able to lower the complexity):</p>\n\n<blockquote>\n  <p>Given two unsorted arrays of distinct integers, $A$ and $B$, of size\n  $m$ and $n$; knowing that common elements between the arrays have the\n  same relative order in both arrays and that for every couple of\n  consecutive common elements, their distance is at most $k$(constant),\n  determine all the common elements between the two arrays maintaining\n  their relative order and using at most $O(k)$ memory in addition to\n  the input arrays.</p>\n</blockquote>\n\n<p>I've tried to think about this second problem but I cannot see how the restrictions change the complexities. What puzzles me is that I believe that finding a single couple of elements between two unsorted arrays is $\\Theta((n+m)\\log(\\min(n,m)))$, and since this is a special instance of this second problem then the restrictions do not add anything to the problem itself.</p>\n\n<p>Are my guesses correct, and if so where can I find proofs for these lower-bounds? Do the restrictions change anything at all or the solution for the first problem is the best we can achieve in both cases?</p>\n\n<hr>\n\n<p>Probably my questions can be summarized by the following:</p>\n\n<blockquote>\n  <p>Given two arrays of distinct integers $A$ and $B$ of size $m$ and $n$,\n  what is the lower-bound complexity for finding <em>one</em> common element?</p>\n</blockquote>\n\n<p>Because, once a common element is found, the second problem can be solved in linear time.</p>\n\n<hr>\n\n<p><strong>Edit</strong></p>\n\n<p>The algorithm(s) that I thought are pretty simple:</p>\n\n<p>For problem 1: Build two heaps for $A$ and $B$(which takes linear time and can be done in-place), then compare the minimum of the heaps, if they match print it and remove both, otherwise remove the smallest one and continue until the heaps are empty(this clearly takes $O((m\\log(m) + n\\log(n)) = O(\\max(m,n)\\log(\\max(m,n)))$).</p>\n\n<p>An other solution is to sort one array in-place, scan the other array and use bisection search to find matches. If we sort the smallest array (with size $m$) then the complexity is $O(m\\log(m) + n\\log(m)) = O(\\max(m,n)\\log(\\min(m,n)))$. This technique yields the values in the other in which their are found in the biggest array, and it can be used to solve the second problem.</p>\n\n<p>But, as you can see, I'm not using the extra restrictions at all and both algorithms use $O(1)$ space instead of $O(k)$(yes, it's still constant but $O(k)$ should give a bit more freedom).</p>\n", 'ViewCount': '325', 'Title': 'Lower-bound complexities for finding common elements between two unsorted arrays', 'LastEditorUserId': '7246', 'LastActivityDate': '2013-05-22T15:04:04.710', 'LastEditDate': '2013-05-22T15:04:04.710', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12210', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7246', 'Tags': '<time-complexity><arrays><lower-bounds>', 'CreationDate': '2013-05-21T12:03:27.047', 'Id': '12182'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here's the problem:</p>\n\n<p>I have a collection of collections, $C$, where each $c\\in C$ is a collection of sets $X\\subset U$.  Denote $c_i$ as the i-th $X$ in $c$.  Informally, I want to map all the sets in each collection to bins, where no two sets in a single collection can occupy the same bin, such that the sum of the sizes of the unions of all sets in each bin is minimized.  More formally:</p>\n\n<p>Let $N = \\max_{c \\in C} |c|$, and let $P_N$ be the set of all permutations of all non-empty subsets of the set $\\{1,2,...,N\\}$.  I wish to define a mapping:</p>\n\n<p>$$F : C \\rightarrow P_{N},\\ s.t.\\ \\forall c \\in C\\ (|F(c)| = |c|)$$</p>\n\n<p>with bin sets\n$$B(k) = \\{X \\subset U : \\exists c \\in C\\ (\\exists i \\in \\{1,2,...,|c|\\}\\ s.t.\\ c_i = X \\wedge (F(c))_i = k)\\}$$</p>\n\n<p>Such that the quantity</p>\n\n<p>$$\\sum_{k=1}^{N} { \\Biggl|\\bigcup_{X \\in B(k)}\\Biggr| } $$</p>\n\n<p>is minimized.</p>\n\n<hr>\n\n<p>Off the bat, I'd guess that this is an NP-hard problem - a reduction from Set Cover seems to be just within reach.  </p>\n\n<p>Even a greedy algorithm that iteratively processes each collection $c \\in C$, producing minimal results each time, requires $O(2^N \\cdot |C|)$ time using dynamic programming, where $|U|$ is assumed to be a constant factor.</p>\n\n<p>I'm having trouble proving whether or not the Greedy algorithm is even optimal - or if a more efficient solution exists.  Anyone have any thoughts?</p>\n\n<hr>\n\n<p>Alternatively, minimizing the quantity:</p>\n\n<p>$$\\max_{1 \\leq k \\leq N} {\\Biggl| \\bigcup_{X \\in B(k)} \\Biggr| }$$</p>\n\n<p>Is also of interest.  It's definitely a different problem, as demonstrated by a simple case where $C$ has 2 collections, one of the form $\\{\\{1\\}, \\{3, 4\\}\\}$, and the other $\\{\\{2\\}, \\{3, 4\\}\\}$.  I am not sure this problem is any easier though</p>\n", 'ViewCount': '74', 'Title': 'Overlap Maximization problem', 'LastEditorUserId': '7614', 'LastActivityDate': '2013-05-22T20:36:21.743', 'LastEditDate': '2013-05-22T20:36:21.743', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7614', 'Tags': '<algorithms><time-complexity><optimization><sets>', 'CreationDate': '2013-05-22T17:58:46.787', 'FavoriteCount': '1', 'Id': '12219'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>May be it's a stupid question (sorry if it's the case).</p>\n\n<p>What is the complexity the decision problem:</p>\n\n<p>Input: $A\\in\\mathcal{M}_{n,m}(\\mathbb{Z})$</p>\n\n<p>does there exists $x\\in\\mathbb{N}^n,x&gt;0$ such that $Ax\\geq 0$?</p>\n\n<p>Where $x\\geq 0$ means for all $x_i\\geq 0$ and $x&gt;0$ means there exists $x_j&gt;0$.</p>\n\n<p>I know that integer linear programming is NP-complete but it's expressed has $Ax\\leq b$ and I failed to reduce $Ax\\leq b$ to $Ax\\geq0$ ... </p>\n", 'ViewCount': '50', 'Title': 'Complexity of $Ax\\geq 0$', 'LastActivityDate': '2013-05-23T18:34:30.377', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12234', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7240', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-05-23T16:37:27.763', 'Id': '12232'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Reachability is defined as follows:\na digraph $G = (V, E)$ and two vertices $v,w \\in V$. Is there a directed path from $v$ to $w$ in $G$?</p>\n\n<p>Is it possible to write a polynomial time algorithm for it?</p>\n\n<p>I asked this question on mathematics and got no answer by far.</p>\n', 'ViewCount': '109', 'Title': 'Does reachability belong to P?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T07:39:49.460', 'LastEditDate': '2013-05-28T07:11:56.720', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '21360', 'Score': '-3', 'PostTypeId': '1', 'OwnerUserId': '51', 'Tags': '<complexity-theory><graph-theory><time-complexity>', 'CreationDate': '2013-05-26T05:57:18.243', 'Id': '12288'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If I use a balanced tree instead of lists in a hash table implementation, and also after initializing my table I don't enlarge nor reduce the size of the table, what would be the worst case complexity?</p>\n", 'ViewCount': '59', 'Title': 'Complexity of a hash tables with balanced trees in the buckets', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-03T21:57:37.240', 'LastEditDate': '2013-06-03T21:55:17.027', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12452', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8510', 'Tags': '<time-complexity><hash><hash-tables>', 'CreationDate': '2013-06-03T20:47:49.693', 'Id': '12450'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '216', 'Title': 'Using software to calculate the complexity of an algorithm', 'LastEditDate': '2013-06-05T21:42:55.220', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'rafiki', 'PostTypeId': '1', 'OwnerUserId': '8537', 'FavoriteCount': '1', 'Body': '<p>I am somewhat a beginner, and I have often seen complexity being calculated for various algorithms but they never actually gave me a very clear idea about how it is done. Can someone please point some resources where I can learn to calculate the complexity of an algorithm?</p>\n\n<p>Secondly, is there some software that calculates the space and time complexity for an algorithm? I have seen that <a href="http://en.wikipedia.org/wiki/Cyclomatic_complexity" rel="nofollow">cyclomatic complexity</a> can be calculated by software.</p>\n', 'Tags': '<time-complexity><complexity-theory>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-06-05T21:42:55.220', 'CommentCount': '1', 'CreationDate': '2013-05-20T11:28:55.373', 'Id': '12475'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Problem</strong>&nbsp;&nbsp;Given a Turing machine $M$ which has known runtime ${O}(g(n))$ with respect to input length $n$, is the runtime of $M \\in {O}(f(n))$?</p>\n</blockquote>\n\n<p>Is the above problem decidable for some nontrivial pairs of $g$ and $f$?A solution is trivial if $g(n) \\in O(f(n))$.</p>\n\n<p>This is related to the problem <a href="http://cstheory.stackexchange.com/questions/5004/are-runtime-bounds-in-p-decidable-answer-no">Are runtime bounds in P decidable? (answer: no)</a>. One can derive from <a href="http://cstheory.stackexchange.com/a/5006/314">Viola\'s answer</a> that if $f(n)\\not \\in o(n)$ and $f(n)\\not \\in O(g(n))$ then the problem is undecidable. </p>\n\n<p>The requirement that $f(n)\\not \\in o(n)$ is because the $M\'$ in Viola\'s proof need $O(n)$ time to find its input size. Thus Viola\'s proof could not work when $f(n)=1$.</p>\n\n<p>It would be interesting if we can decide on the run time of sublinear time algorithms. A special case is when we have arbitrary $g(n)$ and $f(n)=1$. </p>\n', 'ViewCount': '88', 'Title': 'Are runtime bounds decidable for anything nontrivial?', 'LastEditorUserId': '220', 'LastActivityDate': '2013-06-09T02:48:20.960', 'LastEditDate': '2013-06-07T21:14:42.577', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-06-07T20:41:57.460', 'FavoriteCount': '1', 'Id': '12518'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Assuming $P \\neq NP$ Is the following langauge in $P$ or $NPC$:<br>\n$L=\\{\\langle\\phi\\rangle\\mid\\phi$ is a 3CNF formula with an assignment satisfying at least half of the clauses$\\}$</p>\n\n<p>The first thing I tried to do is to find a 3CNF formula $\\phi$ such that  $\\phi \\notin L$ and I haven't managed to do so. Is it possible that simply all 3CNF formulas have such an assignment (and so the problem is in $P$) or am I missing something ?</p>\n", 'ViewCount': '55', 'Title': 'Is the following langauge in $P$ or $NPC$', 'LastActivityDate': '2013-06-14T23:01:55.957', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12680', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7068', 'Tags': '<complexity-theory><time-complexity><np-complete><satisfiability><3-sat>', 'CreationDate': '2013-06-14T22:05:23.980', 'Id': '12678'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>\u0398 - Tightly bound</p>\n\n<p>O - Upper bound</p>\n\n<p>\u03a9 - Lower bound</p>\n\n<p>I understand that for addition, the max asymptotic value is taken, for example...</p>\n\n<p>if \u01921(n) = O(n) and \u01922(n) = O(n log n), then \u01921 + \u01922 = O(max(\u01921, \u01922) = O(n log n)</p>\n\n<p>and for multiplication, the multiplied asymptotic value is taken, for example...</p>\n\n<p>if \u01921(n) = O(n) and \u01922(n) = O(n log n), then \u01921 * \u01922 = O(\u01921 * \u01922) = O((n^2)log n)</p>\n\n<p>My question is, what happens to the formulas for adding and multiplying functions of different growth rates when other bounds besides upper bounds (such as \u03a9 - Lower bound and \u0398 - Tightly bound) are used?</p>\n', 'ViewCount': '45', 'Title': 'Behavior of different multiplied and added time complexities', 'LastActivityDate': '2013-06-15T09:10:03.637', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12686', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8681', 'Tags': '<time-complexity><asymptotics>', 'CreationDate': '2013-06-15T08:24:43.060', 'Id': '12685'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Some context:</strong> I\'m to write a program that sorts the lines of a file in C for Linux. Since I have to read all lines of the file (<code>fgets()</code> for example) I\'m thinking about inserting them in a tree like structure in a sorted manner using the so called tree sort algorithm.</p>\n\n<p>Looking for a self-balancing tree structure I came across two that may be interesting, the <a href="http://en.wikipedia.org/wiki/Red-black_tree" rel="nofollow">red-black tree</a> and a <a href="http://en.wikipedia.org/wiki/Splay_tree" rel="nofollow">splay tree</a>. According to Wikipedia the red-black tree has an <code>O(log n)</code> worst case and the splay tree has <em>amortized</em> <code>O(log n)</code>.</p>\n\n<p><strong>The actual question:</strong> I know how to roughly compare some complexity levels in O notation but what\'s that amortized time? Given two algorithms one that runs in <code>O(log n)</code> and the other in amortized <code>O(log n)</code> which one would be preferrable?</p>\n', 'ViewCount': '540', 'Title': "What's better for an algorithm complexity, O(log n) or amortized O(log n)?", 'LastActivityDate': '2013-06-17T18:55:45.193', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '12715', 'Score': '3', 'OwnerDisplayName': 'James Russell', 'PostTypeId': '1', 'OwnerUserId': '8705', 'Tags': '<time-complexity><sorting>', 'CreationDate': '2013-06-17T09:57:31.023', 'Id': '12714'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '51', 'Title': 'Time complexity of mutually recursive functions', 'LastEditDate': '2013-06-18T19:19:00.467', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4490', 'FavoriteCount': '1', 'Body': '<p>Suppose I have two mutually recursive functions like this:</p>\n\n<pre><code>f1(x)\n{\n   .//some code\n   .\n   .\n   f1(x-1)+g1(x-1); \n\n}\n\ng1(y)\n{\n   .\n   .//some code\n   g1(y-1)+f1(y-1);\n  ..\n\n\n}\n</code></pre>\n\n<p>How can I calculate time complexities in such cases?</p>\n', 'Tags': '<time-complexity><recursion>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-18T19:19:00.467', 'CommentCount': '0', 'AcceptedAnswerId': '12743', 'CreationDate': '2013-06-18T15:43:18.620', 'Id': '12742'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $B=\\{b_1=g_1,\\cdots,b_n=g_n\\}$ be a set of binary variables $b_i$ and their corresponding values $g_i \\in \\{0,1\\}$. Let $M=\\{\\sum_{e \\in A}e \\;:\\; A \\subset B\\}$, i.e., $M$ is the set of all possible linear combinations of the equations in $B$.</p>\n\n<p>Given $S_i \\subset B$ for $i=1,\\cdots,m$, is that possible to compute, in polynomial time, a\n$K \\subset M$ with minimum size such that $S_i \\cup K$ is a full rank system of equations (i.e., the values of all of the variables can be obtained by solving $S_i \\cup K$)?</p>\n\n<p>An example: Let $B=\\{b_1=1,b_2=0,b_3=1\\}$, $S_1=\\{b_1=1,b_2=0\\}$, and $S_2=\\{b_2=0,b_3=1\\}$. \n$K=\\{b_1+b_3=0\\}$ is the solution because both $S_1\\cup K$ and $S_2 \\cup K$ can be solved uniquely and $K$ has the minimum size 1.</p>\n', 'ViewCount': '25', 'Title': 'Is this problem in P: Finding a common key for a collection of systems of equations?', 'LastActivityDate': '2013-06-20T02:54:32.613', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1672', 'Tags': '<complexity-theory><time-complexity><np-hard><polynomial-time><linear-algebra>', 'CreationDate': '2013-06-20T02:54:32.613', 'Id': '12776'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '132', 'Title': 'Why don\'t we emphasize "length of input string" when considering time complexity of sorting algorithms?', 'LastEditDate': '2013-06-26T13:42:50.850', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8809', 'FavoriteCount': '1', 'Body': "<p>The knapsack problem is $O(c\\,n)$ where $c$ is the capacity of knapsack and $n$ is the number of items. Yet it's exponential because the size of the input is $\\log(c)$.</p>\n\n<p>However, why don't we emphasize length of input in other algorithms? To name one example, what would be the <strong>input size $n$</strong> and <strong>worst case time complexity $T$</strong> of the following input when using insertion sort:</p>\n\n<p><code>111111111,101,11,10,1,0</code></p>\n\n<p>Answer A: $n=6$, $T=O(n^2)$<br>\nAnswer B: n = space(all_digits)+space(delimiters_between_numerics)</p>\n\n<p>If B is correct, what is the time complexity $T$?</p>\n", 'Tags': '<algorithms><terminology><time-complexity><runtime-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:42:50.850', 'CommentCount': '1', 'AcceptedAnswerId': '12827', 'CreationDate': '2013-06-22T07:43:42.833', 'Id': '12825'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve read that quantum computers can solve \'certain problems\' exponentially better than classical computers.  As I think I understand it, it\'s NOT the same to say that quantum computers take any problems that are EXPTIME-complete, 2-EXPTIME,... and convert them to linear time or constant-time.</p>\n\n<p>I would like to know something more about this matter:</p>\n\n<ul>\n<li>Why can/can\'t a quantum computer solve exponential problems in sub-exponential time?</li>\n<li>Is it at least theoretically possible to imagine a computer (quantum or otherwise) able to solve EXPTIME-complete problems in constant time? Or does this lead to a contradiction?</li>\n</ul>\n\n<p><strong>EDIT</strong>  a third related item:</p>\n\n<ul>\n<li>Can quantum computers do parallel computing?</li>\n</ul>\n\n<p>Now that the subject came up from comments, the idea about parallel computing, that\'s the usual/pop vision about quantum computers, like if quantum computers were able to compute "all posibilities at once" of any given problem (I think if that were the case, wouldn\'t be necesary to call great Peter Shor to invent a factoring algorithm!). Then "why" question about quantum computers can/cannot do parallel computing is half a computer science and a physics question.</p>\n\n<p>Here a source of confusion: <a href="http://physics.about.com/od/physicsqtot/g/quantumparallel.htm" rel="nofollow">http://physics.about.com/od/physicsqtot/g/quantumparallel.htm</a></p>\n', 'ViewCount': '250', 'Title': 'Quantum computers, parallel computing and exponential time', 'LastEditorUserId': '1396', 'LastActivityDate': '2013-06-25T18:57:45.420', 'LastEditDate': '2013-06-25T18:36:11.587', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1396', 'Tags': '<complexity-theory><time-complexity><quantum-computing>', 'CreationDate': '2013-06-25T15:36:04.140', 'FavoriteCount': '2', 'Id': '12892'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the $n$-fold integral \n$$\nJ = \\int_{\\theta_1 \\in I_1, \\theta_2 \\in I_2 \\ldots, \\theta_n \\in I_n} d\\theta_n\\ldots d\\theta_2 d\\theta_1\n$$</p>\n\n<p>whose intervals are defined by \n$$\n\\begin{align}\nI_1 = [0,1] \\\\\nI_i = [\\max(c_i,\\theta_{i-1}),1] , 2\\leq i\\leq n  \n\\end{align}\n$$</p>\n\n<p>and the $c_i \\in [0,1]$ are predefined <strong>rational</strong> constants. Given a rational $v\\in [0,1]$, </p>\n\n<blockquote>\n  <p>is deciding if $J=v$ NP-hard? </p>\n</blockquote>\n\n<p>Informally , each $\\max$ in the lower limits of the intervals leads to a two-way split in evaluating the integral, and thus to $2^{n-1}$ integrals that sum to $J$. </p>\n', 'ViewCount': '118', 'Title': 'A hard $n$-fold integral', 'LastActivityDate': '2013-07-03T17:50:32.753', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '13064', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '898', 'Tags': '<time-complexity>', 'CreationDate': '2013-06-25T17:33:57.953', 'Id': '12896'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How do you go about analysing coin change recursive solution. i.e,\n<code>T(N,K) = T(N,K-1) + T(N-1,K)</code> for K denominations that add up to amount N.</p>\n\n<p>You can find the problem description and pseudo code here - <a href="http://www.algorithmist.com/index.php/Coin_Change#Recursive_Formulation" rel="nofollow">http://www.algorithmist.com/index.php/Coin_Change#Recursive_Formulation</a></p>\n', 'ViewCount': '177', 'Title': 'Complexity of recursive solution to coin change', 'LastActivityDate': '2013-07-02T01:56:38.320', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'Raghu', 'PostTypeId': '1', 'OwnerUserId': '8298', 'Tags': '<time-complexity><recursion><asymptotics>', 'CreationDate': '2013-06-30T21:17:49.447', 'Id': '13017'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<pre><code>"The designer of an algorithm needs to balance between space complexity and time\ncomplexity." - Comment on the validity of the statement in the context of recursive\nalgorithms.\n</code></pre>\n\n<p>This is a question from my university\'s previous paper. But i couldn\'t find a decent answer. Actually i am confused about how can a developer minimize the time-complexity for any recursive function. I get that if there is a <strong>tail-recursion</strong> then space complexity can be minimized. But can\'t get the idea of time-complexity.  </p>\n', 'ViewCount': '2006', 'Title': 'Time complexity and space complexity in recursive algorithm', 'LastActivityDate': '2013-07-05T16:00:28.187', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13058', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8987', 'Tags': '<algorithms><time-complexity><space-complexity>', 'CreationDate': '2013-07-03T10:49:10.123', 'Id': '13055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I have a problem which depends on two variables, $m$ and $n$.  I also have two algorithms for solving the problem.  How do I decide which algorithm to use?</p>\n\n<p>For example, say I have an array of unique numbers $A$, not necessarily sorted, and a second sorted array $B$.  I want to create a third array $C$ which contains how many times each number in $A$ appears in $B$.  </p>\n\n<p>Algorithm 1 runs in time $O(m \\lg n)$ and algorithm 2 in $O(m \\lg m + n)$.</p>\n\n<p>It seems to me I would need to divide into three cases:</p>\n\n<ol>\n<li>$n=\\Theta(m)$</li>\n<li>$n\\gg m$</li>\n<li>$m \\gg n$</li>\n</ol>\n\n<p>How would I generally proceed from here?</p>\n", 'ViewCount': '81', 'Title': 'Deciding between two algorithms with similar runtime in two parameters', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-07T17:51:42.857', 'LastEditDate': '2013-07-07T17:51:42.857', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '13134', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><time-complexity><asymptotics>', 'CreationDate': '2013-07-07T13:25:17.193', 'Id': '13129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '112', 'Title': "Is there an algorithm which can compute every algorithm's time complexity?", 'LastEditDate': '2013-07-10T08:03:02.860', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8744', 'FavoriteCount': '1', 'Body': "<p>I think of an algorithm which computes the time complexity. It would be great if a code editor could compute the time complexity of the selected lines and even compare two pieces of codes in order to help developer to select one of them. I heard something related with halting problem but couldn't figure out. </p>\n", 'Tags': '<computability><time-complexity>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-10T08:03:02.860', 'CommentCount': '0', 'AcceptedAnswerId': '13197', 'CreationDate': '2013-07-10T07:09:48.463', 'Id': '13196'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a linear system of the form:</p>\n\n<p>$$\\begin{array}{c}\nx_r = a \\quad x_j = b \\\\\nc_1x_1 + c_2x_2 + \\ldots + c_nx_n = N \\\\\nx_1+x_2 + x_3 + \\ldots + x_n = k\\\\\n0 \\le a,b,x_1,x_2,x_3...x_n \\le 1\\\\\nk \\ge 0\n\\end{array}$$</p>\n\n<p>How quickly can the feasibility of the system be checked? To clarify: $x_r,x_j$ are members of $x_1,x_2...x_n$. Would it be $O(n^{3.5})$ since I believe that is the general complexity for running a linear program or would it be less? Can one use gaussian elimination to quickly reduce the first 4 equations in $O(n^3)$ and after that systematically move through the equations starting from the terms with largest coefficient and moving to terms with smallest coefficient assigning values that bring the equations as close to satisfactory as possible?</p>\n\n<p>Additional info:</p>\n\n<p>I am assuming that the number of variables scales linearly. $n \\ne N$ (I think that was clear though). </p>\n', 'ViewCount': '439', 'Title': 'Checking Feasibility of Linear Program in Polynomial Time', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T06:23:52.420', 'LastEditDate': '2013-07-22T06:23:52.420', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '13371', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9271', 'Tags': '<complexity-theory><time-complexity><computational-geometry><linear-programming><linear-algebra>', 'CreationDate': '2013-07-20T23:16:43.717', 'Id': '13370'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've constructed an algorithm that solves the 3SUM problem in $O(n\\lg n) + \\frac{n^2}{4}$ time. I'm new to algorithms and was wondering how good is my running time? Googling didn't help.\nthanks..</p>\n", 'ViewCount': '158', 'Title': 'Computing 3SUM problem in $O(n\\lg n) + \\frac{n^2}{4}$ time', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T05:17:15.700', 'LastEditDate': '2013-07-22T05:17:15.700', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9273', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-07-21T02:47:31.517', 'Id': '13372'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having a little trouble understanding what is meant by a poly-time reduction.\nSuppose I have two algorithms $A$ and $B$ and then I say that $A$ is reducible to $B$. Does polytime reduction mean that the algorithm that solves $A$ using $B$ as a helper runs in $O(n^k)$ for some $k$?</p>\n\n<p>So for example suppose:</p>\n\n<p>$A$ is an algorithm that takes as input a list of numbers and returns whether there is a sublist whose sum is $0$.</p>\n\n<p>$B$ is an algorithm that takes as input a list of numbers, and an integer $k$, and returns whether there is a sublist of length $k$ whose sum is $0$.</p>\n\n<p>Then </p>\n\n<pre><code>def A(L):\n     for i in range (1, len(L)+1)"\n           if B(L, i):\n                return true\n     return false\n</code></pre>\n\n<p>Since this $A$ calling $B$ as a helper runs in $O(n)$ so can this be described as a polytime reduction from $A$ to $B$?</p>\n', 'ViewCount': '450', 'Title': 'What does a polynomial time reduction mean?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-31T15:40:51.483', 'LastEditDate': '2013-07-31T15:40:10.873', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13543', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9410', 'Tags': '<time-complexity><reductions>', 'CreationDate': '2013-07-31T15:06:30.910', 'Id': '13541'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '280', 'Title': 'What linked list data structure adjustments would give me fast random lookup?', 'LastEditDate': '2013-08-07T13:56:23.087', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7284', 'FavoriteCount': '1', 'Body': "<p>I am presently using an doubly linked list (C++ <code>std::list</code>) to hold a bunch of records that each have a unique integer identifier.  The linked list is created in sorted order such that in the list, the next item always has a larger unique identifier than its predecessor.</p>\n\n<p>The issue I'm facing is that occasionally I need to be able to insert an item quickly into its  relative sorted position and using a plain linked list means this operation is $O(n)$ which is causing performance issues for me.  Generally, this would mean I want to use something like a binary tree (C++ <code>std::map</code>), however, I am also depending upon the following feature of a doubly linked list for good performance:</p>\n\n<ul>\n<li>Ability to splice a contiguous section out of one linked list into another in $O(1)$ time. (Amortized $O(1)$ or $O(\\log \\log n)$ would be good enough.)</li>\n</ul>\n\n<p>One feature of my data that I would like to leverage is that I often have long ranges of contiguous records where each one's unique integer is exactly one more than its predecessor.  When searching for an item's relative sorted position, it would always be outside such contiguous records since there are no duplicate identifiers.</p>\n\n<p>I'd like to find a replacement data structure or augmentation to a doubly linked list that will allow me to continue to splice whole sections from one list to another in constant time but allow me to locate the sorted position in which to insert a new record in better than $O(n)$ time.</p>\n\n<p>Other operations include forward and backward iteration across the items. The record indexes begin at zero and grow upwards towards 64 bits, generally sequentially, and the code works well in such cases. Occasionally some records are not available before subsequent ones, it is the insertion of these missing records that causes the performance issues now. </p>\n\n<p>One possible approach that occurs to me is to cache the location of several indexes.  The cache would get invalidated whenever a splice removes items that might overlap the cached entries.  With this cache, instead of doing a linear search, the search could instead begin from the cache point iterator whose unique index is closest to the one whose position is being  searched for.  However, I'd like to more fully utilize the feature of the contiguous records.  I also thought about a hierarchical linked list where I have a top level linked list of contiguous regions, where each region is a linked list of records that are consecutive, but I didn't see a clean way to adapt a linked list to provide this functionality.  Perhaps something like this has been done before?  I find skip lists to be close, but do not see the splice() functionality, plus a generic skip list would not leverage the fact that insertion never occurs within contiguous records.</p>\n", 'Tags': '<data-structures><time-complexity><binary-trees><linked-lists>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-08T10:37:59.160', 'CommentCount': '3', 'AcceptedAnswerId': '13646', 'CreationDate': '2013-08-05T23:00:36.700', 'Id': '13620'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to understand algorithm complexity, and a lot of algorithms are classified as polynomial. I couldn't find an exact definition anywhere. I assume it is the complexity that is not exponential. </p>\n\n<p>Do linear/constant/quadratic complexities count as polynomial? An answer in simple English will be appreciated :)</p>\n", 'ViewCount': '226', 'Title': 'What exactly is polynomial time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-07T16:32:18.580', 'LastEditDate': '2013-08-07T07:59:04.743', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '13626', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9538', 'Tags': '<algorithms><terminology><time-complexity><runtime-analysis><polynomial-time>', 'CreationDate': '2013-08-06T01:28:41.500', 'Id': '13625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have a decision problem $D$ and I encode it to a language $L \\subset \\{0,1\\}^*$. Now, I can also encode it to a different language $L'$.</p>\n\n<p>Is there any theorem relating the time complexity of $L$ and $L'$?</p>\n\n<p>How does the time complexity of a problem change with different encodings of the same problem?</p>\n", 'ViewCount': '211', 'Title': 'Does the time complexity of a problem change with encoding of the problem?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-08-07T19:01:09.883', 'LastEditDate': '2013-08-07T18:59:31.140', 'AnswerCount': '4', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '180', 'Tags': '<complexity-theory><terminology><time-complexity>', 'CreationDate': '2013-08-07T02:49:58.497', 'Id': '13640'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '169', 'Title': "Is the memory-runtime tradeoff an equivalent of Heisenberg's uncertainty principle?", 'LastEditDate': '2013-08-08T10:40:15.420', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9571', 'FavoriteCount': '3', 'Body': '<p>When I work on an algorithm to solve a computing problem, I often experience that speed can be increased by using more memory, and memory usage can be decreased at the price of increased running time, but I can never force the product of running time and consumed memory below a clearly palpable limit. This is formally similar to Heisenberg\'s uncertainty principle: the product of the uncertainty in position and the uncertainty in momentum of a particle cannot be less than a given threshold.</p>\n\n<p>Is there a theorem of computer science, which asserts the same thing? I guess it should be possible to derive something similar from the theory of Turing Machines.</p>\n\n<p>(I asked this question originally on <a href="http://stackoverflow.com/questions/18108578/the-uncertainty-principle-of-computer-science">StackOverflow</a>.)</p>\n', 'Tags': '<algorithms><time-complexity><space-complexity><performance>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-08T10:40:15.420', 'CommentCount': '3', 'AcceptedAnswerId': '13664', 'CreationDate': '2013-08-07T16:45:48.973', 'Id': '13661'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I would appreciate an intuitive way to find the time complexity of dynamic programming problems. Can anyone explain me \u201c#subproblems * time/subproblem\u201d? I am not able to grok it.</p>\n\n<p>Code for LCS - </p>\n\n<pre><code>public static String findLCS(String str1, String str2 ) {\n    // If either string is empty, return the empty string\n    if(null == str1 || null == str2)\n        return "";\n    if("".equals(str1) || "".equals(str2)) {\n        return "";\n    }\n    // are the last characters identical?\n    if(str1.charAt(str1.length()-1) == str2.charAt(str2.length()-1)) {\n        // yes, so strip off the last character and recurse\n        return findLCS(str1.substring(0, str1.length() -1), str2.substring(0, str2.length()-1)) + str1.substring(str1.length()-1, str1.length());\n    } else {\n       // no, so recurse independently on (str1_without_last_character, str2)\n       // and (str1, str2_without_last_character)\n       String opt1 = findLCS(str1.substring(0, str1.length() -1), str2); \n       String opt2 = findLCS(str1, str2.substring(0, str2.length()-1));\n       // return the longest LCS found\n       if(opt1.length() &gt;= opt2.length())\n           return opt1;\n       else\n           return opt2;\n    }\n}\n</code></pre>\n\n<p>I am just providing the actual code instead of pseudo code (i hope pseudo code or the algo is pretty self explanatory from above)</p>\n', 'ViewCount': '623', 'Title': 'Understand the time complexity for this LCS (longest common subsequence) solution', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-11T14:45:43.030', 'LastEditDate': '2013-08-11T11:39:51.457', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13706', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9166', 'Tags': '<algorithm-analysis><time-complexity><dynamic-programming>', 'CreationDate': '2013-08-11T10:28:35.407', 'Id': '13704'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My question is related to <a href="http://math.stackexchange.com/questions/465240/constant-time-algorithm-for-finding-greatest-factor-of-odd-composite-number">this question</a> posted on <a href="http://math.stackexchange.com">math.SE</a>:</p>\n\n<blockquote>\n  <p>Given an odd number, what is the quickest (constant-time) algorithm for finding its largest factor and suppose you can call a helper function $B$ which takes as its input $(N, k)$ and outputs True iff $N$ has a factor greater than or equal to $k$? Obviously, the factor cannot be itself. </p>\n</blockquote>\n\n<p>My slightly altered problem statement goes like this.</p>\n\n<blockquote>\n  <p>Given an odd integer $n$, find its largest factor (that is not itself). You can call a function $B(m,k)$ that returns $1$ iff $m$ has a factor smaller than $k$. The function runs in constant time.</p>\n</blockquote>\n\n<p>Can this be done faster than $O(\\log n)$ in the average case (assuming the input is chosen uniformly at random)? Is my altered problem statement any better than the original? Specifically, I know that the probability some large number will have no factor smaller than $M$ is asymptotic to $\\frac{1}{\\log M}$ (see <a href="http://math.stackexchange.com/questions/94645/expected-smallest-prime-factor">A</a>, and <a href="http://math.stackexchange.com/questions/284500/probability-of-a-number-not-having-factors-below-n">B</a>). Can you use this to your advantage?</p>\n\n<p>You can also assume that division is constant time.</p>\n\n<h1>Edit and Attempt Solution:</h1>\n\n<p>$\n\\newcommand{\\ha}[2]{\\left[#1 \\dots #2\\right)} \n\\newcommand{\\expa}[1]{2^{#1}}\n\\newcommand{\\expb}[1]{2^{2^{#1}}}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\expv}[1]{\\mathrm{E}\\left(#1\\right)}\n\\newcommand{\\sch}{\\mathbb{S}}\n\\newcommand{\\floor}[1]{\\left\\lfloor#1 \\right\\rfloor}\n\\newcommand{\\logb}[1]{\\log \\log #1}\n$\nThe difference between my problem statement and the original, is the definition of the function $B$. In the original, $B(N,k)$ returns $1$ if $N$ has a factor greater than or equal to $k$; in my version, this happens if $N$ has a factor <em>less than</em> $k$. </p>\n\n<p>By making this change, I aim to capitalize on the fact that the probability of a large $N$ having no factors smaller than $M$ is asymptotic to $(\\log M)^{-1}$. While this fact does not change the worst-case performance of an algorithm, it can change average-case performance. Here, by average case, I mean probabilistic analysis of the algorithm over random, uniformly distributed inputs.</p>\n\n<p>(Note that my question involves only exact algorithms)</p>\n\n<p>I conjectured in my comment to the related question that you can benefit from the probability by changing the way in which you partition your search space when performing a binary search (since it is much more likely for the solution to be in $\\ha{1}{1000}$ instead of $\\ha{1000}{N}$). I also conjectured the algorithm could run in $O(\\logb N)$ average-case.</p>\n\n<h2>Attempted Solution</h2>\n\n<p>I decided to do some work on the problem myself, and I think I have a solution. I haven\'t really done this sort of analysis previously, so I may have some error, and there is definitely a lot missing in terms of details. I think the idea is correct, though. Note that here I find the <em>smallest</em> factor of $N$. We can easily find the largest factor by division (which I assume to be contstant time).</p>\n\n<p>I\'m posting it as part of the question because I\'m not sure if it\'s correct, and I still want to know if there\'s a better way.</p>\n\n<p>Let us partition the search space $\\sch = \\ha{1}{N}$ into disjoint integer intervals, \n$$ r_k = \\ha{\\expb k}{\\expb {k+1}} \\qquad 0 \\leq k \\leq \\floor{\\logb N}$$</p>\n\n<p>Note that it can be that,\n$$ \\expb{\\floor{\\logb N+1}} &gt; N$$\nThat doesn\'t really matter; all we want from the partitioning $r$ is to contain the entire search space.</p>\n\n<p>Now, the probability that the smallest factor of $N$, which we will call $A$, is greater than $m$ is asymptotic to $(\\log m)^{-1}$. \nThen let, $P(A &gt; \\expb{k}) = 2^{-k}$, where $A$ is taken to be a random variable. If we let $P(r_k)$ denote the probability that $A \\in r_i$, we can calculate this as:\n$$P(r_k) = P(a &gt; \\expb{k}) - P(a &gt; \\expb{k+1}) = 2^{-(k+1)}$$</p>\n\n<p>We can identify which partition $r_k$ contains $A$ by calling the function $B(N,\\expb{k})$ up to $\\floor{\\logb N} + 1$ times. After finding the $r_k$, we then perform a binary search for $A$ in the partition, which involves $\\log \\abs{r_k}$ operations. Here we note that:\n$$|r_k|=\\expb{k+1}-\\expb{k} = \\expb{k}\\left(\\expb{k} - 1\\right)\\leq \\expb{k+1}$$</p>\n\n<p>Let $X$ be a random variable representing the number of operations taken by the binary search. The value of $X$ for the case when $A \\in r_k$ is given $X_k = \\log \\expb{k+1} = 2^{k+1}$. The expected value of $X$ is then,\n$$\\expv{X} = \\sum_{k=0}^{\\floor{\\logb N} + 1} X_i P(r_i) = \\sum_{k=0}^{\\floor{\\logb N} + 1} 2^{k+1}\\cdot 2^{-(k+1)} = \\floor{\\logb N} +1$$</p>\n\n<h2>Notes</h2>\n\n<p>I\'ve considered partitioning the search space using triple-exponentiation (e.g. $\\expa{\\expb{k}}$), but that provides no benefit. There might be a way to make the search algorithm inside the partitions faster though, but I\'m not sure how. </p>\n\n<p>You can also reduce the search space drastically (such as to something like $\\sqrt{N}$), but I think this will have a constant speedup at most.</p>\n', 'ViewCount': '195', 'Title': 'Time complexity of finding the largest factor of a number (using a specific oracle)', 'LastEditorUserId': '4543', 'LastActivityDate': '2013-08-17T16:51:14.237', 'LastEditDate': '2013-08-17T16:51:14.237', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4543', 'Tags': '<algorithms><time-complexity><number-theory><factoring>', 'CreationDate': '2013-08-16T09:49:37.510', 'FavoriteCount': '2', 'Id': '13773'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Imagine that an algorithm A runs in worst-case time $f(n)$ and that algorithm B runs in worst-case time $g(n)$. Answer either yes, no, or can\u2019t tell and could you explain me why?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(h)=\u03a9(f(n)\\log n)$?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(n)=O(f(n)\\log n)$?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(n)=\u0398(f(n)\\log n)$?</p>\n', 'ViewCount': '133', 'Title': 'Worst-case time algorithm?...which one is faster?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-08-20T17:22:07.640', 'LastEditDate': '2013-08-20T17:22:07.640', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-08-20T03:11:35.790', 'FavoriteCount': '2', 'Id': '13827'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>By a vanilla Turing machine, I mean a Turing machine with one tape (no special input or output tapes).</p>\n\n<p>The problem is as follows: the tape is initially empty, other than a string of $n$ $1$s and $0$s terminated by an end-of-string character. The tape head starts at the beginning of the string. The goal is for the tape to contain the original string in reverse order, terminated by an end-of-string character, with the tape head returned to the beginning of the string when the Turing machine finally halts.</p>\n\n<p>The Turing machine can use as large an alphabet as we like (so long as it contains $0$, $1$, and an end-of-string character), and can have as many states as we like. Is there a fixed Turing machine that can complete this task in time $o(n^2)$?</p>\n\n<p>It's easy to do this in time $O(n^2)$ using just a few states and symbols. It seems intuitively clear that something prevents us from doing it more than a constant factor faster, but I've never been able to prove it, and I often worry late into the night about miraculous applications of network coding or voodoo magic that somehow get a logarithmic speedup...</p>\n", 'ViewCount': '392', 'Title': 'Can you do an in-place reversal of a string on a vanilla turing machine in time $o(n^2)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-28T21:23:11.410', 'LastEditDate': '2013-08-28T09:35:04.997', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '13996', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9868', 'Tags': '<complexity-theory><time-complexity><turing-machines>', 'CreationDate': '2013-08-28T06:19:06.700', 'Id': '13988'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have two polynomials $f(x)$ and $g(x)$ and I somehow represent their coefficients.\nI have a couple of ways to hold a polynomial depending on how many significant coefficients the polynomial has.\nI want to determine the amount of significant coefficients in the results of\n$f(x) + g(x)$ ,$f(x) \\cdot g(x)$ , $f(x) - g(x)$ etc. .</p>\n\n<p>But I'd like to do it before I create the object that holds them, is there some efficient way of doing this without calculating the result twice?</p>\n\n<p>I can assume that I know the current rank and number of elements in $f(x)$ and $g(x)$</p>\n\n<p>If this is not possible knowing that the new polynomial's non-trivial coefficients will be at least half of the rank will suffice, but I'm unsure how to do it as well.</p>\n\n<p>I did try to apply various heuristics but didn't come up with something consistent and fast. </p>\n", 'ViewCount': '67', 'Title': 'Calculate the number of elements after multiplying/adding two polynomials', 'LastActivityDate': '2013-08-28T21:13:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14006', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8709', 'Tags': '<algorithms><time-complexity><optimization><efficiency><arithmetic>', 'CreationDate': '2013-08-28T19:13:48.453', 'Id': '13997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '116', 'Title': 'Is the DPLL algorithm complexity in terms of # of clauses or # of variables?', 'LastEditDate': '2013-08-29T06:21:14.013', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Roak', 'PostTypeId': '1', 'OwnerUserId': '9878', 'Body': '<p>I\'m a bit confused how worst case complexity is estimated for the <a href="http://en.wikipedia.org/wiki/DPLL_algorithm" rel="nofollow">DPLL algorithm</a>. Is it in terms of number of clauses, number of variables, or something else?</p>\n', 'Tags': '<time-complexity><asymptotics>', 'LastEditorUserId': '9878', 'LastActivityDate': '2013-08-30T21:42:43.867', 'CommentCount': '7', 'AcceptedAnswerId': '14048', 'CreationDate': '2013-08-28T21:02:28.893', 'Id': '14004'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to check time complexity of two procedures for which I am not totally convinced that I got it right. Now the first procedure is this:</p>\n\n<pre><code>public static int c(int n) {\n    int i, j, s = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i*=2) {\n            for(j = n; j &gt; 0; j/=2) {\n                s++;\n            }\n            s+=c(n-2);\n        }\n    }\n}\n</code></pre>\n\n<p>Now I have set the following recurrence equation:\n$T(n)=\\log_2n*T(n-2)+\\Theta(\\log_2n)$</p>\n\n<p>Now the height of the recurrence tree is $n/2$ so the $T(n) = n/2 * (\\log_2n+\\log_2n)=\\Theta(n*\\log_2n)$</p>\n\n<p>The next procedure is this:</p>\n\n<pre><code>public static int d(int n, int m) {\n    int i, j, k, ss = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i++) {\n            for(j = i; j &gt; n; j+=2) {\n                ss+=(i+1)-2*j+m;\n            }\n        }\n        for(k=0; k&lt;8; k++) {\n            ss+=d(n/2, m/(k+1))\n        }\n    }\n    return ss;\n}\n</code></pre>\n\n<p>Again I have set this equation:\n$T(n, m)=8*T(n/2, m/(k+1))+\\Theta(n^2)$</p>\n\n<p>Now I think the $m$ parameter is not important because it is not used in for loop as a counter. Where $n/2$ gives us a recurrence tree of height $\\log_2n$. So we get this:\n$T(n, m) = 8 * \\log_2n*n^2=\\Theta(n^2)$</p>\n\n<p>I know that recurrence equations that I have set up are probably right, while the I am not sure about next steps.</p>\n', 'ViewCount': '109', 'Title': 'Finding asymptotically tight bounds $\\Theta$ of two procedures', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-04T09:50:41.463', 'LastEditDate': '2013-09-04T09:22:55.420', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14124', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9974', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-04T08:14:21.707', 'Id': '14121'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '202', 'Title': 'Machines in P undecidable?', 'LastEditDate': '2013-09-09T10:30:31.167', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9515', 'FavoriteCount': '1', 'Body': "<p>Given a Turing machine $M$, we say that $L(M) \\in P$ if the language decided by the machine can be decided by some machine in polynomial time. We say that $M \\in P$ if the machine runs in polynomial time. Note that there can be machines that run needlessly long but still decide a language in $P$. By Rice's theorem, we know that</p>\n\n<p>$\\{ \\langle M \\rangle \\mid M \\mbox{ is a Turing machine such that }L(M) \\in P \\mbox{ } \\}$ is undecidable. Is it known whether:</p>\n\n<p>$\\{ \\langle M \\rangle \\mid M \\mbox{ is a Turing machine such that }M \\in P \\mbox{ } \\}$ is also undecidable?</p>\n", 'Tags': '<computability><time-complexity><turing-machines>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T10:30:31.167', 'CommentCount': '4', 'AcceptedAnswerId': '14216', 'CreationDate': '2013-09-08T17:55:31.410', 'Id': '14215'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Some complicated algorithms (<a href="http://en.wikipedia.org/wiki/Union-find">union-find</a>) have the nearly-constant inverse Ackermann function that appears in the asymptotic time complexity, and are worst-case time optimal if the nearly constant inverse Ackermann term is ignored. </p>\n\n<p>Are there any examples of known algorithms with running times that involve functions that grow fundamentally slower than inverse Ackermann (e.g. inverses of functions that are not equivalent to Ackermann under polynomial or exponential etc. transformations), that give the best-known worst-case time complexity for solving the underlying problem?</p>\n', 'ViewCount': '207', 'Title': 'Do functions with slower growth than inverse Ackermann appear in runtime bounds?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-18T21:26:09.597', 'LastEditDate': '2013-09-18T21:26:09.597', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<reference-request><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-17T23:51:31.520', 'FavoriteCount': '3', 'Id': '14392'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>If the running time of an algorithm scales linearly with the size of its input, we say it has $O(N)$ complexity, where we understand <code>N</code> to represent input size.</p>\n\n<p>If the running time does not vary with input size, we say it\'s $O(1)$, which is essentially saying it varies proportionally to 1; i.e., doesn\'t vary at all (because 1 is constant).</p>\n\n<p>Of course, 1 is not the only constant. <em>Any</em> number could have been used there, right? (Incidentally, I think this is related to the common mistake many CS students make, thinking "$O(2N)$" is any different from $O(N)$.)</p>\n\n<p>It seems to me that 1 was a sensible choice. Still, I\'m curious if there is more to the etymology there\u2014why not $O(0)$, for example, or $O(C)$ where $C$ stands for "constant"? Is there a story there, or was it just an arbitrary choice that has never really been questioned?</p>\n', 'ViewCount': '225', 'Title': 'Why is it O(1) (and not, say, O(2))?', 'LastEditorUserId': '9574', 'LastActivityDate': '2013-09-19T17:17:20.410', 'LastEditDate': '2013-09-18T20:36:25.443', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9574', 'Tags': '<terminology><time-complexity><asymptotics><landau-notation><history>', 'CreationDate': '2013-09-18T16:39:15.317', 'FavoriteCount': '2', 'Id': '14416'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>As the question states, how do we prove that $\\textbf{NTIME}(f(n)) \\subseteq \\textbf{DSPACE}(f(n))$?</p>\n\n<p>Can anyone point me to a proof or outline it here? Thanks!</p>\n', 'ViewCount': '105', 'Title': 'NTIME(f) subset of DSPACE(f)', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-09T02:19:16.840', 'LastEditDate': '2013-09-20T17:45:09.420', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10237', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2013-09-20T16:26:15.297', 'Id': '14475'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can anybody give me a way that i can use to calculate Complexity Time problems, i mean a way that will work on any complex code whatever it was.</p>\n\n<p>I can solve basic problems, but whenever the problem gets a little complex i freeze which means that my way of thinking and solving is wrong.</p>\n\n<p>for example like this:</p>\n\n<p><img src="http://i.stack.imgur.com/7xKdY.png" alt="enter image description here"></p>\n', 'ViewCount': '51', 'ClosedDate': '2013-09-27T19:39:28.710', 'Title': 'How to calculate Complexity Time O()?', 'LastActivityDate': '2013-09-27T13:43:08.540', 'AnswerCount': '0', 'CommentCount': '8', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10361', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-09-27T13:43:08.540', 'Id': '14642'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I can't figure out or find on the Internet what is the time complexity of B-tree in-order depth-first iteration. Can you help me out here, please?</p>\n\n<p>What is the characteristic operation we're using to estimate the running time here, is it a disk seek? Assuming that we have enough space to temporarily store the number of tree nodes equal to the height of the tree, it looks like we will only need to load each node once and keep it in memory while we're examining its children. Am I right?</p>\n", 'ViewCount': '53', 'Title': 'What is time complexity of in-order depth-first iteration over keys in a B-tree?', 'LastEditorUserId': '10378', 'LastActivityDate': '2013-09-28T18:19:24.870', 'LastEditDate': '2013-09-28T18:19:24.870', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10378', 'Tags': '<time-complexity><search-trees>', 'CreationDate': '2013-09-28T14:01:24.307', 'Id': '14654'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>We have started learning about analysis of recursive algorithms and I got the gist of it. However there are some questions, like the one I'm going to post, that confuse me a little.</p>\n\n<h3>The exercise</h3>\n\n<blockquote>\n  <p>Consider the problem of multiplying two big integers, i.e. integers represented by a large number of bits that cannot be handled directly by the ALU of a single CPU. This type of multiplication has applications in data security where big integers are used in encryption schemes. The elementary-school algorithm for multiplying two n-bit integers has a complexity of . To improve this complexity, let x and y be the two n-bit integers, and use the following algorithm</p>\n\n<pre><code>Recursive-Multiply(x,y)\n  Write  x = x1 * 2^(n/2)+x0  //x1 and x0 are high order and low order n/2 bits\n       y = y1 * 2^(n/2)+y0//y1 and y0  are high order and low order n/2 bits\n  Compute x1+x0  and y1+y0\n  p = Recursive-Multiply (x1+x0,y1+y0)\n  x1y1 = Recursive-Multiply (x1,y1)\n  x0y0 = Recursive-Multiply (x0,y0)\n  Return  x1y1*2^n + (p-x1y1-x0y0)*2^(n/2)+x0y0\n</code></pre>\n  \n  <p>(a) Explain how the above algorithm works and provides the correct answer.</p>\n  \n  <p>(b) Write a recurrence relation for the number of basic operations for the above algorithm.</p>\n  \n  <p>(c) Solve the recurrence relation and show that its complexity is $O(n^{\\lg 3})$</p>\n</blockquote>\n\n<h3>My conjecture</h3>\n\n<ol>\n<li>Since the method is being called three times, the complexity is going to be $3C(n/2) + n/2$.</li>\n</ol>\n\n<h3>My questions</h3>\n\n<ol>\n<li><p>What do they mean by hi-lo order bits?</p></li>\n<li><p>How can I use a recurrence relation on this if I don't know how each recursion works?</p></li>\n</ol>\n", 'ViewCount': '228', 'Title': 'Complexity of a recursive bignum multiplication algorithm', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-30T00:24:45.967', 'LastEditDate': '2013-09-29T23:25:48.427', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14690', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10398', 'Tags': '<algorithms><time-complexity><recursion>', 'CreationDate': '2013-09-29T23:02:57.907', 'Id': '14685'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So I\'m studying for my data structures midterm and my professor gave out a sample midterm with the answers, but I\'m having a hard time understanding one of the questions. </p>\n\n<p>Here\'s a screen cap:</p>\n\n<p><img src="http://i.stack.imgur.com/EPFBo.png" alt="enter image description here"></p>\n\n<p>The answer is <code>n^2(n+1)/2</code>.</p>\n\n<p>I would appreciate a very simple walk through and/or a pointer to some good material. </p>\n', 'ViewCount': '96', 'Title': 'Deriving the exact number of execution times', 'LastEditorUserId': '10424', 'LastActivityDate': '2013-10-07T00:35:37.567', 'LastEditDate': '2013-10-06T21:36:25.240', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '14844', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10424', 'Tags': '<time-complexity><runtime-analysis>', 'CreationDate': '2013-10-06T00:39:07.513', 'Id': '14843'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So I\'m studying for a midterm and my professor put out a sample exam with the answers, and I\'m stuck on one of the questions.</p>\n\n<p><img src="http://i.stack.imgur.com/lexzm.png" alt="enter image description here"></p>\n\n<p>The answer is <code>Big-O(n^2 log n)</code></p>\n\n<p>Could someone show me the steps necessary to go from regular time complexity to asymptotic time complexity?</p>\n', 'ViewCount': '117', 'Title': 'Finding asymptotic time complexity', 'LastEditorUserId': '10424', 'LastActivityDate': '2013-10-06T21:35:47.470', 'LastEditDate': '2013-10-06T21:35:47.470', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14846', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10424', 'Tags': '<time-complexity><asymptotics>', 'CreationDate': '2013-10-06T02:04:33.010', 'Id': '14845'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is my approach </p>\n\n<p>First I compute the longest non decreasing sub-sequence in $N \\log N$ time. Algorithm to do this (that only uses arrays and binary search) can be found here:\n<a href="http://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms" rel="nofollow">http://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms</a></p>\n\n<p>Let\'s suppose the longest subsequence has $L$ elements. Then if $L &lt; N - M$, there isn\'t any way to solve the problem since there\'s no subsequence of length $N - M$ that\'s still sorted.</p>\n\n<p>Otherwise, just remove the $N - L$ elements that aren\'t in the subsequence, and then remove more at random until exactly M total have been removed. In all this is an $N \\log N$ algorithm.</p>\n\n<p>I want to know, is there any more efficient algorithm (i.e. $O( N)$ ) to solve this problem ?</p>\n', 'ViewCount': '193', 'Title': 'Given an array of N integers, how can you find M elements to remove so that the array will end up in sorted order?', 'LastEditorUserId': '6890', 'LastActivityDate': '2013-10-08T23:01:04.927', 'LastEditDate': '2013-10-08T08:47:24.010', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8690', 'Tags': '<algorithms><time-complexity><runtime-analysis><subsequences>', 'CreationDate': '2013-10-08T08:40:39.700', 'FavoriteCount': '1', 'Id': '14899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There are $n$ pairs of socks, all different. They all went out of the dryer, so there are now $2n$ socks scattered around. Given two socks, the only operation I can do is to decide whether they are identical (- belong to the same pair) or different (- belong to different pairs). What is the best way to sort the socks in matching pairs?</p>\n\n<p>The obvious algorithm is: grab a sock, scan the pile for its matching sock, and put that pair aside. This algorithm requires time $O(n^2)$.</p>\n\n<p>I Found a related question, <a href="http://www.mail-archive.com/kragen-tol@canonical.org/msg00084.html" rel="nofollow">On the complexity of sock-matching</a> from about 10 years ago, which also describes an $O(n^2)$ algorithm.</p>\n\n<p>If each sock has a numeric ID, or another ordinal property, such as: wavelength of color, then the entire set of socks can be sorted by that ordinal property in time $O(n log n)$, and then the pairs can just be collected with an $O(n)$ scan. But, in this question I assume that there is no order on the socks - we can only tell whether two socks are identical or different.</p>\n\n<p>In a similar question in StackOverflow (<a href="http://stackoverflow.com/questions/14415881/how-to-pair-socks-from-a-pile-efficiently">How to pair socks from a pile efficiently?</a>), the accepted answer suggests to use a hash, but again, this assumes that each sock can be represented by a set of properties such as color, pattern, etc., so that each property can be handled separately. In this question I assume that the properties of the socks are only accessible via a function that tells whether two socks are identical or different.</p>\n\n<p>There is a paper named <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.4654&amp;rep=rep1&amp;type=pdf" rel="nofollow">Sock sorting</a>, which initially seems related, but actually it deals with another problem, where two socks might seem identical although they are different (this probably makes the problem more difficult).</p>\n\n<p>So, my question is: assuming we only have a binary "identical/different" relation on the socks, can we match the socks faster than $O(n^2)$?</p>\n', 'ViewCount': '140', 'Title': 'sock matching algorithm', 'LastActivityDate': '2013-10-16T13:17:21.433', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16135', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><time-complexity><sorting>', 'CreationDate': '2013-10-16T12:42:54.633', 'FavoriteCount': '1', 'Id': '16133'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is an exercise in the book Introduction to Algorithm, 3rd Edition.</p>\n\n<p>The original question is:</p>\n\n<p>Show how to implement GREEDY-SET-COVER in such a way that it runs in time $O(\\sum_{S\\in\\mathcal{F}}|S|)$.</p>\n\n<p>The GREEDY-SET-COVER in the text book is as follows:</p>\n\n<p><img src="http://i.stack.imgur.com/v55Gn.png" alt="greedy-set-cover-in-text-book"></p>\n\n<p>Definition for $(X,\\mathcal{F})$ is given as:</p>\n\n<p><img src="http://i.stack.imgur.com/3aVbz.png" alt="enter image description here"></p>\n', 'ViewCount': '397', 'ClosedDate': '2013-10-30T10:05:07.743', 'Title': 'How to implement GREEDY-SET-COVER in a way that it runs in linear time', 'LastActivityDate': '2013-10-16T18:49:41.563', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><algorithm-analysis><time-complexity><greedy-algorithms>', 'CreationDate': '2013-10-16T17:15:40.530', 'Id': '16142'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '105', 'Title': 'Using a proof-of-work system to discourage piracy or encourage donations', 'LastEditDate': '2013-10-21T04:12:46.827', 'AnswerCount': '3', 'Score': '4', 'OwnerDisplayName': 'LateralFractal', 'PostTypeId': '1', 'OwnerUserId': '10539', 'FavoriteCount': '2', 'Body': '<p><strong>Background</strong></p>\n\n<p>A <a href="http://en.wikipedia.org/wiki/Proof-of-work_system" rel="nofollow">proof-of-work system</a> allows one peer to prove to another peer that a certain amount of computational effort was performed.\nIn a network setting this can be used to throttle peer requests without needing to keep a precise track on the identity of the peers or prior events. \nThe most well known use of proof-of-work is to throttle spam throughput in an email network*.</p>\n\n<p>Some proof-of-work systems allow certain roles on the network (say, a mailing list) to calculate the proof of work much faster by using a secret <em>"short-cut"</em> - typically a pre-calculated trapdoor or a private key depending on the proof-of-work system.</p>\n\n<p><strong>Hypothesis</strong></p>\n\n<ol>\n<li>Any algorithm or a certain class of algorithms can be converted into a proof-of-work version of that algorithm. That is: A deliberately inefficient and incompressible algorithm.</li>\n<li>Such converted proof-of-work algorithms can support proof-of-work shortcuts.</li>\n<li>Such converted proof-of-work algorithms can not be converted back to the original algorithm without considerable computational power; if at all.</li>\n</ol>\n\n<p><strong>Extrapolation</strong></p>\n\n<p>If the hypothesis holds, then selected business logic of an application - specifically that which is unique or value-added by that application vs existing applications - could be converted to proof-of-work equivalents.</p>\n\n<p>End-users could then be provided such an application; which will run slowly either generally or for certain premium features. The development team however, possesses a secret PoW shortcut and set up a subscription, donation or advertising(<a href="http://www.aef.com/on_campus/classroom/book_excerpts/data/1504" rel="nofollow">!</a>)-based service - an <a href="http://en.wikipedia.org/wiki/Software_as_a_service" rel="nofollow">SaaS</a> - for solving the proof-of-work bottleneck for the end-user. The end-user can now choose to run the application slower without the SaaS or faster with the SaaS. </p>\n\n<p>This SaaS needs to process considerably less client-side business logic than a general cloud solution (e.g. Diablo 3 style SaaS); as the goal is the speed of execution rather than no execution - and the SaaS proof-of-work speed ratio is tailored accordingly. </p>\n\n<p>This is especially relevant for software projects supported by charity as the developers <em>do</em> want the software available to anyone but can encourage donations without needing a separate fairly easy to pirate <a href="http://en.wikipedia.org/wiki/Freemium" rel="nofollow">Freemium</a> edition. The Tragedy of the Commons (freeloading) could be discouraged to a fair extent without guilt-tripping or <a href="http://www.theregister.co.uk/2003/01/17/i_poisoned_p2p_networks/" rel="nofollow">rat poisoning</a>, by adjusting the proof-of-work cost relative the value of the application or service.</p>\n\n<blockquote>\n  <p><em>Example 1:</em> A commercial stock market estimation tool licensed per\n  month. If the user forgoes paying for a subscription, the estimation\n  occurs at 1% the normal rate.</p>\n  \n  <p><em>Example 2:</em> A free <a href="http://gamedevtycoon.wikia.com/wiki/AAA" rel="nofollow">Triple A</a> co-op video game runs twice fast if the\n  user donates some money once a year.</p>\n</blockquote>\n\n<p>In either example the user must decide between accepting the default speed; spending money on a faster computer/cloud services; or contributing to the upkeep of the product.</p>\n\n<p><strong>Question</strong></p>\n\n<p><em>Does the hypothesis hold and has anyone attempted to explore or implement this hypothesis?</em></p>\n\n<p>Any example of an open source library or application that attempts to implement the hypothesis qualifies as a sufficient answer (from my perspective); as I could dissect the code.</p>\n\n<p><sub> * Which has a variety of issues for the Digital Divide, but that\'s another story</sub></p>\n', 'Tags': '<time-complexity><one-way-functions><proof-of-work>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-21T13:56:59.407', 'CommentCount': '3', 'CreationDate': '2013-10-03T03:52:24.797', 'Id': '16188'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Problem Statement:\nSuppose we a thousands of words and we need to maintain these words in a data structure in such a way that we should be able to find all anagrams for a given string.\nI tried to achieve this with O(1) complexity.</p>\n\n<p>I am looking for a algorithm to implement above scenario. I implemented this problem with below algo, but i feel that we can improve its complexity. Any suggestion will be helpful.</p>\n\n<p>Algorithms:</p>\n\n<p>Here is trick to utilize hash code, we can also use character histogram.</p>\n\n<p>Step 1:Create an array of prime numbers.</p>\n\n<pre><code>   int primes[] = {2, 3, 5, 7, ...};\n\n   We are using prime number to avoid false collisions.\n</code></pre>\n\n<p>Step 2:Create a method to calculate hash code of a word\\string.</p>\n\n<pre><code>   int getHashCode(String str){\n     int hash = 31;\n     for(i =0 to length of str){\n        hash = hash*primes['a' - str.charAt[i]];\n     }\n     return hash;\n   }\n</code></pre>\n\n<p>Step 3: Now store all words in a HashMap.</p>\n\n<p>void loadDictionary(String[] words){</p>\n\n<pre><code>  for( word from words for i = 0 to length of words)   {\n     int hash  = getHashCode(word);\n     List&lt;String&gt; anagrams = dictionary.get(hash);\n     if(anagrams ! = null){\n         anagrams.add(word);\n     } else\n        List&lt;String&gt; newAnagrams = new ArrayList&lt;String&gt;();\n        newAnagrams.add(word);\n        dictionary.put(hash, newAnagrams);\n     }\n }\n}\n</code></pre>\n\n<p>Step 4: Now here is the approach to find anagrams:</p>\n\n<pre><code>   int findNumberOfAnagrams(String str){\n\n   List&lt;String&gt; anagrams = dictionary.get(getHashCode(str));\n      return anagrams.size();\n\n   }\n</code></pre>\n", 'ViewCount': '635', 'Title': 'Algorithm to write a dictionary using thousands of words to find all anagrams for a given string with O(1) complexity', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-19T14:15:13.607', 'LastEditDate': '2013-10-19T11:43:38.863', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10848', 'Tags': '<algorithms><complexity-theory><time-complexity><strings>', 'CreationDate': '2013-10-19T07:27:25.860', 'FavoriteCount': '1', 'Id': '16221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have been reading a paper <a href="http://academic.research.microsoft.com/Paper/298449" rel="nofollow">Finding Maximal Pairs with Bounded Gap</a>:</p>\n\n<p>An in there, there is a sentence (page 6 second paragraph):</p>\n\n<blockquote>\n  <p>The \u201csmaller-half trick\u201d is used in several methods for finding tandem repeats,\n  e.g. [2, 5, 26]. It says that the sum over all nodes <em>v</em> in an arbitrary binary tree of size <em>n</em> of terms that are $O(n_{1})$, where $n_{1}\\leq n_{2}$ are the numbers of leaves in the subtrees rooted at the two children of <em>v</em>, is $O(n\\log n)$</p>\n</blockquote>\n\n<p>Although it sounds very straightforward, I just cannot see why this is true. Could someone with some experience regarding this trick please explain why this is true?\nI read several other papers but just don\'t see it.</p>\n', 'ViewCount': '57', 'Title': 'Suffix trees - "smaller half trick"', 'LastEditorUserId': '683', 'LastActivityDate': '2013-10-21T02:04:16.937', 'LastEditDate': '2013-10-21T01:40:07.977', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16273', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6697', 'Tags': '<time-complexity><suffix-array>', 'CreationDate': '2013-10-20T11:35:12.720', 'Id': '16253'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to prove that $n^4+2n = \\Omega (n^2)$</p>\n\n<p>Which means I need to point two constants $c_1,c_2$ that from $n_0$:\n$$ c_1 \\leq \\frac{\\log(n^4+2n)}{\\log(n^2)} \\leq c_2$$</p>\n\n<p>Now I know that:\n$\\lim_{n \\to \\infty } \\frac{\\log(n^4+2n)}{\\log(n^2)} = 2$ so I can choose $c_1 = 0.5$ and $c_2 = 3$ but how do I do I calculate the $n_0$ that this sentence become true to?</p>\n', 'ViewCount': '51', 'Title': 'Proving $f = \\Omega (h)$', 'LastActivityDate': '2013-10-20T13:35:13.730', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16258', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10875', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-10-20T12:49:45.960', 'Id': '16255'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been stuck with this problem for 2 weeks. Any idea of how to aproach it?.</p>\n\n<blockquote>\n  <p>Let $L$ be a list of $n$ different integer numbers, assume that the elements of $L$ are in the range $[1,750]$. Design a linear ordering algorithm to order the elements of $L$.</p>\n</blockquote>\n\n<p>I already tried with insertion sort. But I'm not sure if my approach is right:</p>\n\n<ul>\n<li>Construct an array of bits. Initialize them to zero.</li>\n<li>Read the input, for each value you see set the respective bit in the array to 1.</li>\n<li>Scan the array, for each bit set, output the respective value.</li>\n</ul>\n\n<p>Complexity: $O(2n) = O(n)$</p>\n\n<p>I also wanted to use radix sort but I can't understand how to apply it, any idea?</p>\n", 'ViewCount': '146', 'Title': 'Sorting in O(n) time in a finite domain', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-23T10:11:12.493', 'LastEditDate': '2013-10-23T10:11:12.493', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><algorithm-analysis><time-complexity><sorting><radix-sort>', 'CreationDate': '2013-10-23T06:11:54.673', 'Id': '16350'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am interested in what sort of metrics are there that try to give a measure of how complex a given graph is, what are the corresponding algorithms, and what is their time complexity. A short description or list of relevant paper would be great.</p>\n\n<p>It might help to say that I have two graphs and I want to somehow tell which one is ``more complex." I will use this metric as a heuristic, so I would like to try various metrics on empirical data. (It might help even more if I say that those graphs represent two FSMs.)</p>\n', 'ViewCount': '88', 'Title': 'Metrics and algorithms for complexity of a graph', 'LastActivityDate': '2013-11-05T19:40:41.493', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '16518', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'Tags': '<graphs><regular-languages><time-complexity>', 'CreationDate': '2013-10-28T20:28:08.857', 'FavoriteCount': '1', 'Id': '16512'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>At the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Time_complexity" rel="nofollow">time complexity</a>, only a PRAM example is given for polylogarithmic time.</p>\n\n<p>Let $T(n)$ denote the largest number of steps used by a machine to reach a final state on any input with size $n$ bits.</p>\n\n<blockquote>\n  <p>Is there a program for a standard sequential model of computation (e.g. a Turing machine or a sequential random-access machine), solving some natural problem, so that $T(n) \\in \\Theta((\\log n)^k)$ for some fixed $k&gt;1$?</p>\n</blockquote>\n', 'ViewCount': '88', 'Title': 'standard sequential algorithm with polylog runtime?', 'LastEditorUserId': '5323', 'LastActivityDate': '2013-11-06T16:34:41.877', 'LastEditDate': '2013-11-01T15:14:16.587', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16623', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5323', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-11-01T15:03:57.973', 'Id': '16622'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm a fledgling computer science scholar, and I'm being asked to write a paper which involves integer factorization. As a result, I'm having to look into Shor's algorithm on quantum computers.</p>\n\n<p>For the other algorithms, I was able to find specific equations to calculate the number of instructions of the algorithm for a given input size (from which I could calculate the time required to calculate on a machine with a given speed). However, for Shor's algorithm, the most I can find is its complexity: <code>O( (log N)^3 )</code>.</p>\n\n<p>Is there either some way I can find its speed/actual complexity from its Big-O Notation? If not, is there someone who can tell me what I want, or how to find it?</p>\n", 'ViewCount': '161', 'Title': "Shor's Algorithm speed", 'LastActivityDate': '2013-11-04T09:54:16.450', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11162', 'Tags': '<time-complexity><quantum-computing><factoring>', 'CreationDate': '2013-11-04T00:54:24.083', 'Id': '16684'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Two matrices can be stored in either row major or column major order in contiguous memory. Does the time complexity of computing their multiplication vary depending on the storage scheme? That is, I want to know whether it will work faster if stored in row major or column major order.</p>\n', 'ViewCount': '83', 'Title': 'Does the performance of matrix multiplication depend on the storage of the array?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-12T03:18:57.387', 'LastEditDate': '2013-11-10T23:05:57.370', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8533', 'Tags': '<algorithms><time-complexity><storage><multiplication>', 'CreationDate': '2013-11-10T04:00:59.373', 'Id': '17865'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder how I can go about proving that if a language L is decidable in o(nlog(n)) then L must be regular.</p>\n\n<p>I should probably mention that by "decidable" I mean "being decidable by single-tape deterministic turing machine".  </p>\n\n<p>Thanks and regards\nGuillermo</p>\n', 'ViewCount': '68', 'Title': 'Time Complexity of Regular Languages', 'LastActivityDate': '2013-11-13T05:09:30.110', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '17978', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11348', 'Tags': '<regular-languages><time-complexity>', 'CreationDate': '2013-11-13T01:39:47.040', 'Id': '17975'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The following is my proof for $P$ being closed under union. I wish to know if my proof is correct in addition to what it means for the union of two problems.</p>\n\n<p>Proof:</p>\n\n<p>Let $p_1, p_2 \\in P$ Then by definition of $P$ $p_1$ is solvable in $O(n^k)$ for some $k\\in\\mathbb{N}$. Similarly $p_2$ is solvable in $O(n^{k_2}$) for some $k_2\\in\\mathbb{N}$.</p>\n\n<p>Then to solve $p_1 \\cup p_2$, we solve $p_1$ and $p_2$. So the total running time would be $O(n^k) + O(n^{k_2}) = O(n^{\\max(k, k_2)})$</p>\n\n<ol>\n<li>What does it mean to solve $p_1 \\cup p_2$? </li>\n<li>Is my proof correct or on the right track?</li>\n</ol>\n', 'ViewCount': '155', 'Title': 'Proving that the complexity class $P$ is closed under union', 'LastEditorUserId': '6815', 'LastActivityDate': '2013-11-19T13:40:09.620', 'LastEditDate': '2013-11-17T16:20:04.710', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Tags': '<complexity-theory><time-complexity><polynomial-time>', 'CreationDate': '2013-11-14T04:39:04.420', 'Id': '18004'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know that for every $k$-tape DTM that runs in time $O(t(n))$, there exists a 1-tape DTM that runs in $O(t^2(n))$, no matter how large the $k$ (the $k$-part is a formulation from Wikipedia). But what if $k=\\infty$?</p>\n\n<p>If I understand it right, if $k=\\infty$, then the alphabet of the $\\infty$-tape DTM is infinite and therefore the transition function will be infinite as well. That is why it is not NTM by definition, however it should be much more powerful than a DTM (based on the argument that we can encode a countable infinitely long input into one symbol of countable infinitely large alphabet).</p>\n\n<p>So, it is known if a NTM can be simulated by $\\infty$-tape DTM (with preserving polynomially-same accepting times)?</p>\n', 'ViewCount': '71', 'Title': 'Can be non-deterministic Turing machine simulated by $k$-tape deterministic TM where $k=\\infty$ with preserving polynomially-same accepting times?', 'LastActivityDate': '2013-11-14T15:19:57.320', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18016', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11380', 'Tags': '<turing-machines><time-complexity>', 'CreationDate': '2013-11-14T15:07:17.447', 'Id': '18015'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I want to prove that if P = NP, then there is a polynomial time algorithm for finding the largest clique in an undirected graph.</p>\n\n<p>I understand how to use a verifier to find this but my issue is since P = NP it doesn't want me to use a verifier. I'm not sure how to approach this.</p>\n", 'ViewCount': '167', 'Title': 'If P = NP, how do I prove I can find the maximum clique in polynomial time?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-20T12:02:06.427', 'LastEditDate': '2013-11-20T12:02:06.427', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10773', 'Tags': '<graph-theory><time-complexity>', 'CreationDate': '2013-11-20T02:37:23.993', 'Id': '18183'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '128', 'Title': 'Can Euclidean TSP be exactly solved in time better than (sym)metric TSP?', 'LastEditDate': '2013-11-20T22:51:30.667', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2755', 'FavoriteCount': '1', 'Body': u'<p>Symmetric/Metric TSP can be solved via the Held-Karp algorithm in $\\mathcal O(n^2 2^n)$.</p>\n\n<p>See <a href="http://epubs.siam.org/doi/abs/10.1137/0110015">A dynamic programming approach to sequencing problems</a> by Michael Held and Richard M. Karp, 1962.</p>\n\n<p>In <a href="http://faculty.cs.tamu.edu/chen/courses/689/2006/reading/w1.pdf">Exact Algorithms for NP-Hard Problems: A Survey (PDF)</a> Woeginger writes:</p>\n\n<blockquote>\n  <p>This result was published in 1962, and from nowadays point of view almost looks trivial. Still, it yields the best time complexity that is known today.</p>\n</blockquote>\n\n<p>Thus, this is the best known upper-bound.</p>\n\n<p><b>Question:</b></p>\n\n<blockquote>\n  <p>Are there any better results for Euclidean TSP? Or does that best-known bound apply to Euclidean TSP as well.</p>\n</blockquote>\n\n<p>How is Euclidean TSP different? Well,</p>\n\n<ul>\n<li>Euclidean TSP can be encoded into $\\mathcal O(n \\log m)$ space, where $n$ is the number of cities, and $m$ is the bound on the integer coordinates of the city locations. As opposed to (sym)metric TSP variants, which essentially require a distance matrix of size $\\mathcal O(n^2 \\log m)$. Thus, it might be easier to solve; for example, perhaps Euclidean TSP can be more easily encoded into k-SAT, because the distance function is implicit.</li>\n<li><p>Contrary to popular notion, Euclidean TSP\'s reduction from k-SAT is quite different from (sym)metric TSP. UHC (undirected Hamiltonian cycle), symmetric TSP, and metric TSP are pretty directly related to each-other. But formulations of reductions from (sym)metric TSP to Euclidean TSP are not easy to come by. Paragraph, from interesting article, <a href="http://rjlipton.wordpress.com/2012/04/22/the-travelling-salesmans-power/">The Travelling Salesman\u2019s Power</a> by K. W. Regan (bold mine):</p>\n\n<blockquote>\n  <p>Now the reductions from 3SAT to TSP, especially Euclidean TSP, are less familiar, and we ascribe this to their being far more \u201cexpansive.\u201d <b>Texts usually reduce 3SAT to Hamiltonian Cycle, then present the latter as a special case of TSP, but this does not apply to Euclidean TSP</b>. The ${\\mathsf{NP}}$-completeness of Euclidean TSP took a few years until being shown by Christos Papadimitriou, and a 1981 <a href="http://www.cs.technion.ac.il/~itai/publications/Algorithms/Hamilton-paths.pdf">paper</a> by him with Alon Itai and Jayme Luiz Szwarcfiter advertised a \u201cnew, relatively simple, proof.\u201d This proof uses vertex-induced subgraphs of the grid graph in the plane, for which the shortest possible TSP tour and any Hamiltonian cycle have the same length. Despite this simplification, the gadgets involved are large\u2014a diagram of one occupies most of one journal page.</p>\n</blockquote>\n\n<p>Hunting down k-SAT $\\rightarrow$ Euclidean TSP reductions is quite an adventure; so far I\'ve found two of them. One $\\rm k\\text{-}SAT \\rightarrow CircuitSAT \\rightarrow PlanarCircuitSAT \\rightarrow EuclideanTSP$, and another, even tougher one to find, $\\rm k\\text{-}SAT \\rightarrow DHC \\rightarrow UHC \\rightarrow PlanarUHC \\rightarrow EuclideanTSP$. The latter reduction can perhaps be seen to make Euclidean TSP parallel (sym)metric TSP.</p></li>\n</ul>\n', 'Tags': '<graph-theory><reference-request><time-complexity><np-hard><traveling-salesman>', 'LastEditorUserId': '2755', 'LastActivityDate': '2014-02-05T14:16:06.677', 'CommentCount': '0', 'AcceptedAnswerId': '18218', 'CreationDate': '2013-11-20T22:46:01.723', 'Id': '18209'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>CS sometimes seems take for granted that $\\mathcal O(\\text{poly}(n))$ is "easy", while $\\mathcal O\\left(2^{poly(n)}\\right)$ is "difficult". I am interested in research into "difficult" polynomial-time algorithms, where the best-case solution to the constructed problem runs in $\\Theta(n^c)$, where $c$ can be chosen to be large; but the solution could be tested in $O(n)$ time.</p>\n\n<p><b>Question:</b></p>\n\n<blockquote>\n  <p>Given an integer $c$, can we construct problems that would:</p>\n  \n  <ul>\n  <li>Take $\\Theta\\left(n^c\\right)$ best-case-time to solve,</li>\n  <li>While taking $\\tilde{\\mathcal O}(n)$ time, and $\\tilde{\\mathcal O}(n)$ space, to test a solution?</li>\n  </ul>\n</blockquote>\n\n<p>($\\tilde{\\mathcal O}(n)$ is <a href="https://en.wikipedia.org/wiki/Big_O_notation#Extensions_to_the_Bachmann.E2.80.93Landau_notations" rel="nofollow">soft-big-oh</a>, meaning $O(n \\log^k n)$ for some $k$)</p>\n\n<hr>\n\n<p>Something I note - I might be mistaken somewhere here - is that presumably, if there is a $\\mathcal O(n)$ algorithm to test the solution, then perhaps there can be a $\\mathcal O(n)$ reduction to $\\rm k\\text{-}SAT$. If so, and, if $\\rm P=NP$, and there was a polynomial-time algorithm, say ${\\rm S{\\small OLVE}}\\left(\\Phi(\\mathbf x)\\right) \\in O({|\\mathbf x|}^{\\alpha})$ time, then I think this might contradict our $\\Theta(n^c)$ problem, if $\\alpha &lt; c$.</p>\n\n<hr>\n\n<p>The motivation would be to research the possibility of having a "one-way-function", that is linear(ithmic)-time computable, and best-case "difficult"-polynomial-time invert-able, where "difficult" means a high degree polynomial, instead of the usual exponential-time definition of "difficult"; perhaps this might be able to be used for cryptography even if $\\rm P=NP$ (like "post-P-equals-NP-cryptography", similar to how there is a field of "post-quantum-cryptography").</p>\n', 'ViewCount': '156', 'Title': 'Can we construct problems that can be solved in $\\Theta(n^c)$ time, and tested in $O(n)$ time', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-12-10T15:31:18.290', 'LastEditDate': '2013-12-10T15:31:18.290', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><reference-request><time-complexity><algorithm-synthesis>', 'CreationDate': '2013-11-21T14:40:26.493', 'FavoriteCount': '3', 'Id': '18223'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have come across many sorting algorithms during my high school studies. However, I never know which is the fastest (for a random array of integers). So my questions are:</p>\n\n<ul>\n<li>Which is the fastest currently known sorting algorithm?</li>\n<li>Theoretically, is it possible that there are even faster ones? So, what's the least complexity for sorting?</li>\n</ul>\n", 'ViewCount': '2193', 'Title': 'What is a the fastest sorting algorithm for an array of integers?', 'LastActivityDate': '2013-12-03T19:50:58.720', 'AnswerCount': '4', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8870', 'Tags': '<algorithms><time-complexity><optimization><sorting>', 'CreationDate': '2013-12-02T16:15:25.337', 'Id': '18536'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When I was studying Comp Sci, we had <a href="http://rads.stackoverflow.com/amzn/click/0716710455" rel="nofollow">Garey &amp; Johnson</a> as a course textbook, with a large collection of NP-Complete problems. But by that time you could also have a look at the <a href="http://www.ensta-paristech.fr/~diam/ro/online/viggo_wwwcompendium/wwwcompendium.html" rel="nofollow">Compendium of NP Optimization Problems</a>, online.</p>\n\n<p>However, it seems the \'Compendium\' site has not seen any updates in several years. Is that indeed the case? Is there a more up-to-date compendium (perhaps in print) which accounts for further research and contains more problems in more domains?</p>\n', 'ViewCount': '45', 'Title': "Is there a more up-to-date / wider-scope version of the 'Compendium of NP Optimization Problems'", 'LastEditorUserId': '11796', 'LastActivityDate': '2013-12-14T12:47:16.403', 'LastEditDate': '2013-12-14T12:47:16.403', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11796', 'Tags': '<complexity-theory><time-complexity><optimization><np>', 'CreationDate': '2013-12-03T22:00:16.900', 'FavoriteCount': '1', 'Id': '18584'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a multi-label classification problem, in which each input sample has a set of zero or more output labels.</p>\n\n<p>I have a multi-class classifier which, for every input sample, returns a certain score for each of the output labels.</p>\n\n<p>One way to use this multi-class classifier for multi-label classification is to calculate a <strong>threshold score</strong>, and select for the output all labels that get a score above the threshold.</p>\n\n<p>My question is: <strong>what is the most efficient way to calculate the threshold score?</strong></p>\n\n<p>Currently we use the following scheme:</p>\n\n<ul>\n<li>Train the multi-class classifier on 90% of the training set.</li>\n<li>Run the base multi-class classifier on the remaining 10% of the training set (which we call "development set"). Keep all the scores in a large array.</li>\n<li>Create a set of all scores that are returned by the base multi-class classifier for any input and any label. Call the set $S$.</li>\n<li>For every score $s \\in S$, and for every input sample in the development set, calculate the positive labels returned when the threshold is $s$. Compare to the gold-standard. Calculate the precision, recall and $F1$.</li>\n<li>Select the score $s_max$ that yielded the maximum $F1$.</li>\n</ul>\n\n<p>This is not efficient because there are many different scores and the method is linear in the number of different scores.</p>\n\n<p>We plotted the $F1$ against the scores and noticed that this is a concave function - it has a single maximum. So, we thought of using binary search to find the maximum point. But, we are not sure that the function will always be concave.</p>\n\n<p>What advice do you have for making this process more efficient?</p>\n', 'ViewCount': '39', 'Title': 'Calculating the classification threshold efficiently', 'LastActivityDate': '2013-12-04T10:16:54.473', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<time-complexity><machine-learning><efficiency>', 'CreationDate': '2013-12-04T10:16:54.473', 'Id': '18602'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a rough idea that the time complexity is O(mn) where m and n are based on the length of signals, as per this <a href="http://en.wikipedia.org/wiki/Dynamic_time_warping" rel="nofollow">http://en.wikipedia.org/wiki/Dynamic_time_warping</a>. </p>\n\n<p>How can I determine the time complexity of the following code (<a href="http://www.mathworks.com/matlabcentral/fileexchange/16350-continuous-dynamic-time-warping" rel="nofollow">source</a>)</p>\n\n<pre><code>function [Dist,D,k,w,rw,tw]=dtw3(t,r)\n%\n% [Dist,D,k,w,rw,tw]=dtw(r,t,pflag)\n%\n% Dynamic Time Warping Algorithm\n% Dist is unnormalized distance between t and r\n% D is the accumulated distance matrix\n% k is the normalizing factor\n% w is the optimal path\n% t is the vector you are testing against\n% r is the vector you are testing\n% rw is the warped r vector\n% tw is the warped t vector\n\n\n[row,M]=size(r); \nif (row &gt; M) \n     M=row; \n     r=r\'; \nend;\n[row,N]=size(t); \nif (row &gt; N) \n     N=row; \n     t=t\'; \nend;\n\n\nd=((repmat(r\',1,N)-repmat(t,M,1)).^2);\n\nD=zeros(size(d));\nD(1,1)=d(1,1);\n\nfor m=2:M\n    D(m,1)=d(m,1)+D(m-1,1);\nend\nfor n=2:N\n    D(1,n)=d(1,n)+D(1,n-1);\nend\nfor m=2:M\n    for n=2:N\n        D(m,n)=d(m,n)+min(D(m-1,n),min(D(m-1,n-1),D(m,n-1))); \n    end\nend\n\nDist=D(M,N);\nn=N;\nm=M;\nk=1;\nw=[M N];\nwhile ((n+m)~=2)\n    if (n-1)==0\n        m=m-1;\n    elseif (m-1)==0\n        n=n-1;\n    else \n      [values,number]=min([D(m-1,n),D(m,n-1),D(m-1,n-1)]);\n      switch number\n      case 1\n        m=m-1;\n      case 2\n        n=n-1;\n      case 3\n        m=m-1;\n        n=n-1;\n      end\n    end\n    k=k+1;\n    w=[m n; w]; \nend\n\n% warped waves\nrw=r(w(:,1));\ntw=t(w(:,2));\nend\n</code></pre>\n', 'ViewCount': '87', 'Title': 'How can I analyze the time complexity of this Dynamic Time Warping algorithm implemented in MATLAB?', 'LastActivityDate': '2013-12-08T07:54:28.550', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18733', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11922', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-12-08T06:33:52.807', 'Id': '18732'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I been reading Wikipedia for <a href="http://en.wikipedia.org/wiki/Polynomial_time#Polynomial_time" rel="nofollow">polynomial time</a>, and it says:</p>\n\n<blockquote>\n  <p>An algorithm is said to be of polynomial time if its running time is\n  upper bounded by a <a href="http://en.wikipedia.org/wiki/Polynomial_expression" rel="nofollow">polynomial expression</a> in the size of the input for\n  the algorithm, i.e., $T(n) = O(n^k)$ for some constant $k$.</p>\n</blockquote>\n\n<p>My questions are:</p>\n\n<ol>\n<li>Why do we have addition and multiplication but no division?</li>\n<li>Why do we have both multiplication and addition, couldn\'t we express one with the other?</li>\n<li>What is the difference between polynomial time and <a href="http://en.wikipedia.org/wiki/Polynomial_time#Exponential_time" rel="nofollow">exponential time</a> by definition?</li>\n</ol>\n', 'ViewCount': '75', 'Title': 'Questions on the definition of polynomial time', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-14T20:14:17.063', 'LastEditDate': '2013-12-14T20:14:17.063', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '18745', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<time-complexity>', 'CreationDate': '2013-12-08T08:19:42.453', 'Id': '18734'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Wiki define <a href="http://en.wikipedia.org/wiki/Polynomial_time#Polynomial_time" rel="nofollow">Polynomial time</a> as fallow:</p>\n\n<blockquote>\n  <p>An algorithm is said to be of polynomial time if its running time is\n  upper bounded by a <a href="http://en.wikipedia.org/wiki/Polynomial_expression" rel="nofollow">polynomial expression</a> in the size of the input for\n  the algorithm, i.e., $T(n) = O(n^k)$ for some constant $k$</p>\n</blockquote>\n\n<p>I understand that in general speaking the difference between <strong>Polynomial time</strong> and <a href="http://en.wikipedia.org/wiki/Polynomial_time#Exponential_time" rel="nofollow">Exponential time</a> is that exponential function grows strictly faster than any polynomial function, asymptotically(<a href="http://cs.stackexchange.com/a/18743/10572">reference</a>).</p>\n\n<p>I am trying to understand the core definition of <strong>Exponential time</strong>.</p>\n\n<ol>\n<li>What elements will make one algorithm to run in <strong>Exponential time</strong>?</li>\n<li>What change do I need to do in the <strong>polynomial expression</strong> to make it <strong>Exponential time</strong>?(By <em>it</em> I am referring to the algorithm definition in the beginning of the question)</li>\n</ol>\n', 'ViewCount': '140', 'Title': 'Exponential time algorithms', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-24T15:16:50.513', 'LastEditDate': '2013-12-08T13:33:00.860', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '18746', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<time-complexity>', 'CreationDate': '2013-12-08T13:16:06.450', 'Id': '18744'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is proven by <a href="https://en.wikipedia.org/wiki/Time_hierarchy_theorem" rel="nofollow">Time hierarchy theorem</a> that <a href="https://en.wikipedia.org/wiki/EXPTIME-complete" rel="nofollow">EXPTIME-complete</a> do not belong to P even if we do not know if NP belong to P.</p>\n\n<p>Before I read wiki, I found EXPTIME algorithm for solving 3-sat, the brute algorithm $O(2^n)$. Could I announced then, that there could not be polynomial solution for 3-sat?</p>\n\n<ol>\n<li><p><strong>Time hierarchy theorem</strong> prove that there are EXPTIME problems that are not in P. But how can you prove that problem is EXPTIME and not super-polynomial time?</p></li>\n<li><p>Also please tell me what is the deference between super-polynomial and polynomial times in context of <a href="https://en.wikipedia.org/wiki/Polynomial_expression" rel="nofollow">polynomial expression</a>.</p></li>\n</ol>\n\n<p>*If you have solution for just one of the questions it\'s fine.</p>\n', 'ViewCount': '68', 'Title': 'EXPTIME vs Super-polynomial time', 'LastActivityDate': '2013-12-08T15:56:22.263', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18752', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<time-complexity>', 'CreationDate': '2013-12-08T14:58:13.397', 'Id': '18750'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am wondering if given the time complexity of an NP-Complete problem or at least some information about it for example if $ SAT\\in Time(2^{sqrt(n)})$ (hypothetically) could I assume that all languages in NP (which are clearly polynomial time reducible to SAT) are also $\\in Time(2^{sqrt(n)})$</p>\n\n<p>I believe the answer is false because I could basically pick any arbitrary class of exponential time functions and claim that all languages in NP are contained within it while it may actually belong to a class of higher power... but I'm not sure how to formulate this as a proof.</p>\n", 'ViewCount': '96', 'Title': 'Time complexity of languages that are polynomial time reducible to NP complete languages', 'LastActivityDate': '2013-12-09T02:56:43.687', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18768', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11306', 'Tags': '<time-complexity><np-complete><polynomial-time>', 'CreationDate': '2013-12-09T02:24:19.463', 'FavoriteCount': '1', 'Id': '18767'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a collection of $n$ numbers, $S$, the question is to decide whether all the elements of $S$ are distinct from each other. If they are distinct from each other (no two of them are the same), print "Yes". Otherwise print "No".</p>\n\n<p>I know the worst case time complexity of this question is $\\Theta \\left ( n\\log n \\right )$. Of course it is based on comparison among elements. But I can\'t figure out how to prove this. Perhaps using the decision tree?</p>\n', 'ViewCount': '66', 'Title': 'About the complexity of deciding whether no two elements of a collection are the same', 'LastEditorUserId': '11975', 'LastActivityDate': '2013-12-10T10:45:57.403', 'LastEditDate': '2013-12-10T10:45:57.403', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '18818', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11975', 'Tags': '<algorithm-analysis><time-complexity>', 'CreationDate': '2013-12-10T01:41:12.900', 'Id': '18807'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a question asking about a language L with the property: there is a TM that decides L in time O(n^2013 / (log(n))^2012), and if there is a TM that decides L in time O(n^2012.9).</p>\n\n<p>My confusion comes from the first big O given, from what I understand the numerator would dominate as the TM grows towards infinity, so it would end up being O(n^2013), which would grow faster then O(n^2012.9), so there could not be a TM that decides L in time O)n^2012.9). But I'm not sure how to go about proving this. Is there some theorem that let's you compare O(n^2/log(n)) with O(n) or something to that extent?</p>\n", 'ViewCount': '42', 'ClosedDate': '2013-12-10T09:00:33.683', 'Title': 'Big O confusion', 'LastActivityDate': '2013-12-10T03:30:33.863', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11300', 'Tags': '<complexity-theory><time-complexity><landau-notation>', 'CreationDate': '2013-12-10T01:44:42.587', 'FavoriteCount': '1', 'Id': '18808'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am having an extremely tough time with this homework question, wondering if anyone could help me (for all you theory aficionados, this one's for you).</p>\n\n<p><em>There is a language $L$ with the following property: There is a Turing machine that decides $L$ in time $O(n^{2013}/\\log^{2012} n)$, but there is no Turing machine that decides $L$ in time $O(n^{2012.9})$.</em> Is this statement TRUE or FALSE?</p>\n\n<p>My notion/idea is to use this Corollary that stems from the Time Hierarchy Theorem: <em>For any two real numbers $1 \\le e_1 &lt; e_2$, we have $\\mathrm{TIME}(n^{e_1}) \\subsetneq \\mathrm{TIME}(n^{e_2})$</em>. I think I am on the right track but I need some help with this proof.</p>\n", 'ViewCount': '141', 'Title': 'Are there problems with complexity between $O(n^c/\\log^b n)$ and $O(n^{c - \\varepsilon})$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-10T09:37:20.407', 'LastEditDate': '2013-12-10T07:58:18.613', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '18816', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11979', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2013-12-10T03:12:31.800', 'Id': '18810'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>While thinking about different calculi for predicate logic (like natural deduction and sequent calculus), I noticed that these calculi are (often) presented in a form suitable for "human computers". A "human computer" is limited to use <a href="http://en.wikipedia.org/wiki/Write_once_read_many" rel="nofollow">write once read many</a> (WORM) memory when processing large amounts of data. Pure functional programming also seems to favor a WORM memory model. In fact, it seems to me that the WORM memory model is so natural that classifying it as <a href="http://en.wikipedia.org/wiki/Unconventional_computing" rel="nofollow">unconventional computing</a> might underestimate its importance. (Understanding the strengths and limitations of the computing resources available to humans is important.)</p>\n\n<p>What is known about the relation between space and time complexity for machines with WORM memory? What are the keywords to google for available material related to these questions? Do we known whether the time complexity will remain the same, if a small (for example C*log(WORM memory)^n) amount of normal memory is added?</p>\n', 'ViewCount': '91', 'Title': 'Relation between space and time complexity for machines with write once read many (WORM) memory', 'LastEditorUserId': '1557', 'LastActivityDate': '2014-03-03T00:13:28.570', 'LastEditDate': '2013-12-13T09:09:01.290', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22210', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1557', 'Tags': '<time-complexity><runtime-analysis><space-complexity>', 'CreationDate': '2013-12-13T00:04:24.253', 'Id': '18939'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m learning about asymptotic analysis, and have seen some exotic looking complexities living between other common ones. For instance "log log n" is strictly between 1 and log n. It makes me wonder if one can always find complexities between any other two.</p>\n\n<p>Specifically, for any functions f and g with O(f) \u2282 O(g) does there always exist an h such that O(f) \u2282 O(h) \u2282 O(g)?</p>\n\n<p>This isn\'t homework or anything. I\'m just curious if anyone knows.</p>\n', 'ViewCount': '106', 'Title': 'Is there always a Big Oh complexity strictly between any two others?', 'LastActivityDate': '2013-12-14T20:58:16.143', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18994', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '12119', 'Tags': '<complexity-theory><time-complexity><asymptotics>', 'CreationDate': '2013-12-14T20:02:02.963', 'FavoriteCount': '1', 'Id': '18993'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Relevant Information:<br>\n<a href="http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes" rel="nofollow">Sieve of Eratosthenes</a><br>\n<a href="http://en.wikipedia.org/wiki/Sieve_of_Sundaram" rel="nofollow">Sieve of Sundaram</a></p>\n\n<p>Suppose I want to generate all primes in <code>[2,n]</code>, and I have both of these algorithms at my disposal to get the job done. Which is preferable under what conditions?</p>\n\n<p>I read that Sundaram runs in O(n log n) time, whereas Eratosthenes runs in O(n log log n) time, so it seems that Eratosthenes is preferable. However, that is just a very superficial evaluation. Are there other factors (aside from ease of implementation) to be considered? Which is the \'better\' algorithm?</p>\n', 'ViewCount': '81', 'Title': 'Sieve of Eratosthenes vs. Sieve of Sundaram', 'LastActivityDate': '2013-12-19T10:41:54.903', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11093', 'Tags': '<algorithms><time-complexity><primes>', 'CreationDate': '2013-12-19T10:41:54.903', 'Id': '19115'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to show that the language Anfa = {(A,w)| A is an nondeterministic finite automata that accepts w} can be decided in polynomial time. My problem is every solution that I think of requires exponential time.</p>\n\n<p>I would appreciate any help, Thanks in advance..</p>\n', 'ViewCount': '115', 'Title': 'how to solve NFA acceptance problem in polynomial time', 'LastActivityDate': '2013-12-19T17:38:29.810', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12242', 'Tags': '<time-complexity><finite-automata><nondeterminism><polynomial-time>', 'CreationDate': '2013-12-19T17:12:44.090', 'Id': '19126'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The cube root of a natural number n is defined as the largest natural number m such that m^3\u2264n. The complexity of computing the cube root of n (n is represented in binary notation) is</p>\n\n<p>(A) O(n) but not O(n^0.5)</p>\n\n<p>(B) O(n^0.5) but not O((log n)^k) for any constant k > 0</p>\n\n<p>(C) O((log n)^k) for some constant k > 0, but not O((log log n)^m) for any constant m > 0</p>\n\n<p>(D) O((log log n)^k) for some constant k > 0.5, but not O((log log n)^0.5)</p>\n\n<p>I am lost solving this previous year problem .Can any one help me to understand this question</p>\n', 'ViewCount': '118', 'ClosedDate': '2014-01-05T17:25:55.000', 'Title': 'Complexity to find cube root of n', 'LastActivityDate': '2013-12-30T20:34:33.803', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'avi', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<asymptotics><time-complexity>', 'CreationDate': '2013-12-24T23:20:17.427', 'Id': '19361'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am reading a unpublished paper describing an algorithm. In one step of the algorithm, there is a bipartite graph $G(X,Y,E)$, where $X=\\{1,...,n\\}$.</p>\n\n<p>For every subset $X' \\subseteq X$, they define</p>\n\n<p>$$f(X')=\\bigcup_{i \\in X'} \\{j \\in Y| (i,j)\\in E\\}.$$</p>\n\n<p>In other words, $f(X')$ is the set of neighbors of vertices in $X'$.</p>\n\n<p>Then they define:</p>\n\n<p>$$ X^+ = \\arg \\max_{X' \\subseteq \\{2,3,...,n\\}\\ s.t.\\ |X'|\\geq|f(X')|}\\{|X'|\\}$$</p>\n\n<p>i.e., $X^+$ is a largest subset of $\\{2,3,...,n\\}$ such that $X^+$ is at least as large as the set of its neighbors in $Y$ (i.e., the largest subset such that $|X^+| \\ge |f(X^+)|$).</p>\n\n<p>And then they do some stuff with this $X^+$.</p>\n\n<p>MY QUESTION IS: Can this $X^+$ be found in polynomial time? </p>\n\n<p>The authors do not prove that it can, but this is implied by the paper (otherwise the algorithm itself cannot be polynomial). Maybe it is so obvious that only I haven't seen this?</p>\n\n<p>EDIT: The following similar problem can be solved easily:</p>\n\n<pre><code>Find the largest set of vertices in X with less than |Y| neighbours.\n</code></pre>\n\n<p>Solution: find a $y \\in Y$ with a minimal number of neighbours in $X$. Return the set that includes all vertices in $X$ except $y$'s neighbours.</p>\n\n<p>What about the following similar problem?</p>\n\n<pre><code>Find the largest set of vertices in X with less than |Y|/2 neighbours.\n</code></pre>\n", 'ViewCount': '126', 'Title': 'Largest set of vertices that is larger than its set of neighbors', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-01-01T15:40:34.427', 'LastEditDate': '2014-01-01T15:40:34.427', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><graph-theory><time-complexity><runtime-analysis>', 'CreationDate': '2013-12-30T14:31:23.103', 'Id': '19377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a digraph $D = (V, A)$ and $m \\in \\mathbb{N}$, the question is is there a subset $A' \\subseteq A$, such that $\\lvert A' \\rvert \\geq m$ and $d_{D'}^+(u) \\leq d_{D'}^-(v)$ holds for every arc $(u, v) \\in A'$ in the subgraph $D' = (V, A')$, i.e. the out-degree of $u$ is not larger than the in-degree of $v$? Note that the degree constraints should hold in the subgraph $D'$.</p>\n\n<p>This seems like a straight-forward problem, alas I am unable to connect it to some more familiar graph problem. I am mostly interested in its complexity.</p>\n", 'ViewCount': '85', 'Title': 'Digraph problem relating in- and out-degrees', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-04T04:46:54.440', 'LastEditDate': '2014-01-02T15:51:52.913', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12522', 'Tags': '<graphs><time-complexity><decision-problem>', 'CreationDate': '2014-01-01T23:59:45.053', 'Id': '19443'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>The following definitions are from Li, M., &amp; Vit\xe1nyi, P. (1997). An introduction to Kolmogorov complexity and its applications (2nd ed.), pg. 38.</p>\n\n<blockquote>\n  <p>A language $A$ is called <em>polynomial time Turing-reducible</em> to a language $B$, denoted as $A\\leq_T^P B$, if given $B$ as an <em>oracle</em>, there is a deterministic Turing machine that accepts $A$ in polynomial time. That is, we can accept $A$ in polynomial time given answers to membership of $B$ for free.</p>\n  \n  <p>A language $A$ is called <em>polynomial time many-to-one reducible</em> to a language $B$, denoted as $A\\leq_m^P B$, if there is a function $r$ that is a polynomial time computable, and for every $a$, $a\\in A$ iff $r(a)\\in B$. In both cases, if $B\\in P$, then so is $A$.</p>\n</blockquote>\n\n<p>Aren't the two definitions equivalent? What's the difference?</p>\n", 'ViewCount': '41', 'ClosedDate': '2014-02-07T06:13:16.147', 'Title': 'What\'s the difference between "polynomial time Turing-reducible" and "polynomial time many-to-one reducible"?', 'LastActivityDate': '2014-01-13T15:49:05.680', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1474', 'Tags': '<complexity-theory><time-complexity><polynomial-time>', 'CreationDate': '2014-01-13T15:22:35.997', 'Id': '19694'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have recently summarized several algorithms for the <a href="https://en.wikipedia.org/wiki/Maximum_disjoint_set" rel="nofollow">maximum disjoint set</a> problem. This problem is NP-hard, but it has both PTAS and sub-exponential algorithms. These algorithms seem to me closely related. The details vary, but the general idea is:</p>\n\n<ul>\n<li>For a PTAS: remove a small fraction of the input set (e.g. $O(\\sqrt{n})$ shapes). Partition the remaining input set to two subsets. Recursively find an approximate maximum set on each subset, and return the union of these two sets as the approximate solution.</li>\n<li>For a sub-exponential algorithm: instead of removing those $O(\\sqrt{n})$ shapes, check all possible subsets of them. For each subset, do the recursive step as in the PTAS. Return the best solution found. The runtime is dominated by the number of all possible subsets, which is $O(2^\\sqrt{n})$.</li>\n</ul>\n\n<p>Initially I thought that maybe every problem with a PTAS has a sub-exponential exact algorithm, but I haven\'t found such a relation so I assume it is not true. My questions are therefore:</p>\n\n<ul>\n<li>Are there problems with PTAS but provably no subexponential algorithms (no algorithms with runtime $O(2^{n^e})$ with $e&lt;1$)?</li>\n<li>Are there problems with subexponential algorithms but provably no PTAS?</li>\n</ul>\n', 'ViewCount': '27', 'Title': 'PTAS vs. exact-time sub-exponential algorithms', 'LastActivityDate': '2014-01-21T17:05:37.750', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<complexity-theory><time-complexity><np-complete>', 'CreationDate': '2014-01-21T17:05:37.750', 'Id': '19871'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I am trying to create a polynomial time algorithm for a problem defined as follows:</p>\n\n<blockquote>\n  <h3>c-ZPath(cZP)</h3>\n  \n  <p>$c$ is an integer constant $\\geq 1$ </p>\n  \n  <p><strong>Input:</strong> An undirected graph $G=(V,E)$. </p>\n  \n  <p><strong>Question:</strong> Can the vertices in $G$ be colored with two colors such that</p>\n  \n  <ol>\n  <li><p>no edge\u2019s endpoint vertices have the same color and</p></li>\n  <li><p>there is a path in this colored version of $G$ with $\\geq c$ edges in which no vertex or edge repeats and the vertex-colors alternate for the entire length of the path? </p></li>\n  </ol>\n</blockquote>\n\n<p>I understand that the coloring can be checked by a simple breadth first search in polynomial time. </p>\n\n<p>My problem is with the path of length $c$. My professor stated that the reason that this is not NP-complete and analogous to the longest path problem is because $c$ is a constant. I fail to see why this restriction causes it to differ from longest path. If anyone could clarify this for me I'd be really greatful. </p>\n", 'ViewCount': '22', 'Title': 'Restricting longest path with 2-coloring to paths of at most constant length', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T20:34:41.350', 'LastEditDate': '2014-01-29T20:34:41.350', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '20082', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13230', 'Tags': '<complexity-theory><time-complexity><np-complete>', 'CreationDate': '2014-01-29T19:57:44.057', 'Id': '20081'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider this example: a problem of dimension $n$ and $m$ ($m,n$: any given integers).\nhas a search space of size $O(n^n * m^n)$. \nIt is clear that this problem is exponential in $n$,\nwhatsoever $m$ may be.\nMy question: is this same problem polynomial in $m$? \nwhat are the assumptions if we can say that?\nis this way of complexity analysis correct? </p>\n', 'ViewCount': '26', 'Title': 'How to analyse the complexity of a problem with two or more size measures', 'LastEditorUserId': '13107', 'LastActivityDate': '2014-02-07T23:26:10.240', 'LastEditDate': '2014-02-07T23:26:10.240', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14490', 'Tags': '<complexity-theory><time-complexity><asymptotics>', 'CreationDate': '2014-02-07T22:47:41.883', 'FavoriteCount': '1', 'Id': '21435'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>$$t(n)=\\begin{cases}n&amp;\\text{if }n=0,1,2,\\text{ or }3\\\\t(n-1)+t(n-3)-t(n-4)&amp;\\text{otherwise.}\\end{cases} $$\nExpress your answer as simply using the theta notation.</p>\n\n<p>I don't know where to go with this.</p>\n\n<p>$$t(n) - t(n-1) - t(n-3) + t(n-4) = 0$$</p>\n\n<p>Is the characteristic polynomial $x^3 - x^2 - x +1 = 0$?</p>\n", 'ViewCount': '18', 'ClosedDate': '2014-02-09T22:41:59.900', 'Title': 'Recurrence relation help?', 'LastEditorUserId': '14527', 'LastActivityDate': '2014-02-10T17:43:12.060', 'LastEditDate': '2014-02-10T17:43:12.060', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14527', 'Tags': '<algorithm-analysis><time-complexity>', 'CreationDate': '2014-02-09T22:26:41.967', 'Id': '21476'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>T(n)=1</p>\n\n<p>T(n)={ 2T(n/2) +lgn</p>\n\n<p>A=2</p>\n\n<p>B=2</p>\n\n<p>F(n)=lgn</p>\n\n<p>So doing so f(n) is an element of big theta of (nlg^2^2 * log ^k n) for k >= 0.</p>\n\n<p>(2n * log^k n). I'm unsure if I'm headed in the right direction but that is what I've tried so far?</p>\n", 'ViewCount': '6', 'ClosedDate': '2014-02-10T21:56:32.327', 'Title': 'Applying the master theorem? Recurrence relation', 'LastActivityDate': '2014-02-10T20:50:28.717', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14527', 'Tags': '<time-complexity>', 'CreationDate': '2014-02-10T20:50:28.717', 'Id': '21504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>EDIT</strong></p>\n\n<p>As requested, a single question </p>\n\n<p><strong>Why can\'t arbitrary base conversion be done as fast as converting from base $b$ to base $b^k$ ?</strong> </p>\n\n<p>There is a big time complexity difference, so I am also interested in <em>further reading material about it</em>.</p>\n\n<hr>\n\n<p><strong>Old. Original question</strong></p>\n\n<p>Conversion between power-2-radix can be done faster than between non-power-of-2 radix, they can be even done in parallel, as every digit (or some groups of them) can be decoded independently of the rest.</p>\n\n<p>For example the binary number <code>00101001</code> can be converted to hexadecimal <code>0x29</code> nibble by nibble (<code>0010</code> and <code>1001</code>), and vice versa (i.e. every hex-digit can be parsed to 4 bits independently of the rest), but doing that conversion from to decimal (or any other non-power-of-2 radix) it\'s not so easy because digits affects each other.</p>\n\n<p>I\'ve seen time complexity of math operations in <a href="http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions" rel="nofollow">wikipedia</a>, and there is also a related question in <a href="http://stackoverflow.com/questions/17649524/time-complexity-to-convert-a-decimal-to-another-base">stackoverflow</a> saying time complexity of conversions of arbitrary digit length to be $\\mathcal{O}(M(n) log(n))$</p>\n\n<p>I\'m not interested in a "general time complexity bounds for any base conversion" but I would like to know more about the big differences in time complexity between power-of-2 conversions vs any other base conversions. </p>\n\n<p>It\'s could be a general fact about conversions that can be done faster if they are done between numbers where its bases are power among themselves, not only for 2, but the same to a base 10 to base 100.</p>\n\n<p>Is there any known proof or materials around this ?</p>\n', 'ViewCount': '185', 'Title': 'Time complexity of base conversion', 'LastEditorUserId': '1396', 'LastActivityDate': '2014-04-18T19:19:26.113', 'LastEditDate': '2014-02-21T12:31:12.923', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1396', 'Tags': '<complexity-theory><reference-request><time-complexity><binary-arithmetic>', 'CreationDate': '2014-02-17T13:58:42.143', 'FavoriteCount': '1', 'Id': '21736'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '37', 'Title': 'Recurrence relation in 2 variables', 'LastEditDate': '2014-02-20T13:39:25.047', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1100', 'FavoriteCount': '1', 'Body': '<p>When analyzing an algorithm, the following recurrence relation popped up:</p>\n\n<p>$T(n,d)=2T(n/2,d)+T(n,d-1)+O(dn)$</p>\n\n<p>where $T(n,1)=O(n \\log{n})$ and $T(1,d)=O(d)$.</p>\n\n<p>By applying the Master Theorem inductively, for any particular $d$, it holds that $T(n,d)=O(n (\\log{n})^d)$. However, it does not necessarily hold that $T(n,d)=O(n (\\log{n})^d)$ because the constant hidden by the $O$-notation depends on the value of $d$.</p>\n\n<p>I was hoping that the technically incorrect bound given by repeated application of the master theorem would be good enough. It turns out that this is actually a terrible, terrible bound. The actual values of $T(n,d)$ are orders of magnitude lower from what the asymptotic bound would predict. Does anyone know how to get a better bound?</p>\n', 'Tags': '<time-complexity><asymptotics><recurrence-relation>', 'LastEditorUserId': '14720', 'LastActivityDate': '2014-02-20T13:39:25.047', 'CommentCount': '3', 'AcceptedAnswerId': '21817', 'CreationDate': '2014-02-19T14:17:23.563', 'Id': '21812'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '64', 'Title': 'Time complexity of 8-queen, by placeing one by one without attack', 'LastEditDate': '2014-02-22T09:23:17.930', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14922', 'FavoriteCount': '1', 'Body': '<p>I am new to artificial intelligence. I have been trying to analyse the time complexity of 8-queen, by placing one by one without attack.</p>\n\n<p>One approach to achieve goal state is to "add a queen to any square in the leftmost empty column such that it is not attacked by any other queen". And this approach will have a state space of 2057 (also wondering: How to compute this?)</p>\n\n<p>What is the time complexity if I am using Depth First search algorithm (which I think is the most suitable one)? How about the space complexity?</p>\n\n<p>I am puzzled because the brunching of the search tree is reducing greatly when goes deep. $O(8^8)$ looks too much for time complexity, even for worst case.</p>\n', 'Tags': '<time-complexity><search-problem><board-games>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-22T09:23:17.930', 'CommentCount': '2', 'AcceptedAnswerId': '21908', 'CreationDate': '2014-02-21T20:05:40.270', 'Id': '21905'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is known that a nondeterministic universal turing machine (UTM) can simulate another nondeterministic TM with running time $t(n)$ in time $c t(n)$, where $c$ is a constant. It is also known that a deterministic UTM can simulate another deterministic TM with running time $t(n)$ in time $ t(n)\\log(t(n))$. </p>\n\n<p>My question is: why is there a $\\log(t(n))$ slowdown in the simulation of a deterministic TM by a UTM, as opposed to a constant factor slowdown in the nondeterministic case?</p>\n', 'ViewCount': '164', 'Title': 'Difference between deterministic and nondeterministic universal turing machine', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-23T20:36:58.043', 'LastEditDate': '2014-02-23T20:36:58.043', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14961', 'Tags': '<complexity-theory><turing-machines><time-complexity><simulation>', 'CreationDate': '2014-02-23T17:58:30.433', 'Id': '21949'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I read that determining the size of the maximum independent set (and also a clique of maximum size) is in P. The versions that find the actual solution are known to be NP-hard. </p>\n\n<p>With respect to finding clique size, you can sort the node degrees, decrement $i$ from $|V|$ to $0$, and each time check if you have $i$ elements of node degree $i$, pick the power set of those $\\geq i$ elements and verify the clique. However, picking the power set is exponential, and this algorithm would give you the solution itself. I have a hard time figuring out how you can construct an algorithm that decides the presence of a clique (or independent set) of a certain size in polytime, but doesn't give you the solution.</p>\n", 'ViewCount': '101', 'Title': 'Why is determining the size of a maximum independent set or a clique in P?', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-27T07:22:20.957', 'LastEditDate': '2014-02-27T07:22:20.957', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22083', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<algorithms><complexity-theory><graph-theory><graphs><time-complexity>', 'CreationDate': '2014-02-27T05:41:09.613', 'Id': '22080'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m looking for some clarification on some concepts/facts I came across while studying for a class.</p>\n\n<p>I was reading the following wikipedia article. The below specific section and statement intrigued me when looking it over.</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Computational_complexity_theory#Important_complexity_classes" rel="nofollow">http://en.wikipedia.org/wiki/Computational_complexity_theory#Important_complexity_classes</a>\n"It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch\'s theorem"</p>\n\n<p>I also read that NTMs can be simulated by DTMs but that the shortest accepting computation of the DTM is exponential with respect to the shortest accepting computation of the target NTM.</p>\n\n<p><strong>My questions are:</strong></p>\n\n<p>1.) Are PSPACE and NPSPACE the set of all problems that require at least polynomial space to be solved on Deterministic and Non-deterministic Turing machines respectively?</p>\n\n<p>2.) If so, is the actual size of the polynomial space required dependent on the size of the input?</p>\n\n<p>3.) For P and NP, they are each the sets of problems that require at least polynomial time to be solved on DTMs and NTMs respectively correct?</p>\n\n<p>4.) Is the reason that the shortest accepting computation of a DTM simulating a target NTM is exponential with respect to the shortest accepting computation of an NTM due to the exponential explosion of the number of configurations that an NTM supports as input grows for a given problem?</p>\n\n<p>5.) My last and overarching question is: Are the differences in the set of problems that can be solved in polynomial time on DTMs versus NTMs related to time/space tradeoffs where DTMs can\'t run some polynomial NTM algorithms in polynomial time because they don\'t have the same "space" that an NTM has available to it?</p>\n\n<p>I\'d also appreciate any reading you can suggest to me on time/space tradeoffs and NTMs versus DTMs.</p>\n', 'ViewCount': '109', 'Title': 'Relation of Space and Time in Complexity?', 'LastActivityDate': '2014-02-28T04:20:01.920', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '22110', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '14819', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2014-02-27T20:46:01.303', 'FavoriteCount': '2', 'Id': '22109'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to teach myself complexity. I am trying to come up with a reduction from minimum set cover\n(given a set of items I, and a set S of subsets of I and an integer k, is there a subset S' of S Euch that |S'|&lt;=k and union(S')=I)\n to weighted Steiner tree \n(given a graph G=(V, E) and weight function w, and a subset V' of V and integer k>0, is there a subtree G' of G such that the sum of the weight of the edges in G'&lt;=k and V' is contained in G'?) \n, but have gotten a bit stuck. I believe I am on the right path. </p>\n\n<p>Here's what I have so far. Given an instance of minimum set cover, define a root node r, For every subset in Si in S, define a node Si and connect it by an edge to r with weight one. For every element in Si, define a node and connect them to Si via an edge of weight 0. This creates a tree. I believe something like this should work, but I cannot figure out how to define G' for the constructed instance of STG such that there is an answer yes if there is a minimum set cover. </p>\n\n<p>Any help would be greatly appreciated, thanks</p>\n", 'ViewCount': '84', 'Title': 'Reduction from Steiner tree to minimum set cover', 'LastEditorUserId': '13230', 'LastActivityDate': '2014-02-28T15:27:51.023', 'LastEditDate': '2014-02-28T15:27:51.023', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13230', 'Tags': '<complexity-theory><time-complexity><complexity>', 'CreationDate': '2014-02-27T22:53:13.110', 'Id': '22114'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have thought about it for a while, and I'm not really sure what is the best way to:</p>\n\n<blockquote>\n  <p>1.Implement a stack using 2 queues.</p>\n  \n  <p>2.Implement a queue using 2 stacks.</p>\n</blockquote>\n\n<p>I have only though about something trivial that takes O(n) time for dequeue and enqueue, and O(n) time for push and pop.</p>\n\n<p>Can I do better than that?\nAre there more efficient ways to do 1 and 2?</p>\n", 'ViewCount': '16', 'ClosedDate': '2014-02-28T13:23:58.910', 'Title': 'Impelementing a stack using a 2 queues, and a queue using 2 stacks', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-02-28T13:21:49.683', 'LastEditDate': '2014-02-28T13:21:49.683', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<algorithms><time-complexity><stack>', 'CreationDate': '2014-02-28T13:14:28.070', 'Id': '22131'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How many times does the statement count in line 5 executes in terms of $n$?</p>\n\n<pre><code>1.  count=0; \n2.  for (i=1; i&lt;=n; i++) { \n3.  for (j=1; j&lt;=n; j*=2) { \n4.  for (k=1; k&lt;=j; k++) {\n5.        count = count + 1;\n6.      }\n7.    }\n8.  }\n</code></pre>\n\n<p>For the loop in line 3, we can list the numbers for $j$ as, $2^0,2^1,...,2^{\\log n}$\nTherefore, we can refer to the iterations of the exponents as $r$. Doing so would lead to the summations below for counting the number of executions.</p>\n\n<p>$\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}\\sum_{k=1}^{j}1 =\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}j $</p>\n\n<p>I am stuck here, because $\\sum_{r=0}^{\\log n}$ depends on $j$ but I am not sure how to incorporate them.</p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '20', 'Title': 'The number of executions of the count statement; how many?', 'LastActivityDate': '2014-03-01T05:18:02.957', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22154', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2014-03-01T04:34:54.633', 'Id': '22150'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Lets say that I am storing <code>10^9</code> keys in a single BST. \nCompared to having lets say having multiple BSTs of sizes <code>10^6</code> containing chunk of the bigger tree? Search through all of them executing in parallel.</p>\n\n<p>I am talking about only search performance here, Given that processing power is not a bottle neck.</p>\n', 'ViewCount': '26', 'Title': 'Single big BST vs multiple smaller BSTs? Which is faster for search?', 'LastActivityDate': '2014-03-03T19:26:50.577', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'user3374410', 'PostTypeId': '1', 'Tags': '<time-complexity>', 'CreationDate': '2014-03-03T10:21:54.630', 'Id': '22236'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was working on recurrence relation and came across this example </p>\n\n<p>T(n) = 2T(n/2) + log(n)</p>\n\n<p>What will be the time complexity, ie, big O for this relation.</p>\n\n<p>Thanks for any help in advance.</p>\n', 'ViewCount': '19', 'ClosedDate': '2014-03-04T07:40:38.087', 'Title': 'Time complexity by solving recurrence relation', 'LastActivityDate': '2014-03-04T02:42:47.407', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15253', 'Tags': '<time-complexity><recurrence-relation>', 'CreationDate': '2014-03-04T02:42:47.407', 'Id': '22252'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Most books assume that this is obvious, but I can't see how each $\\Sigma_k=NP^{\\Sigma_{k-1}}$ level in the polynomial hierarchy is closed under polynomial-time reductions. Is there something that I'm missing?</p>\n", 'ViewCount': '10', 'ClosedDate': '2014-03-07T18:51:58.633', 'Title': 'Show polynomial hierarchy levels closed under reduction', 'LastActivityDate': '2014-03-07T09:25:11.540', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15383', 'Tags': '<complexity-theory><time-complexity><reductions><complexity-classes>', 'CreationDate': '2014-03-07T09:25:11.540', 'Id': '22369'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Assume $P\\neq NP$.</p>\n\n<p>What can we say about the runtime bounds of all NP-complete problems?</p>\n\n<p>i.e. what are the tightest functions $L,U:\\mathbb{N}\\to\\mathbb{N}$ for which we can guarantee that an optimal algorithm for <strong>any</strong> NP-complete problem runs in time of at least $\\omega(L(n))$ and at most $o(U(n))$ on a input of length $n$?</p>\n\n<p>Obviously, $\\forall c:L(n)=\\Omega(n^c)$.\nAlso, $U(n) = O(2^{n^{\\omega(1)}})$.</p>\n\n<h2>Without assuming $QP\\neq NP$, $ETH$, or any other assumption which is not implied by $P\\neq NP$, can we give any better bounds on $L,U$?</h2>\n\n<p><strong>EDIT:</strong> </p>\n\n<p>Note that at least one of $L,U$ has to be far from the bounds I gave here, since being NPC problems, these problems has poly time reduction between each other, meaning that if some NPC problem has an optimal algorithm of time $f(n)$, then all problems has an algorithm (optimal or not) of runtime $O(f(n^{O(1)}))$.</p>\n', 'ViewCount': '152', 'Title': u'Runtime bounds on algorithms of NP complete problems assuming P\u2260NP', 'LastEditorUserId': '12969', 'LastActivityDate': '2014-03-16T16:00:29.563', 'LastEditDate': '2014-03-16T16:00:29.563', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '12969', 'Tags': '<complexity-theory><time-complexity><np-complete><p-vs-np>', 'CreationDate': '2014-03-11T20:05:43.997', 'FavoriteCount': '1', 'Id': '22511'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there already a worst case time complexity proof for the sum of all elements in a power set? I would assume, naively, you have to just add everything, which would run in about 2^n, where n is the size of the set.</p>\n\n<p>For example: A = {1,2,3} Powerset(A) = {{}, {1}, {2}, {1, 2}, {3}, {1, 3}, {2, 3}, {1, 2, 3} } Sum(Powerset(A)) = {{} + {1} + {2} + {1 + 2} + {3} + {1 + 3} + {2 + 3} + {1 + 2 + 3} }</p>\n\n<p>I'm defining addition between sets as: A = {1}, B = {2,3}, A + B = {1} + {2 + 3} = 6</p>\n", 'ViewCount': '37', 'Title': 'Proof of sum of powerset?', 'LastActivityDate': '2014-03-12T03:02:16.443', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15579', 'Tags': '<complexity-theory><time-complexity>', 'CreationDate': '2014-03-12T02:20:44.307', 'Id': '22523'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '39', 'Title': "What's the time complexity of this append method?", 'LastEditDate': '2014-03-14T16:03:43.027', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15687', 'Body': '<p>I made a method that appends a sequence to another sequence.</p>\n\n<p>So: (append [1,2,3] [4,5,6]) = [1,2,3,4,5,6]</p>\n\n<p><strong>CODE In C#</strong></p>\n\n<pre><code>IEnumerable&lt;int&gt; Append(IEnumerable&lt;int&gt; xs,IEnumerable&lt;int&gt; ys)\n{\n    using(var iteratorX = xs.GetEnumerator())\n    using(var iteratorY = ys.GetEnumerator())\n    {\n        bool isTrueForX = false;\n        bool isTrueForY = false;\n        while((isTrueForX = iteratorX.MoveNext()) || (isTrueForY = iteratorY.MoveNext()))\n        {\n            if(isTrueForX) yield return iteratorX.Current;\n            else if(isTrueForY) yield return iteratorY.Current;\n        }\n    }\n}\n</code></pre>\n\n<p>I would like to know what is the time-complexity of this algorithm.</p>\n', 'ClosedDate': '2014-03-14T16:28:00.820', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'LastEditorUserId': '15687', 'LastActivityDate': '2014-03-14T16:03:43.027', 'CommentCount': '2', 'AcceptedAnswerId': '22617', 'CreationDate': '2014-03-14T11:37:38.200', 'Id': '22615'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for some statistics for </p>\n\n<ol>\n<li>the total number of operations possible by all the computers in all the world now.</li>\n<li>A capacity factor for this (i.e. what fraction of the potential operations are being performed)</li>\n<li>How this has changed over the last century.</li>\n</ol>\n\n<p>Something with a graph with world total FLOPS against time would be perfect. I'd also be interested in seeing the same thing for other measures of capacity, storage space, networking, whatever. I'm not having much luck finding anything but energy requirements.</p>\n", 'ViewCount': '36', 'ClosedDate': '2014-03-17T11:51:44.423', 'Title': 'Total world computational capacity', 'LastActivityDate': '2014-03-17T04:38:39.960', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'OwnerDisplayName': 'Lucas', 'PostTypeId': '1', 'OwnerUserId': '7641', 'Tags': '<reference-request><time-complexity>', 'CreationDate': '2014-03-16T17:19:09.190', 'FavoriteCount': '2', 'Id': '22692'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know these relations :  </p>\n\n<p>\\begin{gather}\n\\mathrm{NC}^1 \\subseteq \\mathrm{NC}^2 \\subseteq \\dots \\subseteq \\mathrm{NC}^i \\subseteq \\dots \\subseteq \\mathrm{NC} \\\\\n\\mathrm{NC}^i \\subseteq \\mathrm{AC}^i \\subseteq \\mathrm{NC}^{i+1} \\\\\n\\mathrm{NC}^1 \\subseteq \\mathrm{L} \\subseteq \\mathrm{NL} \\subseteq \\mathrm{AC}^1 \\subseteq \\mathrm{NC}^2 \\subseteq \\mathrm{P}\n\\end{gather}</p>\n\n<p>But I don't know how to compare an algorithm with time complexity of $\\mathrm{NC}^i$ to an algorithm with Polynomial complexity? </p>\n\n<p>For example, Topological sort $\\mathrm{NC}^2$ with BFS $\\mathcal{O}(|V| + |E|)$ </p>\n", 'ViewCount': '84', 'Title': 'How to compare algorithms in class NC time complexity with other classes?', 'LastEditorUserId': '15050', 'LastActivityDate': '2014-04-02T06:07:46.287', 'LastEditDate': '2014-04-02T06:07:46.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '15050', 'Tags': '<complexity-theory><time-complexity><parallel-computing>', 'CreationDate': '2014-03-17T13:29:30.280', 'FavoriteCount': '1', 'Id': '22709'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the "Generalized Geography" game: on directed graph G with selected start node, players take turns moving along edges, without ever going back to previously visited nodes.  Last player to move wins.</p>\n\n<blockquote>\n  <p>GG = {  : G is a directed graph, b is a node in G, and the next\n                     player to play has a winning strategy for generalized\n                     geography from start node b, i.e., there are moves for\n                     the next player to win no matter how opponent plays }</p>\n</blockquote>\n\n<p>This problem is well-known to be in PSPACE and EXPTIME-hard. My question is:\nIf we allow repetitions in GG, it still belongs to EXPTIME-hard?</p>\n', 'ViewCount': '59', 'Title': 'Generalized Geography with repetitions', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-22T20:01:27.593', 'LastEditDate': '2014-03-22T20:01:27.593', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15966', 'Tags': '<graph-theory><time-complexity><space-complexity>', 'CreationDate': '2014-03-21T11:50:18.843', 'FavoriteCount': '0', 'Id': '22901'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '11', 'Title': 'Generalized Geography with repetitions', 'LastEditDate': '2014-03-22T20:01:21.090', 'AnswerCount': '0', 'Score': '2', 'OwnerDisplayName': 'user3195997', 'PostTypeId': '1', 'OwnerUserId': '15966', 'FavoriteCount': '0', 'Body': '<p>Consider the "Generalized Geography" game: on directed graph G with selected start node, players take turns moving along edges, without ever going back to previously visited nodes. Last player to move wins.</p>\n\n<blockquote>\n  <p>GG = { : G is a directed graph, b is a node in G, and the next player to play has a winning strategy for generalized geography from start node b, i.e., there are moves for the next player to win no matter how opponent plays }</p>\n</blockquote>\n\n<p>This problem is well-known to be in PSPACE and EXPTIME-hard. My question is: If we allow repetitions in GG, it still belongs to EXPTIME-hard?</p>\n', 'ClosedDate': '2014-03-22T20:01:36.047', 'Tags': '<graph-theory><time-complexity><space-complexity>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-22T20:01:21.090', 'CommentCount': '0', 'CreationDate': '2014-03-21T11:53:26.130', 'Id': '22945'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '46', 'Title': u'Assume that SAT \u2208 PSIZE, does it imply that NP = coNP?', 'LastEditDate': '2014-03-24T22:45:55.480', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7068', 'FavoriteCount': '1', 'Body': "<p>Assume that $\\mathrm{SAT} \\in \\mathrm{PSIZE}$, does it imply that $\\mathrm{NP} = \\mathrm{coNP}$ ?</p>\n\n<p>I think that I've managed to show that if $\\mathrm{SAT} \\in \\mathrm{PSIZE}$, then both $\\mathrm{NP}$ and $\\mathrm{coNP}$ are contained in $\\mathrm{PSIZE}$, but I can't see how does help me. Any ideas ?</p>\n", 'Tags': '<complexity-theory><time-complexity><complexity-classes><circuits>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-24T22:45:55.480', 'CommentCount': '8', 'AcceptedAnswerId': '23019', 'CreationDate': '2014-03-24T21:11:49.613', 'Id': '23018'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have been trying to understand the difference between normal polynomial evaluation and horner's method. usually it takes 3n-1 operations while horner's method reduces it to 2n operations. I tried a couple of explanations but they were too theoritical. I would be glad if somebody comes up with a decent and simple explanation.</p>\n", 'ViewCount': '73', 'Title': "Can somebody explain Horner's method of evaluating polynomials and how does it reduce the time complexity to 2n operations?", 'LastActivityDate': '2014-03-26T15:13:03.730', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '23040', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16105', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2014-03-25T17:30:37.520', 'FavoriteCount': '1', 'Id': '23037'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array $A$ of integers in ascending order, how efficiently can it be decided whether there exists an integer $i$ such that $A[i] = i$? How would an optimal algorithm for this problem work?</p>\n', 'ViewCount': '78', 'ClosedDate': '2014-04-01T22:03:12.943', 'Title': 'Given a sorted array $A$, how can it be efficiently determined whether $\\exists i . A[i] = i$?', 'LastEditorUserId': '69', 'LastActivityDate': '2014-03-27T17:46:44.490', 'LastEditDate': '2014-03-27T17:38:23.723', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15702', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2014-03-27T16:03:43.970', 'Id': '23135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>First off I am new to computer science (first semester) and do not know anything about physics, so I am appologizing up front for this not very scientific question.</p>\n\n<p>I've been wondering how data is processed at the <code>LHC</code>. I am sure <code>CERN</code> doesn't just use one programming language to write code but I am curious what the fastest (machine nearest?) language is, keeping in mind that particles at <code>LHC</code> are nearly moving at the speed of light.</p>\n\n<p>So if for example a sensor senses a particle on one side of the <code>LHC</code>, and there is a sensor on the other side of the LHC, data from one side to the other would have a direct and shorter way, but the processing would have to take less than a 50 millionth of second (est.), right?</p>\n", 'ViewCount': '53', 'ClosedDate': '2014-03-29T11:48:09.897', 'Title': 'Which programming language is used at the Large Hadron Collider?', 'LastActivityDate': '2014-03-28T23:46:03.467', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16257', 'Tags': '<time-complexity><programming-languages>', 'CreationDate': '2014-03-28T23:46:03.467', 'FavoriteCount': '1', 'Id': '23200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider, for example, the definition for $\\Sigma_2^p$ complexity class.</p>\n\n<p>$$ x \\in L \\Leftrightarrow \\exists u_1 \\forall u_2 \\;M(x, u_1, u_2) = 1, $$</p>\n\n<p>where $u_1, u_2 \\in \\{0,1\\}^{p(|x|)}$, for some polynomial $p$. Here, $M$ must be polynomial time. But polynomial in the size of what exactly? For example, if we choose (guess) some $u_1$, do I consider it to be fixed size when talking about time complexity of $M$? More precisely, should $M$ be polynomial only in the size of $x$? </p>\n\n<p>An example. Consider the problem whether, given a graph $A$, there exists a graph\n$B$ such that $B$ is subgraph isomorphic to $A$.</p>\n\n<p>$$A \\in L \\Leftrightarrow \\exists B \\; \\text{SubGraphIsomorphic}(A, B) = 1 $$</p>\n\n<p>Now, subgraph isomorphism is NP-complete. If $B$ is fixed, then there is a TM\nthat implements $\\text{SubGraphIsomorphic}$ in deterministic polynomial time. If $B$ is not fixed, then I cannot claim such a thing unless I know $\\sf P=NP$. Is this problem in $\\Sigma_{1}^{p}$, i.e. $\\sf NP$? (Ok, this problem has trivial solutions, but I hope it helps to pinpoint my confusion.)</p>\n\n<p>My confusion generalizes for all $\\Sigma_{i}^p$. </p>\n', 'ViewCount': '35', 'Title': 'Polynomial Hierarchy --- polynomial time TM', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-29T20:09:59.553', 'LastEditDate': '2014-03-29T20:09:59.553', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23207', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'Tags': '<complexity-theory><time-complexity><complexity-classes><polynomial-time>', 'CreationDate': '2014-03-29T01:14:37.587', 'Id': '23204'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an integer $n$, calculate $n!=n\\times(n-1)\\times(n-2)\\dotsc 3\\times2\\times1$. </p>\n\n<p>What is the best time and space complexity of calculating $n!$?</p>\n\n<p>P.S. I do not have any idea about this topic. I was using MATLAB and I needed to compute $200!$ but it said "Out of memory"!! That\'s why I am asking.</p>\n', 'ViewCount': '31', 'Title': 'What is the time/space complexity of $n!$? Can $n!$ has polynomial space complexity?', 'LastActivityDate': '2014-03-29T22:21:47.410', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '23233', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15654', 'Tags': '<time-complexity><space-complexity>', 'CreationDate': '2014-03-29T21:29:34.247', 'Id': '23231'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In a programming book that I'm currently reading it's stated that</p>\n\n<p>$$\\sum\\limits_{i=1}^{n}i^2$$ is $O(n^3)$. My understanding was that $i\\times i$ is a primitive operation and the complexity would be $O(n)$. What am I missing?</p>\n", 'ViewCount': '100', 'Title': 'Why is this algorithm $O(n^3)?$', 'LastEditorUserId': '16275', 'LastActivityDate': '2014-04-17T21:49:15.437', 'LastEditDate': '2014-04-17T21:49:15.437', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23234', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16275', 'Tags': '<time-complexity>', 'CreationDate': '2014-03-29T22:07:32.737', 'Id': '23232'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Say that for a particular problem, e.g., the independent set problem, it has been shown that no polynomial-time algorithm exists to solve it.</p>\n\n<p>Could we get around this by finding an algorithm which approximates the solution to a certain accuracy?</p>\n\n<p>That is, would the result above bar the existence of an algorithm which finds a maximum independent set to an accuracy of 0.5? I.e., it is guaranteed to be less than 0.5 away from the size of a maximum set? (And hence implying that it actually <em>is</em> a maximum independent set.)</p>\n\n<p>It seems to me that the latter wouldn't violate our proofs of non-tractability, which are discrete in nature, while still giving an answer that satisfies the problem from a practical perspective.</p>\n", 'ViewCount': '252', 'Title': 'Approximating NP-complete problems', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-30T16:47:19.050', 'LastEditDate': '2014-03-30T16:47:19.050', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '23239', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12588', 'Tags': '<algorithms><graph-theory><algorithm-analysis><time-complexity>', 'CreationDate': '2014-03-30T01:43:13.067', 'Id': '23236'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>While familiarizing myself with polynomial hierarchy, I have come across a problem\nof showing $NP^{\\Sigma_{k}^{p} \\cap \\Pi_{k}^{p}} \\subseteq \\Sigma_{k}^{p}$. By looking at the proof for $NP^{SAT} \\subseteq \\Sigma_{2}^{p}$, I got the concept\nwhere we can guess the choices of the NTM and answers to SAT call and then encode\nthe correctness of these answers. However, while I understand encoding correctness of answers for SAT calls, I have a problem of doing the same for the oracle $\\Sigma_{k}^{p} \\cap \\Pi_{k}^{p}$, which has no known complete problems. It seems to me there is a cookbook way of proving this that I am missing?</p>\n', 'ViewCount': '24', 'Title': 'Polynomial hierarchy intersection', 'LastActivityDate': '2014-03-30T04:15:53.713', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '23245', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'Tags': '<complexity-theory><time-complexity><polynomial-time>', 'CreationDate': '2014-03-30T02:34:29.467', 'Id': '23243'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>As a programmer with non CS background, I am learning algorithms.</p>\n\n<p>When explaining the performance of quicksort in an <a href="http://www.amazon.de/Algorithms-Robert-Sedgewick/dp/032157351X/ref=sr_1_1?ie=UTF8&amp;qid=1394794069&amp;sr=8-1&amp;keywords=algorithm" rel="nofollow">Algorithm book</a> and also elsewhere on the web, I do not see any reference to the time/space needed for shuffling. I <em>do</em> understand that shuffling is important to have the $O(n\\log n)$ performance, but shuffling itself would have a complexity of $O(n)$. Why does this become irrelevant in the total performance?</p>\n', 'ViewCount': '165', 'Title': 'Performance impact due to time required for shuffling in Quicksort', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-03T22:02:32.870', 'LastEditDate': '2014-04-03T22:02:32.870', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23394', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16409', 'Tags': '<time-complexity><runtime-analysis><quicksort>', 'CreationDate': '2014-04-03T09:34:36.643', 'Id': '23387'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let me ask my general question using a specific example, namely range searching:</p>\n\n<p>Given a set of points in the plane and an axis parallel rectangle, report all points lying in the rectangle. </p>\n\n<p>If the rectangle contains $k$ points, then this can be solved in $O(\\log n + k)$ time (after some preprocessing). I understand that this is in some sense optimal, but what if the points obey some regularity?</p>\n\n<p>Obviously if they are from the integer grid, this can be solved in constant time.</p>\n\n<p>What is less obvious to me is the situation where the points lie equally spaced on some complicated curves: I could report to the user a set of curve-intervals tuples from which he can deduce the number of points. </p>\n\n<p>But am I actually still solving the problem, or am I just outsourcing the work to the user? </p>\n\n<p>I realize that the question is not very precise and I am not sure if such topics have been studied before. \nI would be interested in any literature reference, where algorithms produce output in a non-explicit way.</p>\n', 'ViewCount': '26', 'Title': 'Lower-bounds of running-time for output sensitive Algorithms', 'LastActivityDate': '2014-04-04T23:00:00.927', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16358', 'Tags': '<reference-request><time-complexity><runtime-analysis>', 'CreationDate': '2014-04-04T23:00:00.927', 'Id': '23438'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So I\'ve been thinking about verifiers and a possible relation between a language\'s class and it\'s verifier complexity. From the book, "NP is the class of languages that have polynomial time verifiers". Is there an analog statement that can be said about a P-class verification complexity? Because P is a subset of NP, I understand that the statement of NP still applies to P. Still, my intuition is that there is something more that can be said about a verifier for P that relates to oracles. Is there more that can be said?</p>\n', 'ViewCount': '22', 'Title': 'Complexity as it relates to verifiers of languages', 'LastActivityDate': '2014-04-07T05:11:36.257', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16519', 'Tags': '<time-complexity><np-complete>', 'CreationDate': '2014-04-07T04:43:13.613', 'Id': '23499'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Has the following problem been studied before? If yes, what approaches/algorithms were developed to solve it?</p>\n\n<blockquote>\n  <p><strong>Problem ("Maximum Stacking Height Problem")</strong></p>\n  \n  <p>Given $n$ polygons, find their stable, non-overlapping arrangement\n  that <strong>maximizes their stacking height</strong> on a fixed floor under the\n  influence of gravity.</p>\n</blockquote>\n\n<p><br></p>\n\n<h2>Example</h2>\n\n<p>Three polygons:</p>\n\n<p><img src="http://i.stack.imgur.com/SbCt3.png" alt="enter image description here"></p>\n\n<p>and three of their infinitely many stable, non-overlapping arrangements, with different stacking heights:</p>\n\n<p><img src="http://i.stack.imgur.com/h938i.png" alt="enter image description here"></p>\n\n<p><br></p>\n\n<h2>Clarifications</h2>\n\n<ul>\n<li>All polygons have uniform mass and equal density</li>\n<li>Friction is zero</li>\n<li>Gravity is acting on every point into the downwards direction (i.e. the force vectors are all parallel)</li>\n<li>A configuration is not considered stable if it rests on an unstable equilibrium point (for example, the green triangle in the pictures can not balance on any of its vertices, even if the mass to the left and the right of the balance point is equal)</li>\n<li>To further clarify the above point: A polygon is considered unstable ("toppling") <em>unless</em> it rests on at least one point <em>strictly to the left</em> <strong>and</strong> at least one point <em>strictly to the right</em> of its center of gravity (this definition greatly simplifies simulation and in particular makes position integration etc. unnecessary for the purpose of evaluating whether or not an arrangement is stable.</li>\n<li>The problem in its "physical" form is a continuous problem that can only be solved approximately for most cases. <strong>To obtain a discrete problem that can be tackled algorithmically, constrain both the polygon vertices and their placement in the arrangement to suitable lattices.</strong></li>\n</ul>\n\n<p><br></p>\n\n<h2>Notes</h2>\n\n<ul>\n<li>Brute force approaches of any kind are clearly infeasible. Even with strict constraints on the placement of polygons inside the lattice (such as providing a limited region "lattice space") the complexity simply explodes for more than a few polygons.</li>\n<li>Iterative algorithms must bring some very clever heuristics since it is easy to construct arrangements where removing any single polygon results in the configuration becoming unstable and such arrangements are unreachable by algorithms relying on every intermediate step being stable.</li>\n<li>Since the problem smells at least NP- but more likely EXPTIME-complete in the total number of vertices, even heuristics would be of considerable interest. <strong>One thing that gives hope is the fact that most humans will recognize that the third arrangement in the example is optimal.</strong></li>\n</ul>\n', 'ViewCount': '46', 'Title': 'Maximum Stacking Height Problem', 'LastActivityDate': '2014-04-10T18:20:35.550', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16652', 'Tags': '<algorithms><time-complexity><optimization><computational-geometry><heuristics>', 'CreationDate': '2014-04-10T17:14:32.580', 'Id': '23651'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say we have a set of numbers $A=\\{a_1, a_2, \\dots, a_n\\}$, and we wish to sum over all possible combinations of $k$ terms to compute</p>\n\n<p>$$\n\\sum_{\\substack{C \\subseteq \\{1,2,\\dots,n\\} \\\\ |C|=k}} \\prod_{c \\in C} a_c\n$$</p>\n\n<p>Naively this requires $O(k\\binom{n}{k})$ operations.</p>\n\n<p>This is different from from computing the permanent where there are permutations. </p>\n\n<p>Is this problem known to be NP-hard when $n=2k$ or other conditions such as $n=\\Theta(k^2)$? </p>\n', 'ViewCount': '49', 'Title': 'Is summing over all possible $k$-combinations NP-hard?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T07:34:46.677', 'LastEditDate': '2014-04-12T07:34:46.677', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23687', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '404', 'Tags': '<complexity-theory><time-complexity><np-hard>', 'CreationDate': '2014-04-11T23:17:54.563', 'Id': '23683'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem which essentially reduces to this:</p>\n\n<ol>\n<li>You have a black-box function that accepts inputs of length $n$.</li>\n<li>You can measure the amount of time the function takes to return the answer, but you can\'t see exactly how it was calculated.</li>\n<li>You have to determine whether the time-complexity of this function is polynomial or exponential.</li>\n</ol>\n\n<p>The way I did this was by running thousands of random sample inputs of varying lengths through the function, then plotting them on a scatter plot with times on the y-axis and input length on the x-axis.</p>\n\n<p>What are some metrics and methods I can use to determine if these points best fit to a polynomial curve or to an exponential curve?</p>\n\n<p>(Similar question asking how to draw polynomial/exponential best fit lines in Python on Stack Overflow: <a href="https://stackoverflow.com/questions/23026267/how-to-determine-if-a-black-box-is-polynomial-or-exponential">https://stackoverflow.com/questions/23026267/how-to-determine-if-a-black-box-is-polynomial-or-exponential</a>)</p>\n', 'ViewCount': '272', 'Title': 'How to determine if a black-box is polynomial or exponential', 'LastEditorUserId': '16701', 'LastActivityDate': '2014-04-12T16:26:22.670', 'LastEditDate': '2014-04-12T06:40:45.627', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23688', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16701', 'Tags': '<complexity-theory><time-complexity><polynomial-time><statistics>', 'CreationDate': '2014-04-12T04:54:14.143', 'Id': '23686'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In my computation book by Sipser, he says that since every language that can be decided in time $o(n \\log n)$ is regular, then that can be used to show $TIME(n \\log (\\log n))\\setminus TIME(n)$ must be the empty set. Can anyone show me why this is?</p>\n\n<p>both $TIME(n\\log(\\log n))$ and $TIME(n)$ are regular. I think that only means we can subtract the two sets and the result will still be regular. I just dont understand how its possible to subtract the collection of $O(n\\log(\\log n))$ time TM decidable languages from the collection of $O(n)$ time TM decidable languages and get the empty set. These two collections are not equal so I feel like there will be something left over</p>\n', 'ViewCount': '100', 'Title': u'Why is TIME(n log (log n)) \\ TIME(n) = \u2205?', 'LastEditorUserId': '31', 'LastActivityDate': '2014-04-13T15:30:31.163', 'LastEditDate': '2014-04-13T15:30:31.163', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23726', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14864', 'Tags': '<complexity-theory><regular-languages><time-complexity><complexity-classes>', 'CreationDate': '2014-04-13T01:15:28.793', 'Id': '23721'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I understand this is a slightly vague question, but there are results for P vs. NP, such as the question cannot be easily resolved using oracles. Are there any results like this which have been shown for P vs. NP but have not been shown for P vs PSPACE, so that there is hope that certain proof techniques might resolve P vs PSPACE even though they cannot resolve P vs NP? And are there any non-trivial results that say that if P = PSPACE then there are implications that do not necessarily hold under P = NP? Or anything else non-trivial in the literature that suggests it's easier to prove P != PSPACE than it is to prove P != NP?</p>\n", 'ViewCount': '96', 'Title': 'Has there been any more progress on P vs. PSPACE compared to P vs. NP?', 'LastActivityDate': '2014-04-13T22:02:49.173', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<complexity-theory><time-complexity><space-complexity>', 'CreationDate': '2014-04-13T18:27:42.120', 'FavoriteCount': '1', 'Id': '23745'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>On math.stackexchange, someone asked how to count the number of ways to place $1$'s into a $10 \\times 10$ matrix so that every row and column has $5$ $1$'s.  Each element of the matrix must be either zero or one.</p>\n\n<p>I came up with a recursive solution for an $N \\times 10$ matrix.  Subproblems are indexed by the counts $c_k$ of how many columns have $k$ $1$'s, for $k =0, 1,2,3,4,5$. The counts $c_k$ have to satisfy $\\sum_k c_k = 10$, and they also have to satisfy $\\sum_k kc_k = 5N$ and $c_k = 0$ for $k &gt; N$. The complexity of this algorithm basically boils down to how many distinct sets of valid indices $(c_k)_k$ there are.</p>\n\n<p>For a $10 \\times 10$ matrix I think this approach should work out nicely, but I worry the complexity might get prohibitively large if we wanted to count how many ways to get $M/2$ $1$'s in every row and column of an $M \\times M$ matrix. So I'm wondering, is there a more efficient way to solve this counting problem?  In other words, a better way than solving for $N \\times M$ in increasing order of $N$ and keeping track of subcases indexed by $(c_k)_k$ such that $\\sum_k c_k = M$ and $\\sum_k k c_k = NM/2$? Also, for my solution, can anybody work out a good bound for how many sub-cases I have as a function of $M$?</p>\n", 'ViewCount': '75', 'Title': 'Count number of ways to place ones in an $M \\times M$ matrix so that every row and column has $k$ ones?', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-18T18:41:58.363', 'LastEditDate': '2014-04-18T15:35:14.763', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><time-complexity><combinatorics>', 'CreationDate': '2014-04-16T19:56:14.717', 'FavoriteCount': '1', 'Id': '23869'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am wanting to create a map where the key contains multiple elements and the elements can be empty/null. The empty values are treated as "anything". I want to lookup function to match when the stored key is the lookup value or it is a generalised version - index key has empties where lookup value has values. I think the formalisation would be "the lookup value logically subsumes the index-key". I also want the lookup function to return the most specific index-key, that is the key with the fewest empties.</p>\n\n<p>For example, if the data is stored in a <code>(&lt;key&gt;, &lt;value&gt;)</code> tuple with the key being a tuple of the elements and <code>?</code> representing the empty set/null value:</p>\n\n<pre><code>((1, ?, 6, 3), "hey")\n((1, 5, 6, 3), "hi")\n((2, ?, ?, ?), "hello")\n</code></pre>\n\n<p>So <code>lookup((2, 4, 5, 6)) -&gt; "hello"</code>. And <code>lookup((1, 5, 6, 3)) -&gt; "hi"</code> because <code>(1, 5, 6, 3)</code> is more specific than <code>(1, ?, 6, 3)</code>.</p>\n\n<p>A simple solution is to store them as shown above and simply look through them. This would take $O(nm)$ where $n$ is the number of entries and $m$ is the number of elements in the key. Checking in most-to-least specific would mean a match could be returned immediately.</p>\n\n<p>This there an approach that could improve this?</p>\n\n<p>Thank you</p>\n', 'ViewCount': '26', 'Title': 'Efficient lookup when key is made of multiple elements and elements can be empty', 'LastActivityDate': '2014-04-18T20:13:29.927', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23923', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5373', 'Tags': '<time-complexity>', 'CreationDate': '2014-04-18T12:46:39.140', 'Id': '23913'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is it more plausible that $NP\\subseteq TIME[O(n^{\\log n})]$ than $NP\\subseteq P$? I don't see this mentioned much and is there a reason why? If this question doesn't make sense, explain why.</p>\n", 'ViewCount': '52', 'Title': '$NP\\subseteq TIME[O(n^{\\log n})]$', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-22T22:07:30.020', 'LastEditDate': '2014-04-22T22:07:30.020', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '24034', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16999', 'Tags': '<complexity-theory><time-complexity><complexity-classes><np>', 'CreationDate': '2014-04-22T20:20:35.647', 'Id': '24033'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I can't understand what my professor wrote about these inclusions concerning deterministic classes:</p>\n\n<p>$$\nDTIME(f) \\subseteq DSPACE(f) \\subseteq \\sum_{c\\in\\Bbb N}DTIME(2^{c(log+f)})  \n$$</p>\n\n<p>I understood the first inclusion:</p>\n\n<blockquote>\n  <p>The Turing Machine needs to do at least one step in order to check the\n  next cell on tape</p>\n</blockquote>\n\n<p>I didn't get the second one:</p>\n\n<blockquote>\n  <p>The number of configurations of the Turing Machine with fixed space is finite, and the computation must stop within a maximum number of steps equal to these settings, otherwise wewould have a cycle.</p>\n</blockquote>\n\n<p>I don't understand the argument of the summation: why that $2$ and that $c(log+f)$?\nWhy is it written like that?</p>\n", 'ViewCount': '78', 'Title': 'Inclusion of complexity classes (Deterministic Turing Machine)', 'LastActivityDate': '2014-04-28T17:47:37.320', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '24192', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17046', 'Tags': '<turing-machines><time-complexity><space-complexity><complexity-classes>', 'CreationDate': '2014-04-28T15:51:17.557', 'Id': '24185'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Problem: Input is an integer number $x$ that we know factors as $p_{i_1}\\cdot p_{i_2}\\ldots p_{i_n}$, where the $p_{i_j}$'s are distinct prime numbers. Output is the above factorization of $x$.</p>\n\n<p>Do you know any results/references for the time complexity of this factoring problem? </p>\n\n<p>Note: If the $p_{i_j}$'s are not assumed distinct, then the problem is just integer factorization. This is a very special case.</p>\n", 'ViewCount': '18', 'Title': 'Complexity of factoring products of distinct prime numbers', 'LastActivityDate': '2014-05-02T20:00:41.800', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6610', 'Tags': '<complexity-theory><reference-request><time-complexity><factoring>', 'CreationDate': '2014-05-02T16:15:36.450', 'Id': '24319'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Denote $D$ a set of finite sequences of integers. In Papadimitriou\'s "Computational Complexity" in theorem 2.5 it is proved that if a RAM program $\\Pi$ computes a function $\\phi$ from $D$ to integers in time $f(n)$, then there is a $7$-string Turing machine $M$, which computes $\\phi$ in time $O(f(n)^3)$.</p>\n\n<p>Consider the following example: $\\phi(S)$ is a sum of first two numbers of a finite sequence $S$, or $0$, if $|S| &lt; 2$. It seems to me that a RAM, defined in Papadimitriou\'s book computes $\\phi$ in time $O(1)$. Thought, any Turing machine, computing $\\phi$, should work for at least linear time (we can take an input of length $n$ with only two numbers in sequence). I don\'t see how to cope with this contradiction.</p>\n\n<p>Could you please describe me, where I am wrong? Thanks a lot.</p>\n', 'ViewCount': '12', 'Title': 'Relation between RAM and Turing machine', 'LastActivityDate': '2014-05-03T19:16:35.420', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17302', 'Tags': '<time-complexity><computation-models>', 'CreationDate': '2014-05-03T19:16:35.420', 'Id': '24361'}},