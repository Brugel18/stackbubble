{'Body': '<p>According to <a href="http://books.google.ca/books?id=kWSZ0OWnupkC&amp;pg=PA224#v=onepage&amp;q&amp;f=false">Immerman</a>, the complexity class associated with <a href="http://en.wikipedia.org/wiki/SQL">SQL</a> queries is exactly the class of <em>safe queries</em> in $\\mathsf{Q(FO(COUNT))}$ (first-order queries plus counting operator): SQL captures safe queries. (In other words, all SQL queries have a complexity in $\\mathsf{Q(FO(COUNT))}$, and all problems in $\\mathsf{Q(FO(COUNT))}$ can be expressed as an SQL query.)</p>\n\n<p>Based on this result, from theoretical point of view, there are many interesting problems that can be solved efficiently but are not expressible in SQL. Therefore an extension of SQL which is still efficient seems interesting. So here is my question:</p>\n\n<blockquote>\n  <p>Is there an <strong>extension of SQL</strong> (implemented and <strong>used in the industry</strong>) which <strong>captures $\\mathsf{P}$</strong> (i.e. can express all polynomial-time computable queries and no others)?</p>\n</blockquote>\n\n<p>I want a database query language which stisfies all three conditions. It is easy to define an extension which would extend SQL and will capture $\\mathsf{P}$. But my questions is if such a language makes sense from the practical perspective, so I want a language that is being used in practice. If this is not the case and there is no such language, then I would like to know if there is a reason that makes such a language uninteresting from the practical viewpoint? For example, are the queries that rise in practice usually simple enough that there is no need for such a language?</p>\n', 'ViewCount': '234', 'Title': 'Extension of SQL capturing $\\mathsf{P}$', 'LastEditorUserId': '41', 'LastActivityDate': '2013-04-27T15:07:01.660', 'LastEditDate': '2012-03-12T21:56:17.563', 'AnswerCount': '3', 'CommentCount': '17', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<database-theory><complexity-theory><finite-model-theory><descriptive-complexity>', 'CreationDate': '2012-03-08T16:08:50.840', 'Id': '135'}{'ViewCount': '204', 'Title': 'Can joins be parallelized?', 'LastEditDate': '2012-06-07T14:49:58.440', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1590', 'FavoriteCount': '1', 'Body': '<p>Suppose we want to join two relations on a predicate. Is this in NC?</p>\n\n<p>I realize that a proof of it not being in NC would amount to a proof that $P\\not=NC$, so I\'d accept evidence of it being an open problem as an answer.</p>\n\n<p>I\'m interested in the general case as well as specific cases (e.g. perhaps with some specific data structure it can be parallelized). </p>\n\n<p>EDIT: to bring some clarifications from the comments into this post:</p>\n\n<ul>\n<li>We could consider an equijoin $A.x = B.y$. On a single processor, a hash-based algorithm runs in $O(|A|+|B|)$ and this is the best we can do since we have to read each set</li>\n<li>If the predicate is a "black box" where we have to check each pair, there are $|A|\\cdot|B|$ pairs, and each one could be in or not, so $2^{ab}$ possibilities. Checking each pair divides the possibilities in half, so the best we can do is $O(ab)$.</li>\n</ul>\n\n<p>Could either of these (or some third type of join) be improved to $\\log^k n$ on multiple processors?</p>\n', 'Tags': '<complexity-theory><time-complexity><parallel-computing><database-theory><descriptive-complexity>', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-07-07T01:35:55.073', 'CommentCount': '9', 'AcceptedAnswerId': '2410', 'CreationDate': '2012-06-05T17:38:32.240', 'Id': '2235'}{'Body': '<p>In <em>Algorithmic Randomness and Complexity</em> from Downey and Hirschfeldt, it is stated on page 129 that </p>\n\n<p>$\\qquad \\displaystyle \\sum_{K(\\sigma)\\downarrow} 2^{-K(\\sigma)} \\leq 1$, </p>\n\n<p>where $K(\\sigma)\\downarrow$ means that $K$ halts on $\\sigma$, $\\sigma$ being a binary string. $K$ denotes the prefix-free Kolmogorov complexity.</p>\n\n<p>When does $K$ halt? I think it only halts on a finite number of inputs, since the classical proof on non-computability of the Kolmogorov complexity gives an upper bound on the domain of $K$. But then, the finite set of inputs on which $K$ halts can be chosen arbitrary (one just needs to store the finite number of complexities in the source code).</p>\n\n<p>So is this sum well-defined? In other words, is the domain of $K$ well defined?</p>\n', 'ViewCount': '145', 'Title': 'When does the function mapping a string to its prefix-free Kolmogorov complexity halt?', 'LastEditorUserId': '2069', 'LastActivityDate': '2012-07-08T09:51:02.807', 'LastEditDate': '2012-07-05T09:06:51.267', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '2625', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2069', 'Tags': '<computability><terminology><kolmogorov-complexity><descriptive-complexity>', 'CreationDate': '2012-07-04T15:34:08.397', 'Id': '2614'}{'Body': '<p>I would like to ask how to determine the number of states when designing a Turing machine from the description for a language? For example:</p>\n\n<p>$\\qquad \\displaystyle L = \\{wcw \\mid w \\in \\{0,1\\}^*\\}.$</p>\n\n<p>I mean how to know how many states are there in the set $Q$, with the information from the description of that language. </p>\n', 'ViewCount': '293', 'Title': 'How to calculate the number of states in designing a Turing machine?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-02T12:33:15.027', 'LastEditDate': '2012-09-30T00:56:55.650', 'AnswerCount': '3', 'CommentCount': '5', 'Score': '5', 'OwnerDisplayName': 'Chau', 'PostTypeId': '1', 'Tags': '<formal-languages><turing-machines><descriptive-complexity>', 'CreationDate': '2012-09-29T14:26:38.203', 'Id': '4801'}{'Body': '<p>Suppose we have a list of unbounded integers, written in binary, and we want to write a (formal) proof that the list is sorted in ascending order.</p>\n\n<p>Such a proof might look (informally) like: "2 &lt; 3, and 3 &lt; 5, and ... and 71 &lt; 79, so the list is sorted."</p>\n\n<p>It would seem such a proof must have length $\\Omega(n)$ where $n$ is the length of the list of integers, as you could use the same kind of argument that is used to show that comparison sorting is $\\Omega(n \\log n)$; roughly, if the proof was any shorter than $n$, it would have missed one of the integers on the list.</p>\n\n<p>Is this the case, or have I missed something?  Is there a clever way to construct an asymptotically shorter proof?</p>\n\n<p><strong>Edit</strong>: As Gilles and Yuval Filmas point out below, a specific answer can only be given if we have some constraints on the language in which the formal proof is written.  For the purposes of this question, suppose that no matter what particular proof language is used, <em>the proof must be written such that it can be verified in time at most polynomial in the length of the proof</em>.  This excludes proofs of the form "for all elements of the list, ..."  (I realize this constraint may make the question more difficult than if a particular proof language was chosen, but it really is closest to what I was thinking when I asked the question.)</p>\n', 'ViewCount': '83', 'Title': 'Lower bound on size of proof that a list of integers is sorted', 'LastEditorUserId': '5049', 'LastActivityDate': '2012-12-23T04:12:52.203', 'LastEditDate': '2012-12-15T09:43:11.183', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5049', 'Tags': '<proof-techniques><lower-bounds><descriptive-complexity>', 'CreationDate': '2012-12-14T18:05:51.623', 'FavoriteCount': '1', 'Id': '7396'}{'Body': '<p>When we talk about operators in descriptive complexity, are they something like this: for example, if transitive closure operator $TR$ is available, we can use variable $y$ that we define as $TR(x)$ where $x$ is input? or is it something else? </p>\n\n<p>Also, when we say $FO(t(n))$, what does quantifier block being iterated $t(n)$ times mean?</p>\n', 'ViewCount': '53', 'Title': 'Operators in descriptive complexity', 'LastEditorUserId': '7073', 'LastActivityDate': '2013-02-28T22:34:47.023', 'LastEditDate': '2013-02-28T14:55:57.453', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7073', 'Tags': '<complexity-theory><descriptive-complexity>', 'CreationDate': '2013-02-28T14:47:30.563', 'Id': '10150'}{'Body': '<p>Can anyone show how to express complexity class P using first-order logic with LFP? (descriptive complexity)</p>\n', 'ViewCount': '56', 'Title': 'Expressing complexity class P using first-order logic with LFP', 'LastEditorUserId': '7118', 'LastActivityDate': '2013-03-03T18:55:04.377', 'LastEditDate': '2013-03-03T04:05:32.463', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7118', 'Tags': '<logic><descriptive-complexity>', 'CreationDate': '2013-03-03T03:42:28.370', 'Id': '10220'}{'Body': '<p>The Kolmogorov complexity of a string $x$ is the size of the smallest Turing machine $M$ that started on empty tape produces $x$. To make it computable, we can add a bound on the time used by $M$ to produce $x$: </p>\n\n<p>$C^{t}(x) = \\min \\{|M| : U(M) = x$ in less than $t(n)$ steps $ n = |x| \\}$ </p>\n\n<p>And for a nice function $f(n) &lt; n$ we can define:</p>\n\n<p>$C[f(n),t(n)] = \\{x : C^t(x) \\leq f(n), n = |x| \\}$</p>\n\n<p>i.e. the set of compressible strings $x$ (whose compressed program has size less than $f(n)$) and that can be generated in time $t(n)$.</p>\n\n<p>For example, for unbounded $f$, we have $C[f(n),n^k] \\subseteq C[f(n),n^{k+1}] \\subset C[f(n),\\infty]$</p>\n\n<blockquote>\n<ul><li>Is the first inclusion tight?</li>\n<li>What is known about the *size* of $C[f(n), n^{k+1}] \\setminus C[f(n), n^{k}]$ ?</li>\n<li>Are there known results for particular classes like $C[n/2,n^k]$?</li>\n</ul>\n</blockquote>\n', 'ViewCount': '51', 'Title': 'Interval density of time bounded Kolmogorov complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-11T07:34:56.750', 'LastEditDate': '2013-04-11T07:34:56.750', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '140', 'Tags': '<complexity-theory><reference-request><descriptive-complexity><kolmogorov-complexity>', 'CreationDate': '2013-04-10T22:52:05.313', 'Id': '11214'}{'Body': '<p>Consider a regular language $L$. Let $D(L)$ be a minimal DFA for $L$ and $N(L)$ be a minimal NFA for $L$ (minimal in the sense of the smallest possible number of states for an automaton that recognizes the given language). Write $|A|$ for the size (number of states) of the automaton $A$. In general, $|N(L)|$ can be a lot smaller than $|D(L)|$ (down to $\\lg |D(L)|$, since determinization is exponential in the worst case).</p>\n\n<p>I am interested in languages for which the minimal NFA is guaranteed to be at least a fraction of the size of the DFA: $|N(L)| \\ge k |D(L)|$. What families of regular languages have this property? In other words, for what family of languages $(L_n)$ such that $|D(L_n)| = n$ is $|N(L_n)| = \\Omega(n)$?</p>\n', 'ViewCount': '176', 'Title': u'For what kinds of languages is min |NFA| = \u03a9(min |DFA|)?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-27T21:37:07.360', 'LastEditDate': '2013-09-13T16:00:09.733', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '10106', 'Tags': '<formal-languages><automata><finite-automata><descriptive-complexity>', 'CreationDate': '2013-09-12T16:37:35.247', 'FavoriteCount': '1', 'Id': '14285'}{'Body': '<p>In some paper I read, </p>\n\n<blockquote>\n  <p>A theoretical worst case study shows that a single regular\n  expression of length $n$ can be expressed as an NFA with $O(n)$\n  states. When the NFA is converted into a DFA, it may generate\n  O($\\Sigma^n$) states. The processing complexity for each character in the inpuyt it $O(1)$ in a DFA, but is $O(n^2)$ for an NFA when all $n$ states are active at the same time.</p>\n</blockquote>\n\n<p>Please explain how NFA has at its maximum just $n$ states and its equivalent DFA has at most $O(\\Sigma^n)$ states?</p>\n', 'ViewCount': '92', 'Title': 'NFA and DFA storage cost', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-04T06:39:01.737', 'LastEditDate': '2013-10-04T06:39:01.737', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4624', 'Tags': '<regular-languages><automata><finite-automata><descriptive-complexity>', 'CreationDate': '2013-10-02T16:31:25.310', 'Id': '14755'}{'Body': '<p>In descriptive complexity, we have theorems that look like $\\mathrm{ESO} = \\mathrm{NP}$ or "on linearly ordered structures, $FO(LFP) = P$", but I don\'t really understand the proofs of those.</p>\n\n<p>For the latter, the intuitive equality really means that the statement "$\\mathcal A\\models \\varphi$" can be verified to be true on a linearly ordered structure $\\mathcal A$ in time $n^k$ iff $\\varphi$ is expressible in some fixpoint logic. The right-to-left inclusion then reads $\\forall M\\; \\mathrm{polytime \\;TM}, \\exists \\varphi\\in\\mathrm{FO(LFP)} \\quad \\left[\\mathcal A \\models \\varphi \\Leftrightarrow M(\\mathcal A) = 1\\right]$.</p>\n\n<p>The proof I have been taught in class for the right-to-left inclusion goes as follows: from a Turing machine M (with set of states $S$), we build a formula on the language $\\{T_0,T_1\\} \\cup \\{H_i : i\\in S\\}$, where all relations are of arity $2k$. This formula says that the structure it\'s being evaluated on is a correct computation of $M$ which ends in an accepting state. How does this prove the theorem? Of course, there is a bijection between accepting computations of $M$ and structures $\\mathcal A$ s.t $M(\\mathcal A) = 1$, but the statement of the theorem doesn\'t mention this bijection. In my opinion, the formula $\\varphi$ that we create should really check properties of $\\mathcal A$, and not properties of the computation of $M$ on $\\mathcal A$... Where is my mistake? Am I overthinking this?</p>\n', 'ViewCount': '32', 'Title': 'About proofs in descriptive complexity', 'LastActivityDate': '2013-11-18T11:34:31.343', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18114', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10157', 'Tags': '<descriptive-complexity>', 'CreationDate': '2013-11-18T10:30:37.587', 'Id': '18113'}