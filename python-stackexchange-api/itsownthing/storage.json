{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My organization wants to maintain multiple copies of data in order to preserve access in the case of localized disasters as well as for the purpose of long term preservation. Are there accepted formal models for determining the appropriate variety of media (eg tape, disk) and their placement in the network? Are currently operating distributed solutions (eg LOCKSS) viable long term solutions for large collections of data?</p>\n', 'ViewCount': '51', 'Title': 'Distributed Storage for Access and Preservation', 'LastEditorUserId': '1038', 'LastActivityDate': '2012-05-01T01:46:07.983', 'LastEditDate': '2012-05-01T01:46:07.983', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1038', 'Tags': '<digital-preservation><distributed-systems><storage>', 'CreationDate': '2012-04-30T23:20:10.173', 'Id': '1602'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2246', 'Title': 'Is the 2nd parity bit in RAID 6 a simple calculation?', 'LastEditDate': '2012-06-30T11:11:30.360', 'AnswerCount': '4', 'Score': '5', 'OwnerDisplayName': 'Mitchell Kaplan', 'PostTypeId': '1', 'OwnerUserId': '2016', 'FavoriteCount': '2', 'Body': '<p>I\'m trying to understand how the 2nd parity bit or byte is set in RAID 6. I\'m reading a <a href="http://kernel.org/pub/linux/kernel/people/hpa/raid6.pdf" rel="nofollow">paper by H. Peter Anvin</a>, and it goes into Galois field algebra, which is somewhat new to me. Anyway, a rep from HP was trying to explain RAID 6 to me and she thought it was merely two XOR operations, one for the 1st parity bit and one for the 2nd.  This doesn\'t make sense to me, but since I\'m still working through the paper I don\'t know if it reduces to something simple for RAID 6 as opposed to RAID n. It looks to me like the 2nd parity bit is quite a bit more complicated than the XOR based 1st parity bit. Is that true?</p>\n', 'Tags': '<operating-systems><storage><error-correcting-codes>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-10T16:49:24.050', 'CommentCount': '0', 'CreationDate': '2012-06-29T23:45:44.393', 'Id': '2554'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a simple problem. I can't seem to even find the right search terms to get me pointed in the direction I need to be heading.</p>\n\n<p>I'm writing a bunch of integers to disk. Lot's of them.</p>\n\n<p>Starting with the integer 2, I add it to another integer, write the result to disk, and start the process again using the result as the seed.</p>\n\n<p>This pattern is helping me generate data I need for other research, but I need to apply this pattern until I've reached integers with a length of 10,000 digits or more.</p>\n\n<p>So, here is my simple question: given a set of data of a known data type, how do I calculate the storage space required to store that set?</p>\n\n<p>At the moment, I'm doing this incredibly simple task in Python, recording longs to disk as binary in a single file. But, on my very small machine, I was only able to store the integers my process produced with values between 0 and 42 billion.</p>\n\n<p>Short of my goal of recording a data set with values between 0 and 1e10000.</p>\n\n<p>Given that I have no exposure to formulas for calculating hardware requirements for storing data like this, I have no idea if I'm using the most efficient language and data type for storing as many integers as possible in as little space as possible.</p>\n", 'ViewCount': '175', 'Title': 'Calculate storage requirements for a data set', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-15T05:23:11.350', 'LastEditDate': '2012-10-15T05:23:11.350', 'AnswerCount': '1', 'CommentCount': '13', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3162', 'Tags': '<integers><efficiency><storage>', 'CreationDate': '2012-10-12T04:34:17.103', 'Id': '5032'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If an organization collects an average of 20,000 EPS over eight hours of an ongoing incident, that will require sorting and analysis of 576,000,000 data records. Using a 300 byte average size, that amounts to 172.8 gigabytes of data.</p>\n\n<p><a href="http://www.sans.org/reading_room/analysts_program/eventMgt_Feb09.pdf" rel="nofollow">link</a></p>\n\n<p>I\'m not clear how this 172.8 was calculated. I know 576,000,000 was taken by multiplying 8 hours into seconds. Should i divide the 300 byte average size? Please help.</p>\n', 'ViewCount': '38', 'ClosedDate': '2012-11-09T07:59:48.240', 'Title': 'calculating storage based upon on eps?', 'LastActivityDate': '2012-11-08T09:38:37.470', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6556', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4229', 'Tags': '<storage>', 'CreationDate': '2012-11-08T09:14:11.090', 'Id': '6554'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The following problem was on my final and in <code>Gate 2006</code>, but I don't understand how to solve it:</p>\n\n<p>Different methods of memory management have different overheads:</p>\n\n<ul>\n<li>The paging method for memory management uses two-level paging, and its storage overhead is $P$. </li>\n<li>The storage overhead for the segmentation method is $S$.  </li>\n<li>The storage overhead for the segmentation and paging method is $T$.</li>\n</ul>\n\n<blockquote>\n  <p>What is the relation among the overheads in the concurrent execution of the processes below?</p>\n  \n  <p>(a) $P &lt; S &lt; T \\hspace{2em}$<br>\n  (b) $S &lt; P &lt; T\\hspace{2em}$<br>\n  (c) $S &lt; T &lt; P\\hspace{2em}$<br>\n  (d) $T &lt; S &lt; P$</p>\n</blockquote>\n\n<pre><code>Process    Total Size (in KB)    Number of segments  \n P1              195                    4  \n P2              254                    5  \n P3               45                    3  \n P4              364                    8  \n</code></pre>\n\n<ul>\n<li>The page size is 1 KB.  </li>\n<li>The size of an entry in the page table is 4 bytes.  </li>\n<li>The size of an entry in the segment table is 8 bytes.  </li>\n<li>The maximum size of a segment is 256 KB.  </li>\n</ul>\n", 'ViewCount': '160', 'Title': 'Processes and Segmentation', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-24T15:32:23.540', 'LastEditDate': '2012-11-24T12:57:21.410', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'QueueTank', 'PostTypeId': '1', 'OwnerUserId': '4743', 'Tags': '<operating-systems><memory-management><storage><paging>', 'CreationDate': '2012-11-24T08:51:43.493', 'Id': '6867'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Using the Shortest-Seek-Time-First (SSTF) disk scheduling algorithm (where we select a request with a minimum seek time from the current head position), what happens if the requests in both directions from the current head position are equal? </p>\n\n<p>For example, if the head position is at 25, and the nearest positions are 5 and 45, how do we determine which one to select? </p>\n\n<p>Thanks</p>\n', 'ViewCount': '487', 'Title': 'SSTF disk scheduling algorithm? What if lowest seek times are equal in either direction?', 'LastActivityDate': '2013-01-31T19:53:55.153', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7627', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2420', 'Tags': '<algorithms><operating-systems><storage>', 'CreationDate': '2012-12-28T14:45:26.713', 'Id': '7625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to understand why <a href="https://en.wikipedia.org/wiki/Fragmentation_%28computing%29" rel="nofollow">fragmentation</a> is a problem for <a href="https://en.wikipedia.org/wiki/NTFS" rel="nofollow">NTFS</a> and <a href="https://en.wikipedia.org/wiki/File_Allocation_Table" rel="nofollow">FAT</a> but not when using <a href="https://en.wikipedia.org/wiki/Inode" rel="nofollow">inodes</a>.\nIn all cases, files are not necessarily stored in a contiguous fashion so I don\'t see the problem for the former two.</p>\n\n<p>So where is the crucial difference?</p>\n', 'ViewCount': '168', 'Title': "Why do some filesystems have fragmentation and others don't?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-22T02:46:09.813', 'LastEditDate': '2013-07-19T10:53:57.027', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8546', 'Tags': '<operating-systems><filesystems><storage>', 'CreationDate': '2013-07-19T07:19:23.193', 'Id': '13344'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Two matrices can be stored in either row major or column major order in contiguous memory. Does the time complexity of computing their multiplication vary depending on the storage scheme? That is, I want to know whether it will work faster if stored in row major or column major order.</p>\n', 'ViewCount': '83', 'Title': 'Does the performance of matrix multiplication depend on the storage of the array?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-12T03:18:57.387', 'LastEditDate': '2013-11-10T23:05:57.370', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8533', 'Tags': '<algorithms><time-complexity><storage><multiplication>', 'CreationDate': '2013-11-10T04:00:59.373', 'Id': '17865'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having confusion whether <code>storage media</code> and <code>storage devices</code> are the same terms. I found on <a href="http://answers.yahoo.com/question/index?qid=20120206233945AA4EgT2" rel="nofollow">this yahoo answers</a> page that there is a subtle difference but very tricky to understand.</p>\n\n<p>I have googled a lot but cannot find the satisfying answer.</p>\n\n<p>Can anyone kindly expailn the difference between these two terms with easy-to-understand examples.</p>\n\n<p>Regards</p>\n', 'ViewCount': '611', 'ClosedDate': '2014-03-04T17:24:00.237', 'Title': 'What is the difference between storage media and storage devices', 'LastActivityDate': '2014-03-04T15:07:24.880', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '18385', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11636', 'Tags': '<storage>', 'CreationDate': '2013-11-26T14:16:37.843', 'Id': '18384'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>All data recovery companies, regardless of skill, unanimously say that if the memory chip of a device has just a hair line crack, data recovery is impossible. Not unlikely, not expensive, but impossible. <a href="http://www.recoverfab.com/" rel="nofollow">One company</a> even stated that even the FBI can\'t retrieve the data. Is this true?</p>\n\n<p>Why is this? I find it hard to believe if just a tiny section of an extremely common chip has a tiny crack, all of the data is completely gone.</p>\n\n<p>I would have thought someone talented person somewhere would be able to patch up the area of the chip and get some of the data back...</p>\n\n<p>Is it something to do with the charge? I know flash memory uses transistors to store its ones and zeroes in the form of an electrical charge. If the chip is cracked, do the transistors "short-out", turning them all to zeroes, something like that? Is the data gone rather than irretrievable?</p>\n', 'ViewCount': '23', 'ClosedDate': '2014-04-13T09:55:15.657', 'Title': 'Why is data stored on a flash disk irretrievable if the memory chip is cracked?', 'LastActivityDate': '2014-04-13T00:32:57.070', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16726', 'Tags': '<storage>', 'CreationDate': '2014-04-12T21:41:06.213', 'Id': '23716'}},