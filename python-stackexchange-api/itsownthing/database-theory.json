{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '319', 'Title': "Does the 'difference' operation add expressiveness to a query language that already includes 'join'?", 'LastEditDate': '2012-04-02T15:35:05.827', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '5', 'FavoriteCount': '1', 'Body': '<p>The set difference operator (e.g., <code>EXCEPT</code> in some SQL variants) is one of the many fundamental operators of relational algebra. However, there are some databases that do not support the set difference operator directly, but which support <code>LEFT JOIN</code> (a kind of outer join), and in practice this can be used instead of a set difference operation to achieve the same effect.</p>\n\n<p>Does this mean that the expressive power of a query language is the same even without the set difference operator, so long as the <code>LEFT JOIN</code> operator is maintained? How would one prove this fact?</p>\n', 'Tags': '<database-theory><relational-algebra><finite-model-theory>', 'LastEditorUserId': '69', 'LastActivityDate': '2013-05-29T00:50:34.590', 'CommentCount': '1', 'AcceptedAnswerId': '28', 'CreationDate': '2012-03-06T19:06:05.667', 'Id': '2'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Does SQL  need subqueries?</p>\n\n<p>Imagine a sufficiently generalized implementation of the structured query language for relation databases. Since the structure of the canonical SQL <code>SELECT</code> statement is actually pretty important for this to make sense, I don't appeal directly to relational algebra, but you could frame this in those terms by making appropriate restrictions on the form of expressions.</p>\n\n<p>An SQL <code>SELECT</code> query generally consists of a projection (the <code>SELECT</code> part) some number of <code>JOIN</code> operations (the <code>JOIN</code> part), some number of <code>SELECTION</code>  operations (in SQL, the <code>WHERE</code> clauses), and then set-wise operations (<code>UNION</code>, <code>EXCEPT</code>, <code>INTERSECT</code>, etc.), followed by another SQL <code>SELECT</code> query.</p>\n\n<p>Tables being joined can be the computed results of expressions; in other words, we can have a statement such as:</p>\n\n<pre><code>SELECT t1.name, t2.address\n  FROM table1 AS t1 \n  JOIN (SELECT id, address \n          FROM table2 AS t3 \n         WHERE t3.id = t1.id) AS t2\n WHERE t1.salary &gt; 50,000;\n</code></pre>\n\n<p>We will refer to the use of a computed table as part of an SQL query as a subquery. In the example above, the second (indented) <code>SELECT</code> is a subquery.</p>\n\n<p>Can all SQL queries be written in such a way as to not use subqueries? The example above can:</p>\n\n<pre><code>SELECT t1.name, t2.address\n  FROM table1 AS t1 \n  JOIN table2 AS t2\n    ON t1.id = t2.id\n WHERE t1.salary &gt; 50,000;\n</code></pre>\n\n<p>This example is somewhat spurious, or trivial, but one can imagine instances where considerably more effort might be required to recover an equivalent expression. In other words, is it the case that for every SQL query $q$ with subqueries, there exists a query $q&#39;$ without subqueries such that $q$ and $q&#39;$ are guaranteed to produce the same results for the same underlying tables? Let us limit SQL queries to the following form:</p>\n\n<pre><code>SELECT &lt;attribute&gt;,\n      ...,\n      &lt;attribute&gt;\n FROM &lt;a table, not a subquery&gt;\n JOIN &lt;a table, not a subquery&gt;\n  ...\n JOIN &lt;a table, not a subquery&gt;\nWHERE &lt;condition&gt;\n  AND &lt;condition&gt;\n  ...\n  AND &lt;condition&gt;\n\nUNION\n -or-\nEXCEPT\n -or-\n&lt;similar&gt;\n\nSELECT ...\n</code></pre>\n\n<p>And so on. I think left and right outer joins don't add much, but if I am mistaken, please feel free to point that out... in any event, they are fair game as well. As far as set operations go, I guess any of them are fine... union, difference, symmetric difference, intersection, etc... anything that is helpful. Are there any known forms to which all SQL queries can be reduced? Do any of these eliminate subqueries? Or are there some instances where no equivalent, subquery-free query exists? References are appreciated... or a demonstration (by proof) that they are or aren't required would be fantastic. Thanks, and sorry if this is a celebrated (or trivial) result of which I am painfully ignorant.</p>\n", 'ViewCount': '1403', 'Title': 'Do subqueries add expressive power to SQL queries?', 'LastActivityDate': '2013-06-06T21:52:42.197', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '2267', 'Score': '21', 'PostTypeId': '1', 'OwnerUserId': '69', 'Tags': '<database-theory><relational-algebra>', 'CreationDate': '2012-03-08T05:55:29.920', 'FavoriteCount': '2', 'Id': '129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>According to <a href="http://books.google.ca/books?id=kWSZ0OWnupkC&amp;pg=PA224#v=onepage&amp;q&amp;f=false">Immerman</a>, the complexity class associated with <a href="http://en.wikipedia.org/wiki/SQL">SQL</a> queries is exactly the class of <em>safe queries</em> in $\\mathsf{Q(FO(COUNT))}$ (first-order queries plus counting operator): SQL captures safe queries. (In other words, all SQL queries have a complexity in $\\mathsf{Q(FO(COUNT))}$, and all problems in $\\mathsf{Q(FO(COUNT))}$ can be expressed as an SQL query.)</p>\n\n<p>Based on this result, from theoretical point of view, there are many interesting problems that can be solved efficiently but are not expressible in SQL. Therefore an extension of SQL which is still efficient seems interesting. So here is my question:</p>\n\n<blockquote>\n  <p>Is there an <strong>extension of SQL</strong> (implemented and <strong>used in the industry</strong>) which <strong>captures $\\mathsf{P}$</strong> (i.e. can express all polynomial-time computable queries and no others)?</p>\n</blockquote>\n\n<p>I want a database query language which stisfies all three conditions. It is easy to define an extension which would extend SQL and will capture $\\mathsf{P}$. But my questions is if such a language makes sense from the practical perspective, so I want a language that is being used in practice. If this is not the case and there is no such language, then I would like to know if there is a reason that makes such a language uninteresting from the practical viewpoint? For example, are the queries that rise in practice usually simple enough that there is no need for such a language?</p>\n', 'ViewCount': '234', 'Title': 'Extension of SQL capturing $\\mathsf{P}$', 'LastEditorUserId': '41', 'LastActivityDate': '2013-04-27T15:07:01.660', 'LastEditDate': '2012-03-12T21:56:17.563', 'AnswerCount': '3', 'CommentCount': '17', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<database-theory><complexity-theory><finite-model-theory><descriptive-complexity>', 'CreationDate': '2012-03-08T16:08:50.840', 'Id': '135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In database normalization, 1NF (no multivalued attributes), 2NF (all non-PK attributes depending only on PK attributes) and 3NF (all non-PK attributes depending on all of the PK attributes) are widely known. The 4NF (no part of the PK depending on other part of the PK) is less known, but still reasonably known.</p>\n\n<p>Much less known are the 5NF, 6NF and the intermediates EKNF (Elementary Key normal form), BCNF (Boyce-Codd normal form - 3.5) and DKNF (Domain/Key normal form - 5.5). What exactly are that? Given a database schema, how do I determine if any table violates one of these much less knows normal forms?</p>\n', 'ViewCount': '206', 'Title': 'How to determine if a database schema violates one of the less known normal forms?', 'LastEditorUserId': '24', 'LastActivityDate': '2012-12-09T08:47:04.460', 'LastEditDate': '2012-03-09T16:48:31.170', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '95', 'Tags': '<database-theory><databases>', 'CreationDate': '2012-03-09T07:21:34.827', 'FavoriteCount': '1', 'Id': '151'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://dictionary.reference.com/browse/Codd%27s+reduction+algorithm">Codd\'s Algorithm</a> converts an expression in tuple relational calculus to Relational Algebra.</p>\n\n<ol>\n<li>Is there a standard implementation of the algorithm? </li>\n<li>Is this algorithm used anywhere? (It seems that the industry only needs SQL and variants, I\'m not sure about database theorists in academia.)</li>\n<li>What\'s the complexity of the reduction?</li>\n</ol>\n\n<p><sub> This was posted on <a href="http://stackoverflow.com/questions/4149840/about-codds-reduction-algorithm">SO</a> over a year ago, but it didn\'t receive a good answer. </sub>  </p>\n', 'ViewCount': '197', 'Title': "About Codd's reduction algorithm", 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-03T09:39:42.760', 'LastEditDate': '2012-04-03T02:16:02.700', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1003', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '898', 'Tags': '<algorithms><database-theory>', 'CreationDate': '2012-04-02T17:24:45.813', 'Id': '996'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm applying the Compare-and-Swap technique to a SQL database to create custom row-level locking in my dataset, allowing for safe READ UNCOMMITTED isolation at the database level.</p>\n\n<p>The Resource table includes a LockOwner <code>GUID</code> and a IsLocked <code>BIT</code> field.  To acquire a lock, a dirty-read query gets the ID, LockOwner, and LockStatus.  If <code>Unlocked</code>, attempt to <code>UPDATE</code> the Resource by (ID, LockOwner) with a newly generated LockOwner and LockStatus of <code>Locked</code>.  Abort and start again if no rows are updated - meaning someone else got there first.  Otherwise, the Lock is held in the READ UNCOMMITTED transaction.  The transaction is needed to allow rollback on client failure/abandon, but the dirty reads avoid locks.</p>\n\n<p><strong>This seems to me to work great for resources that are independent of each other.  But what must I add to account for a new kind of lock, ResourceGroup?</strong></p>\n\n<p>ResourceGroup to Resource is a one-to-many relationship.  Resources can be locked individually, but if the ResourceGroup needs to be locked, then all of the Resources must also be locked.  </p>\n\n<p>Locking a ResourceGroup is a far less frequent need than locking a Resource, so the scheme should be optimized for Resource queries, avoiding requiring joins to ResourceGroup if possible.</p>\n\n<p>I am imagining a scenario where locking a ResourceGroup involves marking the member rows with some flag, but I'm not sure what scheme doesn't interfere with the original Resource-only scheme.  Part of the problem comes from the UPDATE of a Resource while it is locked (and therefore already UPDATED in another transaction).  I believe that even if the fields are different within the record, the UPDATE will place an UPDATE LOCK on the row, so any lock on ResourceGroup would introduce blocking that we are trying to avoid.  Even if we could do this, how would the ResourceGroup lock acquisition mechanism know when all of the Resources (which may have had locks in process as we began locking their peers) have been released?</p>\n\n<p>There may be differences in this locking granularity by RDBMS, I'm on MS SQL 2005+.</p>\n", 'ViewCount': '134', 'Title': 'Compare-and-Swap in an RDBMS for custom locks and lock escalation', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-22T14:50:44.323', 'LastEditDate': '2012-05-21T22:19:04.747', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '762', 'Tags': '<concurrency><database-theory>', 'CreationDate': '2012-05-21T13:22:33.523', 'Id': '1974'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following query:</p>\n\n<pre><code>SELECT Customer.Name FROM Customer\nINNER JOIN Order on Order.CustomerId = Customer.Id\nWHERE Customer.Preferred = True AND\n      Order.Complete = False\n</code></pre>\n\n<p>Let\'s suppose all of the relevant attributes (Customer.Preferred, Order.Complete, Order.CustomerId and Customer.Id) are indexed. How can I evaluate this as quickly as possible?</p>\n\n<p>Standard optimization advice would say that I should do the select on each table first, then the join using sort-merge or whatever the cardinality would imply. But this involves two passes through the data - I\'m wondering if there\'s a better way.</p>\n\n<hr>\n\n<p><strong>EDIT</strong>: I think asking if there was a "better way" was too ill-defined. Suppose we are trying to find $\\sigma_a(A)\\bowtie_j\\sigma_b(B)$. Observe that we can find this in $O(\\alpha)$ (where $\\alpha$ is the cardinality of $\\sigma_a(A)$) with the following pseudocode:</p>\n\n<pre><code>for each a in A:\n   find foreign tuple in B  // constant-time, if using hash table\n   check if foreign tuple meets foreign constraint  // again, constant time\n</code></pre>\n\n<p>As mentioned by some answerers, there are various minor permutations (do the for loop over B instead, etc.). But they all seem to be $O(\\alpha)$ or $O(\\beta)$. Is there a better way?</p>\n\n<p>Note that if it the query were a self join, we could just do the merge part of a sort-merge join, (since our indexes would already be sorted) which would run in time proportional to the number of results. So I ask if a similar thing can be done here.</p>\n\n<p>I am more than happy to accept a proof that there is no better method as an answer. I believe that there is no faster algorithm, but I\'m unable to prove it.</p>\n', 'ViewCount': '86', 'Title': 'Optimizing a join where each table has a selection', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-06-11T21:35:22.203', 'LastEditDate': '2012-06-11T21:35:22.203', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1590', 'Tags': '<optimization><database-theory><relational-algebra><databases>', 'CreationDate': '2012-05-21T20:03:25.400', 'FavoriteCount': '0', 'Id': '1980'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '204', 'Title': 'Can joins be parallelized?', 'LastEditDate': '2012-06-07T14:49:58.440', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1590', 'FavoriteCount': '1', 'Body': '<p>Suppose we want to join two relations on a predicate. Is this in NC?</p>\n\n<p>I realize that a proof of it not being in NC would amount to a proof that $P\\not=NC$, so I\'d accept evidence of it being an open problem as an answer.</p>\n\n<p>I\'m interested in the general case as well as specific cases (e.g. perhaps with some specific data structure it can be parallelized). </p>\n\n<p>EDIT: to bring some clarifications from the comments into this post:</p>\n\n<ul>\n<li>We could consider an equijoin $A.x = B.y$. On a single processor, a hash-based algorithm runs in $O(|A|+|B|)$ and this is the best we can do since we have to read each set</li>\n<li>If the predicate is a "black box" where we have to check each pair, there are $|A|\\cdot|B|$ pairs, and each one could be in or not, so $2^{ab}$ possibilities. Checking each pair divides the possibilities in half, so the best we can do is $O(ab)$.</li>\n</ul>\n\n<p>Could either of these (or some third type of join) be improved to $\\log^k n$ on multiple processors?</p>\n', 'Tags': '<complexity-theory><time-complexity><parallel-computing><database-theory><descriptive-complexity>', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-07-07T01:35:55.073', 'CommentCount': '9', 'AcceptedAnswerId': '2410', 'CreationDate': '2012-06-05T17:38:32.240', 'Id': '2235'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose there are two databases, $D_1$ and $D_2$.  Let's further assume $D_1$ is always up and $D_2$ can be down sometimes. When it goes up again, it has to restart.</p>\n\n<p>$D_1$ is filled by say a dozen other systems with event messages.  Those messages have IDs and might be updated or deleted. $D_2$ needs to be in sync with $D_1$, which is realized by:</p>\n\n<ul>\n<li>on restart, pull all data from $D_1$.  During this pull $D_1$ is locked, thus all senders to $D_1$ need to wait.</li>\n<li>otherwise $D_1$ always informs $D_2$ of updates by sending all updates. (During fetching, the data is locked of course.)</li>\n</ul>\n\n<p>Now the question: what kind of blocking behaviour can we expect from $D_1$ and $D_2$?</p>\n\n<p>In particular I find the following corner case interesting and instructive:</p>\n\n<p>$D_1$ currently has a long list of events and the sender systems send a lot of new events/updates. $D_2$ just went down, goes up now and needs to fetch events from $D_1$, thus blocking the whole chain.</p>\n", 'ViewCount': '30', 'Title': 'Uni-directional synchronization and locking issues', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-12T22:04:59.373', 'LastEditDate': '2012-06-12T22:04:59.373', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '5', 'OwnerDisplayName': 'user694971', 'PostTypeId': '1', 'Tags': '<distributed-systems><database-theory>', 'CreationDate': '2012-05-07T13:09:02.477', 'Id': '2345'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We\'re running some benchmarks for an approximative query-answering system. It\'s sufficient to just think of it as running some SQL queries with joins. We are counting the results returned as part of the benchmark. However, the results often contain a lot of redundancy, so just counting results seems coarse.</p>\n\n<p>Consider the following table containing results for a query like "<em>for the US, give me its states and its car manufacturers</em>":</p>\n\n<pre><code>================================\n|| ?us_state | ?us_car_manu   ||\n||============================||\n|| Alabama   | Chrysler       ||\n|| Alaska    | Chrysler       ||\n|| ...         ...            ||\n|| Wyoming   | Chrysler       ||\n|| Alabama   | General Motors ||\n|| Alaska    | General Motors ||\n|| ...         ...            ||\n|| Wyoming   | General Motors ||\n|| Alabama   | Ford           ||\n|| Alaska    | Ford           ||\n|| ...         ...            ||\n|| Wyoming   | Ford           ||\n===============================\n</code></pre>\n\n<p>All 200 (50 \xd7 4) results are of course unique. However, given that there is an inherent Cartesian product, the number of results flatters the amount of "information content" or "entropy" of the table: every additional car manufacturer adds fifty results for the fifty US states. (Again, this is just an example; I\'m not interested in better ways to represent or run this particular query.)</p>\n\n<p>As such, we\'re looking for a metric that will give an indication as to the (loosely speaking) redundancy-free content in the table for better comparison of <em>content</em> across different results for different queries. Other result tables may contain a mix of different types of Cartesian products (e.g., consider generalising the query to any country, where each country itself has its own product of states and car manufacturers, etc.).</p>\n\n<p>Currently we\'re working off a simple metric which just counts unique term\u2013position combinations: for the above example, the metric gives 50 + 4 = 54. This may be sufficient for comparison, but is not sensitive to the combination of terms for individual results.</p>\n\n<p>Thanks to Wikipedia, I\'m aware of\u2014but not familiar with\u2014the notion of <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy in information theory</a>. However, I\'m unclear on how the concept of entropy could be applied to this use-case. (I\'m not interested in the entropy of the result strings; each term can be considered a "symbol".) Roughly speaking, each query variable could be considered as a free variable with the result terms in that column providing a set of possible outcomes and their frequency of occurrence being used as a probability mass function. This way I could compute the Shannon entropy for each column. But thereafter, I don\'t know how columns can be combined, or how tuples or results can be considered ... if a notion of conditional entropy would be better, etc.</p>\n\n<p>And so ...</p>\n\n<blockquote>\n  <p>Does anyone have pointers to related material on the measure of entropy/redundancy/etc. in tables or similar structures? </p>\n  \n  <p>Otherwise, does anyone have any ideas on how to use Shannon entropy in a convincing way for tabular data?</p>\n</blockquote>\n', 'ViewCount': '227', 'Title': 'Measuring entropy for a table (e.g., SQL results)', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T02:38:23.283', 'LastEditDate': '2012-10-23T02:38:23.283', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2357', 'Tags': '<reference-request><information-theory><data-compression><database-theory><entropy>', 'CreationDate': '2012-08-03T20:36:20.237', 'FavoriteCount': '1', 'Id': '3029'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a relation $R(A, B, C, D, E, F)$, with the following functional dependencies:\n$\\{A \\rightarrow BC, CD \\rightarrow E, B \\rightarrow D, E \\rightarrow A\\}$.</p>\n\n<p>The objective is to decompose $R$ into 3NF relations.</p>\n\n<p>So far, I have determined that the following candidate keys are present in the given relation:\n$AF$, $EF$, $CDF$ and $BCF$.</p>\n\n<p>Since every attribute is present as a part of some candidate key, for every $X \\rightarrow A$, $A$ will be part of some candidate key, and so R itself should be in 3NF.</p>\n\n<p>Since the question explicitly, however, states that it is not in 3NF, I have got a little confused. Where am I going wrong? Is there something that I have not understood?</p>\n', 'ViewCount': '274', 'Title': 'Decomposition of a relation to 3NF', 'LastEditorUserId': '69', 'LastActivityDate': '2013-05-29T00:27:39.973', 'LastEditDate': '2013-02-21T21:47:47.420', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2596', 'Tags': '<database-theory>', 'CreationDate': '2012-09-24T15:56:06.103', 'Id': '4720'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am trying to understand how a <code>OUTER UNION</code> $\u222a^\u2733$ works, and why it is only partially compatible. </p>\n\n<p>I am aware this operation was created to take union of tuples from two relations if the relation are not type compatible (which I understand).</p>\n\n<p>Examples of this operation will be great!</p>\n', 'ViewCount': '144', 'Title': 'What is OUTER UNION and why is it partially compatible', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-28T20:04:37.257', 'LastEditDate': '2012-11-29T16:45:39.650', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6999', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4790', 'Tags': '<terminology><database-theory><relational-algebra>', 'CreationDate': '2012-11-28T18:25:12.240', 'Id': '6997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The last step in the design process is the physical design, which maps the local conceptual schemas to the physical storage devices available at the corresponding sites.</p>\n\n<p>What meaning of corresponding sites ? How look like in design ?</p>\n', 'ViewCount': '50', 'Title': 'In DDBMS, What meaning of corresponding sites?', 'LastActivityDate': '2012-12-10T09:51:54.203', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4827', 'Tags': '<distributed-systems><database-theory><databases>', 'CreationDate': '2012-12-10T09:51:54.203', 'Id': '7299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can anybody please solve the following problem step-by-step with explanations, using bond energy and vertical partitioning to obtain a vertical fragmentation of the set of attributes.</p>\n\n<p>I need to understand how bond energy and vertical partitioning algorithm work with this problem. The problem is in the following link: <a href="http://download.bwor.net/V_Fragment.jpg" rel="nofollow">Problem of Vertical Fragment</a></p>\n', 'ViewCount': '206', 'Title': 'Bond energy and vertical partitioning to get a vertical fragmentation in distributed dbms', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-23T00:05:20.297', 'LastEditDate': '2012-12-23T00:05:20.297', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4827', 'Tags': '<distributed-systems><database-theory><databases>', 'CreationDate': '2012-12-22T22:21:46.577', 'Id': '7547'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2244', 'Title': 'What use are groups, monoids, and rings in database computations?', 'LastEditDate': '2013-02-11T19:38:25.807', 'AnswerCount': '6', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '3131', 'FavoriteCount': '21', 'Body': '<p>Why would a company like Twitter be interest in algebraic concepts like groups, monoids and rings. <a href="https://github.com/twitter/algebird">https://github.com/twitter/algebird</a></p>\n\n<p>All I could find is:</p>\n\n<blockquote>\n  <p>Implementations of Monoids for interesting approximation algorithms,\n  such as <a href="http://stackoverflow.com/questions/14790588/what-is-twitters-interest-in-abstract-algebra">Bloom filter</a>, <a href="http://stackoverflow.com/questions/12327004/how-does-the-hyperloglog-algorithm-work">HyperLogLog</a> and <a href="http://dimacs.rutgers.edu/~graham/pubs/papers/cmencyc.pdf">CountMinSketch</a>. These allow you\n  to think of these sophisticated operations like you might numbers, and\n  add them up in hadoop or online to produce powerful statistics and\n  analytics.</p>\n</blockquote>\n\n<p>and in another part of the GitHub page:</p>\n\n<blockquote>\n  <p>It was originally developed as part of Scalding\'s Matrix API, where\n  Matrices had values which are elements of \n  <a href="http://en.wikipedia.org/wiki/Monoid#Relation_to_category_theory">Monoids</a>, <a href="http://en.wikipedia.org/wiki/Group_%28mathematics%29">Groups</a>, or <a href="http://www.math.ku.dk/~gelvin/Modules.pdf">Rings</a>. Subsequently, it was clear that the code had broader application\n  within Scalding and on other projects within Twitter.</p>\n</blockquote>\n\n<p>What could this broader application be? within Twitter and for general interest?</p>\n\n<hr>\n\n<p>It seems like composition aggregations of databases have a monoid-like structure.</p>\n\n<p>Same question on quora: \n<a href="http://www.quora.com/Twitter-1/What-is-Twitters-interest-in-abstract-algebra-with-algebird">http://www.quora.com/Twitter-1/What-is-Twitters-interest-in-abstract-algebra-with-algebird</a></p>\n\n<hr>\n\n<p>I have math background but I\'m not computer scientist. It would be great to have "real-world" uses of monoids and semi-groups.  These are normally considered useless theoretical constructs, and ignored in many abstract algebra courses (for lack of anything interesting to say).</p>\n', 'Tags': '<reference-request><discrete-mathematics><database-theory><applied-theory><group-theory>', 'LastEditorUserId': '3131', 'LastActivityDate': '2013-02-11T23:40:20.447', 'CommentCount': '5', 'AcceptedAnswerId': '9687', 'CreationDate': '2013-02-10T21:59:23.007', 'Id': '9648'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Assume there was a database system that had a data type called <code>VARINT</code> or some variant that allowed instead of fixed-length <code>INT</code>s regardless of value, a <code>1</code> would only take 1 <code>BIT</code> (<code>1</code>), <code>2</code> would take 2 <code>BIT</code>s (<code>10</code>), etc.</p>\n\n<p>In this perfect world, a <code>VARINT</code> could be used to <code>autoincrement</code> a <code>PRIMARY</code> column.  However, there\'s still the issue of the field growing in size, so the "older" parts of this <code>TABLE</code> would be very fast, but the "younger" parts would get slower and slower.</p>\n\n<p>What counting system could be used to hold the space consumed constant and small or at least grow at a much slower rate?</p>\n', 'ViewCount': '19', 'Title': 'VAR autoincrement with constant space consumption for super large tables', 'LastActivityDate': '2013-04-05T20:34:02.580', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11064', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7386', 'Tags': '<integers><database-theory><performance>', 'CreationDate': '2013-03-23T21:49:04.237', 'Id': '10724'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The paper <a href="http://groups.csail.mit.edu/tds/papers/Lynch/podc96-esds.pdf" rel="nofollow">Eventually-Serializable Data Services (PODC\'96)</a> presented an <em>eventually-serializable</em> data service. </p>\n\n<p>In the abstract, it says\uff1a</p>\n\n<blockquote>\n  <p>\u2026 and generalizes their algorithm (in the related work <a href="http://dl.acm.org/citation.cfm?id=93399" rel="nofollow">Lazy replication</a>) by allowing general operations \u2026</p>\n</blockquote>\n\n<p>In section 1.1 (Background of our Work), it says:</p>\n\n<blockquote>\n  <p>\u2026 Also, their algorithm (in the related work <a href="http://dl.acm.org/citation.cfm?id=93399" rel="nofollow">Lazy replication</a>) requires that all operations be either read-only queries or write-only updates \u2026</p>\n</blockquote>\n\n<p>My questions is as follows:</p>\n\n<blockquote>\n  <p>Is the protocol in this paper suitable for transactions (as a kind of general operations) services?</p>\n</blockquote>\n\n<p>Specifically:</p>\n\n<p>In section 3, the authors give an abstract algorithm for implementing eventually-serializable data service. The algorithm is based on lazy replication, in which the operations received by each replica are gossiped in the background. The key property is that the operations requested in a partial order will be eventually total ordered through the gossip protocol. I want to extend this eventually-serializable notion to support transactions. Does this algorithm itself support transactions already? And what are the challenges to support transactions instead of ordinary operations?</p>\n', 'ViewCount': '35', 'Title': 'Is this protocol suitable for transaction services?', 'LastEditorUserId': '4911', 'LastActivityDate': '2013-04-14T14:19:48.937', 'LastEditDate': '2013-04-14T14:19:48.937', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<distributed-systems><database-theory><protocols>', 'CreationDate': '2013-04-12T13:28:05.350', 'Id': '11261'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So I'm just starting to learn about query processing and such in databases and I'm having some trouble.  I don't really understand how to compute the minimum number of block reads given a relation and a query I guess you could say.  If anyone could help me out, it'd be much appreciated.  Here is an example that I'm working on:</p>\n\n<ul>\n<li>R1(A,B,C) A is a primary key and C is a foreign key to R2.C</li>\n<li>R2(C,D,E) C is a primary key</li>\n<li>R1 has 20,000 records, with 200 records per block. There is a primary B+-tree index on\nA with height h = 3</li>\n<li>R2 has 45,000 records, with 4,500 records per block. There is a primary B+-tree index\non C with height hC = 3 and a secondary B+-tree index on D with height hD = 2.</li>\n</ul>\n\n<p>Find the minimum number of block reads for each statement.  I can only hold one block of memory for each relation at a time. </p>\n\n<ul>\n<li>Where B=1(R1)</li>\n<li>Where C=1(R2)</li>\n</ul>\n\n<p>I'm not looking for answers.  I'm looking for explanation of how to actually do it and guide me along the way.  Aka equations, etc. It's kind of difficult to find anything beneficial online at all. </p>\n", 'ViewCount': '79', 'Title': 'Computing number of block reads given relational algebra statement', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-24T06:20:57.237', 'LastEditDate': '2013-04-24T06:20:57.237', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11512', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7233', 'Tags': '<database-theory><databases><relational-algebra>', 'CreationDate': '2013-04-22T16:03:39.790', 'Id': '11492'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '409', 'Title': '"At least one" clause in Relational Algebra', 'LastEditDate': '2013-05-30T23:49:18.537', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8123', 'FavoriteCount': '1', 'Body': '<p>I\'m fairly new to the syntax of relational algebra, and I\'m having a hard time understanding how I could set a "at least one" clause. </p>\n\n<p>Example: I have:</p>\n\n<ul>\n<li>a table with books (listing the title, year published and ID), </li>\n<li>a table with authors (listing their name and ID),</li>\n<li>a table which lists what author wrote what book (through a tuple of the IDs mentioned before).</li>\n</ul>\n\n<p>How could I, in relational algebra, get "All the authors that have published at least one book per year between 2008 and 2010"?</p>\n\n<p>I have figured this so far. At step "b", the Natural join is used since both tables have PublicationID in common. Thus, the resulting table is |PublicationID|AuthorID|Year|. So I\'m simply missing the step "c", where I don\'t understand how to gather a sub-set of the authors that published at least one book per year between 2008 and 2010. </p>\n\n<p>$ a \\leftarrow \\pi_{PublicationID,Year} (Publication)$ </p>\n\n<p>$ b \\leftarrow a \\bowtie AuthorPublication $ </p>\n\n<p>$ c \\leftarrow \\sigma_{something} $</p>\n', 'Tags': '<database-theory><databases><relational-algebra>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-30T23:49:55.977', 'CommentCount': '5', 'AcceptedAnswerId': '12227', 'CreationDate': '2013-05-22T16:16:54.920', 'Id': '12218'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In database theory, there is a notion of transitive closure over relations. I am wondering if join operator over relations is also a special case of transitive closure?</p>\n', 'ViewCount': '155', 'Title': 'What is the difference between Transitive Closure and Join?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-22T12:28:31.773', 'LastEditDate': '2013-05-27T07:03:04.287', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'heykell', 'PostTypeId': '1', 'OwnerUserId': '8413', 'Tags': '<database-theory><databases>', 'CreationDate': '2013-05-27T00:52:41.530', 'Id': '12301'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>A badly explained textbook example asks the following:</p>\n\n<p>Suppose we have a definition of the dictionary our database will use (that is, it\'s relations and constants), defined as:</p>\n\n<p>$&lt;R_\\le (?, ?), R_+(?,?,?), R_*(?, ?, ?), c_0, c_1&gt;$</p>\n\n<p>where $R_{operation}$ defines the nature of the relationship of the values inside (represented as question marks), and $c_{value}$ represents a constant. For example, $R_+(a, b, c)$ means that $a + b = c$.</p>\n\n<p>If we build a database structure using our dictionary over the real numbers with constants defined as $1, 0$ we have</p>\n\n<p>$&lt;\\mathbb{R}, r_\\le, r_+, r_*, 1, 0&gt; $</p>\n\n<p>The questions asks:\nWrite a query that will return all the points on the graph between the lines </p>\n\n<ul>\n<li>$y = x$</li>\n<li>$x = 2$</li>\n<li>$y = 0$</li>\n</ul>\n\n<p><img src="http://i.stack.imgur.com/KOBtP.png" alt="x=y, y=0, x=2"></p>\n\n<p><strong>ANSWER</strong>\nThe answer given is:</p>\n\n<p>$R_\\le(y,x) \\wedge R_\\le(c_0,y) \\wedge \u2203a(R_+(c_1,c_1,a) \\wedge R_\\le(x,a))$</p>\n\n<p>What\'s not explained however, is why the last constraint is so long? Instead of writing $\u2203a(R_+(c_1,c_1,a) \\wedge R_\\le(x,a))$, why don\'t we just say $R_\\le (x, 2)$?</p>\n\n<p>What benefits are we getting by doing it this roudabout way? </p>\n\n<p>I understand it has something to do with only using our constants, but 2 is in our domain, why can\'t we use it?</p>\n\n<p>Please explain, thank you so much.</p>\n', 'ViewCount': '38', 'Title': "In database query theory, why do we not want to use a constant that isn't in the dictionary?", 'LastActivityDate': '2013-10-18T18:07:30.960', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<database-theory>', 'CreationDate': '2013-10-16T14:58:24.520', 'Id': '16140'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When reading the literature on declarative programming technology. A particular technology that sounded promising is active databases. It essentially allows the programmer to specify rules about data that generate more data. On face value it seems like a great thing to have when compared to your neighborhood average imperative code.</p>\n\n<p>There are many production rules systems out there, but they are not integrated with a database system. As I understand it, active databases would have been the logical evolution for  production systems.</p>\n\n<p>But, the subject seems to have died off. It is hard to find articles on Active Databases. And I can't really find a product that implements anything like that. My question is, what happened?</p>\n", 'ViewCount': '57', 'Title': 'What happened to Active Databases?', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-22T00:01:21.793', 'LastEditDate': '2013-10-22T00:01:21.793', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9964', 'Tags': '<programming-languages><history><database-theory>', 'CreationDate': '2013-10-21T15:02:14.870', 'Id': '16295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I came across a slide online that I do not understand and can\'t find any explanation - or anything similar - on it anywhere.</p>\n\n<p>It describes a simple ERD relationship in terms of constraints on the keys, for example, $$K_2 \\wedge K_R -&gt; K_1 \\wedge K_R$$.</p>\n\n<p>I can understand the logic; if $K_2$ and $K_R$ are related then $K_1$ and $K_R$ are also related. But what exactly are $K_1$, $K_2$, $A_R$? Why don\'t we say $E_1$, $E_2$, $E_R$?</p>\n\n<p>I also don\'t understand the notation in the box on the right.\n<img src="http://i.stack.imgur.com/RcKli.jpg" alt="Database Relationship Constraints"> </p>\n', 'ViewCount': '12', 'Title': 'Formal description of entity relationships and constraints in databases', 'LastActivityDate': '2013-10-28T07:01:18.283', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<database-theory>', 'CreationDate': '2013-10-28T07:01:18.283', 'Id': '16490'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am reading about disk redudancy.<br>\nI read the following:  </p>\n\n<blockquote>\n  <p>Suppose that the mean time to failure of a disk is 100,000 hours. Then\n  the mean time to failure of some disk in an array of 100 disks will be\n  100,000/100 = 1000 hours  </p>\n</blockquote>\n\n<p>I don't understand this. Why isn't it 100,000^100 instead?<br>\nThen the same textbook says in the next paragraph concerning 2 mirrored disks:  </p>\n\n<blockquote>\n  <p>If the mean time to failure of a single disk is 100,000 hours and the\n  mean time to repair is 10 hours then the mean time to data loss is\n  (100,000^2)/2*100   </p>\n</blockquote>\n\n<p>Now here it multiplies for the 2 disks but before the MTTF was divided.  </p>\n\n<p>Can anyone please help me figure out how we calculate this.<br>\nIn case it matters the text book is Database Concepts from Silberschatz 4th edition paragraph 11.3.1 (Improvement of reliability via redundancy) page 403</p>\n", 'ViewCount': '87', 'Title': 'Why is the Mean Time To Failure of multiple disks calculated via division and not multiplication?', 'LastActivityDate': '2013-11-12T01:18:56.163', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11321', 'Tags': '<operating-systems><computer-architecture><database-theory><reliability>', 'CreationDate': '2013-11-11T23:22:13.347', 'Id': '17933'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following schema: <code> Students(Id, Name, AdvisorId, AdvisorName, FavouriteAdvisorId) </code></p>\n\n<p>What are the functional dependencies here? Ir seems that it would be:</p>\n\n<p><code> Id --> Name AdvisorId AdvisorName FavouriteAdvisorId </code></p>\n\n<p>If we know the student id then we know his name, his advisor, his advisor id and his favouriteadvisorid. But the correct functional dependencies given are:</p>\n\n<p><code> Id ---> Name FavouriteAdvisorId </code></p>\n\n<p><code> AdvisorId ---> AdvisorName </code></p>\n\n<p>Why is this?</p>\n', 'ViewCount': '52', 'Title': 'Functional Dependencies and BCNF', 'LastActivityDate': '2014-05-02T20:04:14.127', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10224', 'Tags': '<database-theory>', 'CreationDate': '2014-01-31T18:21:33.483', 'Id': '20166'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have the following schema: <code> Drinkers(name, addr, phone, beersLiked) </code></p>\n\n<p>Also suppose $\\text{name} \\twoheadrightarrow \\text{phone}$. Consider the following tuples:</p>\n\n<p><code> Sue a p1 b1 </code></p>\n\n<p><code> Sue a p2 b2 </code></p>\n\n<p>What other tuples must also be in <code> Drinkers </code>? Would it be:</p>\n\n<p><code> Sue a p2 b1 </code></p>\n\n<p><code> Sue a p1 b2 </code></p>\n', 'ViewCount': '12', 'ClosedDate': '2014-02-01T15:24:17.207', 'Title': 'Multvalued Dependencies', 'LastActivityDate': '2014-02-01T04:00:37.533', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10224', 'Tags': '<database-theory>', 'CreationDate': '2014-02-01T04:00:37.533', 'Id': '20176'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am confused about clustered index is dense or sparse. I searched for that, sources saying it is sparse, but can be dense also. What is it exactly?</p>\n', 'ViewCount': '51', 'Title': 'Clustered index is dense or sparse?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T14:25:23.200', 'LastEditDate': '2014-02-06T10:26:50.050', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23079', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12051', 'Tags': '<terminology><database-theory>', 'CreationDate': '2014-02-06T08:46:31.187', 'Id': '21365'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to display distinct rows from single column without using DISTINCT keyword and [having count(*) > 1 ] ?  My question is can we do this using sub-queries or co-related sub query ?\nIf yes how we can do this ?</p>\n\n<p>Please let me know.\nThanks.</p>\n', 'ViewCount': '18', 'ClosedDate': '2014-03-03T03:13:58.500', 'Title': 'How to display distinct rows from single column without using DISTINCT keyword and count() function?', 'LastActivityDate': '2014-03-02T17:16:35.497', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12467', 'Tags': '<database-theory><databases>', 'CreationDate': '2014-03-02T17:16:35.497', 'Id': '22199'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say we have the following simple transaction-schedule:</p>\n\n<pre><code> T1  |  T2  |  T3\n-----+------+-----\n w(x)|      |\n     | w(x) |\n     |      | w(x)\n</code></pre>\n\n<p>T1 comes before T2, so in the precedence graph, we draw an arrow from T1 to T2. T2 comes before T3, so we draw an arrow from T2 to T3.</p>\n\n<p>My question is: <em>is it also necessary to draw an arrow from T1 to T3?</em> T1 comes before T3, and the definition of a precedence graph says nothing about not drawing a line from Tx to Ty if there is some arbitrary Tz inbetween.</p>\n\n<p>On the other hand, T1 -> T2 -> T3 implies T1 -> T3, and as the graph gets larger things will get a bit messy. Is it okay to 'reduce' a precedence graph?</p>\n\n<p>If the answer to the above question is yes, here is a follow up - consider the following schedule:</p>\n\n<pre><code> T1  |  T2  |  T3\n-----+------+-----\n w(x)|      |\n w(y)|      |\n     | w(x) |\n     | w(z) | \n     |      | w(y)\n     |      | w(z)\n</code></pre>\n\n<p>In the precedence graph, for the operations on x we draw an arrow from T1 to T2, for y an arrow from T1 to T3, and for z an arrow from T2 to T3. Would you be allowed to omit the arrow 'caused' by element z?</p>\n", 'ViewCount': '31', 'Title': 'Can you reduce a precedence graph or do *all* relevant nodes need to be connected', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-03-26T16:04:21.830', 'LastEditDate': '2014-03-25T23:12:34.890', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16126', 'Tags': '<database-theory><precedence>', 'CreationDate': '2014-03-25T22:17:08.717', 'Id': '23047'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am learning 2-phase locking and its unclear to me how it achieves/guarantees before-or-after atomicity (i.e. serilizability). </p>\n\n<p>I went to the following notes</p>\n\n<p><a href="http://ocw.mit.edu/resources/res-6-004-principles-of-computer-system-design-an-introduction-spring-2009/online-textbook/atomicity_open_5_0.pdf" rel="nofollow">http://ocw.mit.edu/resources/res-6-004-principles-of-computer-system-design-an-introduction-spring-2009/online-textbook/atomicity_open_5_0.pdf</a></p>\n\n<p>on page 9-73 it argues informally:</p>\n\n<p>"Informally, once a transaction has acquired a lock on a data \nobject, the value of that object is the same as it will be when the transaction reaches its \nlock point, so reading that value now must yield the same result as waiting till then to \nread it. Furthermore, releasing a lock on an object that it hasn\u2019t modified must be harm\xad\nless if this transaction will never look at the object again, even to abort. A formal \nargument that two-phase locking leads to correct before-or-after atomicity can be found \nin most advanced texts on concurrency control and transactions"</p>\n\n<p>But I didn\'t really understand the argument and it seems that a formal proof is not trivial or maybe too complicated. So I was seeking an alternate intuitive argument to its correctness. I don\'t really understand why that is the reason but if someone else has a better way of explaining it or a different perspective, it would be greatly appreciated!</p>\n', 'ViewCount': '36', 'Title': 'What is the intuition that 2-phase locking achieves serializability (also known as before-or-after atomicity)?', 'LastActivityDate': '2014-04-23T15:38:18.553', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<database-theory>', 'CreationDate': '2014-03-30T02:21:40.893', 'Id': '23240'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Any schema that satisfies BCNF also satisfies 3NF. But how can this be true if BCNF does not necessarily guarantee dependency preservation, whereas 3NF guarantees dependency preservation?</p>\n', 'ViewCount': '41', 'Title': 'Boyce-Codd Normal Form vs. 3rd Normal Form', 'LastActivityDate': '2014-04-25T17:02:29.343', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16342', 'Tags': '<database-theory><normal-forms>', 'CreationDate': '2014-04-01T03:34:25.710', 'Id': '23309'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the standard problem of BCNF in front of me, in the table hospital we have:</p>\n\n<pre><code>----------------------------------\n| doctor |  department | patient |\n----------------------------------\n| dr.Tom | Paediatric  | Martin  |\n| dr.Eve | Ophthalmo.. | Martin  |\n| dr.Ann | Paediatric  | Sarah   |\n| ...    | ...         | ...     |\n----------------------------------\n</code></pre>\n\n<ul>\n<li>Each patient has a doctor assigned for each different department</li>\n<li>A patient can be in different departments</li>\n<li>Each department can have multiple doctors</li>\n</ul>\n\n<p>Therefore when identifying the candidate keys I get these functional dependences (correct?):</p>\n\n<ul>\n<li>$ patient, doctor \\implies department $ (candidate key)</li>\n<li>$ patient, department \\implies doctor $ (candidate key)</li>\n<li>$ doctor \\implies department $</li>\n</ul>\n\n<p>On a previous exam paper it says: in this scenario would the above BCNF issue have occurred regardless of which candidate key have been chosen as the actual primary key (considering that the normalisation that preceded BCNF - potentially)?</p>\n\n<p>How would you approach such question?</p>\n', 'ViewCount': '23', 'Title': 'Boyce-Codd Normal form and violation issue', 'LastActivityDate': '2014-04-15T18:09:05.010', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16802', 'Tags': '<database-theory><databases>', 'CreationDate': '2014-04-15T18:09:05.010', 'Id': '23823'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>let's consider a table with</p>\n\n<pre><code>carID | hireDate | manufactory | model | custID | custName | outletNo | outletLoc\n</code></pre>\n\n<p>I want to evaluate all the <strong>functional dependencies</strong> to bring in first, second and then third normal form.</p>\n\n<ul>\n<li><p>Functional dependencies</p>\n\n<pre><code>carID,hireDate -&gt; custID\n</code></pre></li>\n<li><p>Partial dependencies</p>\n\n<pre><code>carID-&gt;manufactory, model, outletNo**\n</code></pre></li>\n<li><p>Transitive dependencies</p>\n\n<pre><code>custID-&gt;custName\noutletNo-&gt;outletLoc\n</code></pre></li>\n</ul>\n\n<p>Since a car is in a outlet only I have in the partial dependecies this:</p>\n\n<pre><code>carID-&gt;manufactory, model, outletNo**\n</code></pre>\n\n<p>However this leads to anomalies in insertion (imagine adding a car with no outlet), so should not that be like this?</p>\n\n<pre><code>carID-&gt;manufactory, model\ncarID-&gt;outletNo\n</code></pre>\n\n<p>But isn't this still an normalisation anomaly? </p>\n", 'ViewCount': '65', 'Title': 'Functional dependencies with the same key?', 'LastActivityDate': '2014-04-23T15:04:45.723', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16802', 'Tags': '<database-theory><databases>', 'CreationDate': '2014-04-18T11:55:15.453', 'Id': '23909'}},