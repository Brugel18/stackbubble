{'ViewCount': '639', 'Title': "Are there improvements on Dana Angluin's algorithm for learning regular sets", 'LastEditDate': '2012-04-03T03:49:58.997', 'AnswerCount': '1', 'Score': '21', 'PostTypeId': '1', 'OwnerUserId': '55', 'FavoriteCount': '6', 'Body': '<p>In her 1987 seminal paper Dana Angluin presents a polynomial time algorithm for learning a DFA from membership queries and theory queries (counterexamples to a proposed DFA).</p>\n\n<p>She shows that if you are trying to learn a minimal DFA with $n$ states, and your largest countexample is of length $m$, then you need to make $O(mn^2)$ membership-queries and at most $n - 1$ theory-queries.</p>\n\n<p>Have there been significant improvements on the number of queries needed to learn a regular set?</p>\n\n<hr>\n\n<h3>References and Related Questions</h3>\n\n<ul>\n<li><p>Dana Angluin (1987) "Learning Regular Sets from Queries and Counterexamples", Infortmation and Computation 75: 87-106</p></li>\n<li><p><a href="http://cstheory.stackexchange.com/q/10958/1037">Lower bounds for learning in the membership query and counterexample model</a></p></li>\n</ul>\n', 'Tags': '<algorithms><learning-theory><machine-learning>', 'LastEditorUserId': '55', 'LastActivityDate': '2013-06-16T02:15:47.507', 'CommentCount': '4', 'AcceptedAnswerId': '1021', 'CreationDate': '2012-03-08T01:12:58.203', 'Id': '118'}{'ViewCount': '466', 'Title': 'What are the mathematical prerequisites for adaptive machine learning algorithms?', 'LastEditDate': '2012-09-28T09:36:27.647', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2798', 'FavoriteCount': '2', 'Body': '<p>I am a PhD student in Computer Science who switched his PhD a little bit towards ML algorithms combined with something else... I am an expert in that something else, say image processing, but not an expert in Machine Learning. What should I read/learn to get the ML mathematics right? Especially for adaptive learning algorithms.</p>\n', 'Tags': '<reference-request><machine-learning><education><learning-theory>', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-26T23:17:51.163', 'CommentCount': '2', 'AcceptedAnswerId': '6324', 'CreationDate': '2012-09-12T07:31:52.607', 'Id': '3510'}{'Body': '<p>I have on a few occasions trained neural networks (back propagation networks) with some rather complicated data sets (backgammon positions and OCR). When doing this, it seems that a lot of the work involves trying out different configurations of the networks, in order to find the optimal configuration for learning. Often there is a compromise between small nets that are faster to use/learn, and bigger nets, that are able to represent more knowledge.</p>\n\n<p>Then I wonder if it could be possible to make some networks that are both fast and big. I\'m thinking that at network where every neuron ain\'t fully connected ought to be faster to calculate than nets with full connection on all layers. It could be the training that detected that certain inputs are not needed by certain neurons, and therefore remove those connections. In the same way the training could also involve adding new neurons if some neurons seems to be "overloaded".</p>\n\n<p>Is this something that have been tried out with any success ? Does any classes of networks exists with this kind of behavior ?</p>\n', 'ViewCount': '110', 'Title': 'Adapting neural network', 'LastActivityDate': '2013-04-04T00:39:42.330', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10986', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7516', 'Tags': '<machine-learning><neural-networks><learning-theory>', 'CreationDate': '2013-03-31T15:39:51.687', 'FavoriteCount': '2', 'Id': '10937'}{'Body': '<p>I am confused about the Vapnik-Chervonenkis dimension of a linear separator in 3 dimensions. </p>\n\n<p>In three dimensions, a linear separator would be a plane, and the classification model would be "everything on one side of a plane." </p>\n\n<p>It\'s apparently proved that the VC dimension of linear separators is d+1, so in 3D, its VC dimension is four. That means it should be able to put any set of 1, 2, 3, or 4 points on one side of a plane. </p>\n\n<p>But, what about this case: four coplanar points on a square with opposite corners same adjacent corners different?</p>\n\n<pre>\n+1    -1\n\n-1    +1\n</pre>\n\n<p>This is the case that a line (2-dimensional linear separator) cannot handle, but the 3-dimensional linear separator is supposed to be able to shatter this. But, I can\'t see how you could put two corners on "one side of a plane" because all four points are coplanar. </p>\n\n<p>Could someone explain how a 3-d linear separator can shatter the four points I just described? </p>\n', 'ViewCount': '90', 'Title': 'VC dimension of linear separator in 3D', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T07:43:51.953', 'LastEditDate': '2013-04-25T07:43:51.953', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7884', 'Tags': '<statistics><learning-theory><vc-dimension><classification>', 'CreationDate': '2013-04-25T03:25:36.933', 'Id': '11548'}{'Body': '<p>The <a href="http://rjlipton.wordpress.com/2009/06/04/the-junta-problem/">junta problem</a> is the following: we have a boolean function $f:\\{0,1\\}^n \\to \\{0,1\\}$ that actually happens to depend on only $k$ of its input variables.  Given the value of $f(x)$ for many random values of $x$, we want to identify the $k$ input variables that $f$ actually depends upon.  In other words, secretly $f(x_1,\\dots,x_n) = g(x_{i_1},\\dots,x_{i_k})$ for some indices $i_1,\\dots,i_k$; given many pairs $(x,f(x))$ where each $x$ is random, we want to find the $i_1,\\dots,i_k$.</p>\n\n<p>I am interested in a variant of the junta problem, where we are allowed membership queries.  In particular, at any point, we can choose a value $x$ and receive the value of $f(x)$.  The goal remains the same: We want to learn the junta, i.e., learn the indices $i_1,\\dots,i_k$.</p>\n\n<p>How many membership queries are needed, and what is the running time to learn the junta?</p>\n\n<p>There is a simple, straightforward algorithm that uses something like $O(n)$ membership queries (first find $x,y$ such that $f(x)\\ne f(y)$, then move from $x$ to $y$ by changing one bit of the input at a time, to identify $x\',y\'$ such that $f(x\')\\ne f(y\')$ and $x\',y\'$ differ in a single bit position).  But can it be done with many fewer membership queries?  For instance, can we learn the junta with, say, $O(\\lg n)$ membership queries?</p>\n', 'ViewCount': '94', 'Title': 'Learning juntas, with membership queries', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-12T15:23:10.917', 'LastEditDate': '2013-09-09T10:27:33.200', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><machine-learning><learning-theory>', 'CreationDate': '2013-09-08T03:20:12.577', 'Id': '14206'}{'Body': '<p>Given a hypothesis $h:X\\rightarrow Y$ ($h$ is returned by an Empirical Risk Minimization (ERM) strategy with realizable case i.e. $h$ is consistent with the sample examples) over $X=[0,1]\\subseteq R$ where $Y=\\{0,1\\}$ and $D$ is the uniform distribution over $X$. How can I prove that for the accuracy parameter $0\\leq\\epsilon\\leq 1$ we might get $err_D(h)&gt;\\epsilon$? where $err_D(h)$ is the true error of $h$ in the distribution $D$.  </p>\n\n<p><strong>This is a homework</strong> and all I can do is using probabilities (as its shown in the course slides and other sources) to give some bounds on the error (i.e. $P(err_D(h)&gt; \\epsilon)\\leq k(1-\\epsilon)^m$) where $k$ is the number of <em>bad</em> hypotheses $|H_B|$ s.t. ($\\bar{h}\\in H_B | err_D(\\bar{h})&gt;\\epsilon)$ and $m$ is the sample size.  </p>\n\n<p>MY question is: is there any other way (beside using probabilities) to prove this? It just seems odd to me when the question asks for a prove and the answer is about probability bound. I am new to the field of learning theory and this might seems like a silly question but I am really trying to understand the intuition behind using probabilities. </p>\n', 'ViewCount': '62', 'Title': 'proving the error bound for a hypothesis', 'LastEditorUserId': '4598', 'LastActivityDate': '2013-09-30T05:07:42.513', 'LastEditDate': '2013-09-29T08:21:38.313', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '14693', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<machine-learning><learning-theory>', 'CreationDate': '2013-09-29T06:56:04.283', 'Id': '14670'}{'ViewCount': '63', 'Title': 'How to determine the size of training data using VC dimension?', 'LastEditDate': '2013-11-25T21:24:28.870', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11576', 'FavoriteCount': '0', 'Body': u'<p>I want to determine the size of training data ($m$) when I know the parameters $VC(H)$, $\u03b4$ and $e$. As I know the $VC$ bound satisfy this equation:</p>\n\n<p>$$ \\mathrm{error}_{\\mathrm{true}}(h) \\le \\mathrm{error}_{\\mathrm{train}}(h) + \\sqrt\\frac{VC(H) \\times \\ln\\left(\\frac{2m}{VC(H)} + 1\\right) + \\ln(4\u03b4)}m\n$$</p>\n\n<p>but how can I determine the size of training data ($m$) if I know the others?</p>\n', 'Tags': '<machine-learning><learning-theory><vc-dimension>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-26T13:09:12.277', 'CommentCount': '0', 'AcceptedAnswerId': '18383', 'CreationDate': '2013-11-23T14:40:08.877', 'Id': '18276'}{'Body': '<p>I am new to statistical learning. I have a structure $X$ where I showed its hypothesis class $H$ has VC dimension $d$. All I know now is that I can bound the number of examples by $m\\geq \\frac{1}{\\epsilon}ln \\frac{d}{\\delta}$ and with probability at least $1-\\delta$ I will get a hypothesis with error at most $\\epsilon$. </p>\n\n<p>My question concerns what is usually the next step(s),with regard to the big picture of learning a structure $X$, after showing its VCD? </p>\n\n<p>I thought about studying other complexity measures for $X$ but wish to hear others suggestions. </p>\n', 'ViewCount': '90', 'Title': 'What is usually the next step after showing the VC dimension?', 'LastActivityDate': '2014-01-23T00:00:54.697', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<machine-learning><learning-theory><vc-dimension>', 'CreationDate': '2014-01-22T04:40:35.833', 'FavoriteCount': '1', 'Id': '19887'}{'Body': '<p>Let $X_1,\\dots,X_n$ be $n$ boolean variables.  I have an unknown predicate $P(X_1,\\dots,X_n)$ on these boolean variables.  Of course, I can view the predicate as a function $f_P : \\{0,1\\}^n \\to \\{0,1\\}$ that maps a vector of $n$ boolean values to the truth value of this predicate on those inputs.</p>\n\n<p>Now I have a truth table of pairs $(x_1,y_1), \\dots, (x_m,y_m)$, and I want to find a predicate $P$ that is consistent with these pairs and that is as "simple" as possible.  In particular, I have two variants of the problem:</p>\n\n<p><strong>Problem 1.</strong> Given $(x_1,y_1), \\dots, (x_m,y_m)$, find a predicate $P$ such that (1) it agrees with the entire truth table (i.e., for all $i$, $f_P(x_i)=y_i$), and (2) out of all such predicates, the complexity of $P$ is minimized.</p>\n\n<p><strong>Problem 2.</strong> Given $(x_1,y_1), \\dots, (x_m,y_m)$ and a threshold $t$, find a predicate $P$ such that (1) $P$ agrees with at least a $t/m$ fraction of the truth table (i.e., there are at least $t$ values of $i$ such that $f_P(x_i)=y_i$), and (2) out of all such predicates, the complexity of $P$ is minimized.</p>\n\n<p>Are there any algorithms for solving either of these problems, in a way that is more efficient than enumerating all predicates?</p>\n\n<p>Of course, to make the problem well-posed, we must agree on a definition of the complexity of a predicate.  Here I can see any number of realistic complexity metrics.  One metric might be that, when we express $P$ as a formula in boolean logic, the length of that formula.  Another might be the number of operators in that formula, or the nesting depth of the formula.  I am interested in any and all algorithms for any plausible notion of complexity.</p>\n\n<p>This can be viewed as a kind of learning problem, where Occam\'s razor suggests that low-complexity predicates are a priori more likely than high-complexity predicates.</p>\n', 'ViewCount': '30', 'Title': 'Boolean formula that agrees with most truth assignments', 'LastActivityDate': '2014-02-28T03:52:00.653', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<optimization><logic><formal-methods><learning-theory><boolean-algebra>', 'CreationDate': '2014-02-28T03:52:00.653', 'FavoriteCount': '1', 'Id': '22122'}