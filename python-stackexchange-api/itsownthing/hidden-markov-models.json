{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was interested on evaluating a catalogue that students would be using to observe how is it being used probabilistically. </p>\n\n<p>The catalogue works by choosing cells in a temporal sequence, so for example:</p>\n\n<ul>\n<li>Student A has: ($t_1$,$Cell_3$),($t_2$,$Cell_4$)</li>\n<li>Student B has: $(t_1,Cell_5),(t_2,Cell_3),(t_3,Cell_7)$. </li>\n</ul>\n\n<p>Assume that the cells of the table are states of a <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="nofollow">Hidden Markov Model</a>, so the transition between states would map in the real world to a student going from a given cell to another.</p>\n\n<p>Assuming that the catalogue is nothing more than guidance, it is expected to have a certain kind of phenomenon to occur on a given artifact. Consider this artifact to be unique, say, for example a program. </p>\n\n<p>What happens to this program is a finite list of observations, thus, for a given cell we have a finite list of observations for following the suggestion mentioned on that cell. On a HMM this would be then the probability associated to a state to generate a given observation in this artifact. </p>\n\n<p>Finally, consider the catalogue to be structured in a way that initially it is expected that the probability to start in a given cell is equal. The catalogue does not suggest any starting point. </p>\n\n<ul>\n<li><p><strong>Question 1</strong>: Is the mapping between the catalogue and the HMM appropriate?</p></li>\n<li><p><strong>Question 2</strong>: Assuming question 1 holds true. Consider now that we train the HMM using as entries $(t_1,Cell_1), (t_2,Cell_3) , ... (t_n,Cell_n)$ for the students. Would the trained HMM, once asked to generate the transition between states that it is most likely yields as result what is the most used way by the people who used the catalogue for a given experiment $\\epsilon$? </p></li>\n</ul>\n', 'ViewCount': '74', 'Title': 'Is it viable to use an HMM to evaluate how well a catalogue is used?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T02:04:32.233', 'LastEditDate': '2012-04-22T16:09:37.950', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1132', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '983', 'Tags': '<probability-theory><empirical-research><modelling><hidden-markov-models>', 'CreationDate': '2012-04-07T23:04:00.013', 'Id': '1122'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'d like to understand the <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" rel="nofollow">Baum-Welch algorithm</a>. I liked <a href="http://www.youtube.com/watch?v=7zDARfKVm7s&amp;feature=related" rel="nofollow">this video</a> on the Forward-Backward algorithm so I\'d like a similar one for Baum-Welch.</p>\n\n<p>I\'m having trouble coming up with good resources for Baum-Welch. Any ideas?</p>\n', 'ViewCount': '275', 'Title': 'Any very user friendly resources on the Baum-Welch algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-29T18:34:46.793', 'LastEditDate': '2012-05-28T23:52:50.387', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '3364', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1680', 'Tags': '<algorithms><reference-request><hidden-markov-models>', 'CreationDate': '2012-05-28T23:43:11.757', 'Id': '2149'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to find the most probable path (i.e., sequence of states) on an hidden Markov model (HMM) using the Viterbi algorithm. However, I don't know the transition and emission matrices, which I need to estimate from the observations (data).</p>\n\n<p>To estimate these matrices, which algorithm should I use: the Baum-Welch algorithm or the Viterbi Training algorithm? Why?</p>\n\n<p>In case I should use the Viterbi training algorithm, can anyone provide me a good pseudocode (it's not easy to find) ?</p>\n", 'ViewCount': '370', 'Title': 'Viterbi training vs. Baum-Welch algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-01-25T04:03:04.310', 'LastEditDate': '2012-11-14T14:20:41.380', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4590', 'Tags': '<algorithms><hidden-markov-models>', 'CreationDate': '2012-11-14T12:46:59.310', 'Id': '6664'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to understand the details regarding using <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model" rel="nofollow">Hidden Markov Model</a> in Tagging Problem.</p>\n\n<p>The best concise description that I found is the <a href="http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf" rel="nofollow">Course notes by Michal Collins</a>.</p>\n\n<p>The goal is to find a function $f(x)=arg max_{y \\in Y} p(y|x)$, where $y$ is the tag set for sentence $x$. </p>\n\n<p><strong>Question 1</strong>. It\'s suggested to use a generative model and to estimate joint probability $p(x,y)$ from the trainig examples, however what the the reason to use generative model and increase the number of computation why not directly to estimate $p(y|x)$, I think it\'s possible to estimate the conditional probability straightforward from the training data.</p>\n\n<p><strong>Addendum</strong>. Do you know the reason why at all we should try to use a generative model in this case (POS tagging). As I  understand if we can estimate $p(x,y)$ that exactly with  the same success we can estimate $p(y|x)$  and directly find the answer to the question, what is the best tagging - $\\hat{y}$ without weak assumption of generative model. There is the reason to use generative model, and I don\'t see it yet. Can you explain me what the reason?</p>\n\n<p><strong>Question 2.</strong> Assume we decided to use a generative model and made estimation to $p(x,y)$ why we decide to decompose it as follows $p(x,y)=p(y)p(x|y)$ and not  $p(x,y)=p(x)p(y|x)$? </p>\n\n<p><strong>Addendum</strong>. I do understand that it\'s very logical to use the decomposition $p(y)p(x|y)$ just because by doing it we approach $p(y|x)$, so mathematically it seems very reasonable, however according to the task I don\'t see what the problem to decompose it like $p(x,y)=p(x)p(y|x)$, there should be sore reason why we can not decompose it so and I don\'t understand why.</p>\n\n<p>I appreciate your help.</p>\n', 'ViewCount': '74', 'Title': 'Hidden Markov Model in Tagging Problem', 'LastEditorUserId': '8473', 'LastActivityDate': '2013-11-07T13:51:13.720', 'LastEditDate': '2013-11-07T13:51:13.720', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '16779', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Tags': '<machine-learning><natural-lang-processing><hidden-markov-models>', 'CreationDate': '2013-11-06T20:02:49.927', 'Id': '16777'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I was looking into how a <strong>next-word prediction engine</strong> like swift key or XT9 can be implemented.</p>\n\n<p>Here's what I did.</p>\n\n<ul>\n<li>I read about <strong>n-grams</strong> here - en.wikipedia.org/wiki/N-gram and aicat.inf.ed.ac.uk/entry.php?id=663</li>\n<li>I read about <strong>Language Models/Markov Model/n-grams/training/Smoothing/Back-Offs</strong> - en.wikipedia.org/wiki/Language_model &amp; www.stanford.edu/class/cs124/lec/languagemodeling.pptx\u200e &amp; www.statmt.org/book/slides/07-language-models.pdf.</li>\n<li>I read about the T9 engine design for next-word prediction based on Tries - courses.cs.washington.edu/courses/cse303/09wi/homework/T9files/T9_Tries.pdf</li>\n<li>I came across <strong>SRILM</strong>, a popular toolkit for building &amp; applying Language Models here - www.speech.sri.com/projects/srilm/ (the toolkit) &amp; www.speech.sri.com/cgi-bin/run-distill?papers/icslp2002-srilm.ps.gz (the documentation)</li>\n<li>I came across the blog where <strong>Google</strong>'s Peter Norvig made an announcement to <strong>share it's huge training corpus of one trillion words</strong> to the entire world - googleresearch.blogspot.in/2006/08/all-our-n-gram-are-belong-to-you</li>\n<li>I came across an <strong>n-gram viewer</strong> based on google books' corpus - books.google.com/ngrams/</li>\n<li>I came across Microsoft's N-gram services - web-ngram.research.microsoft.com/</li>\n<li>I came across <strong>an algorithm for N-Gram Language Models which is as fast as but smaller (in memory footprint) than SRILM's model</strong> (not based on tries, uses encoding) - nlp.cs.berkeley.edu/pubs/Pauls-Klein_2011_LM_paper.pdf (I need to do more work here.)</li>\n<li>I had a look at some <strong>open-source engines</strong> available like <strong>AnySoftKeyboard</strong> - github.com/AnySoftKeyboard. That's is a huge amount of code with no documentation!</li>\n</ul>\n\n<p>Some discussions on <strong>stackoverflow</strong>:</p>\n\n<ul>\n<li>Implementing T9 prediction engine - Implementing T9 text prediction</li>\n<li>A discussion on implementation of autocomplete using tries vs. ternary search trees vs. succint trees - stackoverflow.com/questions/10970416/tries-versus-ternary-search-trees-for-autocomplete</li>\n</ul>\n\n<p>The <strong>major players</strong> in this area:</p>\n\n<ul>\n<li><strong>Swift Key</strong> - en.wikipedia.org/wiki/SwiftKey &amp; www.swiftkey.net/en/</li>\n<li><strong>XT9 by Nuance</strong> - en.wikipedia.org/wiki/XT9 &amp; www.nuance.com/for-business/by-product/xt9/index.htm</li>\n</ul>\n\n<p>Can anybody guide me how to proceed further.</p>\n\n<p>I am relatively new to this site. So please guide me if my question is inappropriate for this site.</p>\n", 'ViewCount': '207', 'Title': 'Next-Word Prediction, Language Models, N-grams', 'LastEditorUserId': '11628', 'LastActivityDate': '2013-11-26T08:15:40.323', 'LastEditDate': '2013-11-26T08:15:40.323', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11628', 'Tags': '<artificial-intelligence><natural-lang-processing><hidden-markov-models>', 'CreationDate': '2013-11-26T05:36:16.227', 'FavoriteCount': '1', 'Id': '18354'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Next-word prediction or phrase-prediction engines used in modern keyboards of mobiles and tablets, like swift key &amp; XT9, which predict the next word the user is going to type based on some pre-defined or dynamic corpus, based on n-grams (maximum frequency of last typed 2-3 words plus the current word) based language models (Markov Model).</p>\n\n<p>What I think is that these engines/algos are a part of AI/NLP.\nBut what I am not sure about is what specific branch of AI/NLP they belong to.</p>\n\n<p>Is it machine learning ?\nIs it data science ?\nIs it big data ?\nIs it Computing Intelligence ?\nIs it decision-making ?\nIs it data-mining ?\nOr statistical pattern recognition/ predictive analytics/ Supervised learning/ Unsupervised learning ?\nOr all/many of these or something else ?</p>\n', 'ViewCount': '55', 'ClosedDate': '2013-12-03T17:35:15.933', 'Title': 'Next-Word Prediction Engines - which branch of AI do they belong', 'LastEditorUserId': '11628', 'LastActivityDate': '2013-11-26T16:09:39.763', 'LastEditDate': '2013-11-26T16:09:39.763', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11628', 'Tags': '<machine-learning><artificial-intelligence><natural-lang-processing><hidden-markov-models>', 'CreationDate': '2013-11-26T15:39:28.363', 'Id': '18387'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a question regarding recursion in <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm" rel="nofollow">Viterbi algorithm</a>.</p>\n\n<p>Define $\\pi(k; u; v)$  which is the maximum probability for any sequence of length $k$, ending in the tag bigram $(u; v)$.</p>\n\n<p>The base case if obvious $\\pi(0,*,*)=1$</p>\n\n<p>The general case.</p>\n\n<p>$\\pi(k,u,v) = max_{w \\in K_{k-2} } \\pi(k-1,w,u) \\cdot q(v|w,u) \\cdot e(x_k|v)$</p>\n\n<p>The author justifies the recursion as folllows: </p>\n\n<blockquote>\n  <p>How can we justify this recurrence? Recall that $\\pi(k, u, v)$ is the highest probability for any sequence $y_{\u22121}...y_k$ ending in the bigram $(u, v)$. Any such sequence must have $y_{k\u22122} = w$ for some state $w$. The highest probability for any sequence of length $k \u2212 1$ ending in the bigram $(w, u)$ is $\\pi(k \u2212 1, w, u)$, hence the highest probability for any sequence of length $k$ ending in the trigram $(w, u, v)$ must be $\\pi(k \u2212 1,w, u) \\cdot q(v|w, u) \\cdot e(x_k |v)$</p>\n</blockquote>\n\n<p>I do not understand why it\'s actually true, I think it\'s possible to reach $\\pi(n,u, v)$ from any $(n-1,w, u)$ not actually the maximum one $\\pi(n-1,w, u)$ just because $q(v|w, u) \\cdot e(x_k |v)$ might have a higher influence on the resulting $(n,u, v)$ than any $\\pi(n-1,w, u)$.</p>\n\n<p>I would appreciate if anyone could explain me why it\'s true.</p>\n', 'ViewCount': '94', 'Title': 'Viterbi algorithm recursive justification', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-04T18:05:36.053', 'LastEditDate': '2014-02-04T18:05:36.053', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19109', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Tags': '<algorithms><dynamic-programming><recursion><correctness-proof><hidden-markov-models>', 'CreationDate': '2013-12-18T13:44:43.103', 'Id': '19093'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is a famous <a href="http://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="nofollow">part-of-speech tagging problem</a> in Natural Language Processing. The popular solution is to use <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model" rel="nofollow">Hidden Markov Models</a>.</p>\n\n<p>So that, given the sentence $x_1 \\dots x_n$ we want to find the sequence of POS tags $y_1 \\dots y_n$ such that $y_1 \\dots y_n = \\arg\\max_{y_1 \\dots y_n}p(Y,X)$.</p>\n\n<p>By Bayes Theorem, $P(X,Y)=P(Y)P(X \\mid Y)$.</p>\n\n<p>Solving POS by HMM implies the assumptions: $p(y_i \\mid y_{i-1})$ and $p(x_i \\mid y_i)$.</p>\n\n<p>The question is there are any particular reason why we prefere to solve it by generative model with a lot of assumption and not directly by estimating $P(Y \\mid X)$, given the training corpus it\'s still possible to estimate $p(y_i \\mid x_i)$.</p>\n\n<p>The second question, even when we convinced that the generative model is preferred why to calculate is as $P(Y,X)=P(Y)P(X \\mid Y)$ and not $P(X,Y)=P(X)P(Y \\mid X)$. In case we have an appropriate generative story I can use $P(X,Y)=P(X)P(Y \\mid X)$ as well, is it mentioned somewhere that assumed generative story is preferred.</p>\n', 'ViewCount': '35', 'Title': 'Solving the part-of-speech tagging problem with HMM', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-01T16:41:25.817', 'LastEditDate': '2014-02-01T15:46:40.370', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '20190', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Tags': '<machine-learning><natural-lang-processing><hidden-markov-models>', 'CreationDate': '2014-02-01T13:49:50.607', 'Id': '20185'}},