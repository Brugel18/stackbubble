510:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In a <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=4S-sx5_cmLU#!" rel="nofollow">video</a> discussing the merits of <a href="http://en.wikipedia.org/wiki/Particle_filter" rel="nofollow">particle filters</a> for localization, it was implied that there is some ambiguity about the complexity cost of particle filter implementations.  Is this correct?  Could someone explain this?</p>\n', 'ViewCount': '138', 'Title': 'What Is The Complexity of Implementing a Particle Filter?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-08T16:27:23.470', 'LastEditDate': '2012-03-08T16:27:23.470', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '133', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '65', 'Tags': '<computational-geometry><knowledge-representation><reasoning><statistics>', 'CreationDate': '2012-03-08T02:38:08.510', 'Id': '122'},511:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2545', 'Title': 'How are statistics being applied in computer science to evaluate accuracy in research claims?', 'LastEditDate': '2014-02-08T00:50:08.267', 'AnswerCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '983', 'FavoriteCount': '3', 'Body': '<p>I have noticed in my short academic life that many published papers in our area sometimes do not have much rigor regarding statistics. This is not just an assumption; I have heard professors say the same. </p>\n\n<p>For example, in CS disciplines I see papers being published claiming that methodology X has been observed to be effective and this is proved by ANOVA and ANCOVA, however I see no references for other researchers evaluating that the necessary constraints have been observed. It somewhat feels like as soon as some \'complex function and name\' appears, then that shows the researcher is using some highly credible method and approach that \'he must know what is he doing and it is fine if he does not describe the constraints\', say, for that given distribution or approach, so that the community can evaluate it. </p>\n\n<p>Sometimes, there are excuses for justifying the hypothesis with such a small sample size. </p>\n\n<p>My question here is thusly posed as a student of CS disciplines as an aspirant to learn more about statistics: How do computer scientists approaches statistics? </p>\n\n<p>This question might seems like I am asking what I have already explained, but that is my <em>opinion</em>. I might be wrong, or I might be focusing on a group of practitioners whereas other groups of CS researchers might be doing something else that follows better practices with respect to statistics rigor. </p>\n\n<p>So specifically, what I want is "Our area is or is not into statistics because of the given facts (papers example, books, or another discussion article about this are fine)". @Patrick answer is closer to this. </p>\n', 'Tags': '<software-engineering><empirical-research><statistics>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-08T17:39:10.957', 'CommentCount': '4', 'AcceptedAnswerId': '1101', 'CreationDate': '2012-04-07T01:53:58.053', 'Id': '1100'},512:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>A Naive Bayes predictor makes its predictions using this formula:</p>\n\n<p>$$P(Y=y|X=x) = \\alpha P(Y=y)\\prod_i P(X_i=x_i|Y=y)$$</p>\n\n<p>where $\\alpha$ is a normalizing factor. This requires estimating the parameters $P(X_i=x_i|Y=y)$ from the data. If we do this with $k$-smoothing, then we get the estimate</p>\n\n<p>$$\\hat{P}(X_i=x_i|Y=y) = \\frac{\\#\\{X_i=x_i,Y=y\\} + k}{\\#\\{Y=y\\}+n_ik}$$</p>\n\n<p>where there are $n_i$ possible values for $X_i$. I'm fine with this. However, for the prior, we have</p>\n\n<p>$$\\hat{P}(Y=y) = \\frac{\\#\\{Y=y\\}}{N}$$</p>\n\n<p>where there are $N$ examples in the data set. Why don't we also smooth the prior? Or rather, <em>do</em> we smooth the prior? If so, what smoothing parameter do we choose? It seems slightly silly to also choose $k$, since we're doing a different calculation. Is there a consensus? Or does it not matter too much?</p>\n", 'ViewCount': '1530', 'Title': 'Smoothing in Naive Bayes model', 'LastEditorUserId': '88', 'LastActivityDate': '2012-08-08T02:02:36.983', 'LastEditDate': '2012-08-08T02:02:36.983', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2341', 'Tags': '<machine-learning><probability-theory><statistics>', 'CreationDate': '2012-08-02T15:47:28.573', 'FavoriteCount': '1', 'Id': '3005'},513:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We have a set X of N elements. We want to get a new set X' having a size M &lt; N.</p>\n\n<pre><code>Choose a first element x from X and put it in X'\nfor each element x in (X - X')\n  Let x' the element from X' which is the closest to x (that is x' = argmin distance(x, x') for all x' in X')\n  d = distance(x, x')\n  if ( uniform_random([0,1]) &lt; d / f )\n     add x to X'\n</code></pre>\n\n<p>How can I choose the value f such that the size of the set X' at the end will be for instance the half of the size of X (that is, M approximates or equals N/2). I suppose that I should choose f such that the probability d / f equals 1/2 (or approximates 1/2 for most values of d), but how to do that ?</p>\n\n<p>Additional details (that are not necesarily usefull for this question): the elements are actually vectors, and the distance between two vectors is the euclidean distance.</p>\n\n<p>Note that d is not a constant (while f is a constant that I want to fix). d depends on the distance between each element x and its closest element x', so d is not always the same.</p>\n\n<p>Suppose that the order in which we test the elements x is always random. For any set X, if we choose the value of f relatively small then we will get a relatively hight number of elements in the final set X', if we choose the value of f relatively big we will get a relatively small number of elements in the final set X'. If I experimentally vary the value of f many times I can always (for any set X) find a value of f for which the final number of elements in X' approaches N/2. So experimentally I can find a good value for f if I test many times which different values of f, but I want to determine it heuristically (not by testing many times and varying f).</p>\n\n<p><strong>EDIT:</strong>\nBy the way, the only one method which seems to give an acceptable results is: let mean_d the mean distance of each x to its nearest x'. We put f = 2mean_d, thus the probability d/f = d/(2mean_d) usually approximate 1/2 if the most of distances d are not far from mean_d. We also put f = (2mean_d)+d' where d' depends on how many distances are higher than mean_d, or f = (2mean_d)-d' where d' depends on how many distances are less than mean_d. Does this make sense ? Do you think it can be improved ?</p>\n", 'ViewCount': '36', 'Title': 'Heuristically determine a value f such that a probability d/f approaches 1/2', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-21T13:07:23.583', 'LastEditDate': '2012-09-21T13:07:23.583', 'AnswerCount': '3', 'CommentCount': '10', 'Score': '0', 'OwnerDisplayName': 'user995434', 'PostTypeId': '1', 'OwnerUserId': '2895', 'Tags': '<probability-theory><mathematical-analysis><heuristics><statistics>', 'CreationDate': '2012-09-17T18:57:42.773', 'Id': '4631'},514:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '155', 'Title': 'Reconstructing a data table from cross-tabulation frequencies', 'LastEditDate': '2013-04-05T21:42:23.620', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2847', 'FavoriteCount': '1', 'Body': '<p>Say there is a data table $D$ that we cannot see, with $M$ columns. We are given exact <a href="https://en.wikipedia.org/wiki/Cross_tabulation" rel="nofollow">cross-tabulation</a> frequencies for all ${M \\choose 2}$ pairs of columns, that is how often each combination of two values occurs.</p>\n\n<p>From the cross-tabulations, we can derive a set of possible rows $R$ of $D$ and maximum frequencies for each possible row.</p>\n\n<p>How can we reconstruct the original table $D$? If there is not enough information to do so, how can we construct a possible table $D\'$ that has the same cross-tab frequencies? In this case, is it possible to count the number of possible tables?</p>\n\n<p>(Edit: As Vor noted, define a table as an unordered collection of rows. A permutation of the rows of a table yields the same table.)</p>\n\n<hr>\n\n<p>For example, if $D$ has rows:</p>\n\n<pre><code>X  A  j\nY  A  k\nX  B  j\nX  B  j\n</code></pre>\n\n<p>We have three sets of cross-tab frequencies:</p>\n\n<pre><code>X  A  1\nX  B  2\nY  A  1\nY  B  0\n\nX  j  3\nX  k  0\nY  j  0\nY  k  1\n\nA  j  1\nA  k  1\nB  j  2\nB  k  2\n</code></pre>\n\n<p>I would like an algorithm which will take the cross-tab frequencies as input and output the original table or a possible original table.</p>\n', 'Tags': '<algorithms><complexity-theory><combinatorics><statistics>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-07T14:24:45.853', 'CommentCount': '5', 'AcceptedAnswerId': '11102', 'CreationDate': '2012-09-28T20:16:24.500', 'Id': '4784'},515:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am looking for an efficient algorithm to find streaming data median. <a href="http://en.wikipedia.org/wiki/Median" rel="nofollow">Median</a> is described as the numerical value separating the higher half of a sample, a population, or a probability distribution, from the lower half. </p>\n\n<p>We have stream of data in our system like 1, 10, -40, 20, 2, 6,.....Our task is find median of data as they arrive.</p>\n', 'ViewCount': '186', 'Title': 'Streaming Median', 'LastActivityDate': '2013-02-16T02:04:01.783', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><statistics>', 'CreationDate': '2013-02-15T17:42:13.830', 'Id': '9816'},516:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m looking for some algorithms that detect statistical anomaly in time series. </p>\n\n<p>For example, Google Trend automatically detects peaks of a specific search query in time, and associates those peaks with most popular news stories at that time.\nSee this for example: <a href="http://www.google.com/trends/explore#q=titan" rel="nofollow">http://www.google.com/trends/explore#q=titan</a></p>\n\n<p>I\'m looking for this kind of algorithms that detect those peaks, or just anomaly in general.</p>\n\n<p>I can think of a simple way to do it, i.e. using short-run EMA minus long-run EMA as a signal. But I\'m sure people have done more comprehensive analysis of tasks of this kind, and I want to learn more about that. Thanks for any pointers!</p>\n', 'ViewCount': '40', 'Title': 'Statistical anomaly detection in time series', 'LastActivityDate': '2013-02-26T08:15:12.427', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7041', 'Tags': '<statistics>', 'CreationDate': '2013-02-26T08:15:12.427', 'Id': '10116'},517:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am reading through PCA and it says that the maximum variance principal component has most of the information. Can we apply that to any data set? If a data set has n attributes and most of the attributes show high variance then can we infer that the dataset has captured lot of useful information? </p>\n\n<p>I am trying to understand how a high variance dataset contains useful information?</p>\n', 'ViewCount': '74', 'Title': 'Maximum variance and useful information of dataset', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-06T14:34:08.443', 'LastEditDate': '2013-03-06T07:11:43.823', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '10307', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<terminology><data-mining><data-sets><statistics>', 'CreationDate': '2013-03-05T03:09:00.700', 'Id': '10279'},518:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1994', 'Title': 'Applying Expectation Maximization to coin toss examples', 'LastEditDate': '2013-03-20T11:17:21.013', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7329', 'FavoriteCount': '3', 'Body': '<p>I\'ve been self-studying the Expectation Maximization lately, and grabbed myself some simple examples in the process:</p>\n\n<p>From <a href="http://cs.dartmouth.edu/~cs104/CS104_11.04.22.pdf">here</a>: There are three coins $c_0$, $c_1$ and $c_2$ with $p_0$, $p_1$ and $p_2$ the respective probability for landing on Head when tossed. Toss $c_0$. If the result is Head, toss $c_1$ three times, else toss $c_2$ three times. The observed data produced by $c_1$ and $c_2$ is like this: HHH, TTT, HHH, TTT, HHH. The hidden data is the result of $c_0$. Estimate $p_0$, $p_1$ and $p_2$.</p>\n\n<p>And from <a href="http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf">here</a>: There are two coins $c_A$ and $c_B$ with $p_A$ and $p_B$ being the respective probability for landing on Head when tossed. Each round, select one coin at random and toss it ten times; record the results. The observed data is the toss results provided by these two coins. However, we don\'t know which coin was selected for a particular round. Estimate $p_A$ and $p_B$.</p>\n\n<p>While I can get the calculations, I can\'t relate the ways they are solved to the original EM theory. Specifically, during the M-Step of both examples, I don\'t see how they\'re maximizing anything. It just seems they are recalculating the parameters and somehow, the new parameters are better than the old ones. Moreover, the two E-Steps don\'t even look similar to each other, not to mention the original theory\'s E-Step.</p>\n\n<p>So how exactly do these examples work?</p>\n', 'Tags': '<probability-theory><statistics>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-20T16:09:52.617', 'CommentCount': '4', 'AcceptedAnswerId': '10657', 'CreationDate': '2013-03-20T05:13:18.053', 'Id': '10637'},519:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When clustering a set of data points, what exactly are the differences between <a href="http://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_c-means_clustering" rel="nofollow">Fuzzy C-Means</a> (aka Soft K-Means) and <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="nofollow">Expectation Maximization</a>?</p>\n\n<p>In slide 30 and 32 of <a href="http://eniac.cs.qc.cuny.edu/andrew/gcml-11/lecture10c.pptx" rel="nofollow">this lecture</a> I found, it says that Soft K-Means is a special case of EM in Soft K-Means only the means are re-estimated and not the covariance matrix, why\'s that and what are the advantages / disadvantages? How does covariance matrix affect the outcomes of EM?</p>\n\n<p>Another question about these two algorithms: When they converge, all the data points will be hard-assigned to a particular cluster if the probability of it being in the said cluster is highest, right?</p>\n', 'ViewCount': '444', 'Title': 'Differences between Fuzzy C-Means and EM', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-29T14:41:12.010', 'LastEditDate': '2013-08-29T14:41:12.010', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14017', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7329', 'Tags': '<algorithms><terminology><algorithm-analysis><machine-learning><statistics>', 'CreationDate': '2013-04-04T17:14:03.000', 'Id': '11022'},5110:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I have my end of year exams next Thursday. I'm generally doing fine but I am having some major issues with this strand of my course, this has to be the biggest issue I have. So, here is the question in the past paper:</p>\n\n<blockquote>\n  <p>You have been tasked with determining the validity of a \n    manufacturer\u2019s claim that their widget is more reliable than their main \n    competitors\u2019 widgets.</p>\n  \n  <p>In order to verify this assertion you test a sample of 25 widgets from \n    the manufacturer\u2019s range and find a mean pass rate of 992 per 1000\n    with a standard deviation of 15 per 1000.</p>\n  \n  <p>Previous studies have shown that the mean pass rate of all other \n    widgets on the market is 979.4 per1000.</p>\n  \n  <p>In order to increase the confidence in the making a decision about \n    the null hypothesis you choose a 99.5% confidence level and find the \n    corresponding t-table value to be 2.947.</p>\n  \n  <ul>\n  <li>State the null hypothesis.</li>\n  </ul>\n</blockquote>\n\n<p>I have revision notes in front of me, but I just don't understand what this question is actually asking. Am I just writing down my own hypothesis, or do I need to use this equation?</p>\n\n<blockquote>\n  <p>H0; \u03bc<br>\n    H1; \u03bc</p>\n</blockquote>\n\n<p>If anyone could go through step by step for what I need to do, then that would be fantastic. My notes are just confusing! </p>\n", 'ViewCount': '96', 'Title': 'Null Hypothesis in Analysis and Testing', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-23T17:35:28.500', 'LastEditDate': '2013-04-23T17:35:28.500', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11521', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7857', 'Tags': '<probability-theory><statistics><reliability>', 'CreationDate': '2013-04-23T15:32:38.180', 'Id': '11519'},5111:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have the dataset which you can find <a href="https://dl.dropboxusercontent.com/u/8546316/Dataset.csv" rel="nofollow">here</a>, containing many different characteristics of different houses, including their types of heating, or the number of adults and children living in the house. In total there are about 500 records. I want to use an algorithm, that can be trained using the dataset above, in order to be able to predict the electricity consumption of a house that is not in the set.</p>\n\n<p>I have tried every possible machine learning algorithm (using weka) (linear regression, SVM etc) . However I had about 350 mean absolute error, which is not good. I tried to make my data to take values from 0 to 1, or to delete some characteristics. I did not managed to find some good results.</p>\n\n<p>I also tried to use R tool, and I did not have good results either...</p>\n\n<p>I would be very grateful, if someone could give me some advice, or if you could examine a little the dataset and run some algorithms on it. What type of preprocessing should I use, and what type of algorithm?</p>\n\n<p>I have posted a <a href="http://cs.stackexchange.com/questions/10392/using-the-appropriate-machine-learning-algorithm">similar question</a> last month, but I did not get any useful answers.</p>\n', 'ViewCount': '132', 'Title': 'Predicting energy consumption of households', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-24T06:43:43.053', 'LastEditDate': '2013-04-24T06:43:43.053', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7196', 'Tags': '<algorithms><machine-learning><statistics>', 'CreationDate': '2013-04-23T22:37:26.683', 'Id': '11527'},5112:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am confused about the Vapnik-Chervonenkis dimension of a linear separator in 3 dimensions. </p>\n\n<p>In three dimensions, a linear separator would be a plane, and the classification model would be "everything on one side of a plane." </p>\n\n<p>It\'s apparently proved that the VC dimension of linear separators is d+1, so in 3D, its VC dimension is four. That means it should be able to put any set of 1, 2, 3, or 4 points on one side of a plane. </p>\n\n<p>But, what about this case: four coplanar points on a square with opposite corners same adjacent corners different?</p>\n\n<pre>\n+1    -1\n\n-1    +1\n</pre>\n\n<p>This is the case that a line (2-dimensional linear separator) cannot handle, but the 3-dimensional linear separator is supposed to be able to shatter this. But, I can\'t see how you could put two corners on "one side of a plane" because all four points are coplanar. </p>\n\n<p>Could someone explain how a 3-d linear separator can shatter the four points I just described? </p>\n', 'ViewCount': '90', 'Title': 'VC dimension of linear separator in 3D', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T07:43:51.953', 'LastEditDate': '2013-04-25T07:43:51.953', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7884', 'Tags': '<statistics><learning-theory><vc-dimension><classification>', 'CreationDate': '2013-04-25T03:25:36.933', 'Id': '11548'},5113:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Is it proper to view conditional probabilities, such as the forms:</p>\n\n<p>P(a|c)</p>\n\n<p>P(a|c,d)</p>\n\n<p>P(a, b|c, d)</p>\n\n<p>...and so forth, as being tensors?</p>\n\n<p>If so, does anyone know of a decent introductory text (online tutorial, workshop paper, book, etc) which develops tensors in that sense for computer scientists/machine learning practitioners?</p>\n\n<p>I have found a number of papers, but those written at an introductory level are written for physicists, and those written for computer scientists are rather advanced.  </p>\n', 'ViewCount': '69', 'Title': 'Conditional Probabilities as Tensors?', 'LastActivityDate': '2013-05-17T23:18:12.193', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1940', 'Tags': '<machine-learning><probability-theory><statistics>', 'CreationDate': '2013-05-17T23:18:12.193', 'Id': '12100'},5114:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '373', 'Title': 'Understanding an example of coin toss expectation maximization', 'LastEditDate': '2013-06-24T08:36:24.557', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8784', 'FavoriteCount': '0', 'Body': '<p>I\'ve been trying to get my head around Expectation maximization algorithms, and I thought I\'d start simple. I found this 3-coin example here: <a href="http://cs.dartmouth.edu/~cs104/CS104_11.04.22.pdf" rel="nofollow">http://cs.dartmouth.edu/~cs104/CS104_11.04.22.pdf</a>  I understand the calculation of all of the probabilities but I don\'t understand how the recalculation of lambda, p1 and p2 is actually done. (Page 18)</p>\n\n<p>I understand how the maximization of a log-likelihood/likelihood function by differentiation works, but can\'t figure out the recalculation method here.</p>\n\n<p>Can anyone explain why the recalculation of lambda, p1 and p2 take the form they do? </p>\n', 'ClosedDate': '2013-07-16T10:28:00.093', 'Tags': '<probability-theory><statistics>', 'LastEditorUserId': '8784', 'LastActivityDate': '2013-07-01T21:12:08.380', 'CommentCount': '3', 'AcceptedAnswerId': '13019', 'CreationDate': '2013-06-21T10:42:06.143', 'Id': '12811'},5115:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '130', 'Title': 'distance between histograms', 'LastEditDate': '2013-06-25T19:10:37.723', 'AnswerCount': '2', 'Score': '1', 'OwnerDisplayName': 'Hani Gotc', 'PostTypeId': '1', 'OwnerUserId': '9033', 'Body': '<p>I have 2 histograms that represent the height of characters in 2 images. \nexample: </p>\n\n<ul>\n<li>1 <strong>**</strong></li>\n<li>2 <strong><em>*</em>***</strong></li>\n<li>3 <strong><em>*</em>**<em>*</em></strong></li>\n<li>.</li>\n<li>.</li>\n<li>.</li>\n<li>100 <strong><em>*</em>**<em>*</em>**</strong></li>\n</ul>\n\n<p>For these 2 histograms I compute the peaks. And To <strong>check</strong> if these 2 images are <strong>similar</strong> I compute the interseciton between the indices of the 2 images.\nExample:\nFor image 1 the indices of the peaks are 1, 10 and 13\nfor image 2 the indices of the peals are 1,10, 14.\nImage1 Inter Image1 = 2 So these images are similar.</p>\n\n<p>But I feel that the intersection is not enough. I think that i should also use the size of the buckets of the histograms peaks.</p>\n\n<p>Is there  a way to use to both of them to measure the similarity in <strong>1 function</strong> So that I can have a stable similarity function?</p>\n', 'Tags': '<machine-learning><probability-theory><statistics>', 'LastEditorUserId': '19', 'LastActivityDate': '2013-06-25T19:10:37.723', 'CommentCount': '1', 'AcceptedAnswerId': '12898', 'CreationDate': '2013-06-18T11:00:45.170', 'Id': '12846'},5116:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I recently did work in the area of machine learning for one of my jobs and was able to build a classifier which evaluated to an F score of 85%. I also have access to correctly and incorrectly classified instances. My boss would like to make sure my classifier is good and and test it for statistical significance. I'm not quite sure what he means by this. I have taken a course in statistics quite some time ago where I did hypothesis testing and stuff with confidence intervals. Do you think that's what he means or does he want me to compare it with other algorithms? I don't want to say what kind of data set I am working with but it's comparable to something such as predicting spam or ham for email messages. </p>\n\n<p>Any help or guidance on this question would be greatly appreciated. I am pretty new to the area of computer science research. But am I right in saying that statistics are used quite a lot to evaluate these types of things?</p>\n\n<p>Thank you!</p>\n", 'ViewCount': '29', 'Title': 'Finding Statistical Signifigance for a Classifier', 'LastActivityDate': '2013-10-07T00:11:48.270', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14870', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4336', 'Tags': '<machine-learning><statistics><classification>', 'CreationDate': '2013-10-06T23:28:43.117', 'Id': '14866'},5117:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose I have $n$ independent observations $x_1,\\dots,x_n$ from some unknown distribution over a known alphabet $\\Sigma$, and I want to estimate the entropy of the distribution.  I can count the frequency $f_s$ of each symbol $s \\in \\Sigma$ among the observations; how should I use them to estimate the Shannon entropy of the source?</p>\n\n<hr>\n\n<p>The obvious approach is to estimate the probability of each symbol $s$ as $\\Pr[X=s]=f_s/n$, and then calculate the entropy using the standard formula for Shannon entropy.  This leads to the following estimate of the entropy $H(X)$:</p>\n\n<p>$$\\text{estimate}(H(X)) = - \\sum_{s \\in \\Sigma} {f_s \\over n} \\lg (f_s/n).$$</p>\n\n<p>However, this feels like it might not produce the best estimate.  Consider, by analogy, the problem of estimating the probability of symbol $s$ based upon its frequency $f_s$.  The naive estimate $f_s/n$ is likely an underestimate of its probability.  For instance, if I make 100 observations of birds in my back yard and none of them were a hummingbird, should my best estimate of the probability of seeing a hummingbird on my next observation be exactly 0?  No, instead, it\'s probably more realistic to estimate the probability is something small but not zero.  (A zero estimate means that a hummingbird is absolutely impossible, which seems unlikely.)</p>\n\n<p>For the problem of estimating the probability of symbol $s$, there are a number of standard techniques for addressing this problem.  <a href="https://en.wikipedia.org/wiki/Laplace_smoothing" rel="nofollow">Additive smoothing</a> (aka Laplace smoothing) is one standard technique, where we estimate the probability of symbol $s$ as $\\Pr[X=s] = (f_s + 1)/(n+|\\Sigma|)$.  Others have proposed Bayesian smoothing or other methods.  These methods are widely used in natural language processing and document analysis, where just because a word never appears in your document set doesn\'t mean that the word has probability zero.  In natural language processing, this also goes by the name <a href="https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques" rel="nofollow">smoothing</a>.</p>\n\n<p>So, taking these considerations into account, how should I estimate the entropy, based upon observed frequency counts?  Should I apply additive smoothing to get an estimate of each of the probabilities $\\Pr[X=s]$, then use the standard formula for Shannon entropy with those probabilities?  Or is there a better method that should be used for this specific problem?</p>\n', 'ViewCount': '108', 'Title': 'Estimate entropy, based upon observed frequency counts', 'LastActivityDate': '2013-10-13T18:18:02.167', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<information-theory><statistics><natural-lang-processing><entropy><information-retrieval>', 'CreationDate': '2013-10-11T21:26:30.400', 'Id': '15010'},5118:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m building a load test harness for a distributed system. Currently I\'m using the "<a href="http://www.cs.rutgers.edu/~muthu/bquant.pdf" rel="nofollow">Cormode, Korn, Muthukrishnan, and Srivastava</a>" method to estimate latency quantiles of system responses.</p>\n\n<p>I\'m now testing systems larger than can be adequately stressed using a single load generator. I\'m looking for a method similar to the one cited above that can be extended to a cluster of load generators. </p>\n\n<p>I would like to measure the latency quantiles of responses to the cluster as a whole, and to individual nodes in the cluster without recording/streaming the latency of every event in the system to a central node. </p>\n\n<p>Does such a method exist? If so, are there any public implementations?</p>\n', 'ViewCount': '24', 'Title': 'Efficiently estimating latency quantiles of a distributed system', 'LastActivityDate': '2013-11-18T17:31:59.213', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11457', 'Tags': '<algorithms><distributed-systems><statistics>', 'CreationDate': '2013-11-18T17:31:59.213', 'Id': '18118'},5119:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm using machine-learning algorithms to solve binary classification problem (i.e. classification can be 'good' or 'bad'). I'm using <code>SVM</code> based algorithms, <code>LibLinear</code> in particular. When I get a classification result, sometimes, the probability estimations over the result are pretty low. For example, I get a classification 'good' with probability of 52% - those kind of results I rather throw away or maybe classify them as 'unknown'.</p>\n\n<p><strong>EDITED - by D.W.'s suggestion</strong></p>\n\n<p>Just to be more clear about it, my output is not only the classification 'good' or 'bad', I also get the confidence level (in %). For example, If I'm the weather guy, I'm reporting that tomorrow it will be raining, and I'm 52% positive at my forecast. In this case, I'm sure you won't take your umbrella when you leave home tomorrow, right? So in those cases where my model does not have a high confidence level I throw away this prediction and don't count it in my estimations.</p>\n\n<p>Unfortunately, I can't find articles regarding thresholding the probability estimations... </p>\n\n<p>Does anyone have an idea what is a normal threshold that I can set over the probability estimations? or at least can refer me to a few articles about it?</p>\n", 'ViewCount': '69', 'Title': 'What would be a decent threshold for classification problem?', 'LastEditorUserId': '11754', 'LastActivityDate': '2014-04-20T10:08:52.793', 'LastEditDate': '2013-12-21T08:24:16.580', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11754', 'Tags': '<machine-learning><probabilistic-algorithms><statistics><classification>', 'CreationDate': '2013-12-20T07:53:10.210', 'Id': '19146'},5120:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There is a new <a href="http://quant.stackexchange.com/">Quantitative Finance SE site</a>. However, I am interested in asking the "CS crowd": </p>\n\n<blockquote>\n  <p>What are some interesting key references or surveys on applying algorithms to stock trading analysis?</p>\n</blockquote>\n\n<p>There are many such references, however, I am particularly interested in those that would appeal to those with a CS background. Further, I am especially interested in those that find surprising applications of TCS or mathematics theory.</p>\n', 'ViewCount': '41', 'Title': 'Applications of algorithms to stock trading analysis', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-31T23:16:57.623', 'LastEditDate': '2014-01-31T23:16:57.623', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<reference-request><machine-learning><optimization><statistics>', 'CreationDate': '2014-01-31T22:55:00.353', 'Id': '20173'},5121:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>It is a well-known fact that "Correlation doesn\'t equal causation", but machine learning seems to be almost entirely based on correlation. I\'m working on a system to estimate the performance of students on questions based on their past performances. Unlike other tasks, like Google search, this doesn\'t seem like the kind of system that can be easily gamed - so causation isn\'t really relevant in that regard.</p>\n\n<p>Clearly if we want to do experiments to optimise the system, we will have to care about the correlation/causation distinction. But from the point of view of just building a system to pick questions that are likely to be of the appropriate difficulty level - does this distinction have any importance?</p>\n', 'ViewCount': '26', 'Title': 'Machine learning - importance of correlation vs. causation', 'LastActivityDate': '2014-02-24T18:17:40.953', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '21987', 'Score': '9', 'OwnerDisplayName': 'Casebash', 'PostTypeId': '1', 'OwnerUserId': '644', 'Tags': '<machine-learning><statistics>', 'CreationDate': '2014-02-18T04:51:45.277', 'Id': '21986'},5122:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know I can use a Bayes classifier to determine if something is one of N classes, but can I also determine if something is NOT in any of the predefined classes?  Or will a Bayes classifier only find the closest class?</p>\n', 'ViewCount': '31', 'Title': 'Can you use a Bayes classifier to determine if something is NOT in a defined class?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T20:47:42.620', 'LastEditDate': '2014-03-29T20:47:42.620', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23147', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16195', 'Tags': '<machine-learning><bayesian-statistics>', 'CreationDate': '2014-03-27T13:30:26.863', 'Id': '23125'},5123:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm looking for public algorithm which gives the engine these abilities:</p>\n\n<ul>\n<li>Query by ranked terms</li>\n<li>Limit outcome by date/time range</li>\n</ul>\n\n<p>Basically, i'd like to concentrate articles (generally <code>title|text|timestamp</code>) identify the source and make N-N correlation to terms (is term for datasource same marking as term for dataentry?)</p>\n\n<p>Given the database of such information</p>\n\n<pre><code>entry_data_type:[type_id|title|description]\nentry_data:[entry_id|data_type_id|data_content]\nentry:[id|entry_type(data,source)|parent_entry_id|created|updated]\nterms(keywords):[id|keyword]\nentry2term:[entry_id|term_id|term_weight]\n</code></pre>\n\n<p>Where keywords are both automatically defined (text frequency analysis) and manually assigned (probably abstract terms in context to entry contents)</p>\n\n<p>I should be able to query by keywords like this: <code>kw1:3 kw2:10 kw3:-2 [range:-7 days]</code><br>\nand output shall be entries sorted by given keyword weights (pattern <code>keyword:weight</code>)</p>\n\n<p>I thought about something similar to EdgeRank, but that is social-graph-oriented, and I'm looking for more straight-forward solution (more selfish, meaning input filter is given by personal preferences, not social-graph-near preferences or social-score ranking)</p>\n\n<p>Also TF-IDF would have to be limited by time, so the document base to calculate the entry score is inserted in given date/time range only. Is there any possible break-down of TF-IDF ranking, eg. to pre-calculate raw-data for each day and then, based on query, merge them for given date-range?</p>\n\n<p>This question is independent of any particular programming language, platform, etc. I'm generally looking for keywords to look for, papers to read or ready implementations to study, but accepted are only answers not using paid or closed-source software parts or non-public-domain patents.</p>\n", 'ViewCount': '16', 'Title': 'TF-IDF query engine in context of terms weight', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-07T16:53:08.100', 'LastEditDate': '2014-04-07T16:53:08.100', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16077', 'Tags': '<reference-request><search-algorithms><statistics><search-problem><ranking>', 'CreationDate': '2014-04-07T01:30:29.197', 'Id': '23494'},5124:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a problem which essentially reduces to this:</p>\n\n<ol>\n<li>You have a black-box function that accepts inputs of length $n$.</li>\n<li>You can measure the amount of time the function takes to return the answer, but you can\'t see exactly how it was calculated.</li>\n<li>You have to determine whether the time-complexity of this function is polynomial or exponential.</li>\n</ol>\n\n<p>The way I did this was by running thousands of random sample inputs of varying lengths through the function, then plotting them on a scatter plot with times on the y-axis and input length on the x-axis.</p>\n\n<p>What are some metrics and methods I can use to determine if these points best fit to a polynomial curve or to an exponential curve?</p>\n\n<p>(Similar question asking how to draw polynomial/exponential best fit lines in Python on Stack Overflow: <a href="https://stackoverflow.com/questions/23026267/how-to-determine-if-a-black-box-is-polynomial-or-exponential">https://stackoverflow.com/questions/23026267/how-to-determine-if-a-black-box-is-polynomial-or-exponential</a>)</p>\n', 'ViewCount': '272', 'Title': 'How to determine if a black-box is polynomial or exponential', 'LastEditorUserId': '16701', 'LastActivityDate': '2014-04-12T16:26:22.670', 'LastEditDate': '2014-04-12T06:40:45.627', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23688', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16701', 'Tags': '<complexity-theory><time-complexity><polynomial-time><statistics>', 'CreationDate': '2014-04-12T04:54:14.143', 'Id': '23686'},5125:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Whilst reading up on <a href="https://en.wikipedia.org/wiki/Xorshift" rel="nofollow">Xorshift</a> I came across the following (emphases added):</p>\n\n<blockquote>\n  <p>The following xorshift+ generator, instead, has 128 bits of state, a maximal period of 2^128 \u2212 1 and passes BigCrush:</p>\n  \n  <p><code>[snip code]</code></p>\n  \n  <p>This generator is one of the fastest generator passing BigCrush; however, <strong>it is only 1-dimensionally equidistributed</strong>.</p>\n</blockquote>\n\n<p>Earlier in the article there\'s the following:</p>\n\n<blockquote>\n  <p><code>[snip code]</code></p>\n  \n  <p>Both generators, as all xorshift* generators of maximal period, emit a sequence of 64-bit values <strong>that is equidistributed in the maximum possible dimension</strong>.</p>\n</blockquote>\n\n<p>What does it mean for a sequence to be equidistributed in one dimension vs. multiple dimensions vs. not at all?</p>\n', 'ViewCount': '64', 'Title': "What does it mean for a random number generator's sequence to be only 1-dimensionally equidistributed?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-25T03:28:25.933', 'LastEditDate': '2014-04-23T18:52:36.400', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24038', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10487', 'Tags': '<randomness><statistics><pseudo-random-generators>', 'CreationDate': '2014-04-23T02:03:10.837', 'Id': '24037'}