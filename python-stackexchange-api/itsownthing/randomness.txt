{'Body': '<p>You have one coin. You may flip it as many times as you want. </p>\n\n<p>You want to generate a random number $r$ such that $a \\leq r &lt; b$ where $r,a,b\\in \\mathbb{Z}^+$. </p>\n\n<p>Distribution of the numbers should be uniform. </p>\n\n<p>It is easy if $b -a = 2^n$:</p>\n\n<pre><code>r = a + binary2dec(flip n times write 0 for heads and 1 for tails) \n</code></pre>\n\n<p>What if $b-a \\neq 2^n$?</p>\n', 'ViewCount': '2142', 'Title': 'Generating uniformly distributed random numbers using a coin', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-24T21:55:20.950', 'LastEditDate': '2012-04-29T20:51:09.547', 'AnswerCount': '7', 'CommentCount': '0', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '244', 'Tags': '<algorithms><probability-theory><randomness><random-number-generator>', 'CreationDate': '2012-03-21T03:12:00.883', 'FavoriteCount': '4', 'Id': '570'}{'Body': '<p>I <a href="http://blog.computationalcomplexity.org/2004/04/blum-complexity-measures.html" rel="nofollow">read</a> that the number of coin tosses of a probabilistic Turing machine (PTM) is not a <a href="http://en.wikipedia.org/wiki/Blum_axioms" rel="nofollow">Blum complexity measure</a>. Why?</p>\n\n<p>Clarification:</p>\n\n<p>Note that since the execution of the machine is not deterministic, one should be careful about defining the number of coin tosses for a PTM $M$ on input $x$ in a way similar to the time complexity for NTMs and PTMs. One way is to define it as the maximum number of coin tosses over possible executions of $M$ on $x$.</p>\n\n<p>We need the definition to satisfy the axiom about decidability of $m(M,x)=k$. We can define it as follows:</p>\n\n<p>$$\nm(M,x) =\n\\begin{cases}\nk &amp; \\text{all executions of $M$ on $x$ halt, $k=\\max$ #coin tosses} \\\\\n\\infty &amp; o.w. \\\n\\end{cases}\n$$</p>\n\n<p>The number of random bits that an algorithm uses is a complexity measure that appears in papers, e.g. "algorithm $A$ uses only $\\lg n$ random bits, whereas algorithm $B$ uses $n$ random bits".</p>\n', 'ViewCount': '119', 'Title': 'Is the number of coin tosses of a probabilistic Turing machine a Blum complexity measure?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-28T17:32:38.167', 'LastEditDate': '2012-03-28T17:32:38.167', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<computability><complexity-theory><randomness><probabilistic-algorithms>', 'CreationDate': '2012-03-27T20:35:16.207', 'Id': '835'}{'ViewCount': '79', 'Title': 'Making random sources uniformly distributed', 'LastEditDate': '2012-05-19T14:50:50.590', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '0', 'Body': '<p>How do I build a random source that outputs the bits 0 and 1 with $prob(0) = prob(1) = 0.5$. We have access to another random source $S$ that outputs $a$ or $b$ with independent probabilities $prob(a)$ and $prob(b) = 1 - prob(a)$ that are unknown to us.</p>\n\n<p>How do I state an algorithm that does the job and that does not consume more than an expected number of\n$(prob(a) \\cdot prob(b))^{-1}$ symbols of $S$ between two output bits and prove its correcteness?</p>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-19T19:13:23.613', 'CommentCount': '1', 'AcceptedAnswerId': '1934', 'CreationDate': '2012-05-19T14:14:55.337', 'Id': '1921'}{'Body': '<p>A source provides a stream of items $x_1, x_2,\\dots$ . At each step $n$ we want to save a random sample $S_n \\subseteq \\{ (x_i, i)|1 \\le i \\le n\\}$ of size $k$, i.e. $S_n$ should be a uniformly chosen sample from all $\\tbinom{n}{k}$ possible samples consisting of seen items. So at each step $n \\ge k$ we must decide whether to add the next item to $S$ or not. If so we must also decide which of the current items to remove from $S$ .</p>\n\n<p>State an algorithm for the problem. Prove its correctness.</p>\n', 'ViewCount': '158', 'Title': 'Online generation of uniform samples', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-03T01:02:26.967', 'LastEditDate': '2012-05-19T15:31:20.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1931', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness><online-algorithms>', 'CreationDate': '2012-05-19T14:38:52.510', 'Id': '1923'}{'Body': '<p>If an encryption algorithm is meant to convert a string to another string which can then be decrypted back to the original, how could this process involve any randomness?  </p>\n\n<p>Surely it has to be deterministic, otherwise how could the decryption function know what factors were involved in creating the encrypted string?</p>\n', 'ViewCount': '210', 'Title': 'How can encryption involve randomness?', 'LastEditorUserId': '157', 'LastActivityDate': '2012-05-26T18:25:38.433', 'LastEditDate': '2012-05-26T18:25:38.433', 'AnswerCount': '5', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1622', 'Tags': '<cryptography><encryption><randomness>', 'CreationDate': '2012-05-23T21:13:58.847', 'Id': '2030'}{'ViewCount': '785', 'Title': 'How to prove correctness of a shuffle algorithm?', 'LastEditDate': '2012-05-30T08:13:00.493', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': "<p>I have two ways of producing a list of items in a random order and would like to determine if they are equally fair (unbiased).</p>\n\n<p>The first method I use is to construct the entire list of elements and then do a shuffle on it (say a Fisher-Yates shuffle). The second method is more of an iterative method which keeps the list shuffled at every insertion. In pseudo-code the insertion function is:</p>\n\n<pre><code>insert( list, item )\n    list.append( item )\n    swap( list.random_item, list.last_item )\n</code></pre>\n\n<p>I'm interested in how one goes about showing the fairness of this particular shuffling. The advantages of this algorithm, where it is used, are enough that even if slightly unfair it'd be okay. To decide I need a way to evaluate its fairness.</p>\n\n<p>My first idea is that I need to calculate the total permutations possible this way versus the total permutations possible for a set of the final length. I'm a bit at a loss however on how to calculate the permutations resulting from this algorithm. I also can't be certain this is the best, or easiest approach.</p>\n", 'Tags': '<algorithms><proof-techniques><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-01T11:23:35.340', 'CommentCount': '5', 'AcceptedAnswerId': '2156', 'CreationDate': '2012-05-29T07:11:15.180', 'Id': '2152'}{'ViewCount': '961', 'Title': 'Most efficient algorithm to print 1-100 using a given random number generator', 'LastEditDate': '2012-07-02T19:46:10.570', 'AnswerCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2042', 'FavoriteCount': '4', 'Body': u'<p>We are given a random number generator <code>RandNum50</code> which generates a random integer uniformly in the range 1\u201350.\nWe may use only this random number generator to generate and print all integers from 1 to 100 in a random order. Every number must come exactly once, and the probability of any number occurring at any place must be equal.</p>\n\n<p>What is the most efficient algorithm for this?</p>\n', 'Tags': '<algorithms><integers><randomness><random-number-generator>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-24T20:32:31.727', 'CommentCount': '6', 'AcceptedAnswerId': '2578', 'CreationDate': '2012-07-02T05:57:26.373', 'Id': '2576'}{'ViewCount': '445', 'Title': 'Is rejection sampling the only way to get a truly uniform distribution of random numbers?', 'LastEditDate': '2012-08-17T17:42:18.317', 'AnswerCount': '3', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '140', 'FavoriteCount': '2', 'Body': '<p>Suppose that we have a random generator that outputs\nnumbers in the range $[0..R-1]$ with uniform distribution and we\nneed to generate random numbers in the range $[0..N-1]$\nwith uniform distribution.</p>\n\n<p>Suppose that $N &lt; R$ and $N$ does not evenly divide $R$;\nin order to get a <strong>truly uniform distribution</strong> we can use the\n<a href="http://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> method:</p>\n\n<ul>\n<li>if $k$ is the greatest integer such that $k N &lt; R$</li>\n<li>pick a random number $r$ in $[0..R-1]$</li>\n<li>if $r &lt; k N$ then output $r \\mod N$, otherwise keep trying with other random numbers r\', r", ... until the condition is met</li>\n</ul>\n\n<blockquote>\nIs rejection sampling the only way to get a truly uniform discrete distribution?\n</blockquote>\n\n<p>If the answer is yes, why? </p>\n\n<p>Note: if $N &gt; R$ the idea is the same: generate a random number $r\'$ in $[0..R^m-1], R^m &gt;= N$, for example $r\' = R(...R(R r_1 + r_2)...)+r_m$ where $r_i$ is a random number in the range $[0..R-1]$</p>\n', 'Tags': '<probability-theory><randomness><random-number-generator><sampling>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-08-18T14:29:51.573', 'CommentCount': '1', 'AcceptedAnswerId': '2619', 'CreationDate': '2012-07-04T07:46:04.420', 'Id': '2605'}{'Body': '<p>I have read that the degree of nodes in a "knowledge" graph of people roughly follows a power law distribution, and more exactly can be approximated with a Pareto-Lognormal distribution.</p>\n\n<p>Where can I find a kind of algorithm that will produce a random graph with this distribution?</p>\n\n<p>See for example the paper <a href="http://www.cs.ucsb.edu/~alessandra/papers/ba048f-sala.pdf" rel="nofollow">Revisiting Degree Distribution Models for Social Graph Analysis</a> (page 4, equation 1) for a mathematical description (distribution function) of the kind of distribution I\'m interested in.</p>\n', 'ViewCount': '226', 'Title': 'How to random-generate a graph with Pareto-Lognormal degree nodes?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T04:40:16.137', 'LastEditDate': '2012-07-05T07:42:21.527', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2808', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2060', 'Tags': '<algorithms><graph-theory><probability-theory><randomness>', 'CreationDate': '2012-07-04T11:43:54.893', 'Id': '2608'}{'ViewCount': '415', 'Title': 'Generating inputs for random-testing graph algorithms?', 'LastEditDate': '2012-08-04T16:01:43.273', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '2', 'Body': '<p>When testing algorithms, a common approach is random testing: generate a significant number of inputs according to some distribution (usually uniform), run the algorithm on them and verify correctness. Modern testing frameworks can generate inputs automatically given the algorithms signature, with some restrictions.</p>\n\n<p>If the inputs are numbers, lists or strings, generating such inputs in straight-forward. Trees are harder, but still easy (using stochastic context-free grammars or similar approaches).</p>\n\n<p>How can you generate random graphs (efficiently)? Usually, picking graphs uniformly at random is not what you want: they should be connected, or planar, or cycle-free, or fulfill any other property. Rejection sampling seems suboptimal, due to the potentially huge set of undesirable graphs.</p>\n\n<p>What are useful distributions to look at? Useful here means that</p>\n\n<ul>\n<li>the graphs are likely to test the algorithm at hand well and</li>\n<li>they can be generated effectively and efficiently.</li>\n</ul>\n\n<p>I know that there are many models for random graphs, so I\'d appreciate some insight into which are best for graph generation in this regard.</p>\n\n<p>If "some algorithm" is too general, please use shortest-path finding algorithms as a concrete class of algorithms under test. Graphs for testing should be connected and rather dense (with high probability, or at least in expectation). For testing, the optimal solution would be to create random graphs around a shortest path so we <em>know</em> the desired result (without having to employ another algorithm).</p>\n', 'Tags': '<algorithms><graphs><randomness><software-testing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T16:33:06.190', 'CommentCount': '1', 'AcceptedAnswerId': '2953', 'CreationDate': '2012-07-30T21:36:29.087', 'Id': '2952'}{'ViewCount': '780', 'Title': 'Uniform sampling from a simplex', 'LastEditDate': '2012-08-17T09:22:28.433', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2553', 'FavoriteCount': '1', 'Body': "<p>I am looking for an algorithm to generate an array of N random numbers, such that the sum of the N numbers is 1, and all numbers lie within 0 and 1. For example, N=3, the random point (x, y, z) should lie within the triangle:</p>\n\n<pre><code>x + y + z = 1\n0 &lt; x &lt; 1\n0 &lt; y &lt; 1\n0 &lt; z &lt; 1\n</code></pre>\n\n<p>Ideally I want each point within the area to have equal probability. If it's too hard, I can drop the requirement. Thanks.</p>\n", 'Tags': '<algorithms><randomness><random-number-generator><sampling>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-09-27T21:24:11.037', 'CommentCount': '2', 'AcceptedAnswerId': '3229', 'CreationDate': '2012-08-16T19:45:16.167', 'Id': '3227'}{'ViewCount': '439', 'Title': 'How asymptotically bad is naive shuffling?', 'LastEditDate': '2012-11-06T20:49:40.330', 'AnswerCount': '2', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '242', 'FavoriteCount': '3', 'Body': "<p>It's well-known that this 'naive' algorithm for shuffling an array by swapping each item with another randomly-chosen one doesn't work correctly:</p>\n\n<pre><code>for (i=0..n-1)\n  swap(A[i], A[random(n)]);\n</code></pre>\n\n<p>Specifically, since at each of $n$ iterations, one of $n$ choices is made (with uniform probability), there are $n^n$ possible 'paths' through the computation; because the number of possible permutations $n!$ doesn't divide evenly into the number of paths $n^n$, it's impossible for this algorithm to produce each of the $n!$ permutations with equal probability.  (Instead, one should use the so-called <em>Fischer-Yates</em> shuffle, which essentially changes out the call to choose a random number from [0..n) with a call to choose a random number from [i..n); that's moot to my question, though.)</p>\n\n<p>What I'm wondering is, how 'bad' can the naive shuffle be?  More specifically, letting $P(n)$ be the set of all permutations and $C(\\rho)$ be the number of paths through the naive algorithm that produce the resulting permutation $\\rho\\in P(n)$, what is the asymptotic behavior of the functions </p>\n\n<p>$\\qquad \\displaystyle M(n) = \\frac{n!}{n^n}\\max_{\\rho\\in P(n)} C(\\rho)$ </p>\n\n<p>and </p>\n\n<p>$\\qquad \\displaystyle m(n) = \\frac{n!}{n^n}\\min_{\\rho\\in P(n)} C(\\rho)$?  </p>\n\n<p>The leading factor is to 'normalize' these values: if the naive shuffle is 'asymptotically good' then </p>\n\n<p>$\\qquad \\displaystyle \\lim_{n\\to\\infty}M(n) = \\lim_{n\\to\\infty}m(n) = 1$.  </p>\n\n<p>I suspect (based on some computer simulations I've seen) that the actual values are bounded away from 1, but is it even known if $\\lim M(n)$ is finite, or if $\\lim m(n)$ is bounded away from 0?  What's known about the behavior of these quantities?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics><probability-theory><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-10T04:07:42.843', 'CommentCount': '13', 'AcceptedAnswerId': '6596', 'CreationDate': '2012-11-06T19:22:56.410', 'Id': '6519'}{'Body': '<p>The ideal random permutation algorithm of <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#The_modern_algorithm" rel="nofollow">Fisher and Yates</a> (Algorithm P in Knuth vol.2) for a sequence of $n$ objects requires $n-1$ random numbers. </p>\n\n<p>In some card games one first does a "cut" and then a ripple shuffle. The cutting point is a random value, the subsequent shuffling could be considered as deterministic. That is, only one random number is being used to generate the permutation, which understandably can\'t be ideal. On the other hand, theoretical perfection isn\'t always necessary in practice. I like hence to know whether, if one keeps the constraint of using one random number in a run, the quality of randomness of the permutation obtained couldn\'t eventually be improved through certain appropriate modifications of the procedure commonly employed in card games, if one is willing to take the trade-off of more work/time, inconvenience, etc. Such trade-offs may not be acceptable for real games, but I suppose there may be other practical applications that could advantageously exploit the same idea, thus without being required to acquire, e.g. via a chosen PRNG, the larger number of random numbers needed for executing the algorithm of Fisher and Yates.</p>\n', 'ViewCount': '184', 'Title': 'Best random permutation employing only one random number', 'LastEditorUserId': '6437', 'LastActivityDate': '2013-01-28T16:06:34.070', 'LastEditDate': '2013-01-28T11:28:18.590', 'AnswerCount': '2', 'CommentCount': '12', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6437', 'Tags': '<randomness><random><sampling><permutations>', 'CreationDate': '2013-01-27T10:54:47.110', 'Id': '9199'}{'Body': '<p>I was wondering since randomness is embedded in genetic algorithms at almost every level, is there a really fine line between genetic algorithms and pure random search?</p>\n\n<p>Ever since I finished my implementation of a GA , since randomness is present in the mutation function,the initialization part (as well as the reinitialization part) and crossbreeding part as well... other than a encoder which tries to sense of the chromsomes (encoder tailored to make sense of the chromosome in context of the problem) and a fitness function , it feels like genetic algorithms are just random search functions in disguise .</p>\n\n<p>So my question is : are GA implementations just plain old random searches with a shot of memory to make it look like there is some sort of meaningful feedback? </p>\n', 'ViewCount': '108', 'ClosedDate': '2013-04-02T22:15:50.887', 'Title': 'Are genetic algorithms special instances of random search done in an unexpectedly short run-time?', 'LastEditorUserId': '7545', 'LastActivityDate': '2013-04-02T21:07:18.593', 'LastEditDate': '2013-04-02T20:35:30.713', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7545', 'Tags': '<search-algorithms><efficiency><randomness><evolutionary-computing><genetic-algorithms>', 'CreationDate': '2013-04-02T15:57:17.913', 'Id': '10975'}{'Body': '<p>I\'m a bit confused about the definition of BPP. The way BPP is defined in typical text books (Arora/Barak for example) is that if M(x) is a Probabilistic Turing Machine (PTM) that recognizes a language $L(x)$, then $Pr[M(x)=L(x)]&gt; 2/3$. My question is, what is the probability taken over? Arora/Barak remark (<a href="http://www.cs.princeton.edu/theory/complexity/bppchap.pdf" rel="nofollow">7.2</a>) that the probability is taken over internal coin tosses of $M(x)$, i.e., fix a value of $x$, and run all possible $2^{T(|x|)}$ experiments of internal coin tosses, and compute the majority of accept state. But if this is true, then Amplification theorem cannot hold because by definition if the probability is computed by executing all $2^{T(|x|)}$ possible coin-flips, then no matter how many times I run the algorithm, the probability is not going to change. (For example, if I have a bag with 2 red balls and 1 blue ball, then no matter how many times I pick a ball from the bag (and return it), the probability of picking a red ball is going to remain 2/3.)</p>\n\n<p>Basically, a PTM is a random process in two variables: The input string $x \\in \\{0,1\\}^*$ and random coin tosses $ r \\in \\{0,1\\}^{T(|x|)}$. For the amplification theorem to hold, I think one needs to fix a value of $r$ and run the machine on all values of $x$, and compute $Pr[M(x) = L(x)]$. Then for a fixed $x$, running $M(x)$ multiple times will have amplification effect, but if the probability is computed over internal coin tosses, then the Amplification theorem cannot hold.</p>\n\n<p>What am I misunderstanding here?</p>\n', 'ViewCount': '94', 'Title': 'Accurate definition of BPP', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:39:16.167', 'LastEditDate': '2013-04-08T14:39:16.167', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'OwnerDisplayName': 'MachPortMassenger', 'PostTypeId': '1', 'Tags': '<complexity-theory><terminology><complexity-classes><randomness>', 'CreationDate': '2013-04-07T03:51:38.153', 'FavoriteCount': '1', 'Id': '11119'}{'Body': '<p>It is well known that the efficiency of randomized algorithms (at least those in BPP and RP) depends on the quality of the random generator used. Perfect random sources are unavailable in practice. Although it is proved that for all $0 &lt; \\delta \\leq \\frac{1}{2}$ the identities BPP = $\\delta$-BPP and RP = $\\delta$-RP hold, it is not true that the original algorithm used for a prefect random source can be directly used also for a $\\delta$-random source. Instead, some simulation has to be done. This simulation is polynomial, but the resulting algorithm is not so efficient as the original one.</p>\n\n<p>Moreover, as to my knowledge, the random generators used in practice are usually not even $\\delta$-sources, but pseudo-random sources that can behave extremely badly in the worst case.</p>\n\n<p>According to <a href="http://en.wikipedia.org/wiki/Randomized_algorithm" rel="nofollow" title="Wikipedia">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.</p>\n</blockquote>\n\n<p>In fact, the implementations of randomized algorithms that I have seen up to now were mere implementations of the algorithms for perfect random sources run with the use of pseudorandom sources.</p>\n\n<p>My question is, if there is any justification of this common practice. Is there any reason to expect that in most cases the algorithm will return a correct result (with the probabilities as in BPP resp. RP)? How can the "approximation" mentioned in the quotation from Wikipedia be formalized? Can the deviation mentioned be somehow estimated, at least in the expected case? Is it possible to argue that a Monte-Carlo randomized algorithm run on a perfect random source will turn into a well-behaved stochastic algorithm when run on a pseudorandom source? Or are there any other similar considerations?</p>\n', 'ViewCount': '92', 'Title': 'Random generator considerations in the design of randomized algorithms', 'LastActivityDate': '2013-05-02T22:23:28.033', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11744', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2091', 'Tags': '<randomized-algorithms><randomness><pseudo-random-generators>', 'CreationDate': '2013-05-02T10:41:01.760', 'Id': '11726'}{'ViewCount': '724', 'Title': 'What randomness really is', 'LastEditDate': '2013-05-19T17:50:20.767', 'AnswerCount': '8', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '8255', 'FavoriteCount': '4', 'Body': '<p>I\'m a Computer Science student and am currently enrolled in System Simulation &amp; Modelling course. It involves dealing with everyday systems around us and simulating them in different scenarios by generating random numbers in different distributional curves, like IID, Gaussian etc. for instance. I\'ve been working on the boids project and a question just struck me that what exactly "random" really is? I mean, for instance, every random number that we generate, even in our programming languages like via the <code>Math.random()</code> method in Java, essentially is generated following an "algorithm".</p>\n\n<p>How do we really know that a sequence of numbers that we produce is in fact, random and would it help us, to simulate a certain model as accurately as possible?</p>\n', 'Tags': '<simulation><randomness><modelling>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-17T01:36:33.863', 'CommentCount': '2', 'AcceptedAnswerId': '12137', 'CreationDate': '2013-05-19T16:49:58.083', 'Id': '12136'}{'Body': '<p>I\'m studying the <a href="http://en.wikipedia.org/wiki/PCP_theorem" rel="nofollow">PCP theorem</a>. </p>\n\n<p>While it is easy to prove that $\\mathsf{P}=\\text{PCP}(O(\\log n),0)$ , proving that $\\text{PCP}(O(\\log n),1)\\subseteq \\mathsf{P}$  i.e. PCP that uses $O(\\log n)$ random bits and read 1 bit of the proof is less obvious, what I tried to do is to take some proof $\\pi$ of length $n^{O(1)}$ (because effectively the message sent by the prover is bounded by $2^{r(n)}q(n)=2^{O(\\log n)}=n^{O(1)}$) then try all the coin tosses each time the verifier read some bit of the message so if the proof is not correct we flip the bit in the proof!   </p>\n', 'ViewCount': '109', 'Title': 'Proving that $\\text{PCP}(O(\\log n),1)\\subseteq \\mathsf{P}$', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-03T06:59:43.993', 'LastEditDate': '2014-03-03T06:59:43.993', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12282', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<complexity-theory><randomness>', 'CreationDate': '2013-05-25T20:05:31.100', 'Id': '12276'}{'Body': '<p>From a very strictly adhering sense to the hardware and circuit-level operations of any standard (non-specialized, DSPs, or supercomputing systems, etc.) microprocessor follow very similar, almost exact in some ways, operations.</p>\n\n<p>The typical role of the (main) processor in a computer, integrated with other hardware circuits or not, and excluding DMA is to have a memory subsystem fetch byte(s) for it to "process" in whatever way. To have a processor "randomly" selective something can be abstracted and seen from a data algorithm <a href="http://en.wikipedia.org/wiki/High-level_programming_language" rel="nofollow">HLL-type</a> point of view, but on the circuit-level the operations can only get so complex. I know some Assembly of x86, so I can demonstrate further on the details of what I\'m asking.</p>\n\n<p>If you fetch a byte, or series of bytes, and then use some schematic to cycle through potential jump offsets, that is the only way to do randomness? Are their other ways?</p>\n', 'ViewCount': '39', 'Title': 'Speaking of "randomness" in computing terms, to what sense can any extant digital processor make "random" results?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-03T07:33:53.753', 'LastEditDate': '2013-07-03T07:33:53.753', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8963', 'Tags': '<randomized-algorithms><randomness>', 'CreationDate': '2013-07-01T21:46:50.027', 'Id': '13021'}{'Body': u"<p>Let us consider the following game: there are some players and a computer. Each player inputs one positive integer and his name (player doesn't know another's numbers, just his own). When all the players made their moves, computer outputs a name of winner \u2013 who submitted the <em>lowest unique</em> number.</p>\n\n<p>How do you think, what is the best strategy for this game?</p>\n", 'ViewCount': '280', 'Title': 'Guessing the smallest unique positive integer', 'LastActivityDate': '2013-07-04T13:07:44.233', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13085', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8991', 'Tags': '<game-theory><randomness>', 'CreationDate': '2013-07-03T13:25:27.730', 'Id': '13061'}{'Body': "<p>I have just used a function 'rand()' in my algorithm. In fact, it was arc4random() that I used. However, it got me thinking, how is randomness created in a computer system? \nCan anything ever truly be random when it is produced by a computer?</p>\n", 'ViewCount': '126', 'Title': "How do computers create 'randomness'?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-26T08:02:26.080', 'LastEditDate': '2013-08-26T08:02:26.080', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9799', 'Tags': '<randomness>', 'CreationDate': '2013-08-23T17:53:37.550', 'FavoriteCount': '1', 'Id': '13893'}{'Body': '<p>I would like to sample a uniformly random point in a polygon...</p>\n\n<p>If sample a large number they\'d be equally likely to fall into two regions if they have the same area.</p>\n\n<p>This would be quite simple if it were a square since I would take two random numbers in [0,1] as my coordinates.</p>\n\n<p>The shape I have is a regular polygon, but I\'d like it to work for any polygon.</p>\n\n<p><a href="http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle">http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle</a></p>\n\n<p><img src="http://mathworld.wolfram.com/images/eps-gif/TrianglePointPicking_700.gif" width="400"></p>\n', 'ViewCount': '192', 'Title': 'Random sampling in a polygon', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-29T14:38:32.687', 'LastEditDate': '2013-08-29T14:38:32.687', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><randomness><sampling><random-number-generator>', 'CreationDate': '2013-08-29T00:36:47.990', 'Id': '14007'}{'Body': '<p>I formulate the question in terms of universal distributions. Fix a version of Solomonoff\'s universal distribution $\\mathbf M$ and consider the following procedure for generating an infinite binary sequence $\\omega$. </p>\n\n<p>Start with some $\\omega_0$. Each subsequent element is given by $\\omega_n=\\arg \\max_a\\mathbf M(a|\\omega_{1:n-1})$.</p>\n\n<p>What are the properties of $\\omega$? For example, is it necessarily computable? Does the answer depend on the choice of $\\mathbf M$?</p>\n\n<p>The equivalent formulation is the following but it does not really help me advance with the problem. </p>\n\n<p>Is there a universal lower semicomputable $\\lambda$-supermartingale $t$ and a binary sequence $\\tilde\\omega$ such that $t$ always "goes up" along $\\tilde\\omega$ but never succeeds?</p>\n\n<p>This type of problem arises when a player plays against a reactive environment and the sequence the player observes is not exogenous. A more general question is what is the short-run behavior of $\\mathbf M$ since asymptotic results do not seem to help here.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>As an example, consider a simple coordination game with payoff matrix:</p>\n\n<p>(1,1)     (0,0)</p>\n\n<p>(0,0)     (1,1)</p>\n\n<p>Consider two players who infinitely repeatedly play this game. Their goal is to maximize the payoff. Assume that the player are myopic that is they care only about the payoff in the current period. </p>\n\n<p>Each player observes his opponent\'s history and tries to match him. Each player has a prior distribution about his opponent\'s play and updates it using Bayes\'s rule. </p>\n\n<p>Now assume that the prior distribution is Solomonoff\'s $\\mathbf M$. Then every period a player chooses an action $\\arg \\max_a\\mathbf M(a|\\omega_{1:n-1})$ where $\\omega_{1:n-1}$ is the opponent\'s history. The other player does the same, thus the rule $\\omega_n=\\arg \\max_a\\mathbf M(a|\\omega_{1:n-1})$.</p>\n', 'ViewCount': '67', 'Title': 'The sequence in which every symbol minimizes conditional complexity?', 'LastEditorUserId': '9931', 'LastActivityDate': '2013-09-02T15:46:46.007', 'LastEditDate': '2013-09-02T15:46:46.007', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9931', 'Tags': '<computability><randomness>', 'CreationDate': '2013-09-01T20:30:50.230', 'Id': '14073'}{'Body': '<p>C++11 has a convenient Bernoulli RNG, illustrated at \n<a href="http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution" rel="nofollow">http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution</a> .\nHowever, distilling an entire random integer into a single random bit seems inefficient when the expectation parameter $p$ is rational with a small or power-of-two denominator.\nIs there a reasonably fast way to generate 32 random Bernoulli bits at once in such cases? My application uses long streams of bits, so I can keep track of statistics if needed (but this would consume runtime).</p>\n', 'ViewCount': '78', 'Title': "Isn't std::bernoulli_distribution inefficient? Designing a bit-parallel Bernoulli generator", 'LastEditorUserId': '5189', 'LastActivityDate': '2013-09-25T04:26:33.097', 'LastEditDate': '2013-09-25T04:26:33.097', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5189', 'Tags': '<randomized-algorithms><integers><randomness><binary-arithmetic>', 'CreationDate': '2013-09-22T21:10:38.360', 'Id': '14525'}{'ViewCount': '110', 'Title': 'Chernoff bounds and Monte Carlo algorithms', 'LastEditDate': '2013-11-25T14:30:48.973', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'FavoriteCount': '1', 'Body': "<p>One of Wikipedia examples of use of Chernoff bounds is the one where an algorithm $A$ computes the correct value of function $f$ with probability $p &gt; 1/2$. Basically, Chernoff bounds are used to bound the error probability of $A$ using repeated calls to $A$ and majority agreement.</p>\n\n<p>I don't understand how, to be frank. It would be nice if somebody could break it down piece by piece. Moreover, does it matter whether $A$ is a decision algorithm or can return more values? How are Chernoff bounds in general used for Monte Carlo algorithms?</p>\n", 'Tags': '<randomized-algorithms><randomness><chernoff-bounds>', 'LastEditorUserId': '8508', 'LastActivityDate': '2013-11-25T19:22:19.073', 'CommentCount': '2', 'AcceptedAnswerId': '18329', 'CreationDate': '2013-11-25T06:16:42.047', 'Id': '18321'}{'Body': '<p>If the clock shows 14:15:36.909302, why not just use the fractions of a second part (09302) as a kind of random number? </p>\n\n<p>What is wrong with this form of generating random numbers?</p>\n\n<p>I am aware that obtaining truly random numbers is a difficult task, so I am assuming that there is something wrong with this method.</p>\n', 'ViewCount': '67', 'Title': "What's the problem of using the clock to generate random numbers?", 'LastActivityDate': '2013-12-31T21:13:25.033', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19426', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10637', 'Tags': '<randomness><random><random-number-generator>', 'CreationDate': '2013-12-31T20:18:04.223', 'Id': '19421'}{'Body': '<p>I was trying to understand better the definition of a strong PSRG and I came across this expression which I am trying to understand better:</p>\n\n<p>$$ Pr_{r \\in \\{0,1\\}^l}[A(r) = \\text{"yes"}]$$</p>\n\n<p>Where r is a truly random bit string and A is a polynomial time deterministic machine.\nI\'ve been having some problems understanding what this expression means conceptually (or intuitively). </p>\n\n<p>So far these are some of my thoughts and I will try to point out my doubts too.</p>\n\n<p>A is just a standard TM so we can image that on l steps, it will yield $2^l$ branches. Each branch has a chance of occurring depending on which r occurs. Therefore, I was wondering if the above probability expression just mean "the fraction of branches that out yes"? Is that basically the same as the chance that A will output yes on the given random bit string? The thing that was confusing me and I was not sure how to deal with it was that, A(r) always outputs the same thing ("yes" or "no") on a given r (say it always accepts or rejects if r = 1010100 or something), it didn\'t seem to me that it a probabilistic sense, unless we randomly choose r. So I was wondering how the community interpreted this equation and what it mean.</p>\n\n<p>Also, since this is a probability, it seems to me that A(r) is just r.v. that only takes two values (yes or no), right? So this distribution only has two probability values, the one that A outputs yes or no, right? I was wondering how that related to the string r and I was not sure how to resolve this.</p>\n', 'ViewCount': '32', 'Title': 'Interpreting probabilistic time turning machines', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-01-24T19:23:21.040', 'LastEditDate': '2014-01-24T19:23:21.040', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19943', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<terminology><probability-theory><randomized-algorithms><randomness>', 'CreationDate': '2014-01-24T04:54:38.433', 'Id': '19932'}{'Body': '<p>Given normally distributed integers with a mean of 0 and a standard deviation $\\sigma$ around 1000, how do I compress those numbers (almost) perfectly?  Given the entropy of the Gaussian distribution, it should be possible to store any value $x$ using $$\\frac{1}{2} \\mathrm{log}_2(2\\pi\\sigma^2)+\\frac{x^2}{2\\sigma^2}\\rm{log}_2\\rm{e}$$<br>\nbits.  The way to accomplish this perfect compression would be <a href="http://en.wikipedia.org/wiki/Arithmetic_coding" rel="nofollow">arithmetic coding</a>.  In principle it\'s not too hard, I can calculate the interval boundaries from the <a href="http://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="nofollow">cumulative distribution function</a> of the gaussian.  In practice I hit considerable difficulties because when using floating point operations I cannot achieve perfect reproduction of the results, and I have no idea how to do this without FP operations.  Perfect reproduction is necessary because the uncompressing code must come up with exactly the same interval boundaries as the compressing code.  So the question is:  How do I compute the interval boundaries?  Or is there any other way to achieve (near) perfect compression of such data?</p>\n\n<p><strong>Edit:</strong>  As Raphael said, strictly speaking the normal distribution is defined only for continuous variables.  So what I mean here are integers x with a probability distribution function $$P(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\rm e^{-\\frac{x^2}{2\\sigma^2}}$$.  </p>\n\n<p><strong>Edit2:</strong>  As Yuval said, this distribution does not sum up exactly to 1, however, for $\\sigma&gt;100$  the difference from 1 is less than $10^{-1000}$ and hence it\'s more precise than any practical calculation would be.</p>\n', 'ViewCount': '101', 'Title': 'Compressing normally distributed data', 'LastEditorUserId': '12710', 'LastActivityDate': '2014-03-04T06:50:22.477', 'LastEditDate': '2014-01-31T17:46:33.920', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22261', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12710', 'Tags': '<information-theory><randomness><data-compression><entropy>', 'CreationDate': '2014-01-31T14:34:01.367', 'Id': '20156'}{'Body': '<p>I have been working on a challenge i found on the internet. It is as follows:</p>\n\n<blockquote>\n  <p>You\'ve stumbled onto a significant vulnerability in a commonly used cryptographic library. It turns out that the random number generator it uses frequently produces the same primes when it is generating keys.</p>\n  \n  <p>Exploit this knowledge to factor the (hexadecimal) keys below, and enter your answer as the last six digits of the largest factor you find (in decimal).</p>\n  \n  <p>Key 1: 1c7bb1ae67670f7e6769b515c174414278e16c27e95b43a789099a1c7d55c717b2f0a0442a7d49503ee09552588ed9bb6eda4af738a02fb31576d78ff72b2499b347e49fef1028182f158182a0ba504902996ea161311fe62b86e6ccb02a9307d932f7fa94cde410619927677f94c571ea39c7f4105fae00415dd7d</p>\n  \n  <p>Key 2: \n   2710e45014ed7d2550aac9887cc18b6858b978c2409e86f80bad4b59ebcbd90ed18790fc56f53ffabc0e4a021da2e906072404a8b3c5555f64f279a21ebb60655e4d61f4a18be9ad389d8ff05b994bb4c194d8803537ac6cd9f708e0dd12d1857554e41c9cbef98f61c5751b796e5b37d338f5d9b3ec3202b37a32f</p>\n</blockquote>\n\n<p>These seem to be common RSA 1024-bit keys.</p>\n\n<p>My approach to the problem was to implement <a href="http://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm" rel="nofollow">Pollard\'s rho algorithm</a> to find factors, then once a factor is found, try dividing the decimal form of the keys by that factor until it is not divisible anymore. Iterate. </p>\n\n<p>Now, i used Pollard\'s rho and tried to divide until the key is not divided anymore because of the information the problem gave: the keys are not completely random. </p>\n\n<p>But here comes the question: assuming the algorithm generates two primes and multiplies them to get a co-prime, which is the key, the low randomness doesn\'t help much, does it? I mean, even if both keys share a common factor, finding it the first time would take the regular-impractical-exponential time.</p>\n\n<p>That seems to be the case, as my Python algorithm is running for about 5 hours now and has not found any factor to the second key, which i decided to start with.</p>\n\n<p>As it is a challenge, i assume there is a practical way of finding the answer. \nSo what im i doing wrong? Is just the algorithm choice wrong, as Pollard\'s rho is intended mainly for integer with small factors? Is my assumption that i can only use the lack of randomness after i find the first of the four factor, to then try to break the other key with the same factor, wrong?</p>\n\n<p>I would like if someone could just point me in right direction, instead of just giving the answer. Thank you. </p>\n', 'ViewCount': '33', 'Title': 'Finding prime factors of non-random key generator', 'LastActivityDate': '2014-03-05T05:41:29.360', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22291', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15293', 'Tags': '<algorithms><cryptography><randomness><primes>', 'CreationDate': '2014-03-05T03:22:05.833', 'Id': '22289'}{'Body': '<p>Say I plugged in a <a href="http://en.wikipedia.org/wiki/Hardware_random_number_generator" rel="nofollow">hardware true-random number generator (TRNG)</a> to my computer, then wrote programs with output that depends on the TRNG\'s output. Can it do anything non-trivial that a Turing machine with a psuedo-random number generator can\'t do? (a trivial thing it can do would be generating truly random numbers)</p>\n', 'ViewCount': '37', 'Title': 'Are there any practical differences between a Turing machine with a PRNG and a probabilistic Turing machine?', 'LastActivityDate': '2014-03-17T20:49:58.900', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15790', 'Tags': '<turing-machines><randomness><probabilistic-algorithms>', 'CreationDate': '2014-03-17T19:39:13.557', 'Id': '22720'}{'ViewCount': '2223', 'LastEditorDisplayName': 'user15782', 'Title': 'Can PRNGs be used to magically compress stuff?', 'LastEditDate': '2014-03-26T15:26:43.373', 'AnswerCount': '6', 'Score': '23', 'OwnerDisplayName': 'user15782', 'PostTypeId': '1', 'FavoriteCount': '3', 'Body': '<p>This idea occurred to me as a kid learning to program and\non first encountering PRNG\'s. I still don\'t know how realistic\nit is, but now there\'s stack exchange.</p>\n\n<p>Here\'s a 14 year-old\'s scheme for an amazing compression algorithm: </p>\n\n<p>Take a PRNG and seed it with seed <code>s</code> to get a long sequence \nof pseudo-random bytes. To transmit that sequence to another party, \nyou need  only communicate a description of the PRNG, the appropriate seed \nand the length of the message. For a long enough sequence, that \ndescription would be much shorter then the sequence itself.</p>\n\n<p>Now suppose I could invert the process. Given enough time and \ncomputational resources, I could do a brute-force search and find \na seed (and PRNG, or in other words: a program) that produces my\ndesired sequence (Let\'s say an amusing photo of cats being mischievous).</p>\n\n<p>PRNGs repeat after a large enough number of bits have been generated,\nbut compared to "typical" cycles my message is quite short so this \ndosn\'t seem like much of a problem.</p>\n\n<p>Voila, an effective (if rube-Goldbergian) way to compress data.</p>\n\n<p>So, assuming:</p>\n\n<ul>\n<li>The sequence I wish to compress is finite and known in advance.</li>\n<li>I\'m not short on cash or time (Just as long as a finite amount \nof both is required)</li>\n</ul>\n\n<p>I\'d like to know:</p>\n\n<ul>\n<li>Is there a fundamental flaw in the reasoning behind the scheme? </li>\n<li>What\'s the standard way to analyse these sorts of thought experiments?</li>\n</ul>\n\n<p><em>Summary</em></p>\n\n<p>It\'s often the case that good answers make clear not only the answer, \nbut what it is that I was really asking. Thanks for everyone\'s patience \nand detailed answers. </p>\n\n<p>Here\'s my nth attempt at a summary of the answers:</p>\n\n<ul>\n<li>The PRNG/seed angle doesn\'t contribute anything, it\'s no more \nthen a program that produces the desired sequence as output.</li>\n<li>The pigeonhole principle: There are many more messages of \nlength > k then there are (message generating) programs of \nlength &lt;= k. So some sequences simply cannot be the output of a \nprogram shorter then the message. </li>\n<li>It\'s worth mentioning that the interpreter of the program \n(message) is necessarily fixed in advance. And it\'s design \ndetermines the (small) subset of messages which can be generated\nwhen a message of length k is received.</li>\n</ul>\n\n<p>At this point the original PRNG idea is already dead, but there\'s \nat least one last question to settle:</p>\n\n<ul>\n<li>Q: Could I get lucky and find that my long (but finite) message just \nhappens to be the output of a program of length &lt; k bits?</li>\n</ul>\n\n<p>Strictly speaking, it\'s not a matter of chance since the \nmeaning of every possible message (program) must be known \nin advance. Either it <em>is</em> the meaning of some message \nof &lt; k bits <em>or it isn\'t</em>.</p>\n\n<p>If I choose a random message of >= k bits randomly (why would I?),\nI would in any case have a vanishing probability of being able to send it\nusing less then k bits, and an almost certainty of not being able \nto send it at all using less then k bits.</p>\n\n<p>OTOH, if I choose a specific message of >= k bits from those which\nare the output of a program of less then k bits (assuming there is \nsuch a message), then in effect I\'m taking advantage of bits already\ntransmitted to the receiver (the design of the interpreter), which \ncounts as part of the message transferred.</p>\n\n<p>Finally:</p>\n\n<ul>\n<li>Q: What\'s all this <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy</a>/<a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">kolmogorov complexity</a> business?</li>\n</ul>\n\n<p>Ultimately, both tell us the same thing as the (simpler) piegonhole \nprinciple tells us about how much we can compress: perhaps \nnot at all, perhaps some, but certainly not as much as we fancy\n(unless we cheat).</p>\n', 'Tags': '<information-theory><randomness><data-compression>', 'LastActivityDate': '2014-03-26T15:26:43.373', 'CommentCount': '13', 'AcceptedAnswerId': '23020', 'CreationDate': '2014-03-24T17:02:03.550', 'Id': '23010'}{'Body': u'<p>Whilst reading up on <a href="https://en.wikipedia.org/wiki/Xorshift" rel="nofollow">Xorshift</a> I came across the following (emphases added):</p>\n\n<blockquote>\n  <p>The following xorshift+ generator, instead, has 128 bits of state, a maximal period of 2^128 \u2212 1 and passes BigCrush:</p>\n  \n  <p><code>[snip code]</code></p>\n  \n  <p>This generator is one of the fastest generator passing BigCrush; however, <strong>it is only 1-dimensionally equidistributed</strong>.</p>\n</blockquote>\n\n<p>Earlier in the article there\'s the following:</p>\n\n<blockquote>\n  <p><code>[snip code]</code></p>\n  \n  <p>Both generators, as all xorshift* generators of maximal period, emit a sequence of 64-bit values <strong>that is equidistributed in the maximum possible dimension</strong>.</p>\n</blockquote>\n\n<p>What does it mean for a sequence to be equidistributed in one dimension vs. multiple dimensions vs. not at all?</p>\n', 'ViewCount': '64', 'Title': "What does it mean for a random number generator's sequence to be only 1-dimensionally equidistributed?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-25T03:28:25.933', 'LastEditDate': '2014-04-23T18:52:36.400', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24038', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10487', 'Tags': '<randomness><statistics><pseudo-random-generators>', 'CreationDate': '2014-04-23T02:03:10.837', 'Id': '24037'}