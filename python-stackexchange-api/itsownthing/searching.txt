{'Body': u'<p>Let $A_P = (Q,\\Sigma,\\delta,0,\\{m\\})$ the <em>string matching automaton</em> for pattern $P \\in \\Sigma^m$, that is </p>\n\n<ul>\n<li>$Q = \\{0,1,\\dots,m\\}$</li>\n<li>$\\delta(q,a) = \\sigma_P(P_{0,q}\\cdot a)$ for all $q\\in Q$ and $a\\in \\Sigma$</li>\n</ul>\n\n<p>with $\\sigma_P(w)$ the length of the longest prefix of $P$ that is a Suffix of $w$, that is</p>\n\n<p>$\\qquad \\displaystyle \\sigma_P(w) = \\max \\left\\{k \\in \\mathbb{N}_0 \\mid P_{0,k} \\sqsupset w \\right\\}$.</p>\n\n<p>Now, let $\\pi$ the <em>prefix function</em> from the <a href="https://secure.wikimedia.org/wikipedia/en/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm" rel="nofollow">Knuth-Morris-Pratt algorithm</a>, that is</p>\n\n<p>$\\qquad \\displaystyle \\pi_P(q)= \\max \\{k \\mid k &lt; q \\wedge P_{0,k} \\sqsupset P_{0,q}\\}$.</p>\n\n<p>As it turns out, one can use $\\pi_P$ to compute $\\delta$ quickly; the central observation is:</p>\n\n<blockquote>\n  <p>Assume above notions and $a \\in \\Sigma$. For $q \\in \\{0,\\dots,m\\}$ with $q = m$ or $P_{q+1} \\neq a$, it holds that</p>\n  \n  <p>$\\qquad \\displaystyle \\delta(q,a) = \\delta(\\pi_P(q),a)$</p>\n</blockquote>\n\n<p>But how can I prove this?</p>\n\n<hr>\n\n<p>For reference, this is how you compute $\\pi_P$:</p>\n\n<pre><code>m \u2190 length[P ]\n\u03c0[0] \u2190 0\nk \u2190 0\nfor q \u2190 1 to m \u2212 1 do\n  while k &gt; 0 and P [k + 1] =6 P [q] do\n    k \u2190 \u03c0[k]\n    if P [k + 1] = P [q] then\n       k \u2190 k + 1\n    end if\n    \u03c0[q] \u2190 k\n end while\nend for\n\nreturn \u03c0\n</code></pre>\n', 'ViewCount': '1117', 'Title': 'Connection between KMP prefix function and string matching automaton', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T15:53:30.563', 'LastEditDate': '2012-05-17T23:59:30.850', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '1900', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1378', 'Tags': '<algorithms><finite-automata><strings><searching>', 'CreationDate': '2012-05-05T09:56:27.257', 'Id': '1669'}{'ViewCount': '114', 'Title': 'Randomized String Searching', 'LastEditDate': '2012-05-10T14:52:46.263', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '2', 'Body': "<p>I need to detect whether a binary pattern $P$ of length $m$ occurs in a binary text $T$ of length $n$ where $m &lt; n$.</p>\n\n<p>I want to state an algorithm that runs in time $O(n)$ where we assume that arithmetic operations on $O(\\log_2 n)$ bit numbers can be executed in constant time. The algorithm should accept with probability $1$ whenever $P$ is a substring of $T$ and reject with probability of at least $1 - \\frac{1}{n}$ otherwise.</p>\n\n<p>I think fingerprinting could help here. But I can't get it.</p>\n", 'Tags': '<algorithms><strings><searching><probabilistic-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T14:52:46.263', 'CommentCount': '3', 'AcceptedAnswerId': '1718', 'CreationDate': '2012-05-07T07:56:27.833', 'Id': '1712'}{'Body': '<p>I need to create a bloom filter of 208 million URLs. What would be a good choice of bit vector size and number of hash functions? I tried a bit vector of size 1 GB and 4 hash functions, but it resulted in too many false positives while reading.</p>\n\n<p>I have a huge web corpus containing web content of billions of URLs. I need to process the web content of URLs satisfying certain criteria: the URL should have appeared in web search results in the past 7 days at least 5 times. This is represented by a list of 208 million URLs. Joining the list directly with the web corpus is not feasible because of volume. So I am considering creation of a bloom filter out of the list and then using the bloom filter to prune out unnecessary URLs from the web corpus.</p>\n', 'ViewCount': '387', 'Title': 'Bloom Filter for 208 million URLs', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:17:46.310', 'LastEditDate': '2013-06-26T13:17:46.310', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2881', 'Tags': '<data-structures><probabilistic-algorithms><searching><big-data><bloom-filters>', 'CreationDate': '2012-09-19T15:58:11.313', 'Id': '4616'}{'Body': '<p><a href="http://academia.stackexchange.com/questions/5079/are-there-hacks-or-smaller-scientific-search-engines-offering-you-context-sensit">I was asking on academia.se</a>, if anyone knows scientific search engines offering a <a href="http://en.wikipedia.org/wiki/Proximity_search_%28text%29" rel="nofollow">proximity operator</a> like Google Web Search does, while Google Scholar Search does not. That\'s sad, because this operator would be most useful for literature research offering you a nearly semantic/context-sensitive search and I\'ve seen requests for this feature in many blogs.</p>\n\n<p>The answer on my question linked above shows that 2 search engines offer something similar, but those operators also only work on titles and abstracts of papers, if I understand correctly. </p>\n\n<p>The wikipedia article doesn\'t explain what exactly limits the implementation of this kind of operator (exponentially rising indexing time, index size,... I\'m no search algorithm expert), but when Google Web Search offers it (the amount of web-text is much bigger), what possibly hinders the scientific search engines from offering it for full article text (cost-benefit ratio? I doubt this, as 99,9% of Google Web Search user don\'t know the AROUND(X) operator and the majority doesn\'t use <a href="http://www.googleguide.com/advanced_operators_reference.html" rel="nofollow">many operators</a> at all)?</p>\n\n<p>PS: If this question better fits SO, move it there, but I\'m more looking for a general explanation, what parameters determine and limit the implementation of an proximity operator.</p>\n', 'ViewCount': '96', 'Title': 'What limits the implementation of proximity operators for text indexing and searching?', 'LastEditorUserId': '298', 'LastActivityDate': '2012-11-10T15:30:49.783', 'LastEditDate': '2012-11-10T15:30:49.783', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '298', 'Tags': '<search-algorithms><data-mining><searching>', 'CreationDate': '2012-11-04T19:35:42.203', 'Id': '6477'}{'Body': '<p><a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a> has a good cheat sheet, but however it does not involve no. of comparisons or swaps. (though no. of swaps is usually decides its complexity). So I created the following. Is the following info is correct ? Please let me know if there is any error, I will correct it.</p>\n\n<p><strong>Insertion Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case : $\\Theta(n^2)$ ; happens when input is\nalready sorted in descending order</li>\n<li>Best Case : $\\Theta(n)$ ; when input is already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(n)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in Best case</li>\n</ul>\n\n<p><strong>Selection Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best Case: $\\Theta(n^2)$ </li>\n<li>No. of comparisons : $\\Theta(n^2)$</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Merge Sort :</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best case : $\\Theta(nlgn)$ ; doesn\'t matter at all whether the input is sorted or not</li>\n<li>No. of comparisons : $\\Theta(n+m)$ in worst case &amp; $\\Theta(n)$ in best case ; assuming we are merging two array of size n &amp; m where $n&lt;m$</li>\n<li>No. of swaps : No swaps ! [but requires extra memory, not in-place sort]</li>\n</ul>\n\n<p><strong>Quick Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$ ; happens input is already sorted</li>\n<li>Best Case : $\\Theta(nlogn)$ ; when pivot divides array in exactly half</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(nlogn)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Bubble Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$</li>\n<li>Best Case : $\\Theta(n)$ ; on already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Linear Search:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n)$ ; search key not present or last element</li>\n<li>Best Case : $\\Theta(1)$ ; first element</li>\n<li>No. of comparisons : $\\Theta(n)$ in worst case &amp; $1$ in best case</li>\n</ul>\n\n<p><strong>Binary Search:</strong></p>\n\n<ul>\n<li>Worst case/Average case : $\\Theta(logn)$</li>\n<li>Best Case : $\\Theta(1)$ ; when key is middle element</li>\n<li>No. of comparisons : $\\Theta(logn)$ in worst/average case &amp; $1$ in best case</li>\n</ul>\n\n<hr>\n\n<ol>\n<li>I have considered only basic searching &amp; sorting algorithms. </li>\n<li>It is assumed above that sorting algorithms produce output in ascending order</li>\n<li>Sources : The awesome <a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">CLRS</a> and this <a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a></li>\n</ol>\n', 'ViewCount': '9952', 'ClosedDate': '2014-02-09T15:34:28.957', 'Title': 'Complexities of basic operations of searching and sorting algorithms', 'LastActivityDate': '2014-02-09T07:07:24.577', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><sorting><searching>', 'CreationDate': '2013-04-03T13:09:39.333', 'FavoriteCount': '1', 'Id': '10991'}{'Body': '<p>How can I compute the search space complexity of a directional semantic network (i.e. a network where each semantics consist of two nodes and a directional link stands for their semantic relation)?</p>\n\n<p>Edit: For example, each semantics would be something like <code>lion ~&gt; IsA ~&gt; animal</code>, <code>animal ~&gt; IsInstanceOf ~&gt; living</code> etc...</p>\n', 'ViewCount': '39', 'Title': 'Search space complexity of directional semantic network', 'LastEditorUserId': '6899', 'LastActivityDate': '2013-06-04T18:56:16.000', 'LastEditDate': '2013-06-04T18:56:16.000', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6899', 'Tags': '<complexity-theory><knowledge-representation><searching><semantic-networks>', 'CreationDate': '2013-06-03T08:12:17.507', 'Id': '12440'}{'ViewCount': '218', 'Title': 'Randomizd String Searching', 'LastEditDate': '2013-11-01T08:10:11.917', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10988', 'Body': u"<p>I need to detect whether a binary pattern P of length m occurs in a binary text T of length n where m\n\n<p>I want to state an algorithm that runs in time O(n) where we assume that arithmetic operations on O(log2n) bit numbers can be executed in constant time. The algorithm should accept with probability 1 whenever P is a substring of T and reject with probability of at least 1\u22121n otherwise.</p>\n\n<p>I think fingerprinting could help here. But I can't get it.</p>\n", 'ClosedDate': '2013-10-30T14:42:32.020', 'Tags': '<searching>', 'LastEditorUserId': '10988', 'LastActivityDate': '2013-11-01T08:10:11.917', 'CommentCount': '4', 'AcceptedAnswerId': '16573', 'CreationDate': '2013-10-29T14:17:56.640', 'Id': '16546'}{'ViewCount': '90', 'Title': 'Published or widely known failures of Page Rank algorithm', 'LastEditDate': '2013-11-17T17:21:13.813', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '96', 'FavoriteCount': '1', 'Body': '<p>I use Google a lot for searching articles and publications. Occasionally, I find Google totally misses what I am looking for and I get completely unexpected results for some search queries. </p>\n\n<blockquote>\n  <p>What are the published or widely known failures of PageRank algorithm?</p>\n</blockquote>\n\n<p>EDIT: Changed the post to address comments.</p>\n', 'ClosedDate': '2013-11-17T01:13:31.840', 'Tags': '<algorithms><reference-request><searching>', 'LastEditorUserId': '96', 'LastActivityDate': '2013-11-17T17:21:13.813', 'CommentCount': '8', 'CreationDate': '2013-11-14T15:48:56.063', 'Id': '18018'}{'ViewCount': '25', 'Title': 'How feasible is it for a non-distributed web crawler running on consumer hardware to search the internet?', 'LastEditDate': '2014-01-22T20:12:45.690', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2605', 'Body': '<p>I am looking for an automated way to answer the question: what are the URLs on the world wide web that contain at least two strings from a set of strings. </p>\n\n<p>So if I have a set of strings {"A", "B" and "C"} -- I want to know, what pages on the world wide web contain "A" and B", "A" and "C", "B" and "C" or "A", "B" and "C."</p>\n\n<p>Obviously, for this simple example: Google it! </p>\n\n<p>But I want a scaleable, automated, and free solution. Google does <a href="https://support.google.com/webmasters/answer/66357?hl=en" rel="nofollow">not permit</a> automated queries. Yahoo makes you pay.</p>\n\n<p>One idea I have is (1) start with a URL, (2) check the text at that URL for the search strings (3) parse out the links from the text (4) record that you have checked the page and if it contains the strings then (5) search the links from the initial URL. Repeat until you have searched the tree. </p>\n\n<p>How feasible is this in terms of time and space on a single commodity machine -- given the size the internet? The internet is really, really big -- but only a comparatively few pages will contain these strings (they are proper names). </p>\n\n<p>I don\'t want to index the whole web as if my laptop were google!</p>\n\n<p>Most of the crawler\'s time will be spent confirming that the pages don\'t contain the strings. </p>\n\n<p>I\'m trying to get a rough ballpark to understand if this is even remotely feasible. </p>\n', 'ClosedDate': '2014-01-22T21:35:12.443', 'Tags': '<algorithm-analysis><search-problem><searching>', 'LastEditorUserId': '2605', 'LastActivityDate': '2014-01-22T20:47:27.130', 'CommentCount': '3', 'AcceptedAnswerId': '19902', 'CreationDate': '2014-01-22T19:00:09.453', 'Id': '19898'}{'Body': '<p>I just completed a detailed theoretical mathematical study about the PageRank algorithm, I want to make a sample implementation to make some tests and play with it, supposing that I have a local set/folder full of a several webpages, representing a mini Web, I want to make a small software where I can make searches by implementing the PageRank algorithm (with some modifications in the algorithm) and make tests and simulations.</p>\n\n<p>From what I know, I should crawl and index those web pages, then play with my ranking.</p>\n\n<p>Is there a solution that provide me crawling and indexing and spares me the effort of working on them so I can focus on my main problem ?</p>\n', 'ViewCount': '10', 'ClosedDate': '2014-04-29T20:54:41.253', 'Title': 'Steps to implement PageRank Algorithm on a local set of webpages', 'LastActivityDate': '2014-04-29T12:21:55.330', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14915', 'Tags': '<search-algorithms><searching>', 'CreationDate': '2014-04-29T12:21:55.330', 'Id': '24218'}{'Body': "<p>I'm trying to implement <code>PageRank</code> algorithm on a set of web pages, for that I need a sample <code>dataset</code> of web pages, and the web graph corresponding to them, this web graph represents the links between the pages that the data set contains.</p>\n\n<p>I need the web graph so I can get the transition matrix and do the calculation needed</p>\n\n<p>Example : </p>\n\n<pre><code>      URL1 -&gt; URL2\n      URL3390-&gt;URL5\n</code></pre>\n\n<p>and the URLxxxx as an id, is mapped somehow to the corresponding web page so I can know how to map.</p>\n\n<p>My question is: how/where can I get this resource (I've tried many links on the internet but nothing really helps), I would also like it to be not of a very large size, (internet connexion limitation), if I can't have this as it is, advice me what should I do ?</p>\n", 'ViewCount': '8', 'ClosedDate': '2014-04-30T09:05:45.803', 'Title': 'Where to get a web graph with corresponding web pages dataset', 'LastActivityDate': '2014-04-29T23:06:59.003', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14915', 'Tags': '<search-algorithms><searching>', 'CreationDate': '2014-04-29T23:06:59.003', 'Id': '24242'}