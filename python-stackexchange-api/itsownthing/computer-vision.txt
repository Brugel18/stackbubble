{'Body': '<p>I have read many documents about <a href="https://en.wikipedia.org/wiki/Convolution" rel="nofollow"><em>convolution</em></a> in image processing, and most of them say about its formula, some additional parameters. No one explains the intuition and real meaning behind doing convolution on an image. For example, intuition of derivation on the graph is make it more linear for example.</p>\n\n<p>I think a quick summary of the definition is: convolution is multiplied overlap square between image and kernel, after that sum again and put it into anchor. And this doesn\'t make any sense with me.</p>\n\n<p>According to <a href="http://www.aishack.in/2010/08/image-convolution-examples/" rel="nofollow">this article about convolution</a> I cannot imagine why convolution can do some "unbelievable" things. For example, line and edge detection on the last page of this link. Just choose appropriate convolution kernel can make a nice effects (detect line or detect edge). </p>\n\n<p>Can anyone provide some intuition (doesn\'t need to have to be a neat proof) on how it can do that?</p>\n', 'ViewCount': '241', 'Title': 'Intuition for convolution in image processing', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-13T20:38:44.307', 'LastEditDate': '2012-08-16T09:55:11.913', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2538', 'Tags': '<terminology><intuition><graphics><computer-vision>', 'CreationDate': '2012-08-16T08:41:24.633', 'Id': '3215'}{'Body': '<p>In computer vision, scales are important when we carry out a scene analysis. Choosing different scales affect the result of the analysis. For example, if a face is relatively small in the scene, then the details including nose, eyes will be omitted. On the other hand, details on larger faces become relatively more salient.</p>\n\n<p>I know both Gaussian Blur with different sigmas and Down Sampling on the image can generate different scales. Which is more reasonable on a cognitive sense?</p>\n', 'ViewCount': '95', 'Title': 'Which Is a Better Way of Obtaining Scales, Gaussian Blur or Down Sampling?', 'LastActivityDate': '2012-08-24T18:29:39.153', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3319', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<machine-learning><computer-vision><image-processing>', 'CreationDate': '2012-08-24T08:40:02.273', 'Id': '3310'}{'Body': "<p>Shannon's entropy [plog(1/p)] for an image is a probabilistic method for comparing two pixels or a group of pixels.Suppose an image with a  matrix of 3x3 has pixel intensity values</p>\n\n<pre><code>1 1 2\n2 3 3\n4 4 5\n</code></pre>\n\n<p>and another image with 3x3 matrix has group of pixels having intensity values</p>\n\n<pre><code>5 5 6\n6 7 7\n8 8 9\n</code></pre>\n\n<p>Then shannon's entropy for the images would be the same.So in this case the entropy values would point out that the images are same though in actual they are different.So image matching using this technique doesn't help.On basis of supervised classification where I classify an image based on trained databases of shannon's entropy ,we use the concept of entropy to find similarity between two images.Is there any method or research paper where this entropy can be used or modified for image matching for the above case..?</p>\n", 'ViewCount': '2300', 'Title': "Shannon's entropy for an image", 'LastEditorUserId': '157', 'LastActivityDate': '2013-10-11T21:42:02.737', 'LastEditDate': '2012-10-23T02:37:47.963', 'AnswerCount': '3', 'CommentCount': '7', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '3093', 'Tags': '<pattern-recognition><image-processing><entropy><computer-vision>', 'CreationDate': '2012-10-07T20:51:48.457', 'Id': '4935'}{'ViewCount': '2786', 'Title': 'DIfferences between computer vision and image processing', 'LastEditDate': '2012-12-01T14:05:16.613', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4813', 'FavoriteCount': '1', 'Body': '<p>I am basically an Electronics and Communication Engineering student. My relevant coursework include Probability Theory and Stochastic Processes, Engineering Mathematics, Signals and Systems, Digital Signal Processing (this semester).</p>\n\n<p>Now, this area Computer Vision - Image Processing - Object recognition - etc caught my eye and I am thinking to study this subject and write a nice research paper in one of the areas later on.</p>\n\n<p>So, could anyone tell me what the differences between Computer Vision and Image Processing are? Say, we consider Object recognition - what are the roles of vision and image processing?</p>\n', 'Tags': '<education><image-processing><computer-vision>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-05T14:59:59.047', 'CommentCount': '7', 'AcceptedAnswerId': '7087', 'CreationDate': '2012-11-30T15:25:15.030', 'Id': '7050'}{'Body': "<p>I'm looking for an OCR technique (PCA or SVM or anything else) in a peculiar setting. I want to detect the motion of the finger so that if someone writes something in front of the camera in the air, I want to recognize the characters online (meaning as soon as they are written).</p>\n", 'ViewCount': '166', 'Title': 'Handwritten character recognition as characters are being traced', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-02T09:11:28.823', 'LastEditDate': '2013-01-30T21:31:31.013', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6621', 'Tags': '<machine-learning><computer-vision><pattern-recognition><ocr>', 'CreationDate': '2013-01-30T09:16:29.270', 'FavoriteCount': '1', 'Id': '9302'}{'Body': '<p>I am facing a problem with processing some frontal facial images.</p>\n\n<p>I need to adjust the centers of the eyes (irises) to be at certain specific pixels in the image. In other words, i need to transform the image until the centers of the eyes are at specific pixels.</p>\n\n<p>Can anyone help me with this problem, an algorithm, technique, ...etc. that might solve this problem ?</p>\n\n<p>Thanks,</p>\n', 'ViewCount': '63', 'Title': 'How center the eyes of human face at specific pixels?', 'LastActivityDate': '2013-10-29T18:15:09.450', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7031', 'Tags': '<image-processing><computer-vision>', 'CreationDate': '2013-02-25T19:58:04.990', 'Id': '10090'}{'Body': '<p>I am self-studying computer graphics (both 2d and 3d) and was wondering if someone could provide me with some references like websites, literature, etc.  </p>\n\n<p>I am a complete beginner to computer graphics so I would like to go bottom-up. I have been searching Google but mostly what I get are lecture notes which are, quite frankly, not-so-well explained. They are intended for students who attended those lectures.  </p>\n\n<p>So, references please for self-study.</p>\n', 'ViewCount': '137', 'Title': 'Computer Graphics reference request', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-04T09:50:16.193', 'LastEditDate': '2013-09-04T09:22:37.280', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '8418', 'Tags': '<reference-request><graphics><computer-vision>', 'CreationDate': '2013-05-28T20:46:56.150', 'Id': '12344'}{'Body': '<p>Recently I\'ve seen this very good apps</p>\n\n<p><a href="https://play.google.com/store/apps/details?id=com.kingjim.shotnote&amp;hl=en" rel="nofollow">https://play.google.com/store/apps/details?id=com.kingjim.shotnote&amp;hl=en</a>\n<a href="https://itunes.apple.com/JP/app/id411332997?mt=8" rel="nofollow">https://itunes.apple.com/JP/app/id411332997?mt=8</a></p>\n\n<p>this apps reconstruct the image from the camera based on the marker (this is what I think :) ) it also have a special paper that have marker. What I\'ve still haven\'t got is what is the algorithm to do this? what computer vision theory used in this one? I understand artificial intellegence and basic computer vision, is there anyone have the ide how can this be done? at first I thought this based on stereo image but based on what I\'ve studied before stereo vision need two camera. I hope my question is clear enough but if is there any information needed please ask in the comment :)</p>\n\n<p>Thank you!</p>\n', 'ViewCount': '112', 'Title': 'Computer Vision - Algorithm for reconstructing tilted image', 'LastActivityDate': '2014-03-07T09:52:25.013', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4573', 'Tags': '<algorithms><artificial-intelligence><computer-vision>', 'CreationDate': '2013-06-11T12:45:29.420', 'Id': '12618'}{'Body': '<p>I am working on a program that involves detecting faces in photos, i am reading a paper that someone has wrote and they said that they had more success detecting faces once he they applied some enhancements to the image, one of these being a contrast enhancement. The exact words are...</p>\n\n<blockquote>\n  <p>Contrast enhancement improves the overall quality of the image. Gaussian convolution using \n  the Gaussian function G(x, y) is first applied with the input value channel image in the HSV \n  space. The convolution process can be expressed as follows</p>\n</blockquote>\n\n<p>They then give you Equation 3: $V_{con} = V_1(x,y) \\oplus G(x,y)$ NOTE: $V_1$ is the original value</p>\n\n<blockquote>\n  <p>$V_{CON}$ in Eq. 3 denotes the convolution result, which contains the luminance information from \n  the surrounding pixels. The center pixel value is now compared with the Gaussian convolution \n  result in order to find the amount of contrast enhancement of that center pixel. This process is \n  described by the following equation</p>\n</blockquote>\n\n<p>Equation 4: $V_{ce}(x,y) = 255V_{le}(x,y)^{E(x,y)}$ NOTE: $V_{le}$ is the result of another enhancement done previously.</p>\n\n<blockquote>\n  <p>where $V_{CE}(x, y)$ is the result of contrast enhancement and $E(x, y)$ is given by the following relation</p>\n</blockquote>\n\n<p>$E(x,y) = (V_{con}(x,y)/V_1(x,y))^g$</p>\n\n<blockquote>\n  <p>Here g is the image dependent parameter determined by using the standard deviation of the \n  input value channel image. If the center pixel is brighter then the surrounding pixels, the contrast \n  of the pixel is pulled up. On the other hand, if the center pixel is darker then the neighboring \n  pixel then the contrast of the pixel is lowered. </p>\n</blockquote>\n\n<p>The problem is that with my understanding of maths (NCEA Level 3, Last year at collage) i don\'t understand how to do the first part, any help would be much appreciated</p>\n\n<p>The full paper is here: <a href="http://jips-k.org/dlibrary/JIPS_v09_no1_paper9.pdf" rel="nofollow">http://jips-k.org/dlibrary/JIPS_v09_no1_paper9.pdf</a></p>\n', 'ViewCount': '105', 'Title': 'Contrast Enhancement For An Image Using A Gaussian Function', 'LastActivityDate': '2013-06-18T07:40:19.073', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8727', 'Tags': '<computer-vision>', 'CreationDate': '2013-06-18T07:40:19.073', 'Id': '12734'}{'Body': "<p>Is developing an optical character recognition system for an alphabet which has no previous ocr considered a reputable conference or journal worthy research project, given the fact that there are so many commercial ocr system? Though, lots of conference proceeding and journal entries shows up when googled, but most of them are old and are about performance of a specific algorithm. There are also research on ocr for cursive alphabets which are hard to segment, like Arabic. The alphabet I am developing ocr for is of Indic origin and cursive as my final year undergraduate thesis. So, I'm wondering if it's a good research project.</p>\n", 'ViewCount': '74', 'ClosedDate': '2013-07-30T19:31:03.937', 'Title': 'Is developing an OCR considered a research project?', 'LastActivityDate': '2013-07-30T14:03:58.407', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '13519', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9421', 'Tags': '<computer-vision><pattern-recognition>', 'CreationDate': '2013-07-30T09:15:38.420', 'Id': '13512'}{'Body': "<p>I'm interested in learning about how facial recognition works.  I'm especially interested in the algorithms or approach that is used.  What are the leading methods for facial recognition?  Is there a good overview or source to learn about a few of the most widely used algorithms for facial recognition?  What would be the best handful of research papers to read first?</p>\n", 'ViewCount': '90', 'Title': 'How does facial recognition work?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T10:59:55.660', 'LastEditDate': '2013-09-02T10:33:34.133', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<reference-request><computer-vision><pattern-recognition><facial-recognition>', 'CreationDate': '2013-09-02T04:54:02.340', 'Id': '14075'}{'Body': "<p>Some humans can lip-read fairly well: by watching someone who is speaking, they can tell what the speaker is saying (even without hearing the speech).</p>\n\n<p>Has there been any work on building computer software to lip-read?  In other words, given a video of someone speaking, is it possible to build software to infer what the person is saying (with access only to the video stream, without audio)?  Has there been any research on this problem, or even deployed systems?</p>\n\n<p>Background and motivation: In the US, certain laws may forbid recording audio without consent.  However, there is generally no prohibition on recording video without consent of the people being recorded.  (That's why you see surveillance cameras all over the place, and why they record only video but never audio.)  I am curious whether technology has advanced enough that, solely from video, it might be possible for automated methods to tell what people are saying -- or whether that might become feasible in the near-future.  And, apart from the privacy implications, such a technology might be pretty useful.</p>\n", 'ViewCount': '117', 'Title': 'Automated lip-reading: inferring what someone is saying, based upon video of them speaking', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T10:57:04.990', 'LastEditDate': '2013-09-02T10:34:40.780', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<reference-request><computer-vision><pattern-recognition><facial-recognition>', 'CreationDate': '2013-09-02T05:05:58.230', 'Id': '14076'}{'Body': '<p>Many sites give the <a href="http://en.wikipedia.org/wiki/Sobel_operator" rel="nofollow">Sobel operators</a> as the convolution mask for smoothing an image. However, I haven\'t found a single site that describes how you can derive the operators from partial first derivatives. If anyone can explain the derivation, I would highly appreciate it.</p>\n', 'ViewCount': '212', 'Title': 'Deriving the Sobel equations from derivatives', 'LastEditorUserId': '7459', 'LastActivityDate': '2013-09-10T14:07:50.747', 'LastEditDate': '2013-09-10T02:17:44.400', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14245', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10052', 'Tags': '<image-processing><computer-vision>', 'CreationDate': '2013-09-09T19:12:38.067', 'Id': '14239'}{'Body': "<p>I have in mind a particular 3D object.  Given an image taken by a camera, I want to check whether that image contains an instance of my object.</p>\n\n<p>For instance, let's say that the object is a bathroom sink.  There are many kinds of bathroom sinks, but they tend to share some common elements (e.g., shape, size, color, function).  There can also be significant variation in lighting and pose.  Given an image, I want to know whether the image contains a bathroom sink.</p>\n\n<p>How do I do that?  What technique/algorithm would be appropriate?  Is there research on this topic?</p>\n\n<p>Of course, it is easy to use Google Images to obtain many example images that are known to contain a bathroom sink (or whatever the object I'm looking for might be), which could be used for training some sort of machine learning algorithm.  This suggests to me that maybe some combination of computer vision plus machine learning might be a promising approach, but I'm not sure exactly what the specifics might look like.</p>\n", 'ViewCount': '100', 'Title': 'Object recognition - given an image, does it contain a particular 3D object of interest?', 'LastActivityDate': '2013-10-29T18:12:04.513', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '14719', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<machine-learning><image-processing><computer-vision><pattern-recognition>', 'CreationDate': '2013-09-30T23:58:18.703', 'Id': '14717'}{'ViewCount': '53', 'Title': 'Job sites for applied/interdisciplinary mathematics related to computer science?', 'LastEditDate': '2013-11-27T13:43:45.137', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11167', 'FavoriteCount': '0', 'Body': "<p>I'm looking for job sites in applied/interdisciplinary mathematics, more specially, say postdocs or higher positions in mathematics and medical imaging, mathematics and computer vision. I'm aware of mostly all the popular job sites, mathjobs, euro math jobs, jobs.ac.uk, nordic math jobs etc etc, but most of the jobs there are of 'pure' nature, with very few for applied/interdisciplinary.</p>\n\n<p>I'm trying to find postdoctoral position in mathematical imaging problems, which would use significant amount of conformal/quasiconformal mappings, Riemann surfaces, differential geometry etc. Looking into individual group's webpage is too much work. But if there's an webpage containing all the information, that'll be much better! </p>\n\n<p>So, if you know any such website for the above (for Europe(preferable) and US), I'd appreciate if you could pass them onto me. Thanks! </p>\n", 'ClosedDate': '2013-11-27T19:41:43.750', 'Tags': '<computational-geometry><image-processing><computer-vision>', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-11-27T13:43:45.137', 'CommentCount': '1', 'CreationDate': '2013-11-04T09:22:05.450', 'Id': '16697'}{'Body': '<p>I have a question about preparing the dataset of positive samples for a cascaded classifier that will be used for object detection.</p>\n\n<p>As positive samples, I have been given 3 sets of images:</p>\n\n<ol>\n<li>a set of <strong>colored</strong> images in full size (about 1200x600) with a <strong>white background</strong> and with the object displayed at a different angles in each image</li>\n<li>another set with the same images in grayscale and with a <strong>white background</strong>, scaled down to the detection window size (60x60)</li>\n<li>another set with the same images in grayscale and with a <strong>black background</strong>, scaled down to the detection window size (60x60)</li>\n</ol>\n\n<p>My question is that in Set 1, should the background really be white? Should it not instead be an <strong>environment</strong> that the object is likely to be found in in the testing dataset? Or should I have a fourth set where the images are in their natural environments? How does environment figure into the training samples?</p>\n', 'ViewCount': '36', 'Title': 'Environment requirement in training image dataset for classifier', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-27T12:25:13.433', 'LastEditDate': '2013-11-12T16:52:26.290', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<machine-learning><image-processing><computer-vision><data-sets><classification>', 'CreationDate': '2013-11-12T09:44:33.823', 'Id': '17949'}{'Body': "<p>I'm searching for computer vision or machine learning algorithms/methods that are used to classify or differentiate two outdoor environments. Given an image with vehicles, I need to be able to detect whether the vehicles are in a natural open landscape (desert, in particular), or whether they're in the city. </p>\n\n<p>I've searched but can't seem to find relevant work on this. Perhaps because I'm new at computer vision, I'm using the wrong search terms.</p>\n\n<p>Any ideas? Is there any work (or related) available in this direction?</p>\n", 'ViewCount': '70', 'Title': 'Environment detection - How to detect "city" versus "landscape" background environment in computer vision?', 'LastActivityDate': '2013-12-19T18:52:08.227', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<image-processing><computer-vision><classification>', 'CreationDate': '2013-11-28T06:55:27.333', 'Id': '18437'}{'Body': '<p><img src="http://i.stack.imgur.com/bNQ68.png" alt="enter image description here"></p>\n\n<p>I\'m working on semi automatic texture segmentation using level set and gabor feature vector described by Manjunath und Ma [1]. I consider their feature representation is better than others since I can get good result on several images but somehow performs really bad on other images data. So I guess there\'s something wrong with my understanding.</p>\n\n<p>They describe that the feature vectors consist of $\\mu_{mn}$ (mean) and $\\sigma_{mn}$ (standard deviation). I use scale $S = 4$ and orientation $K = 6$, thus my feature vector will be 48 dimension (4 * 6 * 2 = 48):</p>\n\n<p>$f = \\{  \\mu_{00}, \\sigma_{00}, \\mu_{01}, \\sigma_{01}, \\mu_{02}...\\mu_{35}, \\sigma_{35}  \\} \\tag1$</p>\n\n<p>I am confused with their equation:</p>\n\n<p>$W(x, y) = \\int I(x_1,y_1)g_{mn} * (x - x_1, y - y_1) dx_1 dy_1 \\tag2$</p>\n\n<p>$\\mu_{mn} = \\int \\int |W_{mn}(x,y)|dxdy \\tag3$</p>\n\n<p>$\\sigma_{mn} = \\sqrt{ \\int \\int (|W_{mn}(x,y) - \\mu_{mn}|)^2 dxdy } \\tag4$</p>\n\n<p>So my first question is: <strong>are $\\mu$ and $\\sigma$ simply mean and average?</strong> So far, I just average every pixel value of the magnitude to obtain $\\mu$ and compute the standard deviation to obtain $\\sigma$.</p>\n\n<p>After I compute the feature vector, I need to do distance measure between two vectors $f_i$ and $f_j$</p>\n\n<p>$d(i, j) = \\sum_{}^{m}\\sum_{}^{n} d_{mn}(i, j) \\tag5$</p>\n\n<p>$d_{mn}(i, j) = |\\frac{\\mu_{mn}^i - \\mu_{mn}^j}{\\alpha(\\mu_{mn})}|  + |\\frac{\\sigma_{mn}^i - \\sigma_{mn}^j}{\\alpha(\\sigma_{mn})}| \\tag6$</p>\n\n<p>I\'m really puzzled with $\\alpha(\\mu_{mn})$ and $\\alpha(\\sigma_{mn})$ ? <strong>What are they?</strong> I have tried to calculate 48 standard deviations from $ (1)$</p>\n\n<p>$\\alpha = \\{ \\alpha(\\mu_{00}), \\alpha(\\sigma_{00}), \\alpha(\\mu_{01}), \\alpha(\\sigma_{01}), \\alpha(\\mu_{02})...\\alpha(\\mu_{35}), \\alpha(\\sigma_{35}) )\\} \\tag7$</p>\n\n<p>and use it to calculate the distance function but I\'m not convinced with my calculation since <strong>it seems not better than simple euclidean distance.</strong></p>\n\n<hr>\n\n<ol>\n<li><a href="http://vision.ece.ucsb.edu/publications/96PAMITrans.pdf" rel="nofollow">Texture Features for Browsing and Retrieval of Image Data</a> by B.S. Manjunath and W.Y. Ma (1996)</li>\n</ol>\n', 'ViewCount': '32', 'Title': 'How to compute Gabor feature vector?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-20T18:13:21.637', 'LastEditDate': '2014-01-20T11:42:09.483', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12978', 'Tags': '<algorithms><image-processing><computer-vision>', 'CreationDate': '2014-01-20T10:59:27.953', 'FavoriteCount': '1', 'Id': '19847'}{'Body': '<p>If my goal were to compress say 10,000 images and I could include a dictionary or some sort of common database that the compressed data for each image would reference, could I use a large dictionary shared by the entire catalog and therefore get much smaller file sizes?  Could this be expanded to work with images in general, i.e. to replace something like JPEG?</p>\n\n<p>Are there existing compression systems that operate like this, where there is a large common set of bits transmitted and loaded before decompression, that has been built by analyzing many images?</p>\n\n<p>For example, is there an existing computer science/machine learning research effort using sparse autoencoding over a large set of images and this concept of distributing a network derived from that encoding with the decompressor?</p>\n\n<p>Note: I have deleted quite a bit of context from this question because it was claimed that this made the question too "opinionated".  I also had to edit the title after someone else modified the title in such a way as to change the meaning of the question.  If you are looking for clarification about my question, please ask.</p>\n', 'ViewCount': '165', 'Title': 'Does there exist a data compression algorithm that uses a large dataset distributed with the encoder/decoder?', 'LastEditorUserId': '15323', 'LastActivityDate': '2014-04-06T22:59:48.903', 'LastEditDate': '2014-03-07T02:16:22.923', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15323', 'Tags': '<machine-learning><neural-networks><data-compression><computer-vision>', 'CreationDate': '2014-03-05T21:00:31.943', 'Id': '22317'}{'Body': '<p>I have two shapes in a 2D space, not necessarily convex, and I\'d like to compare how similar they are. How can I define a robust distance metric to measure their similarity, and how can I compute it?</p>\n\n<p><img src="http://i.stack.imgur.com/COt2d.png" alt="Distance between two shapes"></p>\n\n<p>I am looking for a method which provides a short distance in case of:</p>\n\n<ol>\n<li>scaling;</li>\n<li>rotation;</li>\n<li>perhaps local scaling or rotation.</li>\n</ol>\n\n<hr>\n\n<p>I see two possible solutions:</p>\n\n<ol>\n<li>transform the shapes into <strong>pixel-based matrices</strong> (bitmap) and compute a Levenshtein distance (but without enough robustness in the distance, in case of rotation for instance);</li>\n<li>transform the shapes into <strong>graphs</strong> and try to define a distance between them.</li>\n</ol>\n', 'ViewCount': '58', 'Title': 'Similarity between two geometric shapes', 'LastEditorUserId': '755', 'LastActivityDate': '2014-03-19T00:40:49.737', 'LastEditDate': '2014-03-19T00:40:49.737', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15843', 'Tags': '<algorithms><graph-theory><computational-geometry><computer-vision>', 'CreationDate': '2014-03-18T23:43:32.437', 'FavoriteCount': '2', 'Id': '22781'}{'Body': "<p>I've been researching on AdaBoost and GentleBoost classifiers, but can't seem to find a clear answer to the question:</p>\n\n<ul>\n<li>What is Adaboost better at classifying in computer vision?</li>\n<li>What is GentleBoost better at classifying?</li>\n</ul>\n\n<p>I've been told that AdaBoost is good for things with soft edges, like facial recognition, while GentleBoost is good for things with harder and more symmetrical features and edges, like vehicles. Is this true? Is there any proof for this or any evidence to back up this claim?</p>\n", 'ViewCount': '26', 'Title': 'Advantages of adaboost over gentleboost in applications, or vice versa?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T13:37:38.917', 'LastEditDate': '2014-04-01T13:37:38.917', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<algorithms><reference-request><machine-learning><classification><computer-vision>', 'CreationDate': '2014-04-01T10:34:09.543', 'FavoriteCount': '1', 'Id': '23316'}