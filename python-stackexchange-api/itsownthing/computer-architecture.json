{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have observed that there are two different types of states in branch prediction.</p>\n\n<ol>\n<li><p>In superscalar execution, where the branch prediction is very important, and it is mainly in execution delay rather than fetch delay.</p></li>\n<li><p>In the instruction pipeline, where the fetching is more problem since the instructions do not actually get executed till later.</p></li>\n</ol>\n\n<p>Which of these is very important (as in which of these really matters in the CPU now-a-days)? If both are equally important or in case the second one is more important then Why do we not have two instruction pipeline (probably of half the length ) and then depending on the branches, just choose one of them and then again start the population from the beginning?</p>\n', 'ViewCount': '217', 'Title': 'Which kind of branch prediction is more important?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-11-14T00:40:54.050', 'LastEditDate': '2012-03-07T05:32:20.863', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '76', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '123', 'Tags': '<cpu-pipelines><computer-architecture>', 'CreationDate': '2012-03-07T05:11:43.907', 'Id': '73'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder whether the massively parallel computation units provided in graphic cards nowadays (one that is programmable in <a href="http://en.wikipedia.org/wiki/OpenCL" rel="nofollow">OpenCL</a>, for example) are good enough to simulate 1D cellular automata (or maybe 2D cellular automata?) efficiently.</p>\n\n<p>If we choose whatever finite grid would fit inside the memory of the chip, can we expect one transition of a cellular automaton defined on this grid to be computed in (quasi)constant time?</p>\n\n<p>I assume 2D cellular automata would require more bandwidth for communication between the different parts of the chips than 1D automata.</p>\n\n<p>I\'d also be interested by the same question in the case of FPGA programming or custom chips.</p>\n', 'ViewCount': '317', 'Title': "Are today's massive parallel processing units able to run cellular automata efficiently?", 'LastEditorUserId': '68', 'LastActivityDate': '2012-09-28T19:50:44.353', 'LastEditDate': '2012-09-17T14:03:09.767', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '68', 'Tags': '<computer-architecture><parallel-computing><cellular-automata>', 'CreationDate': '2012-03-12T22:02:55.090', 'FavoriteCount': '2', 'Id': '256'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In our computer systems lecture we were introduced to the MIPS processor. It was (re)developed over the course of the term and has in fact been quite easy to understand. It uses a <a href="https://en.wikipedia.org/wiki/Reduced_instruction_set_computing">RISC</a> design, that is its elementary commands are regularly encoded and there are only few of them in order to keep the wires simple.</p>\n\n<p>It was mentioned that <a href="https://en.wikipedia.org/wiki/Complex_instruction_set_computing">CISC</a> follows a different philosophy. I looked briefly at the <a href="https://en.wikipedia.org/wiki/X86_instruction_listings">x86 instruction set</a> and was shocked. I can not image how anyone would want to build a processor that uses so complex a command set!</p>\n\n<p>So I figure there have to be good arguments why large portions of the processor market use CISC architectures. What are they? </p>\n', 'ViewCount': '1282', 'Title': 'Why would anyone want CISC?', 'LastActivityDate': '2013-03-21T01:31:31.047', 'AnswerCount': '5', 'CommentCount': '4', 'AcceptedAnswerId': '284', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<computer-architecture>', 'CreationDate': '2012-03-13T00:18:11.893', 'FavoriteCount': '1', 'Id': '269'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When reporting algorithmic complexity of an algorithm, one assumes the underlying computations are performed on some abstract machine (e.g. RAM) that approximates a modern CPU. Such models allow us to report time and space complexity of algorithms. Now, with the spread out of <a href="http://en.wikipedia.org/wiki/GPGPU">GPGPUs</a>, one wonders whether there are well known models where one can take into account power consumption as well.</p>\n\n<p>GPUs are well known to consume considerable amount of power and certain instructions fall into different categories of power consumption based on their complexity and location on the sophisticated chip. Hence instructions, from an energy of view, are not of unit (or even fixed) cost. A trivial extension would be assigning weights to operation cost, but I\'m looking for a powerful model where an operation/instruction might cost <em>non-constant</em> units of energy, e.g. polynomial amount (or even more complex e.g.: function of time elapsed since start of the algorithm; or taking into account probability of failure of cooling system, which will heat up the chips, and slow down the clock frequency etc.)</p>\n\n<p>Are there such models where non-trivial costs and faults can be incorporated?</p>\n', 'ViewCount': '144', 'Title': 'Is there an abstract machine that can capture power consumption?', 'LastEditorUserId': '48', 'LastActivityDate': '2012-03-26T15:00:28.613', 'LastEditDate': '2012-03-13T02:52:43.603', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '281', 'Score': '9', 'OwnerDisplayName': 'user20', 'PostTypeId': '1', 'Tags': '<complexity-theory><computer-architecture><power-consumption><machine-models>', 'CreationDate': '2012-03-13T00:48:16.690', 'Id': '271'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '401', 'Title': 'Are generational garbage collectors inherently cache-friendly?', 'LastEditDate': '2012-03-25T22:35:09.003', 'AnswerCount': '2', 'Score': '24', 'PostTypeId': '1', 'OwnerUserId': '39', 'FavoriteCount': '4', 'Body': '<p>A typical <a href="http://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Generational_GC_.28ephemeral_GC.29">generational garbage collector</a> keeps recently allocated data in a separate memory region. In typical programs, a lot of data is short-lived, so collecting young garbage (a minor GC cycle) frequently and collecting old garbage infrequently is a good compromise between memory overhead and time spent doing GC.</p>\n\n<p>Intuitively, the benefit of a generational garbage collector compared with a single-region collector should increase as the latency ratio of main memory relative to cache increases, because the data in the young region is accessed often and kept all in one place. Do experimental results corroborate this intuition?</p>\n', 'Tags': '<programming-languages><computer-architecture><cpu-cache>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-07T16:20:58.843', 'CommentCount': '1', 'AcceptedAnswerId': '608', 'CreationDate': '2012-03-19T12:31:48.340', 'Id': '495'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that since ~2004, Moore's law stopped working for CPU clock speed.\nI'm looking for a graph showing this, but am unable to find it: most charts out there show the transistor count or the capacity per year.</p>\n\n<p>Where can I find some data showing the CPU frequency of computers (anything is fine, personal computers, servers, laptops, ...) from the last few decades to today?<br>\nRaw data that I can plot myself would be fine as well (hum, probably even better).</p>\n", 'ViewCount': '1715', 'Title': 'CPU frequency per year', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T16:21:22.313', 'LastEditDate': '2012-04-22T16:21:22.313', 'AnswerCount': '4', 'CommentCount': '5', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '489', 'Tags': '<computer-architecture><empirical-research><data-sets>', 'CreationDate': '2012-03-21T10:27:51.997', 'FavoriteCount': '2', 'Id': '594'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '229', 'Title': 'Research on evaluating the performance of cache-obliviousness in practice', 'LastEditDate': '2012-03-26T20:58:01.420', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '2', 'Body': u'<p><a href="http://en.wikipedia.org/wiki/Cache-oblivious_algorithm" rel="nofollow">Cache-oblivious algorithms and data structures</a> are a rather new thing, introduced by Frigo et al. in <a href="http://userweb.cs.utexas.edu/~pingali/CS395T/2009fa/papers/coAlgorithms.pdf" rel="nofollow">Cache-oblivious algorithms, 1999</a>. Prokop\'s <a href="http://www.google.fi/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http%3A%2F%2Fsupertech.csail.mit.edu%2Fpapers%2FProkop99.pdf&amp;ei=Dc1tT-aLI8bm4QSC4YjAAg&amp;usg=AFQjCNHWhtzqOQqUonQWHduna8_nbQYx2g&amp;sig2=Nf_YDGY3NZLj7q0FY6TZgw" rel="nofollow">thesis</a> from the same year introduces the early ideas as well.</p>\n\n<p>The paper by Frigo et al. present some experimental results showing the potential of the theory and of the cache-oblivious algorithms and data structures. Many cache-oblivious data structures are based on static search trees. Methods of storing and navigating these trees have been developed quite a bit, perhaps most notably by Bender et al. and also by Brodal et al. Demaine gives a nice <a href="http://www.cs.uwaterloo.ca/~imunro/cs840/DemaineCache.pdf" rel="nofollow">overview</a>.</p>\n\n<p>The experimental work of investigating the cache behaviour in practice was done at least by Ladner et al. in <a href="http://www.cs.amherst.edu/~ccm/cs34/papers/ladnerbst.pdf" rel="nofollow">A Comparison of Cache Aware and Cache Oblivious Static Search Trees Using Program Instrumentation, 2002</a>. Ladner et al. benchmarked the cache behaviour of algorithms solving the binary search problem, using the classic algorithm, cache-oblivious algorithm and cache-aware algorithm. Each algorithm was benchmarked with both implicit and explicit navigation methods. In addition to this, the thesis by <a href="http://www.diku.dk/forskning/performance-engineering/frederik/thesis.pdf" rel="nofollow">R\xf8nn, 2003</a> analyzed the same algorithms to quite high detail and also performed even more thorough testing of the same algorithms as Ladner et al.</p>\n\n<p><strong>My question is</strong></p>\n\n<blockquote>\n  <p>Has there been any newer research on <em>benchmarking</em> the cache behaviour of cache-oblivious algorithms in <em>practice</em> since? I\'m especially interested in the performance of the static search trees, but I would also be happy with any other cache-oblivious algorithms and data structures.</p>\n</blockquote>\n', 'Tags': '<algorithms><data-structures><computer-architecture><reference-request><cpu-cache>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-03-26T20:58:01.420', 'CommentCount': '1', 'AcceptedAnswerId': '741', 'CreationDate': '2012-03-24T13:51:44.963', 'Id': '740'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Logic gates are an abstract device which can be implemented with electromagnetic relays, vacuum tubes, or transistors.  These implemenations have been successful in computing in part because of various properties of chainability, durability, and size beyond their basic binary stability.  They also work well because electricity is the energy source which can rather easily be shipped around.</p>\n\n<p>I\'ve seen adders built out of <a href="http://blog.makezine.com/archive/2007/06/binary-marble-adding-mach.html" rel="nofollow">wood, marbles, and gravity</a>.  I\'ve seen <a href="http://www.technologyreview.com/biomedicine/21784/" rel="nofollow">"lab on a chip" capilary-action-driven prototypes</a>.  I\'ve seen all kinds of specialty mechanical calculators (<a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=video&amp;cd=1&amp;ved=0CDsQtwIwAA&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DHYsOi6L_Pw4&amp;ctbm=vid&amp;ei=mrlxT-uoOIq-0QHIqpzHAQ&amp;usg=AFQjCNECIg5HDIV-9uL3GnwU_aSXriVDGA" rel="nofollow">Curta</a>, slide rule).  I\'ve seen <a href="http://www.youtube.com/watch?v=SudixyugiX4" rel="nofollow">domino trails</a> as single-use logic gates.</p>\n\n<p>I\'m interested in other illustrative computing devices that aren\'t <em>necessarily</em> convenient, durable, or fast, but which exploit properties of everyday materials to perform computation and which are directly visible.  The dominoes trails are close, but are a little too complicated to reset.</p>\n\n<p>Magneto-mechanical arrangements?  Water in pipes/troughs?  More general marble contraptions?</p>\n\n<p>PS.  Here\'s a new one.  <a href="http://www.liorelazary.com/index.php?option=com_content&amp;view=article&amp;id=46%3amechanical-cpu-clock&amp;catid=10%3aclocks&amp;Itemid=15" rel="nofollow">Mechanical CPU Clock</a></p>\n', 'ViewCount': '1667', 'Title': 'Logic gates from everyday materials', 'LastEditorUserId': '762', 'LastActivityDate': '2012-04-23T03:43:15.613', 'LastEditDate': '2012-04-16T22:55:09.193', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '762', 'Tags': '<computer-architecture><didactics>', 'CreationDate': '2012-03-27T03:52:57.487', 'Id': '809'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2934', 'Title': 'What happens to the cache contents on a context switch?', 'LastEditDate': '2012-04-06T23:13:15.797', 'AnswerCount': '2', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '59', 'FavoriteCount': '5', 'Body': "<p>In a multicore processor, what happens to the contents of a core's cache (say L1) when a context switch occurs on that cache?</p>\n\n<p>Is the behaviour dependent on the architecture or is it a general behaviour followed by all chip manufacturers?</p>\n", 'Tags': '<computer-architecture><operating-systems><cpu-cache>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-25T15:07:48.650', 'CommentCount': '0', 'AcceptedAnswerId': '1093', 'CreationDate': '2012-04-06T20:42:26.020', 'Id': '1088'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What are the complete specifications that must be documented in order to ensure the correct execution of a particular program written in Java? For instance, if one were archiving a program for long-term preservation, and no testing or porting would be done.</p>\n\n<p>I need to be able to compile and execute the Java program. Thus preserving the byte code or capturing the whole thing as a VMware image are excluded. The JVM could be saved as a VMware image though, and compiled libraries that are linked to the compiled code are OK, too. However, if there are dependencies on the OS, the architecture of the machine executing the JVM, the networking environment, external libraries, specification of the Java version used, etc. etc. these must all be listed. Some tech leaders in Dig Pres claim that any program written in Java will be executable "forever". How to do it?</p>\n', 'ViewCount': '109', 'Title': 'Requirements for emulation', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-25T19:41:39.720', 'LastEditDate': '2012-04-25T18:54:50.717', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '1506', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1038', 'Tags': '<operating-systems><computer-architecture><digital-preservation>', 'CreationDate': '2012-04-25T01:36:43.243', 'Id': '1493'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am currently learning assembly programming on wombat 4, I am looking at Frame pointers. I understand exactly what a frame pointer is: it is a register and are used to access parameters on a stack. But i'm confused on how they affect the program counter and why they are preferred over normal registers. </p>\n\n<p>Could some one explain, please.  </p>\n", 'ViewCount': '190', 'Title': 'Frame Pointers in Assembler', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-05T20:27:16.077', 'LastEditDate': '2012-05-05T20:27:16.077', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1668', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1376', 'Tags': '<computer-architecture><compilers>', 'CreationDate': '2012-05-05T06:55:17.640', 'Id': '1665'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '237', 'Title': 'Big-Endian/Little-Endian argument - paper by Danny Cohen', 'LastEditDate': '2012-05-06T19:09:51.153', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '78', 'FavoriteCount': '1', 'Body': '<p>Reading a book I was redirected to <a href="http://www.ietf.org/rfc/ien/ien137.txt" rel="nofollow">"On holy wars and a plea for peace"</a> paper by Danny Cohen, which covers the "holy war" between big-endians and little-endians considering byte-order.</p>\n\n<p>Reaching the summary of the memory section I got confused as the author sais:</p>\n\n<blockquote>\n  <p>To  the best of my knowledge only the Big-Endians of Blefuscu have\n  built systems with a consistent order  which  works  across \n  chunk-boundaries, registers,   instructions   and   memories.      I<br>\n  failed  to  find  a Little-Endians\' system which is totally\n  consistent.</p>\n</blockquote>\n\n<p>Which kind of contradicts his previous text sections covering little-endian:</p>\n\n<p>e.g.</p>\n\n<blockquote>\n  <p>When they add the bit order and the byte order they get:</p>\n\n<pre><code>               ...|---word2---|---word1---|---word0---|\n              ....|C3,C2,C1,C0|C3,C2,C1,C0|C3,C2,C1,C0|\n             .....|B31......B0|B31......B0|B31......B0|\n</code></pre>\n  \n  <p>In  this regime, when word W(n) is shifted right, its LSB moves into\n  the MSB of word W(n-1).\n                                     4</p>\n  \n  <p>English  text  strings  are  stored  in  the  same order, with the\n  first character in C0 of W0, the next in C1 of W0, and so on.</p>\n  \n  <p>This order is very consistent with itself, with the Hebrew language,\n  and (more importantly) with mathematics, because significance\n  increases with increasing item numbers (address).</p>\n</blockquote>\n\n<p>he even lateron sais:</p>\n\n<blockquote>\n  <p>The  Big-Endians struck again, and without any resistance got their\n  way. The decimal number 12345678 is stored in the VAX memory in this\n  order:</p>\n\n<pre><code>                       7 8  5 6  3 4  1 2\n                  ...|-------long0-------|\n                 ....|--word1--|--word0--|\n                .....|-C1-|-C0-|-C1-|-C0-|\n               ......|B15....B0|B15....B0|\n</code></pre>\n  \n  <p>This ugliness cannot be hidden even by the standard Chinese trick.</p>\n</blockquote>\n\n<p><strong>How did the author get to this completely different conclusion on overall consistency?</strong> </p>\n\n<p>An answer does not have to only base on the text, but may also include other sources which might clear up how the statement is sound.</p>\n', 'Tags': '<terminology><computer-architecture>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-06T19:09:51.153', 'CommentCount': '6', 'AcceptedAnswerId': '1691', 'CreationDate': '2012-05-05T23:34:43.757', 'Id': '1681'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1291', 'Title': 'What is meant by interrupts in the context of operating systems?', 'LastEditDate': '2012-05-18T07:45:37.680', 'AnswerCount': '4', 'Score': '7', 'OwnerDisplayName': 'user28694', 'PostTypeId': '1', 'OwnerUserId': '3017', 'Body': '<p>I\'ve decided to read <a href="http://rads.stackoverflow.com/amzn/click/0470128720" rel="nofollow">Operating Systems Concepts</a> by Silberschatz, Galvin Gagne (8th edition) over the summer. I\'ve gotten to a topic that\'s confusing me - interrupts and their role as it relates to operating systems. </p>\n\n<p>The text says that an operating system will begin a first process such as "init" and then wait for an "event" to occur and this event is usually signaled by an interrupt. The text also says that the interrupt can come from either the hardware or the software. How does this work, in a little more detail? Is the operating system driven by interrupts? </p>\n\n<p>I am just looking for some big picture understanding. </p>\n', 'Tags': '<operating-systems><computer-architecture><process-scheduling>', 'LastEditorUserId': '1541', 'LastActivityDate': '2012-05-18T07:45:42.593', 'CommentCount': '0', 'AcceptedAnswerId': '1701', 'CreationDate': '2012-05-05T21:03:18.007', 'Id': '1700'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What are devices and their interconnections used alongwith Quantum Processors? Are they compatible with hardware devices like Cache, RAM, Disks of current computers?</p>\n', 'ViewCount': '236', 'Title': 'Organisation and Architecture of Quantum Computers', 'LastActivityDate': '2012-05-07T14:37:34.633', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '1720', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '191', 'Tags': '<computer-architecture><quantum-computing>', 'CreationDate': '2012-05-07T04:34:49.127', 'FavoriteCount': '1', 'Id': '1710'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am currently reviewing the potentials of cloud computing regarding energy efficiency and green IT. In connection with this review I am having a look on techniques for increasing energy-efficiency in data centers (computing), hardware, networking and storage devices.</p>\n\n<p>Specificially for computing/servers I have found already a few:</p>\n\n<ul>\n<li>energy-aware scheduling techniques utilizing frequency and voltage scaling </li>\n<li>virtualization to consolidate server resources</li>\n<li>energy-saving hardware, e.g. ACPI, several processor techniques, especially for mobile devices etc.</li>\n</ul>\n\n<p>However, for networking devices it is rather hard to get information about energy-saving technologies. I have read that people are thinking about new protocols and alternative routing methods to be able to switch off hardware if the network is under low load. Does anyone know of such examples? </p>\n\n<p>Which other points should be added, either for networking or computing</p>\n', 'ViewCount': '146', 'Title': 'What techniques exist for energy-efficient computing and networking?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-24T15:04:08.463', 'LastEditDate': '2012-05-10T15:17:18.700', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1413', 'Tags': '<reference-request><computer-architecture><operating-systems><computer-networks><power-consumption>', 'CreationDate': '2012-05-08T10:05:00.307', 'Id': '1729'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m looking for some relatively simple examples of when <a href="http://en.wikipedia.org/wiki/Hazard_%28computer_architecture%29#Structural_hazards">structural hazards</a> occur in a pipelined architecture.</p>\n\n<p>The only scenario I can think of is when memory needs to be accessed during different stages of the pipeline (ie, the initial instruction fetch stage and the later memory read/write stage).</p>\n\n<p>I\'m thinking that there are many more structural hazards in more complex architectures, such as superscalar. Does it class as a structural hazard when an instruction is dispatched to an execution unit but is queued because the unit is in use?</p>\n\n<p>If this is highly architecture-specific, then just assume MIPS or something similar.</p>\n', 'ViewCount': '531', 'Title': 'When do structural hazards occur in pipelined architectures?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-13T14:32:05.693', 'LastEditDate': '2012-05-19T21:12:39.830', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6646', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1554', 'Tags': '<computer-architecture><cpu-pipelines>', 'CreationDate': '2012-05-19T20:49:42.443', 'Id': '1936'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '948', 'Title': 'Ternary processing instead of Binary', 'LastEditDate': '2012-05-24T10:55:18.980', 'AnswerCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1543', 'FavoriteCount': '2', 'Body': '<p>Most of the computers available today are designed to work with binary system. It comes from the fact that information comes in two natural form, <strong>true</strong> or <strong>false</strong>.</p>\n\n<p>We humans accept another form of information called "maybe" :)</p>\n\n<p>I know there are ternary processing computers but not much information about them.</p>\n\n<ol>\n<li>What is the <strong>advantages</strong> / <strong>disadvantages</strong> of designing and using ternary or higher levels of data  signals in computers? </li>\n<li>Is it feasible? </li>\n<li>In which domain can it be better than classic binary systems?</li>\n<li>Can we give computers the chance to make mistakes and expect to see performance \nimprovements in most situations by this way? (I think performance gains can be observed if computers are not so strict about being absolutely correct)</li>\n</ol>\n\n<p><strong>EDIT:</strong> Are there difficulties differentiating between 3 levels of signal? Would it be too hard to keep data in memory since memory voltage is frequently released and loaded, (maybe hundreds of time a second?).</p>\n', 'Tags': '<computability><computer-architecture>', 'LastEditorUserId': '1543', 'LastActivityDate': '2012-05-30T05:30:06.650', 'CommentCount': '5', 'AcceptedAnswerId': '2042', 'CreationDate': '2012-05-24T09:10:58.547', 'Id': '2040'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I first asked this question at cstheory, but they suggested to ask my question here, so here it goes ...</p>\n\n<p>I'm working on my masters thesis and I need to have theoretical value of the (average) speed gain that a quadcore processor brings compared to a singlecore processor, when they both use the same frequency. So for example the speed gain of a 2 GHz singlecore vs. 2 Ghz quadcore.</p>\n\n<p>Somewhere on the internet I've read that a quadcore is 2.6 times faster than singlecore, but the author didn't mention any source so I cannot use that in my thesis.</p>\n\n<p>I've been trying to calculate some things myself, but didn't come to a conclusion. I tried like this:</p>\n\n<pre><code>threads | quad core | single core | ratio \n--------|-----------|-------------|-------\n1       | 1         | 1           | 1\n2       | 2         | 1/2         | 4\n3       | 3         | 1/3         | 9\n4       | 4         | 1/4         | 16\n5       | 3+1/2     | 1/5         | 17.5\n6       | 2+2(1/2)  | 1/5         | 15\n7       | 1+3(1/2)  | 1/5         | 12.5\n...\n</code></pre>\n\n<p>This table represents the timeslices available to execute a task (I've taken a fair 50/50 usage for each thread). For example when using a singlecore and a application uses 3 threads, each thread can work 1/3 of the time, while with a quadcore each thread can work 100% of the time, because 3 threads can be spread accross a separate core. After playing with some calculations in Excel, I could not come to any conclusion.</p>\n\n<p>I'm a bit stuck and need some fresh ideas on how to get a theoretical number that represents how much faster a quad core is (on average) compared to a single core. Maybe some of you know some empirical numbers with a good reference to the source? Anyway, all the help is welcome, because I'm a bit stuck and this is the last part I need to cover in my thesis.</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '1055', 'Title': 'Theoretical speed gain of quad core vs. single core', 'LastActivityDate': '2012-06-10T05:32:08.937', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1807', 'Tags': '<computer-architecture><performance>', 'CreationDate': '2012-06-09T14:41:45.407', 'Id': '2300'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In a recent CACM article [1], the authors present a way to improve scalability of shared  and coherent caches. The core ingredient is assuming the caches are <em>inclusive</em>, that is higher-level caches (e.g. L3, one global cache) contain all blocks which are stored in their descendant lower-level caches (e.g. L1, one cache per core).</p>\n\n<p>Typically, higher-level caches are larger than their respective descendant caches together. For instance, some models of the Intel Core i7 series with four cores have an 8MB shared cache (L3) and 256KB private caches (L2), that is the shared cache can hold eight times as many blocks as the private caches in total.</p>\n\n<p>This seems to suggest that whenever the shared cache has to evict a block (in order to load a new block) it can find a block that is shared with none of the private caches\xb2 (pigeon-hole principle). However, the authors write:</p>\n\n<blockquote>\n  <p>[We] can potentially eliminate all recalls, but only if the associativity, or number of places in which a specific block may be cached, of the shared cache exceeds the aggregate associativity of the private caches. With sufficient associativity, [the shared cache] is guaranteed to find a nonshared block [...]. Without this worst-case associativity, a pathological cluster of misses could lead to a situation in which all blocks in a set of the shared cache are truly shared.</p>\n</blockquote>\n\n<p>How is this possible, that is how can, say, 1MB cover 8MB? Clearly I miss some detail of how such cache hierarchies work. What does "associativity" mean here? "number of places in which a specific block may be cached" is not clear; I can only come up with the interpretation that a block can be stored multiple times in each cache, but that would make no sense at all. What would such a "pathological cluster of misses" look like?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1145/2209249.2209269">Why On-Chip Cache Coherence is Here to Stay</a> by M. M. K. Martin, M. D. Hill, D. J. Sorin  (2012)</li>\n<li>Assuming the shared caches knows which blocks are shared where. This can be achieved by explicit eviction notifications and tracking bit, which is also discussed in [1].</li>\n</ol>\n', 'ViewCount': '207', 'Title': 'Why can L3 caches hold only shared blocks?', 'LastActivityDate': '2012-08-02T14:29:37.697', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3004', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<terminology><computer-architecture><cpu-cache><shared-memory>', 'CreationDate': '2012-08-02T11:38:21.877', 'Id': '3001'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Ok, I have been a computer nerd for many many years. I can program in quite a few languages, and I can even build them.  I sat down with a buddy the other day and asked how a computer actually takes electricity and does something with it, and we just couldnt figure it out, and Google wasn't much help either.</p>\n\n<p>I mean, how does a computer take a constant flow of electricity and turn it into 1's and 0's and then actually do something with those 1's and 0's like turn a light on for 15 seconds?</p>\n\n<p>I understand gates (and/or/nor/nand) and a little about diodes, resistors and transistors but i figured this would be the perfect place to have it explained in true laymens terms!</p>\n\n<p>Can anybody point me in the right direction or give me a brief explanation?</p>\n", 'ViewCount': '1212', 'Title': 'How does a computer work?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-18T21:11:34.387', 'LastEditDate': '2013-06-06T16:03:33.617', 'AnswerCount': '5', 'CommentCount': '4', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '2703', 'Tags': '<computer-architecture><education><reference-question>', 'CreationDate': '2012-09-01T17:11:12.853', 'FavoriteCount': '14', 'Id': '3390'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am currently reading the IEEE paper <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1705612" rel="nofollow">A Memory-Efficient Parallel String Matching\nArchitecture for High-Speed Intrusion Detection</a> by Kai Zheng,  Bin Liu,  Xin Zhang, and  Yunhao Liu. </p>\n\n<p>In the paper they propose a model for\n a memory-efficient multiple-character-approaching architecture\nconsisting of multiple parallel DFAs. </p>\n\n<p>I have read the paper and I have understood to an extent. </p>\n\n<p>But when I think of implementation, could I simulate the SRAM, LE,BCAM etc.. as in the case of a network simulator (NS-2,OPNET,OMNET).<img src="http://i.stack.imgur.com/Y4xYV.png" alt="enter image description here"> </p>\n', 'ViewCount': '118', 'Title': 'Could I simulate the implementation of memory components', 'LastActivityDate': '2013-04-08T17:02:36.107', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '3163', 'Tags': '<computer-architecture><simulation>', 'CreationDate': '2012-10-12T05:55:03.977', 'Id': '5033'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From <a href="http://rads.stackoverflow.com/amzn/click/013293633X" rel="nofollow">Computer Organisation and Architecture</a>: </p>\n\n<blockquote>\n  <p>The IAS operates by repetitively performing an instruction cycle. Each\n  instruction cycle consists of two sub cycles. During a fetch cycle,\n  the opcode of the next instruction is loaded into the IR and the\n  address portion is loaded into MAR. This instruction may be taken from\n  the IBR, or it can be obtained from memory by loading a word int he\n  MBR, and then down to the IBR, IR, and MAR.</p>\n</blockquote>\n\n<p>Now here, is there part I do not understand: </p>\n\n<blockquote>\n  <p><strong>Why the indirection?</strong> These operations are controlled by electronic circuitry and result in the use of data paths. To simplify the\n  electronics, there is only register that is used to specify the\n  address in memory for a read or write and only one register used for\n  the source or destination.</p>\n</blockquote>\n\n<p>Can anyone explain this to me more intuitively?  Why is this indirection being used?</p>\n', 'ViewCount': '150', 'Title': 'Indirection in IAS computer', 'LastActivityDate': '2012-10-14T20:15:22.093', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6073', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '3179', 'Tags': '<computer-architecture>', 'CreationDate': '2012-10-13T11:51:51.720', 'Id': '5042'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider a fixed point representation which can be regarded as a degenerate case of a floating number. It is entirely possible to use 2's complement for negative numbers. But why is a sign bit necessary for floating point numbers, shouldn't mantissa bits be using 2's complements?</p>\n\n<p>Also why do the exponent bits use a bias instead of a signed-magnitude representation (similar to the mantissa bits) or 2's complement representation?</p>\n\n<p>Update: Sorry if I didn't make it clear. I was looking for the reason of how floating point representation is shaped. If there is no strong implementation trade-off between the alternatives, then could someone explain the historical aspects of the floating point representation?</p>\n", 'ViewCount': '730', 'Title': "Why floating point representation uses a sign bit instead of 2's complement to indicate negative numbers", 'LastEditorUserId': '4183', 'LastActivityDate': '2012-10-16T11:47:32.687', 'LastEditDate': '2012-10-16T10:23:11.770', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4183', 'Tags': '<computer-architecture><floating-point><number-formats>', 'CreationDate': '2012-10-13T22:11:40.590', 'FavoriteCount': '1', 'Id': '6048'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to know how can i find number of cpu cores/processor supported given i have the MIPS value?</p>\n\n<p>For e.g I want to know the number of matching cores/processor to process speed of 18 triilion instructions / sec.</p>\n', 'ViewCount': '335', 'Title': 'How to convert process / cpu core based upon MIPS?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-16T10:10:55.983', 'LastEditDate': '2013-01-16T10:10:55.983', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4229', 'Tags': '<computer-architecture>', 'CreationDate': '2012-10-17T09:23:17.413', 'Id': '6119'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '889', 'Title': "Using Amdahl's law how do you determine execution time after an improvement?", 'LastEditDate': '2012-10-22T21:23:53.303', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1480', 'FavoriteCount': '1', 'Body': "<p>Speeding up a new floating-point unit by 2 slows down data cache accesses by a factor of 2/3 (or a 1.5 slowdown for data caches).  If old FP unit took 20% of program's execution time and data cache accesses took 10% of program's execution time, what is the overall speed up? </p>\n\n<p>I solved this problem using amdahl's law: </p>\n\n<p>FeFP = floating point enhanced fraction = .2</p>\n\n<p>FeDC = data cache access enhanced fraction = .1</p>\n\n<p>SeFP = floating point enhanced speedup = 2</p>\n\n<p>SeDC = data cache access enhanced speedup = 2/3</p>\n\n<p>Speedup overall = 1 / (   (1 - FeFP - FeDC)   +   FeFP/SeFP   +    FeDC * SeDC    )</p>\n\n<p>= 1 /  (   (   1 - .2 - .1  ) + .2/2 + (.1) * (2/3)   )\n = 1.154. </p>\n\n<p>I hope I did this correctly, but I'm confused about the next part asking what percentage of execution time is spent on floating point operations after implementing the new FP unit? </p>\n\n<p>I know that T[improved ] = T[affected] / improvement factor   + T[unaffected]</p>\n\n<p>But I'm unclear how to use it in the context of this problem.  Would appreciate all / any advice. </p>\n", 'Tags': '<computer-architecture><program-optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-22T21:23:53.303', 'CommentCount': '1', 'AcceptedAnswerId': '6204', 'CreationDate': '2012-10-20T22:07:45.577', 'Id': '6200'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have shown my work below for the problem and would appreciate if someone can let me know if I'm on the right track or point me in the right direction if not. </p>\n\n<p>An 8 processor file server handles a million operations each day. If each processor remains idle 5% of the time what is the utilization of the entire file server? </p>\n\n<p>I believe the utilization of the entire file server is 95% because it says all processors at idle 5% of the time. Is this correct?</p>\n\n<p>If the file server is fully utilized how much more work could it do? </p>\n\n<p>We know the file server does 10^6 operations each day and we know each processor is utilized 95% of the time.  Thus 10^6 / .95 = 1,052,632 operations / day would be the total number of operation if it were fully utilized.  </p>\n\n<p>The last question has me a little stumped: If 10% of the operations are used to move files between processors, how efficient is the file server? </p>\n", 'ViewCount': '118', 'Title': 'How to determine the Utilization and Efficiency of a file server?', 'LastEditorUserId': '41', 'LastActivityDate': '2014-04-24T23:19:09.250', 'LastEditDate': '2013-01-21T09:31:32.920', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '1480', 'Tags': '<computer-architecture><databases><performance>', 'CreationDate': '2012-10-20T22:14:29.313', 'Id': '6201'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The following equation is a matrix expression where $B_i$ and $C_i^T$ are $n\\times n$ matrices and k is a positive integer:</p>\n\n<p>$$P = \\sum_{i=1}^k  B_i  C_i^T $$</p>\n\n<p>So $P = B_1 C_1^T + B_2 C_2^T + \\cdots +B_k C_k^T   $</p>\n\n<p>If $B_i $ and $C_i$ are $n\\times n$ matrices themselves, we have a total of 2 $\\times$ k matrices that some how need to be stored in this vector architecture. </p>\n\n<p>So this means P will end up being an $n\\times n$ matrix after all the computation has completed.   </p>\n\n<p>What is the simplest possible vector processor architecture that is required to perform the matrix computation above?</p>\n\n<p>Is there any literature or articles out there that discuss how this can be done?</p>\n\n<p>Would appreciate all / any advise </p>\n', 'ViewCount': '161', 'Title': 'How do you go about designing a vector processor architecture for the sum of matrix products?', 'LastEditorUserId': '1480', 'LastActivityDate': '2012-12-13T02:27:21.687', 'LastEditDate': '2012-10-31T01:30:23.080', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '6430', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1480', 'Tags': '<computer-architecture><parallel-computing><matrices>', 'CreationDate': '2012-10-28T03:10:23.307', 'Id': '6347'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '201', 'Title': 'Commonly used Error Correcting Codes', 'LastEditDate': '2012-11-05T20:43:28.147', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2250', 'FavoriteCount': '1', 'Body': '<p>We know error correcting codes are parameterized as (n,k,d) codes. I wanted to know the values of these parameters for some commonly used error correcting codes in computer memories or in DRAMs, etc.</p>\n\n<p>I just wanted to see some values for these parameters, used in real life applications.</p>\n', 'Tags': '<computer-architecture><error-correcting-codes>', 'LastEditorUserId': '2806', 'LastActivityDate': '2012-11-05T20:43:28.147', 'CommentCount': '0', 'AcceptedAnswerId': '6481', 'CreationDate': '2012-11-05T04:08:50.493', 'Id': '6480'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '209', 'Title': 'Mathematical model on which current computers are built', 'LastEditDate': '2012-11-07T10:38:26.373', 'AnswerCount': '2', 'Score': '6', 'OwnerDisplayName': 'user5507', 'PostTypeId': '1', 'OwnerUserId': '947', 'Body': '<p>It is said that "The Turing machine is not intended as practical computing technology, but rather as a hypothetical device representing a computing machine. Turing machines help computer scientists understand the limits of mechanical computation." [Wikipedia]</p>\n\n<p>So on which model current machines are built?</p>\n', 'Tags': '<turing-machines><computer-architecture><machine-models>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-08T00:47:15.140', 'CommentCount': '0', 'AcceptedAnswerId': '6530', 'CreationDate': '2012-11-07T01:50:09.030', 'Id': '6528'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p><strong>Question 1:</strong> What is the average access time for a 3-level memory system with access time $T_1$, $2T_1$ and $3T_1$? (Hit ratio $h_1$ = $h_2$ = 0.9)</p>\n\n<p>The solution given is: $0.9[T_1] + 0.1(0.9[2*T_1] + 0.1[3*T_1]) = 1.11[T_1]$  <strong>(Method 1)</strong></p>\n\n<p>Here, they have considered the page won't be copied to the lower level. Otherwise, it would have been like the following</p>\n\n<p><code>If a page is not there in cache, it would be copied from main memory to cache and then accessed.</code> $T_1 + 2T_1$</p>\n\n<p><code>If a page is not there even in main memory, it would be brought to main memory, then cache and then accessed.</code> $T_1 + 2T_1 + 3T_1$</p>\n\n<p>$0.9[T_1] + 0.1(0.9[T_1+2*T_1] + 0.1[T_1 + 2*T_2 + 3*T_1]) = 1.23[T_1]$  <strong>(Method 2)</strong></p>\n\n<p>I went through another similar problem.</p>\n\n<p><strong>Question 2</strong></p>\n\n<pre><code>Cache Access Time = 20ns\nMemory Access Time = 120ns\nHit Ratio = 0.8\nSome other useless information below...\nCache Block size = 16 words\nSet size = 2 blocks\nNumber of sets = 128\nSize of main memory address = 21bits\nWhat is the hit ratio if the average access time is increased by 40ns?\n(A) Remains same      (B) 0.921     (C) 0.467      (D) 0.592\n</code></pre>\n\n<p>I simply calculated it using <strong>Method 1</strong> as follows</p>\n\n<pre><code>Effective access time = 0.8*20 + 0.2*(120) = 40ns\nIncrease by 40ns, so new time = 80ns\n80 = h*20 + (1-h)*120\nHit ratio = 0.4\n</code></pre>\n\n<p>But this is not in the options</p>\n\n<p>But when I calculated it using <strong>Method 2</strong></p>\n\n<pre><code>Effective access time = 0.8*20 + 0.2*(20 + 120) = 44ns\nIncrease by 40ns, so new time = 84ns\n84 = h*20 + (1-h)*120\nHit ratio = 0.467\n</code></pre>\n\n<p>That is option (C)</p>\n\n<p>Here, the answer is coming using Method 2 but in the above question they are using Method 1.</p>\n\n<p><strong>How do I know which method to take while solving such problems? Whether would the missed page be brought into the lower memory (cache) or not?</strong></p>\n", 'ViewCount': '3408', 'Title': 'Doubt regarding cache hit ratios and access time', 'LastEditorUserId': '4422', 'LastActivityDate': '2013-11-01T14:27:12.103', 'LastEditDate': '2012-12-04T18:06:20.643', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7151', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4422', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2012-12-01T06:04:25.320', 'Id': '7071'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>It might sound like a stupid question but I'm really curious to know how a computer knows that $1&lt;2$? Also, how does a computer know that the order of integer is $1,2,3,4,5,\\ldots$ and alphabet is A,B,C,D,...? Is it somewhere stored in the hardware or does the operating system provide this kind of information?</p>\n", 'ViewCount': '1872', 'Title': 'How does the computer determine whether a number is smaller or greater than another?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-10-18T13:34:24.403', 'LastEditDate': '2013-06-06T16:05:51.050', 'AnswerCount': '7', 'CommentCount': '2', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '4824', 'Tags': '<computer-architecture><reference-question>', 'CreationDate': '2012-12-01T09:09:39.713', 'FavoriteCount': '5', 'Id': '7074'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '109', 'Title': 'categories of registers and and storage in them', 'LastEditDate': '2013-01-04T22:05:56.153', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Registered User', 'PostTypeId': '1', 'OwnerUserId': '5309', 'FavoriteCount': '0', 'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Processor_register" rel="nofollow">Wikipedia article on processor registers</a> mentions:</p>\n\n<blockquote>\n  <p>Address registers hold addresses and are used by instructions that indirectly access primary memory.</p>\n</blockquote>\n\n<p>Which addresses does this sentence refer to?</p>\n', 'Tags': '<computer-architecture><memory-access>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-05T10:26:19.113', 'CommentCount': '0', 'AcceptedAnswerId': '7773', 'CreationDate': '2013-01-04T21:07:55.110', 'Id': '7772'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I took a course on compilers in my undergraduate studies in which we wrote a compiler that compiles source programs in a toy Java-like language to a toy assembly language (for which we had an interpreter). In the project we made some assumptions about the target machine closely related to "real" native executables, including:</p>\n\n<ul>\n<li>a run-time stack, tracked by a dedicated stack pointer ("SP") register</li>\n<li>a heap for dynamic object allocation, tracked by a dedicated heap pointer ("HP") register</li>\n<li>a dedicated program counter register ("PC")</li>\n<li>the target machine has 16 registers</li>\n<li>operations on data (as opposed to, e.g., jumps) are register-to-register operations</li>\n</ul>\n\n<p>When we got to the unit on using register allocation as an optimization, it made me wonder: What is the theoretical minimum number of registers for such a machine? You can see by our assumptions that we made use of five registers (SP, HP, PC, plus two for use as storage for binary operations) in our compiler. While optimizations like register allocation certainly can make use of <em>more</em> registers, is there a way to get by with fewer while still retaining structures like the stack and heap? I suppose with register addressing (register-to-register operations) we need <em>at least two</em> registers, but do we need more than two?</p>\n', 'ViewCount': '448', 'Title': 'Theoretical minimum number of registers for a modern computer?', 'LastActivityDate': '2013-12-11T00:06:01.113', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5291', 'Tags': '<compilers><computer-architecture>', 'CreationDate': '2013-01-15T06:31:34.123', 'FavoriteCount': '1', 'Id': '8941'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let r be the number of rows in a DRAM array, and c be the number of columns.</p>\n\n<p>Apparently, DRAM with organization 16x1 requires least pins when r = c = 4 because fewer address bits are required to represent them, and so does DRAM with organization 16x4. Why are these the same? Doesn't the latter have 4 columns instead of 1?</p>\n", 'ViewCount': '44', 'Title': 'Where do these DRAM row/column calculations come from?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-24T02:04:22.740', 'LastEditDate': '2013-01-23T19:43:04.990', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9125', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<computer-architecture><memory-hardware>', 'CreationDate': '2013-01-23T15:34:15.350', 'Id': '9111'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Just a basic question to ask</p>\n\n<p><strong>Does the write through cache copies the whole block or just the byte which is updated?</strong></p>\n\n<p>I went through the following question</p>\n\n<p><em>Array A contains 256 elements of 4 bytes each. Its first element is stored at physical address 4096.\nArray B contains 512 elements of 4 bytes each. Its first element is stored at physical address 8192. Assume that only arrays A and B can be cached in an initially empty, physically addressed, physically tagged, direct mapped, 2K-byte cache with an 8 byte block size. The following loop is the executed</em></p>\n\n<pre><code>for(i=0; i&lt;256; i++)\nA[i] = A[i] + B[2*i];\n</code></pre>\n\n<p><em>How many bytes will be written to memory if the cache has a write-through policy?</em></p>\n\n<p>I calculated it as follows:</p>\n\n<p>The cache can store the whole array with 2 elements per block (block size = 8bytes, element size = 4bytes). For every write, the whole block will be copied. For $0^{th}$ element, the block containing $0^{th}$ and $1^{st}$ element would be written. The same would be done for $1^{st}$ element as well.</p>\n\n<p>So, for every iteration the 2 elements would be written. This makes the number of bytes as $256*2* (4bytes / element) = 2048bytes$.</p>\n\n<p>But in the solution, they have just calculated the number of loop iterations ($256$) multiplied by the element size ($4byte$) which makes the answer $1024bytes$.\nIf this is true, then the cache would update only the updated byte (not the whole block). </p>\n\n<p>Which is correct?</p>\n', 'ViewCount': '158', 'Title': 'Does the write through cache copies the whole block or just the byte which is updated?', 'LastActivityDate': '2013-01-29T16:41:15.240', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '9264', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4422', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-01-27T19:36:19.230', 'Id': '9221'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1713', 'Title': 'The amount of ROM needed to implement a 4-bit multiplier?', 'LastEditDate': '2013-02-04T14:29:19.593', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6699', 'FavoriteCount': '0', 'Body': '<p>For a 4-bit multiplier there are $2^4 \\cdot 2^4 = 2^8$ combinations.</p>\n\n<p>The output of 4-bit multiplication is 8 bits, so the amount of ROM needed is $2^8 \\cdot 8 = 2048$ bits.</p>\n\n<p>Why is that?  Why does the ROM need all the combinations embedded into it?</p>\n\n<p>What will be the case with RAM?</p>\n', 'Tags': '<computer-architecture>', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-02-04T16:20:13.643', 'CommentCount': '2', 'AcceptedAnswerId': '9477', 'CreationDate': '2013-02-04T13:38:48.620', 'Id': '9471'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the following homework problem:</p>\n\n<blockquote>\n  <p>A 10 TB disk drive has an MTTF of 6,000,000 hours.  How much data can we store in a system comprised of these disks, if we want the system MTTF to be at least 1.2M hours?</p>\n  \n  <p>If we are allowed to make it redundant (i.e., two of them operating in a parallel mode), and if the MTTR is 120 hours, what is the system MTTF?</p>\n</blockquote>\n\n<p>I think it is</p>\n\n<p>$\\qquad \\operatorname{MTTF}(\\text{System}) = \\frac{1}{\\sum_i \\frac{1}{\\operatorname{MTTF}_i}}$.     </p>\n', 'ViewCount': '338', 'Title': "What is the system's mean time to failure?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-07T15:11:15.657', 'LastEditDate': '2013-02-06T22:30:19.627', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6722', 'Tags': '<computer-architecture><reliability>', 'CreationDate': '2013-02-05T23:05:49.330', 'FavoriteCount': '0', 'Id': '9526'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given this formula how do you calculate the following? I don\'t understand, can some one explain?</p>\n\n<pre><code>                      1\nMTTF(System) = ------------------------\n           [Summation]      1\n                       ------------\n                          MTTFi       \n</code></pre>\n\n<p>(was going to upload an image of the formula but "they" don\'t let me)</p>\n\n<ul>\n<li>A 10 TB disk drive has an MTTF of 6,000,000 hours.  How much data can we store in a system comprised of these disks, if we want the system MTTF to be at least 1.2M hours?</li>\n<li>If we are allowed to make it redundant (i.e., two of them operating in a parallel mode), and if the MTTR is 120 hours, what is the system MTTF?</li>\n</ul>\n', 'ViewCount': '23', 'ClosedDate': '2013-02-06T22:27:27.413', 'Title': 'Mean time to failure calculation help', 'LastActivityDate': '2013-02-06T21:38:51.780', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6722', 'Tags': '<computer-architecture>', 'CreationDate': '2013-02-06T15:12:37.010', 'FavoriteCount': '0', 'Id': '9546'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>CPU caches are used by exploiting temporal and spatial locality. My question is who is responsible for managing these caches? Is this Operating system that identifies a particular access pattern and then manages (i.e store the data in) cache, using low level OS function calls? </p>\n', 'ViewCount': '136', 'Title': 'CPU Cache is managed by which software component?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T21:06:59.673', 'LastEditDate': '2013-02-24T16:08:45.417', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '10054', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-02-24T08:20:21.490', 'Id': '10043'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading William Stallings Computer Organization &amp; Architecture to understand about control unit &amp; micro-operations. </p>\n\n<p>Stallings explain that interrupt cycle requires 3 time units to complete : </p>\n\n<pre><code>t1 : MBR &lt;- (PC)\nt2 : MAR &lt;- Save_Address\n     PC &lt;- Routine_Address\nt3 : Memory &lt;- (MBR) \n</code></pre>\n\n<p>t1 : Save the current PC value to MBR</p>\n\n<p>t2 : Save_Address provides the memory location where the value of PC has to be stored. It is saved in MAR.</p>\n\n<p>Routine_Address gives the address for Interrupt Service Routine</p>\n\n<p>t3 : The MBR (which has old value of PC) is stored to memory location (whose address provided by MAR)</p>\n\n<p>But this is how I am thinking, the above operations can be completed in only 2 time units. </p>\n\n<pre><code>t1 : MBR &lt;- (PC)\n     MAR &lt;- Save_Address\nt2 : PC &lt;- Routine_Address\n     Memory &lt;- (MBR) \n</code></pre>\n\n<p>I checked twice &amp; I find no dependency among micro-operations performed in time unit t1 &amp; t2. </p>\n\n<p>So, is my approach correct ?</p>\n', 'ViewCount': '451', 'Title': 'Time units required for Interrupt Cycle', 'LastActivityDate': '2013-03-20T19:11:53.357', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10660', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture>', 'CreationDate': '2013-03-02T14:01:26.820', 'Id': '10210'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Which of the following modes of data transfer is the fastest?</p>\n\n<p>a. DMA</p>\n\n<p>b. Interrupt-based</p>\n\n<p>c. Polling</p>\n\n<p>d. All are equally fast</p>\n\n<p>I do not have the answer, so I cannot check, that's why I am posting here. </p>\n\n<p>I think the answer is D. All are equally fast. In all data transfer cases the speed of data transfer is same, but in case of DMA we save CPU time by that we improve the efficiency of CPU since data transfer job is done by DMA. Is my understanding correct ? </p>\n", 'ViewCount': '1046', 'Title': 'Fastest mode of data transfer', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-03-03T20:06:25.740', 'LastEditDate': '2013-03-03T18:26:36.387', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10243', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture>', 'CreationDate': '2013-03-03T18:02:08.450', 'Id': '10238'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there any kind of CPU which doesn't contain an ALU ?</p>\n", 'ViewCount': '407', 'Title': 'Does any CPU not contain an ALU?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-03-31T03:59:16.537', 'LastEditDate': '2013-03-16T14:50:36.330', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7301', 'Tags': '<computer-architecture>', 'CreationDate': '2013-03-16T11:23:03.913', 'Id': '10550'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would I solve the following can anyone help me.I know MIPS is basically how many instruction the processor can do per second but what should I do?</p>\n\n<p>Assume that we are receiving a message across a network using a modem with a rate of 56,000 bits/second. Furthermore assume that we are working on a workstation with an instruction rate of 500 mips. How many instructions can the processor execute between the receipt of each individual bit of the message?</p>\n', 'ViewCount': '413', 'Title': 'Network modem question', 'LastEditorUserId': '157', 'LastActivityDate': '2013-03-18T01:17:39.627', 'LastEditDate': '2013-03-17T21:29:54.510', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '10589', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<computer-architecture><coding-theory><arithmetic>', 'CreationDate': '2013-03-16T20:33:11.283', 'Id': '10561'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How is Perfect shuffle a better interconnect scheme for parallel processing? For example if we consider a problem of sum reduction, I want to understand how this scheme is useful when implementing sum reduction in parallel , for example on a GPU?    </p>\n', 'ViewCount': '300', 'Title': 'Perfect shuffle in parallel processing', 'LastActivityDate': '2013-03-17T14:33:37.193', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10582', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<computer-architecture><parallel-computing>', 'CreationDate': '2013-03-17T05:12:07.500', 'FavoriteCount': '1', 'Id': '10572'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In all computer architecture books we study that Cache memory could be divided into 3 levels (L1,L2 and L3) and its very beneficial to do so. Why don't we use the same approach in case of main memory (RAM). Is there any particular reason that we avoid this?</p>\n", 'ViewCount': '107', 'Title': 'Computer Architecture-3 level RAM hierarchy', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-20T20:49:01.840', 'LastEditDate': '2013-03-20T20:49:01.840', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '10647', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7347', 'Tags': '<computer-architecture><memory-hardware>', 'CreationDate': '2013-03-20T10:27:37.090', 'Id': '10641'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading Computer Architecture &amp; Organization by William Stallings to understand I/O operations. Stallings pretty well explains why Programmed I/O (CPU keeps checking the I/O module register status) &amp; Interrupt I/O (CPU still has to over look data transfer between I/O module &amp; memory) are not efficient &amp; introduces to DMA, where DMA itself handles everything. </p>\n\n<p>But, however, he also mentions that during a DMA operation, CPU sits idle &amp; has no control over memory bus. If CPU has to sit idle, then how it is better than other two methods ? </p>\n\n<p>Page no. 415, Computer Architecture &amp; Orgazination by Morris Mano:</p>\n\n<blockquote>\n  <p>During the DMA transfer, the CPU is idle and has no control of the memory.</p>\n</blockquote>\n\n<p>Only way it make sense to me is that, CPU can perform any operation which does not involve memory bus during a DMA operation. So, CPU will not be idle. Or am I missing something ?</p>\n\n<p>I think author has formulated in a bad way. It can be phrased like: </p>\n\n<blockquote>\n  <p>During DMA transfer, the CPU has no control of memory buses and thus cannot perform any operations involving memory. However it can perform other operations like arithmetic, logical or can operate on data in cache. </p>\n</blockquote>\n\n<p>Am I right?</p>\n', 'ViewCount': '470', 'Title': 'How DMA improves I/O operation efficiency?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T20:13:03.183', 'LastEditDate': '2013-04-02T15:15:16.130', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '10900', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture><memory-hardware><memory-access>', 'CreationDate': '2013-03-29T15:48:19.407', 'Id': '10896'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider an array A[100] &amp; each element occupies 4 word. A 32 word cache is used and divided into 8 word blocks. What is the hit ratio for the following statement. Assume one block is read into cache in case of miss:</p>\n\n<pre><code>for(i=0;i&lt;100;i++)\n  A[i]=A[i]+10\n</code></pre>\n\n<p>each array element is of 4 word &amp; each cache block size is 8 words, so we load 8 words into cache.</p>\n\n<p>When program tries to read A[0], for the first time it will be miss. Hence it will be brought to memory (&amp; also A[1]). Next, A[1] will be hit. So, it will be like : </p>\n\n<ul>\n<li>A[0] - Miss  </li>\n<li>A[1] - Hit  </li>\n<li>A[2] - Miss  </li>\n<li>A[3] - Hit</li>\n<li>.....</li>\n</ul>\n\n<p>So hit ratio is 50% ? Am I wrong anywhere ? </p>\n\n<p>I also have one more doubt. First when it tries to access A[0], it will be miss. And then when it brings A[0] to cache, CPU tries to access A[0] again, now it should be considered as hit ? Like, </p>\n\n<ul>\n<li>A[0] (Read) - Miss</li>\n<li>A[0] (Write) - Hit</li>\n<li>A[1] (Read) - Hit</li>\n<li>A[1] (Write) - Hit</li>\n</ul>\n\n<p>If above is correct, then Hit ratio will be 75%.</p>\n\n<p>Any help regarding this is appreciated.</p>\n', 'ViewCount': '345', 'Title': 'Finding hit ratio of a cache', 'LastEditorUserId': '6665', 'LastActivityDate': '2014-04-29T12:23:20.487', 'LastEditDate': '2013-04-08T05:00:21.610', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-04-08T03:35:41.280', 'Id': '11132'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://www.mikroe.com/img/publication/pic-books/programming-in-c/chapter/03/fig3-3.gif" rel="nofollow">PIC16F887 Block Diagram</a></p>\n\n<p>According to the block diagram above, since we already have Program Memory, which may be used to store our program, why should we still need EEPROM? What is it for?</p>\n', 'ViewCount': '153', 'Title': 'Why we need EEPROM in this micro-controller', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-10T19:46:32.367', 'LastEditDate': '2013-04-10T19:46:32.367', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11193', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1374', 'Tags': '<computer-architecture><memory-hardware>', 'CreationDate': '2013-04-10T14:28:37.250', 'Id': '11192'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Following question was asked in one of entrance exams for a graduation programme. Please help me try to solve it : </p>\n\n<blockquote>\n  <p>A computer system has an L1 cache, an L2 cache, and a main memory\n  unity connected as shown below. The block size in L1 cache is 4 words.\n  The block size in L2 cache is 16 words. The memory access times are 2\n  nanoseconds, 20 nanoseconds and 200 nanoseconds for L1 cache, L2 cache\n  and main memory unit respectively.</p>\n  \n  <p><img src="http://i.stack.imgur.com/7zo5X.gif" alt="cache-image"></p>\n  \n  <ol>\n  <li>When there is a miss in L1 cache and a hit in L2 cache, a block is\n  transferred from L2 cache to L1 cache. What is the time taken for this\n  transfer?</li>\n  </ol>\n  \n  <p>(A) 2 nanoseconds \n  (B) 20 nanoseconds\n  (C) 22 nanoseconds \n  (D) 88 nanoseconds</p>\n  \n  <ol>\n  <li>When there is a miss in both L1 cache and L2 cache, first a block\n  is transferred from main memory to L2 cache, and then a block is\n  transferred from L2 cache to L1 cache. What is the total time taken\n  for these transfers?</li>\n  </ol>\n  \n  <p>(A) 222 nanoseconds (B) 888 nanoseconds\n  (C) 902 nanoseconds (D) 968 nanoseconds</p>\n</blockquote>\n\n<p>First thing that came to my mind was, how to calculate the transfer time using the given access time. During a miss, a block of data is moved from main memory to cache. Then CPU will access it. So, wouldn\'t be access time > transfer time ?</p>\n\n<p>Then I thought, lets assume access time = transfer time &amp; do the calculation. </p>\n\n<p>Now first question. The question already states there is a miss in L1, so I will not consider L1 access time. Since there is a miss in L1 &amp; hit in L2, a entire block from L2 has to be moved to L1. L2 block size is 16 words, but data bus size is 4 words. </p>\n\n<p>So we have to move 4 words * 4 times. </p>\n\n<p>To transfer 4 word it takes 20 ns. To transfer 4 words, its 80ns. Isn\'t it the time transferred from L2 to L1 ? The question does not say anything about accessing L1 after moving the data. But 80ns is not in the option ! </p>\n\n<p>Similar case with second question also. </p>\n\n<p>Time to move main memory to L2 = 4 words * 4 times = 4 * 200 = 800ns </p>\n\n<p>Time to move L2 to L1 = 80ns [earlier calculation]</p>\n\n<p>So total time taken is 880ns. Which is again not in the option. </p>\n\n<p>Either I am doing a very big mistake or options are wrong or question isn\'t framed correctly. If I am doing anything wrong, please give me some hint &amp; I will try to work on this exercise again. </p>\n', 'ViewCount': '465', 'Title': 'Finding cache block transfer time in a 3 level memory system', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T18:04:13.117', 'LastEditDate': '2013-04-14T11:46:23.223', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '11311', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<computer-architecture><cpu-cache><memory-access>', 'CreationDate': '2013-04-14T09:56:15.187', 'Id': '11309'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>The Situation</strong></p>\n\n<p>I\'m trying to read the book \'Digital Design Computer Architecture\'.<br>\nIn the part of Performance Analysis(7.3.4 in the book), Author refers to clock cycle for MIPS single cycle processor. But I think there\'s something wrong with author\'s evaluating clock cycle.  </p>\n\n<p>Author says,</p>\n\n<pre><code>T(Clock cycle) = T(pcq_pc) + T(mem) + Max[tRfread, t(sext)] + t(Mux)  \n                 + T(ALU) + T(mem) + t(mux) + T(RFsetup)\n</code></pre>\n\n<p>But I think clock cycle \'T\' would be ( if T(RFread) > T(mux) + T(sext) )</p>\n\n<pre><code>T(Clock cycle) = T(pcq_pc) + T(mem) + T(RFread) + T(ALU) + T(mem) + T(mux) + T(Rfsetup)\n</code></pre>\n\n<p>Since I think the MUX followed by ALU <strong>already selects</strong> known sign-extend immediate before register value(the pin <code>RD1</code> in below diagram) is known. </p>\n\n<p>and if T(RFread) is less than T(mux) plus T(sext).</p>\n\n<pre><code>T(Clock cycle) = T(pcq_pc) + T(mem) + T(sext) + T(Mux)  \n                 + T(ALU) + T(mem) + T(mux) + T(RFsetup)\n</code></pre>\n\n<p><strong>Some referenced information by the book.</strong></p>\n\n<ul>\n<li>T(pcq_pc) is propagation delay for PC.  </li>\n<li>T(mem) is read-propagation delay for instruction memory and data memory.  </li>\n<li>T(sext) is propagation delay for Sign Extend logic.  </li>\n<li>T(RFread) is read-progagation delay for register file.  </li>\n<li>T(mux) is propagation delay for MUX.  </li>\n<li>T(ALU) is propagation delay for ALU.  </li>\n<li>T(RFsetup) is setup time for Register file.(Register value is written on positive clock edge)</li>\n</ul>\n\n<p>Here is Diagram for MIPS single cycle CPU.</p>\n\n<p><img src="http://i.stack.imgur.com/t4Tab.png" alt="enter image description here"></p>\n\n<p>Is my thought correct? If you think author is right, Could you tell my why?<br>\nAny help would be awesome!</p>\n', 'ViewCount': '250', 'Title': 'How to evaluate the clock cycle for MIPS single cycle CPU', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-05-04T13:56:00.810', 'LastEditDate': '2013-05-04T13:56:00.810', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '11353', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7508', 'Tags': '<computer-architecture>', 'CreationDate': '2013-04-15T15:56:19.100', 'Id': '11338'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A 1-bit ripple carry full adder uses 7 AND gates in parallel at the first level of inputs and use 2 OR gates at the second level.If each gate has the propagation delay = .002 micro second , then the delay at each bit adder module will be .004 micro second.What will be the time taken by 8 bit adder.</p>\n', 'ViewCount': '162', 'Title': 'ripple carry full adder', 'LastActivityDate': '2013-05-08T18:38:09.247', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8093', 'Tags': '<computer-architecture>', 'CreationDate': '2013-05-08T18:38:09.247', 'Id': '11898'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question stems from a few answers and comments on a question I posted in signal processing <a href="http://dsp.stackexchange.com/questions/9132/implement-a-software-pid-for-the-first-time-in-real-time-software-i-can-find-p">found here</a>.</p>\n\n<p>I guess I am a little confused. Are there any concrete differences between realtime software/hardware and just a regular PC? </p>\n\n<p>I would try to list what I think the differences are but the list I have come up with is ambiguous and short. </p>\n\n<p>The only thing I think makes sense to separate the two ideas is that PC software can run part of its code part of the time, real-time runs all of its code each time. So a PC can load a program but not all programs and run just that program for however long it pleases, real-time is just a single program with a bunch of if-else, switch logic basically a huge abstract collection of relays (PLC) that gets ran through every time it is called. </p>\n\n<p>Yes I realize this is a pretty poor description, so I ask is there any good concrete ways to separate these ideas? </p>\n', 'ViewCount': '122', 'Title': 'Realtime hardware/software versus PC software/hardware, how are these distinct and alike?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-19T11:49:29.343', 'LastEditDate': '2013-05-17T06:52:01.733', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12073', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8214', 'Tags': '<terminology><computer-architecture>', 'CreationDate': '2013-05-16T13:24:39.720', 'Id': '12067'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If I am correct, chips cannot get much smaller because of Heisenberg's uncertainty principle. My friend and I want to perform an experiment (which is cheap, i.e. doesn't require million-dollar equipment) which shows that if a chip is too small, it will mess up. Is there any such experiment?</p>\n", 'ViewCount': '88', 'ClosedDate': '2013-05-19T07:04:46.030', 'Title': "Is there an affordable experiment which shows chips can't get much smaller?", 'LastActivityDate': '2013-05-16T16:21:42.987', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8215', 'Tags': '<computer-architecture>', 'CreationDate': '2013-05-16T13:47:22.937', 'FavoriteCount': '1', 'Id': '12069'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am reading two books, <em>Computer organization and design</em> by <em>David A Patterson</em>, and <em>Digital Design and Computer Architecture</em> by <em>Harris and Harris</em>. These books claim that a 1 bit branch predictor mispredicts on the first and last iterations of every loop.<br>\nI don't understand why it mispredicts on the first iteration. I think that the first prediction result depends on the history of the CPU, so we don't know whether the predictor would predict correctly or not.<br>\nMy question is whether I have to assume that the initial state of the 1 bit predictor is for the branch to be taken.  </p>\n", 'ViewCount': '231', 'Title': 'Why do most books say that a 1 bit branch predictor mispredicts on the first loop iteration?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-05-22T15:59:19.990', 'LastEditDate': '2013-05-22T15:59:19.990', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12215', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7508', 'Tags': '<computer-architecture>', 'CreationDate': '2013-05-22T09:02:25.200', 'Id': '12213'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '710', 'Title': 'What are flops and how are they benchmarked?', 'LastEditDate': '2013-06-11T22:04:30.953', 'AnswerCount': '4', 'Score': '1', 'OwnerDisplayName': 'Vincent Warmerdam', 'PostTypeId': '1', 'OwnerUserId': '8627', 'Body': '<p>Apple has just proudly stated that their new mac pro will be able to give up to 7 teraflops of computing power. Flops stands for Floating Point Operations Per Second. How exactly is this benchmarked though? Certain floating point operations are much heavier than others, so how exactly would a FLOP be a benchmark for computing power?  </p>\n', 'Tags': '<computer-architecture><efficiency><benchmarking>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-06-30T14:30:31.150', 'CommentCount': '1', 'AcceptedAnswerId': '12615', 'CreationDate': '2013-06-10T22:10:32.837', 'Id': '12606'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a deep appreciation for formalisms like the Turing Machine and the $\\lambda$-Calculus, and enjoy studying them and learning more about how they relate to physical computers. I am now learning about writing GUI programs, and the graphical library (GTK) relies on things like signals and callbacks, which I have not seen modeled by either Turing Machines or the $\\lambda$-Calculus; <strong>can either the $\\lambda$-Calculus or Turing Machines model things like signals, callbacks, sleeping/waiting, or buses?</strong> If so, where can I find some good reference material to learn more? If not, why not? and are there any formalisms which are capable of expressing such things?</p>\n', 'ViewCount': '146', 'Title': 'Can the Lambda Calculus or Turing Machines model signals, callbacks, sleep/wait, or buses?', 'LastActivityDate': '2013-09-18T18:46:18.427', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5291', 'Tags': '<turing-machines><computer-architecture><lambda-calculus><church-turing-thesis>', 'CreationDate': '2013-06-20T00:58:47.687', 'Id': '12771'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I pulled this right from Wikipedia to compare, staring with an SoC:</p>\n\n<blockquote>\n  <p>A system on a chip or system on chip (SoC or SOC) is an integrated circuit (IC) that integrates all components of a computer or other electronic system into a single chip.</p>\n</blockquote>\n\n<p>And to compare, here's the microcontroller:</p>\n\n<blockquote>\n  <p>A microcontroller (sometimes abbreviated \xb5C, uC or MCU) is a small computer on a single integrated circuit containing a processor core, memory, and programmable input/output peripherals.</p>\n</blockquote>\n\n<p>What is the difference? If they are both small computers that integrate all components on a single integrated circuit, and are limited, why are they regarded as different concepts or terms? A microcontroller has everything an SoC has, so why are they claimed to be different? </p>\n\n<p>To elaborate further, what draws any tangible line here on any noteworthy differences?</p>\n\n<p>They both are in the area of embedded systems, but aside from minimal differences, they are both seemingly exact in almost every way.</p>\n", 'ViewCount': '594', 'Title': 'Difference between an a microcontroller and a system on a chip?', 'LastActivityDate': '2013-06-21T04:41:52.500', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8789', 'Tags': '<computer-architecture>', 'CreationDate': '2013-06-20T20:40:38.507', 'Id': '12798'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Regarding Processor Direct Cache, what is the proper mathematical technique for discovering how many words are loaded on a cache miss? </p>\n\n<p>For example if you have a direct mapped cache with a total data cache size of 32 words and a direct mapped cache of 4-word blocks. I believe I have a cache index of 8 cache blocks:</p>\n\n<p>32/4 = 8</p>\n\n<p>Example: Finding a block in python:</p>\n\n<pre><code>var1 = 56\nprint ((var1 / 8)% 8)\n</code></pre>\n\n<p>The answer: 7</p>\n\n<p>So finding where to store the blocks is simple enough but I'm uncertain on a miss exactly how <strong>MANY</strong> words will be loaded? </p>\n", 'ViewCount': '68', 'Title': 'How many words loaded on a cache miss', 'LastActivityDate': '2013-06-29T15:59:56.883', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8925', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-06-29T15:59:56.883', 'Id': '12967'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I recall hearing briefly about this back in one of my CS courses on hardware, but I can't recall many of the details, nor can I find anything online that talks about it. A search on Google only yields a bunch of pages about Android emulators and people complaining that PCSX2 is slow. I'd like to know, in general, why software emulation of hardware is inefficient.</p>\n\n<p>Can anybody point me to a good resource or explain briefly here?</p>\n", 'ViewCount': '127', 'Title': 'Why is software emulation of alternative architectures so hard/slow?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-07T22:58:43.460', 'LastEditDate': '2013-07-06T13:36:45.243', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6569', 'Tags': '<computer-architecture><simulation>', 'CreationDate': '2013-07-05T23:06:29.567', 'FavoriteCount': '1', 'Id': '13100'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '436', 'Title': 'Despite it is so important, why don\'t computer science departments offer a class named "memory management"?', 'LastEditDate': '2013-07-16T10:09:37.950', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Kutluhan Metin', 'PostTypeId': '1', 'OwnerUserId': '8744', 'Body': "<p>I don't understand why computer science departments don't offer a class named memory management? I see that most of the problems encountered in computer science are about memory management concepts. As being a computer science student, my department does not offer a class about that. I took a class named file organization which helped me understand harddisk management, I would like to learn memory management very well. If is there anybody who can explain this, it would be very useful.</p>\n", 'ClosedDate': '2014-02-02T11:25:24.150', 'Tags': '<computer-architecture><education><memory-management>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-30T17:48:20.853', 'CommentCount': '0', 'CreationDate': '2013-07-15T06:23:02.950', 'Id': '13278'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Specifically:</p>\n\n<p>1) A <strong>direct-mapped cache</strong> with 4096 blocks/lines in which each block has 8 32-bit words. How many bits are needed for the tag and index fields, assuming a 32-bit address?</p>\n\n<p>2) Same question as 1) but for <strong>fully associative cache</strong>?</p>\n\n<p>Correct me if I'm wrong, is it:</p>\n\n<blockquote>\n  <p>tag bits = address bit length - exponent of index - exponent of offset? </p>\n  \n  <p>[Is the offset = 3 due to 2^3 = 8 or is it 5 from 2^5 = 32?] </p>\n</blockquote>\n", 'ViewCount': '4829', 'Title': 'How to calculate the tag, index and offset fields of different caches?', 'LastEditorUserId': '9161', 'LastActivityDate': '2013-12-15T09:15:20.893', 'LastEditDate': '2013-07-20T09:26:03.933', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '13359', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9161', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2013-07-20T09:08:01.743', 'Id': '13356'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In his lecture <a href="http://cm.bell-labs.com/who/ken/trust.html" rel="nofollow"><em>Reflections on Trusting Trust</em></a>, Ken Thompson describes a virus that infects a compiler; the infected compiler installs backdoors into programs, but the key part is that the infected compiler also infects all compilers it compiles.</p>\n\n<p>K. Thompson notes that if this was implemented at the Microcode level, it would be almost impossible to stop.</p>\n\n<p>My question is this; in general terms (i.e. I\'m not going to ask for details on how to create the most insidious trojan that has ever existed) how can you write microcode that would inject Thompson\'s trojan into every program compiled on a computer with that microcode.</p>\n\n<p>As far as I can tell, the core of this question (i.e. the most important part, theoretically speaking) is:</p>\n\n<blockquote>\n  <p>Can the code implementing machine code (such as microcode) detect when the machine code it is implementing is a compiler that is compiling (in order to inject trojans into the compiled program), and if so, how?</p>\n</blockquote>\n', 'ViewCount': '105', 'Title': 'In general terms, how could the Thompson Hack be implemented in Microcode?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-25T12:44:17.090', 'LastEditDate': '2013-08-26T11:15:16.100', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9797', 'Tags': '<computer-architecture><compilers><security>', 'CreationDate': '2013-08-23T10:26:52.827', 'Id': '13885'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Wikipedia says that <a href="https://en.wikipedia.org/wiki/Shared_memory" rel="nofollow">shared memory</a> comes with lots of costs associated with cache coherence costs. But I thought the whole idea of shared memory is that all the CPUs access the same memory? So if one CPU changes that memory then other CPUs would access the same value? It would seem like this would require FEWER cache coherence costs? Is the idea that if one CPU changes its local cache before it writes to shared memory then other CPUs have to be notified?</p>\n', 'ViewCount': '249', 'Title': 'Why do you have to worry about cache coherence if you are using shared memory?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T10:29:48.617', 'LastEditDate': '2013-09-10T10:29:48.617', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '14241', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2605', 'Tags': '<computer-architecture><distributed-systems><parallel-computing>', 'CreationDate': '2013-09-10T00:25:05.073', 'Id': '14240'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A DRAM that must be given a refresh cycle 64 times per ms.Each refresh requires 150ns,a memory cycle requires 250 ns. What is the approximate percentage of the  memory's total operating time must be given to refreshes?</p>\n\n<p>Can someone explain how to find the solution?</p>\n", 'ViewCount': '80', 'Title': "Approximate percentage of the memory's total operating time for refreshes while refreshing DRAM", 'LastActivityDate': '2013-09-13T17:33:48.733', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '10109', 'Tags': '<computer-architecture>', 'CreationDate': '2013-09-12T18:39:37.370', 'Id': '14287'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '216', 'Title': 'Confused by Floating Point Spacing', 'LastEditDate': '2013-09-17T12:06:36.347', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Mike N.', 'PostTypeId': '1', 'OwnerUserId': '10176', 'Body': "<p>I'm currently taking a numerical analysis class in college and we're covering floating point systems. For the most part, I have a good grasp on it. However, something I can't seem to visualize, and haven't seen any totally lucid explanations about after searching extensively, is spacing between floating point numbers. Also of note is that I'm talking about IEEE-754 here, but it applies to general systems too.</p>\n\n<h3>The Things I Do Understand:</h3>\n\n<ul>\n<li>The area between $[-1,1]$ is a denormalized area.</li>\n<li>The areas after $1$ and less than $-1$ are where the normalized floating point numbers reside.</li>\n<li>The floating point numbers <em>between</em> perfect powers of the base are uniformly spaced, but the spacing varies from one perfect power of the base to another.</li>\n<li>The spacing between values between two perfect powers is proportional to the power on the left for positive numbers and the power on the right for negative numbers. (i.e. on a number line, the uniformly-spaced values between two low powers are closer together than between a higher power.)</li>\n</ul>\n\n<h3>What I'm Struggling to Understand</h3>\n\n<ul>\n<li><p>From my understanding, the machine epsilon $\\epsilon$ is a fundamental unit of spacing with respect to the floating point number line. That is, between $[1,B]$ where $B$ is the base, all values are $\\epsilon$ apart. Then, you can scale any arbitrary floating point number by that fundamental machine epsilon and that product is the uniform spacing for that floating point number's associated power range. Is this even a correct interpretation? </p>\n\n<p>I also read that $\\epsilon$ is an upper bound for relative error, so I'm not really sure how that fits into my explanation of it being an indivisible spacing unit.</p></li>\n<li><p>One of the questions I haven't been able to answer is what the minimum and maximum spacing between two positive floating point numbers is. I can trick myself into thinking I understand why multiplying the x's associated $B^e \\cdot \\epsilon$, where x is an arbitrary floating point number and $e$ is that number's corresponding exponent, yields the upper bound on error and therefore spacing, so $B^e \\cdot \\epsilon$ would be the maximum spacing. </p>\n\n<p>Minimum spacing truly boggles my mind right now, though. If the machine epsilon is the indivisible unit of spacing, then for example, how could we have more minimal spacing than between $1$ and $1 + \\epsilon$? Wouldn't that just be left to the rounding rule used (if round-to-nearest, it would depend whether the number is closer to $1$ or $1 + \\epsilon$, since it'll be rounded to one of those two).</p></li>\n</ul>\n\n<p>Basically, if you could explain this in plain-english it would really help me get a solid understanding of what's going on at the number line level.</p>\n", 'Tags': '<computer-architecture><floating-point><number-formats>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-17T12:06:36.347', 'CommentCount': '0', 'AcceptedAnswerId': '14370', 'CreationDate': '2013-09-16T23:24:15.270', 'Id': '14369'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>if a,b are both binary bit. a=0, and b=0, what is least significant bit,and most significant bit of a+b, which means 0+0?\nif c is the carry bit, and c=1, what is the least significant bit,and most significant bit of a+b when a=0,b=1</p>\n\n<p>My thought is 0+0=0, thus least significant bit,and most significant bit are same, which is 0. and if C is carry bit, than a+b=1, then add carry bit, which is 10, so the the least significant bit is 0, and msb is 1..am I right? someone help</p>\n', 'ViewCount': '193', 'Title': 'least significant bit,and most significant bit', 'LastActivityDate': '2013-09-27T07:33:53.723', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10355', 'Tags': '<computer-architecture>', 'CreationDate': '2013-09-27T04:12:25.113', 'Id': '14635'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to calculate the average memory access time of a 2-level cache with a split L1 cache. I am given the 3 formulas below:</p>\n\n<p><strong>Given</strong></p>\n\n<p>Basic Formula:</p>\n\n<pre><code>AMAT = HitTimeL1 + MissRateL1 x MissPenaltyL1\n</code></pre>\n\n<p>2-Level Cache:</p>\n\n<pre><code>*Substitution: MissPenaltyL1 = HitTimeL2 + MissRateL2 x MissPenaltyL2\n\nAMAT = HitTimeL1 + MissRateL1 x (HitTimeL2 + MissRateL2 x MissPenaltyL2)\n</code></pre>\n\n<p>Split-Level Cache:</p>\n\n<pre><code>AMAT_SplitL1 = AMAT_InstructionCache + AMAT_DataCache\n</code></pre>\n\n<p><strong>Assumptions</strong></p>\n\n<p>Based on the formulas above I have substituted the 2-Level cache formulas into each of the split L1 caches. I am not sure if this is correct. If anyone could verify this for me or provide me with some documentation that supports this type of calculation that would be awesome. Thanks.</p>\n\n<pre><code>AMAT = (HitTimeL1Inst + MissRateL1Inst x (HitTimeL2 + MissRateL2 x MissPenaltyL2))\n     + (HitTimeL1Data + MissRateL1Data x (HitTimeL2 + MissRateL2 x MissPenaltyL2))\n</code></pre>\n', 'ViewCount': '896', 'ClosedDate': '2014-05-03T23:50:27.440', 'Title': 'Average Memory Access Time for Split/2-Level Cache', 'LastEditorUserId': '10776', 'LastActivityDate': '2014-04-24T14:16:30.577', 'LastEditDate': '2013-10-17T01:27:34.370', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10776', 'Tags': '<computer-architecture><cpu-cache><memory-access>', 'CreationDate': '2013-10-16T00:33:03.687', 'Id': '16121'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Any CS class about caches will at some point address this classical formula (or a variant of it)</p>\n\n<pre><code>Effective_access_time = hit_time + miss_penalty * miss_rate\n</code></pre>\n\n<p>My question is simple: <strong>does this formula have a "name" ?</strong> (i.e. unambiguous, and ideally, well-accepted too). I found plenty of references to many <em>variants</em> of that concept. I\'d like to know whether there exists one name to denote them all. By the way, the question makes sense : there are plenty of variants of the following sentence... </p>\n\n<blockquote>\n  <p>In any square triangle, the square of the length of the hypotenuse is equal\n  to the sum of the squares of the lengths of the two shorter sides.</p>\n</blockquote>\n\n<p>...but we prefer to refer to them all by the name "Pythagoras\' theorem".</p>\n', 'ViewCount': '98', 'Title': u'Is there a prefered name for the \u201ceffective access time\u201d formula?', 'LastActivityDate': '2013-10-23T16:16:48.327', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6719', 'Tags': '<terminology><computer-architecture><cpu-cache>', 'CreationDate': '2013-10-23T16:16:48.327', 'FavoriteCount': '1', 'Id': '16368'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Are they used for <i>saving</i> the information; <i>sending</i> the information to the processor or <em>finding</em> the information? (or something else)</p>\n\n<p>Edit: I am talking about the <i>direct</i>,  <i>fully assotiative</i> and  <i>set associative </i> mapping. I hope this clears things up a bit.</p>\n', 'ViewCount': '39', 'Title': 'What are the cache mapping algorithms used for?', 'LastEditorUserId': '11137', 'LastActivityDate': '2013-11-03T13:52:04.777', 'LastEditDate': '2013-11-03T13:52:04.777', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11137', 'Tags': '<algorithms><computer-architecture>', 'CreationDate': '2013-11-02T15:33:48.060', 'Id': '16652'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am reading about disk redudancy.<br>\nI read the following:  </p>\n\n<blockquote>\n  <p>Suppose that the mean time to failure of a disk is 100,000 hours. Then\n  the mean time to failure of some disk in an array of 100 disks will be\n  100,000/100 = 1000 hours  </p>\n</blockquote>\n\n<p>I don't understand this. Why isn't it 100,000^100 instead?<br>\nThen the same textbook says in the next paragraph concerning 2 mirrored disks:  </p>\n\n<blockquote>\n  <p>If the mean time to failure of a single disk is 100,000 hours and the\n  mean time to repair is 10 hours then the mean time to data loss is\n  (100,000^2)/2*100   </p>\n</blockquote>\n\n<p>Now here it multiplies for the 2 disks but before the MTTF was divided.  </p>\n\n<p>Can anyone please help me figure out how we calculate this.<br>\nIn case it matters the text book is Database Concepts from Silberschatz 4th edition paragraph 11.3.1 (Improvement of reliability via redundancy) page 403</p>\n", 'ViewCount': '87', 'Title': 'Why is the Mean Time To Failure of multiple disks calculated via division and not multiplication?', 'LastActivityDate': '2013-11-12T01:18:56.163', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11321', 'Tags': '<operating-systems><computer-architecture><database-theory><reliability>', 'CreationDate': '2013-11-11T23:22:13.347', 'Id': '17933'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to subtract these two binary numbers:</p>\n\n<p>$  1110\n- 1011$</p>\n\n<p>First I convert 1011 to two's complement by doing 1011 to 0100 and then adding 1 to get 0101. Then I add the first number to the converted two's complement number:</p>\n\n<p>$1110$</p>\n\n<p>$+$ $0101$</p>\n\n<p>$-----$</p>\n\n<p>$0011$</p>\n\n<p>0 + 1 = 1</p>\n\n<p>1 + 0 = 1</p>\n\n<p>1 + 1 = 0 carry 1</p>\n\n<p>carry 1 + 1 + 0 = 0</p>\n\n<p>For a final answer of $0011$ which is 3 in decimal, however, the answer in my book says it should be 2 or 0010...?</p>\n", 'ViewCount': '57', 'Title': "Subtracting binaries using two's complement", 'LastActivityDate': '2013-11-12T03:52:34.110', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '17943', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11322', 'Tags': '<computer-architecture><binary-arithmetic>', 'CreationDate': '2013-11-12T01:20:50.483', 'Id': '17939'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>First I apologize if I confused therms DFA and FSM, to me it seems that is the same thing. The question is simple: Are the flowcharts (sequence, branching and jumping) equivalent to DFA resp. FSM? I am a bit confused about this. There are classes where using logical synthesis, Karunaugh maps, state encodings, flip flops etc. one is able to construct hardware consisting of logic gates and flip-flops which realizes the desired DFA. Basically all processes that runs on the computer (no matter if is written in C# or Assembler), are at the lowest level realized through logical gates, zeros and ones. So it seems that programs firstly needs to be converted (by compiler I suppose) to some form as I've described. This might imply that every problem that is solvable using C# is solvable using FSM. But this is in contradiction to Chomsky hierarchy and all this theory related stuff, which says that you cannot do the same magic with regular expressions (which are based on FSM) that you can do on Turing machine (which is equivalent of any programming language, if I am wrong correct me please). Moreover, if flowcharts (or even C#, Java ... source codes) were equivalent to FSM why we do not have all software formally verified so far? There is mathematical apparatus for FSM and related stuff, so why do not formally verify everything and ensure the correctness? What I am missing here?</p>\n", 'ViewCount': '59', 'Title': 'Flowcharts vs DFA resp FSM equivalency', 'LastActivityDate': '2013-11-20T23:19:14.123', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11513', 'Tags': '<formal-languages><turing-machines><finite-automata><computer-architecture>', 'CreationDate': '2013-11-20T23:19:14.123', 'FavoriteCount': '1', 'Id': '18210'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I came to know that the graphic processing unit have something called memory coalescing. On reading on it I was not clear on the topic. Is this any way related to Memory Level Parallelism.</p>\n\n<p>I have searched in Google but was not able to obtain a satisfactory answer. </p>\n\n<p>It would be helpful if someone gives a more comprehensive, easy-to-understand explanation.</p>\n', 'ViewCount': '216', 'Title': 'What is "memory coalescing"?', 'LastEditorUserId': '11539', 'LastActivityDate': '2013-11-22T06:01:41.810', 'LastEditDate': '2013-11-22T06:01:41.810', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '18243', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11539', 'Tags': '<terminology><reference-request><computer-architecture><memory-management>', 'CreationDate': '2013-11-21T17:58:55.743', 'Id': '18229'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I came across this question was I was browsing online</p>\n\n<p>An operation OP has the following characteristics</p>\n\n<pre><code>    OP Latency = 7 clocks\n    OP cycles/issue =2\n</code></pre>\n\n<p>Derive the minimum number of cycles required for the following computation:</p>\n\n<pre><code> X[0] OP X[1] OP X[2] OP ....... OP X[N-1] \n</code></pre>\n\n<p>The answer was written as 7 + 2(n-2). But I was not able to achieve the result. Can someone help me to solve this.</p>\n\n<p><strong>My try:</strong> As suggested by D.W. to check the value for n=3,4,5\nFor n=3, I am getting lower bound as 7+2(2)</p>\n\n<p>For n=4, I am getting lower bound as 7+3(2)</p>\n\n<p>For n=5, I am getting lower bound as 2(7)+4(2)</p>\n\n<p>For n=6, I am getting lower bound as 2(7)+5(2)</p>\n\n<p>Am I going correct. But it deviates from the answer given</p>\n', 'ViewCount': '80', 'Title': 'What is the lower bound of the following computation', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-11-22T21:21:07.850', 'LastEditDate': '2013-11-22T10:17:34.597', 'AnswerCount': '2', 'CommentCount': '12', 'AcceptedAnswerId': '18265', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11539', 'Tags': '<computer-architecture>', 'CreationDate': '2013-11-21T18:39:28.860', 'Id': '18232'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to study for an exam and I realized I\'m confused about how the TLB and data cache work.</p>\n\n<p>I understand that the TLB is essentially a cache of most recently used physical addresses. However, I was looking at a diagram in my textbook (shown below), and I don\'t understand what\'s going on in it. It suddenly splits up the physical address and uses it to index the cache, I guess. But why is it showing the cache and data separately? and why is the byte offset just left floating? I\'m pretty sure the cache is supposed to store data as well. I don\'t think its sole purpose is to determine whether or not there\'s a hit or miss inside of it.</p>\n\n<p>I apologize for my ignorance in advance, but the book barely covers TLB\'s (it\'s like a little more than a page) and it doesn\'t do a very good job at explaining the relationship between a TLB and cache.</p>\n\n<p><img src="http://i.stack.imgur.com/MlEIQ.jpg" alt="Figure"></p>\n', 'ViewCount': '235', 'Title': 'How does a TLB and data cache work?', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-23T08:53:27.340', 'LastEditDate': '2013-11-25T05:57:16.537', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '18320', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<computer-architecture><cpu-cache><virtual-memory>', 'CreationDate': '2013-11-25T01:55:24.453', 'Id': '18313'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>All the article I can find seems to talk about multitasking and context switch as its a two different thing. It seems that multitasking and context switch are the same thing.</p>\n', 'ViewCount': '68', 'Title': 'Is it possible to do multitasking without context switch with just one cpu?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-03T08:41:44.767', 'LastEditDate': '2013-12-03T08:41:44.767', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11764', 'Tags': '<terminology><computer-architecture><operating-systems><concurrency>', 'CreationDate': '2013-12-02T19:44:56.067', 'Id': '18546'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that a processor can address directly up to 4 Gigabyte main memory and can operate words with size 32 bit. Find how big should be the size of the "<a href="http://en.wikipedia.org/wiki/Memory_address_register" rel="nofollow">MAR</a>" (memory address registers), "<a href="http://en.wikipedia.org/wiki/Memory_data_register" rel="nofollow">MDR</a>" (memory data registers) and accumulator registers in this computer?</p>\n\n<p>My answer: MDR is 32 bit wide since it exchanges data not only via the data bus but with the CPU data registers. How about MAR and accumulator? how are they related to the 4 gigabyte main memory?</p>\n', 'ViewCount': '143', 'Title': 'Size of address registers and data registers in relation with memory size', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-25T14:45:33.897', 'LastEditDate': '2013-12-27T11:14:59.383', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11906', 'Tags': '<computer-architecture><memory-management>', 'CreationDate': '2013-12-07T18:20:34.063', 'Id': '18724'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I understand that MESI is a subset of the MOESI cache coherency protocol. But what does the Owned state in the MOESI protocol represent? What are the differences in state transition due to the extra Owned state in MOESI as compared to MESI? </p>\n\n<p>For example consider same cache line in processor P1 is in OWNED state &amp; processor P2 is in SHARED state. What happens when there is a write request to P2?</p>\n', 'ViewCount': '36', 'Title': 'Owned state in MOESI protocol-transitions?', 'LastActivityDate': '2014-03-25T23:30:53.750', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12182', 'Tags': '<computer-architecture><cpu-cache><protocols>', 'CreationDate': '2013-12-19T13:50:58.513', 'Id': '19119'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a system with a two-level paging scheme in which a regular memory access takes 150 nsec and servicing a page fault takes 8 millisec. An average instruction takes 100 nsec of CPU time and two memory accesses. The TLB hit ratio is 90%, and the page fault rate is one in every 10000 instructions. What is the effective average instruction execution time?<br>\na) 645 nsec b) 1050 nsec c) 1215 nsec d) 1230 nsec</p>\n\n<p>My Thinking :  </p>\n\n<p>Memory access = 150 ns </p>\n\n<p>No Page fault =  0.9999<br>\nPage fault     = 0.0001    </p>\n\n<p>TLB Hit =  0.9<br>\nTLB miss = 0.1   </p>\n\n<p>For TLB Hit  = 1 memory access<br>\nFor TLB miss = 3 memory access ( 2 for page table and 1 for actual data )   </p>\n\n<p>For TLB hit and miss equation will be   : ( 0.9 * 150 + 0.1 * 450 )   </p>\n\n<p>i.e. NO PAGE FAULT = 0.9999 * ( 0.9 * 150 + 0.1 * 450 )  </p>\n\n<p>For page fault : 0.0001 * ( 8 * 10 ^ 6 )  </p>\n\n<p>Average execution time = 0.9999 * ( 0.9 * 150 + 0.1 * 450 ) +  0.0001 * ( 8 * 10 ^ 6 )   </p>\n\n<p>Now my question is , what is the use of this line ? : An average instruction takes 100 nsec of CPU time and two memory accesses.  </p>\n\n<p>What is the meaning of average instruction ?<br>\nPlease explain the use and  meaning of the above line and correct me in above problem.</p>\n', 'ViewCount': '38', 'ClosedDate': '2014-01-19T02:10:59.667', 'Title': 'Finding TLB hit and miss', 'LastActivityDate': '2014-01-04T14:58:52.823', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<computer-architecture><cpu-cache><virtual-memory>', 'CreationDate': '2014-01-04T14:58:52.823', 'Id': '19497'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm a little confused about the difference of the memory access and the write-back stage in a <code>RISC pipeline</code>.</p>\n\n<p>We learned in class these following assumptions:</p>\n\n<pre><code>arithmetic &amp; logic: IF, OF, EX, WB\nload: IF, OF, EX, MA, WB\nstore: IF, OF, EX, MA\nbranch: IF, OF, EX\n\nIF=Instruction Fetch, OF=Operand Fetch, EX=Execution, MA=Memory Access, WB=Write-Back\n</code></pre>\n\n<p>Lets say we have the following code now:<br><br>\n    <code>I1: LD R1, 0(R2) ; load R1 from address 0 + R2</code><br>\n    <code>I2: ADD R1, R1, #1 ; R1 = R1 + 1</code><br>\n    <code>I3: ST 0(R2), R1 ; Store R1 at address 0 + R2</code><br></p>\n\n<p>According to what I've learned <code>I1</code> will pass all five stages, <code>I2</code> won't have to access the memory, and <code>I3</code> won't have a write-back.</p>\n\n<p>But then I wonder, how and where does <code>I3</code> store the value then? Just in the memory? And <code>I2</code> fetches the value from memory, but needs to write-back to some place other than the memory? So does that mean that write-back is always to the HDD?</p>\n\n<p>I think I'm missing some core concepts here, as to where the operand is fetched from and where it gets stored to.</p>\n", 'ViewCount': '148', 'Title': 'Difference between memory access and write-back in RISC pipeline', 'LastEditorUserId': '12340', 'LastActivityDate': '2014-01-13T07:08:01.177', 'LastEditDate': '2014-01-13T07:08:01.177', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '19681', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12774', 'Tags': '<computer-architecture><cpu-pipelines>', 'CreationDate': '2014-01-12T14:04:36.507', 'Id': '19668'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I did not understand why unconditional transfer of control instruction is used in cpu.So if we already know we have to jump to an instruction and skip  some instruction irrespective of any condition then why do we not avoid writing those instruction as they will not be executed and replace them with instruction that will be executed after jump instruction</p>\n', 'ViewCount': '47', 'Title': 'use of unconditional transfer of control instruction', 'LastActivityDate': '2014-01-12T18:17:10.243', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19673', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10380', 'Tags': '<computer-architecture>', 'CreationDate': '2014-01-12T17:48:34.020', 'Id': '19672'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Seemingly, a byte has established itself to be 8bit (is that correct?). RAM and NOR-flash can be normally accessed on a quite granular level, but it is up to the system architecture to determine if the smallest addressable unit is 8bit, 16bit or any other power of two bit number. Would the correct terminology be to call this word-addressable? Or asked differently, is a word the size of smallest addressable unit? Or is there some other term to describe this? </p>\n\n<p>Are mabye nibble, byte, word, double word all variable in bit-length and only defined by the architecture? And it is therefore only coincidence that a byte is always 8 bit? E.g. someone could design some new CPU and memory type and define her byte to be 16bit?</p>\n\n<p><strong>Main question:</strong> What is the precise term for the smallest addressable memory block?</p>\n\n<p><strong>Side question:</strong> What is the antonym to this word I'm looking for (e.g. used in NAND-flash)? Page-addressable, block-addressable? Are both correct or is one inprecise?</p>\n", 'ViewCount': '323', 'Title': 'Word- or byte-addressable? Correct terminology', 'LastEditorUserId': '10634', 'LastActivityDate': '2014-01-20T16:42:35.543', 'LastEditDate': '2014-01-20T16:42:35.543', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10634', 'Tags': '<terminology><computer-architecture><memory-management><memory-access>', 'CreationDate': '2014-01-20T12:16:10.260', 'Id': '19848'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Yes, this is a homework question, I've tried working it out and was hoping I could get pointed in the right direction.</p>\n\n<p>Here's the question: </p>\n\n<p>You are designing the instruction set for a new type of computer. The \ncomputer has 64 instructions, 16 general-purpose registers. It supports a \nbyte-addressable memory of up-to 32MB. Answer the following questions. \na. For a 3-operand ADD instruction that only uses register addressing \nmode, how long (number of bits) should the instruction be? \nb. For a 2-operand ADD instruction, in which one of the operands is a \nmemory location with direct addressing mode, how long (number of \nbits) should the instruction be? </p>\n\n<p>I know the question's been asked recently but both questions and answers weren't helpful.</p>\n\n<p>I know that with 64 instructions and 16 registers, there must be 4 bits per register.  I don't exactly know the usage for the 32MB memory right now.</p>\n\n<p>A similar example of part (a) exists on wikipedia, where it states 4 instructions are needed for a 3-operand register addressing mode (load reg1 into a, load reg2 into b, add reg1 and reg2 to reg3, store reg3 in c).  Because it mentions 4 instructions, I thought that would be 1 bit, but that's a really small amount.  There are 3 registers used in the instruction, which would be 12 bits.</p>\n\n<p>For part (b), I believe one is a memory location with direct addressing mode (as stated) and the other is like part (a), using register addressing mode.  There are only two operands, but only one is using register addressing mode, meaning only one register is used?  In that case, the length of the instruction should be 4 bits.</p>\n\n<p>I would appreciate any help! If I'm misunderstanding some of the terms please let me know! Thanks!</p>\n", 'ViewCount': '171', 'Title': 'Information Set Architecture Question', 'LastActivityDate': '2014-01-28T20:02:22.443', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '13151', 'Tags': '<computer-architecture><memory-management>', 'CreationDate': '2014-01-27T01:31:30.527', 'Id': '20003'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So this is a homework question but I have some solution and I am just confused, so a detailed example or help would be nice.</p>\n\n<p>You are designing the instruction set for a new type of computer. The computer has 64 instructions, 16 general-purpose registers. It supports a byte-addressable memory of up-to 32MB. Answer the following questions. a. For a 3-operand ADD instruction that only uses register addressing mode, how long (number of bits) should the instruction be? b. For a 2-operand ADD instruction, in which one of the operands is a memory location with direct addressing mode, how long (number of bits) should the instruction be?</p>\n\n<p>The question has been asked a couple times, and they are denied because they are homework question, but i just want some insight and help maybe example.</p>\n\n<p>here is my solution for problem a, would that be like: 3(cuz 3 add operand)(log2(64) + log2(16) +25(32MB to bites)) = 105? is that right or am i missing something wrong?</p>\n', 'ViewCount': '12', 'ClosedDate': '2014-01-28T22:34:46.180', 'Title': 'Instruction Set Architecture- Question', 'LastActivityDate': '2014-01-28T21:02:52.607', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13197', 'Tags': '<computer-architecture><memory-management>', 'CreationDate': '2014-01-28T21:02:52.607', 'Id': '20046'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '102', 'Title': 'What is Simultaneous Multithreading', 'LastEditDate': '2014-01-29T17:04:27.047', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11539', 'FavoriteCount': '1', 'Body': '<p>I come from an electronics background. I know that there are three types of implementations of multithreading (see <em>Computer Architecture: A Quantitative Approach, 5th Edition</em>):</p>\n\n<ol>\n<li><p><strong>Fine-grain multithreading</strong> issues instructions for different threads after every cycle.</p></li>\n<li><p><strong>Coarse-grain multithreading</strong> only switches to issue instructions from another thread when the current executing thread causes some long latency events (like page fault etc.)</p></li>\n<li><p><strong>Simultaneous multithreading</strong> issues multiple instructions from multiple threads in one cycle. The processor must be superscalar to do so.</p></li>\n</ol>\n\n<p>Does that mean that SMT is an extension of fine-grain and coarse-grain multithreading, and fine-grain and coarse-grain multithreading are only suitable for single core processor?</p>\n\n<p>What is the exact difference between SMT and the other approaches?</p>\n', 'Tags': '<terminology><operating-systems><computer-architecture>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-11T11:23:32.637', 'CommentCount': '0', 'AcceptedAnswerId': '20077', 'CreationDate': '2014-01-29T14:41:51.280', 'Id': '20063'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>NOTE</strong>: Let me point out that I did try extensively to solve this on my own. The problem is that, based on that circuit, it would appear that this processor cannot jump. At best the jump instruction will propagate through the pipeline with no effect. There is no data path defined for a jump that tells the processor to change the PC. The only thing that changes the PC (aside from the normal PC+4) is a <strong>beq</strong>.</p>\n\n<hr>\n\n<p>I\'m learning about MIPS pipelining and stages, but what is excruciatingly unclear is how a jump instruction is executed. On an assignment question, I\'m asked to trace the pipeline with the command <strong>"j 16"</strong>, but there does not appear to be any details about how the logic is handled. The closest thing I can find of any relevance is to do with <strong>beq</strong>, but the opcodes are different... <strong>beq</strong> is <strong>000100</strong> and <strong>j</strong> is <strong>000010</strong>. The following table outlines how the control codes work for four classes of opcodes, but it doesn\'t explain what happens for jump, and subsequently, how the machine knows to jump and what it does with the command...</p>\n\n<p><img src="http://i.stack.imgur.com/Pc9Vh.png" alt="Datapath diagram with control registers">\n<img src="http://i.stack.imgur.com/9QPlv.png" alt="Opcode-Control Codes"></p>\n\n<p>So, if I have the instruction <strong>000010 00000000000000000000000100</strong>, how does this get handled by the data path?</p>\n', 'ViewCount': '64', 'Title': 'How are the control signals derived in the MIPS pipeline?', 'LastEditorUserId': '6569', 'LastActivityDate': '2014-01-30T14:48:47.240', 'LastEditDate': '2014-01-30T14:48:47.240', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6569', 'Tags': '<computer-architecture><cpu-pipelines>', 'CreationDate': '2014-01-30T03:29:24.877', 'Id': '20093'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Latency is defined as the number of intervening cycles between an instruction that produces a result and an instruction that uses the result. The initiation or repeat interval is the number of cycles that must elapse between issuing two operations of a given type.</p>\n\n<p>In the Appendix C, pages 52-54 of <strong>Computer Architecture: A Quantitative approach 5th edition</strong>, it was mentioned that </p>\n\n<ol>\n<li><p>the FP adder is of 4 stages and pipelined, so latency =3(No. of pipeline stages -1) and initiation interval =1 (as pipelined)</p></li>\n<li><p>the FP multiply is of 7 stages  and pipelined, so latency =6(No. of pipeline stages -1) and initiation interval =1 (as pipelined)</p></li>\n</ol>\n\n<p>So for\n3. The FP divide of 24 stages and unpipelined, so latency =23(No. of pipeline stages -1) and initiation interval =24 (as unpipelined). But it was mentioned that   the latency was 24 and initiation interval was 25.</p>\n\n<p><a href="http://postimg.org/image/tdykzabdb/" rel="nofollow">latency and initiation interval</a> and <a href="http://postimg.org/image/6y59uu86d/" rel="nofollow">FP pipeline</a></p>\n\n<p>Why is this?</p>\n\n<p>Can anyone explain me? As I missing something</p>\n', 'ViewCount': '38', 'Title': 'Query about FP Divide latency and Initiation Interval', 'LastEditorUserId': '11539', 'LastActivityDate': '2014-03-20T10:43:12.523', 'LastEditDate': '2014-03-20T10:43:12.523', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11539', 'Tags': '<terminology><computer-architecture>', 'CreationDate': '2014-01-30T11:21:13.110', 'Id': '20102'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>So, i was trying:</p>\n\n<p>$(-10.75)_{10}$ and to convert it into 32 bit binary floating point representation.</p>\n\n<p>i did this:<br>\nAccording to IEEE standard:  $(-1)^{-s} * 1.M * 2^{E-bias} $</p>\n\n<pre><code> sign bit= 1 bit\n exponent= 8 bits\n mantissa= 23 bits\n</code></pre>\n\n<p>bias= $2^{n-1}-1 = 127$</p>\n\n<pre><code>  - 10   . 75\n  \u21d3  \u21d3      \u21d3\n= 1 1010 . 11\n= 1 1.01011 x 2^-3\n= 1 1.01011 x 2^(124-127)\n= 1 01111100 0101100 0000 0000 0000 0000   = 32 bits\n  \u21d3 ________ ____________________________\n  \u21d3    \u21d3                  \u21d3\n sign  Exponent         Mantissa\n</code></pre>\n\n<p>But the answer presented is:</p>\n\n<pre><code>  - 10   . 75\n  \u21d3  \u21d3      \u21d3\n= 1 1010 . 11\n= 1 1.101011 x 2^-4\n      -------&gt; why this happened, and why is 1 before '.'   \n= 1 1.101011 x 2^(123-127)\n= 1 01111011 1010110 0000 0000 0000 0000   = 32 bits\n  \u21d3 ________ ____________________________\n  \u21d3    \u21d3                  \u21d3\n sign  Exponent         Mantissa\n</code></pre>\n\n<p>If i am wrong, where is it and please explain why..\nAny help is appreciated.</p>\n", 'ViewCount': '66', 'Title': 'Problem in finding the floating point representation?', 'LastEditorUserId': '10564', 'LastActivityDate': '2014-03-15T16:02:51.047', 'LastEditDate': '2014-02-06T05:54:45.750', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10564', 'Tags': '<computer-architecture><floating-point>', 'CreationDate': '2014-02-05T07:44:11.500', 'Id': '21312'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My knowledge is very vague as all we have are visual diagrams etc, but we have memory address and registers, the ALU being the heart(apparently). Single core CPUs process one instruction at a time AFAIK and multi-core have parallelism to some degree. So where do the millions of transistors come in and how do 32 registers manage everything. We have FPU's I know, how many transistors would these use roughly. Any way to get a fairly simple idea of what the bulk of the transistors do, why more means faster and how the registers 'manage' everything.</p>\n", 'ViewCount': '551', 'Title': 'Why do we need so many transistors in a chip, and how are they managed?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T01:49:23.753', 'LastEditDate': '2014-02-07T07:51:48.597', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12979', 'Tags': '<computer-architecture>', 'CreationDate': '2014-02-07T02:24:54.983', 'FavoriteCount': '2', 'Id': '21413'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Qualcomm is creating a Neuromorphic Processing Unit or an NPU called <a href="http://www.qualcomm.com/media/blog/2013/10/10/introducing-qualcomm-zeroth-processors-brain-inspired-computing" rel="nofollow">zeroth</a>.</p>\n\n<p>IBM is also working on a brain inspired chip under Synapse program.</p>\n\n<p>Standford\'s <a href="http://www.stanford.edu/group/brainsinsilicon/neurogrid.html" rel="nofollow">Neurogrid</a> might be a similar example.</p>\n\n<p>Neuromorphic systems are inherently based on mixed signal and analog chips. Considering a wide application of neuromorphic computers in the field of robotics, how must be the software applications defined and process in the computer. Do we need a specialized programming language for that? </p>\n\n<p>How are such systems programmed?</p>\n', 'ViewCount': '81', 'Title': "Is there any defined programming model for 'Self-Learning' NPUs?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-10T11:44:32.070', 'LastEditDate': '2014-02-10T11:44:32.070', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14545', 'Tags': '<programming-languages><computer-architecture>', 'CreationDate': '2014-02-10T10:05:04.647', 'Id': '21491'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In case of branch Instructions such as <code>beq</code>, <code>bne</code>, we use PC-relative addressing. But I am not clear why it is said in most of the books that MIPS address(while calculating branch target) is actually relative to the address of following(or sequentially next) instruction (PC+4) as opposed to the current instruction (PC)?</p>\n\n<p>Anybody please explain me. Thanks.</p>\n', 'ViewCount': '35', 'ClosedDate': '2014-02-15T22:30:01.307', 'Title': 'MIPS Architecture : PC-relative addressing', 'LastActivityDate': '2014-02-14T21:34:10.820', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14640', 'Tags': '<computer-architecture>', 'CreationDate': '2014-02-13T14:03:34.173', 'FavoriteCount': '0', 'Id': '21600'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In case of branch Instructions such as <code>beq</code>, <code>bne</code>, we use PC-relative addressing. \nBut I am really not clear why it is said in most of the books that MIPS address(while calculating branch target) is actually relative to the address of following instruction <code>PC+4</code> as opposed to the current instruction <code>PC</code>?</p>\n\n<p>Anybody please explain me.\nThanks.</p>\n', 'ViewCount': '65', 'Title': 'MIPS Architecture : PC-relative addressing', 'LastActivityDate': '2014-02-15T22:20:43.040', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '21683', 'Score': '0', 'OwnerDisplayName': 'user1612078', 'PostTypeId': '1', 'OwnerUserId': '14640', 'Tags': '<computer-architecture>', 'CreationDate': '2014-02-13T13:03:05.277', 'FavoriteCount': '0', 'Id': '21682'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>will you recommend use of branch prediction buffers for 5 stage integer MIPS pipeline. Does this increase the efficiency or not?</p>\n', 'ViewCount': '49', 'ClosedDate': '2014-02-22T08:32:32.340', 'Title': 'branch prediction buffer - 5 stage integer MIPS', 'LastActivityDate': '2014-02-21T19:55:22.810', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14917', 'Tags': '<computer-architecture>', 'CreationDate': '2014-02-21T16:07:16.093', 'Id': '21890'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading about the various phases of the <a href="http://en.wikipedia.org/wiki/Instruction_cycle" rel="nofollow">Instruction Execution</a>, I found out that we have three phases like below.</p>\n\n<ol>\n<li>Fectch</li>\n<li>Decode</li>\n<li>Execute</li>\n</ol>\n\n<p>Now if the part I don\'t understand is why do we need a decode phase ? The instruction will already be stored in a binary format at some memory location, why not just fetch and execute it.</p>\n', 'ViewCount': '58', 'Title': 'What happens at the decode phase of the instruction cycle?', 'LastActivityDate': '2014-02-22T08:48:12.637', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11438', 'Tags': '<computer-architecture>', 'CreationDate': '2014-02-21T17:18:27.967', 'FavoriteCount': '1', 'Id': '21895'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was trying to solve a question dealing with n instructions in an uneven pipeline with k stages. I came across a generic formula for even pipelines i.e. (k + n - 1) * clock cycle. But I feel this should not work for uneven pipelines as the above formula is based on fact that the 1st instruction takes time = sum(time of k stages) and the 2nd instruction onwards takes time = maximum of stage delays. </p>\n\n<p>A example: A pipeline is designed with 5 stages having execution times respectively as 3ns, 4ns, 2ns and 4ns. How much time will it take to execute 1000 instructions?</p>\n', 'ViewCount': '140', 'Title': 'Execution time of an uneven pipeline', 'LastEditorUserId': '16189', 'LastActivityDate': '2014-03-27T18:36:40.790', 'LastEditDate': '2014-03-27T18:36:40.790', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '14939', 'Tags': '<computer-architecture><cpu-pipelines>', 'CreationDate': '2014-02-22T16:04:48.137', 'Id': '21924'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '69', 'Title': 'Why Instruction Decode and Register Read are in the same stage of MIPS pipeline', 'LastEditDate': '2014-02-25T21:03:57.187', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14769', 'FavoriteCount': '0', 'Body': '<p>Why are instruction decoding and register read are combined in single stage of a 5-stage MIPS-pipeline, even though they serve two different operation?</p>\n', 'Tags': '<computer-architecture><cpu-pipelines>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-25T21:04:13.483', 'CommentCount': '3', 'AcceptedAnswerId': '21939', 'CreationDate': '2014-02-23T04:34:32.810', 'Id': '21937'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given below are 3 different pipelined processors:</p>\n\n<p>$P_1:\\ 4\\ stages\\ with\\ delays\\ \\ \\ \\ 0.6_{ms}\\ \\ 0.8_{ms}\\ \\ 0.6_{ms}\\ \\ 1.1_{ms}\\\\\nP_2:\\ 4\\ stages\\ with\\ delays\\ \\ \\ \\ 2.0_{ms}\\ \\ 1.8_{ms}\\ \\ 2.0_{ms}\\ \\ 1.0_{ms}\\\\\nP_3:\\ 5\\ stages\\ with\\ delays\\ \\ \\ \\ 1.0_{ms}\\ \\ 0.8_{ms}\\ \\ 1.0_{ms}\\ \\ 1.5_{ms}\\ \\ 1.5_{ms}$</p>\n\n<p>One has to find the <strong>Peak Clock Frequencies</strong> of each $\\mathbf{P_i}$. Pipeline buffer register latency is Zero.</p>\n\n<p>I don't know how to solve this, but I've come up with an intuitive formula for calculating peak frequency of clock for processor $P_i$ as $$\\mathcal{C_{p.f}}=\\frac{1}{max(d_i)}$$ where $d_i$ is individual stage delay for processor $P_i$. For example for $P_2$ it's $\\frac{1}{2}KHz = 0.5 KHz$.</p>\n\n<pre><code>Now I don't know whether it's the correct way to calculate peak frequency.\nIf it's not, can anyone tell me what is it?\n\nP.S: I also don't know whether this is the right place to ask this question.\nBut I didn't have any idea about where to post it otherwise.\n</code></pre>\n", 'ViewCount': '45', 'Title': 'A question from Computer Organization on Peak Clock Frequency', 'LastActivityDate': '2014-03-02T16:13:09.683', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11949', 'Tags': '<computer-architecture><cpu-pipelines>', 'CreationDate': '2014-03-02T15:49:40.287', 'Id': '22196'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've always wondered why processors stopped at 32 registers. It's by far the fastest piece of the machine, why not just make bigger processors with more registers? Wouldn't that mean less going to the RAM?</p>\n", 'ViewCount': '3018', 'Title': 'Why does a processor have 32 registers?', 'LastActivityDate': '2014-03-14T18:50:27.030', 'AnswerCount': '4', 'CommentCount': '4', 'AcceptedAnswerId': '22591', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '15649', 'Tags': '<computer-architecture>', 'CreationDate': '2014-03-13T14:57:49.047', 'FavoriteCount': '8', 'Id': '22589'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m doing the exercises from "Computer Organization and Design, Fourth Edition: The Hardware and Software Interface" by David Patterson and John Hennessy.</p>\n\n<p>I\'ve come across a problem that states:</p>\n\n<p><img src="http://i.stack.imgur.com/40BEA.png" alt="enter image description here"></p>\n\n<p><img src="http://i.stack.imgur.com/qOX9T.png" alt="enter image description here"></p>\n\n<p>Where Figure 4.2 looks like:\n<img src="http://i.stack.imgur.com/2KB3l.png" alt="enter image description here"></p>\n\n<p>I was able to correctly calculate the the critical path before the improvement, however determining it afterwards, as well as the cost afterwards, is confusing me. This is what the solutions manual states: <img src="http://i.stack.imgur.com/1pb3Q.png" alt="enter image description here"></p>\n\n<p>I can only guess that the cycle time stays the same because the +300ps latency only applies on multiply instructions, which then would still have a lower latency. (if the multiply path is IMem,Regs,Mux,(ALU plus 300),Mux)... Is the assumption correct?</p>\n\n<p>Also, I am very confused on the calculation of the new cost. The solution adds 2*20. Where do the two 20 costs come from? I initially thought the cost would have 600 added to it, as that is how much more the ALU costs.</p>\n\n<p>Thank you for any help.</p>\n', 'ViewCount': '44', 'Title': 'Critical path of MIPS single cycle CPU', 'LastEditorUserId': '15665', 'LastActivityDate': '2014-03-13T23:39:50.310', 'LastEditDate': '2014-03-13T23:39:50.310', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15665', 'Tags': '<computer-architecture>', 'CreationDate': '2014-03-13T23:24:34.767', 'Id': '22599'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My lecturer told me that when I use an arithmetic I-Type command (ADDI,SUBI etc.) , the IMM field gets sign extended, and when I use a logic I-Type command (ORI,ANDI etc.) , the IMM field is just bits.</p>\n\n<p>He also said that if the IMM field contains a negative number, it is represented in 2's complement.</p>\n\n<p>Let's say I have a command, and two binary numbers X Y, so that X's 2's complement representation looks exactly like Y in regular representation.</p>\n\n<p>Note: X is negative and not the same as Y.</p>\n\n<p>Given a question in which I should say what is the value in the IMM field in an arithmetic I-Type command, how do I know if it's X or Y?</p>\n", 'ViewCount': '36', 'Title': 'MIPS: sign extend in I-Type commands', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-03-17T16:02:14.063', 'LastEditDate': '2014-03-17T15:42:52.267', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<computer-architecture><number-formats>', 'CreationDate': '2014-03-17T14:55:12.590', 'Id': '22712'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The definition of the both architecture looks pretty same. They are parallel computing architecture with different type of cores. </p>\n\n<p>What distinguish their definition, actually?</p>\n', 'ViewCount': '27', 'Title': "Heterogenous and Asymmetric Computing's differences", 'LastActivityDate': '2014-03-19T08:19:01.053', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15857', 'Tags': '<computer-architecture><parallel-computing>', 'CreationDate': '2014-03-19T08:19:01.053', 'Id': '22797'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For example, on x86, we have a set of general registers, each named to the function it carries out. </p>\n\n<p>We have an Accumulator, which is a storage for a results of different fixed point operations, we have a Base register, which is used for addressing elements in the array, a Cycle register, which holds a incremented or decremented counter for cycles, we have a Data register, which can hold an operand for arithmetic operations, and many many others (like base and stack pointers, indexes, and etc.)  </p>\n\n<p>But on typical RISC architectures, like SH4, for example, you won't find such an explicit names for registers. On Sh4, for example, there's no explicit name for register, which must hold a stack pointer, so you can store it elsewhere upon your choice from 16 available registers. Same for arithmetic operations, and so on. </p>\n\n<p>So why it is that?<br>\nIt is simply a matter of design, choices, taken by Intel engineers, or some significant sign of RISC processors?  </p>\n", 'ViewCount': '46', 'Title': "Why does x86 has explicit register definitions, and RISC's doesn't?", 'LastActivityDate': '2014-03-20T03:13:01.180', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22848', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8715', 'Tags': '<computer-architecture>', 'CreationDate': '2014-03-19T19:54:32.330', 'FavoriteCount': '1', 'Id': '22821'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '39', 'Title': 'Does exploiting a spatial Locality in Cache always leads to a lower miss rate?', 'LastEditDate': '2014-03-26T10:00:52.293', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14769', 'FavoriteCount': '1', 'Body': "<p>I've read that, incorporating many words(spatial locality) per cache blocks leads to lower miss rate. Is it the case always? \nOne possibility of such approach is to make a single cache block of size equal to the size of the cache, but that would be meaningless as far as benefits of memory hierarchy are concerned. Isn't it so?</p>\n", 'Tags': '<computer-architecture><memory-management><cpu-cache>', 'LastEditorUserId': '14769', 'LastActivityDate': '2014-03-26T10:00:52.293', 'CommentCount': '0', 'AcceptedAnswerId': '22974', 'CreationDate': '2014-03-23T05:56:25.657', 'Id': '22960'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The question I ask is in reference to the <a href="http://people.cs.clemson.edu/~mark/464/appG.pdf%E2%80%8E" rel="nofollow">Appendix G</a> of Hennessy Patterson 4th Edition computer architecture book. </p>\n\n<p>On page <strong>G-23</strong>, it is written that if there are <strong>64 memory banks</strong> and the stride is <strong>32</strong> then stall takes place every other access. (Total number of elements to be read is 64)</p>\n\n<p>However, <strong>according to me</strong> the first two elements can be accessed by 12+1+1 cycles. Then, for 3rd element we will have to wait 6 cycles(Because, bank busy time is 6 cycles and we returned to this bank in 3rd cycle. So, it will wait till the first element is read and then initiate reading of the 3rd element). Again, 4th element can\'t be read in next cycle as 2nd element is still being read and so again we wait till its over and then read 4th element. So, stall takes place on every access. So, effectively for all the remaining 62 elements too there is a lag of 6 cycles. so, total time should be 12+1+1+31*6+ 31*6. </p>\n\n<p>I know that above explanation of my  point of view is <strong>not</strong> clear so I won\'t blame you if you can\'t understand it(but this is the best way in which I could convey my understanding).</p>\n\n<p>So, can anyone tell me <strong>what am I doing wrong</strong>(if you understood the explanation). Otherwise just explain the <strong>book\'s point</strong> which says that every second access is only stalled and give the total time taken in the same. </p>\n\n<p>Also, can anyone tell me the <strong>difference</strong> between bank busy time and memory latency. Is bank busy time a part of the memory latency?</p>\n', 'ViewCount': '25', 'Title': 'Memory latency, bank busy time and stride in vector architecture', 'LastActivityDate': '2014-03-23T16:56:10.677', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16032', 'Tags': '<computer-architecture>', 'CreationDate': '2014-03-23T16:56:10.677', 'Id': '22975'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have completed a computer architecture course and the last topic i have learned was cache memories. I have peeked in random tests of course from the last few years and all of them have a given mips assembler with a question about a cache.</p>\n\n<p>I have no idea how to calculate things like miss rate or hit rate from the given code.\nI know that what I should notice is lw,sw,lb and other memory stuff but i have no idea how to approach questions like that, and i always have alot of mistakes trying to solve them.</p>\n\n<p>How can i calculate the hit rate of a given cache using a given code?</p>\n\n<p>Edit: I see that nobody is answering, so I will give an example of a question:</p>\n\n<p>Given the following code:</p>\n\n<blockquote>\n  <p>1 ADDI R30, R0, 0x2000 </p>\n  \n  <p>2 ORI R10, R0, -10 </p>\n  \n  <p>LOOP: </p>\n  \n  <p>3 LW R2, 0x4000(R30)</p>\n  \n  <p>4 LW R3, 0x8000(R30) </p>\n  \n  <p>5 SUB R4, R3, R2</p>\n  \n  <p>6 BGE R4, R0, 1 </p>\n  \n  <p>7 SW R4, 0xC000(R30) </p>\n  \n  <p>8 ADDI R30, R30, 4</p>\n  \n  <p>9 ADDI R10, R10, 1</p>\n  \n  <p>10 BGE R0, R10, LOOP</p>\n  \n  <p>END</p>\n</blockquote>\n\n<p>And the following cache:\nDirect-Mapped cache of size 16KB.\nBlock size is 32 bits.</p>\n\n<p>Now, I know that the set is of size 9 bits, but how do I calculate the hit rate of the cache in the data accesses? (SW and LW)</p>\n\n<p>I have asked the lecturer and he told me the hit rate is 0%, but he left before I could ask why. That's why I'm asking it here.</p>\n", 'ViewCount': '37', 'Title': 'Caches: connection between a given code and a cache', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-03-25T22:23:46.277', 'LastEditDate': '2014-03-25T18:57:02.623', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<computer-architecture><cpu-cache>', 'CreationDate': '2014-03-25T16:36:59.733', 'Id': '23036'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>System: Application</p>\n\n<p>OS: Scheduler, VMM, IPC, FS Drivers, dispatchers, VFS</p>\n\n<p>The above would be a monolithic kernel. In a monolithic kernel all core OS functions are separate from user spaaaaaace. Functions such as the FS are handled completely by the kernel. This leaves the end user/dev open to write applications that rely solely on the kernel to emulate and process the functions. The BSD tree, AIX, and HP-UX are good examples of monolithic kernels.</p>\n\n<p>System: Application</p>\n\n<p>User Mode: Application IPC, Unix Server, Drivers, File Server</p>\n\n<p>OS: Basic IPC, VMM, Scheduling</p>\n\n<p>This is a microkernel. A microkernel is responsible solely for physical emulation of the machine code. It provides a more secure functionality, but also allows for better hardware processing. User spaaaaaace is where drivers are handled, sometimes grating DMA. Also, it can improve computing processing time if the user spaaaaaace is streamlined for direct machine emulation. JXOS and various nanokernels are good examples.</p>\n\n<p>System: Application</p>\n\n<p>User Mode: Drivers, Unix Server, Application IPC</p>\n\n<p>OS: Microkernel, Kernel Drivers, IPC, HAL</p>\n\n<p>This is a hybrid kernel. A hybrid kernel is mix between a monolithic kernel and a microkernel. Software drivers are handled in user mode, but hardware drivers are handled by the kernel. This allows for a lot of flexibility in design, as some packages and application functions can run directly against the kernel. However, this leaves the kernel open to a lot of security vulnerabilities. The most well-known hybrid kernel is Windows NT.</p>\n\n<p>My architect was testing my knowledge of kernels, and this is what I sent him. He said that I wasn't wrong, I just have a very different approach to it. When interrogated about it, he just told me it was different. How is it different? Does it look different?</p>\n", 'ViewCount': '40', 'Title': 'Is my understanding of kernels correct?', 'LastActivityDate': '2014-04-03T17:25:58.583', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16415', 'Tags': '<computer-architecture><kernel>', 'CreationDate': '2014-04-03T14:29:55.443', 'Id': '23396'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>So I am rewriting this to make it clear so I will start with one question</p>\n\n<p>I have a problem that states</p>\n\n<p>Consider a 32-bit hexadecimal number stored in memory as follows:</p>\n\n<pre><code>Address Value\n0xC86E4 2A\n0xC86E5 C2\n0xC86E6 08\n0xC86E7 1B\n</code></pre>\n\n<p>If the machine is big endian and uses 2\u2019s complement representation for integers, write the 32-bit integer number stored at address 0xC86E4 (you may write the number in hex).</p>\n\n<p>now what does the machine being big endian have to do with this, to start I would convert the address to 2's compliment</p>\n\n<p>so 0xC86E4 = 0000 0000 0000 01100 1000 0110 1110 0100</p>\n\n<p>but now I don't know what to do next</p>\n", 'ViewCount': '48', 'Title': 'Basics of endianness for memory access', 'LastEditorUserId': '39', 'LastActivityDate': '2014-04-11T08:48:10.453', 'LastEditDate': '2014-04-11T08:48:07.870', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '16656', 'Tags': '<computer-architecture>', 'CreationDate': '2014-04-10T18:34:04.987', 'Id': '23656'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '39', 'Title': 'Which kind of interrupt has the highest priority on 8086 processors?', 'LastEditDate': '2014-04-12T09:38:13.807', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16705', 'Body': '<p>Which of the following interrupts has the highest priority in 8086 micro-processor:</p>\n\n<ol>\n<li>Overflow,</li>\n<li>NMI or</li>\n<li>Type 255?</li>\n</ol>\n\n<p>The book I read suggests that type 255 has highest priority.\nBut most of the searches on google showed that it is NMI. Which is true?</p>\n', 'ClosedDate': '2014-04-14T17:42:05.803', 'Tags': '<computer-architecture>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T19:51:25.630', 'CommentCount': '0', 'AcceptedAnswerId': '23712', 'CreationDate': '2014-04-12T09:35:23.607', 'Id': '23696'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question about the common course "Computer Architecture".</p>\n\n<p>How is it possible to have a LOCAL Branch Predecitor with 1024 entries, 3 bits for HISTORY but without a TAG.</p>\n\n<p>As I understand,in that way i will get a Global branch predictor because i dont have a way to map a branch to an entry. Am I right? If not,How can it be possible?</p>\n', 'ViewCount': '8', 'Title': 'Branch Prediction - LOCAL BHR without tag', 'LastActivityDate': '2014-04-13T21:37:33.410', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23753', 'Score': '0', 'OwnerDisplayName': 'Anton', 'PostTypeId': '1', 'OwnerUserId': '16678', 'Tags': '<computer-architecture>', 'CreationDate': '2014-03-28T14:50:22.213', 'Id': '23752'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When learning about the architecture of computers and how it works, we are thought that the lowest language that we can find that the machine understands is binary as 1&amp;0. And anything that we input will have to be transformed/converted to binary, but being binary <strong>numbers</strong> wouldn't that mean that we would need another interpreter/compiler to transform binary into actual machine language? We all know from electronics that a computer is mainly composed of cpu which is an IC that is therefore made out of transistors etc, and the only thing that those tools understand is electricity, therefore electricity will be the lowest understandable language for a computer. So my concern is, is binary really 1s&amp;0s or the 1s&amp;0s are just used to represent the absence and or presence of electricity?\nSupposing that it's just a representation for absence or presence of electricity, wouldn't there be another intermediate or even lower language between the commands that we input and binary, so that the circuits would know where to send the current to and where not to?</p>\n", 'ViewCount': '131', 'Title': 'Is there anything lower than the bit level of 1s and 0s?', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-15T23:30:40.260', 'LastEditDate': '2014-04-15T17:41:53.900', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16793', 'Tags': '<computer-architecture>', 'CreationDate': '2014-04-15T14:42:13.747', 'Id': '23813'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>With regards to introductory (beginner) Von Neumann computer architecture, how does a program change the order in which instructions are executed?</p>\n\n<p>I know the control unit is responsible for retrieving the instruction from memory and then decodes and executes the instruction. Their connected by a bus. The control unit and ALU are together in the CPU.</p>\n\n<p>I'm guessing different lines of code are stored in different addresses in the RAM?</p>\n\n<p>Is it a easy question, and I'm just thinking its difficult?</p>\n", 'ViewCount': '39', 'Title': 'Computer Architecture - Von Neumann', 'LastActivityDate': '2014-04-15T21:25:27.747', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23833', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16807', 'Tags': '<computer-architecture>', 'CreationDate': '2014-04-15T21:01:57.593', 'Id': '23829'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Just understanding some syntax. On my Ram (6116) and Rom (27C64) it has a asserted low CE and OE pins. These I believe are control pins.</p>\n\n<p>I'm assuming to use the RAM for example, chip enable (ce) has to be low. then Output enable has to be low for data to be sent/read to cpu? </p>\n\n<p>If im right so far by looking at the 6116 ram the data bus pins can be either input/output. So CE or OE don't determine whether data is being read or written to that address location. Theres another pin called WE which im assuming is a control pin for data to be input/output. </p>\n\n<p>What does WE stand for and am i right with what I've assumed? </p>\n", 'ViewCount': '13', 'ClosedDate': '2014-04-19T18:51:59.677', 'Title': 'Computer Architecture: control pins, CE OE', 'LastActivityDate': '2014-04-19T01:21:57.997', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16899', 'Tags': '<computer-architecture>', 'CreationDate': '2014-04-19T01:21:57.997', 'Id': '23927'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a CS course at Uni. Had an exam about two last week with a question I did not get, but still, am not totally comfortable with the expected answer.</p>\n\n<p>Basically, we were asked</p>\n\n<blockquote>\n  <p>Many processes can execute simultaneously on the Von Neuman\n  architecture\n  [True/False]</p>\n</blockquote>\n\n<p>I wrote True. Got it wrong, lost some points.</p>\n\n<p>But I wonder though if I could a bit argue with the teacher to get those points back.\nAfter all, x86 is Von Neuman but still, some also have multiple cores?\nAlso, for example x86's HyperThreading, doesn't it allow to execute parallel tasks even on a single core?</p>\n\n<p>Don't you think there's some ambiguity relative to the question?</p>\n\n<p>I'm not totally sure about those arguments though, hence why I ask.</p>\n\n<p>Regards</p>\n", 'ViewCount': '20', 'Title': 'Simultaneous execution on a Von Neuman architecture', 'LastActivityDate': '2014-04-23T18:45:23.750', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17024', 'Tags': '<computer-architecture>', 'CreationDate': '2014-04-23T18:45:23.750', 'Id': '24056'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to install Linux on my server (Dell Powerege 2850) but I can not because even the hard drivers are not detected !</p>\n\n<p>How can I resolve this problem ?</p>\n', 'ViewCount': '6', 'ClosedDate': '2014-04-25T16:32:36.363', 'Title': 'Why HDDs are not detected on Dell PowerEdge 2850?', 'LastActivityDate': '2014-04-25T16:03:15.873', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17079', 'Tags': '<computer-architecture>', 'CreationDate': '2014-04-25T16:03:15.873', 'Id': '24109'}}