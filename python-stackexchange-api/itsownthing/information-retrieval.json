{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My problem is I need to find the same people in two different lists using their info like first name, last name, email, job title, etc. For example, if person X changes his company, his title, company and location will change. I want to find out that two different records of this guy are actually pointing to a same guy. I've thought about classifiers and linear regression to find weights for each field but nothing more. Does anyone know a paper o a research in Information Retrieval and Recommender Systems about this problem?</p>\n", 'ViewCount': '22', 'Title': 'Eliminating duplicate people using their information', 'LastEditorUserId': '8397', 'LastActivityDate': '2013-05-27T22:41:46.390', 'LastEditDate': '2013-05-27T22:41:46.390', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8397', 'Tags': '<information-retrieval>', 'CreationDate': '2013-05-27T22:12:19.217', 'Id': '12315'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '81', 'Title': 'Document definition in information retrieval', 'LastEditDate': '2013-08-23T10:35:35.517', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9741', 'FavoriteCount': '1', 'Body': '<p>Is there common formal definition of <strong>Document</strong> in information retrieval field? In many researches authors don\'t define the term <strong>Document</strong>(maybe because it is evident for them). Wikipedia says "text file, document (computer science) a computer file that contains text" but it doesn\'t seem as common formal definition. </p>\n\n<p>Do you know the common formal definition of <strong>Document</strong>, or, if not, do you know any researches where that term was defined? </p>\n', 'Tags': '<terminology><data-mining><information-retrieval>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-23T10:35:35.517', 'CommentCount': '4', 'AcceptedAnswerId': '13825', 'CreationDate': '2013-08-19T19:20:52.343', 'Id': '13823'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I have $n$ independent observations $x_1,\\dots,x_n$ from some unknown distribution over a known alphabet $\\Sigma$, and I want to estimate the entropy of the distribution.  I can count the frequency $f_s$ of each symbol $s \\in \\Sigma$ among the observations; how should I use them to estimate the Shannon entropy of the source?</p>\n\n<hr>\n\n<p>The obvious approach is to estimate the probability of each symbol $s$ as $\\Pr[X=s]=f_s/n$, and then calculate the entropy using the standard formula for Shannon entropy.  This leads to the following estimate of the entropy $H(X)$:</p>\n\n<p>$$\\text{estimate}(H(X)) = - \\sum_{s \\in \\Sigma} {f_s \\over n} \\lg (f_s/n).$$</p>\n\n<p>However, this feels like it might not produce the best estimate.  Consider, by analogy, the problem of estimating the probability of symbol $s$ based upon its frequency $f_s$.  The naive estimate $f_s/n$ is likely an underestimate of its probability.  For instance, if I make 100 observations of birds in my back yard and none of them were a hummingbird, should my best estimate of the probability of seeing a hummingbird on my next observation be exactly 0?  No, instead, it\'s probably more realistic to estimate the probability is something small but not zero.  (A zero estimate means that a hummingbird is absolutely impossible, which seems unlikely.)</p>\n\n<p>For the problem of estimating the probability of symbol $s$, there are a number of standard techniques for addressing this problem.  <a href="https://en.wikipedia.org/wiki/Laplace_smoothing" rel="nofollow">Additive smoothing</a> (aka Laplace smoothing) is one standard technique, where we estimate the probability of symbol $s$ as $\\Pr[X=s] = (f_s + 1)/(n+|\\Sigma|)$.  Others have proposed Bayesian smoothing or other methods.  These methods are widely used in natural language processing and document analysis, where just because a word never appears in your document set doesn\'t mean that the word has probability zero.  In natural language processing, this also goes by the name <a href="https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques" rel="nofollow">smoothing</a>.</p>\n\n<p>So, taking these considerations into account, how should I estimate the entropy, based upon observed frequency counts?  Should I apply additive smoothing to get an estimate of each of the probabilities $\\Pr[X=s]$, then use the standard formula for Shannon entropy with those probabilities?  Or is there a better method that should be used for this specific problem?</p>\n', 'ViewCount': '108', 'Title': 'Estimate entropy, based upon observed frequency counts', 'LastActivityDate': '2013-10-13T18:18:02.167', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<information-theory><statistics><natural-lang-processing><entropy><information-retrieval>', 'CreationDate': '2013-10-11T21:26:30.400', 'Id': '15010'}}