1180:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1359', 'Title': 'Randomized Selection', 'LastEditDate': '2012-04-18T19:51:56.180', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1134', 'FavoriteCount': '2', 'Body': u'<p>The randomized selection algorithm is the following:</p>\n\n<p>Input: An array $A$ of $n$ (distinct, for simplicity) numbers and a number $k\\in [n]$</p>\n\n<p>Output: The the "rank $k$ element" of $A$ (i.e., the one in position $k$ if $A$ was sorted)</p>\n\n<p>Method:</p>\n\n<ul>\n<li>If there is one element in $A$, return it</li>\n<li>Select an element $p$ (the "pivot") uniformly at random</li>\n<li>Compute the sets $L = \\{a\\in A : a &lt; p\\}$ and $R = \\{a\\in A : a &gt; p\\}$</li>\n<li>If $|L| \\ge k$, return the rank $k$ element of $L$.</li>\n<li>Otherwise, return the rank $k - |L|$ element of $R$</li>\n</ul>\n\n<p>I was asked the following question:</p>\n\n<blockquote>\n  <p>Suppose that $k=n/2$, so you are looking for the median, and let $\\alpha\\in (1/2,1)$\n  be a constant.  What is the probability that, at the first recursive call, the \n  set containing the median has size at most $\\alpha n$?</p>\n</blockquote>\n\n<p>I was told that the answer is $2\\alpha - 1$, with the justification "The pivot selected should lie between $1\u2212\\alpha$ and $\\alpha$ times the original array"</p>\n\n<p>Why? As $\\alpha \\in (0.5, 1)$, whatever element is chosen as pivot is either larger or smaller than more than half the original elements. The median always lies in the larger subarray, because the elements in the partitioned subarray are always less than the pivot. </p>\n\n<p>If the pivot lies in the first half of the original array (less than half of them), the median will surely be in the second larger half, because once the median is found, it must be in the middle position of the array, and everything before the pivot is smaller as stated above. </p>\n\n<p>If the pivot lies in the second half of the original array (more than half of the elements), the median will surely first larger half, for the same reason, everything before the pivot is considered smaller. </p>\n\n<p>Example:</p>\n\n<p>3 4 5 8 7 9 2 1 6 10</p>\n\n<p>The median is 5.</p>\n\n<p>Supposed the chosen pivot is 2. So after the first iteration, it becomes:</p>\n\n<p>1 2 ....bigger part....</p>\n\n<p>Only <code>1</code> and <code>2</code> are swapped after the first iteration. Number 5 (the median) is still in the first greater half (accroding to the pivot 2). The point is, median always lies on greater half, how can it have a chance to stay in a smaller subarray?</p>\n', 'Tags': '<algorithms><algorithm-analysis><probability-theory><randomized-algorithms>', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-18T19:51:56.180', 'CommentCount': '5', 'AcceptedAnswerId': '1343', 'CreationDate': '2012-04-18T08:21:27.190', 'Id': '1334'},1181:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The usual simple algorithm for finding the median element in an array $A$ of $n$ numbers is:</p>\n\n<ul>\n<li>Sample $n^{3/4}$ elements from $A$ with replacement into $B$</li>\n<li>Sort $B$ and find the rank $|B|\\pm \\sqrt{n}$ elements $l$ and $r$ of $B$</li>\n<li>Check that $l$ and $r$ are on opposite sides of the median of $A$ and that there are at most $C\\sqrt{n}$ elements in $A$ between $l$ and $r$ for some appropriate constant $C &gt; 0$.  Fail if this doesn\'t happen.</li>\n<li>Otherwise, find the median by sorting the elements of $A$ between $l$ and $r$</li>\n</ul>\n\n<p>It\'s not hard to see that this runs in linear time and that it succeeds with high probability. (All the bad events are large deviations away from the expectation of a binomial.)</p>\n\n<p>An alternate algorithm for the same problem, which is more natural to teach to students who have seen quick sort is the one described here: <a href="http://cs.stackexchange.com/questions/1334/randomized-selection/1343">Randomized Selection</a></p>\n\n<p>It is also easy to see that this one has linear expected running time: say that a "round" is a sequence of recursive calls that ends when one gives a 1/4-3/4 split, and then observe that the expected length of a round is at most 2.  (In the first draw of a round, the probability of getting a good split is 1/2 and then after actually increases, as the algorithm was described so round length is dominated by a  geometric random variable.)</p>\n\n<p>So now the question: </p>\n\n<blockquote>\n  <p>Is it possible to show that randomized selection runs in linear time with high probability?</p>\n</blockquote>\n\n<p>We have $O(\\log n)$ rounds, and each round has length at least $k$ with probability at most $2^{-k+1}$, so a union bound gives that the running time is $O(n\\log\\log n)$ with probability $1-1/O(\\log n)$.</p>\n\n<p>This is kind of unsatisfying, but is it actually the truth?</p>\n', 'ViewCount': '98', 'Title': 'Sharp concentration for selection via random partitioning?', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-19T19:52:47.013', 'LastEditDate': '2012-04-19T19:52:47.013', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1350', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '657', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms>', 'CreationDate': '2012-04-18T20:22:01.597', 'Id': '1346'},1182:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '250', 'Title': 'Algorithm to chase a moving target', 'LastEditDate': '2012-04-20T22:53:41.330', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '4', 'Body': "<p>Suppose that we have a black-box $f$ which we can query and reset. When we reset $f$, the state $f_S$ of $f$ is set to an element chosen uniformly at random from the set $$\\{0, 1, ..., n - 1\\}$$ where $n$ is fixed and known for given $f$. To query $f$, an element $x$ (the guess) from $$\\{0, 1, ..., n - 1\\}$$ is provided, and the value returned is $(f_S - x) \\mod n$. Additionally, the state $f_S$ of$f$ is set to a value $f_S&#39; = f_S \\pm k$, where $k$ is selected uniformly at random from $$\\{0, 1, 2, ..., \\lfloor n/2 \\rfloor - ((f_S - x) \\mod n)\\} $$</p>\n\n<p>By making uniformly random guesses with each query, one would expect to have to make $n$ guesses before getting $f_S = x$, with variance $n^2 - n$ (stated without proof).</p>\n\n<p>Can an algorithm be designed to do better (i.e., make fewer guesses, possibly with less variance in the number of guesses)? How much better could it do (i.e., what's an optimal algorithm, and what is its performance)?</p>\n\n<p>An efficient solution to this problem could have important cost-saving implications for shooting at a rabbit (confined to hopping on a circular track) in a dark room.</p>\n", 'Tags': '<algorithms><probability-theory><randomized-algorithms>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-10T13:45:31.237', 'CommentCount': '3', 'AcceptedAnswerId': '1976', 'CreationDate': '2012-04-20T14:48:58.600', 'Id': '1392'},1183:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'m trying to find the best algorithm for converting an \u201cordinary\u201d linked list into an \u201cideal" skip list. </p>\n\n<p>The definition of an \u201cideal skip list\u201d is that in the first level we\'ll have all the elements, half of them in the next level, a quarter of them in the level after that, and so on.</p>\n\n<p>I\'m thinking about a $\\mathcal{O}(n)$ run-time algorithm involving throwing a coin for each node in the original linked-list, to determine for any given node whether it should be placed in a higher or lower level, and create a duplicate node for the current node at a higher level. This algorithm should work in $\\mathcal{O}(n)$; is there any better algorithm? </p>\n', 'ViewCount': '302', 'Title': 'Building ideal skip lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-16T14:45:04.503', 'LastEditDate': '2012-05-09T10:47:32.457', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '776', 'Tags': '<algorithms><data-structures><randomized-algorithms><lists>', 'CreationDate': '2012-04-30T13:03:01.867', 'FavoriteCount': '1', 'Id': '1589'},1184:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '79', 'Title': 'Making random sources uniformly distributed', 'LastEditDate': '2012-05-19T14:50:50.590', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '0', 'Body': '<p>How do I build a random source that outputs the bits 0 and 1 with $prob(0) = prob(1) = 0.5$. We have access to another random source $S$ that outputs $a$ or $b$ with independent probabilities $prob(a)$ and $prob(b) = 1 - prob(a)$ that are unknown to us.</p>\n\n<p>How do I state an algorithm that does the job and that does not consume more than an expected number of\n$(prob(a) \\cdot prob(b))^{-1}$ symbols of $S$ between two output bits and prove its correcteness?</p>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-19T19:13:23.613', 'CommentCount': '1', 'AcceptedAnswerId': '1934', 'CreationDate': '2012-05-19T14:14:55.337', 'Id': '1921'},1185:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '174', 'Title': 'Deterministic and randomized communication complexity of set equality', 'LastEditDate': '2012-06-06T13:47:19.663', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '1', 'Body': u'<p>Two processors $A, B$ with inputs $a \\in  \\{0, 1\\}^n$ (for $A$) and $b \\in  \\{0, 1\\}^n$\n(for $B$) want to decide whether $a = b$. $A$ does not know $B$\u2019s input and vice versa.</p>\n\n<p>A can send a message $m(a) \\in  \\{0, 1\\}^n$ which $B$ can use to decide $a = b$. The communication and computation rules are called a <em>protocol</em>.</p>\n\n<ul>\n<li>Show that every deterministic protocol must satisfy $|m(a)| \\ge  n$.</li>\n<li>State a randomized protocol that uses only $O(\\log_2n)$ Bits. The protocol should always accept if $a = b$ and accept with probability at most $1/n$ otherwise. Prove its correctness.</li>\n</ul>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-06-06T13:47:19.663', 'CommentCount': '5', 'AcceptedAnswerId': '1978', 'CreationDate': '2012-05-19T14:30:57.650', 'Id': '1922'},1186:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A source provides a stream of items $x_1, x_2,\\dots$ . At each step $n$ we want to save a random sample $S_n \\subseteq \\{ (x_i, i)|1 \\le i \\le n\\}$ of size $k$, i.e. $S_n$ should be a uniformly chosen sample from all $\\tbinom{n}{k}$ possible samples consisting of seen items. So at each step $n \\ge k$ we must decide whether to add the next item to $S$ or not. If so we must also decide which of the current items to remove from $S$ .</p>\n\n<p>State an algorithm for the problem. Prove its correctness.</p>\n', 'ViewCount': '158', 'Title': 'Online generation of uniform samples', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-03T01:02:26.967', 'LastEditDate': '2012-05-19T15:31:20.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1931', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness><online-algorithms>', 'CreationDate': '2012-05-19T14:38:52.510', 'Id': '1923'},1187:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '785', 'Title': 'How to prove correctness of a shuffle algorithm?', 'LastEditDate': '2012-05-30T08:13:00.493', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': "<p>I have two ways of producing a list of items in a random order and would like to determine if they are equally fair (unbiased).</p>\n\n<p>The first method I use is to construct the entire list of elements and then do a shuffle on it (say a Fisher-Yates shuffle). The second method is more of an iterative method which keeps the list shuffled at every insertion. In pseudo-code the insertion function is:</p>\n\n<pre><code>insert( list, item )\n    list.append( item )\n    swap( list.random_item, list.last_item )\n</code></pre>\n\n<p>I'm interested in how one goes about showing the fairness of this particular shuffling. The advantages of this algorithm, where it is used, are enough that even if slightly unfair it'd be okay. To decide I need a way to evaluate its fairness.</p>\n\n<p>My first idea is that I need to calculate the total permutations possible this way versus the total permutations possible for a set of the final length. I'm a bit at a loss however on how to calculate the permutations resulting from this algorithm. I also can't be certain this is the best, or easiest approach.</p>\n", 'Tags': '<algorithms><proof-techniques><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-01T11:23:35.340', 'CommentCount': '5', 'AcceptedAnswerId': '2156', 'CreationDate': '2012-05-29T07:11:15.180', 'Id': '2152'},1188:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Consider an array $X$ of $n$ cells, each containing a number from $\\{1,..., n\\}$. There is at least\none duplicate number, i.e., a number that appears at least twice. I want output <em>some</em> duplicate number. When streaming we may pass over $X$ more than once. The inspection of a cell generates cost $1$. The cost of a run of an algorithm is the sum of all individual costs. I can at most store $\\log_2n$ bit numbers.\nI tried to do that with a streaming algorithm that uses additional memory $O(1)$ with costs $O(n^2)$. Is it possible to state a ramdom access algorithm that uses additional memory $O(\\log_2n)$ with costs $O(n)$?.</p>\n\n<p>Which algorithm solves the problem by using additional memory $O(1)$ with costs $O(n^2)$?.\nWhich algorithm solves the problem by using additional memory $O(\\log_2n)$ with costs $O(n)$?.</p>\n\n<p>My problem is similar to the cycle detection problem, but I don't know how to use the cycle detection problem to solve mine. Is there maybe a simpler way that I can't see now?</p>\n", 'ViewCount': '115', 'Title': 'Streaming algorithm and random access', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-03T22:21:04.637', 'LastEditDate': '2012-06-02T18:27:50.347', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><randomized-algorithms><random><streaming-algorithm>', 'CreationDate': '2012-06-02T14:46:56.317', 'FavoriteCount': '2', 'Id': '2199'},1189:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '749', 'Title': 'Sorting algorithms which accept a random comparator', 'LastEditDate': '2012-06-12T20:52:01.087', 'AnswerCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': u'<p>Generic sorting algorithms generally take a set of data to sort and a comparator function which can compare two individual elements. If the comparator is an order relation\xb9, then the output of the algorithm is a sorted list/array.</p>\n\n<p>I am wondering though which sort algorithms would actually <em>work</em> with a comparator that is not an order relation (in particular one which returns a random result on each comparison). By "work" I mean here that they continue return a permutation of their input and run at their typically quoted time complexity (as opposed to degrading to the worst case scenario always, or going into an infinite loop, or missing elements). The ordering of the results would be undefined however. Even better, the resulting ordering would be a uniform distribution when the comparator is a coin flip.</p>\n\n<p>From my rough mental calculation it appears that a merge sort would be fine with this and maintain the same runtime cost and produce a fair random ordering. I think that something like a quick sort would however degenerate,  possibly not finish, and not be fair.</p>\n\n<p>What other sorting algorithms (other than merge sort) would work as described with a random comparator?</p>\n\n<hr>\n\n<ol>\n<li><p>For reference, a comparator is an order relation if it is a proper function (deterministic) and satisfies the axioms of an order relation:</p>\n\n<ul>\n<li>it is deterministic: <code>compare(a,b)</code> for a particular <code>a</code> and <code>b</code> always returns the same result.</li>\n<li>it is transitive: <code>compare(a,b) and compare(b,c) implies compare( a,c )</code></li>\n<li>it is antisymmetric <code>compare(a,b) and compare(b,a) implies a == b</code></li>\n</ul></li>\n</ol>\n\n<p>(Assume that all input elements are distinct, so reflexivity is not an issue.)</p>\n\n<p>A random comparator violates all of these rules. There are however comparators that are not order relations yet are not random (for example they might violate perhaps only one rule, and only for particular elements in the set).</p>\n', 'Tags': '<algorithms><randomized-algorithms><sorting>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-03-20T18:11:29.530', 'CommentCount': '11', 'AcceptedAnswerId': '2349', 'CreationDate': '2012-06-12T11:39:54.473', 'Id': '2336'},11810:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Are there any problems in $\\mathsf{P}$ that have randomized algorithms beating lower bounds on deterministic algorithms? More concretely, do we know any $k$ for which $\\mathsf{DTIME}(n^k) \\subsetneq \\mathsf{PTIME}(n^k)$? Here $\\mathsf{PTIME}(f(n))$ means the set of languages decidable by a randomized TM with constant-bounded (one or two-sided) error in $f(n)$ steps. </p>\n\n<blockquote>\n  <p>Does randomness buy us anything inside $\\mathsf{P}$?</p>\n</blockquote>\n\n<p>To be clear, I am looking for something where the difference is asymptotic (preferably polynomial, but I would settle for polylogarithmic), not just a constant.</p>\n\n<p><em>I am looking for algorithms asymptotically better in the worst case. Algorithms with better expected complexity are not what I am looking for. I mean randomized algorithms as in RP or BPP not ZPP.</em></p>\n', 'ViewCount': '332', 'Title': 'Problems in P with provably faster randomized algorithms', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-29T08:14:04.840', 'LastEditDate': '2013-07-29T08:08:49.313', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '208', 'Tags': '<algorithms><complexity-theory><randomized-algorithms>', 'CreationDate': '2012-06-13T13:44:19.383', 'FavoriteCount': '4', 'Id': '2362'},11811:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We are given a set of objects, say integers, $S$. In addition, we are given a predicate $P$, for example $P(i): \\Leftrightarrow i \\geq 0$. We don't know in advance how many elements of $S$ satisfy the predicate $P$, but we would like to sample or choose an element uniformly at random from $S' = \\{ i \\mid i \\in S \\wedge P(i) \\}$.</p>\n\n<p>The naive approach is to scan $S$ and for example record all the integers or indices for which $P$ holds, then choose one of them uniformly at random. The downside is that in the worst-case, we need $|S|$ space.</p>\n\n<p>For large sets or in say a streaming environment the naive approach is not acceptable. Is there an in-place algorithm for the problem?</p>\n", 'ViewCount': '112', 'Title': 'Choosing an element from a set satisfying a predicate uniformly at random in $O(1)$ space', 'LastActivityDate': '2012-07-21T21:45:31.217', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2856', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><randomized-algorithms><streaming-algorithm><in-place>', 'CreationDate': '2012-07-21T21:45:31.217', 'Id': '2855'},11812:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What differences and relationships are between randomized algorithms and nondeterministic algorithms?</p>\n\n<p>From Wikipedia</p>\n\n<blockquote>\n  <p>A <strong>randomized algorithm</strong> is an algorithm which employs a degree of\n  randomness as part of its logic. The algorithm typically uses\n  uniformly random bits as an auxiliary input to guide its behavior, in\n  the hope of achieving good performance in the "average case" over all\n  possible choices of random bits. Formally, the algorithm\'s performance\n  will be a random variable determined by the random bits; thus either\n  the running time, or the output (or both) are random variables.</p>\n  \n  <p>a <strong>nondeterministic algorithm</strong> is an algorithm that can exhibit\n  different behaviors on different runs, as opposed to a deterministic\n  algorithm. There are several ways an algorithm may behave differently\n  from run to run. A <strong>concurrent algorithm</strong> can perform differently on\n  different runs due to a race condition. A <strong>probabalistic algorithm</strong>\'s\n  behaviors depends on a random number generator. An algorithm that\n  solves a problem in nondeterministic polynomial time can run in\n  polynomial time or exponential time depending on the choices it makes\n  during execution.</p>\n</blockquote>\n\n<p>Are randomized algorithms and probablistic algorithms the same concept? </p>\n\n<p>If yes, so are randomized algorithms just a kind of nondeterministic algorithms?</p>\n', 'ViewCount': '1555', 'Title': 'Differences and relationships between randomized and nondeterministic algorithms?', 'LastEditorUserId': '31', 'LastActivityDate': '2014-02-02T18:42:47.173', 'LastEditDate': '2014-01-23T10:11:12.273', 'AnswerCount': '4', 'CommentCount': '7', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><terminology><randomized-algorithms><machine-models><nondeterminism>', 'CreationDate': '2012-10-11T05:19:33.520', 'FavoriteCount': '5', 'Id': '5008'},11813:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '172', 'Title': 'Classfication of randomized algorithms', 'LastEditDate': '2012-10-22T00:56:03.857', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '3', 'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Randomized_algorithm">Wikipedia</a> about randomized algorithms</p>\n\n<blockquote>\n  <p>One has to distinguish between <strong>algorithms</strong> that use the random\n  input to reduce the expected running time or memory usage, but always\n  terminate with a correct result in a bounded amount of time, and\n  <strong>probabilistic algorithms</strong>, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo\n  algorithms) or fail to produce a result (Las Vegas algorithms) either\n  by signalling a failure or failing to terminate.</p>\n</blockquote>\n\n<ol>\n<li>I was wondering how the first kind of "<strong>algorithms</strong> use the random\ninput to reduce the expected running time or memory usage, but\nalways  terminate with a correct result in a bounded amount of time?</li>\n<li>What differences are between it and Las Vegas algorithms which may\nfail to produce a result?</li>\n<li>If I understand correctly,  probabilistic algorithms and randomized algorithms are not the same concept. Probabilistic algorithms are just one\nkind of randomized algorithms, and the other kind is those use the\nrandom  input to reduce the expected running time or memory usage,\nbut always  terminate with a correct result in a bounded amount of\ntime?</li>\n</ol>\n', 'Tags': '<algorithms><terminology><randomized-algorithms><nondeterminism><machine-models>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-04-07T14:10:29.767', 'CommentCount': '0', 'AcceptedAnswerId': '6222', 'CreationDate': '2012-10-22T00:53:00.617', 'Id': '6221'},11814:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '157', 'Title': 'Are randomized algorithms constructive?', 'LastEditDate': '2012-10-25T11:05:48.563', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '1', 'Body': '<p>From , the proofs by the probabilistic method are often said to be non-constructive.</p>\n\n<p>However,  a proof by probabilistic method indeed designs a randomized algorithm and uses it for proving existence. Quoted from p103 of <a href="http://books.google.com/books?id=QKVY4mDivBEC&amp;pg=PR5&amp;lpg=PP1&amp;dq=randomized%20algorithms#v=onepage&amp;q&amp;f=false">Randomized Algorithms\n By Rajeev Motwani, Prabhakar Raghavan</a>:</p>\n\n<blockquote>\n  <p>We could view the proof  by the probabilistic method as a randomized\n  algorithm. This would then require a further analysis bounding the\n  probability that the algorithm fails to find a good partition on a\n  given execution. The main difference between a thought experiment in\n  the probabilistic method and a randomized algorithm is the end that\n  each yields. When we use the probabilistic method, we are only\n  concerned with showing that a combinatorial object exists; thus, we\n  are content with showing that a favorable event occurs with non-zero\n  probability. With a randomized algorithm, on the other hand,\n  efficiency is an important consideration - we cannot tolerate a\n  miniscule success probability.</p>\n</blockquote>\n\n<p>So I wonder if randomized algorithms are viewed as not constructive, although they do output a solution at the end of each run, which may or may not be an ideal solution.</p>\n\n<p>How is an algorithm or proof being "constructive" defined?</p>\n\n<p>Thanks!</p>\n', 'Tags': '<algorithms><terminology><randomized-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-30T16:24:48.083', 'CommentCount': '5', 'AcceptedAnswerId': '6289', 'CreationDate': '2012-10-24T12:19:26.950', 'Id': '6288'},11815:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>There are $N$ number of people and $X$ amount of objects with different values. Each person will choose an object and will obtain that objects value. If multiple people choose the same object then the value of the object is shared among the people. For example, If there are 2 objects and 3 people, one of the objects will be chosen by at least 2 people, thus the value of the object is divided over 2. </p>\n\n<p>Each round the people make their decisions with new objects and new values, how can a person maximize their expected values accumulated from their choices in Q number of rounds, in order to win(i.e accumulated most value)? ($Q$ belongs to [1, +infinite).</p>\n\n<p>Note that keeping track of other player's decisions and their accumulated values can help in future decision making.</p>\n\n<p>Note: An approximation would be great as well, so far my strategy is to choose a random object at each round, I am looking for ways to maximize this strategy. </p>\n", 'ViewCount': '142', 'Title': 'Guessing the best choice to maximize returns', 'LastEditorUserId': '4365', 'LastActivityDate': '2012-11-16T09:25:51.250', 'LastEditDate': '2012-11-16T08:50:52.197', 'AnswerCount': '2', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4365', 'Tags': '<artificial-intelligence><randomized-algorithms>', 'CreationDate': '2012-11-15T21:33:25.423', 'Id': '6688'},11816:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '164', 'Title': 'Randomized Rounding of Solutions to Linear Programs', 'LastEditDate': '2012-11-30T04:10:17.983', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/Linear_programming#Integral_linear_programs" rel="nofollow">Integer linear programming</a> (ILP) is an incredibly powerful tool in combinatorial optimization. If we can formulate some problem as an instance of an ILP then solvers are guaranteed to find the global optimum. However, enforcing integral solutions has runtime that is exponential in the worst case. To cope with this barrier, several approximation methods related to ILPs can be used,</p>\n\n<ul>\n<li>Primal-Dual Schema</li>\n<li>Randomized Rounding</li>\n</ul>\n\n<p>The Primal-Dual Schema is a versatile method that gives us a "packaged" way to come up with a greedy algorithm and prove its approximation bounds using the relaxed dual LP. Resulting combinatorial algorithms tend to be very fast and perform quite well in practice. However its relation to linear programming is closer tied to the analysis. Further because of this analysis, we can easily show that constraints are not violated.</p>\n\n<p>Randomized rounding takes a different approach and solves the relaxed LP (using interior-point or ellipsoid methods) and rounds variables according to some probability distribution. If approximation bounds can be proven this method, like the Primal-Dual schema, is quite useful. However, one portion is not quite clear to me:</p>\n\n<blockquote>\n  <p>How do randomized rounding schemes show that constraints are not violated?</p>\n</blockquote>\n\n<p>It would appear that naively flipping a coin, while resulting in a 0-1 solution, could violate constraints! Any help illuminating this issue would be appreciated. Thank you.</p>\n', 'Tags': '<optimization><randomized-algorithms><linear-programming><approximation>', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-30T04:10:17.983', 'CommentCount': '4', 'AcceptedAnswerId': '6949', 'CreationDate': '2012-11-27T05:05:34.050', 'Id': '6941'},11817:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a graph $G = (V,E)$, where $|V| = n$. What is a fast algorithm for generating the collection of all 2-hop neighborhood lists of all nodes in $V$. </p>\n\n<p>Naively, you can do that in $O(n^3)$. With power of matrices, you can do that with $O(n^{2.8})$ using Strassen algorithm. You can do better than this using another matrix multiplication algorithm. Any better method ? Any Las Vegas algorithm ? </p>\n', 'ViewCount': '422', 'Title': 'Algorithm to find all 2-hop neighbors lists in a graph', 'LastActivityDate': '2012-12-06T15:48:01.947', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '7183', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '867', 'Tags': '<algorithms><algorithm-analysis><graphs><randomized-algorithms>', 'CreationDate': '2012-12-05T04:50:09.537', 'Id': '7169'},11818:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Count-Min_sketch">Count-Min Sketch</a> is an awesome data structure for estimating the frequencies of different elements in a data stream.  Intuitively, it works by picking a variety of hash functions, hashing each element with those hash functions, and incrementing the frequencies of various slots in various tables.  To estimate the frequency of an element, the Count-Min sketch applies the hash functions to those elements and takes the minimum value out of all the slots that are hashed to.</p>\n\n<p>The <a href="http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf">original paper on the Count-Min Sketch</a> mentions that the data structure requires pairwise independent hash functions in order to get the necessary guarantees on its expected performance.  However, looking over the structure, I don\'t see why pairwise independence is necessary.  Intuitively, I would think that all that would be required would be that the hash function be <a href="http://en.wikipedia.org/wiki/Universal_hashing">a universal hash function</a>, since universal hash functions are hash functions with low probabilities of collisions.  The analysis of the collision probabilities in the Count-Min Sketch looks remarkably similar to the analysis of collision probabilities in a chained hash table (which only requires a family of universal hash functions, not pairwise independent hash functions), and I can\'t spot the difference in the analyses.</p>\n\n<p>Why is it necessary for the hash functions in the Count-Min Sketch to be pairwise independent?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '467', 'Title': 'Why does the Count-Min Sketch require pairwise independent hash functions?', 'LastActivityDate': '2012-12-09T21:05:41.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7279', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><data-structures><randomized-algorithms><hash>', 'CreationDate': '2012-12-09T19:52:37.693', 'Id': '7275'},11819:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In their book <a href="http://www.amazon.co.uk/Randomized-Algorithms-Cambridge-International-Computation/dp/0521474655/ref=sr_1_1?ie=UTF8&amp;qid=1356380410&amp;sr=8-1"><em>Randomized Algorithms</em>,</a> Motwani and Raghavan open the introduction with a description of their RandQS function -- Randomized quicksort -- where the pivot, used for partitioning the set into two parts, is chosen at random.</p>\n\n<p>I have been racking my (admittedly somewhat underpowered) brains over this for some time, but I haven\'t been able to see what advantage this algorithm has over simply picking, say, the middle element (in index, not size) each time.</p>\n\n<p>I suppose what I can\'t see is this: if the initial set is in a random order, what is the difference between picking an element at a random location in the set and picking an element at a fixed position?</p>\n\n<p>Can someone enlighten me, in fairly simple terms? </p>\n', 'ViewCount': '1069', 'Title': 'What is the advantage of Randomized Quicksort?', 'LastActivityDate': '2014-05-03T22:52:13.613', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7583', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '5178', 'Tags': '<algorithm-analysis><sorting><randomized-algorithms>', 'CreationDate': '2012-12-24T20:26:07.713', 'FavoriteCount': '1', 'Id': '7582'},11820:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '432', 'Title': 'Discrepancy between heads and tails', 'LastEditDate': '2012-12-26T14:53:11.573', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '5195', 'FavoriteCount': '4', 'Body': "<p>Consider a sequence of $n$ flips of an unbiased coin. Let $H_i$ denote the absolute value of the excess of the number of heads over tails seen in the first $i$ flips. Define $H=\\text{max}_i H_i$. Show that $E[H_i]=\\Theta ( \\sqrt{i} )$ and $E[H]=\\Theta( \\sqrt{n} )$. </p>\n\n<p>This problem appears in the first chapter of `Randomized algorithms' by Raghavan and Motwani, so perhaps there is an elementary proof of the above statement. I'm unable to solve it, so I would appreciate any help.</p>\n", 'Tags': '<probability-theory><randomized-algorithms>', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-31T01:55:08.807', 'CommentCount': '0', 'AcceptedAnswerId': '7601', 'CreationDate': '2012-12-26T07:03:36.157', 'Id': '7600'},11821:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '206', 'Title': 'randomized algorithm for checking the satisfiability of s-formulas, that outputs the correct answer with probability at least $\\frac{2}{3}$', 'LastEditDate': '2013-01-05T20:18:01.783', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '1', 'Body': "<p>I'm trying to practice myself with random algorithms.</p>\n\n<p>Lets call a CNF formula over n variables s-formula if it is either unsatisable or it has at\nleast $\\frac{2^n}{n^{10}}$ satisfying assignments.</p>\n\n<p>I would like your help with show a randomized algorithm for checking the\nsatisfiability of s-formulas, that outputs the correct answer with probability at\nleast $\\frac{2}{3}$.</p>\n\n<p>I'm not really sure how to prove it. First thing that comes to my head is this thing- let's accept with probability $\\frac{2}{3}$ every input. Then if the input in the language, it was accepted whether in the initial toss($\\frac{2}{3}$) or it was not and then the probability to accept it is $\\frac{1}{3}\\cdot proability -to-accept$ which is bigger than $\\frac{2}{3}$. Is this the way to do that or should I use somehow Chernoff inequality which I'm not sure how.</p>\n", 'Tags': '<complexity-theory><time-complexity><randomized-algorithms>', 'LastEditorUserId': '1589', 'LastActivityDate': '2013-01-05T20:18:01.783', 'CommentCount': '0', 'AcceptedAnswerId': '7748', 'CreationDate': '2012-12-29T09:38:45.680', 'Id': '7641'},11822:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '241', 'Title': 'Concrete understanding of difference between PP and BPP definitions', 'LastEditDate': '2013-02-15T07:31:32.413', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5356', 'FavoriteCount': '3', 'Body': '<p>I am confused about how  <strong>PP</strong> and <strong>BPP</strong> are defined. Let us assume $\\chi$ is the characteristic function for a language $\\mathcal{L}$. <em>M</em> be the probabilistic Turing Machine. Are the following definitions correct:<br>\n$BPP =\\{\\mathcal{L} :Pr[\\chi(x) \\ne M(x)] \\geq \\frac{1}{2} + \\epsilon \\quad \\forall x \\in \\mathcal{L},\\ \\epsilon &gt; 0 \\}$<br>\n$PP =\\{\\mathcal{L} :Pr[\\chi(x) \\ne M(x)] &gt; \\frac{1}{2} \\}$  </p>\n\n<p>If the definition are wrong, please try to make minimal change to make them correct (i.e. do not give other equivalent definition which use counting machine or some modified model). I can not properly distinguish the conditions on probability on both the definitions.  </p>\n\n<p>Some concrete examples with clear insight into the subtle points would be very helpful. </p>\n', 'Tags': '<complexity-theory><terminology><randomized-algorithms><complexity-classes>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-15T07:31:32.413', 'CommentCount': '0', 'AcceptedAnswerId': '7849', 'CreationDate': '2013-01-09T11:28:04.633', 'Id': '7848'},11823:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I try to figure out <strong>a redundant power of two-sided error randomized Karp - reduction.</strong></p>\n\n<p>It\'s well known fact and it is relatively hard to show that <a href="http://en.wikipedia.org/wiki/BPP_(complexity)" rel="nofollow">BPP</a> is reducible by a one-sided error randomized Karp-reduction to coRP (in case of promise problem).</p>\n\n<p>Without delving into details it make sense that the combination of the one - sided error probability of the reduction and the one-sided error probability of coRP leads to two-sided error probability of BPP. Of course the proof of that is not so intuitive.</p>\n\n<p>The question it is possible by two-sided error randomized Karp-reduction to reduce BPP to some constant set in P? In the light of the power of one - sided randomized Karp - reduction, it make sense that two-sided randomized Karp - reduction is strong enough to reduce BPP to constant set, but how to show it formally?</p>\n\n<p><strong>Addendum:</strong></p>\n\n<p><strong>BPP</strong> is the set of the problems that is solvable in polynomial time by two-sided error randomized algorithm, so as a result of two - sided error randomized algorithm we will get some output, them the problem in BPP can be reduced to problem P by two-sided error randomized Karp - reduction in sense that reduction is allowed to make error on both sides. Does it mean that two - sided error randomized reduction will justify the two-sided error that was made by the algorithm in solving the problem in BPP?</p>\n', 'ViewCount': '97', 'Title': 'The Power of Randomized Reduction', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-10-05T07:50:18.583', 'LastEditDate': '2013-01-21T09:54:55.740', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<complexity-theory><reductions><randomized-algorithms>', 'CreationDate': '2013-01-19T09:56:05.707', 'Id': '9037'},11824:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to solve exercise 6.5 on page 309 from Richard Crandall\'s "Prime numbers - A computational perspective". It basically asks for an algorithm to factor integers in randomized polynomial time given an oracle for taking square roots modulo $n$.</p>\n\n<p>I think, the basic idea is the following: Given a composite number $n$, to take a random element $r$ in $\\left.\\mathbb{Z}\\middle/n\\mathbb{Z}\\right.$ and square it. If $r$ was a square, $r^2$ can have up to $4$ different square roots and the basic idea of the algorithm is that the oracle has some chance not to choose $\\pm r$, but one of the other two roots. It will turn out that we then can determine a factor of $n$ using Euclidean\'s algorithm. </p>\n\n<p>I formalized this to</p>\n\n<p><strong>Input</strong>: $n=pq\\in\\mathbb{Z}$ with primes $p$ and $q$.</p>\n\n<p><strong>Output</strong>: $p$ or $q$</p>\n\n<ol>\n<li>Take a random number $r$ between $1$ and $n-1$</li>\n<li>If $r\\mid n$ then return $r$ (we were lucky)</li>\n<li>$s:= r^2\\pmod{n}$</li>\n<li>$t:=\\sqrt{s}\\pmod{n}$ (using the oracle)</li>\n<li>If $t\\equiv \\pm r\\pmod{n}$ then goto step 1.</li>\n<li>Return $\\gcd(t-r,n)$</li>\n</ol>\n\n<p>One can show that $t \\not\\equiv \\pm r\\pmod{n}$ implies that $\\gcd(t-r,n)\\neq 1,n$ and therefore get that the return value of the algorithm is a non-trivial factor of $n$. </p>\n\n<p>Inspired by my main question "How do I prove that the running time is polynomial in the bit-size of the input?" I have some follow up questions:</p>\n\n<ol>\n<li>Do I have to show that a lot of numbers between $1$ and $n-1$ are squares? There must be a well-known theorem or easy fact that shows this (well... not well-known to me ;-). </li>\n<li>Are there any more details I have consider? </li>\n<li>Has every square of a square exactly $4$ square roots modulo $n$? </li>\n</ol>\n', 'ViewCount': '109', 'Title': 'Solve Integer Factoring in randomized polynomial time with an oracle for square root modulo $n$', 'LastActivityDate': '2013-01-23T15:57:01.553', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9114', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2103', 'Tags': '<randomized-algorithms><integers><factoring>', 'CreationDate': '2013-01-23T09:39:28.187', 'FavoriteCount': '1', 'Id': '9106'},11825:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What I only got currently from PCP theorem is that it needs at most $O(\\log n)$ randomness and $O(1)$ query of proof to approximate. So how does this result relate to the fact that solution to NP problems are hard to approximate?</p>\n', 'ViewCount': '84', 'Title': 'Why does PCP theorem imply that NP problems are hard to approximate?', 'LastActivityDate': '2013-03-05T19:11:45.947', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10299', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7154', 'Tags': '<complexity-theory><randomized-algorithms>', 'CreationDate': '2013-03-05T12:56:15.360', 'Id': '10289'},11826:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Inspired by <a href="http://cs.stackexchange.com/q/2336/71">this question</a> in which the asker wants to know if the running time changes when the comparator used in a standard search algorithm is replaced by a fair coin-flip, and also <a href="http://www.robweir.com/blog/2010/02/microsoft-random-browser-ballot.html">Microsoft\'s</a> prominent failure to write a uniform permutation generator, my question is thus:</p>\n\n<p>Is there a comparison based sorting algorithm which will, depending on our implementation of the comparator:</p>\n\n<ol>\n<li>return the elements in sorted order when using a <em>true</em> comparator (that is, the comparison does what we expect in a standard sorting algorithm) </li>\n<li>return a uniformly random permutation of the elements when the comparator is replaced by a fair coin flip (that is, return <code>x &lt; y = true</code> with probability 1/2, regardless of the value of x and y)</li>\n</ol>\n\n<p>The code for the sorting algorithm must be the same. It is only the code inside the comparison "black box" which is allowed to change.</p>\n', 'ViewCount': '296', 'Title': 'Is there a "sorting" algorithm which returns a random permutation when using a coin-flip comparator?', 'LastActivityDate': '2013-03-23T11:27:52.923', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '10658', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<sorting><randomized-algorithms><permutations>', 'CreationDate': '2013-03-20T18:14:44.773', 'Id': '10656'},11827:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From <a href="http://programmers.stackexchange.com/questions/194552/what-is-the-difference-between-quantum-annealing-and-simulated-annealing">that question</a> about differences between Quantum annealing and simulated annealing, we found (in commets to answer) that physical implementation of quantum annealing is exists (D-Wave quantum computers).</p>\n\n<p>Can anyone explain that algorithm in terms of quantum gates and quantum algorithms, or in physical terms (a part of algorithm that depends on quantum hardware)?</p>\n\n<p>Does anyone have any ideas about that?\nPlease tell me, if you know some links related this question.</p>\n', 'ViewCount': '419', 'Title': 'The physical implementation of quantum annealing algorithm', 'LastActivityDate': '2013-04-19T20:20:45.790', 'AnswerCount': '4', 'CommentCount': '3', 'AcceptedAnswerId': '11362', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7672', 'Tags': '<algorithms><randomized-algorithms><quantum-computing>', 'CreationDate': '2013-04-11T07:05:16.517', 'FavoriteCount': '2', 'Id': '11218'},11828:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Cormen talks briefly about the advantages of picking a random pivot in quicksort.  However as pointed out <a href="http://algoviz.org/OpenDSA/Books/OpenDSA/html/Quicksort.html" rel="nofollow">here</a>(4th to the last paragraph):</p>\n\n<blockquote>\n  <p>Using a random number generator to choose the positions is relatively\n  expensive</p>\n</blockquote>\n\n<p>So how is picking a random pivot actually implemented in practice, and how random is it?  It can\'t be too expensive, since from what I understand one of quicksort\'s main advantages over other $\\cal{O}(n \\lg n)$ sorts is the small constant factors, and spending allot of cycles picking pivots would undermine that advantage.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>As an example, the <code>C</code> code from "<a href="http://www.catonmat.net/blog/three-beautiful-quicksorts/" rel="nofollow">Three Beautiful Quicksorts</a>" actually calls the <code>C</code> library <code>rand</code> function:</p>\n\n<pre><code>int randint(int l, int u) {\n    return rand()%(u-l+1)+l;\n}\n\nvoid quicksort(int l, int u) {\n    int i, m;\n    if (l &gt;= u) return;\n    swap(l, randint(l, u));\n    m = l;\n    for (i = l+1; i &lt;= u; i++)\n        if (x[i] &lt; x[l])\n            swap(++m, i);\n    swap(l, m);\n    quicksort(l, m-1);\n    quicksort(m+1, u);\n}\n</code></pre>\n\n<p>While the pivot picking code here is clearly $\\cal{O}(1)$, it would seem that the hidden $c$ here is relatively high.</p>\n', 'ViewCount': '207', 'Title': 'From Whence the Randomization in Randomized Quicksort', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-23T09:45:57.250', 'LastEditDate': '2013-04-21T14:02:28.190', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11387', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><randomized-algorithms><quicksort>', 'CreationDate': '2013-04-18T18:04:50.873', 'Id': '11385'},11829:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>It is well known that the efficiency of randomized algorithms (at least those in BPP and RP) depends on the quality of the random generator used. Perfect random sources are unavailable in practice. Although it is proved that for all $0 &lt; \\delta \\leq \\frac{1}{2}$ the identities BPP = $\\delta$-BPP and RP = $\\delta$-RP hold, it is not true that the original algorithm used for a prefect random source can be directly used also for a $\\delta$-random source. Instead, some simulation has to be done. This simulation is polynomial, but the resulting algorithm is not so efficient as the original one.</p>\n\n<p>Moreover, as to my knowledge, the random generators used in practice are usually not even $\\delta$-sources, but pseudo-random sources that can behave extremely badly in the worst case.</p>\n\n<p>According to <a href="http://en.wikipedia.org/wiki/Randomized_algorithm" rel="nofollow" title="Wikipedia">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.</p>\n</blockquote>\n\n<p>In fact, the implementations of randomized algorithms that I have seen up to now were mere implementations of the algorithms for perfect random sources run with the use of pseudorandom sources.</p>\n\n<p>My question is, if there is any justification of this common practice. Is there any reason to expect that in most cases the algorithm will return a correct result (with the probabilities as in BPP resp. RP)? How can the "approximation" mentioned in the quotation from Wikipedia be formalized? Can the deviation mentioned be somehow estimated, at least in the expected case? Is it possible to argue that a Monte-Carlo randomized algorithm run on a perfect random source will turn into a well-behaved stochastic algorithm when run on a pseudorandom source? Or are there any other similar considerations?</p>\n', 'ViewCount': '92', 'Title': 'Random generator considerations in the design of randomized algorithms', 'LastActivityDate': '2013-05-02T22:23:28.033', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11744', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2091', 'Tags': '<randomized-algorithms><randomness><pseudo-random-generators>', 'CreationDate': '2013-05-02T10:41:01.760', 'Id': '11726'},11830:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm taking a grad level randomized algorithms course in the fall. The professor is known for being very detail oriented and mathematically rigorous, so I will be required to have an in-depth understanding of probability. What would be a good probability book to learn from that would be intuitive, but also have some mathematical rigor to it?</p>\n", 'ViewCount': '79', 'Title': 'Randomized Algorithms Probability', 'LastActivityDate': '2013-05-19T13:03:22.573', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12133', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8242', 'Tags': '<probability-theory><randomized-algorithms>', 'CreationDate': '2013-05-18T14:20:52.533', 'Id': '12113'},11831:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been recently studying Monte-Carlo and other randomized methods for a lot of applications, and one that popped into my mind was making an (approximate) convex hull by examining random points, and try to get them inside the convex hull. I would like to know if there are algorithms for convex hulls that can improve the $O(n \\log n)$ bound of comparison based algorithms, and the $O(n\\cdot h)$ bound for Jarvis march and related to $O(n)$, either by building an approximate convex hull in $O(n)$ (with or without some approximation criteria) or by building an exact convex hull in expected linear time.</p>\n", 'ViewCount': '43', 'Title': 'Randomized convex hull', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-21T23:02:42.530', 'LastEditDate': '2013-05-21T19:34:43.807', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12208', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2138', 'Tags': '<algorithms><asymptotics><computational-geometry><randomized-algorithms>', 'CreationDate': '2013-05-21T19:30:47.060', 'Id': '12199'},11832:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to understand how the Mersenne Twister random number generator works (in particular, the 32-bit TinyMT). I am still relatively new to the concept of RNG. As I read the source code, I noticed there were two ways to seed the MT: with a single integer seed or an array of integer seeds. What is the point in seeding with an array? Does it produce a better distribution or a longer period? </p>\n\n<p>Also, I would appreciate it if somebody could explain to me what is meant by the "state" of the RNG, as I am seeing that word all over the source code. Is it like a finite state machine in a way? </p>\n\n<p>Thanks for your time!</p>\n', 'ViewCount': '152', 'Title': 'Seeding the Mersenne Twister Random Number Generator', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-24T14:19:28.083', 'LastEditDate': '2014-03-24T14:19:28.083', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<algorithms><randomized-algorithms><pseudo-random-generators><random-number-generator>', 'CreationDate': '2013-06-20T16:09:56.607', 'Id': '12792'},11833:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to implement and optimize the <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/TINYMT/" rel="nofollow">Tiny Mersenne Twister (TinyMT)</a> algorithm as required by an API I am developing with my team at work. The algorithm utilizes a C structure with 32-bit unsigned integers "mat1", "mat2", "tmat", and an array called "status" which is four 32-bit unsigned integers wide.</p>\n\n<p>I am relatively new to the subject of random number generation; however, I have been able to teach myself a lot about the subject over the past couple of weeks. I know what the purpose of a seed is, different methods such as linear congruent, LSFR, GFSR, etc. So I\'ve been doing my "homework" and researching the topic to the best of my ability (contrary to what the guys at Stack Overflow think). Unfortunately, the Mersenne Twister in general is extremely poorly documented and very few documents exist to explain the code and math side-by-side. The TinyMT\'s documentation is even worse, it\'s virtually non-existent! So developing accurate Doxygen comments for this part of the API is going to be tricky.</p>\n\n<p>With that said, hopefully somebody more qualified than I can help me out here. What is the significance of the aforementioned parameters? What do they do, what do they mean, what do they stand for, etc? My guess would be as follows:</p>\n\n<ul>\n<li>mat1 - Matrix 1</li>\n<li>mat2 - Matrix 2</li>\n<li>tmat - Tempering Matrix</li>\n<li>status - 127 bit wide "seed" (where the last bit goes, I\'m not sure)</li>\n</ul>\n\n<p>Given that the user provides values for "mat1", "mat2", and "tmat," are there any precautions they need to take before supplying values for them? Again, this is for an API and its documentation, so I would like to be able to give the customers a good idea of what they need to use the RNG and hopefully make the lives of other fellow programmers easier. Thanks!</p>\n', 'ViewCount': '76', 'Title': 'Significance of parameters in Tiny Mersenne Twister algorithm', 'LastActivityDate': '2013-06-26T02:46:19.197', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12904', 'Score': '1', 'OwnerDisplayName': 'audiFanatic', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<linear-algebra><randomized-algorithms><matrices>', 'CreationDate': '2013-06-25T18:06:30.620', 'Id': '12902'},11834:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From a very strictly adhering sense to the hardware and circuit-level operations of any standard (non-specialized, DSPs, or supercomputing systems, etc.) microprocessor follow very similar, almost exact in some ways, operations.</p>\n\n<p>The typical role of the (main) processor in a computer, integrated with other hardware circuits or not, and excluding DMA is to have a memory subsystem fetch byte(s) for it to "process" in whatever way. To have a processor "randomly" selective something can be abstracted and seen from a data algorithm <a href="http://en.wikipedia.org/wiki/High-level_programming_language" rel="nofollow">HLL-type</a> point of view, but on the circuit-level the operations can only get so complex. I know some Assembly of x86, so I can demonstrate further on the details of what I\'m asking.</p>\n\n<p>If you fetch a byte, or series of bytes, and then use some schematic to cycle through potential jump offsets, that is the only way to do randomness? Are their other ways?</p>\n', 'ViewCount': '39', 'Title': 'Speaking of "randomness" in computing terms, to what sense can any extant digital processor make "random" results?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-03T07:33:53.753', 'LastEditDate': '2013-07-03T07:33:53.753', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8963', 'Tags': '<randomized-algorithms><randomness>', 'CreationDate': '2013-07-01T21:46:50.027', 'Id': '13021'},11835:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>During my involvement in a course on dealing with NP-hard problems I have encountered the PCP theorem, stating</p>\n\n<p>$\\qquad\\displaystyle \\mathsf{NP} = \\mathsf{PCP}(\\log n, 1)$. </p>\n\n<p>I understand the technical definition of a PCP verifier, so I know in principle what kind of algorithm has to exist for every NP problem: a randomised algorithm that checks $O(1)$ bits of the given certificate for the given input using $O(\\log n)$ random bits, so that this algorithm is essentially a one-sided error Monte-Carlo verifier.</p>\n\n<p>However, I have trouble imagining how such an algorithm can deal with an NP-complete problem. Short of reading the proof of the PCP theorem, are there concrete examples for such algorithms?</p>\n\n<p>I skimmed the relevant sections of <a href="http://www.cs.princeton.edu/theory/complexity/" rel="nofollow">Computational Complexity: A Modern Approach</a> by Arora and Barak (2009) but did not find any.</p>\n\n<p>An example using a $\\mathsf{PCP}(\\_,\\ll n)$ algorithm would be fine.</p>\n', 'ViewCount': '220', 'Title': 'Example for a non-trivial PCP verifier for an NP-complete problem', 'LastActivityDate': '2013-07-19T09:24:04.750', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><complexity-theory><np-complete><approximation><randomized-algorithms>', 'CreationDate': '2013-07-12T11:10:36.380', 'Id': '13246'},11836:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>does $BPP\\subseteq P^{NP}$ ? it seems reasonable but I don't know if there is a proof of this!could any one post a proof or any material that discusses the statement or something that look like this .  </p>\n", 'ViewCount': '54', 'Title': 'BPP upper bound', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-17T05:48:37.233', 'LastEditDate': '2013-07-17T05:48:37.233', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13301', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8570', 'Tags': '<complexity-theory><complexity-classes><randomized-algorithms><oracle-machines>', 'CreationDate': '2013-07-16T12:50:19.600', 'Id': '13300'},11837:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose I want to write a classical software simulator of a quantum circuit with $N$ qubits.  When it comes time to simulate the quantum Fourier transform I can evaluate all $2^N$ states to determine the probability amplitudes, and then perform a Fast-Fourier Transform on the probability amplitudes in time $o(N 2^N)$.  Finally in $o(2^N)$ time I can generate a scan of the partial sums of the probabilities of all the result states.  Then I can choose a random number in the range $[0,1]$ and use it to do a binary search of the partial sums.</p>\n\n<p>This results in a simulator that, each time is run outputs a single $N$ bit binary number with the probability distribution predicted by theory.</p>\n\n<p>Can I do better?  Of course I can\'t do exponentially better in general, but perhaps I could reduce the time to simulate a single experiment to $o(2^N)$?</p>\n\n<p>I can do significantly better under some circumstances.  For example, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.7607">Gilbert, Guha, Indyk, Muthukrishnan, and Strauss, "Near-Optimal Sparse Fourier Representations via Sampling", <em>ACM Symp Theory Comp</em>, STOC-44:152-161, 2002,</a> seems to indicate that if there are only $B$ frequencies (or if the $B$ frequencies make up "most" of the power of the signal) then there is a randomized algorithm that will recover all of them (and their amplitudes) in time, space and number of samples polynomial in $B$ and $N$.</p>\n\n<p>I guess I\'m hoping for something like that, but only to get one frequency, and to have some guarantee that the probability of getting a particular frequency will be proportional to the amplitude of its coefficient.</p>\n', 'ViewCount': '61', 'Title': '(Slightly) faster simulation of quantum Fourier transform', 'LastActivityDate': '2013-07-18T03:50:14.887', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7459', 'Tags': '<randomized-algorithms><quantum-computing><fourier-transform>', 'CreationDate': '2013-07-18T03:50:14.887', 'FavoriteCount': '1', 'Id': '13325'},11838:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m studying randomized algorithms and I sometimes come across results like</p>\n\n<blockquote>\n  <p>(1) The algorithm has an expected $O(f(n))$ cost.</p>\n</blockquote>\n\n<p>and</p>\n\n<blockquote>\n  <p>(2) With constant probability, the cost is bounded by $O(f(n))$.</p>\n</blockquote>\n\n<p>I\'m perfectly fine with statements like (2), but I\'m puzzled to what extent a statement like (1) is useful: \nFor certain probability distributions of a random variable, the expected value itself occurs with less than constant probability; for other distributions, it occurs with $1-1/n$ probability. Of course, in many cases, (1) is extended via concentration bounds to show high probability, but in cases where this isn\'t done, it doesn\'t seem that a statement on the "expected cost" lets us derive any implications on the actual performance of the algorithm, right?</p>\n', 'ViewCount': '88', 'Title': 'Interpretation of "expected cost" of an algorithm', 'LastEditorUserId': '9398', 'LastActivityDate': '2013-07-29T15:43:02.297', 'LastEditDate': '2013-07-29T03:06:35.717', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9398', 'Tags': '<probability-theory><randomized-algorithms>', 'CreationDate': '2013-07-29T01:58:55.503', 'Id': '13484'},11839:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>While reading a cryptography textbook, i find the definition of a function that is hard on the average.(More precisely, it is 'hard on the average but easy with auxiliary input', but i omit latter for simplicity.)</p>\n\n<blockquote>\n  <p><strong>Definition : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>there exists</strong> a probabilistic polynomial-time algorithm $G$ such that<br>\n  for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>My question is why the statement of the existence of qualified algorithm G is sufficient? </p>\n\n<p>In other words, why the above definition gives a formal definition of 'hardness on the average' instead of following definition, which is more intuitive(?) to understand and more strict.\nWhy is the above definition sufficient? </p>\n\n<p>( Now I'm thinking that problem might occur when $G$ has only polynomial number of possible outputs, but if so, let's replace 'for any $G$' with 'for any $G$ which have exponentially many possible outputs' in following definition.)</p>\n\n<blockquote>\n  <p><strong>(strong?) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>for any</strong> probabilistic polynomial-time algorithm $G$ and for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>Another question is that whether a following simpler definition is equivalent to original definition or not?</p>\n\n<blockquote>\n  <p><strong>(simple) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(U_n)=h(U_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $U_n$ is a random variable uniformly distributed over $\\{0,1\\}^n$.</p>\n</blockquote>\n", 'ViewCount': '64', 'Title': "Completeness of formal definition of 'hardness on the average'", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-09T07:02:13.240', 'LastEditDate': '2013-08-09T07:02:13.240', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13678', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9565', 'Tags': '<complexity-theory><terminology><cryptography><randomized-algorithms><average-case>', 'CreationDate': '2013-08-08T12:43:21.820', 'Id': '13674'},11840:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A shuffling algorithm is supposed to generate a random permutation of a given finite set. So, for a set of size $n$, a shuffling algorithm should return any of the $n!$ permutations of the set uniformly at random. </p>\n\n<p>Also, conceptually, a randomized algorithm can be viewed as a deterministic algorithm of the input and some random seed. Let $S$ be any shuffling algorithm. On input $X$ of size $n$, its output is a function of the randomness it has read. To produce $n!$ different outputs, $S$ must have read at least $\\log(n!) = \\Omega(n \\log n)$ bits of randomness. Hence, any shuffling algorithm must take $\\Omega(n \\log n)$ time.</p>\n\n<p>On the other hand, the <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle" rel="nofollow">Fisher-Yates shuffle</a> is <a href="http://www.codinghorror.com/blog/2007/12/the-danger-of-naivete.html" rel="nofollow">widely</a> <a href="http://c2.com/cgi/wiki?LinearShuffle" rel="nofollow">believed</a> to run in $O(n)$ time. Is there something wrong with my argument? If not, why is this belief so widespread?</p>\n', 'ViewCount': '161', 'Title': 'How can you shuffle in $O(n)$ time if you need $\\Omega(n \\log n)$ random bits?', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-08-29T08:52:52.413', 'LastEditDate': '2013-08-28T18:42:00.367', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9847', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><randomized-algorithms>', 'CreationDate': '2013-08-28T07:42:37.830', 'Id': '13990'},11841:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In the last section of chapter 3 (page 54) in <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em> by Mitzenmacher and Upfal, a randomized algorithm is discussed for finding the median of a set $S$ of distinct elements in $O(n)$ time. As a substep the algorithm requires sorting a multiset $R$ of values sampled from $S$, where $|R| = \\lceil n^{3/4} \\rceil$. I'm sure this is a fairly simple, and straight forward question compared to the others on this site, but how is sorting a multiset of this size bounded by $O(n)$? </p>\n\n<p>I assume the algorithm is using a typical comparison-based, deterministic sorting algorithm that runs in $O(n\\log$ $n)$. So, If I have the expression $O(n^{3/4}\\log(n^{3/4}))$, isn't this just equivalent to $O(\\frac{3}{4}n^{3/4}\\log(n)) = O(n^{3/4}\\log(n))$? How does the reduction down to $O(n)$ proceed?</p>\n", 'ViewCount': '107', 'Title': 'Randomized Median Element Algorithm in Mitzenmacher and Upfal: O(n) sorting step?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:46:22.310', 'LastEditDate': '2013-09-16T07:46:22.310', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10128', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><randomized-algorithms>', 'CreationDate': '2013-09-13T20:07:11.977', 'Id': '14296'},11842:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know how to use the FFT for multiplying two equations in $O(n\\,log\\,n)$ time, but is there a way to use FFT to compute the expanded equation before simplifying?</p>\n\n<p>For example, if you are multiplying $A(x) = 1 + 2x + 6x^2$ and $B(x) = 4 + 5x + 3x^2$ such that you get $C(x) = A(x) \\cdot B(x) = 4 + 5x + 3x^2 + 8x + 10x^2 + 6x^3 + 24x^2 + 30x^3 + 18x^4$ without going directly to the simplified answer?</p>\n\n<p>Furthermore, is it possible to use FFT to do this expanded form multiplication in $O(n\\,log\\,n)$ time? If so, can you show me how to apply FFT to this scenario?</p>\n', 'ViewCount': '76', 'Title': 'FFT for expanded form of equation multiplication', 'LastEditorUserId': '10052', 'LastActivityDate': '2014-03-31T23:46:56.920', 'LastEditDate': '2013-09-22T04:25:36.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14510', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10052', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><fourier-transform>', 'CreationDate': '2013-09-22T04:17:41.320', 'Id': '14509'},11843:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>C++11 has a convenient Bernoulli RNG, illustrated at \n<a href="http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution" rel="nofollow">http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution</a> .\nHowever, distilling an entire random integer into a single random bit seems inefficient when the expectation parameter $p$ is rational with a small or power-of-two denominator.\nIs there a reasonably fast way to generate 32 random Bernoulli bits at once in such cases? My application uses long streams of bits, so I can keep track of statistics if needed (but this would consume runtime).</p>\n', 'ViewCount': '78', 'Title': "Isn't std::bernoulli_distribution inefficient? Designing a bit-parallel Bernoulli generator", 'LastEditorUserId': '5189', 'LastActivityDate': '2013-09-25T04:26:33.097', 'LastEditDate': '2013-09-25T04:26:33.097', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5189', 'Tags': '<randomized-algorithms><integers><randomness><binary-arithmetic>', 'CreationDate': '2013-09-22T21:10:38.360', 'Id': '14525'},11844:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve got 30 elements which has to be  grouped/sorted into 10 ordered 3-tuple. \nThere are several rules and constraints about grouping/sorting.\nFor example: Element $A$ must not be in the same tuple same unit $B$. \nElement $C$ must not be right in front of element $A$, etc.</p>\n\n<p>I am searching for an approximated algorithm:</p>\n\n<ol>\n<li>We don\'t need to achieve the exact optimum </li>\n<li>It is OK for some rules not to be satisfied, if it helps to fulfill more rules.</li>\n</ol>\n\n<p>Do you know of any algorithm/proceeding that solve this problem or a similar one?\nI fear to solve it in an optimal way, you have to try out every possible solution-> $2 ^ {30}$</p>\n\n<p><strong>EDIT</strong>: Sorry for the bad explanation. I am trying to make it a bit clearer:\nI got 30 elements for example: $\\{1,2,3,\\ldots,30\\}$.\nI need to group them into 3-tuples so that i get something like: $(1,2,3)$, $(4,5,6)$,$\\ldots$,$(28,29,30)$.</p>\n\n<p>There are several constraints. For example: </p>\n\n<ul>\n<li>1 cannot precede 2 in an ordered tuple, so, for instance  $(1,2,3)$ is not a valid tuple.</li>\n<li>5 must be together with 4. </li>\n</ul>\n\n<p>Those constraints can be broken and its possible that there is no solution where all rules can be fulfilled. <br />An solution is considered as good if the amount of rules broken is "low".</p>\n\n<p>Hope that makes it clearer and thanks for the help so far.</p>\n', 'ViewCount': '81', 'Title': 'Algorithm for sorting with constraints', 'LastEditorUserId': '157', 'LastActivityDate': '2013-12-16T01:48:30.027', 'LastEditDate': '2013-12-16T00:37:15.273', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '18345', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11601', 'Tags': '<algorithms><sorting><randomized-algorithms><greedy-algorithms>', 'CreationDate': '2013-11-25T00:14:58.160', 'Id': '18312'},11845:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '110', 'Title': 'Chernoff bounds and Monte Carlo algorithms', 'LastEditDate': '2013-11-25T14:30:48.973', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'FavoriteCount': '1', 'Body': "<p>One of Wikipedia examples of use of Chernoff bounds is the one where an algorithm $A$ computes the correct value of function $f$ with probability $p &gt; 1/2$. Basically, Chernoff bounds are used to bound the error probability of $A$ using repeated calls to $A$ and majority agreement.</p>\n\n<p>I don't understand how, to be frank. It would be nice if somebody could break it down piece by piece. Moreover, does it matter whether $A$ is a decision algorithm or can return more values? How are Chernoff bounds in general used for Monte Carlo algorithms?</p>\n", 'Tags': '<randomized-algorithms><randomness><chernoff-bounds>', 'LastEditorUserId': '8508', 'LastActivityDate': '2013-11-25T19:22:19.073', 'CommentCount': '2', 'AcceptedAnswerId': '18329', 'CreationDate': '2013-11-25T06:16:42.047', 'Id': '18321'},11846:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '127', 'Title': 'NP-complete decision problems - how close can we come to a solution?', 'LastEditDate': '2014-01-02T15:28:30.023', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '2', 'Body': '<p>After we prove that a certain <strong>optimization</strong> problem is NP-hard, the natural next step is to look for a polynomial algorithm that comes close to the optimal solution - preferrably with a constant approximation factor.</p>\n\n<p>After we prove that a certain <strong>decision</strong> problem is NP-complete, what is the natural next step? Obviously we cannot "approximate" a boolean value...</p>\n\n<p>My guess is that, the next step is to look for a randomized algorithm that returns the correct solution with a high probability. Is this correct?</p>\n\n<p>If so, what probability of being correct can we expect to get from such a randomized algorithm?</p>\n\n<p>As far as I understand from Wikipedia, <a href="https://en.wikipedia.org/wiki/PP_%28complexity%29" rel="nofollow">PP contains NP</a>. This means that, if the problem is in NP, it should be easy to write an algorithm that is correct more than $0.5$ of the times.</p>\n\n<p>However, <a href="https://en.wikipedia.org/wiki/Bounded-error_probabilistic_polynomial" rel="nofollow">it is not known whether BPP contains NP</a>. This means that, it may be difficult (if not impossible) to write an algorithm that is correct more than $0.5+\\epsilon$ of the times, for every positive $\\epsilon$ independent of the size of input.</p>\n\n<p>Did I understand correctly?</p>\n', 'Tags': '<np-complete><approximation><randomized-algorithms>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-01-03T01:18:29.533', 'CommentCount': '2', 'AcceptedAnswerId': '19419', 'CreationDate': '2013-12-31T16:03:52.760', 'Id': '19412'},11847:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was trying to understand better the definition of a strong PSRG and I came across this expression which I am trying to understand better:</p>\n\n<p>$$ Pr_{r \\in \\{0,1\\}^l}[A(r) = \\text{"yes"}]$$</p>\n\n<p>Where r is a truly random bit string and A is a polynomial time deterministic machine.\nI\'ve been having some problems understanding what this expression means conceptually (or intuitively). </p>\n\n<p>So far these are some of my thoughts and I will try to point out my doubts too.</p>\n\n<p>A is just a standard TM so we can image that on l steps, it will yield $2^l$ branches. Each branch has a chance of occurring depending on which r occurs. Therefore, I was wondering if the above probability expression just mean "the fraction of branches that out yes"? Is that basically the same as the chance that A will output yes on the given random bit string? The thing that was confusing me and I was not sure how to deal with it was that, A(r) always outputs the same thing ("yes" or "no") on a given r (say it always accepts or rejects if r = 1010100 or something), it didn\'t seem to me that it a probabilistic sense, unless we randomly choose r. So I was wondering how the community interpreted this equation and what it mean.</p>\n\n<p>Also, since this is a probability, it seems to me that A(r) is just r.v. that only takes two values (yes or no), right? So this distribution only has two probability values, the one that A outputs yes or no, right? I was wondering how that related to the string r and I was not sure how to resolve this.</p>\n', 'ViewCount': '32', 'Title': 'Interpreting probabilistic time turning machines', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-01-24T19:23:21.040', 'LastEditDate': '2014-01-24T19:23:21.040', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19943', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<terminology><probability-theory><randomized-algorithms><randomness>', 'CreationDate': '2014-01-24T04:54:38.433', 'Id': '19932'},11848:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have tried hard , but i'm unable to come up with the expected running time for the number of comparisons to find the Randomized Median (find the median of an unsorted array). \nAlso i wanted to make sure that we CANNOT take expectation of the recurrence that we use to find the randomized mean , or any other recurrence in any other problem as they belong to different probability spaces? Is this statement right?</p>\n", 'ViewCount': '32', 'ClosedDate': '2014-01-29T17:01:16.563', 'Title': 'Randomised Median', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T17:01:04.093', 'LastEditDate': '2014-01-29T17:01:04.093', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13212', 'Tags': '<algorithms><randomized-algorithms>', 'CreationDate': '2014-01-29T05:10:11.183', 'Id': '20055'},11849:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am somewhat confused with the running time analysis of a program here which has recursive calls which depend on a RNG.  (Randomly Generated Number)</p>\n\n<p>Let\'s begin with the pseudo-code, and then I will go into what I have thought about so far related to this one.</p>\n\n<pre><code>    Func1(A, i, j)\n    /* A is an array of at least j integers */\n\n 1  if (i \u2265 j) then return (0);\n 2  n \u2190 j \u2212 i + 1 ; /* n = number of elements from i to j */\n 3  k \u2190 Random(n);\n 4  s \u2190 0; //Takes time of Arbitrary C\n\n 5  for r \u2190 i to j do\n 6      A[r] \u2190 A[r] \u2212 A[i] \u2212 A[j]; //Arbitrary C\n 7      s \u2190 s + A[r]; //Arbitrary C\n 8  end\n\n 9  s \u2190 s + Func1(A, i, i+k-1); //Recursive Call 1\n10  s \u2190 s + Func1(A, i+k, j); //Recursive Call 2\n11  return (s);\n</code></pre>\n\n<p>Okay, now let\'s get into the math I have tried so far.  I\'ll try not to be too pedantic here as it is just a rough, estimated analysis of expected run time.  </p>\n\n<p>First, let\'s consider the worst case.  Note that the K = Random(n) must be at least 1, and at most n.  Therefore, the worst case is the K = 1 is picked.  This causes the total running time to be equal to T(n) = cn + T(1) + T(n-1).  Which means that overall it takes somewhere around cn^2 time total (you can use Wolfram to solve recurrence relations if you are stuck or rusty on recurrence relations, although this one is a fairly simple one).  </p>\n\n<p>Now, here is where I get somewhat confused.  For the expected running time, we have to base our assumption off of the probability of the random number K.  Therefore, we have to sum all the possible running times for different values of k, plus their individual probability.  By lemma/hopefully intuitive logic: the probability of any one Randomly Generated k, with k between 1 to n, is equal 1/n.  </p>\n\n<p><strong>Therefore, (in my opinion/analysis) the expected run time is:</strong></p>\n\n<p><strong>ET(n) = cn + (1/n)*Summation(from k=1 to n-1) of (ET(k-1) + ET(n-k))</strong></p>\n\n<p>Let me explain a bit.  The cn is simply for the loop which runs i to j.  This is estimated by cn.  The summation represents all of the possible values for k.  The (1/n) multiplied by this summation is there because the probability of any one k is (1/n).  <strong>The terms inside the summation represent the running times of the recursive calls of Func1.</strong>  The first term on the left takes ET(k-1) because this recursive call is going to do a loop from i to k-1 (which is roughly ck), and then possibly call Func1 again.  The second is a representation of the second recursive call, which would loop from i+k to j, which is also represented by n-k.</p>\n\n<p><strong>Upon expansion of the summation, we see that the overall function ET(n)  is of the order n^2.</strong>  <em>However</em>, as a test case, plugging in k=(n/2) gives a total running time for Func 1 of roughly nlog(n).  <em>This</em> is why I am confused.  How can this be, if the estimated running time is of the order n^2?  Am I considering a "good" case by plugging in n/2 for k?  Or am I thinking about k in the wrong sense in some way?  </p>\n', 'ViewCount': '66', 'Title': 'Algorithm Analysis: Expected Running Time of Recursive Function Based on a RNG', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-17T19:50:53.527', 'LastEditDate': '2014-02-12T09:12:25.723', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14596', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><average-case>', 'CreationDate': '2014-02-12T05:31:29.490', 'Id': '21558'},11850:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '58', 'Title': 'Advantage of the Monte Carlo method over a regular periodic sampling', 'LastEditDate': '2014-02-27T10:10:16.087', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15001', 'Body': "<p>I am unclear on when to use the Monte Carlo random sampling method for algorithm design. The classic example that I keep seeing is using random points within some bounding rectangle to determine the area of some irregular figure. Wouldn't a regular periodic sampling provide more repeatable results for this application then using the Monte Carlo (random sampling) method?</p>\n", 'ClosedDate': '2014-02-27T07:44:07.457', 'Tags': '<algorithms><randomized-algorithms><monte-carlo>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-27T10:10:16.087', 'CommentCount': '1', 'AcceptedAnswerId': '22004', 'CreationDate': '2014-02-24T20:37:44.313', 'Id': '22002'},11851:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '126', 'Title': 'Selecting random points at general position', 'LastEditDate': '2014-03-01T22:33:17.750', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '1', 'Body': '<p>How will you find a random collection of $n$ points in the plane, all with integer coordinates in a specified range (e.g. -1000 to 1000), such that no 3 of them are on the same line?</p>\n\n<p>The following algorithm eventually works, but seems highly inefficient:</p>\n\n<ol>\n<li>Select $n$ points at random.</li>\n<li>Check all $O(n^3)$ triples of points. If any of them are on the same line, then discard one of the points, select an alternative point at random, and go back to 2.</li>\n</ol>\n\n<p>Is there a more efficient algorithm?</p>\n', 'Tags': '<algorithms><computational-geometry><randomized-algorithms>', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-03-01T22:50:12.840', 'CommentCount': '1', 'AcceptedAnswerId': '22171', 'CreationDate': '2014-03-01T20:49:34.203', 'Id': '22167'},11852:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm wondering why the following argument doesn't work for showing that the existence of a Las Vegas algorithm also implies the existence of a deterministic algorithm:</p>\n\n<p>Suppose that there is a Las Vegas algorithm $A$ that solves some graph problem $P$, i.e., $A$ takes an $n$-node input graph $G$ as input (I'm assuming the number of edges is $\\le n$) and eventually yields a correct output, while terminating within time $T(G)$ with some nonzero probability.</p>\n\n<p>Suppose that there is no deterministic algorithm that solves $P$. Let $A^\\rho$ be the deterministic algorithm that is given by running the Las Vegas algorithm $A$ with a fixed bit string $\\rho$ as its random string. \nLet $k=k(n)$ be the number of $n$-node input graphs (with $\\le n$ edges).\nSince there is no deterministic algorithm for $P$, it follows that, for any $\\rho$, the deterministic algorithm $A^\\rho$ fails on at least one of the $k$ input graphs. Returning to the Las Vegas algorithm $A$, this means that $A$ has a probability of failure of $\\ge 1/k$, a contradiction to $A$ being Las Vegas. </p>\n", 'ViewCount': '194', 'Title': 'Relationship between Las Vegas algorithms and deterministic algorithms', 'LastActivityDate': '2014-03-10T21:34:04.607', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15471', 'Tags': '<algorithms><complexity-theory><randomized-algorithms>', 'CreationDate': '2014-03-10T02:22:57.270', 'Id': '22448'},11853:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '360', 'Title': 'Is this method really uniformly random?', 'LastEditDate': '2014-03-25T22:35:47.140', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16107', 'FavoriteCount': '2', 'Body': "<p>I have a list and want to select a random item from the list.</p>\n\n<p>An algorithm which is said to be random:</p>\n\n<blockquote>\n  <p>When you see the first item in the list, you set it as the selected item.</p>\n  \n  <p>When you see the second item, you pick a random integer in the range [1,2]. If it's 1, then the new item becomes the selected item. Otherwise you skip that item.</p>\n  \n  <p>For each item you see, you increase the count, and with probability 1/count, you select it. So at the 101st item, you pick a random integer in the range [1,101]. If it's 100, that item is the new selected node.</p>\n</blockquote>\n\n<p>Is it really uniformly random?</p>\n\n<p><strong>My thoughts are:</strong></p>\n\n<p>As the number of nodes increases, the probability for them being selected \ndecreases, so the best chance of selection is for items 1, 2, 3, ..., not for 20, 21, ..., 101.</p>\n\n<p>Each node will not have equal probability of being selected.</p>\n\n<p>Please clarify, as I have trouble understanding this.</p>\n", 'Tags': '<algorithms><probability-theory><randomized-algorithms><sampling>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-25T22:35:47.140', 'CommentCount': '5', 'AcceptedAnswerId': '23041', 'CreationDate': '2014-03-25T17:55:00.713', 'Id': '23038'},11854:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's say I can solve problem $A$ in polynomial time using only $\\log n$ bits of randomness, with a $\\ge \\frac{2}{3}$ chance of a correct answer.  Then surely I can solve $A$ determistically by running my algorithm for $A$ over all random strings of length $\\log n$ (of which there are a polynomial number) and take a popular vote of the outcomes.</p>\n\n<p>I don't understand, then, why we would ever talk about $O(\\log n)$ amounts of randomness in complexity classes that are closed under polynomial factors.  More specifically, the PCP theorem says $NP = PCP[O(\\log n), O(1)]$ - why isn't that the same as $PCP[0, O(1)]$?</p>\n", 'ViewCount': '25', 'Title': "Why can't we derandomize the PCP theorem by iterating over all possible $\\log n$ random strings?", 'LastActivityDate': '2014-03-26T23:35:06.183', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23107', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16172', 'Tags': '<randomized-algorithms>', 'CreationDate': '2014-03-26T23:13:50.927', 'Id': '23104'},11855:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A town has $N$ people.\nAt Day 0, a person has a secret. At Day 1, he calls a random person and tells him the secret. At Day 2, every person who knows the secret calls a person at random to tell the secret. In this way secret propagates.</p>\n\n<p>Let X be the number of days till everybody knows the secret. What will be the Expectation of X? If $X_i$ be the number of people who know the secret at end of day $i$, what will be the Expectation of $X_i$?</p>\n\n<p>$Z$ = min{$k$|$X_k = N$}. Then what will be the bound of $Z$? What is the expected number of phone calls required so that $X_i = N$?</p>\n', 'ViewCount': '20', 'ClosedDate': '2014-04-07T13:54:25.063', 'Title': 'Expected time taken to spread message in gossip-based protocol', 'LastActivityDate': '2014-04-07T11:47:03.013', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'user3505352', 'PostTypeId': '1', 'OwnerUserId': '16533', 'Tags': '<randomized-algorithms>', 'CreationDate': '2014-04-07T05:21:23.793', 'Id': '23507'},11856:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In section 9.2 of CLRS (<em>Introduction to Algorithms; page 185 in the 2nd edition and page 215 in the 3rd edition</em>), a randomized selection algorithm is presented. </p>\n\n<p>For its analysis, $T(n)$ is a random variable denoting the time required on an input array $A[p \\cdots r]$ of $n$ elements and $X_k$ is an indicator random variable $X_k = I \\{ \\text{the subarray } A[p \\cdots q] \\text{ has exactly } k \\text{ elements (due to the pivot)} \\}$. </p>\n\n<p>It has been claimed that $X_k$ and $T(\\max(k-1, n-k))$ are independent (page 187 in the 2nd edition and page 218 in the 3rd edition). However, I find it quite counter-intuitive to understand. How to verify it?</p>\n', 'ViewCount': '26', 'Title': 'Why are the two random variables independent in the analysis of Randomized Selection algorithm in CLRS?', 'LastActivityDate': '2014-04-15T17:13:42.643', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23814', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithm-analysis><randomized-algorithms>', 'CreationDate': '2014-04-15T13:18:26.367', 'Id': '23811'},11857:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose we have $n$ balls and $n$ bins. We put the balls into the bins randomly. If we count the maximum number of balls in any bin, the expected value of this is  $\\Theta(\\ln n/\\ln\\ln n)$. How can we derive this fact? Are Chernoff bounds helpful?</p>\n', 'ViewCount': '63', 'ClosedDate': '2014-04-23T16:47:14.277', 'Title': 'Expected maximum bin load, for balls in bins with equal number of balls and bins', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-19T00:38:01.780', 'LastEditDate': '2014-04-19T00:38:01.780', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15406', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><probabilistic-algorithms><chernoff-bounds>', 'CreationDate': '2014-04-18T22:43:53.940', 'Id': '23925'}