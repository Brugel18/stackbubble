680:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '867', 'Title': 'Why polynomial time is called "efficient"?', 'LastEditDate': '2012-04-22T16:34:25.370', 'AnswerCount': '3', 'Score': '25', 'PostTypeId': '1', 'OwnerUserId': '157', 'FavoriteCount': '5', 'Body': "<p>Why in computer science any complexity which is at most polynomial is considered efficient?</p>\n\n<p>For any practical application<sup>(a)</sup>, algorithms with complexity $n^{\\log n}$ are way faster than algorithms that run in time, say, $n^{80}$, but the first is considered inefficient while the latter is efficient. Where's the logic?!</p>\n\n<p><sup>(a) Assume, for instance, the number of atoms in the universe is approximately $10^{80}$.</sup></p>\n", 'Tags': '<algorithms><complexity-theory><terminology><efficiency>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-04T19:18:03.357', 'CommentCount': '10', 'AcceptedAnswerId': '215', 'CreationDate': '2012-03-10T20:49:58.150', 'Id': '210'},681:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I came across this figure which shows that context-free and regular languages are (proper) subsets of efficient problems (supposedly $\\mathrm{P}$). I perfectly understand that efficient problems are a subset of all decidable problems because we can solve them but it could take a very long time. </p>\n\n<p>Why are <em>all</em> context-free and regular languages efficiently decidable? Does it mean solving them will not take a long time (I mean we know it without more context)?</p>\n\n<p><img src="http://i.stack.imgur.com/xdEBQ.jpg" alt="enter image description here"></p>\n', 'ViewCount': '1486', 'Title': 'Are all context-free and regular languages efficiently decidable?', 'LastEditorUserId': '51', 'LastActivityDate': '2012-03-13T17:33:24.600', 'LastEditDate': '2012-03-13T17:33:24.600', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '51', 'Tags': '<formal-languages><regular-languages><context-free><efficiency>', 'CreationDate': '2012-03-13T15:59:26.123', 'Id': '315'},682:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>A polynomial-time Turing machine algorithm is considered efficient if its run-time, in the worst-case, is bounded by a polynomial function in the input size. I'm aware of the strong Church-Turing thesis:</p>\n\n<blockquote>\n  <p>Any reasonable model of computation can be efficiently simulated on Turing machines</p>\n</blockquote>\n\n<p>However, I'm not aware of solid theory for analyzing the computational complexity of algorithms of $\\lambda$-calculus.</p>\n\n<p>Do we have a notion of computational efficiency for every known model of computation? Are there any models that are only useful for computability questions but useless for computational complexity questions?</p>\n", 'ViewCount': '301', 'Title': 'Notions of efficient computation', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-02T03:50:01.950', 'LastEditDate': '2012-04-02T03:50:01.950', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '96', 'Tags': '<complexity-theory><efficiency><computation-models>', 'CreationDate': '2012-03-20T15:52:42.367', 'FavoriteCount': '1', 'Id': '540'},683:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '6955', 'Title': 'Adding elements to a sorted array', 'LastEditDate': '2012-04-01T07:07:56.053', 'AnswerCount': '5', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '1', 'Body': '<p>What would be the fastest way of doing this (from an algorithmic perspective, as well as a practical matter)?</p>\n\n<p>I was thinking something along the following lines.</p>\n\n<p>I could add to the end of an array and then use bubblesort as it has a best case (totally sorted array at start) that is close to this, and has linear running time (in the best case).</p>\n\n<p>On the other hand, if I know that I start out with a sorted array, I can use a binary search to find out the insertion point for a given element.</p>\n\n<p>My hunch is that the second way is nearly optimal, but curious to see what is out there.</p>\n\n<p>How can this best be done?</p>\n', 'Tags': '<algorithms><efficiency><arrays><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-06T17:17:29.770', 'CommentCount': '3', 'AcceptedAnswerId': '931', 'CreationDate': '2012-04-01T01:49:35.277', 'Id': '930'},684:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '3560', 'Title': 'What is most efficient for GCD?', 'LastEditDate': '2012-12-01T00:11:19.023', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '1', 'Body': "<p>I know that Euclid's algorithm is the best algorithm for get the GCD (great common divisor) for a list the positive integer numbers. \nBut, in the practice, you can write two codes por evaluate the gcd (for my case, i decided use java, but c/c++ may be another option).</p>\n\n<p>I need to get the most efficient code of two possibilities form to programming. </p>\n\n<p>Recursive Mode, you can write...</p>\n\n<pre><code>static long gcd(long a, long b){\n    a = Math.abs(a); b = Math.abs(b);\n    return (a==0)?b:gcd(b, a%b);\n  }\n</code></pre>\n\n<p>And, iterative mode, looks like ...</p>\n\n<pre><code>static long gcd(long a, long b) {\n  long r, i;\n  while(b!=0){\n    r = a % b;\n    a = b;\n    b = r;\n  }\n  return a;\n}\n</code></pre>\n\n<p>Regards, </p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>We can do that with the Binary GCD, and the easy code is like that</p>\n\n<pre><code>int gcd(int a, int b)\n{\n    while(b) b ^= a ^= b ^= a %= b;\n    return a;\n}\n</code></pre>\n\n<p>Great Discussion.</p>\n", 'Tags': '<algorithms><efficiency>', 'LastEditorUserId': '1152', 'LastActivityDate': '2013-12-31T12:09:28.093', 'CommentCount': '7', 'AcceptedAnswerId': '1449', 'CreationDate': '2012-04-22T18:18:25.867', 'Id': '1447'},685:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '665', 'Title': 'Dealing with intractability: NP-complete problems', 'LastEditDate': '2013-06-06T14:11:05.583', 'AnswerCount': '6', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '1219', 'FavoriteCount': '8', 'Body': '<p>Assume that I am a programmer and I have an NP-complete problem that I need to solve it. What methods are available to deal with NPC problems? Is there a survey or something similar on this topic?</p>\n', 'Tags': '<algorithms><reference-request><np-complete><efficiency><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-06T14:11:05.583', 'CommentCount': '4', 'AcceptedAnswerId': '1481', 'CreationDate': '2012-04-24T03:28:23.417', 'Id': '1477'},686:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Do you know any algorithm that calculates the factorial after modulus efficiently?</p>\n\n<p>For example, I want to program:</p>\n\n<pre><code>for(i=0; i&lt;5; i++)\n  sum += factorial(p-i) % p;\n</code></pre>\n\n<p>But, <code>p</code> is a big number (prime) for applying factorial directly $(p \\leq 10^ 8)$.</p>\n\n<p>In Python, this task is really easy, but i really want to know how to optimize.</p>\n', 'ViewCount': '2032', 'Title': 'What is the most efficient way to compute factorials modulo a prime?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-15T22:02:22.787', 'LastEditDate': '2012-10-06T22:43:33.660', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><efficiency><integers>', 'CreationDate': '2012-04-25T03:24:45.137', 'FavoriteCount': '6', 'Id': '1495'},687:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Dynamic programming can reduce the time needed to perform a recursive algorithm. I know that dynamic programming can help reduce the time complexity of algorithms. Are the general conditions such that if satisfied by a recursive algorithm would imply that using dynamic programming will reduce the time complexity of the algorithm? When should I use dynamic programming?</p>\n', 'ViewCount': '1373', 'Title': 'When can I use dynamic programming to reduce the time complexity of my recursive algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-25T11:10:59.523', 'LastEditDate': '2012-05-25T11:10:59.523', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1634', 'Tags': '<algorithms><dynamic-programming><efficiency><algorithm-design>', 'CreationDate': '2012-05-24T22:26:27.710', 'FavoriteCount': '1', 'Id': '2057'},688:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m looking for a data structure that supports efficient approximate lookups of keys (e.g., Levenshtein distance for strings), returning the closest possible match for the input key. The best suited data structure I\'ve found so far are <a href="http://en.wikipedia.org/wiki/BK-tree">Burkhard-Keller trees</a>, but I was wondering if there are other/better data structures for this purpose.</p>\n\n<p>Edit:\nSome more details of my specific case:</p>\n\n<ul>\n<li>Strings usually have a fairly large Levenshtein difference from each other.</li>\n<li>Strings have a max length of around 20-30 chars, with an average closer to 10-12.</li>\n<li>I\'m more interested in efficient lookup than insertion as I will be building a set of mostly static data that I want to query efficiently.</li>\n</ul>\n', 'ViewCount': '787', 'Title': 'Efficient map data structure supporting approximate lookup', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-27T17:48:11.330', 'LastEditDate': '2012-05-26T14:41:45.563', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '1658', 'Tags': '<data-structures><strings><efficiency>', 'CreationDate': '2012-05-26T13:11:38.040', 'FavoriteCount': '4', 'Id': '2093'},689:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '276', 'Title': 'Generating number of possibilites of popping two stacks to two other stacks', 'LastEditDate': '2012-06-13T15:33:40.447', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1784', 'FavoriteCount': '1', 'Body': '<p>Context: I\'m working on <a href="http://stackoverflow.com/questions/10875675/how-to-find-out-all-the-popping-out-possibilities-of-two-stacks">this problem</a>:</p>\n\n<blockquote>\n  <p>There are two stacks here:</p>\n\n<pre><code>A: 1,2,3,4 &lt;- Stack Top\n  B: 5,6,7,8\n</code></pre>\n  \n  <p>A and B will pop out to other two stacks: C and D. For example: </p>\n\n<pre><code> pop(A),push(C),pop(B),push(D).\n</code></pre>\n  \n  <p>If an item have been popped out , it must be pushed to C or D immediately.</p>\n</blockquote>\n\n<p>The goal is to enumerate all possible stack contents of C and D after moving all elements.</p>\n\n<p>More elaborately, the problem is this: If you have two source stacks with $n$ unique elements (all are unique, not just per stack) and two destination stacks and you pop everything off each source stack to each destination stack, generate all unique destination stacks - call this $S$.</p>\n\n<p>The stack part is irrelevant, mostly, other than it enforces a partial order on the result. If we have two source stacks and one destination stack, this is the same as generating all permutations without repetitions for a set of $2N$ elements with $N$ \'A\' elements and $N$ \'B\' elements. Call this $O$.</p>\n\n<p>Thus</p>\n\n<p>$\\qquad \\displaystyle |O| = (2n)!/(n!)^2$</p>\n\n<p>Now observe all possible bit sequences of length 2n (bit 0 representing popping source stack A/B and bit 1 pushing to destination stack C/D), call this B. |B|=22n. We can surely generate B and check if it has the correct number of pops from each destination stack to generate |S|. It\'s a little faster to recursively generate these to ensure their validity. It\'s even faster still to generate B and O and then simulate, but it still has the issue of needing to check for duplicates.</p>\n\n<p>My question</p>\n\n<p>Is there a more efficient way to generate these?</p>\n\n<p>Through simulation I found the result follows <a href="http://oeis.org/A084773" rel="nofollow">this sequence</a> which is related to Delannoy Numbers, which I know very little about if this suggests anything.</p>\n\n<p>Here is my Python code</p>\n\n<pre><code>def all_subsets(list):\n    if len(list)==0:\n        return [set()]\n    subsets = all_subsets(list[1:])\n\n    return [subset.union(set([list[0]])) for subset in subsets] + subsets\n\ndef result_sequences(perms):\n    for perm in perms:\n        whole_s = range(len(perm))\n        whole_set = set(whole_s)\n        for send_to_c in all_subsets(whole_s):\n            send_to_d = whole_set-set(send_to_c)\n            yield [perm,send_to_c,send_to_d]\n\nn = 4\nperms_ = list(unique_permutations([n,n],[\'a\',\'b\'])) # number of unique sequences                                                                                                               \nresult = list(result_sequences(perms_))\n</code></pre>\n', 'Tags': '<algorithms><combinatorics><efficiency>', 'LastEditorUserId': '1784', 'LastActivityDate': '2012-06-14T22:35:54.300', 'CommentCount': '2', 'AcceptedAnswerId': '2305', 'CreationDate': '2012-06-07T16:36:42.153', 'Id': '2257'},6810:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>A graph is 2\u2200-connected if it remains connected even if any single edge is removed. Let G = (V, E) be a connected undirected graph. Develop an algorithm as fast as possible to check 2\u2200-connectness of G.</p>\n\n<p>I know the basic idea is to build a DFS searching tree and then check each edge is not on a circle with DFS. Any help would be appreciated.</p>\n\n<p>What I expect to see is a detailed algorithm description(especially the initialization of needed variables which is obscure sometimes), complexity analysis could be omitted.</p>\n', 'ViewCount': '100', 'Title': u'Algorithm to check the 2\u2200-connectness property of a graph', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-17T13:44:12.877', 'LastEditDate': '2012-06-17T13:39:54.627', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2397', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1875', 'Tags': '<algorithms><graphs><efficiency>', 'CreationDate': '2012-06-16T15:33:06.990', 'Id': '2394'},6811:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I have a high interest in priority-queues (E.g., see my answers on: <a href="http://cs.stackexchange.com/q/524">Does there exist a priority queue with $O(1)$ extracts?</a>), and was wondering if there is a priority-queue or similar data-structure where you can sort by multiple values?</p>\n\n<p>For example, if I wanted to sort by <code>numval</code> and sort by <code>strval</code>, and be able to get the highest (G\xf6del numbering for str) in $\\mathcal{O}(1)$.</p>\n\n\n\n<pre><code>struct Node {\n    int numval;\n    std::string strval;\n};\n</code></pre>\n\n<p>Easily I can think to just maintain two priority-queues, but this would require twice the memory.</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '214', 'Title': 'Queue that can sort by multiple priorities?', 'LastActivityDate': '2012-07-05T13:37:54.173', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<data-structures><asymptotics><efficiency><priority-queues><memory-management>', 'CreationDate': '2012-07-05T13:37:54.173', 'FavoriteCount': '2', 'Id': '2629'},6812:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1273', 'Title': 'Removing Left Recursion from Context-Free Grammars - Ordering of nonterminals', 'LastEditDate': '2014-01-23T17:17:28.390', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'clebert', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': u'<p>I have recently implemented the Paull\'s algorithm for removing left-recursion from context-free grammars:</p>\n\n<blockquote>\n  <p>Assign an ordering $A_1, \\dots, A_n$ to the nonterminals of the grammar.</p>\n  \n  <p>for $i := 1$ to $n$ do begin<br>\n  $\\quad$ for $j:=1$ to $i-1$ do begin<br>\n  $\\quad\\quad$ for each production of the form $A_i \\to A_j\\alpha$ do begin<br>\n  $\\quad\\quad\\quad$ remove $A_i \\to A_j\\alpha$ from the grammar<br>\n  $\\quad\\quad\\quad$ for each production of the form $A_j \\to \\beta$ do begin<br>\n  $\\quad\\quad\\quad\\quad$ add $A_i \\to \\beta\\alpha$ to the grammar<br>\n  $\\quad\\quad\\quad$ end<br>\n  $\\quad\\quad$ end<br>\n  $\\quad$ end<br>\n  $\\quad$ transform the $A_i$-productions to eliminate direct left recursion<br>\n  end</p>\n</blockquote>\n\n<p>According to <a href="http://research.microsoft.com/pubs/68869/naacl2k-proc-rev.pdf" rel="nofollow" title="Removing Left Recursion from Context-Free Grammars">this document</a>, the efficiency of the algorithm crucially depends on the ordering of the nonterminals chosen in the beginning; the paper discusses this issue in detail and suggest optimisations.</p>\n\n<p>Some notation:</p>\n\n<blockquote>\n  <p>We will say that a symbol $X$ is a <em>direct left corner</em> of\n  a nonterminal $A$, if there is an $A$-production with $X$ as the left-most symbol on the right-hand side. We define the <em>left-corner relation</em> to be the reflexive transitive closure of the direct-left-corner relation, and we define the <em>proper-left-corner relation</em> to be the transitive closure of\n  the direct-left-corner relation. A nonterminal is <em>left recursive</em> if it is a proper left corner of itself; a nonterminal is <em>directly left recursive</em> if it is a direct left corner of itself; and a nonterminal is <em>indirectly left recursive</em> if it is left recursive, but not directly left recursive.</p>\n</blockquote>\n\n<p>Here is what the authors propose:</p>\n\n<blockquote>\n  <p>In the inner loop of Paull\u2019s algorithm, for nonterminals $A_i$ and $A_j$, such that $i &gt; j$ and $A_j$ is a direct left corner of $A_i$, we replace all occurrences of $A_j$ as a direct left corner of $A_i$ with all possible expansions of $A_j$.</p>\n  \n  <p>This only contributes to elimination of left recursion from the grammar if $A_i$ is a left-recursive nonterminal, and $A_j$ lies on a path that makes $A_i$ left recursive; that is, if $A_i$ is a left corner of $A_j$ (in addition to $A_j$ being a left corner of $A_i$).</p>\n  \n  <p>We could eliminate replacements that are useless in removing left recursion if we could order the nonterminals of the grammar so that, if $i &gt; j$ and $A_j$ is a direct left corner of $A_i$, then $A_i$ is also a left corner of $A_j$.</p>\n  \n  <p>We can achieve this by ordering the nonterminals in decreasing order of the number of distinct left corners they have.</p>\n  \n  <p>Since the left-corner relation is transitive, if C is a direct left corner of B, every left corner of C is also a left corner of B.</p>\n  \n  <p>In addition, since we defined the left-corner relation to be reflexive, B is a left corner of itself.</p>\n  \n  <p>Hence, if C is a direct left corner of B, it must follow B in decreasing order of number of distinct left corners, unless B is a left corner of C.</p>\n</blockquote>\n\n<p>All I want is to know how to order the nonterminals in the beginning, but I don\'t get it from the paper. Can someone explain it in a simpler way? Pseudocode would help me to understand it better.</p>\n', 'Tags': '<algorithms><context-free><formal-grammars><efficiency><left-recursion>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-23T17:17:28.390', 'CommentCount': '0', 'AcceptedAnswerId': '2793', 'CreationDate': '2012-05-23T12:50:45.647', 'Id': '2792'},6813:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve been looking for a way to represent the <a href="http://en.wikipedia.org/wiki/Golden_ratio_base" rel="nofollow">golden ratio ($\\phi$) base</a> more efficiently in binary.  The standard binary golden ratio notation works but is horribly space inefficient.  The  Balanced Ternary Tau System (BTTS) is the best I\'ve found but is quite obscure.  The paper describing it in detail is <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.880" rel="nofollow">A. Stakhov, Brousentsov\'s Ternary Principle, Bergman\'s Number System and Ternary Mirror-symmetrical Arithmetic, 2002</a>.  It is covered in less depth by <a href="http://neuraloutlet.wordpress.com/tag/ternary-tau-system/" rel="nofollow">this blog post</a>.</p>\n\n<p>BTTS is a <a href="http://en.wikipedia.org/wiki/Balanced_ternary" rel="nofollow">balanced ternary representation</a> that uses $\\phi^2 = \\phi + 1$ as a base and 3 values of $\\bar 1$ ($-1$), $0$, and $1$ to represent addition or subtraction of powers of $\\phi^2$.  The table on page 6 of the paper lists integer values from 0 up to 10, and it can represent any $\\phi$-based number as well.</p>\n\n<p>BTTS has some fascinating properties, but being ternary, I didn\'t think I\'d be able to find a compact bit representation for it.</p>\n\n<p>Then I noticed that because of the arithmetic rules, the pattern $\\bar 1 \\bar 1$ never occurs as long as you only allow numbers $\\ge 0$.  This means that the nine possible combinations for each pair of trits ($3^2$) only ever has 8 values, so we can encode 2 trits with 3 bits ($2^3$, a.k.a octal).  Also note that the left-most bit (and also right-most for integers because of the mirror-symmetric property) will only ever be $0$ or $1$ (again for positive numbers only), which lets us encode the left-most trit with only 1 bit.</p>\n\n<p>So a $2^n$-bit number can store $\\lfloor 2^n/3\\rfloor * 2 + 1$ balanced trits, possibly with a bit left over (maybe a good candidate for a sign bit).  For example, we can represent $10 + 1 = 11$ balanced trits with $15 + 1 = 16$  bits, or $20 + 1 = 21$ balanced trits with $30 + 1 = 31$ bits, with 1 left over (32-bit).  This has much better space density than ordinary golden ratio base binary encoding.</p>\n\n<p>So my question is, what would be a good octal (3-bit) encoding of trit pairs such that we can implement the addition and other arithmetic rules of the BTTS with as little difficulty as possible.  One of the tricky aspects of this system is that carries happen in both directions, i.e. <br/>\n$1 + 1 = 1 \\bar 1 .1$ and $\\bar 1 + \\bar 1 = \\bar 1 1.\\bar 1$.</p>\n\n<p>This is my first post here, so please let me know if I need to fix or clarify anything.</p>\n\n<p>--<strong>Edit</strong>--</p>\n\n<p>ex0du5 asked for some clarification of what I need from a binary representation:</p>\n\n<ol>\n<li>I want to be able to represent positive values of both integers and powers of $\\phi$.  The range of representable values need not be as good as binary, but it should be better than phinary per bit.  I want to represent the largest possible set of phinary numbers in the smallest amount of space possible.  Space takes priority over operation count for arithmetic operations.</li>\n<li>I need addition to function such that carries happen in both directions.  Addition will be the most common operation for my application.  Consequently it should require as few operations as possible.  If a shorter sequence of operations are possible using a longer bit representation (conflicting with goal 1), then goal 1 takes priority.  Space is more important than speed.</li>\n<li>Multiplication only needs to handle integers > 0 multiplied to a phinary number, not arbitrary phinary number multiplication, and so can technically be emulated with a series of additions, though a faster algorithm would be helpful.</li>\n<li>I\'m ignoring division and subtraction for now, but having algorithms for them would be a bonus.</li>\n<li>I need to eventually convert a phinary number to a binary floating point approximation of it\'s value, but this will happen only just prior to output.  There will be no converting back and forth.</li>\n</ol>\n', 'ViewCount': '294', 'Title': 'What is a good binary encoding for $\\phi$-based balanced ternary arithmetic algorithms?', 'LastEditorUserId': '2230', 'LastActivityDate': '2012-11-18T06:37:19.827', 'LastEditDate': '2012-07-26T14:22:13.457', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2230', 'Tags': '<algorithms><data-structures><efficiency><coding-theory>', 'CreationDate': '2012-07-20T21:32:29.663', 'Id': '2847'},6814:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>What's the complexity of Conflict-Driven Clause Learning SAT solvers, compared to DPLL solvers? Was it proven that CDCL is faster in general? Are there instances of SAT that are hard for CDCL but easy for DPLL?</p>\n", 'ViewCount': '138', 'Title': 'Running time of CDCL compared to DPLL', 'LastEditorUserId': '472', 'LastActivityDate': '2012-11-25T21:52:35.260', 'LastEditDate': '2012-11-25T21:52:35.260', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3015', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2345', 'Tags': '<complexity-theory><time-complexity><efficiency><satisfiability><sat-solvers>', 'CreationDate': '2012-08-03T07:31:02.560', 'Id': '3014'},6815:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '202', 'Title': 'How to compute linear recurrence using matrix with fraction coefficients?', 'LastEditDate': '2012-08-09T21:16:25.387', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '991', 'FavoriteCount': '1', 'Body': '<p>What I\'m trying to do is generate <a href="http://en.wikipedia.org/wiki/Motzkin_number" rel="nofollow">Motzkin numbers</a> mod a large number $10^{14} + 7$ (not prime), and it needs to compute the $n$th Motzkin number as fast as possible. From Wikipedia, the formula for the $n$th Motzkin number is defined as following:</p>\n\n<p>$\\qquad \\displaystyle \\begin{align}\n  M_{n+1} &amp;= M_n + \\sum_{i=0}^{n-1} M_iM_{n-1-i} \\\\\n          &amp;= \\frac{2n+3}{n+3}M_n + \\frac{3n}{n+3}M_{n-1}\n\\end{align}$ </p>\n\n<p>My initial approach is to use the second formula which is obviously faster, but the problem I ran into is the division since modular arithmetic rule doesn\'t apply.</p>\n\n<pre><code>void generate_motzkin_numbers() {\n    motzkin[0] = 1;\n    motzkin[1] = 1;\n    ull m0 = 1;\n    ull m1 = 1;\n    ull numerator;\n    ull denominator;\n    for (int i = 2; i &lt;= MAX_NUMBERS; ++i) {\n        numerator = (((2*i + 1)*m1 + 3*(i - 1)*m0)) % MODULO;\n        denominator = (i + 2);\n        motzkin[i] = numerator/denominator;\n        m0 = m1;\n        m1 = motzkin[i];\n    }\n}\n</code></pre>\n\n<p>Then I tried the second formula, but the running time is horribly slow because the summation:</p>\n\n<pre><code>void generate_motzkin_numbers_nested_recurrence() {\n    mm[0] = 1;\n    mm[1] = 1;\n    mm[2] = 2;\n    mm[3] = 4;\n    mm[4] = 9;\n    ull result;\n    for (int i = 5; i &lt;= MAX_NUMBERS; ++i) {\n        result = mm[i - 1];\n        for (int k = 0; k &lt;= (i - 2); ++k) {\n            result = (result + ((mm[k] * mm[i - 2 - k]) % MODULO)) % MODULO;\n        }\n        mm[i] = result;\n    }\n}\n</code></pre>\n\n<p>Next, I\'m thinking of using matrix form which eventually can be speed up using exponentiation squaring technique, in other words $M_{n+1}$ can be computed as follows:\n$$M_{n+1} = \\begin{bmatrix} \\dfrac{2n+3}{n+3} &amp; \\dfrac{3n}{n+3} \\\\ 1 &amp; 0\\end{bmatrix}^n \\cdot \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}$$ \nWith exponentiation by squaring, this method running time is $O(\\log(n))$ which I guess the fastest way possible, where <code>MAX_NUMBERS = 10,000</code>. Unfortunately, again the division with modular is killing me. After apply the modulo to the numerator, the division is no longer accurate. So my question is, is there another technique to compute this recurrence modulo a number? I\'m think of a dynamic programming approach for the summation, but I still think it\'s not as fast as this method.  Any ideas or suggestions would be greatly appreciated. </p>\n', 'Tags': '<algorithms><recurrence-relation><efficiency><integers>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-10T13:32:37.340', 'CommentCount': '6', 'AcceptedAnswerId': '3116', 'CreationDate': '2012-08-09T19:06:02.450', 'Id': '3109'},6816:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Is there any book or tutorial that teaches us how to efficiently apply the common algorithms (sorting, searching, etc.) on large data (i.e. data that cannot be fully loaded into main memory) and how to efficiently apply those algorithms considering the cost of block transfer from external memory ? For example, almost all algorithm textbooks say that B and B+-trees can be used to store data on disk. However, actually how this can be done, especially handling the pointers where the data is present on disk is not explained. Similarly, though many books teach searching techniques, they do not consider data present in secondary memory. </p>\n\n<p>I have checked Knuth's book. Although it discusses these ideas, I still did not understand how to actually apply them in a high-level language. Is there any reference that discusses these details?</p>\n", 'ViewCount': '261', 'Title': 'Applying algorithms on large data', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-18T21:47:06.810', 'LastEditDate': '2012-10-18T21:21:53.663', 'AnswerCount': '3', 'CommentCount': '10', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2596', 'Tags': '<algorithms><efficiency><memory-management><big-data>', 'CreationDate': '2012-08-22T11:10:59.273', 'Id': '3287'},6817:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '611', 'Title': 'Dynamic programming with large number of subproblems', 'LastEditDate': '2012-10-09T12:39:47.157', 'AnswerCount': '2', 'Score': '7', 'OwnerDisplayName': 'Alexandre', 'PostTypeId': '1', 'OwnerUserId': '3101', 'FavoriteCount': '2', 'Body': '<p>Dynamic programming with large number of subproblems. So I\'m trying to solve this problem from Interview Street:</p>\n\n<blockquote>\n  <p><strong>Grid Walking</strong> (Score 50 points)<br>\n  You are situated in an $N$-dimensional grid at position $(x_1,x_2,\\dots,x_N)$. The dimensions of the grid are $(D_1,D_2,\\dots,D_N$). In one step, you can walk one step ahead or behind in any one of the $N$ dimensions. (So there are always $2N$ possible different moves). In how many ways can you take $M$ steps such that you do not leave the grid at any point? You leave the grid if for any $x_i$, either $x_i \\leq 0$ or $x_i &gt; D_i$.</p>\n</blockquote>\n\n<p>My first try was this memoized recursive solution:</p>\n\n<pre><code>def number_of_ways(steps, starting_point):\n    global n, dimensions, mem\n    #print steps, starting_point\n    if (steps, tuple(starting_point)) in mem:\n        return mem[(steps, tuple(starting_point))]\n    val = 0\n    if steps == 0:\n        val = 1\n    else:\n        for i in range(0, n):\n            tuple_copy = starting_point[:]\n            tuple_copy[i] += 1\n            if tuple_copy[i] &lt;= dimensions[i]:\n                val += number_of_ways(steps - 1, tuple_copy)\n            tuple_copy = starting_point[:]\n            tuple_copy[i] -= 1\n            if tuple_copy[i] &gt; 0:\n                val += number_of_ways(steps - 1, tuple_copy)\n    mem[(steps, tuple(starting_point))] = val\n    return val\n</code></pre>\n\n<p>Big surprise: it fails for a large number of steps and/or dimensions due to a lack of memory.</p>\n\n<p>So the next step is to improve my solution by using dynamic programming. But before starting, I\'m seeing a major problem with the approach. The argument <code>starting_point</code> is an $n$-tuple, where $n$ is as large as $10$. So in fact, the function could be <code>number_of_ways(steps, x1, x2, x3, ... x10)</code> with  $1 \\leq x_i \\leq 100$.</p>\n\n<p>The dynamic programming problems I\'ve seen in textbooks almost all have twp variables, so that only a two-dimensional matrix is needed. In this case, a ten-dimensional matrix would be needed. So $100^{10}$ cells in total.</p>\n\n<p>With 2-D matrixes in dynamic programming, usually only the previous row of calculations is needed for the next calculation, hence reducing the spatial complexity from $mn$ to $\\min(m,n)$. I\'m not sure how I would do the same in this case. Visualizing a table isn\'t feasible, so the answer would have to come directly from the recursion above. </p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Using Peter Shor\'s suggestions, and making some minor corrections, notably the need to keep track of position in the $W(i, t_i)$ function, and rather than only splitting dimensions into two sets A and B, doing the splitting recursively, effectively using a divide-and-conquer method, until a base case is reached where only one dimension is in the set.</p>\n\n<p>I came up with the following implementation, which passed all tests below the maximum execution time:</p>\n\n<pre><code>def ways(di, offset, steps):\n    global mem, dimensions\n    if steps in mem[di] and offset in mem[di][steps]:\n        return mem[di][steps][offset]\n    val = 0\n    if steps == 0:\n        val = 1\n    else:\n        if offset - 1 &gt;= 1:\n            val += ways(di, offset - 1, steps - 1)\n        if offset + 1 &lt;= dimensions[di]:\n            val += ways(di, offset + 1, steps - 1)\n    mem[di][steps][offset] = val\n    return val\n\n\ndef set_ways(left, right, steps):\n    # must create t1, t2, t3 .. ti for steps\n    global mem_set, mem, starting_point\n    #print left, right\n    #sleep(2)\n    if (left, right) in mem_set and steps in mem_set[(left, right)]:\n        return mem_set[(left, right)][steps]\n    if right - left == 1:\n        #print \'getting steps for\', left, steps, starting_point[left]\n        #print \'got \', mem[left][steps][starting_point[left]], \'steps\'\n        return mem[left][steps][starting_point[left]]\n        #return ways(left, starting_point[left], steps)\n    val = 0\n    split_point =  left + (right - left) / 2 \n    for i in xrange(steps + 1):\n        t1 = i\n        t2 = steps - i\n        mix_factor = fact[steps] / (fact[t1] * fact[t2])\n        #print "mix_factor = %d, dimension: %d - %d steps, dimension %d - %d steps" % (mix_factor, left, t1, split_point, t2)\n        val += mix_factor * set_ways(left, split_point, t1) * set_ways(split_point, right, t2)\n    mem_set[(left, right)][steps] = val\n    return val\n\nimport sys\nfrom time import sleep, time\n\nfact = {}\nfact[0] = 1\nstart = time()\naccum = 1\nfor k in xrange(1, 300+1):\n    accum *= k\n    fact[k] = accum\n#print \'fact_time\', time() - start\n\ndata = sys.stdin.readlines()\nnum_tests = int(data.pop(0))\nfor ignore in xrange(0, num_tests):\n    n_and_steps = data.pop(0)\n    n, steps = map(lambda x: int(x), n_and_steps.split())\n    starting_point = map(lambda x: int(x), data.pop(0).split())\n    dimensions = map(lambda x: int(x), data.pop(0).split())\n    mem = {}\n    for di in xrange(n):\n        mem[di] = {}\n        for i in xrange(steps + 1):\n            mem[di][i] = {}\n            ways(di, starting_point[di], i)\n    start = time()\n    #print \'mem vector is done\'\n    mem_set = {}\n    for i in xrange(n + 1):\n        for j in xrange(n + 1):\n            mem_set[(i, j)] = {}\n    answer = set_ways(0, n, steps)\n    #print answer\n    print answer % 1000000007\n    #print time() - start\n</code></pre>\n', 'Tags': '<algorithms><efficiency><dynamic-programming>', 'LastEditorUserId': '3101', 'LastActivityDate': '2012-12-11T02:10:15.510', 'CommentCount': '4', 'AcceptedAnswerId': '4949', 'CreationDate': '2012-10-07T23:07:21.413', 'Id': '4941'},6818:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a simple problem. I can't seem to even find the right search terms to get me pointed in the direction I need to be heading.</p>\n\n<p>I'm writing a bunch of integers to disk. Lot's of them.</p>\n\n<p>Starting with the integer 2, I add it to another integer, write the result to disk, and start the process again using the result as the seed.</p>\n\n<p>This pattern is helping me generate data I need for other research, but I need to apply this pattern until I've reached integers with a length of 10,000 digits or more.</p>\n\n<p>So, here is my simple question: given a set of data of a known data type, how do I calculate the storage space required to store that set?</p>\n\n<p>At the moment, I'm doing this incredibly simple task in Python, recording longs to disk as binary in a single file. But, on my very small machine, I was only able to store the integers my process produced with values between 0 and 42 billion.</p>\n\n<p>Short of my goal of recording a data set with values between 0 and 1e10000.</p>\n\n<p>Given that I have no exposure to formulas for calculating hardware requirements for storing data like this, I have no idea if I'm using the most efficient language and data type for storing as many integers as possible in as little space as possible.</p>\n", 'ViewCount': '175', 'Title': 'Calculate storage requirements for a data set', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-15T05:23:11.350', 'LastEditDate': '2012-10-15T05:23:11.350', 'AnswerCount': '1', 'CommentCount': '13', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3162', 'Tags': '<integers><efficiency><storage>', 'CreationDate': '2012-10-12T04:34:17.103', 'Id': '5032'},6819:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <a href="https://en.wikipedia.org/wiki/Radon_transform" rel="nofollow">Radon transform</a> is used to take 2d projections of an object and create a 3d representation.</p>\n\n<p>It seems like it would be possible to apply such a transform in 3d graphics in games (although possibly too slow to be practical).</p>\n\n<p>For example, a very simple way to display an object is to use a 3d rectangle and texture map each side. This is relatively fast but the 3d detail is limited. When a side is parallel with the visual plane it will represent the detail 100% (so the visual detail would be limited to that of the texture map). Of course it won\'t represent external 3d effects properly, like lighting.</p>\n\n<p>But by using the Radon transform one could gain a true 3d approximation of the object from the six textures/projections used. By increasing the number of textures/projections the approximation is better.</p>\n\n<p>I\'m curious if the idea has potential. Possibly for high-quality 3d models it might pay off in performance and size. Of course 3d models can be optimized to limit their size which also increases speed but visually doesn\'t change much.</p>\n', 'ViewCount': '207', 'Title': 'Radon transform for advanced 3d graphics and games?', 'LastEditorUserId': '4244', 'LastActivityDate': '2013-07-17T02:17:03.387', 'LastEditDate': '2012-10-19T06:48:32.120', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4244', 'Tags': '<algorithms><computational-geometry><efficiency><graphics>', 'CreationDate': '2012-10-18T05:20:19.123', 'FavoriteCount': '1', 'Id': '6147'},6820:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>This is a homework problem for my introduction to algorithms course.</p>\n\n<blockquote>\n  <p>Recall the scheduling problem from Section 4.2 in which we sought to\n  minimize the maximum lateness. There are $n$ jobs, each with a deadline\n  $d_i$ and a required processing time $t_i$, and all jobs are available to be\n  scheduled starting at time $s$. For a job $i$ to be done, it needs to be assigned\n  a period from $s_i \\geq s$ to $f_i$ = $s_i + t_i$, and different jobs should be assigned\n  nonoverlapping intervals. As usual, such an assignment of times will be\n  called a schedule.</p>\n  \n  <p>In this problem, we consider the same setup, but want to optimize a\n  different objective. In particular, we consider the case in which each job\n  must either be done by its deadline or not at all. We\u2019ll say that a subset $J$ of\n  the jobs is schedulable if there is a schedule for the jobs in $J$ so that each\n  of them finishes by its deadline. Your problem is to select a schedulable\n  subset of maximum possible size and give a schedule for this subset that\n  allows each job to finish by its deadline.</p>\n  \n  <p>(a) Prove that there is an optimal solution $J$ (i.e., a schedulable set of\n  maximum size) in which the jobs in $J$ are scheduled in increasing\n  order of their deadlines.</p>\n  \n  <p>(b) Assume that all deadlines $d_i$ and required times $t_i$ are integers. Give\n  an algorithm to find an optimal solution. Your algorithm should\n  run in time polynomial in the number of jobs $n$, and the maximum\n  deadline $D = \\max_i d_i$.</p>\n</blockquote>\n\n<p>I've solved the problem as worded with the recurrence </p>\n\n<p>$Opt(i, d) = \\max\\left \\{ \n\\begin{array}\n \\\\ Opt(i-1, d-t_i) + 1 \\hspace{20 mm} d\\leq d_i\n \\\\ Opt(i-1, d) \n\\end{array}\n\\right \\}$</p>\n\n<p>but our instructor added a new requirement that our algorithm must not be dependent on D. This recurrence seems like it would produce an $O(nD)$ running time if implemented with dynamic programming.</p>\n\n<p>I can't figure out how to reduce its running time from $O(nD)$ to $O(n^k)$. To me it seems like it's a variation on the knapsack problem with all values equal to 1. In which case it seems like this is the best that can be done.</p>\n\n<p>If I'm doing something wrong could someone point me in the right direction, or if I've done everything right so far, could someone at least give me a hint as to how I can make an $O(n^k)$ recurrence or algorithm.</p>\n", 'ViewCount': '560', 'Title': 'Maximum Schedulable Set Zero-Lateness Deadline Scheduling', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-30T18:55:05.237', 'LastEditDate': '2012-10-31T09:59:21.227', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4294', 'Tags': '<algorithms><time-complexity><dynamic-programming><efficiency><scheduling>', 'CreationDate': '2012-10-20T23:13:27.230', 'FavoriteCount': '2', 'Id': '6202'},6821:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a container with a certain dimension. A number of small boxes that may be different in size is to be packed into the container. How to arrange the small boxes such that the container contains as many as possible?</p>\n\n<ul>\n<li>No rotation is allowed.</li>\n<li>The heavier boxes must not be on the top of the lighter ones.</li>\n<li>Approximation is allowed.</li>\n</ul>\n\n<p>I am looking for the algorithm so I can implement it in a software.</p>\n', 'ViewCount': '213', 'Title': 'Algorithm to pack any small boxes into a big box', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:25:11.543', 'LastEditDate': '2013-05-24T03:25:11.543', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4542', 'Tags': '<algorithms><combinatorics><efficiency><approximation><knapsack-problems>', 'CreationDate': '2012-11-10T19:18:01.317', 'Id': '6606'},6822:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A binary counter is represented by an infinite array of 0 and 1.</p>\n\n<p>I need to implement the action $\\text{add}(k)$ which adds $k$ to the value represented in the array.</p>\n\n<p>The obvious way is to add 1, k times. Is there a more efficient way?</p>\n', 'ViewCount': '214', 'Title': 'Implementing addition for a binary counter', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-12T11:35:22.097', 'LastEditDate': '2012-11-12T11:35:22.097', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6628', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4561', 'Tags': '<algorithms><data-structures><efficiency>', 'CreationDate': '2012-11-12T08:21:57.977', 'Id': '6627'},6823:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a <a href="http://en.wikipedia.org/wiki/Binary_expression_tree" rel="nofollow">binary expresion tree</a>, with addition and multiplication operations, how can we optimize it\'s evaluation?</p>\n\n<p>Can we learn from <a href="http://en.wikipedia.org/wiki/Matrix_chain_multiplication" rel="nofollow">matrix chain multiplication</a>? A <a href="http://en.wikipedia.org/wiki/Matrix_chain_multiplication#Generalizations" rel="nofollow">generalization</a> of matrix chain multiplication is defined as:</p>\n\n<blockquote>\n  <p>Given a linear sequence of objects, an associative binary operation on those objects, and a way to compute the cost of performing that operation on any two given objects (as well as all partial results), compute the minimum cost way to group the objects to apply the operation over the sequence.</p>\n</blockquote>\n\n<p>What happens if we put <em>two</em> binary operators? <strong>Can the algorithm for <em>Matrix chain multiplication</em> be further generalized (or how can we otherwise solve this problem) to <em>two</em> binary operators in a <em>binary expresion tree</em>, given the cost functions of these operations?</strong> In particular, <strong>multiplication and addition, which complicates things further by allowing distribution</strong>. Also, does it matter that mind that some of the numbers can be negative, allowing reduction in size of intermediate results (see <a href="http://cs.stackexchange.com/q/1424/2755">Overflow safe summation</a>)?</p>\n\n<p><strong>Also, how does this relate to  <a href="http://en.wikipedia.org/wiki/Graph_reduction" rel="nofollow">Graph Reduction</a>?</strong></p>\n\n<p>I also remember learning about database <a href="http://en.wikipedia.org/wiki/Query_optimization" rel="nofollow">query optimization</a> which seemed to do something similar to determine how early to execute particular joins to keep the intermediate values smaller.</p>\n', 'ViewCount': '422', 'Title': 'Chained operations on sequences with two operators', 'LastEditorUserId': '2755', 'LastActivityDate': '2012-11-28T22:31:40.290', 'LastEditDate': '2012-11-28T22:31:40.290', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<optimization><binary-trees><dynamic-programming><efficiency><arithmetic>', 'CreationDate': '2012-11-20T18:58:03.297', 'FavoriteCount': '1', 'Id': '6790'},6824:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For what size alphabet does it take longer to construct <a href="http://en.wikipedia.org/wiki/Suffix_tree" rel="nofollow">a suffix tree</a> - for a really small alphabet size (because it has to go deep into the tree) or for a large alphabet size? Or is it dependent on the algorithm you use? If it is dependent, how does the alphabet size affect <a href="http://en.wikipedia.org/wiki/Ukkonen%27s_algorithm" rel="nofollow">Ukkonen\'s algorithm</a>?</p>\n', 'ViewCount': '88', 'Title': 'What are the effects of the alphabet size on construct algorithms for suffix trees?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-23T09:43:36.243', 'LastEditDate': '2012-11-23T09:30:49.477', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4696', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings><efficiency>', 'CreationDate': '2012-11-22T22:14:10.630', 'Id': '6842'},6825:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say I had the choice of choosing one out of the following two optimization problems which I could use to solve my problem. Which choice is the fastest? How much of a trade-off would it be?  Is the improvement in speed by many factors!?</p>\n\n<ol>\n<li><p>Minimizing a convex function $L(X)$ in one matrix variable with orthogonality constraints over the matrix-essentially in my case this ends up to solving an eigen-decomposition.</p></li>\n<li><p>Minimizing the same convex function $L(X)$ with linear constraints in $X$.</p></li>\n</ol>\n\n<p>I know that 2.) should be faster. But what is the direction of work I need to do- to compare the improvement in speed-especially in terms of using the fastest available eigen solver for 1.)-what would be the corresponding fastest approach to solve 2.)?</p>\n', 'ViewCount': '61', 'Title': 'Time - Complexity Convex Optimization and Eigen Decomposition', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T00:32:56.540', 'LastEditDate': '2012-12-06T10:17:22.360', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'VSPC', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity><optimization><efficiency><linear-algebra>', 'CreationDate': '2012-10-16T18:17:29.447', 'Id': '7206'},6826:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '904', 'Title': 'Retrieving the shortest path of a dynamic graph', 'LastEditDate': '2012-12-10T15:45:53.627', 'AnswerCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '4916', 'FavoriteCount': '3', 'Body': "<p>I'm studying shortest paths in directed graphs currently. There are many efficient algorithms for finding the shortest path in a network, like dijkstra's or bellman-ford's. But what if the graph is dynamic? By saying dynamic I mean that we can insert or remove vertices during the execution of the program. I'm trying to find an efficient algorithm for updating the shortest paths from a vertex $v$ to every other vertex $u$, after inserting an edge $e$, without needing to run the shortest path algorithm in the new graph again. How can I do this? Thanks in advance.</p>\n\n<ul>\n<li><em>Note:</em> the changes can be done after the first iteration of the algorithm</li>\n<li><em>Note[2]:</em> two nodes are given, $s$ the source and $t$ the target. I need to find the shortest path between these nodes. When the graph is updated I only have to update $\\pi(s,t)$, which is the shortest path between $s$ and $t$.</li>\n<li><em>Note[3]:</em> I'm only interested in the edge insertion case.</li>\n</ul>\n\n<blockquote>\n  <p><strong>A formal definition</strong>: Given a graph $G = (V,E)$. Define an <em>update operation</em> as 1) an insertion of an edge $e$ to $E$ or 2) a a deletion of an edge $e$ from $E$. The objective is to find efficiently the cost of all pairs shortest paths after an update operation. By efficiently, we mean at least better than executing an All-Pairs-Shortest-Path algorithm, such as Bellman-Ford algorithm, after each update operation.</p>\n</blockquote>\n\n<hr>\n\n<p><strong>Edit:</strong> Below there is a simplified version of the problem:</p>\n\n<blockquote>\n  <p>A weighted graph $G(V,E)$ is given, consisting of unidirectional edges, and two critical vertices $s$ and $t$. A set $C$ of candidate <em>bidirectional</em> edges is also given. I have to build an edge $(u,v) \\in C$ to minimize the distance from $s$ to $t$.</p>\n</blockquote>\n", 'Tags': '<algorithms><data-structures><graphs><efficiency><shortest-path>', 'LastEditorUserId': '4916', 'LastActivityDate': '2013-06-12T10:15:55.483', 'CommentCount': '7', 'AcceptedAnswerId': '7260', 'CreationDate': '2012-12-08T11:22:28.103', 'Id': '7250'},6827:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am researching collaborative editing systems for some work, but so far my search is turning up blank.</p>\n\n<p>Collaborative real-time editing systems almost all have features like:</p>\n\n<ul>\n<li>Many users can edit the document at the same time</li>\n<li>All changes are dated and saved in a linear revision history</li>\n<li>Edits are visible in semi-real time</li>\n<li>Who-wrote-what is tracked for all content</li>\n</ul>\n\n<p>I cannot think of a data- and procedure model to encompass all of these without huge inefficiencies, for example:</p>\n\n<ul>\n<li>Take user input as a few typestrokes at a time, thus enabling real-time visibility but with horrible server loads.</li>\n<li>Take user input as element changes, thus disabling real-time visibility and complicating many-user-editing of one paragraph.</li>\n<li>Edit a XML Document Model Object in place responding to inputs, but making it difficult to version/revision track.</li>\n<li>Edit linear text stored in an appropriate editing structure responding to inputs, making it difficult to verify that correct markup is being generated.</li>\n<li>Store revision history accumulating, requiring computational resources but easing on space.</li>\n<li>Store revision history whole, requiring space but easing on computation.</li>\n</ul>\n\n<p>Am I off track? Does there exist good solutions to these problems? What, if any, is the relevant litterature?</p>\n', 'ViewCount': '116', 'Title': 'What sort of algorithm/communication model/data structure do collaborative real time editors use?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-17T17:26:32.057', 'LastEditDate': '2013-01-15T14:13:45.613', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6428', 'Tags': '<data-structures><efficiency>', 'CreationDate': '2013-01-15T13:32:56.770', 'FavoriteCount': '2', 'Id': '8946'},6828:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Supposing we have two grammars which define the same languge: regular one and LALR(1) one.</p>\n\n<p>Both regular and LALR(1) algorithms are O(n) where n is input length.</p>\n\n<p>Regexps are usually preferred for parsing regular languages. Why? Is there a formal proof (or maybe that's obvious) that they are faster?</p>\n", 'ViewCount': '125', 'Title': 'Regular vs LALR(1): what is faster', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-26T18:03:37.970', 'LastEditDate': '2013-01-26T18:03:37.970', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6564', 'Tags': '<regular-languages><efficiency><parsers>', 'CreationDate': '2013-01-25T23:42:16.297', 'Id': '9159'},6829:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '49', 'Title': 'Cache strategies, what reference article could I study?', 'LastEditDate': '2013-01-28T10:11:13.810', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2100', 'FavoriteCount': '1', 'Body': '<p>So as to optimize an application, I must implement data caching: not to recompute some data - those heavy on cpu but that don\'t change often.</p>\n\n<p>When playing with the idea, I imagined something like the way win32/MFC manages the windows screen i.e.:</p>\n\n<ul>\n<li>While a part is valid, it is not repainted.</li>\n<li>When a rectangle or a region is invalidated, this part is repainted during the next painting session - launched by the OS.</li>\n</ul>\n\n<p>I was imagining a way to validate and invalidate my cached value, so as to recompute only what is necessary when it is necessary.</p>\n\n<p>Then I read <a href="http://en.wikipedia.org/wiki/Cache_algorithms" rel="nofollow">this wikipedia page about Cache Algorithms</a>, and none of the listed algorithm was using the technique I explained above. So I feel ill at ease, and I need to read some work about caching. </p>\n\n<p>Do you know of some resources that I could rely on before I start implementing my own cache process ?</p>\n', 'Tags': '<reference-request><efficiency><cpu-cache>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-28T10:18:37.300', 'CommentCount': '3', 'AcceptedAnswerId': '9194', 'CreationDate': '2013-01-27T02:47:20.523', 'Id': '9187'},6830:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>On a PC I am implementing an algorithm in which a number from a look table will be chosen randomly, and will be multiplied by 1000 or 10000. Instead of multiplying by 1000 or 10000 I am thinking of simply padding the numbers with the required 0s. This padding will be nothing but some write operations (though random writes) in memory. </p>\n\n<p>Which is more computationally efficient: multiplication or 0 padding?      </p>\n', 'ViewCount': '92', 'Title': 'Which is more computationally efficient: multiplication or 0 padding?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-01T10:45:11.737', 'LastEditDate': '2013-02-01T10:45:11.737', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '9352', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<efficiency><binary-arithmetic>', 'CreationDate': '2013-01-31T11:45:12.727', 'Id': '9350'},6831:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1104', 'Title': 'Can we say DFA is more efficient than NFA?', 'LastEditDate': '2013-05-17T04:01:16.283', 'AnswerCount': '6', 'Score': '8', 'OwnerDisplayName': 'avi', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Body': "<p>I just started reading about theory of computation. If we compare which is more powerful (in accepting strings), both are same. But what about efficiency ? DFA will be fast compared to NFA, since it has only one outgoing edge &amp; there will be no ambiguity. But in case of NFA we have to check all possible cases &amp; that surely takes time. So can we say DFA is more efficient than NFA ? </p>\n\n<p>But, my other part of brain is also thinking that NFA exists only in theory, so we cannot compare it's efficiency with DFA. </p>\n", 'Tags': '<finite-automata><efficiency><nondeterminism>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-17T07:39:06.093', 'CommentCount': '0', 'AcceptedAnswerId': '9393', 'CreationDate': '2013-02-01T14:42:41.990', 'Id': '9389'},6832:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Is there a linear-time algorithm to check that a sequence of characters is a concatenation of palindromes? The only thing that comes to my mind is the naive solution:</p>\n\n<pre><code>1. k = 1\n2. Split string into k substrings (all possibilities) and check\n3. k++\n4. repeat\n</code></pre>\n\n<p>Note: the answer is trivially yes if length 1-strings are defined to be palindromes. Let's assume that this is not the case.</p>\n", 'ViewCount': '1027', 'Title': 'Is there an algorithm for checking if a string is a catenation of palindromes?', 'LastEditorUserId': '2499', 'LastActivityDate': '2013-04-22T17:01:18.820', 'LastEditDate': '2013-04-22T17:01:18.820', 'AnswerCount': '3', 'CommentCount': '9', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><efficiency><strings>', 'CreationDate': '2013-02-06T11:22:25.257', 'FavoriteCount': '2', 'Id': '9540'},6833:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am taking the <a href="https://class.coursera.org/compilers-selfservice/class/index" rel="nofollow">Coursera class</a> on compilers and in the lesson about lexers it is hinted that there is a time-space tradeoff between using non-deterministic finite automaton (NFA) and deterministic finite automaton (DFA) to parse regular expressions. If I understand correctly, the tradeoff is that a NFA is smaller, but is more time consuming to traverse because all possible states have to be regarded at the same time and therefore it is most of the time transformed into a DFA.  Are there any lexers that use NFAs instead of DFAs in "real"-life i.e. some compiler that is used in production and not a just a proof of concept?</p>\n', 'ViewCount': '200', 'Title': 'Are there real lexers that use NFAs directly instead of first transforming them to DFAs?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-05T19:53:06.097', 'LastEditDate': '2013-04-02T07:28:48.933', 'AnswerCount': '4', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6843', 'Tags': '<automata><finite-automata><compilers><efficiency><nondeterminism>', 'CreationDate': '2013-02-12T17:55:33.023', 'Id': '9708'},6834:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am building a system and I have a couple of architectures in mind. I want to have an idea of which architecture is likely to be most performant (quickest).</p>\n\n<p>I can make different decisions like \n1) Do everything in one thread\n2) Separate this part into one thread, that part into another thread since it can run in parallel and so on</p>\n\n<p>Instead of actually building the system in two different ways and seeing how it performs, is there a way to quickly model/prototype the system where I can express the above concepts and the times it takes for inter thread I/O etc (of which I have a very good estimate) and use the model/prototype to estimate which architecture is better?</p>\n', 'ViewCount': '42', 'Title': 'Modeling timing characterists of an architecture', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-26T17:22:25.630', 'LastEditDate': '2013-03-26T17:22:25.630', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6981', 'Tags': '<efficiency><software-engineering>', 'CreationDate': '2013-02-21T21:08:49.160', 'Id': '10020'},6835:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I bought a great puzzle called <a href="http://en.wikipedia.org/wiki/Icosoku" rel="nofollow">Icosoku</a>. Wikipedia describes it as: "The puzzle frame is a blue plastic icosahedron, and the pieces are 20 white equilateral-triangular snap-in tiles with black dots and 12 yellow pins for the corners. The pins are printed with the numbers from 1 to 12, and the triangular tiles have up to three dots in each of their corners. The object of the game is, for any arrangement of the pins, to choose the positions and orientations of the triangles so that the total number of dots on the five joining corners of each pin equals the number of the pin." </p>\n\n<p>I created a computer program which solves this puzzle by randomly choosing any two triangular tiles and then exchanging them (giving each tile a random orientation) if and only if the total discrepency (which can be measured in various ways) between each number on the pins and the total number of dots on the five joining corners of each pin decreases. But I have found that it takes hundreds or even thousands of iterations to finally get a solution for most initial configurations. I would like my program to take less time. Any suggestions?</p>\n', 'ViewCount': '223', 'Title': 'Speeding up a program solving Icosoku', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-24T18:17:14.713', 'LastEditDate': '2013-03-05T07:00:05.383', 'AnswerCount': '4', 'CommentCount': '0', 'AcceptedAnswerId': '10244', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7115', 'Tags': '<efficiency><computer-games>', 'CreationDate': '2013-03-03T01:41:53.140', 'Id': '10216'},6836:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am a beginner in data structures and recently came across a vector implemented on an array, which is extended on demand. Of course the table cannot be extended "in place", we must allocate a new array, then copy elements from the previous one which is a linear operation itself (invoked log(n) times, where n is the number of insertion operations). Are there better implementations of such data structure while preserving constant item access time? For example, how about implementing a concept known from disk file systems - an allocation table; whenever we need to extend our array, an allocation table entry is created for newly reserved memory, without touching previously inserted items. Indexing time could still be constant, if only the allocation table would be implemented wisely (for example with using constant "page" size)\nWhat I\'ve written could be complete nonsense; it\'s just an idea.</p>\n', 'ViewCount': '95', 'Title': 'A vector-like data structure with allocation table; O(1) indexing time required', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-20T20:59:27.867', 'LastEditDate': '2013-03-20T20:59:27.867', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'OwnerDisplayName': 'Mariusz', 'PostTypeId': '1', 'Tags': '<data-structures><efficiency><arrays>', 'CreationDate': '2013-03-20T19:35:45.130', 'Id': '10662'},6837:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In a circular linked list, if an elements needs to be inserted at front [just before the node pointed by head], can be done in O(1) (see the answer <a href="http://stackoverflow.com/questions/1096505/implement-an-algorithm-to-insert-a-node-into-a-circular-linked-list-without-trav">here</a>)</p>\n\n<p>But in a book currently, I have, it is mentioned that it is done in O(n) (the usual method). I also saw few lecture ppts, they all mention the usual method of traversing the list &amp; adding an element.</p>\n\n<p>My question is :</p>\n\n<ol>\n<li><p>In practical scenarios which method is used ?</p></li>\n<li><p>I am about to attend an exam, which consists of MCQs, if above question is asked shall I mark O(n), since that is the standard answer ?</p></li>\n</ol>\n', 'ViewCount': '452', 'Title': 'Complexity of algorithm inserting an element in a circular linked list at the front end', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-23T16:37:13.557', 'LastEditDate': '2013-03-23T15:58:50.753', 'AnswerCount': '1', 'CommentCount': '12', 'AcceptedAnswerId': '10704', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<data-structures><efficiency><linked-lists>', 'CreationDate': '2013-03-23T09:26:46.557', 'Id': '10701'},6838:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am writing a program to store, retrieve and delete "blocks" of data of varying sizes.</p>\n\n<p>The way it currently works is by keeping a database storing the locations of the blocks and the locations of free space in the file.</p>\n\n<p>The file is split into pages such that in each page there are no two free chunk spaces next to each other (During a delete operation, any free chunks which are adjacent are merged into one bigger chunk)</p>\n\n<p>The problem with this is that I am seeing horrible IO performance when removing a bunch of blocks and inserting new ones of different sizes (blocks range from 1k to about 200k and may be written anywhere in the file provided they fit in an existing free chunk in the file. If no such free chunk is found, a new page is created at the end of the file).</p>\n\n<p>Can anyone suggest a way to improve on this, or maybe point me in the right direction?</p>\n', 'ViewCount': '44', 'Title': 'Random file access in a block based file format', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T00:41:38.463', 'LastEditDate': '2013-03-27T12:14:28.923', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7438', 'Tags': '<efficiency><databases><filesystems>', 'CreationDate': '2013-03-27T10:08:47.073', 'Id': '10822'},6839:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was wondering since randomness is embedded in genetic algorithms at almost every level, is there a really fine line between genetic algorithms and pure random search?</p>\n\n<p>Ever since I finished my implementation of a GA , since randomness is present in the mutation function,the initialization part (as well as the reinitialization part) and crossbreeding part as well... other than a encoder which tries to sense of the chromsomes (encoder tailored to make sense of the chromosome in context of the problem) and a fitness function , it feels like genetic algorithms are just random search functions in disguise .</p>\n\n<p>So my question is : are GA implementations just plain old random searches with a shot of memory to make it look like there is some sort of meaningful feedback? </p>\n', 'ViewCount': '108', 'ClosedDate': '2013-04-02T22:15:50.887', 'Title': 'Are genetic algorithms special instances of random search done in an unexpectedly short run-time?', 'LastEditorUserId': '7545', 'LastActivityDate': '2013-04-02T21:07:18.593', 'LastEditDate': '2013-04-02T20:35:30.713', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7545', 'Tags': '<search-algorithms><efficiency><randomness><evolutionary-computing><genetic-algorithms>', 'CreationDate': '2013-04-02T15:57:17.913', 'Id': '10975'},6840:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Note: This is a part of a homework question</p>\n\n<p>Were asked to construct a multi-tape Turing Machine for language {$a^n b^n c^n \\mid n \\geq 0$}</p>\n\n<p>Then it says "Discuss how much time your machines saves over a one-tape DTM using the same algorithm"\nAny hint?</p>\n\n<p>Here\'s my algorithm:</p>\n\n<p>(1) Cut-and-paste c\'s to tape 3</p>\n\n<p>(2) Cut-and-paste b\'s to tape 2</p>\n\n<p>(3) Cross out each triplets, accept if last round cuts all three, reject if there\'s leftover</p>\n\n<p>Which seems like it has a time complexity of $O(n+n+3n)=O(5n)$\nThen how do we determine the time complexity for the one-tape version?</p>\n', 'ViewCount': '197', 'Title': 'How do we determine how much time a multi-tape DTM saves over a one-tape DTM?', 'LastEditorUserId': '6980', 'LastActivityDate': '2013-05-01T11:05:47.477', 'LastEditDate': '2013-05-01T11:05:47.477', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11671', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6980', 'Tags': '<time-complexity><turing-machines><efficiency>', 'CreationDate': '2013-04-05T11:39:26.017', 'Id': '11053'},6841:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>My task is to compare templates of two websites. I am ready with my algorithm. But it takes too much time to give a final answer. \nHere, "template" means the way any page presents its contents.</p>\n\n<p><strong>Example:</strong></p>\n\n<p>Any shopping website have page of any Shoes, that contains,</p>\n\n<pre><code>Images in the left.\nPrice and Size in the right.\nReviews in the bottom.\n</code></pre>\n\n<p>If two websites are of any specific product, then it returns "Both are from same templates". Example, <a href="http://www.jabong.com/Lara-Karen-Full-Sleeve-Black-Polyester-Top-With-Cotton-Lace-196636.html" rel="nofollow">this link</a> and <a href="http://www.jabong.com/Puma-Flash-Ind-Black-Running-Shoes-187831.html" rel="nofollow">this link</a> have the same template.</p>\n\n<p>If one website shows any product and another website shows any category, then it shows "No match".\nExample, <a href="http://www.jabong.com/Lara-Karen-Full-Sleeve-Black-Polyester-Top-With-Cotton-Lace-196636.html" rel="nofollow">this link</a> and <a href="http://www.jabong.com/women/clothing/womens-tops/" rel="nofollow">this link</a> are from different template. </p>\n\n<p>I think that this algorithm requires some optimization, that\'s why I am posting this question in this forum.</p>\n\n<p><strong>My algorithm</strong></p>\n\n<ol>\n<li>Fetch, parse two input URLS and make their <a href="http://en.wikipedia.org/wiki/Document_Object_Model" rel="nofollow">DOM trees</a>.</li>\n<li>Then if any page contains , UL and TABLE , then remove that tag. I done this because, may be two pages contains different number of items.</li>\n<li>Then, I count number of tags in both URLS. say, initial_tag1, initial_tag2.</li>\n<li>Then, I start removing tags that have same position on corresponding pages and same Id and their below subtree, if that tree has number of nodes less than 10.</li>\n<li>Then, I start removing tags that have same position on coresponding pages and same Class name and their below subtree, if that tree has number of nodes less than 10..</li>\n<li>Then, I start removing tags that have no Id ,and No Class name and their below subtree, if that tree has number of nodes less than 10.</li>\n<li>Steps 4, 5, 6 have (N*N) complexity. Here, N, is number of tags. [In this way, in every step DOM tree going to shrink]</li>\n<li>When it comes out from this recursion, then I check final_tag1 and final_tag2.</li>\n<li>If final_tag1 and final_tag2 is less than initial_tag1*(0.2) and initial_tag2*(0.2) then I <strong>can</strong> say that <code>Two URL matched</code>, otherwise <code>not</code>.</li>\n</ol>\n\n<p>I wrote my code in <em>Java</em> using <em>Jsoup</em> and <em>Selenium</em>. I <a href="http://stackoverflow.com/questions/15718235/optimized-algorithm-to-compare-templates-of-two-urls">asked before on Stack Overflow</a>, but the answers did not help me.</p>\n\n<p>I think a lot about this algorithm, and I found that removing node from DOM tree is pretty slow process. This may be the culprit for slowing this algorithm.</p>\n\n<p>I discussed with some geeks, and </p>\n\n<blockquote>\n  <p>they said that use a score for every tag instead of removing them, and add them , and \n  at the end return (score I Got)/(accumulatedPoints) or something similar, and on the \n  basis of that you decide two websites are either similar or not.</p>\n</blockquote>\n\n<p>But I didn\'t understand this. So can you explain this statement, or can you give any other algorithm that solves this problem efficiently?</p>\n', 'ViewCount': '102', 'Title': 'Optimized algorithm to compare templates of two websites', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T13:36:52.260', 'LastEditDate': '2013-04-07T13:36:52.260', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2715', 'Tags': '<algorithms><efficiency>', 'CreationDate': '2013-04-07T10:07:13.130', 'Id': '11094'},6842:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The problem I have is like this bin packing problem, but instead I have $n$ bins and a collection of items with discrete masses. I need to put at least $m$ kg of stuff in each bin.</p>\n\n<p>Is there an efficient way of doing this? Is there a way that will assure there is approximately the same amount in each bin? Does having a good guess at the probability distribution of the masses help?</p>\n\n<p><strong>More explicitly:</strong></p>\n\n<p>I have $q$ objects $\\{o_1...o_q\\}$, each has a size $w(o_i) \\in \\mathbb{N}$.</p>\n\n<p>I need to find a collection of $n$ disjoint bins $B = \\{b_1...b_n\\}$ containing the objects such that</p>\n\n<p>$$\\forall b_i \\in B: \\sum_{o \\in b_i}w(o) &gt; m$$</p>\n\n<p>for some $m$. When it is possible that is.</p>\n', 'ViewCount': '53', 'Title': 'Relaxed Bin Packing Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:55:04.787', 'LastEditDate': '2013-04-08T14:55:04.787', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7641', 'Tags': '<algorithms><efficiency><packing>', 'CreationDate': '2013-04-08T11:11:38.520', 'Id': '11139'},6843:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How would you explain why the Fast Fourier Transform is faster than the Discrete Fourier Transform, if you had to give a presentation about it for the general (non-mathematical) public?</p>\n', 'ViewCount': '978', 'Title': 'Explaining why FFT is faster than DFT for the general public?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-18T11:54:12.870', 'LastEditDate': '2013-04-18T08:25:48.223', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<algorithms><efficiency><education><didactics><fourier-transform>', 'CreationDate': '2013-04-17T22:22:57.513', 'Id': '11371'},6844:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've ran some tests and found that Shellsort runs much faster on ordered and reversed lists compared to random lists and almost ordered lists.</p>\n\n<pre><code>Results:\n        Random Reverse Order AlmostOrder\n  time    24      5      4        29\n</code></pre>\n\n<p>The problem that is confusing me is that Shellsort performs insertion sorts on lists separated by gaps, and insertion sort only runs very fast on ordered lists, not reversed lists.</p>\n\n<p>So my question is why does Shellsort work well on ordered and reversed lists when it uses insertion sort and insertion sort doesn't work well on reversed lists?</p>\n", 'ViewCount': '160', 'Title': 'Why does Shellsort work well on Sorted and Reverse ordered lists?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:52:46.560', 'LastEditDate': '2013-05-19T14:52:46.560', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'clay', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-05-18T02:16:41.193', 'FavoriteCount': '0', 'Id': '12124'},6845:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '710', 'Title': 'What are flops and how are they benchmarked?', 'LastEditDate': '2013-06-11T22:04:30.953', 'AnswerCount': '4', 'Score': '1', 'OwnerDisplayName': 'Vincent Warmerdam', 'PostTypeId': '1', 'OwnerUserId': '8627', 'Body': '<p>Apple has just proudly stated that their new mac pro will be able to give up to 7 teraflops of computing power. Flops stands for Floating Point Operations Per Second. How exactly is this benchmarked though? Certain floating point operations are much heavier than others, so how exactly would a FLOP be a benchmark for computing power?  </p>\n', 'Tags': '<computer-architecture><efficiency><benchmarking>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-06-30T14:30:31.150', 'CommentCount': '1', 'AcceptedAnswerId': '12615', 'CreationDate': '2013-06-10T22:10:32.837', 'Id': '12606'},6846:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm looking for a data structure that can work as a priority queue with reasonable maintenance complexities (like $O(\\log n)$ for insertion and deletion) and that has a theoretical unbounded limit for its number of elements (like a tree structure, that is bounded only by the computer's available memory, and unlike a traditional heap, that uses an static array, which is too costly to augment).</p>\n\n<p>The reason is that I'm implementing a program that makes use of a priority queue and I don't know <em>a priori</em> how many elements I'm going to insert in this queue at once, so sometimes I'm out of space to add another element.</p>\n\n<p>There is no way to estimate this number, and to create a huge array to support a static type of queue is a terrible option, as maybe not even a half of it will be used and I'll be short on memory to allocate other objects.</p>\n\n<p>I've heard of something like a <em>Dynamic Heap</em> (or something in the like) that is some sort of linked list of arrays, whose elements are dynamically allocated when needed, but I'm not sure this is the best strategy to follow, moreover, I would like to know if there were other options.</p>\n\n<hr>\n\n<p>Just for the record, I'm implementing a Branch-and-Bound algorithm for solving a linear integer optimization problem, with each node stored on the queue being the abstraction of an active node on the algorithm. The number of active nodes cannot be estimated at any time, so a theoretically unbounded queue would help a lot.</p>\n", 'ViewCount': '166', 'Title': 'Priority queue with ubounded number of elements (i.e., with dynamic storage)', 'LastEditorUserId': '8399', 'LastActivityDate': '2013-06-13T14:11:06.613', 'LastEditDate': '2013-06-13T02:16:56.120', 'AnswerCount': '3', 'CommentCount': '9', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8399', 'Tags': '<data-structures><efficiency><priority-queues>', 'CreationDate': '2013-06-12T14:29:55.157', 'Id': '12637'},6847:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '141', 'Title': 'Largest reported speedup for a parallel computation?', 'LastEditDate': '2013-06-23T16:17:43.197', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5323', 'FavoriteCount': '1', 'Body': u'<p>A parallel implementation of a computation is usually compared to a sequential version of the same computation.  The ratio of the time taken by the sequential version to the time taken by the parallel version is called the <em>speedup</em>.  So if 8 cores run their smaller parts of the computation in 2 time units, and one core runs the whole computation in 8 time units, then the speedup is 4.</p>\n\n<blockquote>\n  <p>What is the largest speedup reported for a real computation?</p>\n</blockquote>\n\n<p>It is possible to reach essentially infinite speedup in a search problem, since one of the parallel pieces of the search space may lead to a fast solution by that parallel instance, while the sequential solution has to work through the entire search space to get to that point.  More generally, I want to exclude any problem where one of the parallel processes can reach a shortcut.  So I am only interested in computations where the amount of work done by the parallel processes is the same as done by the sequential process.  This is common in solving PDEs by grid methods, or in discrete event simulation.\nSo with $n$ processors, one should never get more than $n$ speedup for these kinds of problems.</p>\n\n<p>I would also like to exclude embarrassingly parallel problems like parallel rendering, since there one really has a vast number of independent tiny problems.  I am interested in problems where it is not possible to partition the computation into strictly disjoint pieces.</p>\n\n<p>For a large speedup, one has to have many processors.  Given the restrictions on scope that I have conveniently labelled as "real computations", this question is then essentially about how efficiently the very large processor arrays that exist have been programmed.</p>\n\n<p>I am aware of reported speedups of ~500 using arrays of GPUs, but surely larger speedups exist.</p>\n\n<hr>\n\n<p><em>Edit:</em> To address some of the comments, I dug up some further motivation, which will hopefully be precise enough for the tastes of those more mathematically inclined.  This is quite different in style from the above, so I append it here as a postscript.</p>\n\n<p>For $n$ iid random variables $X_1, X_2,\\dots, X_n$ with mean $\\mu$,\ndenote their maximum by $X_{(n)}$ and their sum by $S_n$.\nO\'Brien has shown that $X_{(n)}/S_n \\to 0$ <a href="http://en.wikipedia.org/wiki/Convergence_of_random_variables#Almost_sure_convergence" rel="nofollow">almost surely</a> as $n \\to \\infty$ iff $\\mu &lt; \\infty$.</p>\n\n<p>Letting $X_i$ be the time taken by the $i$-th processor to complete its task, and assuming that there is a timeout/recompute mechanism to ensure that $\\mu$ is finite, this hints that the inverse of the speedup should be essentially unbounded.  (This is not necessarily the case: the techniques used may not carry over to the inverse, so I have to leave this a bit vague.)</p>\n\n<p>This is a nice theoretical prediction, and the question arises: <em>is this prediction borne out in practice?</em>  Or do implicit dependencies or diverging behaviours of the different processors (i.e. a breakdown in the iid assumption) tend to curtail this supposedly unbounded increase?</p>\n\n<p>The iid case corresponds to the embarrassingly parallel case.  Where the processors have to synchronize, independence breaks down.</p>\n\n<p>My question can therefore also be rephrased as: how badly does non-negligible dependence between parts of a computation as seen in practice affect the large-scale speedups that have been demonstrated?  Given that <a href="http://math.stackexchange.com/a/427162/3362">the bounds for expectation of the maximum in the non-independent case are quite weak</a>, some pointers to empirical data would be useful.</p>\n\n<ul>\n<li>G. L. O\'Brien, <em><a href="http://www.jstor.org/stable/3213043" rel="nofollow">A Limit Theorem for Sample Maxima and Heavy Branches in Galton-Watson Trees</a></em>, Journal of Applied Probability <strong>17</strong> 539\u2013545, 1980.</li>\n</ul>\n', 'Tags': '<reference-request><efficiency><parallel-computing>', 'LastEditorUserId': '5323', 'LastActivityDate': '2013-06-24T07:04:13.960', 'CommentCount': '18', 'AcceptedAnswerId': '12829', 'CreationDate': '2013-06-22T07:51:11.100', 'Id': '12826'},6848:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="http://en.wikipedia.org/wiki/Selection_sort#Comparison_to_other_sorting_algorithms">It is written on Wikipedia</a> that "... selection sort almost always outperforms bubble sort and gnome sort." Can anybody please explain to me why is selection sort considered faster than bubble sort even though both of them have:  </p>\n\n<ol>\n<li><p><strong>Worst case time complexity</strong>: $\\mathcal O(n^2)$  </p></li>\n<li><p><strong>Number of comparisons</strong>:      $\\mathcal O(n^2)$</p></li>\n<li><p><strong>Best case time complexity</strong> :  </p>\n\n<ul>\n<li>Bubble sort: $\\mathcal O(n)$</li>\n<li>Selection sort: $\\mathcal O(n^2)$</li>\n</ul></li>\n<li><p><strong>Average case time complexity</strong> :  </p>\n\n<ul>\n<li>Bubble sort: $\\mathcal O(n^2)$</li>\n<li>Selection sort: $\\mathcal O(n^2)$ </li>\n</ul></li>\n</ol>\n', 'ViewCount': '5657', 'Title': 'Why is selection sort faster than bubble sort?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-09T19:48:10.230', 'LastEditDate': '2013-07-06T14:23:05.397', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8951', 'Tags': '<algorithms><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-07-06T09:33:35.463', 'Id': '13106'},6849:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '628', 'Title': 'What is the most power/energy efficient sorting algorithm?', 'LastEditDate': '2013-08-01T12:57:53.000', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7858', 'FavoriteCount': '1', 'Body': '<p>I am writing a Android phone application that needs to be very power efficient, and I would like to use the most power efficient sorting algorithm. I will implement it in C for extra power efficiency. What algorithm is the most power efficient algorithm for sorting arbitrary text strings?</p>\n', 'Tags': '<algorithms><sorting><efficiency><power-consumption>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-01T13:10:14.323', 'CommentCount': '6', 'AcceptedAnswerId': '13549', 'CreationDate': '2013-07-31T22:27:29.337', 'Id': '13548'},6850:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '242', 'Title': 'What are efficient data structures for inserting and accessing elements?', 'LastEditDate': '2013-08-05T20:40:21.293', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9085', 'FavoriteCount': '1', 'Body': '<p>Is there a data structure to keep a list of elements (not necessarily sorted) that performs the Access (by index) and Insert operations in a reasonable asymptotic time?</p>\n\n<p>When I say "reasonable time", I mean that it should be equal to or better than $O(\\log N)$.</p>\n\n<p>I\'m looking for a structure similar to a <a href="https://en.wikipedia.org/wiki/Dynamic_array" rel="nofollow">dynamic array</a>, but I need a better behavior in the Insertion operation. When the array size grows, the time grows exponentially (Except at the end of the array).</p>\n', 'Tags': '<data-structures><efficiency>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-06T09:02:13.103', 'CommentCount': '4', 'AcceptedAnswerId': '13602', 'CreationDate': '2013-08-04T16:00:01.230', 'Id': '13598'},6851:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '117', 'Title': 'What is the impact of synchronisation overhead on parallel speedup?', 'LastEditDate': '2013-08-06T09:11:51.160', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8767', 'FavoriteCount': '0', 'Body': '<p>When implementing a parallel version of an algorithm, what is the impact of synchronization delays on speedup efficiency? Does this depend on the platform used?</p>\n\n<p>Is coarse-grained parallelism better than fine-grained parallelism in certain situations?</p>\n', 'ClosedDate': '2013-08-11T12:10:46.710', 'Tags': '<efficiency><parallel-computing><concurrency><synchronization>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-06T09:11:51.160', 'CommentCount': '5', 'CreationDate': '2013-08-05T12:48:59.380', 'Id': '13611'},6852:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to keep a collection on integers in the range 0 to 65535 so that I can quickly do the following:</p>\n\n<ul>\n<li>Insert a new integer</li>\n<li>Insert a range of contiguous integers</li>\n<li>Remove an integer</li>\n<li>Remove all integers below an integer</li>\n<li>Test if an integer is present</li>\n</ul>\n\n<p>My data has the property that it often contains runs of integers in the collection.  For example, the collection might at one point in time be:</p>\n\n<pre><code>{ 121, 122, 123, 124, 3201, 3202, 5897, 8912, 8913, 8914, 18823, 18824, 40891 }\n</code></pre>\n\n<p>The simplest approach is just to use a balanced binary tree like the C++ std::set, however, using that, I am not leveraging the fact that I often have runs of numbers.  Perhaps it would be better to store a collection of ranges?  But that means a range needs to be able to be broken up if an integer in its middle is removed, or joined together if the space between two ranges in filled in.</p>\n\n<p>Are there any existing data structures that would be well suited for this problem?</p>\n', 'ViewCount': '402', 'Title': 'What data structure would efficiently store integer ranges?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-26T17:37:25.270', 'LastEditDate': '2013-08-26T11:07:37.533', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7284', 'Tags': '<data-structures><efficiency><search-trees><integers>', 'CreationDate': '2013-08-22T20:22:55.240', 'Id': '13874'},6853:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have two polynomials $f(x)$ and $g(x)$ and I somehow represent their coefficients.\nI have a couple of ways to hold a polynomial depending on how many significant coefficients the polynomial has.\nI want to determine the amount of significant coefficients in the results of\n$f(x) + g(x)$ ,$f(x) \\cdot g(x)$ , $f(x) - g(x)$ etc. .</p>\n\n<p>But I'd like to do it before I create the object that holds them, is there some efficient way of doing this without calculating the result twice?</p>\n\n<p>I can assume that I know the current rank and number of elements in $f(x)$ and $g(x)$</p>\n\n<p>If this is not possible knowing that the new polynomial's non-trivial coefficients will be at least half of the rank will suffice, but I'm unsure how to do it as well.</p>\n\n<p>I did try to apply various heuristics but didn't come up with something consistent and fast. </p>\n", 'ViewCount': '67', 'Title': 'Calculate the number of elements after multiplying/adding two polynomials', 'LastActivityDate': '2013-08-28T21:13:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14006', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8709', 'Tags': '<algorithms><time-complexity><optimization><efficiency><arithmetic>', 'CreationDate': '2013-08-28T19:13:48.453', 'Id': '13997'},6854:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The energy function in a Hopfield network to determine whether it has converged seems to be the major sink of computational time and makes the Hopfield network run very slowly. Is there a fast substitute for it? </p>\n', 'ViewCount': '20', 'Title': 'Fast energy function substitute for Hopfield network?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:35:57.190', 'LastEditDate': '2013-09-16T08:35:57.190', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10158', 'Tags': '<algorithms><artificial-intelligence><efficiency><neural-networks><heuristics>', 'CreationDate': '2013-09-15T18:54:05.840', 'Id': '14335'},6855:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a Set $S$ of objects, a set $U$ of users and a map $c: U \\rightarrow S^{\\prime}$, where<br>\n$S^{\\prime} \\subset S$ and $\\emptyset \\notin S^{\\prime}$.</p>\n\n<p>Every time I add a new entry to $c$, i.e. adding a new user $u$ with her associated subset of $S^{\\prime}$, I need to get the list of other users with whom she covers $S$. However, I don't need to get all possible covers. I only need to get with how many $x$ other users she covers $S$, at most. In other words, I need to find who's complementing each other to cover $S$ 2 by 2, 3 by 3, ... , $x$ by $x$.</p>\n\n<p>Is there an efficient method to do it? Or an efficient data structure? If no, is there any trick I can make to get good results? e.g. encode the elements of $S$ and do some sort of mathematical operations? Is any efficient solution possible at all?</p>\n\n<p>Please let me know if I haven't explained my problem well enough.</p>\n", 'ViewCount': '25', 'Title': 'Finding users covering a set x by x', 'LastActivityDate': '2013-09-19T00:29:01.480', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10207', 'Tags': '<data-structures><efficiency><sets>', 'CreationDate': '2013-09-18T23:36:33.543', 'Id': '14426'},6856:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2654', 'Title': 'Factorial algorithm more efficient than naive multiplication', 'LastEditDate': '2013-09-20T08:18:53.507', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7307', 'FavoriteCount': '2', 'Body': "<p>I know how to code for factorials using both iterative and recursive (e.g. <code>n * factorial(n-1)</code> for e.g.). I read in a textbook (without been given any further explanations) that there is an even more efficient way of coding for factorials by dividing them in half recursively. </p>\n\n<p>I understand why that may be the case. However I wanted to try coding it on my own, and I don't think I know where to start though. A friend suggested I write base cases first. and I was thinking of using arrays so that I can keep track of the numbers... but I really can't see any way out to designing such a code.</p>\n\n<p>What kind of techniques should I be researching?</p>\n", 'Tags': '<algorithms><efficiency><arithmetic>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-16T08:26:18.543', 'CommentCount': '0', 'AcceptedAnswerId': '14476', 'CreationDate': '2013-09-20T02:00:56.260', 'Id': '14456'},6857:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1826', 'Title': 'Are algorithms (and efficiency in general) getting less important?', 'LastEditDate': '2013-10-13T19:34:48.053', 'AnswerCount': '8', 'Score': '24', 'PostTypeId': '1', 'OwnerUserId': '10637', 'FavoriteCount': '10', 'Body': "<p>Since buying computation power is much affordable than in the past, are the knowledge of algorithms and being efficient getting less important? It's clear that you would want to avoid an infinite loop, so, not everything goes. But if you have better hardware, could you have somehow worse software?</p>\n", 'Tags': '<efficiency>', 'LastEditorUserId': '10637', 'LastActivityDate': '2013-10-24T10:29:58.533', 'CommentCount': '9', 'AcceptedAnswerId': '15018', 'CreationDate': '2013-10-12T15:53:02.207', 'Id': '15017'},6858:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '152', 'Title': 'What do I need to know about algorithms?', 'LastEditDate': '2013-10-14T07:49:48.907', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '2', 'Body': '<p>Drawing from <a href="http://cs.stackexchange.com/questions/15017/are-algorithms-and-efficiency-in-general-getting-less-important">Are algorithms (and efficiency in general) getting less important?</a>, I get that algorithms are more important than ever, but I need to know anything about them other than their complexities?</p>\n\n<p>I would expect to use algorithms out of a library of some sort that has been written by people that are much much more knowledgeable than me, and have been thoroughly tested (most of the STL comes to mind here, but also boost, and probably every other library that someone would care to name).</p>\n\n<p>Thus, unless I am a library maintainer/implementer of some sort, do I need to know anything about these algorithms?</p>\n', 'Tags': '<algorithms><efficiency>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-14T07:49:48.907', 'CommentCount': '2', 'AcceptedAnswerId': '16052', 'CreationDate': '2013-10-13T18:59:13.733', 'Id': '16051'},6859:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $S$ be a set of $n$ integers. Consider the following <em>weighted permutations problem</em>.</p>\n\n<blockquote>\n  <p>Let $m&lt;n$ be an integer. What is an efficient algorithm to enumerate all subsets of $m$ integers of $S$ such that they are listed in order of the sum of the integers in each subset?</p>\n</blockquote>\n\n<p>Each subset is a permutation, and each permutation has a total weight that is the sum of the integers in the permutation.</p>\n\n<p>The idea is to come up with an algorithm that is not the trivial algorithm of enumerating all subsets, and then sorting them, i.e. more of a "streaming" type algorithm. That is, efficiency in terms not so much of time but of "small" space.</p>\n\n<p>Maybe this is published somewhere in the literature although I have not seen it.</p>\n', 'ViewCount': '89', 'Title': 'Enumerating weighted permutations in sorted order problem', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-18T21:03:07.127', 'LastEditDate': '2013-11-18T21:03:07.127', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><reference-request><sorting><efficiency><enumeration>', 'CreationDate': '2013-11-05T17:32:02.033', 'FavoriteCount': '1', 'Id': '16744'},6860:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I participated in one algorithmic competition. I got stuck in one problem, I am asking the same here.</p>\n\n<p><strong>Problem Statement</strong></p>\n\n<p>XOR-sum array is to XOR all the numbers of that sub-array.\nAn array is given to you, you have to add all possible such XOR-sub-array.</p>\n\n<p><strong>Example</strong></p>\n\n<p><strong>Input</strong></p>\n\n<p>Array :- 1 2</p>\n\n<p><strong>Output</strong> :- 6</p>\n\n<p><strong>Explanation</strong></p>\n\n<p>F(1, 1) = A[1] = 1, F(2, 2) = A[2] = 2 and F(1, 2) = A[1] XOR A[2] = 1 XOR 2 = 3. Hence the answer is 1 + 2 + 3 = 6.</p>\n\n<p>I found an $O(N^2)$ solution, but it was too inefficient one and wasn\'t accepted in the competition.</p>\n\n<p>I saw the best solution of this problem <a href="http://www.codechef.com/viewplaintext/2928504" rel="nofollow">on Code Chef</a>. But in this code, I didn\'t understand below module, please help me to understand that.</p>\n\n<pre><code>for (int i=0, p=1; i&lt;30; i++, p&lt;&lt;=1) {\n    int c=0;\n    for (int j=0; j&lt;=N; j++) {\n        if (A[j]&amp;p) c++;\n    }\n    ret+=(long long)c*(N-c+1)*p;\n}\n</code></pre>\n\n<p>In pseudocode:</p>\n\n<ul>\n<li>Set $r = 0$</li>\n<li>For $i$ from $0$ to $29$, let $p = 2^i$.\n<ul>\n<li>Set $c=0$</li>\n<li>Iterate over the array $A$. For each element $A[j]$:\n<ul>\n<li>If $A[j] \\mathbin\\&amp; p \\ne 0$ then increment $c$. ($\\&amp;$ is bitwise and.)</li>\n</ul></li>\n<li>Add $c \\times (N-c+1) \\times p$ to $r$</li>\n</ul></li>\n</ul>\n', 'ViewCount': '374', 'Title': 'Algorithm to add sum of every possible xor-sum sub-array', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-21T12:19:12.663', 'LastEditDate': '2013-11-21T12:19:12.663', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2715', 'Tags': '<algorithms><efficiency>', 'CreationDate': '2013-11-20T10:40:38.763', 'Id': '18194'},6861:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a multi-label classification problem, in which each input sample has a set of zero or more output labels.</p>\n\n<p>I have a multi-class classifier which, for every input sample, returns a certain score for each of the output labels.</p>\n\n<p>One way to use this multi-class classifier for multi-label classification is to calculate a <strong>threshold score</strong>, and select for the output all labels that get a score above the threshold.</p>\n\n<p>My question is: <strong>what is the most efficient way to calculate the threshold score?</strong></p>\n\n<p>Currently we use the following scheme:</p>\n\n<ul>\n<li>Train the multi-class classifier on 90% of the training set.</li>\n<li>Run the base multi-class classifier on the remaining 10% of the training set (which we call "development set"). Keep all the scores in a large array.</li>\n<li>Create a set of all scores that are returned by the base multi-class classifier for any input and any label. Call the set $S$.</li>\n<li>For every score $s \\in S$, and for every input sample in the development set, calculate the positive labels returned when the threshold is $s$. Compare to the gold-standard. Calculate the precision, recall and $F1$.</li>\n<li>Select the score $s_max$ that yielded the maximum $F1$.</li>\n</ul>\n\n<p>This is not efficient because there are many different scores and the method is linear in the number of different scores.</p>\n\n<p>We plotted the $F1$ against the scores and noticed that this is a concave function - it has a single maximum. So, we thought of using binary search to find the maximum point. But, we are not sure that the function will always be concave.</p>\n\n<p>What advice do you have for making this process more efficient?</p>\n', 'ViewCount': '39', 'Title': 'Calculating the classification threshold efficiently', 'LastActivityDate': '2013-12-04T10:16:54.473', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<time-complexity><machine-learning><efficiency>', 'CreationDate': '2013-12-04T10:16:54.473', 'Id': '18602'},6862:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need a data structure which can include millions of elements, minimum and maximum must be accesable in constant time and inserting and erasing element time complexity must be better than linear.</p>\n', 'ViewCount': '202', 'Title': 'Which data structure to use for accessing min/max in constant-time?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-07T00:05:10.370', 'LastEditDate': '2014-01-06T06:26:13.367', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12619', 'Tags': '<data-structures><efficiency>', 'CreationDate': '2014-01-05T23:15:52.053', 'Id': '19518'},6863:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When studying operation systems, is context switch time included in CPU utilization time? For example, if 5 percent of CPU time is wasted to context switching, is CPU utilization 95 percent?</p>\n', 'ViewCount': '44', 'Title': 'How do context switches factor into CPU utilization?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-10T19:47:17.553', 'LastEditDate': '2014-01-12T21:52:43.070', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11579', 'Tags': '<operating-systems><efficiency>', 'CreationDate': '2014-01-12T17:18:17.283', 'Id': '19670'},6864:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I am looking for implementation of the set data type. That is, we have to</p>\n\n<ul>\n<li>maintain a dynamic subset $S$ (of size $n$) from the universe $U = \\{0, 1, 2, 3, \\dots , u \u2013 1\\}$ of size $u$ with</li>\n<li>operations <code>insert(x)</code> (add an element <code>x</code> to $S$) and <code>find(x)</code> (checks whether element <code>x</code> is a member of $S$).</li>\n</ul>\n\n<p>I don't care about other operations. For orientation, in applications I'm working with we have $u \\approx 10^{10}$.</p>\n\n<p>I know of implementations that provide both operations in time $O(1)$, so I worry mostly about the size of data structure. I expect <em>billions</em> of entries but want to avoid swapping as much as possible.</p>\n\n<p>I am willing to sacrifice runtime if necessary. Amortised runtime of $O(\\log n)$ is what I can admit; expected runtimes or runtimes in $\\omega(\\log n)$ are not admissable.</p>\n\n<p>One idea I have is that if $S$ can be represented as a union of ranges <code>[xmin, xmax]</code>, then we will be able to save on storage size with the price of some performance decrease. Also, some other data patterns are possible, like <code>[0, 2, 4, 6]</code>.</p>\n\n<p>Could you please point me to data structures which can do something like that?</p>\n", 'ViewCount': '155', 'Title': 'Looking for a set implementation with small memory footprint', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-03T09:34:06.393', 'LastEditDate': '2014-01-31T08:03:57.357', 'AnswerCount': '2', 'CommentCount': '11', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11850', 'Tags': '<data-structures><efficiency><space-complexity><sets><dictionaries>', 'CreationDate': '2014-01-29T16:42:55.737', 'Id': '20070'},6865:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><strong>Background</strong></p>\n\n<p>I need to find a largest set of non-overlapping axis-parallel squares, out of a given collection of candidate squares.</p>\n\n<p>This problem is NP-complete. Many papers suggest approximation algorithms (see <a href="https://en.wikipedia.org/wiki/Maximum_disjoint_set" rel="nofollow">Maximum Disjoint Set in Wikipedia</a>), but I need an exact algorithm. </p>\n\n<p>My current solution uses the following divide-and-conquer strategy:</p>\n\n<ul>\n<li>Calculate all horizontal and vertical lines that pass through corners of the candidate squares. Each such line separates the candidates into three groups: candidates that are entirely at one side of the line, candidates that are entirely at the other side of the line, and candidates that are intersected by the line. Now there are two cases:\n<ul>\n<li><em>Easy Case</em>: There is a separator line $L$ that does not intersect any candidate square. Then, recursively calculate the maximum-disjoint-set among the squares on one side of $L$, recursively calculate the maximum-disjoint-set among the squares on the other side of $L$, and return the union of these two sets. The separator line guarantees that the union is still a disjoint set.</li>\n<li><em>Hard Case</em>: All separator lines intersect one or more candidate squares. Select one of the separator lines, $L$; suppose that $L$ intersects $k$ squares. Calculate all $2^k$ subsets of these intersected squares. For each subset $X$ that is in itself a disjoint set, calculate the maximum-disjoint-set recursively as in the Easy Case, under the assumption that $X$ is in the set. I.e., recursively calculate the maximum-disjoint-set among the squares on one side of $L$ that do not intersect $X$, recursively calculate the maximum-disjoint-set among the squares on the other side of $L$ that do not intersect $X$, and calculate the union of these two sets with $X$. Out of all $2^k$ unions, return the largest one.</li>\n</ul></li>\n</ul>\n\n<p><strong>Question</strong></p>\n\n<p>My question is: <em>What is the best way to select the separator line $L$</em>?</p>\n\n<p>There are two conflicting considerations: On one hand, we want $L$ to intersect as few squares as possible, so that the power set is not too large. On the other hand, we want $L$ to separate the candidate squares to subsets of balanced size, preferrably equal size, so that the recursion ends as fast as possible. What is the best way to balance these conflicting considerations?</p>\n\n<p>EDIT: <strong>Additional details</strong></p>\n\n<p>My current heuristic is to pick the separator line that intersects the least number of squares. This heuristic allows the algorithm to process input sets with up to $n=30$ candidates, in several seconds. The optimal solution in these cases has about 10 squares. In general, the number of squares in the optimal solution is near $2\\cdot\\sqrt{n}$.</p>\n\n<p>When the input grows beyond 30 candidates, the running time becomes much slower (several minutes and more). My goal is to find a heuristic that will allow me to process larger sets of candidates.</p>\n', 'ViewCount': '143', 'Title': 'A heuristic for finding a maximum disjoint set', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-02-04T14:43:12.340', 'LastEditDate': '2014-01-31T10:01:52.057', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '20140', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><efficiency><heuristics>', 'CreationDate': '2014-01-30T18:29:37.270', 'Id': '20126'},6866:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a set of line segments, how do we identify a subset of maximal cardinality where all line segments are pairwise non-intersecting?</p>\n\n<p>Brute force we would get $2^n$ sets to check where $n$ is the number of line segments, so that isn\'t viable. Anyone got a bright idea how the do this efficiently? I tried doing it this way: remove a line segment that intersects with the most other line segments, iterate until no line segments intersect anymore; but that didn\'t work.</p>\n\n<hr>\n\n<p><a href="http://jsfiddle.net/afaucogney/RwNXL/" rel="nofollow">Here</a> is a "ready to help me" place, where you can test your solution; it visualizes the set of line segments.</p>\n\n<p>To try it out, please implement your attempt in the following function on the linked site:</p>\n\n<pre><code>function showAnalysis() {\n    debug("Just do it");\n}\n</code></pre>\n\n<p>and then click on the top canvas. The fiddle generates randoms segments in the top canvas, and the bottom canvas is the place where an optimal subset will be displayed.</p>\n', 'ViewCount': '117', 'Title': 'Efficiently pick a largest set of non-intersecting line segments', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-07T07:22:28.420', 'LastEditDate': '2014-02-07T07:22:28.420', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '1', 'OwnerDisplayName': 'Anthony', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><np-hard><efficiency>', 'CreationDate': '2014-01-22T14:04:07.477', 'Id': '20263'},6867:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is a popular question: </p>\n\n<blockquote>\n  <p>What is the most efficient (in time complexity) way to sort 1 million 32-bit integers? </p>\n</blockquote>\n\n<p>Most answers seem to agree that one of the best ways would be to use radix sort since the number of bits in those numbers is assumed to be constant. This is also a very common thought exercise when CS students are first learning non-comparison based sorts. However, what I haven\'t seen described in detail (or at least clearly) is how to optimally choose the radix (or number of buckets) for the algorithm.</p>\n\n<p><a href="http://www.quora.com/Sorting-Algorithms/What-is-the-most-efficient-way-to-sort-a-million-32-bit-integers" rel="nofollow">In this observed answer</a>, the selection of the radix/number of buckets was done empirically and it turned out to be $2^8$ for 1 million 32-bit integers. I\'m wondering if there is a better way to choose that number? In "Introduction to Algorithms" (p.198-199) it explains Radix\'s run-time should be $\\Theta(d(n+k))$ (d=digits/passes, n=number of items, k=possible values). It then goes further and says that given n b-bit numbers, and any positive integer $r \\leq b$, radix-sort sorts the number in $\\Theta((b/r)(n+2^r))$ time. It then says: </p>\n\n<blockquote>\n  <p>If $b \\geq \\lfloor \\lg(n) \\rfloor$, choosing $r = \\lfloor \\lg(n) \\rfloor$ gives the best time to within a constant factor.</p>\n</blockquote>\n\n<p>But, if we choose $r = \\lg(10^6) =20$, which is not $8$ as the observed answer suggests.</p>\n\n<p>This tells me that I\'m very likely misinterpreting the "choosing of $r$" approach the book is suggesting and missing something (very likely) or the observed answer didn\'t choose the optimal value.</p>\n\n<p>Could anyone clear this up for me?</p>\n', 'ViewCount': '130', 'Title': 'Choosing the optimal radix/number-of-buckets when sorting n-bit integers using radix sort', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T16:28:50.420', 'LastEditDate': '2014-02-06T16:28:50.420', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'sigue', 'PostTypeId': '1', 'Tags': '<algorithms><sorting><efficiency>', 'CreationDate': '2014-01-13T03:18:00.663', 'FavoriteCount': '1', 'Id': '21385'},6868:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I understand the basis of A* as being a derivative of Dijkstra, however, I recently found out about D*. From Wikipedia, I can understand the algorithm. What I do not understand is why I would use D* over Dijkstra. To my understanding, Dijkstra gives a best path and D* works backwards from the end goal, but unlike A* it seems to do many calculations, so it doesn't seem as efficient.</p>\n", 'ViewCount': '116', 'Title': 'Why choose D* over Dijkstra?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-05T09:28:49.120', 'LastEditDate': '2014-03-05T07:08:16.793', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22299', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '15288', 'Tags': '<algorithms><graphs><search-algorithms><efficiency>', 'CreationDate': '2014-03-05T00:30:16.657', 'Id': '22284'},6869:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What will happen if sequence number is not used in an ARQ protocol (e.g, Go-back-N, selective repeat)?</p>\n', 'ViewCount': '23', 'ClosedDate': '2014-04-02T16:34:43.273', 'Title': 'ARQ Protocols and sequence numbers', 'LastActivityDate': '2014-03-14T04:59:29.223', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15652', 'Tags': '<efficiency><computer-networks>', 'CreationDate': '2014-03-13T16:41:38.160', 'Id': '22592'},6870:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am trying to get an idea of how the <a href="https://en.wikipedia.org/wiki/Aks_primality_test" rel="nofollow">AKS primality test</a> should be interpreted as I learn about it, e.g. a corollary for proving that PRIMES \u2286 P, or an actually practical algorithm for primality testing on computers.</p>\n\n<p>The test has polynomial runtime but with high degree and possible high constants. So, in practive, at which $n$ does it surpass other primality tests?\nHere, $n$ is the number of digits of the prime, and "surpass" refers to the approximate runtime of the tests on typical computer architectures.</p>\n\n<p>I am interested in functionally comparable algorithms, that is deterministic ones that do not need conjectures for correctness.</p>\n\n<p>Additionally, is using such a test over the others practical given the test\'s memory requirements?</p>\n', 'ViewCount': '294', 'Title': 'When is the AKS primality test actually faster than other tests?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-02T22:41:02.053', 'LastEditDate': '2014-03-31T07:48:51.077', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '16292', 'Tags': '<algorithms><efficiency><primes>', 'CreationDate': '2014-03-30T13:40:07.150', 'FavoriteCount': '3', 'Id': '23260'},6871:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm a GCSE Computing student and I've been asked to compare writing programs in a block programming environment (I think this means using programs such as Scratch or App Inventor but I'm not entirely sure) compared to writing programs using high level programming languages such as Python, Java etc.\nI'm honestly not trying to ask you to do my work for me, I just desperately need some help on the advantages and disadvantages of both ways of writing programs because we haven't really been taught it by my teacher and I haven't found much on the Internet about it. </p>\n", 'ViewCount': '63', 'Title': 'Writing programs with a block programming environment vs. Writing programs using high level prgramming language', 'LastActivityDate': '2014-04-05T13:46:30.090', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16473', 'Tags': '<programming-languages><efficiency>', 'CreationDate': '2014-04-05T13:46:30.090', 'Id': '23449'}