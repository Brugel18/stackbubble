31_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume a computer has a precise clock which is not initialized. That is, the time on the computer\'s clock is the real time plus some constant offset. The computer has a network connection and we want to use that connection to determine the constant offset $B$.</p>\n\n<p>The simple method is that the computer sends a query to a time server, noting the local time $B + C_1$. The time server receives the query at a time $T$ and sends a reply containing $T$ back to the client, which receives it at a time $B + C_2$. Then $B + C_1 \\le T \\le B + C_2$, i.e. $T - C_2 \\le B \\le T - C_1$.</p>\n\n<p>If the network transmission time and the server processing time are symmetric, then $B = T - \\dfrac{C_1 + C_2}{2}$. As far as I know, <a href="http://en.wikipedia.org/wiki/Network_Time_Protocol">NTP</a>, the time synchronization protocol used in the wild, operates on this assumption.</p>\n\n<p>How can the precision be improved if the delays are not symmetric? Is there a way to measure this asymmetry in a typical Internet infrastructure?</p>\n', 'ViewCount': '1881', 'Title': 'Clock synchronization in a network with asymmetric delays', 'LastEditorUserId': '41', 'LastActivityDate': '2013-02-11T13:29:09.433', 'LastEditDate': '2012-03-07T20:25:33.727', 'AnswerCount': '5', 'CommentCount': '8', 'Score': '24', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<clocks><distributed-systems><computer-networks>', 'CreationDate': '2012-03-07T16:59:53.447', 'FavoriteCount': '5', 'Id': '103'},31_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose that a certain parallel application uses a master-slave design to process a large number of workloads. Each workload takes some number of cycles to complete; the number of cycles any given workload will take is given by a known random variable $X$. Assume that there are $n$ such workloads and $m$ equivalent slaves (processing nodes). Naturally, a more general version of this question addresses the case of slaves of differing capabilities, but we ignore this for now.</p>\n\n<p>The master cannot process workloads, but can distribute workloads to slave nodes and monitor progress of slave nodes. Specifically, the master can perform the following actions:</p>\n\n<ol>\n<li>Instantaneously begin processing of any $k$ workloads on any free node.</li>\n<li>Instantaneously receive confirmation of the completion by a node of a previously initiated batch of $k$ workloads.</li>\n<li>At any point in time, and instantaneously, determine the state of all nodes (free or busy) as well as the number of workloads completed and the number of workloads remaining.</li>\n</ol>\n\n<p>For simplicity\'s sake, assume $k$ divides $n$.</p>\n\n<p>There are at least two categories of load balancing strategies for minimizing the total execution time of all workloads using all slaves (to clarify, I\'m talking about the makespan or wall-clock time, not the aggregate process time, which is independent of the load-balancing strategy being used under the simplifying assumptions being made in this question): static and dynamic. In a static scheme, all placement decisions are made at time $t = 0$. In a dynamic scheme, the master can make placement decisions using information about the progress being made by some slaves, and as such, better utilization can be attained (in practice, there are overheads associated with dynamic scheduling as compared to static scheduling, but we ignore these). Now for some questions:</p>\n\n<ol>\n<li>Is there a better way to statically schedule workloads than to divide batches of $k$ workloads among the $m$ slaves as evenly as possible (we can also assume, for simplicity\'s sake, that $m$ divides $n/k$, so batches could be statically scheduled completely evenly)? If so, how?</li>\n<li>Using the best static scheduling policy, what should the mean and standard deviation be for the total execution time, in terms of the mean $\\mu$ and standard deviation $\\sigma$ of $X$? </li>\n</ol>\n\n<p>A simple dynamic load balancer might schedule $i$ batches of $k$ workloads to each slave initially, and then, when nodes complete the initial $i$ batches, schedule an additional batch of $k$ workloads to each slave on a first-come, first-served basis. So if two slave nodes are initially scheduled 2 batches of 2 workloads each, and the first slave finishes its two batches, an additional batch is scheduled to the first slave, while the second slave continues working. If the first slave finishes the new batch before the second batch finishes its initial work, the master will continue scheduling to the first slave. Only when the second slave completes executing its work will it be issued a new batch of workloads. Example:</p>\n\n<pre><code>         DYNAMIC           STATIC\n         POLICY            POLICY\n\n     slave1  slave2    slave1  slave2\n     ------  ------    ------  ------\n\nt&lt;0    --      --        --      --\n\nt&lt;1  batch1  batch3    batch1  batch3\n     batch2  batch4    batch2  batch4\n                       batch5  batch7\n                       batch6  batch8\n\nt=1    --    batch3    batch5  batch3\n             batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt&lt;2  batch5  batch3    batch5  batch3\n             batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt=2    --    batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt&lt;3  batch6  batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt=3    --      --        --    batch7\n                               batch8\n\nt&lt;4  batch7  batch8      --    batch7\n                               batch8\n\nt=4    --      --        --    batch8\n\nt&lt;5      -DONE-          --    batch8\n\nt=5                      --      --\n\nt &lt; 6                      -DONE-\n</code></pre>\n\n<p>For clarification, batches 1 and 2 take 1/2 second each to be processed, batch 3 takes 2 seconds to be processed, and batches 4-8 take 1 second each to be processed. This information is not known a-priori; in the static scheme, all jobs are distributed at t=0, whereas in the dynamic scheme, distribution can take into account what the actual runtimes of the jobs "turned out" to be. We notice that the static scheme takes one second longer than the dynamic scheme, with slave1 working 3 seconds and slave2 working 5 seconds. In the dynamic scheme, both slaves work for the full 4 seconds.</p>\n\n<p>Now for the question that motivated writing this:</p>\n\n<ol>\n<li>Using the dynamic load balancing policy described above, what should the mean and standard deviation be for the total execution time, in terms of the mean $\\mu$ and standard deviation $\\sigma$ of $X$?</li>\n</ol>\n\n<p>Interested readers have my assurances that this isn\'t homework, although it probably isn\'t much harder than what one might expect to get as homework in certain courses. Given that, if anyone objects to this being asked and demands that I show some work, I will be happy to oblige (although I don\'t know when I\'ll have time in the near future). This question is actually based on some work that I never got around to doing a semester or two ago, and empirical results were where we left it. Thanks for help and/or effort, I\'ll be interested to see what you guys put together.</p>\n', 'ViewCount': '198', 'Title': 'Analyzing load balancing schemes to minimize overall execution time', 'LastEditorUserId': '69', 'LastActivityDate': '2012-03-09T17:09:49.673', 'LastEditDate': '2012-03-09T17:09:49.673', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '140', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '69', 'Tags': '<scheduling><distributed-systems><parallel-computing>', 'CreationDate': '2012-03-08T16:00:04.720', 'Id': '134'},31_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is a puzzle about measuring network latency that I created. I believe the solution is that it\'s impossible, but friends disagree. I\'m looking for convincing explanations either way. (Though it is posed as a puzzle I think it fits on this web site because of its applicability to the design and experience of communication protocols such as in online games, not to mention NTP.)</p>\n\n<p>Suppose two robots are in two rooms, connected by a network with differing one-way latencies as shown in the graphic below. When robot A sends a message to robot B it takes 3 seconds for it to arrive, but when robot B sends a message to robot A it takes 1 second to arrive. The latencies never vary.</p>\n\n<p>The robots are identical and do not have a shared clock, though they can measure the passage of time (e.g. they have stop watches). They do not know which of them is robot A (whose messages are delayed 3s) and which is robot B (whose messages are delayed by 1s).</p>\n\n<p>A protocol to discover the round trip time is \'when receiving TICK send TOCK, start watch, send TICK, await TOCK, stop watch, record time difference (4s)\'.</p>\n\n<p>Is there a protocol to determine the one way trip delays? Can the robots discover which of them has the longer message sending delay?</p>\n\n<p><img src="http://i.stack.imgur.com/uYIGW.png" alt="Two robots one asymmetric network"></p>\n', 'ViewCount': '668', 'Title': 'Measuring one way network latency', 'LastEditorUserId': '41', 'LastActivityDate': '2013-07-10T20:57:00.673', 'LastEditDate': '2012-03-25T17:06:14.083', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '535', 'Tags': '<computer-networks><protocols><distributed-systems>', 'CreationDate': '2012-03-21T12:54:47.823', 'FavoriteCount': '5', 'Id': '602'},31_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am looking for an efficient algorithm that lets me process the minimax search tree for chess with <a href="http://en.wikipedia.org/wiki/Alpha-beta_pruning" rel="nofollow">alpha-beta pruning</a> on a distributed architecture. The algorithms I have found (PVS, YBWC, DTS see below) are all quite old (1990 being the latest). I assume there have been many substantial advancements since then. What is the current standard in this field?</p>\n\n<p>Also please point me to an idiot\'s explanation of DTS as I can\'t understand it from the research papers that I have read.</p>\n\n<p>The algorithms mentioned above:</p>\n\n<ul>\n<li>PVS: Principle Variation Splitting</li>\n<li>YBWC: Young Brothers Wait Concept</li>\n<li>DTS: Dynamic Tree Splitting</li>\n</ul>\n\n<p>are all are discussed <a href="http://chessprogramming.wikispaces.com/Parallel+Search" rel="nofollow">here</a>.</p>\n', 'ViewCount': '250', 'Title': 'distributed alpha beta pruning', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-03T02:19:30.217', 'LastEditDate': '2012-04-03T02:19:30.217', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '905', 'Tags': '<algorithms><distributed-systems><board-games>', 'CreationDate': '2012-04-02T21:00:57.743', 'Id': '998'},31_4:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '7302', 'Title': 'Distributed vs parallel computing', 'LastEditDate': '2012-04-30T07:46:49.827', 'AnswerCount': '3', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '1307', 'FavoriteCount': '12', 'Body': "<p>I often hear people talking about <em>parallel</em> computing and <em>distributed</em> computing, but I'm under the impression that there is no clear boundary between the 2, and people tend to confuse that pretty easily, while I believe it is very different:</p>\n\n<ul>\n<li><em>Parallel</em> computing is more tightly coupled to multi-threading, or how to make full use of a single CPU.</li>\n<li><em>Distributed</em> computing refers to the notion of divide and conquer, executing sub-tasks on different machines and then merging the results.</li>\n</ul>\n\n<p>However, since we stepped into the <em>Big Data</em> era, it seems the distinction is indeed melting, and most systems today use a combination of parallel and distributed computing.</p>\n\n<p>An example I use in my day-to-day job is Hadoop with the Map/Reduce paradigm, a clearly distributed system with workers executing tasks on different machines, but also taking full advantage of each machine with some parallel computing.</p>\n\n<p>I would like to get some advice to understand how exactly to make the distinction in today's world, and if we can still talk about parallel computing or there is no longer a clear distinction. To me it seems distributed computing has grown a lot over the past years, while parallel computing seems to stagnate, which could probably explain why I hear much more talking about distributing computations than parallelizing.</p>\n", 'Tags': '<terminology><distributed-systems><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-20T12:16:12.150', 'CommentCount': '2', 'AcceptedAnswerId': '1582', 'CreationDate': '2012-04-29T21:13:33.690', 'Id': '1580'},31_5:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>My organization wants to maintain multiple copies of data in order to preserve access in the case of localized disasters as well as for the purpose of long term preservation. Are there accepted formal models for determining the appropriate variety of media (eg tape, disk) and their placement in the network? Are currently operating distributed solutions (eg LOCKSS) viable long term solutions for large collections of data?</p>\n', 'ViewCount': '51', 'Title': 'Distributed Storage for Access and Preservation', 'LastEditorUserId': '1038', 'LastActivityDate': '2012-05-01T01:46:07.983', 'LastEditDate': '2012-05-01T01:46:07.983', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1038', 'Tags': '<digital-preservation><distributed-systems><storage>', 'CreationDate': '2012-04-30T23:20:10.173', 'Id': '1602'},31_6:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '411', 'Title': 'All soldiers should shoot at the same time', 'LastEditDate': '2012-05-02T22:20:43.843', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '0', 'Body': '<p>When I was a student, I saw a problem in a digital systems/logic design textbook, about N soldiers standing in a row, and want to shoot at the same time. A more difficult version of the problem was that the soldiers stand in a general network instead of a row. I am sure this is a classical problem, but I cannot remember its name. Can you remind me?</p>\n', 'Tags': '<algorithms><distributed-systems><clocks>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-02T22:20:43.843', 'CommentCount': '0', 'AcceptedAnswerId': '1633', 'CreationDate': '2012-05-02T10:03:40.080', 'Id': '1632'},31_7:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>It seems to me that the <em>per-record timeline consistency</em> as defined by Cooper et al. in "PNUTS: Yahoo!\u2019s Hosted Data Serving Platform" mimics the (older?) definition of <em>monotonic writes</em>. From the paper:</p>\n\n<blockquote>\n  <p>per-record timeline consistency: all replicas of a given record apply\n  all updates to the record in the same order.</p>\n</blockquote>\n\n<p>This is quite similar to <a href="http://regal.csep.umflint.edu/~swturner/Classes/csc577/Online/Chapter06/img26.html" rel="nofollow">a definition for monotonic writes</a>:</p>\n\n<blockquote>\n  <p>A write operation by a process on data item x is completed before any\n  successive write operation on x by the same process.</p>\n</blockquote>\n\n<p>Can I conclude that those things are the same, or is there a difference that I misunderstand? Note that the link above also mentions possible copies of data item <code>x</code>, so monotonic write includes replicas.</p>\n', 'ViewCount': '135', 'Title': 'per-record timeline consistency vs. monotonic writes', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-13T12:16:22.567', 'LastEditDate': '2012-06-12T22:13:07.710', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2361', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1837', 'Tags': '<terminology><distributed-systems>', 'CreationDate': '2012-06-12T14:36:36.953', 'Id': '2339'},31_8:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose there are two databases, $D_1$ and $D_2$.  Let's further assume $D_1$ is always up and $D_2$ can be down sometimes. When it goes up again, it has to restart.</p>\n\n<p>$D_1$ is filled by say a dozen other systems with event messages.  Those messages have IDs and might be updated or deleted. $D_2$ needs to be in sync with $D_1$, which is realized by:</p>\n\n<ul>\n<li>on restart, pull all data from $D_1$.  During this pull $D_1$ is locked, thus all senders to $D_1$ need to wait.</li>\n<li>otherwise $D_1$ always informs $D_2$ of updates by sending all updates. (During fetching, the data is locked of course.)</li>\n</ul>\n\n<p>Now the question: what kind of blocking behaviour can we expect from $D_1$ and $D_2$?</p>\n\n<p>In particular I find the following corner case interesting and instructive:</p>\n\n<p>$D_1$ currently has a long list of events and the sender systems send a lot of new events/updates. $D_2$ just went down, goes up now and needs to fetch events from $D_1$, thus blocking the whole chain.</p>\n", 'ViewCount': '30', 'Title': 'Uni-directional synchronization and locking issues', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-12T22:04:59.373', 'LastEditDate': '2012-06-12T22:04:59.373', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '5', 'OwnerDisplayName': 'user694971', 'PostTypeId': '1', 'Tags': '<distributed-systems><database-theory>', 'CreationDate': '2012-05-07T13:09:02.477', 'Id': '2345'},31_9:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>At my current project I had a network problem come up for which I could not find a solution. In a peer-to-peer network I needed to send an action to all peers, and each peer was to act on it only if it could verify that all other peers would also act on it.</p>\n\n<p>That is, given a network of peers $P = { P_1, ..., P_n }$. We wish to send, from some source peer $P_s$ a message to all other peers. This message contains an action which must be performed. The peer should perform this action if and only if every other peer will perform the action. That is, it performs the action if it can verify that all other peers will also have receipt of the action and can perform the same verification.</p>\n\n<p>The problem is subject to these conditions:</p>\n\n<ol>\n<li>There is no implicit message delivery guarantee: if $P_x$ sends a message to $P_y$ there is no way for $P_x$ to know if $P_y$ gets the message. (Of course $P_y$ can send a receipt, but that receipt is subject to the same constraint)</li>\n<li>Additional messages with any payload may be created.</li>\n<li>There is no total ordering on the messages received by peers. Messages can arrive in a different time-order than which they were sent. This time-order may be unique per peer. <em>Two messages sent in order from $P_x$ to $P_y$ are very unlikely to arrive out of order.</em></li>\n<li>Messages can arrive at any point in the future (so not only are they not ordered, they can be indefintely delayed). A message cannot inherently be detected as lost. <em>Most messages will be delivered quickly, or truly lost.</em></li>\n<li>Each peer has a synchronized clock. It is accurate enough in the domain of scheduling an action and to approximately measure transmission delays. It is however not accurate enough to establish a total ordering on messages using timestamps.</li>\n</ol>\n\n<p>I was not able to find a solution. I'm interested in a <em>guarantee</em> and not simply a high probability of being correct (which can be done simply be repeatedly sending confirmations from peer to peer and rejections upon any likely loss.) My stumbling block is the inability to verify that any particular message actually arrived. So even if $P_x$ determines there is an erorr, there is no guaranteed way to tell the other peers about it.</p>\n\n<p>A negative confirmation is also acceptable. I have a suspicion that a guarantee cannot actually be achieved, only an arbitrarily high probability.</p>\n", 'ViewCount': '71', 'Title': 'Message receipt verification in a cluster', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-03T09:56:09.247', 'LastEditDate': '2012-08-03T09:56:09.247', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '2802', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1642', 'Tags': '<algorithms><distributed-systems><computer-networks><fault-tolerance>', 'CreationDate': '2012-07-17T07:56:19.143', 'Id': '2784'},31_10:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '256', 'Title': 'Origins of the term "distributed hash table"', 'LastEditDate': '2012-07-23T09:49:38.700', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1837', 'FavoriteCount': '1', 'Body': "<p>I am currently researching for my diploma thesis in computer science with a topic in the area of distributed hash tables. Naturally, I came to the question were the term <em>distributed hash table</em> came from. (I know it is not rocket science to just derive it from <em>distributing a hash table</em>, but somebody somewhere must have come up with it).</p>\n\n<p>Most papers I read referred to the original paper on <em>consistent hashing</em> and one of the first algorithms making use of it (e.g Chord). I know that there was a lot of research on distributed databases in the 80s, so I figure that the term, or maybe the idea behind it, should be older than ~15 years.</p>\n\n<p>The motivation behind this question is that knowing an earlier date and maybe another term for a similar idea would possibly widen the range of useful information I could gather for my research. For example, what have others done that is similar to what I want to do and where have they failed. Etc. etc.</p>\n\n<p>I tried to find more on this subject using <em>Structured Overlay Networks</em> as a search keyword, but the resulting definitions/papers are also quite young, which leaves me with the impression that the research topic might not be so old after all.</p>\n\n<p>Does anybody of you know of earlier research (maybe pre-90s?) in the topics of distributed hash tables and/or structured overlay networks? I'd be glad to hear some keywords which could lead me to more historic papers.</p>\n", 'Tags': '<data-structures><terminology><distributed-systems><hash-tables><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-21T07:15:27.943', 'CommentCount': '6', 'AcceptedAnswerId': '8961', 'CreationDate': '2012-07-23T09:43:04.950', 'Id': '2872'},31_11:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '11355', 'Title': 'What is the novelty in MapReduce?', 'LastEditDate': '2012-08-04T09:11:58.820', 'AnswerCount': '4', 'Score': '39', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '16', 'Body': u'<p>A few years ago, <a href="https://en.wikipedia.org/wiki/Mapreduce">MapReduce</a> was hailed as revolution of distributed programming. There have also been <a href="http://craig-henderson.blogspot.de/2009/11/dewitt-and-stonebrakers-mapreduce-major.html">critics</a> but by and large there was an enthusiastic hype. It even got patented! [1]</p>\n\n<p>The name is reminiscent of <code>map</code> and <code>reduce</code> in functional programming, but when I read (Wikipedia)</p>\n\n<blockquote>\n  <p><strong>Map step:</strong> The master node takes the input, divides it into smaller sub-problems, and distributes them to worker nodes. A worker node may do this again in turn, leading to a multi-level tree structure. The worker node processes the smaller problem, and passes the answer back to its master node.</p>\n  \n  <p><strong>Reduce step:</strong> The master node then collects the answers to all the sub-problems and combines them in some way to form the output \u2013 the answer to the problem it was originally trying to solve.</p>\n</blockquote>\n\n<p>or [2] </p>\n\n<blockquote>\n  <p><strong>Internals of MAP:</strong> [...] MAP splits up the input value into words. [...] MAP is meant to associate each given key/value pair of the input with potentially many intermediate key/value pairs.</p>\n  \n  <p><strong>Internals of REDUCE:</strong> [...] [REDUCE] performs imperative aggregation (say, reduction): take many values, and reduce them to a single value.</p>\n</blockquote>\n\n<p>I can not help but think: this is <a href="https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm">divide &amp; conquer</a> (in the sense of Mergesort), plain and simple! So, is there (conceptual) novelty in MapReduce somewhere, or is it just a new implementation of old ideas useful in certain scenarios?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PALL&amp;p=1&amp;u=/netahtml/PTO/srchnum.htm&amp;r=1&amp;f=G&amp;l=50&amp;s1=7,650,331.PN.&amp;OS=PN/7,650,331&amp;RS=PN/7,650,331"> US Patent 7,650,331: "System and method for efficient large-scale data processing "</a> (2010)</li>\n<li><a href="http://dx.doi.org/10.1016/j.scico.2007.07.001">Google\u2019s MapReduce programming model \u2014 Revisited</a> by R. L\xe4mmel (2007)</li>\n</ol>\n', 'Tags': '<algorithms><distributed-systems><parallel-computing><algorithm-design>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-11T04:57:53.957', 'CommentCount': '4', 'AcceptedAnswerId': '3020', 'CreationDate': '2012-08-03T14:04:19.350', 'Id': '3019'},31_12:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Surely <a href="http://www.spatial-computing.org/" rel="nofollow">spatial computing</a> and <a href="http://en.wikipedia.org/wiki/Amorphous_computing" rel="nofollow">amorphous computing</a> share similarities and overlap. Is spatial computing a subset of amorphous computing? How are the two different? </p>\n', 'ViewCount': '41', 'Title': 'How is amorphous computing different from spatial computing?', 'LastEditorUserId': '31', 'LastActivityDate': '2012-08-07T15:46:14.707', 'LastEditDate': '2012-08-07T15:46:14.707', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2389', 'Tags': '<distributed-systems><parallel-computing><agent-based-computing>', 'CreationDate': '2012-08-07T12:04:55.460', 'Id': '3073'},31_13:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '691', 'Title': "How Does Populating Pastry's Routing Table Work?", 'LastEditDate': '2012-08-13T09:14:36.520', 'AnswerCount': '1', 'Score': '15', 'OwnerDisplayName': 'Paddy Foran', 'PostTypeId': '1', 'OwnerUserId': '2653', 'FavoriteCount': '2', 'Body': u'<p>I\'m trying to implement the Pastry Distributed Hash Table, but some things are escaping my understanding. I was hoping someone could clarify.</p>\n\n<p><strong>Disclaimer</strong>: I\'m not a computer science student. I\'ve taken precisely two computer science courses in my life, and neither dealt with anything remotely complex. I\'ve worked with software for years, so I feel I\'m up to the implementation task, if I could just wrap my head around the ideas. So I may just be missing something obvious.</p>\n\n<p>I\'ve read the paper that the authors published [1], and I\'ve made some good progress, but I keep getting hung up on this one particular point in how the routing table works:</p>\n\n<p>The paper claims that</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into $\\lceil \\log_{2^b} N\\rceil$ \n  rows with $2^b - 1$ entries each. The $2^b - 1$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>The $b$ stands for an application-specific variable, usually $4$. Let\'s use $b=4$, for simplicity\'s sake. So the above is</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into  $\\lceil \\log_{16} N\\rceil$ rows \n  with $15$ entries each. The $15$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>I understand that much. Further, $N$ is the number of servers in the cluster. I get that, too.</p>\n\n<p>My question is, if the row an entry is placed into depends on the shared length of the key, why the seemingly random limit on the number of rows? Each nodeId has 32 digits, when $b=4$ (128 bit nodeIds divided into digits of b bits). So what happens when $N$ gets high enough that $\\lceil\\log_{16} N\\rceil &gt; 32$? I realise it would take 340,282,366,920,938,463,463,374,607,431,768,211,457 (if my math is right) servers to hit this scenario, but it just seems like an odd inclusion, and the correlation is never explained.</p>\n\n<p>Furthermore, what happens if you have a small number of servers? If I have fewer than 16 servers, I only have one row in the table. Further, under no circumstances would every entry in the row have a corresponding server. Should entries be left empty? I realise that I\'d be able to find the server in the leaf set no matter what, given that few servers, but the same quandary is raised for the second row--what if I don\'t have a server that has a nodeId such that I can fill every possible permutation of the nth digit? Finally, if I have, say, four servers, and I have two nodes that share, say, 20 of their 32 digits, by some random fluke... should I populate 20 rows of the table for that node, even though that is far more rows than I could even come close to filling?</p>\n\n<p>Here\'s what I\'ve come up with, trying to reason my way through this:</p>\n\n<ol>\n<li>Entries are to be set to a null value if there is not a node that matches that prefix precisely.</li>\n<li>Empty rows are to be added until enough rows exist to match the shared length of the nodeIds.</li>\n<li>If, and only if, there is no matching entry for a desired message ID, fall back on a search of the routing table for a nodeId whose shared length is greater than or equal to the current nodeId\'s and whose entry is mathematically closer than the current nodeId\'s to the desired ID.</li>\n<li>If no suitable node can be found in #3, assume this is the destination and deliver the message.</li>\n</ol>\n\n<p>Do all four of these assumptions hold up? Is there somewhere else I should be looking for information on this?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1007/3-540-45518-3_18" rel="nofollow">Pastry: Scalable, decentralized object location and routing for large-scale peer-to-peer systems</a> by A. Rowstrong and P. Druschel (2001) -- <a href="http://research.microsoft.com/~antr/PAST/pastry.pdf" rel="nofollow">download here</a></li>\n</ol>\n', 'Tags': '<algorithms><data-structures><distributed-systems><hash-tables>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T17:34:24.577', 'CommentCount': '2', 'AcceptedAnswerId': '6069', 'CreationDate': '2012-05-23T22:02:33.000', 'Id': '3138'},31_14:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I am unable to understand why there is a requirement of 3m+1 generals overall given m traitors. As per my understanding, the steps are as follows:</p>\n\n<ol>\n<li>Each general decides a strategy and forwards their strategy to all the other ($3m$) generals.</li>\n<li>Each general forwards a vector containing the strategies (of all the other generals they received) to all the generals ($m$)</li>\n<li>Now, each general has a list of vectors corresponding to the vote made by each general and using a majority criteria (majority vote) decides each general's strategy and thus, the strategy that it itself chooses.</li>\n</ol>\n\n<p>If there are $3m+1$ generals then, with $m$ (loyal) generals choosing to 'attack' and $m+1$ (loyal) generals choosing to 'defend' and $m$ traitors, then since every loyal general forwards the message correctly, each of the $m$ 'attacking' generals will receive $m-1 + m$ (from the other 'attacking' generals and the $m$ 'defending' generals) confirmations of attack for all the other $m-1$ 'attacking' generals, similarly $m-1 + m$ confirmations of 'defend' for the strategy adopted by each one of the defending generals. The traitors however can alter either by sending whatever they'd like (different bits) to the attackers and the defenders [send <code>d</code> for all the defenders and <code>a</code> for all the attackers, but send conflicting values \u2014 <code>d</code> to the defenders and <code>a</code> to the attackers \u2014 for all the traitors' values], thus making them follow a different plan (defenders will defend, while attackers will attack).</p>\n\n<p>This can't be right, because all the papers and presentations I've gone through state that $3m+1$ with only $m$ traitors will definitely give you the correct solution (all loyal generals follow the same plan). Can anybody go through my understanding, find the flaw and re-explain it to me?</p>\n\n<p>Also, I have understood the $m = 1$ example, it would be nice if someone could explain to me how it fails when $m = 2$ and there are 6 generals and how it succeeds if $m = 2$ and there are 7 generals... I've tried a lot and am stuck.</p>\n", 'ViewCount': '213', 'Title': 'Impossibility condition in the Byzantine Generals problem', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-11T10:33:46.873', 'LastEditDate': '2012-09-10T13:56:22.730', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2777', 'Tags': '<distributed-systems><fault-tolerance>', 'CreationDate': '2012-09-10T13:00:42.823', 'FavoriteCount': '0', 'Id': '3489'},31_15:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1683', 'Title': 'Start learning about Theory of Distributed Systems?', 'LastEditDate': '2012-09-29T14:26:42.870', 'AnswerCount': '6', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '687', 'FavoriteCount': '8', 'Body': "<p>What's the best way that anyone can do to have a good introduction to the theory of distributed system, any books or references, and topics should be covered first and requirements to start learning in this topic.</p>\n", 'Tags': '<reference-request><education><distributed-systems>', 'LastEditorUserId': '157', 'LastActivityDate': '2013-02-18T20:06:39.400', 'CommentCount': '0', 'AcceptedAnswerId': '4798', 'CreationDate': '2012-09-29T14:24:11.433', 'Id': '4793'},31_16:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What is the difference between eScience infrastructures and grid computing or distributed computing infrastructures? And what are some of their examples.</p>\n\n<p>I can not distinguish them clearly. Is it how a grid is used make it an e-science platform?</p>\n', 'ViewCount': '40', 'Title': 'What is the difference between E-Science Infrastructures and Distributed Computing Infrastructures?', 'LastActivityDate': '2012-10-13T13:48:32.340', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3180', 'Tags': '<distributed-systems>', 'CreationDate': '2012-10-13T13:16:18.523', 'Id': '5043'},31_17:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I experience a difficulty in solving exercises in distributed algorithm. Below is the the exercise I try to solve, it looks like I miss basic idea. </p>\n\n<p><strong>Exercise.</strong> Consider a 15-processor asynchronous network with processors 0,\u2026,14. The processors constantly run a synchronizer. Let $v$ and $v'$ be two processors in the network, and suppose that at a certain moment, the pulse counter at $v$ shows $p=27$. What is the range of possible pulse numbers at $v'$ in each of the following cases:</p>\n\n<p>a) The network is a ring, $v$ is processor number 11, $v'$ is processor number 2 and the synchronizer used is $\\alpha$.</p>\n\n<p><strong>Idea</strong>: if $v'$ hasn't sent any message up to pulse 27 of $v$, pulse of $v'$ is still 0, therefore lower bound of pulse of $v'$ is 0. The model of synchronizer is $\\alpha$ it means every node informs all nodes about it's safe(v,p) state, hence I assume that $v'$ might be 11-2=9 pulses before $v$. </p>\n\n<p>b) The network is a full balanced binary tree (4 levels), $v$ is the root, $v'$ is one of the leaves and the synchronizer used $\\beta$.</p>\n\n<p><strong>Idea</strong>: $v'$ also might have pulse 27, in this case $v$ sends at speed of $v'$.</p>\n\n<p>c) The same as in (b), except both $v$ and $v'$ are leaves.</p>\n\n<p>Honestly, I am completely confused by this exercise, I wrote few ideas, but I don't have any understanding and any intuition behind the answers.</p>\n\n<p>I will appreciate if someone show me the way how to solve such exercises.</p>\n", 'ViewCount': '145', 'Title': 'Distributed algorithms - $\\alpha, \\beta$ synchronizers', 'LastEditorUserId': '3094', 'LastActivityDate': '2012-11-29T04:18:41.997', 'LastEditDate': '2012-11-29T04:18:41.997', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7002', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><distributed-systems><synchronization>', 'CreationDate': '2012-11-28T20:58:24.443', 'Id': '7001'},31_18:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was wondering if someone knew the origin of the client server model. Where does the term come from (paper, software application, book)?</p>\n', 'ViewCount': '349', 'Title': 'What is the origin of the client server model?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-04T23:06:54.407', 'LastEditDate': '2012-12-02T11:34:11.250', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '7060', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '4809', 'Tags': '<terminology><reference-request><distributed-systems><history>', 'CreationDate': '2012-11-30T10:05:42.000', 'Id': '7038'},31_19:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<blockquote>\n  <p>The access patterns of user requests may be <strong>static</strong>, so that they do\n  not change over time, or dynamic. It is obviously considerably easier to plan for\n  and manage the <strong>static</strong> environments than would be the case for dynamic distributed\n  systems. Unfortunately, it is difficult to find many real-life distributed applications\n  that would be classified as <strong>static</strong>. The significant question, then, is not whether a\n  system is static or dynamic, but how dynamic it is. Incidentally, it is along this\n  dimension that the relationship between the distributed database design and query\n  processing is established.</p>\n</blockquote>\n\n<p>What does it mean for an access pattern to be \u201cstatic\u201d? Could you show a practical example of a static access pattern?</p>\n', 'ViewCount': '84', 'Title': 'Static access pattern in Distributed Databases', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-04T21:55:20.093', 'LastEditDate': '2012-12-04T21:55:20.093', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7140', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4827', 'Tags': '<distributed-systems><databases>', 'CreationDate': '2012-12-04T00:17:57.830', 'Id': '7139'},31_20:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume a map-reduce program has $m$ mappers and $n$ reducers ($m &gt; n$). The output of each mapper is partitioned according to the key value and all records having the same key value go into the same partition (within each mapper), and then each partition is sent to a reducer. Thus there might be a case in which there are two partitions with the same key from two different mappers going to 2 different reducers. How to prevent this from occurring? That is, how to send all partitions (from different mappers) with the same key to the same reducer?</p>\n', 'ViewCount': '189', 'Title': 'How partitioning in map-reduce work?', 'LastActivityDate': '2012-12-04T19:29:25.950', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4868', 'Tags': '<distributed-systems><parallel-computing><algorithm-design>', 'CreationDate': '2012-12-04T17:54:20.870', 'FavoriteCount': '1', 'Id': '7159'},31_21:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The last step in the design process is the physical design, which maps the local conceptual schemas to the physical storage devices available at the corresponding sites.</p>\n\n<p>What meaning of corresponding sites ? How look like in design ?</p>\n', 'ViewCount': '50', 'Title': 'In DDBMS, What meaning of corresponding sites?', 'LastActivityDate': '2012-12-10T09:51:54.203', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4827', 'Tags': '<distributed-systems><database-theory><databases>', 'CreationDate': '2012-12-10T09:51:54.203', 'Id': '7299'},31_22:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am trying to understand the <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">distributed 6-color algorithm for vertex coloring</a> (on page 10).</p>\n\n<p>Here is a short description</p>\n\n<p>Idea of the algorithm: We start with color labels that have $\\log n$ bits. In each synchronous round we compute a new label with exponentially smaller size than the previous label, still guaranteed to have a valid vertex coloring.</p>\n\n<p>Let $i$ be the smallest index where $c_v$ and $c_p$ differ;\nthe new label is $i$ (as a bitstring) followed by the bit $c_v(i)$ itself</p>\n\n<pre><code>Grand-parent 0010110000 -&gt; 10010 -&gt; \u2026\nParent       1010010000 -&gt; 01010 -&gt; 111\nChild        0110010000 -&gt; 10001 -&gt; 001\n</code></pre>\n\n<p>The problem I cannot understand this example. Let\'s take Grand-parent($c_p$ = 0010110000) and parent($c_v$ = 1010010000), on the round when $c_v$ receives $c_p$, so we need to change $c_v$. They differ in 5th bit, counting from 0 (5 in binary is 101), so according to definition, $c_p$ is "$101$"+$c_p[5]=1010$, but in example it\'s 01010, what I get wrong?</p>\n', 'ViewCount': '167', 'Title': 'Distributed 6-color Vertex Coloring', 'LastEditorUserId': '1636', 'LastActivityDate': '2012-12-18T07:23:55.717', 'LastEditDate': '2012-12-18T06:01:55.390', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7460', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><distributed-systems><colorings>', 'CreationDate': '2012-12-17T11:30:23.563', 'Id': '7459'},31_23:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Can anybody please solve the following problem step-by-step with explanations, using bond energy and vertical partitioning to obtain a vertical fragmentation of the set of attributes.</p>\n\n<p>I need to understand how bond energy and vertical partitioning algorithm work with this problem. The problem is in the following link: <a href="http://download.bwor.net/V_Fragment.jpg" rel="nofollow">Problem of Vertical Fragment</a></p>\n', 'ViewCount': '206', 'Title': 'Bond energy and vertical partitioning to get a vertical fragmentation in distributed dbms', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-23T00:05:20.297', 'LastEditDate': '2012-12-23T00:05:20.297', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4827', 'Tags': '<distributed-systems><database-theory><databases>', 'CreationDate': '2012-12-22T22:21:46.577', 'Id': '7547'},31_24:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I try to provide a strict and mathematical rigorous proof to the following problem in Distributed Algorithms. </p>\n\n<p>Prove or make a contradiction: if to vertices $a$ and $b$ on the network $G$ are located on the distance $k$ from each other, and there are $k^2$ different paths on different edges, so it is possible to transfer $k^2$ messages from $a$ to $b$ in time $\\text{O}(k)$.</p>\n\n<p>In my opinion. it\'s not true, I will try to show why. Let\'s say we have a network as on the picture with vertices $a$ and $b$, all vertices are different and there are 3 paths. The distance from $a$ to $b$ is $k$, in the case of synchronous communication, on every $k/2$ vertices the is collision of $k^2$ messages (the number of paths). In general, it\'s possible to build a network (not like on the picture), where the worst case occurs (first outgoing messages goes on the longest path and last outgoing message on the shortest path, finally they will make collision again).</p>\n\n<p>Finally, we will get, $\\sum_{0}^{\\frac{k}{2}} k^2$ - collisions, which is greater than $\\text{O}(k)$.</p>\n\n<p><img src="http://i.stack.imgur.com/l35N0.png" alt="enter image description here"></p>\n\n<p>Does it make sense? Is it enough rigorous?</p>\n', 'ViewCount': '93', 'Title': 'Proof of message complexity on the network', 'LastActivityDate': '2012-12-25T21:22:30.883', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<proof-techniques><distributed-systems>', 'CreationDate': '2012-12-25T21:22:30.883', 'FavoriteCount': '1', 'Id': '7598'},31_25:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I try to solve an exercise in <strong>Distributed Algorithm</strong> described as follow.</p>\n\n<p>Let\'s define a <em>label assignment problem</em> as follow. There is a anonymous network (vertices don\'t have unique ID\'s , nodes don\'t have any clue about $n$ - number of vertices nor about network topology ). The labels assigned to vertices must be distinct and there is no limitation on the length of the label. <em>The initiator is given.</em> Show the algorithm for solving the problem in asynchronous model with time complexity $\\text{O}(D)$ and using $\\text{O}(m)$ messages (where $D$ is the diameter of graph and $m$ number of edges), prove that the labels are indeed distinct. Show the changes in complexities if the algorithm is executed on the synchronous network.</p>\n\n<p>Let\'s say that the initiator is the node A, wlog assign it $A_{id} = 1$, broadcast $A_{id} = 1$ to all children of $A$ by <a href="http://en.wikipedia.org/wiki/Flooding_%28computer_networking%29" rel="nofollow">flooding algorithm</a>, any node $B$ by receiving message from it\'s parent for the first time will assign $B_{id} = PARENT_{id}+1$ and send $B_{id}$ to it\'s children, if $B_{id}$ is already defined, drop the message. $T=\\text{O}(D)$ - time complexity, $M=\\text{O}(m)$ - message complexity for broadcast.</p>\n\n<p>It seems like on synchronous network the complexities will be the same, just becasue <strong>broadcast</strong> and <strong>convergecast</strong> on synchronous and asynchronous network run in $T=\\text{O(D)}$ and $M=\\text{O}(m)$.</p>\n\n<p>If there are any change in complexities on the synchronous network? Does the above algorithm look good?</p>\n', 'ViewCount': '142', 'Title': 'Label Assignment Problem', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-04-26T09:08:40.590', 'LastEditDate': '2012-12-27T07:22:32.760', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><distributed-systems>', 'CreationDate': '2012-12-26T09:48:16.627', 'Id': '7603'},31_26:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There is an exercise in <strong>Distributed Algorithm</strong> I have some difficulties to solve. There are few ideas, however nothing useful at the time. I will appreciate any help with it.</p>\n\n<p>Graph $G$ is a $k$-tree if it\'s possible to divide all it\'s edges to $k$ trees ($k$ subgraphs without a loop). Given network $N$ every processor knows it\'s parent and it\'s children in every tree. Using algorithm for 3-coloring of an arbitrary tree (as describe in <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">Vertex Coloring</a>) show that the given graph $G$ is $3k$-colorable in the same time complexity $O(\\log^*n)$.</p>\n\n<p><strong>Ideas</strong>: every vertex $v$ could belong to one or more trees among $k$ trees. For every particular tree vertex $v$ belongs to, vertex $v$ knows it\'s parent in this tree and it\'s children in this tree. Exercise asks to color a graph $G$ in $3k$ colors.   Running the mentioned algorithm for any particular tree we will get correct 3-colorable $k$ trees, each vertex will have $m$ different color labels, where $m$ is number of trees, vertex $v$ belongs to.</p>\n\n<p>Now the difficult part, concatenation of all $m$ color labels in one color for vertex $v$ will result in $3^k$ different colors for graph $G$.</p>\n\n<p>In general, simplification of the problem is on the picture.</p>\n\n<p><img src="http://i.stack.imgur.com/uF8tv.png" alt="enter image description here"></p>\n\n<p>Vertices $v$ and $u$ are connected by an edge (on the picture the edge belongs to the 1 tree, in principle it doesn\'t matter, we could draw a parallel edge that belongs to 2 tree), $v$ and $u$ belong to 1 tree and 2 tree. After running 3-colorability algorithm for every tree we will get the correct colorability, now the problem is to make color assignments to vertices $v$ and $u$ as vertices of the graph $G$ based on the color assignment of these vertices for 1 tree and 2 tree, and of course because $u$ and $v$ are connected by an edge in graph $G$, the colors should be different.  </p>\n', 'ViewCount': '276', 'Title': 'k-Trees Graph Coloring', 'LastActivityDate': '2013-01-03T18:56:48.330', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><proof-techniques><distributed-systems>', 'CreationDate': '2013-01-03T18:56:48.330', 'Id': '7736'},31_27:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Let\'s consider distributed version of algorithm for finding MIS of any graph $A$.</p>\n\n<p>For details, MIS - <a href="http://en.wikipedia.org/wiki/Maximal_independent_set" rel="nofollow">Maximimal Independent Set</a>.</p>\n\n<p>Slow version of distributed algorithm for MIS, page 2 -  <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">Distributed algorithms. Maximal Independent Set</a></p>\n\n<p>In worst case, time complexity of the algorithm is $\\text{O}(n)$ and a message complexity is $\\text{O}(m)$. If nodes of the network are not unique than it\'s not possible to find MIS.</p>\n\n<p>I am interested in few special cases, when the graph is a <strong>path</strong> and <strong>ring</strong>.</p>\n\n<p><strong>Exercise</strong>: Consider a path graph $G$ with vertex UIDs to be a random permutation of $\\{1,\u2026,n\\}$, time complexity of MTS_Slow is $T \\leq c\\log n$ for a constant $c$ with probability $1-\\frac{1}{n}$.\nWhat is a time complexity and it\'s probability on a ring $G$ with vertex UIDs to be a random permutation of $\\{1,\u2026,n\\}$.</p>\n\n<p><strong>Ideas:</strong> Time complexity has $\\log n$ factor, therefore on each phase number of candidates for MIS nodes from a path graph $G$ should be divided by constant factor. Lets consider the worst case, when only one node is choicen to join MIS on each phase, it occurs when the nodes\' UID\'s are located in increasing or decreasing order, it happes with probability $P(B)=\\frac{1}{2^n}$, $P(A)=\\frac{1}{2}$, where B - nodes\' UID is placed in increasing order, A - the next node\'s UID is bigger the the current one. But how to show that the number of candidates is getting lower by any arbitrary constant factor. The problem is I have a difficulties in defining probability for taking arbitrary constant factor of nodes on each phase. </p>\n\n<p>Case with ring should be similar to a path graph case.</p>\n\n<p>I will appreciate any help in solving this exercise.</p>\n', 'ViewCount': '145', 'Title': 'Maximimal Independent Set on Ring and Path', 'LastActivityDate': '2013-01-15T01:57:56.640', 'AnswerCount': '1', 'CommentCount': '10', 'AcceptedAnswerId': '8940', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4799', 'Tags': '<algorithms><graphs><distributed-systems>', 'CreationDate': '2013-01-08T13:45:19.680', 'Id': '7832'},31_28:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>FRP is about streaming events and behaviours through pure functions. The Actor model - at least, as implemented in Akka - is about streaming immutable messages (which can be considered to be discrete events) through potentially impure objects, called actors.</p>\n\n<p>So on the surface they seem related.</p>\n\n<p>What else can we say about how they related? Also, what can say about which of them might be more appropriate for different application domains?</p>\n', 'ViewCount': '1008', 'Title': 'How do Functional Reactive Programming and the Actor model relate to each other?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-19T18:05:36.947', 'LastEditDate': '2013-01-19T18:05:36.947', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1617', 'Tags': '<distributed-systems><functional-programming><programming-paradigms>', 'CreationDate': '2013-01-19T10:45:05.057', 'FavoriteCount': '5', 'Id': '9038'},31_29:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am interested in precise time complexity of distributed algorithm for finding MIS (Maximum Independent Set) of a given graph $G$.</p>\n\n<p>I investigate the Slow MIS distributed algorithm (from <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">these lecture notes</a>, page 2).</p>\n\n<p>Following is the more detailed version than in lecture notes.</p>\n\n<ul>\n<li>Every node sends its UID to it\'s neighbors.</li>\n<li>Run procedure join</li>\n</ul>\n\n<p><strong>on event</strong>: getting a message <em>decide(1)</em>  from a neighbor $w$ do</p>\n\n<ul>\n<li><p>Set $b = 0$ - flag that node terminates and not participating in further phases.</p></li>\n<li><p>Send <em>decide(0)</em> to all neighbors</p></li>\n</ul>\n\n<p><strong>on event</strong>: getting a message <em>decide(0)</em> from a neighbor $w$ do:</p>\n\n<ul>\n<li>Invoke procedure <strong>join</strong>.</li>\n</ul>\n\n<p>Procedure <strong>Join</strong></p>\n\n<p>if every neighbor $w$ of $v$ with a larger identifier has decided $b(w) = 0$, then do</p>\n\n<ul>\n<li>Set $b = 1$.</li>\n<li>Send <em>decided(1)</em> to all neighbors.</li>\n</ul>\n\n<p>The question is what\'s time complexity of the algorithm $\\Theta(n)$ or $\\Theta(D)$, where $D$ is a diameter of $G$.</p>\n\n<p>In the lecture notes linked above, they say that time complexity is $O(n)$. I  think that in our case it can be expressed as $\\Theta(D)$ (simultaneously $O(D)$ and $\\Omega(D)$) for special cases.</p>\n\n<p>The problem is how to prove that that time complexity in general is $O(D)$ and there are special cases when time complexity is $\\Omega(D)$ if it\'s right at all.</p>\n\n<p>Let\'s take a look at the example I have in mind.</p>\n\n<p><img src="http://i.stack.imgur.com/Q3dGs.png" alt="enter image description here"></p>\n\n<p>$D=1$ and $n=4$, and as I understood the algorithm every vertex will decide to join MIS on the first round.</p>\n\n<p>If you have an idea how to show that, please, share it with us. </p>\n', 'ViewCount': '131', 'Title': 'Slow MIS Distributed Algorithm', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-02-03T17:10:12.623', 'LastEditDate': '2013-02-02T19:53:16.413', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><distributed-systems>', 'CreationDate': '2013-02-01T06:56:49.720', 'FavoriteCount': '1', 'Id': '9379'},31_30:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '623', 'Title': 'Destination-based vs source-based routing', 'LastEditDate': '2013-03-23T19:42:08.950', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'user1796218', 'PostTypeId': '1', 'OwnerUserId': '7414', 'FavoriteCount': '0', 'Body': '<p>I understand that destination-based routing builds the "route" from the destination backwards to the source (e.g. if using a spanning tree, then the tree is routed at the destination). With source-based routing the opposite is true: the route is build from the source onwards towards the destination.</p>\n\n<p>However I don\'t understand the practical difference. How does it make a difference if I base my decision on the source or on the destination. Say, a shortest path algorithm such as Dijkstra\'s should give the same result regardless?</p>\n\n<p>Could someone explain?</p>\n', 'Tags': '<algorithms><distributed-systems><shortest-path><routing>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-03-25T16:40:32.177', 'CommentCount': '2', 'AcceptedAnswerId': '10779', 'CreationDate': '2013-03-22T19:34:10.097', 'Id': '10718'},31_31:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I have to implement a limitation algorithm in order to avoid to reach a throughput limit imposed by the service I'm interacting with.</p>\n\n<p>The limit is specified as \xabN request over 1 day\xbb where N is of the order of magnitude of 10^6.</p>\n\n<p>I have a distributed system of clients interacting with the service so they should share the measure.</p>\n\n<p>An exact solution should involve to record all the events and than computing the limit \xabwhen\xbb the event of calling the service occur: of course this approach is too expensive and so I'm looking for an approximate solution.</p>\n\n<p>The first one I devised imply to discretize the detection of the events: for example maintaing 24 counters at most and recording the number of requests occurred within an hour.</p>\n\n<p>Acceptable.</p>\n\n<p>But I feel that a more elegant, even if leaded by different \xabforces\xbb, is to declinate the approach to the continuum.</p>\n\n<p>Let's say recording the last N events I could easily infer the \xabcurrent\xbb throughput. Of course this algorithm suffer for missing consideration of the past events occurred the hours before. I could improve with with an aging algorithm but\u2026 and here follow my question:</p>\n\n<p>Q: \xabThere's an elegant approximate solution to the problem of estimating the throughput of a service?\xbb</p>\n", 'ViewCount': '26', 'Title': 'Throughput measure', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T11:58:18.983', 'LastEditDate': '2013-03-28T11:58:18.983', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7177', 'Tags': '<optimization><distributed-systems><approximation>', 'CreationDate': '2013-03-28T10:33:22.480', 'FavoriteCount': '1', 'Id': '10864'},31_32:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>The paper <a href="http://groups.csail.mit.edu/tds/papers/Lynch/podc96-esds.pdf" rel="nofollow">Eventually-Serializable Data Services (PODC\'96)</a> presented an <em>eventually-serializable</em> data service. </p>\n\n<p>In the abstract, it says\uff1a</p>\n\n<blockquote>\n  <p>\u2026 and generalizes their algorithm (in the related work <a href="http://dl.acm.org/citation.cfm?id=93399" rel="nofollow">Lazy replication</a>) by allowing general operations \u2026</p>\n</blockquote>\n\n<p>In section 1.1 (Background of our Work), it says:</p>\n\n<blockquote>\n  <p>\u2026 Also, their algorithm (in the related work <a href="http://dl.acm.org/citation.cfm?id=93399" rel="nofollow">Lazy replication</a>) requires that all operations be either read-only queries or write-only updates \u2026</p>\n</blockquote>\n\n<p>My questions is as follows:</p>\n\n<blockquote>\n  <p>Is the protocol in this paper suitable for transactions (as a kind of general operations) services?</p>\n</blockquote>\n\n<p>Specifically:</p>\n\n<p>In section 3, the authors give an abstract algorithm for implementing eventually-serializable data service. The algorithm is based on lazy replication, in which the operations received by each replica are gossiped in the background. The key property is that the operations requested in a partial order will be eventually total ordered through the gossip protocol. I want to extend this eventually-serializable notion to support transactions. Does this algorithm itself support transactions already? And what are the challenges to support transactions instead of ordinary operations?</p>\n', 'ViewCount': '35', 'Title': 'Is this protocol suitable for transaction services?', 'LastEditorUserId': '4911', 'LastActivityDate': '2013-04-14T14:19:48.937', 'LastEditDate': '2013-04-14T14:19:48.937', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<distributed-systems><database-theory><protocols>', 'CreationDate': '2013-04-12T13:28:05.350', 'Id': '11261'},31_33:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am interested in Distributed Algorithms especially in communication in network with failures.</p>\n\n<p>I look for the proof of the following randomized algorithm of communication in network with failures. For me it seems like very general result in the communication, nevertheless I haven\u2019t found the proof yet. </p>\n\n<p><strong>Algorithm</strong>: Initially only vertex $v_0$ has the message, at the end of the algorithm every vertex of the network should have the message. </p>\n\n<p>On every round every vertex that has the message choice the neighbour randomly and sends it the message.</p>\n\n<p><strong>Assumptions</strong>: only $f$ failures might happen on the edges between the vertices.\n$T = O(\\log n)$ - time complexity and the entire network will know the message with high probability, when $f&lt;n/3$, where n - number of vertices.</p>\n\n<p>I will appreciate for link or reference to the paper.</p>\n', 'ViewCount': '79', 'Title': 'Algorithm of Communication with Failures', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T06:47:58.020', 'LastEditDate': '2013-05-28T06:47:58.020', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11593', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4778', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2013-04-26T22:08:48.070', 'Id': '11590'},31_34:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '84', 'Title': 'Formalisms in concurrent and/or distributed programming?', 'LastEditDate': '2013-04-30T05:30:14.553', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7974', 'FavoriteCount': '1', 'Body': '<p>My background came from imperative languages, primarily C, C++, and Python. I picked up Scala, Erlang, and a bit of Haskell a few years later and have since become very interested in functional programming and the formalisms behind it.</p>\n\n<p>I am also interested in concurrent and distributed programming and have been looking into formalisms behind that, especially those that have seen at least a tiny bit of the "light of day" (e.g. real world use, or at least an implementation somewhere). So far I know of Communicating Sequential Processes, the Actor model, Algebra of Communicating Processes, and the Calculus of Communicating Systems. Among these I know the Actor model has realized itself in languages like Erlang, Scala, and Haskell.</p>\n\n<p>I am wondering if there are foundations I should learn and practice before tackling these fields, if there is a "classic" one that I should study first, and if there are any other popular ones that I may have missed?</p>\n', 'Tags': '<distributed-systems><concurrency>', 'LastEditorUserId': '7974', 'LastActivityDate': '2013-05-02T05:29:38.057', 'CommentCount': '0', 'AcceptedAnswerId': '11669', 'CreationDate': '2013-04-30T05:18:44.353', 'Id': '11666'},31_35:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There is a famous <a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29" rel="nofollow">Consensus Problem</a> in Distributed Computing.</p>\n\n<p>Let\'s consider and try to find the best possible algorithm for a simplified version of the consensus problem. </p>\n\n<p><strong>Assumptions:</strong> a process may undergo only crash failure (process abruptly stops and does not resume), process represent a complete graph.</p>\n\n<p><strong>Simplification</strong>: a crash failure may occur only between round, so there no a case when a process succeeds in sending some message and fails to send other message during the same round.</p>\n\n<p>The algorithm for the general case <strong><em>FloodSet</em></strong> when a process may crash during the round is described in <a href="http://books.google.co.il/books?id=2wsrLg-xBGgC&amp;lpg=PP1&amp;pg=PA103#v=onepage&amp;q&amp;f=false" rel="nofollow">Distributed algorithms - Nancy Ann Lynch</a>. By the analysis it was shown that $f+1$ rounds is enough to reach a consensus.</p>\n\n<p>Intuitively, it looks like that the simplification completely changes the approach to the solution. It is may be enough just one round, every processes send an input message to every other processes and agree on the minimal value. </p>\n\n<p>What is the simplest possible algorithms for simplified problem ? </p>\n\n<p>What can we do in the case of general graph?</p>\n', 'ViewCount': '83', 'Title': 'Is this simplified consensus problem easier than the original?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-16T06:17:45.843', 'LastEditDate': '2013-05-16T06:17:45.843', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12059', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2013-05-15T16:55:04.127', 'Id': '12043'},31_36:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So I'm learning about distributed computing, and I do understand a lot of the prerequisite material on algorithms and automata and whatnot... but I'm curious about the definitions I'm seeing as I read my textbook.</p>\n\n<blockquote>\n  <p>A system consists of $n$ processors $p_{0},\\dots,p_{n-1}$, where $i$ is the index of the processor $p_i$. Each processor is modeled with a (possibly infinite) state machine with state set $Q_i$. The processor is identified with a particular node in the topology graph. The edges incident on $p_i$ are labeled arbitrarily with the numbers $1$ thru $r$, where $r$ is the degree of the $p_i$. <strong>Each state of the processor $p_i$ contains $2r$ special components: $inbuf_i[\\mathcal{l}]$ and $outbuf_i[\\mathcal{l}]$.</strong></p>\n</blockquote>\n\n<p>Does this mean that each state of an n-state state machine has a separate inbuffer and outbuffer? Based on the wording of this, it seems to imply that there are n of each buffer, which would say to me that an outbuffer is mapped directly to the inbuffer of another state in another processor.</p>\n", 'ViewCount': '61', 'Title': 'Principles of Distributed Computing - "processors" and "states"', 'LastActivityDate': '2013-05-20T17:27:07.473', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12163', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6569', 'Tags': '<distributed-systems>', 'CreationDate': '2013-05-20T15:33:47.323', 'Id': '12159'},31_37:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>In the seminal distributed systems paper <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf" rel="nofollow">The Part Time Parliament</a> (the Paxos protocol), Leslie Lamport names fictional legislators who are involved in the Paxon parliament protocol.</p>\n\n<p>According to <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos" rel="nofollow">this writing</a>, he notes that:</p>\n\n<blockquote>\n  <p>I gave the Greek legislators the names of computer scientists working in the field, transliterated with Guibas\'s help into a bogus Greek dialect.</p>\n</blockquote>\n\n<p>Does anyone have any information on the scientists that the legislators are named after? A list of the legislators in the paper and the corresponding computer scientists would be the ideal answer.</p>\n\n<p>I think the first legislator mentioned in the paper, "\u039b\u03b9\u03bd\u03c7\u2202", is named after <a href="http://en.wikipedia.org/wiki/Nancy_Lynch" rel="nofollow">Nancy Lynch</a> since it could be pronounced as "Linch". Also, "\u039b\u03b5\u03c9\u03bd\u03af\u03b4\u03b1\u03c2 \u0393\u03ba\u03af\u03bc\u03c0\u03b1\u03c2" from the bibliography is <a href="https://en.wikipedia.org/wiki/Leonidas_J._Guibas" rel="nofollow">Leo Guibas</a>. I\'m completely lost as to who the others are.</p>\n', 'ViewCount': '147', 'Title': 'Who are the legislators of Paxos?', 'LastEditorUserId': '6785', 'LastActivityDate': '2013-05-31T19:25:53.577', 'LastEditDate': '2013-05-31T19:25:53.577', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6785', 'Tags': '<distributed-systems><history>', 'CreationDate': '2013-05-31T15:24:36.223', 'FavoriteCount': '1', 'Id': '12401'},31_38:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to understand what this text means in my textbook about distributed leader election algorithms, but I can\'t make any sense of it. Either they didn\'t explain what was meant, or I missed it somewhere.</p>\n\n<blockquote>\n  <p>In this section, we show that the leader election algorithm of Section 3.3.2 is asymptotically optimal. That is, we show that any algorithm for electing a leader in and asynchronous ring sends at least $\\Omega(n\\log n)$ messages. The lower bound we prove is for uniform algorithms, namely, algorithms that do not know the size of the ring.</p>\n  \n  <p>We prove the lower bound for a special variant of the leader election problem, where the elected leader must be the processor with the maximum identifier in the ring; in addition, all the processors must know the identifier of the elected leader. The proof of the lower bound for the more general definition of the leader election problem follows by reduction.</p>\n  \n  <p>Assume we are given a uniform algorithm $A$ that solves the above variant of the leader election problem. We will show that there exists an admissible execution of $A$ in which $\\Omega(n\\log n)$ messages are sent. <strong>Intuitively, this is done by building a "wasteful" execution of the algorithm for rings of size $n/2$, in which many messages are sent.</strong> Then we "paste together" two different rings of size $n/2$ to form a ring of size $n$, in such a way that we can combine the wasteful executions of the smaller rings and force $\\Theta(n)$ additional messages to be received.</p>\n  \n  <p>Although the preceding discussion referred to pasting together exections, we will actually work with <strong>schedules</strong>. The reason is that executions include configurations, which pin down the number of processors in the ring. We will want to apply the same sequence of events to different rings, with different numbers of processors. Before presenting the details of the lower bound proof, we first define schedules that can be "pasted together".</p>\n  \n  <p><em>A schedule $\\sigma$ of $A$ for a particular ring is open if there exists an edge $e$ of the ring such that in $\\sigma$ no message is delivered over the edge $e$ in either direction; $e$ is an open edge of $\\sigma$.</em></p>\n</blockquote>\n\n<p>There is lots more, but I don\'t know if I should type it all out if only for copyright reasons. I hope this is enough to help clarify my question and get an explanation.</p>\n', 'ViewCount': '40', 'Title': 'What is a "wasteful" execution? And a "schedule"?', 'LastActivityDate': '2013-06-03T07:03:00.550', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6569', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2013-06-02T18:42:41.907', 'Id': '12435'},31_39:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let  consider a general version of <a href="http://en.wikipedia.org/wiki/Two_Generals%27_Problem" rel="nofollow">Two Generals\' Problem</a>, when there are $n$ generals located on the arbitrary graph and they should agree on exactly the same value whether to attack or not to attack. </p>\n\n<p>It\'s well known  that Two Generals\' Problem represents a version of the <a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29" rel="nofollow">Consensus Problem</a> with unlimited number of the stopping failures, I think this is the only reason why Two Generals\' Problem and Generals\' Problem (with $n$ generals) don\'t have solution.</p>\n\n<p>There is a proof of lacking solution for Two Generals\' Problem (can be found in the textbook of Lynch).</p>\n\n<p>The following is the exercise from the textbook of Lynch, that I have not solved so far.</p>\n\n<p>Show that a solution to the (deterministic) coordinated attack problem (Generals\' Problem) for any nontrivial connected graph implies a solution for the simple graph consisting of two processes connected by one edge. (Therefore, this problem is unsolvable in any nontrivial graph.)</p>\n\n<p>Apparently, there is a reduction from an edge case to graph. But how to show it mathematically rigorous?</p>\n\n<p><strong>Addendum:</strong></p>\n\n<p>Can I say something like this?\nWhen we are given the primary problem of Two Generals and they initial values $a_i$ (inclination whether to attack or not), we in arbitrarily add more dummy generals with the only requirement if $a_1=a_0=a$ for primary problem set all dummy\'s general input to $a$, otherwise set arbitrary input value $b \\in \\{0,1\\}$. Find the solution on the graph, the solution on the graph is the solution for the primary problem. </p>\n', 'ViewCount': '157', 'LastEditorDisplayName': 'user742', 'Title': 'Coordinated Attack Problem On The Arbitrary Graph', 'LastActivityDate': '2013-09-20T17:48:32.680', 'LastEditDate': '2013-09-20T09:33:24.167', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4778', 'Tags': '<algorithms><complexity-theory><graph-theory><reductions><distributed-systems>', 'CreationDate': '2013-06-07T13:49:01.323', 'Id': '12508'},31_40:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '36', 'Title': 'Broadcast with Limited Spreading', 'LastEditDate': '2013-06-12T15:26:28.733', 'AnswerCount': '2', 'Score': '1', 'OwnerDisplayName': 'user16168', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Body': '<p>Let consider well-known problem in distributed computing <a href="http://en.wikipedia.org/wiki/Broadcasting_%28networking%29" rel="nofollow">Broadcast Problem</a> in network with stopping crashes.</p>\n\n<p>The most appropriate solution to the broadcast problem is <a href="http://en.wikipedia.org/wiki/Flooding_%28computer_networking%29" rel="nofollow">flooding algorithm</a>, when the source of the message just send it to all available nodes of the network on the first round, if there are crashes on the edges some number of message will fail. Then on the second round, the nodes that received messages from the previous round send it to all available nodes and so on.</p>\n\n<p>Let consider slightly different model, when on  every round the source node with message is capable to send a message to only one single node of course stopping failure might occur. It\'s very useful model, which can represent a sort of controllable or limited spreading. </p>\n\n<p>Unfortunately I don\'t know the formal name of the model. Do you familiar with result in the model of "limited spreading", I am sure there are must some work in this direction.</p>\n', 'Tags': '<distributed-systems>', 'LastEditorUserId': '1169', 'LastActivityDate': '2013-06-13T03:11:47.553', 'CommentCount': '1', 'AcceptedAnswerId': '12646', 'CreationDate': '2013-06-08T15:51:19.383', 'Id': '12613'},31_41:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I try to get a better understanding about Failure Detectors in the field of Distributed Computing.</p>\n\n<p>Let consider the algorithm for reaching consensus with Perfect Failure Detector, it\'s named as Perfect FDAgreement in the textbook <em>Distributed Algorithms by Nancy A. Lynch</em> on the page 679. In addition, it can be found in the following presentation <a href="http://vega.cs.kent.edu/~mikhail/classes/pda.s03/Chapter_21.ppt" rel="nofollow">Asynchronous Network Computing with Process Failures. Slide 16</a>.</p>\n\n<p>The basic idea of the algorithm is each process $P_i$ attempts to stabilize two pieces of data: a vector val, indexed by $\\{1,2,...,n\\}$, with values in $V \\cup \\{null\\}$.$val(j) = v \\in V$, it means that $P_i$ knows that $P_j$\u2019s initial value is $v$. A set stopped of process indices. If $j \\in stopped$, it means that $P_i$ knows that $P_j$ has stopped.</p>\n\n<p>The question is as one of the step of the algorithm <em>every process that receive a message ignores it from the processors it has already placed in stopped.</em> </p>\n\n<p>I don\'t understand why we need this, even we don\'t ignore the message from stopped process it won\'t broke anything. This case may occur when  $P_j$ sends a message to $P_i$ and crashes, FailureDetector see the crash and informs the process $P_i$ that $P_j$ is stopped after that $P_j$\'s message reaches $P_i$, but this message contains a correct data, and it doesn\'t change the state of $P_j$ in stopped set, because only INSERT (insert new values that previously was null) and COMPARE are allowed operations on the set.</p>\n\n<p>If you see the real reason for ignoring messages from the crashed process please share it with us.</p>\n\n<p><strong>Addendum</strong>: Unfortunately I didn\'t find any use of the aforementioned fact (ignoring message from crashed node) in the algorithm (Perfect FDAgreement in the textbook Distributed Algorithms by Nancy A. Lynch).</p>\n\n<p>In addition, in main papers <a href="http://www.ecommons.cornell.edu/bitstream/1813/7192/1/95-1535.pdf" rel="nofollow">Tushar Deepak Chandra and Sam Toueg. Unreliable failure detectors forasynchronous systems</a> and <a href="https://www.google.co.il/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDwQFjAC&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.98.6512%26rep%3Drep1%26type%3Dpdf&amp;ei=cmDaUeS8HvLV4ATNmoGwBg&amp;usg=AFQjCNFP5kPq9bXxfT725bRVepfZ2-dFNA&amp;bvm=bv.48705608,d.bGE" rel="nofollow">Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. The weakest\nfailure detector for solving consensus</a> slightly different algorithm can be found without ignoring messages from crashed nodes.</p>\n', 'ViewCount': '70', 'Title': 'Consensus and Perfect Failure Detector', 'LastEditorUserId': '8473', 'LastActivityDate': '2013-07-08T07:23:53.893', 'LastEditDate': '2013-07-08T07:23:53.893', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Tags': '<distributed-systems>', 'CreationDate': '2013-07-07T07:38:14.570', 'Id': '13123'},31_42:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There is a famous <a href="http://en.wikipedia.org/wiki/Two_Generals%27_Problem" rel="nofollow">Coordinated Attack Problem</a>. Let define a simple message-passing system $S$ with requirements </p>\n\n<ul>\n<li>Uniform Agreement: No two processes decide dierently.</li>\n<li>Validity:</li>\n</ul>\n\n<p>(a) If all processes start with 0, then no process decides 1.</p>\n\n<p>(b) If all processes start with 1, and there are no failures, then no process decides 0.</p>\n\n<ul>\n<li>Termination: All nonfaulty processes eventually decide.</li>\n</ul>\n\n<p>System $S$ consisting of two processes $p$ and $q$ connected by a  bidirectional communication link such that (a) processes proceed\nin synchronous rounds, (b) processes do not fail, and (c) any number of messages can be lost.</p>\n\n<p>It\'s well known that system $S$ cannot satisfy all above requirements, therefore let consider different requirements.</p>\n\n<ul>\n<li>Weak Termination: If there are no failures, then all processes eventually decide.</li>\n</ul>\n\n<p>Is the problem consisting of Agreement, Validity, and Weak Termination unsolvable in system S?</p>\n\n<blockquote>\n  <p>All processes eventually decide when there is no failures, so when inputs are $0$ and $0$ and no failures, both generals decide $0$. By dropping one of the message I cannot ensure the termination, therefore the standard reasoning doesn\'t work here.</p>\n</blockquote>\n\n<ul>\n<li>Unanimous Termination: If any process decides, then all processes eventually decide.</li>\n</ul>\n\n<p>Is the problem consisting of Agreement, Validity, and Weak Termination unsolvable in system S?</p>\n\n<blockquote>\n  <p>It\'s also very tricky requirement. Assume the both inputs are $0$ and $0$ and no failures than both decide $0$, let drop the message from $P_1$ to $P_0$, regarding to $P_1$ it is the undistinguishable execution, therefore $P_1$ still decide $0$ and according to unanimous termination, someday $P_0$ will decide the same $0$ according to agreement, noq according to the standard schema, change the input of $P_0$ to $1$ and this undistinguishable execution to $P_0$, so the same agreement and so forth we reach a contradiction, no solution. </p>\n</blockquote>\n\n<p>I am sure if it\'s correct. In addition the case with weak termination I don\'t know how to solve, even if it\'s solvable.</p>\n', 'ViewCount': '51', 'Title': 'Coordinated Attack Problem Different Requirements', 'LastEditorUserId': '4778', 'LastActivityDate': '2013-07-08T04:20:03.047', 'LastEditDate': '2013-07-08T04:08:17.387', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13147', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4778', 'Tags': '<proof-techniques><distributed-systems>', 'CreationDate': '2013-07-07T14:54:54.857', 'Id': '13133'},31_43:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have the notion of a social network which is robust against malicious attacks from the outside. My vision is a system that is structurally built up as a distributed network of equal servers that operate on the same data and offer the identical services. Users should all interact on the same network, independent of the respective server they communicate with. The idea is to prevent (at least theoretically) every opportunity to bring the whole system down and to achieve a total failure of the whole service. Obviously the design of such a system is not trivial because of proper synchronization mechanisms. On the other hand there is the challenge to convey the participants to the appropriate servers. Hope you got my idea roughly.</p>\n\n<p>The thing is that I only have a vague notion how such a system can function, and furthermore have seemingly successful passed over the relevant lectures about distributed systems in my cs studies. I'm therefore a little bit in a lack of an overview of relevant literature, scientific papers describing theoretical models and also real world examples.</p>\n\n<p>Can anyone help me out with references, links and helpful explanations? </p>\n", 'ViewCount': '112', 'Title': 'Theoretical foundations of robust and distributed services', 'LastEditorUserId': '31', 'LastActivityDate': '2013-10-09T14:53:41.000', 'LastEditDate': '2013-07-13T18:45:48.023', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9169', 'Tags': '<distributed-systems>', 'CreationDate': '2013-07-13T17:51:02.200', 'Id': '13261'},31_44:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When we talk about distributed computing - it often refers to problems in biology or web search where there is a large quantity of data to process, or email where there is a large number of concurrent users. </p>\n\n<p>Often in financial services, the problem space is OLTP for online banking, or overnight batch processing for summing up EOD totals and reporting. </p>\n\n<p>Is there an example problem in financial services that requires the tools available in distributed computing?</p>\n', 'ViewCount': '67', 'Title': 'Distributed problems in Financial Services?', 'LastActivityDate': '2013-07-27T08:46:52.023', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13462', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1709', 'Tags': '<distributed-systems>', 'CreationDate': '2013-07-27T02:58:25.080', 'Id': '13459'},31_45:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Wikipedia says that <a href="https://en.wikipedia.org/wiki/Shared_memory" rel="nofollow">shared memory</a> comes with lots of costs associated with cache coherence costs. But I thought the whole idea of shared memory is that all the CPUs access the same memory? So if one CPU changes that memory then other CPUs would access the same value? It would seem like this would require FEWER cache coherence costs? Is the idea that if one CPU changes its local cache before it writes to shared memory then other CPUs have to be notified?</p>\n', 'ViewCount': '249', 'Title': 'Why do you have to worry about cache coherence if you are using shared memory?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T10:29:48.617', 'LastEditDate': '2013-09-10T10:29:48.617', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '14241', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2605', 'Tags': '<computer-architecture><distributed-systems><parallel-computing>', 'CreationDate': '2013-09-10T00:25:05.073', 'Id': '14240'},31_46:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to flesh out a proposed application model where all peers in a distributed network discover each other in the least possible propagation time. </p>\n\n<p>But I\'m reached <a href="https://en.wikipedia.org/wiki/Four_stages_of_competence" rel="nofollow">the limit of my knowledge</a> of network theory and my naive search keywords return too many disparate PDF white-papers to narrow down.</p>\n\n<p>So my question is: <strong>What protocol or class of algorithm for discovering every peer in a network prioritises <em>low latency</em> over every other network characteristic?</strong></p>\n', 'ViewCount': '48', 'Title': 'Peer discovery: What class of network algorithm prioritises low latency above all else?', 'LastActivityDate': '2013-10-07T04:04:28.533', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10539', 'Tags': '<distributed-systems>', 'CreationDate': '2013-10-07T04:04:28.533', 'Id': '14875'},31_47:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to solve the following problem related to distribution-</p>\n\n<p>I have a list L of items (I1, I2,....In) sorted in order of importance, I1 being the most important. Each item has multiple tags assigned to it and the combination of these tags can be different on each item, so I1 may have tags T1 and T2, I2 can have tags t2, t3 and t4, and I3 can have tag T1, and so on.</p>\n\n<p>Now, I have to make batches from this list L, with a distribution of items (according to the tags) subject to the following constraints-</p>\n\n<p>Each batch has a fixed size B\nEach tag has a range of items in the batch distribution, ranging from a minimum to maximum. So, B should contain minimum x1 items with tag t1, x2 items with tag t2, and maximum y1 items of t1, y2 items of t2 and so on.\nWe start picking items from the top of L and keep filling the batch until we reach the final distribution that satisfies the constraints. If, say, L has 300 items, and we have to make a batch size of 50, we can go until any number of items in the list and pick the items to make the desired distribution.\nRemember that if an item is picked from the list, count of all the tags assigned to it goes up by 1.\nI was thinking of a solution where in first, I make lists of items corresponding to each specific tag. I pick the minimum desired items for a particular tag from its list. So, I'd pick x1 items with tag t1 from the list of items with tags t1, irrespective of whether the items contain any other tag. This way I'd ensure that the 'minimum' criteria of all the tags are satisfied. But for the max part, each tag will most definitely go overboard. How do I recursively keep replacing items from the batch with the remaining items in L to make the final desired distribution?</p>\n\n<p>Any other solution would be great. Or any existing algorithm that can get me in the right direction to approach this problem.</p>\n\n<p>I know the question is a bit too wordy, and probably a bit confusing, but I'v tried to explain it as well as I can, and of course, the problem might be a lot interesting I suppose.</p>\n", 'ViewCount': '92', 'Title': 'Distribution algorithm according to weighted parameters (with a min-max constraint)', 'LastActivityDate': '2013-10-13T05:35:12.270', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '15030', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10652', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2013-10-11T06:21:39.850', 'Id': '15001'},31_48:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '238', 'Title': 'Maximum degree of concurrency in task dependency graphs', 'LastEditDate': '2013-11-09T15:10:52.437', 'AnswerCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11256', 'FavoriteCount': '1', 'Body': "<p>I've been researching ways of modeling and executing tasks which are dependent on each other (but in an acyclic way) and came up with task graphs. But the question that's bugging me is how can I find out the maximum degree of concurrency in a given task graph.</p>\n\n<p>In my case, I'm talking of a relatively small graph, around 100 nodes, but nodes, representing tasks, are long running tasks. So the occuracy, more then complexity of such an algorithm would matter.</p>\n\n<p>Assuming I came up of such a degree, the second problem, is how should I distrubute tasks? I've read about topological sort, and transforming the result in a list of sets, with each set being run in parallel. But again, I suspect if this is the best approach.</p>\n", 'Tags': '<optimization><distributed-systems><concurrency>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-07T15:51:08.327', 'CommentCount': '1', 'AcceptedAnswerId': '16829', 'CreationDate': '2013-11-08T13:56:22.220', 'Id': '16823'},31_49:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m building a load test harness for a distributed system. Currently I\'m using the "<a href="http://www.cs.rutgers.edu/~muthu/bquant.pdf" rel="nofollow">Cormode, Korn, Muthukrishnan, and Srivastava</a>" method to estimate latency quantiles of system responses.</p>\n\n<p>I\'m now testing systems larger than can be adequately stressed using a single load generator. I\'m looking for a method similar to the one cited above that can be extended to a cluster of load generators. </p>\n\n<p>I would like to measure the latency quantiles of responses to the cluster as a whole, and to individual nodes in the cluster without recording/streaming the latency of every event in the system to a central node. </p>\n\n<p>Does such a method exist? If so, are there any public implementations?</p>\n', 'ViewCount': '24', 'Title': 'Efficiently estimating latency quantiles of a distributed system', 'LastActivityDate': '2013-11-18T17:31:59.213', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11457', 'Tags': '<algorithms><distributed-systems><statistics>', 'CreationDate': '2013-11-18T17:31:59.213', 'Id': '18118'},31_50:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What are differences in services provided by grid computing and cloud computing?</p>\n\n<p>From <a href="http://ccskguide.org/cloud-computing-vs-grid-computing/" rel="nofollow">http://ccskguide.org/cloud-computing-vs-grid-computing/</a> and <a href="http://www.ibm.com/developerworks/web/library/wa-cloudgrid/" rel="nofollow">http://www.ibm.com/developerworks/web/library/wa-cloudgrid/</a></p>\n\n<blockquote>\n  <p>While grid computing facilitates access to computing resources as if they were <strong>public utilities</strong>, cloud computing offers <strong>on-demand resource provisioning</strong>.</p>\n</blockquote>\n\n<p>The article says grid computing provide computing resources just like public utilities (such as electricity). Are public utilities different from on-demand resources provisioning?</p>\n\n<p>It also says one component of cloud computing is utility computing, just like paying for a public utility such as electricity:</p>\n\n<blockquote>\n  <p>Cloud computing is a development on grid computing. It requires three\n  fundamental components:</p>\n  \n  <ul>\n  <li>Thin clients (alternatively, clients with a thin-thick switch may be used)</li>\n  <li>Grid computing</li>\n  <li>Utility computing (i.e. paying for resources used from shared servers, similar to paying for a public utility, such as electricity)</li>\n  </ul>\n</blockquote>\n\n<p>So do both grid computing and cloud computing provide their services just like electricity?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '24', 'ClosedDate': '2014-01-29T17:04:07.717', 'Title': 'Differences in services provided by grid computing and cloud computing', 'LastEditorUserId': '336', 'LastActivityDate': '2014-01-29T15:09:40.367', 'LastEditDate': '2014-01-29T15:09:40.367', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<distributed-systems>', 'CreationDate': '2014-01-29T14:33:53.987', 'Id': '20062'},31_51:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have some experiences of working on clusters and grids for parallel computing. But I wonder what distributed computing can do besides parallel computing?\nIf I understand correctly, parallel computing is to solve a common task by multiple processors (computers).</p>\n\n<p>Are the Internet and a intranet considered distributed systems? They don't necessarily involve parallel computing, or solving a common task by multiple computers.</p>\n", 'ViewCount': '47', 'Title': 'What can distributed computing do besides parallel computing?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T18:19:58.970', 'LastEditDate': '2014-01-29T17:02:54.650', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<parallel-computing><distributed-systems>', 'CreationDate': '2014-01-29T15:08:31.753', 'FavoriteCount': '1', 'Id': '20064'},31_52:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>More specifically, how can project administrators be sure that the answers they receive from nodes are correct and not just simple calculations performed quickly and submitted as complete work units? As various cheating scandals have shown, people are not above stooping to low levels to get to the top of project points tables...</p>\n', 'ViewCount': '22', 'ClosedDate': '2014-02-10T17:42:07.087', 'Title': 'In a BOINC project, how are responses checked for accuracy?', 'LastActivityDate': '2014-02-10T14:46:30.517', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14546', 'Tags': '<distributed-systems>', 'CreationDate': '2014-02-10T13:14:34.780', 'Id': '21495'},31_53:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>While reading the research paper <a href="http://dl.acm.org/citation.cfm?id=2108244" rel="nofollow">Polylogarithmic Concurrent Data Structures from Monotone Circuits [@JACM\'2012]</a> by James Aspnes, Hagit Attiya, and Keren Censor-Hillel, I am not sure about some points and need some verification and explanation.  </p>\n\n<p>In the first three sections of the paper, the authors presents constructions of useful concurrent data structures, including <strong>max register</strong> and <strong>counters</strong> with bounded values, with step complexity that is <em>polylogarithmic</em> in the number of values the object can take or the number of operations applied to it. Specifically (and extremely in brief),  </p>\n\n<blockquote>\n  <p>The max register is an object $r$ supporting both <code>WriteMax(r,t)</code> and <code>ReadMax(r)</code> operations. It is recursively constructed from a tree of increasingly large max registers. The implementation is wait-free and linearizable.    </p>\n  \n  <p>The counter, supporting an <code>CounterIncrement()</code> operation and a <code>ReadCounter()</code> operation, is structured as a binary tree of max registers. The implementation is also wait-free and linearizable. </p>\n</blockquote>\n\n<p>My problems are as follows:</p>\n\n<blockquote>\n  <p>(1) <strong>On the max register:</strong> What is the space complexity, i.e., the number of base objects of multi-writer multi-reader (MWMR, for short) registers, of the recursive implementation?  </p>\n</blockquote>\n\n<p>[[<strong>In my opinion:</strong>]] It is $2m - 1$ for there is exactly one MWMR register for each node in the tree. In particular, the tree can be thought of as the logic structure of an underlying array of $2m-1$ MWMR registers.</p>\n\n<blockquote>\n  <p>(2) <strong>Also on the max register:</strong> Is it possible to implement a max register with only a single MWMR register? Are there any related work? </p>\n</blockquote>\n\n<p>[[<strong>(EDIT) In my opinion:</strong>]] I have found a <a href="http://www.cs.bgu.ac.il/~satcc112/wiki.files/Jayanti-Time-and-Space-Lower-Bounds.pdf" rel="nofollow">related paper: Time and Space Lower Bounds for Non-blocking Implementations [@PODC\'1996]</a>, in which Jayanti et al. show that </p>\n\n<blockquote>\n  <p>Operations must take $\\Omega(n)$ <strong>space</strong> and $\\Omega(n)$ steps in the worst case, for many common data structures, including (unbounded) max registers and (unbounded) counters, where $n$ is the number of concurrent processes.</p>\n</blockquote>\n\n<p>However, I have not realized similar conclusions concerning about value-bounded data structures. </p>\n\n<blockquote>\n  <p>(3) <strong>On both the max register and the counter:</strong> Are there any related work on the max&amp;min register supporting both <code>ReadMax(r)</code> and <code>ReadMin(r)</code>? Similarly, are there any related work on the inc&amp;dec counter supporting both <code>CounterIncrement()</code> and <code>CounterDecrement()</code>?</p>\n</blockquote>\n', 'ViewCount': '22', 'Title': 'Polylogarithmic value-bounded concurrent data structures such as max register, counter, and monotone circuit', 'LastEditorUserId': '4911', 'LastActivityDate': '2014-02-13T05:53:17.450', 'LastEditDate': '2014-02-13T05:53:17.450', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<reference-request><data-structures><distributed-systems><concurrency>', 'CreationDate': '2014-02-12T08:03:37.973', 'Id': '21561'},31_54:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been reading text books about distributed algorithms. They usually present a large collection of algorithms under various hypothesis and guarantees, with their proof of correctness. But they often fail to mention the practical application of those algorithms, and their relative usefulness and importance. For instance, various forms of broadcast, snapshot, election, consensus...</p>\n\n<p>My questions are :</p>\n\n<ol>\n<li>What are the most fundamental distributed algorithms (the few ones\nthat you really need to know)</li>\n<li>Could you give me very concrete cases where they are used</li>\n<li>What are the most widely used distributed algorithms </li>\n</ol>\n\n<p>Thanks in advance. I'm feeling overwhelmed :)</p>\n", 'ViewCount': '50', 'ClosedDate': '2014-02-15T04:33:47.070', 'Title': 'Widely-used and fundamental distributed algorithms', 'LastEditorUserId': '867', 'LastActivityDate': '2014-02-13T19:56:23.207', 'LastEditDate': '2014-02-13T19:56:23.207', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14642', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2014-02-13T16:23:37.980', 'Id': '21601'},31_55:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve stumbled at the first OralMessage algorithm in Lamport, et al\'s paper.</p>\n\n<p>I\'ve searched the web and there are dozens of sites, restating in exactly the same terms and examples, which isn\'t helping me.</p>\n\n<p>Lamport claims the algorithm can handle (n-1)/3 traitors, and works when the commander is a traitor.</p>\n\n<p>My restatement of the algorithm:</p>\n\n<ol>\n<li><p>The commander sends a value to each of the lieutenants.(round 0)</p></li>\n<li><p>Each lieutenant:\nforwards each message he receives to the other lieutenants:<br>\ndon\'t forward messages that already have your name          (eg you are b and receive \'cb1\')<br>\ndon\'t forward messages if they already have (N - 1)/3 names. (eg N=10 and you receive \'gcd0\')<br>\nadd your name to front of message before forwarding         (eg you are b and receive \'c0\', send \'bc0\')</p></li>\n<li><p>after all messages have been sent, each lieutenant:<br>\nexamines the received messages and makes their decision.<br>\nif its a tie, then decide 0.  </p>\n\n<p>I\'m not sure how to do 3, the paper says the algorithm "assumes a sequence of [majority] functions" (nested?)<br>\nIn the example, I\'m assuming to take the majority in each vector of round 2 (ie left to right), and then take the majority of these.  </p></li>\n</ol>\n\n<p>EXAMPLE</p>\n\n<p>Commander is a traitor, N=7, M=(7-1)/3=2, so 6 lieutenants one of whom is a traitor. I have assigned the lieutenants letters b-g.</p>\n\n<p>Here are the messages <strong>received</strong> at each node in rounds 1 &amp; 2, assuming a node can send to itself. (the messages in brackets are redundant from B\'s point of view. I don\'t know if this is important.):</p>\n\n<pre><code>&lt;(b1) ,c0   ,d0   ,eX   ,f1   ,g1   &gt;\n\n&lt;     ,(cb1),(db1),(ebX),(fb1),(gb1)&gt;\n&lt;(bc0),     ,dc0  ,ecX  ,fc0  ,gc0  &gt;\n&lt;(bd0),cd0  ,     ,edX  ,fd0  ,gd0  &gt;\n&lt;(beX),ceX  ,deX  ,     ,feX  ,geX  &gt;\n&lt;(bf1),cf1  ,df1  ,efX  ,     ,gf1  &gt;\n&lt;(bg1),cg1  ,dg1  ,egX  ,fg1  ,     &gt;\n</code></pre>\n\n<p>Note:<br>\n\'dc0\' is sent to everyone by \'d\' (because \'d\' was the last to prepend their name)<br>\n\'X\' indicates an unreliable message. \'e\' is a traitor and always sends unreliable messages  </p>\n\n<p>BUT \'step 3\' gives 1,0,0,X,1,1 which is no better than round 1.<br>\nAND the majority of these is 1 if X is 1, and 0 if X is 0. So the traitor can confound us.</p>\n\n<p>What am I doing wrong?</p>\n', 'ViewCount': '103', 'Title': "Lamport's Byzantine Generals Algorithm", 'LastEditorUserId': '12863', 'LastActivityDate': '2014-04-09T03:32:24.300', 'LastEditDate': '2014-03-09T04:58:42.557', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '15342', 'Tags': '<distributed-systems><fault-tolerance>', 'CreationDate': '2014-03-06T12:44:42.930', 'FavoriteCount': '1', 'Id': '22338'},31_56:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to implement various Randomized Distributed algorithms for MIS, Broadcast, Coloring in Radio Networks. I want use very high level language or any simulator for this. Please suggest on this.</p>\n', 'ViewCount': '23', 'ClosedDate': '2014-03-10T13:16:57.813', 'Title': 'simulation of Randomized distributed algorithms', 'LastActivityDate': '2014-03-09T12:46:57.177', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15446', 'Tags': '<distributed-systems><simulation>', 'CreationDate': '2014-03-09T12:46:57.177', 'FavoriteCount': '1', 'Id': '22427'},31_57:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was reading the following FDS paper:</p>\n\n<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf" rel="nofollow">https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf</a></p>\n\n<p>and it says that the the following hash function does not distribute things uniformly (instead it creates a binomial distribution):</p>\n\n<p>$$i = (hash(g + t)) \\pmod n$$</p>\n\n<p>while the following did distribute things uniformly:</p>\n\n<p>$$i = (hash(g) + t) \\pmod n$$</p>\n\n<p>Why does the above distribute things evenly while the other one doesn\'t?</p>\n\n<p>g = is the global UID for a blob</p>\n\n<p>t = tract number. (tract is the measure/units of reads and writes to a blob).</p>\n\n<p>n = is the number of tract servers or a multiple of the tract servers.</p>\n\n<p>hash = SHA-1 (according to the paper)</p>\n\n<hr>\n\n<p>Paper Reference:</p>\n\n<p>Title: Flat Datacenter Storage</p>\n\n<p>Author(s): Edmund B. Nightingale, Jeremy Elson, Jinliang Fan, Owen Hofmann, Jon Howell, Yutaka Suzue</p>\n\n<p>Institution(s): Microsoft Research, Univeristy of Texas at Austin</p>\n', 'ViewCount': '28', 'Title': 'Why does the following function distribute things in a binomial distribution?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-03-28T04:58:28.227', 'LastEditDate': '2014-03-28T04:58:28.227', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<distributed-systems><hash><hash-tables>', 'CreationDate': '2014-03-28T00:27:13.283', 'Id': '23153'},31_58:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was reading the following FDS paper:</p>\n\n<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf" rel="nofollow">https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf</a></p>\n\n<p>The paper has a TLT (tract locator table) for identifying where to write in each server. The table is basically a hash table where the index i maps to some (tract) server to do writes to or reads to.</p>\n\n<p>Say that there are n (tract) servers. It says that if instead of just having index i mapping to server i, if instead we appended n more indexes mapping to some permutation of the servers (and having a table of size 2n in this example), we can avoid having a bottleneck at any specific server when doing reads or writes. Basically it claims that as we append more permutations the better. Why is this?</p>\n\n<p>Why would this be?</p>\n\n<p>Just for reference the index mapping function is:</p>\n\n<p>$$i = (hash(g) + t) \\pmod n$$</p>\n\n<p>where:</p>\n\n<p>g = is the global UID for a blob</p>\n\n<p>t = tract number. (tract is the measure/units of reads and writes to a blob).</p>\n\n<p>n = is the number of tract servers or a multiple of the tract servers.</p>\n\n<p>hash = SHA-1 (according to the paper)</p>\n\n<hr>\n\n<p>Paper Reference:</p>\n\n<p>Title: Flat Datacenter Storage</p>\n\n<p>Author(s): Edmund B. Nightingale, Jeremy Elson, Jinliang Fan, Owen Hofmann, Jon Howell, Yutaka Suzue</p>\n\n<p>Institution(s): Microsoft Research, Univeristy of Texas at Austin</p>\n', 'ViewCount': '89', 'Title': 'Why does appending permutations of servers at the end of hash table avoid bottlenecks?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-03-28T04:58:50.027', 'LastEditDate': '2014-03-28T04:58:50.027', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23159', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<distributed-systems><hash><hash-tables>', 'CreationDate': '2014-03-28T00:36:28.957', 'Id': '23155'},31_59:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was reading the following computer systems paper:</p>\n\n<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf" rel="nofollow">https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf</a></p>\n\n<p>And I was trying to understand why it claims that it does not need data locality for it be perform well.</p>\n\n<p>Basically, in the abstract it says:</p>\n\n<p>"...FDS multiplexes an application\'s large scale I/O across the available throughput and latency budget of every disk in a closer. FDS therefore makes many optimizations around data locality unnecessary."</p>\n\n<p>What I was not sure was, why does multiplexing an applications I/O across the servers in the datacenter, make it unnecessary to have to optimize in terms of data locality?</p>\n\n<p>Basically, as I read the paper they make a big deal that locality is something they don\'t need and that they can achieve the benefits of it without having servers being close to the clients they provide data access. The part I am not sure is, what is the crux of what makes them not depend on data locality? Is it the CLOS network? Is it because they don\'t serve clients around the world? Is it because their servers are all in one data center anyways and not scattered around the world? What is it that makes their performance be so amazing?</p>\n\n<hr>\n\n<p>Paper Reference:</p>\n\n<p>Title: Flat Datacenter Storage</p>\n\n<p>Author(s): Edmund B. Nightingale, Jeremy Elson, Jinliang Fan, Owen Hofmann, Jon Howell, Yutaka Suzue</p>\n\n<p>Institution(s): Microsoft Research, Univeristy of Texas at Austin</p>\n', 'ViewCount': '18', 'Title': 'How does FDS (flat datacenter storage) make optimizations around locality unnecessary?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-03-28T12:39:47.403', 'LastEditDate': '2014-03-28T04:59:30.653', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<distributed-systems><fault-tolerance>', 'CreationDate': '2014-03-28T03:37:05.833', 'Id': '23163'},31_60:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was wondering, what is so special about a majority of nodes having a value? Why is that the key fact for consensus to work on Paxos?</p>\n\n<p>It says on the paper that "any two majorities have at least one acceptor in common, this works if an acceptor can accept at most one value" (page 2). Its true that if we have two majorities then at least one node between the two must be the same one, but I was not entirely sure if I fully appreciated why that was special for consensus. How does this guarantee that a single value gets accepted? Or how does it aid in that goal?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '29', 'Title': 'What is so special about a majority and why is it the key for Paxos to work? (Paxos made simple)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:34:51.380', 'LastEditDate': '2014-04-30T16:34:51.380', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T06:45:51.627', 'FavoriteCount': '2', 'Id': '23165'},31_61:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>On page 3 it says "Since all numbers are totally ordered, condition P2 guarantees the crucial safety property that only a single value is chosen."</p>\n\n<p>Where P2 = "If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v."</p>\n\n<p>What I was a little confused was what "totally ordered" meant and how does it guarantee p2?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '34', 'Title': 'What does it mean that numbers are "totally ordered"? (paxos made simple)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:34:05.207', 'LastEditDate': '2014-04-30T16:34:05.207', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23186', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T16:57:03.753', 'Id': '23185'},31_62:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was reading page 4 of the paper where it says condition $P2^c$:</p>\n\n<p>"For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that </p>\n\n<ul>\n<li><p>(a) no acceptor in S has accepted any proposal numbered less than n, or </p></li>\n<li><p>(b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S."</p></li>\n</ul>\n\n<p>(Where S = any set consisting of a majority of acceptors. \nC = the set of acceptors that have accepted some value c, the letter C stands for the majority that has Chosen a value).</p>\n\n<p>I was trying to understand better the conditions for issuing a proposal, specifically (b) is the one causing troubles for me.</p>\n\n<p>For me (a), makes sense because we don\'t want to issue a new proposal with a higher sequence number if there has been any proposal that has been chosen from the majority we are able to see (i.e. S). Since anything that has been chosen is accepted and since something chosen is part of the majority C, if we issue a new proposal, we could risk confusing the current paxos instance when its already chosen a value.</p>\n\n<p>However, (b) is less clear to me why we want it to hold. Recall (b) is:</p>\n\n<p>(b) = "v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S".</p>\n\n<p>Why are we interested in having that condition? Which safety properties does that condition help us maintain?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '54', 'Title': 'Rational of why Paxos only issues new values if its value is the largest one in the majority that he can see?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:33:18.723', 'LastEditDate': '2014-04-30T16:33:18.723', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T17:50:41.200', 'FavoriteCount': '1', 'Id': '23187'},31_63:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>On page 4 of the paper, at the bottom where they outline what a prepare request looks like it says:</p>\n\n<blockquote>\n  <p>"A proposer chooses a new proposal number n and sends a request to\n  each member of some set of acceptors, asking it to respond with:</p>\n  \n  <p>(a) A promise never again to accept a proposal numbered less than n, </p>\n  \n  <p>(b) ... "</p>\n</blockquote>\n\n<p>Why does prepare message extract such promise from the set of acceptors that receive such a prepare message? What is the goal and what safety guarantee does it provide? How does it aid Paxos into reaching a consensus to a certain value? How is it crucial so that a majority can reach agreement?</p>\n\n<p>I did some further research to try to answer this question and went to the following yale university notes:</p>\n\n<p><a href="http://pine.cs.yale.edu/pinewiki/Paxos" rel="nofollow">http://pine.cs.yale.edu/pinewiki/Paxos</a></p>\n\n<p>These notes try to justify this point by saying:</p>\n\n<blockquote>\n  <p>"... proposer tests the waters by sending a prepare(n) message to all\n  accepters where n is the proposal number. An accepter responds to this\n  with a promise never to accept any proposal with a number less than n\n  (<strong>so that old proposals don\'t suddenly get ratified</strong>)... "</p>\n</blockquote>\n\n<p>Basically, the way they justify that step is by saying so that old proposals don\'t get ratified/accepted. Which I kind of see why they said that, since if a decision has happened, that rule definitively avoids the consensus from being reversed, which is nice. But, however, the reason I did not feel very convinced about it yet is because it seems to damage termination a lot. What if if a majority was about to be formed in the past and the proposer was ready to send the propose(n) and now because a different prepare(n\') was sent its not able to reach agreement? I can\'t seem to convince myself that that this step is 100% a good idea. </p>\n\n<p>Unless the only reason for that step is because they just want to guarantee that once a value has been chosen/decided, by adopting this rule, its impossible that any old proposal reverses the decision?</p>\n\n<p>Again, I did some further research by reading the yale article and now it says:</p>\n\n<blockquote>\n  <p>"The rule that an acceptor doesn\'t accept any proposal earlier than a\n  round it has acknowledged means that the value v in an ack(n, v, n_v)\n  message never goes out of date\u2014there is no possibility that an\n  acceptor might retroactively accept some later value in round n\' with\n  nv &lt; n\' &lt; n. So the ack message values tell a consistent story about\n  the history of the protocol, even if the rounds execute out of order."</p>\n</blockquote>\n\n<p>However, its not obvious to me why retroactivity might be bad (unless in the case that I already mentioned). Retroactivity could be good if it somehow manages our protocol to reach consensus. Still the only reason I see for such a rule is so that one doesn\'t reverse a decision by accident once a value has been agreed on. </p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '55', 'Title': 'Why does a prepare message wants a promise that an acceptor is never to accept a proposal numbered less than its propose sequence value? (Paxos)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T17:25:43.177', 'LastEditDate': '2014-04-30T17:25:43.177', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T18:47:38.740', 'Id': '23190'},31_64:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and was wondering, what guarantees Paxos to converge and not run forever without a consensus/agreement on a value? Is it guaranteed to converge always or is it a probabilistic bound that the probability that it does not converge is really really small?</p>\n\n<p>Because I was reading condition $P2^c$ and it seems possible to me that because of that condition, Paxos might loop forever if we are not careful (I think, maybe I am wrong, but I would love to know why!) Take the following case:</p>\n\n<p>I was a little worried about the case when, say that a majority was close to forming but there is a new node that wants to propose his new value but was unlucky and did not happen to communicate with that growing set that nearly formed a majority (its not a majority yet, but it was reaalllyyy close!). That node was ready to prepare his value but was that unlucky and reasoned "Ok, none of the nodes I spoke to had a value, thus, time to propose my value!" but his sequence number is much higher than the previous one that was forming on the other side and it starts to spread, wouldn\'t it be because we want to satisfy (b) in $P2^c$? i.e. doesn\'t it damage the convergence to a decision (never mind the run time, it might never converge...)? Even in the case of a very good quality network..?</p>\n\n<p>Because now this new node has a higher value and his value spreads like a disease and it <em>could</em> happen again, right before it was about to form a majority with his own value. That situation could happen again and again and again no value is decided! Right? What prevents Paxos from being in this situation? Or what is the argument to convince me that its really unlikely to happen many times and that it converges in polynomial time most of the time?</p>\n\n<p>Recall condition $P2^c$ is the following:</p>\n\n<p>$P2^c$</p>\n\n<p>"For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that\n(a) no acceptor in S has accepted any proposal numbered less than n,\nor\n(b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S."</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '36', 'Title': 'What guarantees Paxos to converge (terminate)? (i.e. not run forever without a consensus)', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T10:20:41.300', 'LastEditDate': '2014-04-29T10:20:41.300', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2014-03-29T03:28:29.840', 'Id': '23208'},31_65:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was confused about one specific part. How does or why does property $P2^b$ satisfy property $P2^c$?</p>\n\n<p>These are the properties:</p>\n\n<p>$P2^b = $ If a proposal with value $v$ is chosen, then every higher-numbered (i.e. approx. later in time)  proposal issue by any proposer has value $v$.</p>\n\n<p>$P2^c$ = For any $v$ and $n$, if a proposal with value $v$ and number $n$ is issued, then there is a set $S$ (some majority) of acceptors such that either:</p>\n\n<p>(a)no acceptor in $S$ has accepted any proposal numbered less than $n$, or</p>\n\n<p>(b)$v$ is the value of the highest-numbered proposal among all proposals numbered less than $n$ accepted by the acceptors in $S$ (some majority).</p>\n\n<p>The paper uses $S$ to denote some majority and $C$ to denote some majority that has actually <em>chosen</em> a value.</p>\n\n<p>The thing that I am confused about is, for me $P2^b$ is saying, ok once a value has been chosen, say at sequence number $n$ (i.e. roughly time $n$), then after that time, we want to make sure that any proposer is only able to propose the value of the majority (chosen value). If we have that, then, we do not risk the already formed majority from reverting weirdly. i.e. once we have formed a majority, we want it to stick and stay like that. However, it was not 100% clear to me why property $P2^c$ satisfied that requirement. I kind of see why (a) is a nice property to have, since, having (a) means that its safe to issue a new proposal $(n, v)$ since we contacted some majority $S$ and none of them had accepted anything in a time earlier than now $n$. So, if a majority had formed we would have seen at least one value and we did not see anything accepted, its safe to propose something since a majority has not formed.  </p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '16', 'Title': "How do we make sure in Paxos that we don't propose a different value if a majority has formed?", 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:35:20.363', 'LastEditDate': '2014-04-30T16:35:20.363', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-30T06:57:57.560', 'FavoriteCount': '1', 'Id': '23247'},31_66:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was trying to understand page 4, specifically, the following paragraph:</p>\n\n<p>"To maintain the invariance of $P2^c$, a proposer that wants to issue a proposal\nnumbered n must learn the highest-numbered proposal with number\nless than n, if any, that has been or <strong>will be accepted by each acceptor in\nsome majority of acceptors</strong>. Learning about proposals already accepted is\neasy enough; predicting future acceptances is hard. Instead of trying to predict\nthe future, the proposer controls it by extracting a promise that there\nwon\u2019t be any such acceptances. In other words, the proposer requests that\nthe acceptors not accept any more proposals numbered less than n. This\nleads to the following algorithm for issuing proposals."</p>\n\n<p>Where condition $P2c$ is:</p>\n\n<p>"For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that</p>\n\n<p>(a) no acceptor in S has accepted any proposal numbered less than n, or</p>\n\n<p>(b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S."</p>\n\n<p>I was specifically confused about the section I have in bold. I was confused why, if we want to develop some hypothetical consensus distributed algorithm, why do we need to know about higher sequence numbers that have not happened? What is the intuition behind that?</p>\n\n<p>Sorry if my title is a little strange, I was not sure what was a good title for the question.</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '38', 'Title': 'How does Paxos maintain the promise that future proposal will not break the current chosen value without knowing the future?', 'LastActivityDate': '2014-05-02T00:57:28.797', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<distributed-systems>', 'CreationDate': '2014-04-01T06:06:13.900', 'Id': '23312'},31_67:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>To understand the effectiveness and/or efficiency of a protocol implemented in distributed storage systems, it is often desirable to evaluate it in a quantitative way under a hypothetical model. </p>\n\n<p>Obviously, every model is an approximation to the reality and there is not an all-purpose one. I expect to do a mini survey of the models of distributed storage systems in the literature. Specifically, I am interested in the models on <em>system throughput, read/write rate, read/write pattern, read/write duration (interval), read/write latency, communication delay, failure pattern, and more professional ones such as data consistency, quorum size, recovery mechanism, and so on</em>. </p>\n\n<p>Therefore, I am requesting references, including but not limited to</p>\n\n<ol>\n<li>statistical reports of commercial/open source distributed storage systems</li>\n<li>hypothetical models used in research papers.<br>\na typical example: <a href="http://www.cs.odu.edu/~mukka/cs775s07/papers/availability.pdf" rel="nofollow">costs and availability of replicated services\n</a> which studies availability, consistency, partition. </li>\n<li>experimental results given in research papers, talks, and technical reports.<br>\ntypical examples: <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" rel="nofollow">Dynamo in Amazon [section 6]</a> and <a href="http://infolab.stanford.edu/~usriv/papers/pnuts.pdf%5d" rel="nofollow">PNUTS in Yahoo! [section 5]</a>.</li>\n<li>personal comments on such topics from blogs, non-research articles, and you.</li>\n</ol>\n', 'ViewCount': '34', 'Title': 'What are the models of distributed storage systems in the literature?', 'LastEditorUserId': '4911', 'LastActivityDate': '2014-04-13T08:22:19.077', 'LastEditDate': '2014-04-13T08:22:19.077', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<reference-request><distributed-systems>', 'CreationDate': '2014-04-12T06:19:43.117', 'Id': '23690'},31_68:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m studying Distributed Systems and synchronization and I didn\'t catch this solution of totally ordered multicast with Lamport timestamps. I read that it doesn\'t need ack to deliver a message to the application, but</p>\n\n<blockquote>\n  <p>"It is sufficient to multicast any other type of message, as long as that message has a timestamp larger than the received message. The condition for delivering a message m to the application, is that another message has been received from each other process with a large timestamp. This guarantees that there are no more messages underway with a lower timestamp."</p>\n</blockquote>\n\n<p>This is a definition from a book. I tried to apply this definition to an example but I guess that something is wrong.</p>\n\n<h3>Example.</h3>\n\n<blockquote>\n  <p>There are 4 processes and they multicast the following messages (second number in parentheses is timestamp) :<br>\n  P1 multi-casts (m11, 5);  (m12, 12); (m13, 14);<br>\n  P2 multi-casts (m21, 6); (m22, 14);<br>\n  P3 multi-casts (m31, 5); (m32, 7); (m33, 11);<br>\n  P4 multi-casts (m41, 8); (m42, 15); (m43, 19).</p>\n</blockquote>\n\n<p>Supposing that there are no acknoledgments, can I guess which messages can be delivered and which not? Based on definition, my guess is that only m11 and m31 can be delivered to the application, because all the other messages received will have a timestamp greater, but this seems very strange, and I think I didn\'t understand the delivery condition very well. I have an exam next week and in general I\'d like to understand this mechanism.</p>\n', 'ViewCount': '73', 'Title': 'totally ordered multicast with Lamport timestamp', 'LastEditorUserId': '16819', 'LastActivityDate': '2014-04-20T13:25:52.457', 'LastEditDate': '2014-04-20T13:25:52.457', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16819', 'Tags': '<distributed-systems><computer-networks><synchronization><communication-protocols><message-passing>', 'CreationDate': '2014-04-16T07:26:03.173', 'FavoriteCount': '1', 'Id': '23847'},31_69:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was reading the Paxos notes from yale from the following link:</p>\n\n<p><a href="http://pine.cs.yale.edu/pinewiki/Paxos" rel="nofollow">http://pine.cs.yale.edu/pinewiki/Paxos</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was trying to better understand the revoting mechanism to avoid deadlocks in Paxos. The revoting mechanism is explained as follows in the article: </p>\n\n<blockquote>\n  <p>The revoting mechanism now works like this: before taking a vote, a\n  proposer tests the waters by sending a prepare(n) message to all\n  accepters where n is the proposal number. An accepter responds to this\n  with a promise never to accept any proposal with a number less than n\n  (so that old proposals don\'t suddenly get ratified) <strong>together with\n  the highest-numbered proposal that the accepter has accepted</strong> (so\n  that the proposer can substitute this value for its own, in case the\n  previous value was in fact ratified).</p>\n</blockquote>\n\n<p>The bold section is the one that I was trying to understand better. The author tries to justify it with:</p>\n\n<blockquote>\n  <p>"...so that the proposer can substitute this value for its own, in case the\n  previous value was in fact ratified..."</p>\n</blockquote>\n\n<p>But I didn\'t really understand why the proposer would want to ratify the previous value. By doing this, what crucial safety property is he guaranteeing? Why is he responding with that and not something else? Responding with the highest could be problem, right, since the current proposal would get lost?</p>\n', 'ViewCount': '22', 'Title': 'Why does an acceptor send the highest numbered proposal with number less than n as a response to prepare(n) in paxos?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:32:48.753', 'LastEditDate': '2014-04-30T16:32:48.753', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-04-29T04:00:18.340', 'Id': '24209'},31_70:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '28', 'Title': "Suzuki Kasami's Broadcast algorithm", 'LastEditDate': '2014-04-30T08:09:21.650', 'AnswerCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17207', 'FavoriteCount': '0', 'Body': "<p>I have referred wikipedia and i have the following doubt...</p>\n\n<p>We want to achieve mutual exclusion in distributed systems through a token based algorithm.</p>\n\n<p>What i would do?\nSuppose there are 4 sites S1,S2,S3,S4 and each site has a queue  of pending req (Q).</p>\n\n<p>Suppose S1 has token and it wants to go to CS it goes directly.Now suppose S2 requests for the token S1 would put that req in Q.After it has exited from CS it would pass the token to the head of the queue...i.e req which arrived 1st.</p>\n\n<p>DOUBT:: But in Suzuki-Kasami's algorithm for what reason have they taken LN[1..N] and RN[1..N]??</p>\n", 'ClosedDate': '2014-05-03T23:50:40.483', 'Tags': '<algorithms><distributed-systems>', 'LastEditorUserId': '17207', 'LastActivityDate': '2014-04-30T08:09:21.650', 'CommentCount': '0', 'CreationDate': '2014-04-30T07:38:31.890', 'Id': '24252'},31_71:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know Distributed Graph Coloring algorithm in O(log* n)\nwhich is given at P11: <a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">Vertex Coloring</a></p>\n\n<p>same for Maximal Independent Set [MIS] they gave remark like algorithms exist in O(log* n) time at P70: <a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">Maximal Independetn Set</a></p>\n\n<p>How we can reduce Graph coloring problem to MIS in O(log* n) time?</p>\n\n<p>If you feel difficulty in understanding algorithm then please comment. </p>\n', 'ViewCount': '11', 'Title': 'MIS algorithm for Tree in O(log* n) time', 'LastActivityDate': '2014-05-04T02:29:39.507', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9665', 'Tags': '<algorithms><graph-theory><data-structures><distributed-systems>', 'CreationDate': '2014-05-03T22:14:11.747', 'Id': '24369'}