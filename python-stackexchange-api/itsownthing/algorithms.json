{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '45533', 'Title': 'Why is quicksort better than other sorting algorithms in practice?', 'LastEditDate': '2012-03-23T13:15:39.290', 'AnswerCount': '10', 'Score': '90', 'PostTypeId': '1', 'OwnerUserId': '24', 'FavoriteCount': '58', 'Body': '<p>In a standard algorithms course we are taught that <strong>quicksort</strong> is $O(n \\log n)$ on average and $O(n^2)$ in the worst case. At the same time, other sorting algorithms are studied which are $O(n \\log n)$ in the worst case (like <strong>mergesort</strong> and <strong>heapsort</strong>), and even linear time in the best case (like <strong>bubblesort</strong>) but with some additional needs of memory.</p>\n\n<p>After a quick glance at <a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms">some more running times</a> it is natural to say that quicksort <strong>should not</strong> be as efficient as others.</p>\n\n<p>Also, consider that students learn in basic programming courses that recursion is not really good in general because it could use too much memory, etc. Therefore (and even though this is not a real argument), this gives the idea that quicksort might not be really good because it is a recursive algorithm.</p>\n\n<p><strong>Why, then, does quicksort outperform other sorting algorithms in practice?</strong> Does it have to do with the structure of <em>real-world data</em>? Does it have to do with the way memory works in computers? I know that some memories are way faster than others, but I don\'t know if that\'s the real reason for this counter-intuitive performance (when compared to theoretical estimates).</p>\n\n<hr>\n\n<p><strong>Update 1:</strong> a canonical answer is saying that the constants involved in the $O(n\\log n)$ of the average case are smaller than the constants involved in other $O(n\\log n)$ algorithms. However, I have yet to see a proper justification of this, with precise calculations instead of intuitive ideas only.</p>\n\n<p>In any case, it seems like the real difference occurs, as some answers suggest, at memory level, where implementations take advantage of the internal structure of computers, using, for example, that cache memory is faster than RAM. The discussion is already interesting, but I\'d still like to see more detail with respect to memory-management, since it appears that <em>the</em> answer has to do with it.</p>\n\n<hr>\n\n<p><strong>Update 2:</strong> There are several web pages offering a comparison of sorting algorithms, some fancier than others (most notably <a href="http://www.sorting-algorithms.com/">sorting-algorithms.com</a>). Other than presenting a nice visual aid, this approach does not answer my question.</p>\n', 'Tags': '<algorithms><sorting>', 'LastEditorUserId': '24', 'LastActivityDate': '2014-03-12T10:54:49.140', 'CommentCount': '10', 'AcceptedAnswerId': '90', 'CreationDate': '2012-03-06T19:11:07.127', 'Id': '3'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1842', 'Title': 'Generating Combinations from a set of pairs without repetition of elements', 'LastEditDate': '2012-03-07T14:20:14.907', 'AnswerCount': '6', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '59', 'FavoriteCount': '3', 'Body': '<p>I have a set of pairs. Each pair is of the form (x,y) such that x,y belong to integers from the range <code>[0,n)</code>.</p>\n\n<p>So, if the n is 4, then I have the following pairs:</p>\n\n<pre><code>(0,1) (0,2) (0,3)\n(1,2) (1,3) \n(2,3) \n</code></pre>\n\n<p>I already have the pairs. Now, I have to build a combination using <code>n/2</code> pairs such that none of the integers are repeated (in other words, each integer appears at least once in the final combination). Following are the examples of a correct and an incorrect combination for better understanding</p>\n\n<pre><code> 1. (0,1)(1,2) [Invalid as 3 does not occur anywhere]\n 2. (0,2)(1,3) [Correct]\n 3. (1,3)(0,2) [Same as 2]\n</code></pre>\n\n<p>Can someone suggest me a way to generate all possible combinations, once I have the pairs.</p>\n', 'Tags': '<algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-07T15:45:01.743', 'CommentCount': '8', 'AcceptedAnswerId': '18', 'CreationDate': '2012-03-06T19:54:40.243', 'Id': '11'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '4932', 'Title': 'Evaluating the average time complexity of a given bubblesort algorithm.', 'LastEditDate': '2012-03-07T18:26:03.053', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '78', 'FavoriteCount': '1', 'Body': '<p>Considering this pseudo-code of a bubblesort:</p>\n\n<pre><code>FOR i := 0 TO arraylength(list) STEP 1  \n    switched := false\n    FOR j := 0 TO arraylength(list)-(i+1) STEP 1\n        IF list[j] &gt; list[j + 1] THEN\n            switch(list,j,j+1)\n            switched := true\n        ENDIF\n    NEXT\n    IF switched = false THEN\n        break\n    ENDIF\nNEXT\n</code></pre>\n\n<p>What would be the basic ideas I would have to keep in mind to evaluate the average time-complexity? I already accomplished calculating the worst and best cases, but I am stuck  deliberating how to evaluate the average complexity of the inner loop, to form the equation. </p>\n\n<p>The worst case equation is:</p>\n\n<p>$$\r\n\\sum_{i=0}^n \\left(\\sum_{j=0}^{n -(i+1)}O(1) + O(1)\\right) = O(\\frac{n^2}{2} + \\frac{n}{2}) = O(n^2)\r\n$$</p>\n\n<p>in which the inner sigma represents the inner loop, and the outer sigma represents the outer loop. I think that I need to change both sigmas due to the "if-then-break"-clause, which might affect the outer sigma but also due to the if-clause in the inner loop, which will affect the actions done during a loop (4 actions + 1 comparison if true, else just 1 comparison).</p>\n\n<p>For clarification on the term average-time: This sorting algorithm will need different time on different lists (of the same length), as the algorithm might need more or less steps through/within the loops until the list is completely in order. I try to find a mathematical (non statistical way) of evaluating the average of those rounds needed. </p>\n\n<p>For this I expect any order to be of the same possibility.</p>\n', 'Tags': '<algorithms><time-complexity><average-case>', 'LastEditorUserId': '12', 'LastActivityDate': '2013-01-30T17:00:13.023', 'CommentCount': '16', 'AcceptedAnswerId': '26', 'CreationDate': '2012-03-06T20:51:24.880', 'Id': '20'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3042', 'Title': 'How does one know which notation of time complexity analysis to use?', 'LastEditDate': '2013-06-06T14:12:06.230', 'AnswerCount': '3', 'Score': '41', 'PostTypeId': '1', 'OwnerUserId': '110', 'FavoriteCount': '18', 'Body': '<p>In most introductory algorithm classes, notations like $O$ (Big O) and $\\Theta$ are introduced, and a student would typically learn to use one of these to find the time complexity.</p>\n\n<p>However, there are other notations, such as $o$, $\\Omega$ and $\\omega$. Are there any specific scenarios where one notation would be preferable to another?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2014-04-01T22:12:52.083', 'CommentCount': '1', 'AcceptedAnswerId': '61', 'CreationDate': '2012-03-07T01:42:10.933', 'Id': '57'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to generate a completely random <a href="http://en.wikipedia.org/wiki/Sudoku" rel="nofollow">Sudoku</a>.</p>\n\n<p>Define a Sudoku grid as a $9\\times9$ grid of integers between $1$ and $9$ where some elements can be omitted. A grid is a valid puzzle if there is a <strong>unique</strong> way to complete it to match the Sudoku constraints (each line, column and aligned $3\\times3$ square has no repeated element) and it is minimal in that respect (i.e. if you omit any more element the puzzle has multiple solutions).</p>\n\n<p>How can I generate a random Sudoku puzzle, such that all Sudoku puzzles are equiprobable?</p>\n', 'ViewCount': '1340', 'Title': 'Random Sudoku generator', 'LastEditorUserId': '39', 'LastActivityDate': '2012-03-09T16:52:33.813', 'LastEditDate': '2012-03-09T16:52:33.813', 'AnswerCount': '1', 'CommentCount': '20', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '122', 'Tags': '<algorithms><random><sudoku>', 'CreationDate': '2012-03-07T05:09:28.573', 'FavoriteCount': '2', 'Id': '72'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '227', 'Title': 'Is Smoothed Analysis used outside academia?', 'LastEditDate': '2012-05-16T23:22:33.777', 'AnswerCount': '2', 'Score': '15', 'OwnerDisplayName': 'user20', 'PostTypeId': '1', 'FavoriteCount': '2', 'Body': '<p>Did the <a href="http://en.wikipedia.org/wiki/Smoothed_analysis">smoothed analysis</a> find its way into main stream analysis of algorithms? Is it common for algorithm designers to apply smoothed analysis to their algorithms?</p>\n', 'Tags': '<algorithms><complexity-theory><algorithm-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-17T23:41:36.817', 'CommentCount': '6', 'AcceptedAnswerId': '100', 'CreationDate': '2012-03-07T06:57:12.917', 'Id': '74'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I need to create a recursive algorithm to see if a binary tree is a binary search tree as well as count how many complete branches are there (a parent node with both left and right children nodes) with an assumed global counting variable. This is an assignment for my data structures class. </p>\n\n<p>So far I have</p>\n\n<pre><code>void BST(tree T) {\n   if (T == null) return\n   if ( T.left and T.right) {\n      if (T.left.data &lt; T.data or T.right.data &gt; T.data) {\n        count = count + 1\n        BST(T.left)\n        BST(T.right)\n      }\n   }\n}\n</code></pre>\n\n<p>But I can't really figure this one out. I know that this algorithm won't solve the problem because the count will be zero if the second if statement isn't true.</p>\n\n<p>Could anyone help me out on this one? </p>\n", 'ViewCount': '1727', 'Title': 'Algorithm to test whether a binary tree is a search tree and count complete branches', 'LastEditorUserId': '79', 'LastActivityDate': '2012-03-07T22:25:59.700', 'LastEditDate': '2012-03-07T22:25:59.700', 'AnswerCount': '3', 'CommentCount': '14', 'AcceptedAnswerId': '113', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '79', 'Tags': '<algorithms><recursion><trees>', 'CreationDate': '2012-03-07T19:47:33.067', 'Id': '105'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '639', 'Title': "Are there improvements on Dana Angluin's algorithm for learning regular sets", 'LastEditDate': '2012-04-03T03:49:58.997', 'AnswerCount': '1', 'Score': '21', 'PostTypeId': '1', 'OwnerUserId': '55', 'FavoriteCount': '6', 'Body': '<p>In her 1987 seminal paper Dana Angluin presents a polynomial time algorithm for learning a DFA from membership queries and theory queries (counterexamples to a proposed DFA).</p>\n\n<p>She shows that if you are trying to learn a minimal DFA with $n$ states, and your largest countexample is of length $m$, then you need to make $O(mn^2)$ membership-queries and at most $n - 1$ theory-queries.</p>\n\n<p>Have there been significant improvements on the number of queries needed to learn a regular set?</p>\n\n<hr>\n\n<h3>References and Related Questions</h3>\n\n<ul>\n<li><p>Dana Angluin (1987) "Learning Regular Sets from Queries and Counterexamples", Infortmation and Computation 75: 87-106</p></li>\n<li><p><a href="http://cstheory.stackexchange.com/q/10958/1037">Lower bounds for learning in the membership query and counterexample model</a></p></li>\n</ul>\n', 'Tags': '<algorithms><learning-theory><machine-learning>', 'LastEditorUserId': '55', 'LastActivityDate': '2013-06-16T02:15:47.507', 'CommentCount': '4', 'AcceptedAnswerId': '1021', 'CreationDate': '2012-03-08T01:12:58.203', 'Id': '118'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Initially, <a href="http://en.wikipedia.org/wiki/Matroid">Matroids</a> were introduced to generalize the notions of linear independence of a collection of subsets $E$ over some ground set $I$. Certain problems that contain this structure permit greedy algorithms to find optimal solutions. The concept of <a href="http://en.wikipedia.org/wiki/Greedoid">Greedoids</a> was later introduced to generalize this structure to capture more problems that allow for optimal solutions to be found by greedy methods.</p>\n\n<p>How often do these structures arise in algorithm design? </p>\n\n<p>Furthermore, more often than not a greedy algorithm will not be able to fully capture what is necessary to find optimal solutions, but may still find very good approximate solutions (Bin Packing for example). Given that, is there a way to measure how "close" a problem is to a greedoid/matroid?</p>\n', 'ViewCount': '366', 'Title': 'How fundamental are matroids and greedoids in algorithm design?', 'LastActivityDate': '2012-03-08T03:56:04.670', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '124', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '19', 'Tags': '<algorithms><combinatorics><optimization>', 'CreationDate': '2012-03-08T01:48:58.297', 'FavoriteCount': '5', 'Id': '119'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to know if the following problem is decidable and how to find out. Every problem I see I can say "yes" or "no" to it, so are most problems and algorithms decidable except a few (which is provided <a href="http://en.wikipedia.org/wiki/List_of_undecidable_problems" rel="nofollow">here</a>)?</p>\n\n<blockquote>\n  <p>Input: A directed and finite graph $G$, with $v$ and $u$ as vertices<br>\n  Question: Does a path in $G$ with $u$ as initial vertex and $v$ as final vertex exist?</p>\n</blockquote>\n', 'ViewCount': '327', 'Title': 'Is this finite graph problem decidable? What factors make a problem decidable?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-30T21:37:25.987', 'LastEditDate': '2012-04-30T21:37:25.987', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '153', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '51', 'Tags': '<algorithms><computability><graph-theory><undecidability>', 'CreationDate': '2012-03-09T00:47:22.190', 'Id': '143'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '668', 'Title': 'Type-checking algorithms', 'LastEditDate': '2012-04-29T12:25:23.307', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '95', 'FavoriteCount': '4', 'Body': '<p>I am starting a personal bibliographic research on type-checking algorithms and want some tips. What are the most commonly used type-checking algorithms, strategies and general techniques?</p>\n\n<p>I am particularly interested in complex type-checking algorithms that were implemented in widely known strongly static typed languages such as, for example, C++, Java 5+, Scala or others. I.E, type-checking algorithms that are not very simple due to the very simple typing of the underlying language (like Java 1.4 and below).</p>\n\n<p>I am not per se interested in a specific language X, Y or Z. I am interested in type-checking algorithms regardless of the language that they target. If you provide a answer like "language L that you never heard about which is strongly typed and the typing is complex has a type-checking algorithm that does A, B and C by checking X and Y using the algorithm Z", or "the strategy X and Y used for Scala and a variant Z of A used for C# are cool because of the R, S and T features that works in that way", then the answers are nice.</p>\n', 'Tags': '<algorithms><programming-languages><reference-request><type-checking>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-29T12:25:23.307', 'CommentCount': '15', 'AcceptedAnswerId': '152', 'CreationDate': '2012-03-09T05:27:29.120', 'Id': '148'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '303', 'Title': 'Efficient compression of unlabeled trees', 'LastEditDate': '2012-03-20T20:21:39.787', 'AnswerCount': '5', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '2', 'Body': '<p>Consider unlabeled, rooted binary trees. We can <em>compress</em> such trees: whenever there are pointers to subtrees $T$ and $T&#39;$ with $T = T&#39;$ (interpreting $=$ as structural equality), we store (w.l.o.g.) $T$ and replace all pointers to $T&#39;$ with pointers to $T$. See <a href="http://cs.stackexchange.com/a/177/98">uli\'s answer</a> for an example.</p>\n\n<p>Give an algorithm that takes a tree in the above sense as input and computes the (minimal) number of nodes that remain after compression. The algorithm should run in time $\\cal{O}(n\\log n)$ (in the uniform cost model) with $n$ the number of nodes in the input.</p>\n\n<p>This has been an exam question and I have not been able to come up with a nice solution, nor have I seen one.</p>\n', 'Tags': '<algorithms><data-structures><trees><binary-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-20T20:21:39.787', 'CommentCount': '11', 'AcceptedAnswerId': '174', 'CreationDate': '2012-03-09T17:54:38.383', 'Id': '168'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've not gone much deep into CS. So, please forgive me if the question is not good or out of scope for this site.</p>\n\n<p>I've seen in many sites and books, the big-O notations like $O(n)$ which tell the time taken by an algorithm. I've read a few articles about it, but I'm still not able to understand how do you calculate it for a given algorithm.</p>\n", 'ViewCount': '7841', 'Title': 'How to come up with the runtime of algorithms?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-06T16:07:12.323', 'LastEditDate': '2013-06-06T16:07:12.323', 'AnswerCount': '5', 'CommentCount': '5', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '132', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><reference-question>', 'CreationDate': '2012-03-10T12:03:19.397', 'FavoriteCount': '11', 'Id': '192'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3710', 'Title': 'The time complexity of finding the diameter of a graph', 'LastEditDate': '2012-08-08T14:16:53.283', 'AnswerCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '51', 'FavoriteCount': '2', 'Body': '<blockquote>\n  <p>What is the time complexity of finding the diameter of a graph\n  $G=(V,E)$?</p>\n  \n  <ul>\n  <li>${O}(|V|^2)$</li>\n  <li>${O}(|V|^2+|V| \\cdot |E|)$</li>\n  <li>${O}(|V|^2\\cdot |E|)$</li>\n  <li>${O}(|V|\\cdot |E|^2)$</li>\n  </ul>\n</blockquote>\n\n<p>The diameter of a graph $G$ is the longest distance between two vertices in graph.</p>\n\n<p>I have no idea what to do about it, I need a complete analysis on how to solve a problem like this.</p>\n', 'Tags': '<algorithms><time-complexity><graph-theory>', 'LastEditorUserId': '72', 'LastActivityDate': '2013-03-23T22:43:49.053', 'CommentCount': '4', 'AcceptedAnswerId': '213', 'CreationDate': '2012-03-10T12:24:48.097', 'Id': '194'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been working on dynamic programming for some time. The canonical way to evaluate a dynamic programming recursion is by creating a table of all necessary values and filling it row by row. See for example <a href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=11866">Cormen, Leiserson et al: "Introduction to Algorithms"</a> for an introduction.</p>\n\n<p>I focus on the table-based computation scheme in two dimensions (row-by-row filling) and investigate the structure of cell dependencies, i.e. which cells need to be done before another can be computed. We denote with $\\Gamma(\\mathbf{i})$ the set of indices of cells the cell $\\mathbf{i}$ depends on. Note that $\\Gamma$ needs to be cycle-free.</p>\n\n<p>I abstract from the actual function that is computed and concentrate on its recursive structure. Formally, I consider a recurrrence $d$ to be <em>dynamic programming</em> if it has the form</p>\n\n<p>$\\qquad d(\\mathbf{i}) = f(\\mathbf{i}, \\widetilde{\\Gamma}_d(\\mathbf{i}))$</p>\n\n<p>with $\\mathbf{i} \\in [0\\dots m] \\times [0\\dots n]$, $\\widetilde{\\Gamma}_d(\\mathbf{i}) = \\{(\\mathbf{j},d(\\mathbf{j})) \\mid \\mathbf{j} \\in \\Gamma_d(\\mathbf{i}) \\}$ and $f$ some (computable) function that does not use $d$ other than via $\\widetilde{\\Gamma}_d$.</p>\n\n<p>When restricting the granularity of $\\Gamma_d$ to rough areas (to the left, top-left, top, top-right, ... of the current cell) one observes that there are essentially three cases (up to symmetries and rotation) of valid dynamic programming recursions that inform how the table can be filled:</p>\n\n<p><img src="http://i.stack.imgur.com/AhnK7.png" alt="Three cases of dynamic programming cell dependencies"></p>\n\n<p>The red areas denote (overapproximations of) $\\Gamma$. Cases one and two admit subsets, case three is the worst case (up to index transformation). Note that it is not strictly required that the <em>whole</em> red areas are covered by $\\Gamma$; <em>some</em> cells in every red part of the table are sufficient to paint it red. White areas are explictly required to <em>not</em> contain any required cells.</p>\n\n<p>Examples for case one are <a href="https://en.wikipedia.org/wiki/Edit_distance">edit distance</a> and <a href="https://en.wikipedia.org/wiki/Longest_common_subsequence_problem#Code_for_the_dynamic_programming_solution">longest common subsequence</a>, case two applies to <a href="https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm">Bellman &amp; Ford</a> and <a href="https://en.wikipedia.org/wiki/CYK">CYK</a>. Less obvious examples include such that work on the diagonals rather than rows (or columns) as they can be rotated to fit the proposed cases; see <a href="http://cs.stackexchange.com/a/211/98">Joe\'s answer</a> for an example.</p>\n\n<p>I have no (natural) example for case three, though! So my question is: What are examples for case three dynamic programming recursions/problems?</p>\n', 'ViewCount': '806', 'Title': 'A Case Distinction on Dynamic Programming: Example Needed!', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-24T16:54:31.097', 'LastEditDate': '2012-03-13T10:04:12.807', 'AnswerCount': '5', 'CommentCount': '2', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2012-03-10T13:26:09.637', 'FavoriteCount': '2', 'Id': '196'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '867', 'Title': 'Why polynomial time is called "efficient"?', 'LastEditDate': '2012-04-22T16:34:25.370', 'AnswerCount': '3', 'Score': '25', 'PostTypeId': '1', 'OwnerUserId': '157', 'FavoriteCount': '5', 'Body': "<p>Why in computer science any complexity which is at most polynomial is considered efficient?</p>\n\n<p>For any practical application<sup>(a)</sup>, algorithms with complexity $n^{\\log n}$ are way faster than algorithms that run in time, say, $n^{80}$, but the first is considered inefficient while the latter is efficient. Where's the logic?!</p>\n\n<p><sup>(a) Assume, for instance, the number of atoms in the universe is approximately $10^{80}$.</sup></p>\n", 'Tags': '<algorithms><complexity-theory><terminology><efficiency>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-04T19:18:03.357', 'CommentCount': '10', 'AcceptedAnswerId': '215', 'CreationDate': '2012-03-10T20:49:58.150', 'Id': '210'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '544', 'Title': 'Implementing the GSAT algorithm - How to select which literal to flip?', 'LastEditDate': '2012-04-19T05:46:33.270', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '174', 'FavoriteCount': '1', 'Body': "<p>The GSAT algorithm is, for the most part, straight forward: You get a formula in conjunctive normal form and flip the literals of the clauses until you find a solution that satisfies the formula or you reach the max_tries/max_flips limit and find no solution.</p>\n\n<p>I'm implementing the following algorithm:</p>\n\n<pre><code>procedure GSAT(A,Max_Tries,Max_Flips)\n  A: is a CNF formula\n  for i:=1 to Max_Tries do\n    S &lt;- instantiation of variables\n    for j:=1 to Max_Iter do\n      if A satisfiable by S then\n        return S\n      endif\n      V &lt;- the variable whose flip yield the most important raise in the number of satisfied clauses;\n      S &lt;- S with V flipped;\n    endfor\n  endfor\n  return the best instantiation found\nend GSAT\n</code></pre>\n\n<p>I'm having trouble interpreting the following line:  </p>\n\n<pre><code>V &lt;- the variable whose flip yield the most important raise in the number of satisfied clauses;\n</code></pre>\n\n<p>Isn't the maximum number of satisfied clauses what we're looking for? It seems to me that we're trying to use the solution or approximations to it to find the solution. </p>\n\n<p>I've thought of some ways to do this but It'd be good to hear other points of view (The assumption is that once the variable is flipped once it is selected.):</p>\n\n<ul>\n<li>Generate a state space with all possible flips and search the space for a literal that results in the best approximation to the goal state.</li>\n<li>Randomly select the variable that I will flip starting with the literals that are more common.</li>\n<li>Pick a random literal.</li>\n</ul>\n", 'Tags': '<algorithms><satisfiability><3-sat>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-19T05:46:33.270', 'CommentCount': '0', 'AcceptedAnswerId': '220', 'CreationDate': '2012-03-11T15:48:43.030', 'Id': '219'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '177', 'Title': 'Identifying events related to dates in a paragraph', 'LastEditDate': '2012-03-17T16:51:17.110', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '191', 'FavoriteCount': '1', 'Body': '<p>Is there an <s><em>algorithmic</em></s> approach to identify that dates given in a paragraph correlate to particular events (phrases) in the paragraph?</p>\n\n<p>Example, consider the following paragraph:</p>\n\n<blockquote>\n  <p>In June 1970, the great leader took the oath. But it was only after May 1972, post the death of the Minister of State, that he took over the reins of the country. While he enjoyed popular support until Mid-1980, his influence began to fall thereafter.</p>\n</blockquote>\n\n<p>Is there an algorithm (deterministic or stochastic)# that can generate a 2-tuple (date, event), where the <em>event</em> is implied, by the paragraph, to have occured on the <em>date</em>? In the above case:</p>\n\n<ul>\n<li>(June 1970, great leader took oath)</li>\n<li><p>(May 1972, took over the reins)   </p>\n\n<p>or better yet</p></li>\n<li>(May 1972, <em>the great leader</em> took over the reins)</li>\n<li>(1980, fall in influence)  </li>\n</ul>\n\n<hr>\n\n<p>#Later addition</p>\n', 'Tags': '<algorithms><data-mining><natural-lang-processing>', 'LastEditorUserId': '191', 'LastActivityDate': '2012-03-17T18:56:43.870', 'CommentCount': '6', 'AcceptedAnswerId': '474', 'CreationDate': '2012-03-11T17:29:14.580', 'Id': '221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '410', 'Title': 'Analyzing a modified version of the card-game "War"', 'LastEditDate': '2012-03-25T20:54:36.950', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '1', 'Body': '<p>A simple game usually played by children, the game of War is played by two people using a standard deck of 52 playing cards. Initially, the deck is shuffled and all cards are dealt two the two players, so that each have 26 random cards in a random order. We will assume that players are allowed to examine (but not change) both decks, so that each player knows the cards and orders of cards in both decks. This is typically note done in practice, but would not change anything about how the game is played, and helps keep this version of the question completely deterministic.</p>\n\n<p>Then, players reveal the top-most cards from their respective decks. The player who reveals the larger card (according to the usual order: 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace) wins the round, placing first his card (the high card) at the bottom of his deck, and then his opponent\'s card (the low card) at the bottom of the deck (typically, the order of this isn\'t enforced, but to keep the first version of this question deterministic, such an ordering will be enforced).</p>\n\n<p>In the event of a tie, each player reveals four additional cards from the top of their decks. If the fourth card shown by one player is higher than the fourth card shown by another player, the player with the higher fourth card wins all cards played during the tie-breaker, in which case the winner\'s cards are first placed at the bottom of the winner\'s deck (in first-in, first-out order; in other words, older cards are placed at the bottom first), followed by the loser\'s cards (in the same order).</p>\n\n<p>In the event of subsequent ties, the process is repeated until a winner of the tie is determined. If one player runs out of cards and cannot continue breaking the tie, the player who still has cards is declared the winner. If both players run out cards to play at the same time the game is declared a tie.</p>\n\n<p>Rounds are played until one player runs out of cards (i.e., has no more cards in his deck), at which point the player who still has cards is declared the winner.</p>\n\n<p>As the game has been described so far, neither skill nor luck is involved in determining the outcome. Since there are a finite number of permutations of 52 cards, there are a finite number of ways in which the decks may be initially dealt, and it follows that (since the only state information in the game is the current state of both players\' decks) the outcome of each game configuration can be decided a priori. Certainly, it is possibly to win the game of War, and by the same token, to lose it. We also leave open the possibility that a game of War might result in a Tie or in an infinite loop; for the completely deterministic version described above, such may or may not be the case.</p>\n\n<p>Several variations of the game which attempt to make it more interesting (and no, not all involve making it into a drinking game). One way which I have thought of to make the game more interesting is to allow players to declare automatic "trumps" at certain rounds. At each round, either player (or both players) may declare "trump". If one player declares "trump", that player wins the round regardless of the cards being played. If both players declare "trump", then the round is treated as a tie, and play continues accordingly.</p>\n\n<p>One can imagine a variety of rules limiting players\' ability to trump (unlimited trumping would always result in a Tie game, as players would trump every turn). I propose two versions (just off the top of my head; more interesting versions along these lines are probably possible) of War based on this idea but using different trump limiting mechanisms:</p>\n\n<ol>\n<li>Frequency-War: Players may only trump if they have not trumped in the previous $k$ rounds.</li>\n<li>Revenge-War: Players may only trump if they have not won a round in the previous $k$ rounds.</li>\n</ol>\n\n<p>Now for the questions, which apply to each of the versions described above:</p>\n\n<blockquote>\n  <ol>\n  <li>Is there a strategy such that, for some set of possible initial game configurations, the player using it always wins (strongly winning strategy)? If so, what is this strategy? If not, why not?</li>\n  <li>Is there a strategy such that, for some set of possible initial game configurations, the player using it can always win or force a tie (winning strategy)? If so, what is this strategy? If not, why not?</li>\n  <li>Are their initial game configurations such that there is no winning strategy (i.e., a player using any fixed strategy $S$ can always be defeated by a player using fixed strategy $S\'$)? If so, what are they, and explain?</li>\n  </ol>\n</blockquote>\n\n<p>To be clear, I am thinking of a "strategy" as a fixed algorithm which determines at what rounds the player using the strategy should trump. For instance, the algorithm "trump whenever you can" is a strategy, and an algorithm (a heuristic algorithm). Another way of what I\'m asking is this:</p>\n\n<blockquote>\n  <p>Are there any good (or provably optimal) heuristics for playing these games?</p>\n</blockquote>\n\n<p>References to analyses of such games are appreciated (I am unaware of any analysis of this version of War, or of essentially equivalent games). Results for any $k$ are interesting and appreciated (note that, in both cases, $k=0$ leads to unlimited trumping, which I have already discussed).</p>\n', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-29T16:31:04.390', 'CommentCount': '2', 'AcceptedAnswerId': '872', 'CreationDate': '2012-03-12T18:58:22.227', 'Id': '248'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1098', 'Title': 'How to determine likely connections in a social network?', 'LastEditDate': '2012-04-22T16:33:09.770', 'AnswerCount': '4', 'Score': '20', 'PostTypeId': '1', 'OwnerUserId': '151', 'FavoriteCount': '2', 'Body': '<p>I am curious in determining an approach to tackling a "suggested friends" algorithm.</p>\n\n<p><a href="http://facebook.com">Facebook</a> has a feature in which it will recommended individuals to you which it thinks you may be acquainted with. These users normally (excluding the edge cases in <a href="http://www.facebook.com/help/?faq=154758887925123#How-do-I-suggest-a-friend-to-someone?">which a user specifically recommends a friend</a>) have a highly similar network to oneself. That is, the number of friends in common are high. I assume Twitter follows a similar path for their "Who To Follow" mechanism.</p>\n\n<p><a href="http://stackoverflow.com/a/6851193/321505">Stephen Doyle (Igy)</a>, a Facebook employee suggested that the related newsfeed that uses <a href="http://www.quora.com/How-does-Facebook-calculate-weight-for-edges-in-the-EdgeRank-formula">EdgeRank formula</a> which seems to indicate that more is to valued than friends such as appearance is similar posts. Another user suggested the Google Rank system. </p>\n\n<p>Facebook states their News Feed Optimization as $\\sum u_{e}w_{e}d_{e}$ where</p>\n\n<p>$u_{e}$ = affinity score between viewing user and edge creator<br>\n$w_{e}$ = weight for this edge (create, comment, like, tag, etc)<br>\n$d_{e}$ = time decay factor based on how long ago the edge was created   </p>\n\n<p>Summing these items is supposed to give an object\'s rank which I assume as Igy hinted, means something in a similar format is used for suggested friends.</p>\n\n<p>So I\'m guessing that this is the way in which connections for all types are done in general via a rank system?</p>\n', 'Tags': '<algorithms><machine-learning><modelling><social-networks>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T16:33:09.770', 'CommentCount': '2', 'AcceptedAnswerId': '314', 'CreationDate': '2012-03-12T23:24:54.360', 'Id': '261'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When implementing a dictionary ('I want to look up customer data by their customer IDs'), the typical data structures used are hash tables and binary search trees. I know for instance that the C++ STL library implements dictionaries (they call them maps) using (balanced) binary search trees, and the .NET framework uses hash tables under the hood.</p>\n\n<blockquote>\n  <p>What are the advantages and disadvantages of these data structures? Is there some other option that is reasonable in certain situations?</p>\n</blockquote>\n\n<p>Note that I'm not particularly interested in cases where the keys have a strong underlying structure, say, they are all integers between 1 and n or something.</p>\n", 'ViewCount': '4628', 'Title': 'Hash tables versus binary trees', 'LastActivityDate': '2012-03-13T01:43:44.703', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '278', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<algorithms><data-structures><binary-trees><hash-tables>', 'CreationDate': '2012-03-13T00:30:42.750', 'FavoriteCount': '2', 'Id': '270'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><em>Originally on <a href="http://math.stackexchange.com/questions/74453/how-is-the-loop-invarient-obtained-in-this-square-root-bound-finding-algorithm">math.SE</a> but unanswered there.</em></p>\n\n<p>Consider the following algorithm.</p>\n\n<pre><code>u := 0\nv := n+1;\nwhile ( (u + 1) is not equal to v) do\n   x :=  (u + v) / 2;\n   if ( x * x &lt;= n) \n     u := x;\n   else\n     v := x;\n   end_if\nend_while \n</code></pre>\n\n<p>where u, v, and n are integers and the division operation is integer division. </p>\n\n<ul>\n<li>Explain what is computed by the algorithm. </li>\n<li>Using your answer to part I as the post-condition for the algorithm, establish a loop invariant and show that \nthe algorithm terminates and is correct.</li>\n</ul>\n\n<p>In class, the post-condition was found to be $0 \\leq u^2 \\leq n &lt; (u + 1)^2$ and the \nInvariant is $0 \\leq u^2 \\leq n &lt; v^2, u + 1 \\leq v$.  I don\'t really understand on how the post-condition and invariants were obtained.  I figure the post condition was $u + 1 = v$... which is clearly not the case.  So I am wondering on how the post-condition and invariant was obtained.  I\'m also wondering on how the pre-condition can be obtained by using the post-condition.</p>\n', 'ViewCount': '271', 'Title': 'How is the loop invariant obtained in this square root bound finding algorithm?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-10T03:28:29.270', 'LastEditDate': '2012-05-10T03:28:29.270', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5', 'Tags': '<algorithms><loop-invariants><correctness-proof>', 'CreationDate': '2012-03-13T06:06:53.057', 'FavoriteCount': '1', 'Id': '288'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When searching graphs, there are two easy algorithms: <strong>breadth-first</strong> and <strong>depth-first</strong> (Usually done by adding all adjactent graph nodes to a queue (breadth-first) or stack (depth-first)).</p>\n\n<p>Now, are there any advantages of one over another?</p>\n\n<p>The ones I could think of:</p>\n\n<ul>\n<li>If you expect your data to be pretty far down inside the graph, <em>depth-first</em> might find it earlier, as you are going down into the deeper parts of the graph very fast.</li>\n<li>Conversely, if you expect your data to be pretty far up in the graph, <em>breadth-first</em> might give the result earlier.</li>\n</ul>\n\n<p>Is there anything I have missed or does it mostly come down to personal preference?</p>\n', 'ViewCount': '10527', 'Title': 'Graph searching: Breadth-first vs. depth-first', 'LastEditorUserId': '755', 'LastActivityDate': '2013-12-24T23:35:26.743', 'LastEditDate': '2013-09-09T01:44:33.060', 'AnswerCount': '6', 'CommentCount': '1', 'Score': '31', 'PostTypeId': '1', 'OwnerUserId': '101', 'Tags': '<algorithms><graph-theory><search-algorithms><graph-traversal>', 'CreationDate': '2012-03-13T10:05:58.093', 'FavoriteCount': '14', 'Id': '298'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '966', 'Title': 'Do you get DFS if you change the queue to a stack in a BFS implementation?', 'LastEditDate': '2012-03-13T20:44:07.413', 'AnswerCount': '1', 'Score': '19', 'PostTypeId': '1', 'OwnerUserId': '15', 'FavoriteCount': '1', 'Body': '<p>Here is the standard pseudocode for breadth first search:</p>\n\n<pre><code>{ seen(x) is false for all x at this point }\npush(q, x0)\nseen(x0) := true\nwhile (!empty(q))\n  x := pop(q)\n  visit(x)\n  for each y reachable from x by one edge\n    if not seen(y)\n      push(q, y)\n      seen(y) := true\n</code></pre>\n\n<p>Here <code>push</code> and <code>pop</code> are assumed to be queue operations. But what if they are stack operations? Does the resulting algorithm visit vertices in depth-first order?</p>\n\n<hr/>\n\n<p>If you voted for the comment "this is trivial", I\'d ask you to explain why it is trivial. I find the problem quite tricky.</p>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '15', 'LastActivityDate': '2012-03-14T23:07:45.207', 'CommentCount': '4', 'AcceptedAnswerId': '337', 'CreationDate': '2012-03-13T18:03:19.993', 'Id': '329'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1833', 'Title': 'In-place algorithm for interleaving an array', 'LastEditDate': '2012-03-17T00:02:08.030', 'AnswerCount': '2', 'Score': '31', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '9', 'Body': "<p>You are given an array of $2n$ elements </p>\n\n<p>$$a_1, a_2, \\dots, a_n, b_1, b_2, \\dots b_n$$</p>\n\n<p>The task is to interleave the array, using an in-place algorithm such that the resulting array looks like</p>\n\n<p>$$b_1, a_1, b_2, a_2, \\dots , b_n, a_n$$</p>\n\n<p>If the in-place requirement wasn't there, we could easily create a new array and copy elements giving an $\\mathcal{O}(n)$ time algorithm.</p>\n\n<p>With the in-place requirement, a divide and conquer algorithm bumps up the algorithm to be $\\theta(n \\log n)$.</p>\n\n<p>So the question is:</p>\n\n<blockquote>\n  <p>Is there an $\\mathcal{O}(n)$ time algorithm, which is also in-place?</p>\n</blockquote>\n\n<p>(Note: You can assume the uniform cost WORD RAM model, so in-place translates to $\\mathcal{O}(1)$ space restriction).</p>\n", 'Tags': '<algorithms><in-place><arrays>', 'LastEditorUserId': '139', 'LastActivityDate': '2012-04-05T10:15:30.180', 'CommentCount': '7', 'AcceptedAnswerId': '400', 'CreationDate': '2012-03-13T20:16:32.067', 'Id': '332'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Suppose the datatype for a BST is defined as follows (in SML)</p>\n\n<pre><code>datatype \'a bst_Tree =\n   Empty\n | Node of (int * \'a) * \'a bst_Tree * \'a bst_Tree;\n</code></pre>\n\n<p>So there are two cases one in which the BST is <code>Empty</code> or it can have a (key,value) as well as two children.</p>\n\n<p>Now, for the case of an AVL where the condition is </p>\n\n<blockquote>\n  <p>In an AVL tree, the heights of the two child subtrees of any node differ by at most one<br>\n  <sub>- <a href="http://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL tree Wikipedia</a></sub></p>\n</blockquote>\n\n<p>I want to able to create a height function for use to check whether the tree is balanced. My current setup is as follows</p>\n\n<pre><code>fun height (Empty) = ~1\n  | height (Node(v, Empty, Empty)) = 0 (* Redundant matching because of third case *)\n  | height (Node(v, L, R)) = 1 + Int.max(height(L),height(R))\n</code></pre>\n\n<p>I tried to separate the Tree into three conditions</p>\n\n<ol>\n<li>A empty Tree</li>\n<li>A Tree with a root node</li>\n<li>A populated tree</li>\n</ol>\n\n<p>The reason for this is that there does not seem to be a canonical source on what the value is for the height of an <code>Empty</code> Tree as opposed to one in which only has a root. For the purposes of my balance function it did the job, but I rather try to understand why there isn\'t a canonical answer for the height of an <code>Empty</code> Tree.</p>\n\n<p>There is a canonical answer, in a matter of speaking on <a href="http://en.wikipedia.org/wiki/Tree_height#Terminology" rel="nofollow">Wikipedia</a> but while initially doing research on this on Stack Overflow I arrived at many comments stating this to be wrong/incorrect/unconventional</p>\n\n<blockquote>\n  <p>Conventionally, the value \u22121 corresponds to a subtree with no nodes, whereas zero corresponds to a subtree with one node.)</p>\n</blockquote>\n\n<p>I grabbed the question from which my uncertainty appeared</p>\n\n<p><a href="http://stackoverflow.com/questions/2209777/what-is-the-definition-for-the-height-of-a-tree">What is the definition for the height of a tree?</a></p>\n\n<blockquote>\n  <p>I think you should take a look at the <a href="http://www.itl.nist.gov/div897/sqg/dads/" rel="nofollow">Dictionary of Algorithms and Data Structures</a> at the NIST website. There definition for height says a single node is height 0.</p>\n  \n  <p>The <a href="http://www.itl.nist.gov/div897/sqg/dads/HTML/tree.html" rel="nofollow">definition of a valid tree</a> does include an empty structure. The site doesn\'t mention the height of such a tree, but based on the definition of the height, it should also be 0.</p>\n</blockquote>\n', 'ViewCount': '271', 'Title': 'What is the height of an empty BST when using it in context for balancing?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T16:28:40.197', 'LastEditDate': '2012-04-22T16:28:40.197', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '346', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '151', 'Tags': '<algorithms><data-structures><terminology>', 'CreationDate': '2012-03-13T21:46:09.257', 'Id': '335'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '4378', 'Title': 'How to verify number with Bob without Eve knowing?', 'LastEditDate': '2012-03-15T07:02:35.387', 'AnswerCount': '7', 'Score': '26', 'PostTypeId': '1', 'OwnerUserId': '71', 'FavoriteCount': '1', 'Body': '<p>You need to check that your friend, Bob, has your correct phone number, but you cannot ask him directly. You must write the question on a card which and give it to Eve who will take the card to Bob and return the answer to you. What must you write on the card, besides the question, to ensure Bob can encode the message so that Eve cannot read your phone number?</p>\n\n<p><em>Note:</em> This question is on a list of "google interview questions". As a result, there are tons of versions of this question on the web, and many of them don\'t have clear, or even correct answers. </p>\n\n<p><em>Note 2:</em> The snarky answer to this question is that Bob should write "call me". Yes, that\'s very clever, \'outside the box\' and everything, but doesn\'t use any techniques that field of CS where we call our hero "Bob" and his eavesdropping adversary "Eve". </p>\n\n<p><strong>Update:</strong> <br>\nBonus points for an algorithm that you and Bob could both reasonably complete by hand.</p>\n\n<p><strong>Update 2:</strong> <br>\nNote that Bob doesn\'t have to send you any arbitrary message, but only confirm that he has your correct phone number without Eve being able to decode it, which may or may not lead to simpler solutions.</p>\n', 'Tags': '<algorithms><cryptography>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T13:59:02.063', 'CommentCount': '7', 'AcceptedAnswerId': '399', 'CreationDate': '2012-03-14T08:18:00.390', 'Id': '358'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '377', 'Title': 'How to find a superstar in linear time?', 'LastEditDate': '2014-02-06T21:20:48.667', 'AnswerCount': '4', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '2', 'Body': '<p>Consider directed graphs. We call a node $v$ <em>superstar</em> if and only if no other node can be reached from it, but all other nodes have an edge to $v$. Formally:</p>\n\n<p>$\\qquad \\displaystyle $v$ \\text{ superstar } :\\Longleftrightarrow \\mathrm{outdeg}(v) = 0 \\land \\mathrm{indeg}(v) = n-1$</p>\n\n<p>with $n$ the number of nodes in the graph. For example, in the below graph, the unfilled node is a superstar (and the other nodes are not).</p>\n\n<p><img src="http://i.stack.imgur.com/MIGky.png" alt="A Superstar"><br>\n<sup>[<a href="https://github.com/akerbos/sesketches/blob/gh-pages/src/cs_411.dot" rel="nofollow">source</a>]</sup></p>\n\n<p>How can you identify all superstars in a directed graphs in $\\mathcal{O}(n)$ time? A suitable graph representation can be chosen from the <a href="https://en.wikipedia.org/wiki/Graph_%28abstract_data_type%29#Representations" rel="nofollow">usual candidates</a>; please refrain from using representations that move the problem\'s complexity to preprocessing.</p>\n\n<p>No assumptions regarding density can be made. We don\'t assume the graph contains a superstar; if there is none, the algorithm should recognize it.</p>\n\n<p><em>Notation</em>: $\\mathrm{outdeg}$ is a node\'s number of outgoing edges, $\\mathrm{indeg}$ similar for incoming edges.</p>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-06T21:20:48.667', 'CommentCount': '6', 'AcceptedAnswerId': '417', 'CreationDate': '2012-03-15T07:55:01.690', 'Id': '411'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3515', 'Title': 'How hard is counting the number of simple paths between two nodes in a directed graph?', 'LastEditDate': '2012-03-26T05:19:12.447', 'AnswerCount': '1', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '137', 'FavoriteCount': '5', 'Body': "<p>There is an easy polynomial algorithm to decide whether there is a path between two nodes in a directed graph (just do a routine graph traversal with, say, depth-first-search).</p>\n\n<p>However it seems that, surprisingly, the problem gets much harder if instead of testing for the existence we want want to <em>count</em> the number of paths.</p>\n\n<p>If we allow paths to reuse vertices then there is a dynamic programming solution to find the number of paths from <em>s</em> to <em>t</em> with <em>n</em> edges. <strong>However, if we only allow simple paths, that don't reuse vertices, the only solution I can think of is brute force enumeration of the paths</strong>, something that has exponential time complexity.</p>\n\n<p>So I ask,</p>\n\n<ul>\n<li>Is counting the number of simple paths between two vertices hard?</li>\n<li>If so, is it kind of NP-complete? (I say kind of because it is technically not a decision problem...)</li>\n<li>Are there other problems in P that have a hard counting versions like that too?**</li>\n</ul>\n", 'Tags': '<algorithms><complexity-theory><graph-theory>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-26T05:19:12.447', 'CommentCount': '3', 'AcceptedAnswerId': '445', 'CreationDate': '2012-03-15T18:22:52.533', 'Id': '423'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Definition from wikipedia:</p>\n\n<blockquote>\n  <p>A graph is an ordered pair $G = (V, E)$ comprising a set $V$ of nodes together with a set $E$ of edges, which are two-element subsets of $V$.</p>\n</blockquote>\n\n<p>The set of all finite graphs (modulo isomorphism: we don't want nodes to have identities) is countable and could be enumerated. But what would be an <em>efficient</em> (low-complexity, from a programming point of view) injection from graphs to $\\mathbb{N}$?</p>\n\n<p><em><strong>Edit:</strong> Gilles' comment indicates that it is not know whether there is a such function feasible in polynomial time. An example of an exponential-complexity function would be good enough; we can surely do better than a brute enumeration?</em></p>\n", 'ViewCount': '86', 'Title': 'An indexing function for graphs', 'LastEditorUserId': '68', 'LastActivityDate': '2012-03-15T22:15:41.130', 'LastEditDate': '2012-03-15T21:33:24.697', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '68', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-03-15T19:51:06.763', 'Id': '427'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2234', 'Title': 'Which combinations of pre-, post- and in-order sequentialisation are unique?', 'LastEditDate': '2012-03-17T09:20:08.613', 'AnswerCount': '1', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '5', 'Body': '<p>We know post-order,</p>\n\n<pre><code>post L(x)     =&gt; [x]\npost N(x,l,r) =&gt; (post l) ++ (post r) ++ [x]\n</code></pre>\n\n<p>and pre-order</p>\n\n<pre><code>pre L(x)     =&gt; [x]\npre N(x,l,r) =&gt; [x] ++ (pre l) ++ (pre r)\n</code></pre>\n\n<p>and in-order traversal resp. sequentialisation.</p>\n\n<pre><code>in L(x)     =&gt; [x]\nin N(x,l,r) =&gt; (in l) ++ [x] ++ (in r)\n</code></pre>\n\n<p>One can easily see that neither describes a given tree uniquely, even if we assume pairwise distinct keys/labels.</p>\n\n<p>Which combinations of the three can be used to that end and which can not?</p>\n\n<p>Positive answers should include an (efficient) algorithm to reconstruct the tree and a proof (idea) why it is correct. Negative answers should provide counter examples, i.e. different trees that have the same representation. </p>\n', 'Tags': '<algorithms><binary-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-20T19:49:58.823', 'CommentCount': '0', 'AcceptedAnswerId': '441', 'CreationDate': '2012-03-16T21:11:12.777', 'Id': '439'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '596', 'Title': 'How Do Common Pathfinding Algorithms Compare To Human Process', 'LastEditDate': '2012-03-20T22:57:18.727', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '65', 'FavoriteCount': '2', 'Body': '<p>This might border on computational cognitive science, but I am curious as to how the process followed by common pathfinding algorithms (such as <a href="http://en.wikipedia.org/wiki/A_star_search_algorithm">A*</a>) compares to the process humans use in different pathfinding situations (given the same information). Are these processes similar?</p>\n', 'Tags': '<algorithms><graphs><artificial-intelligence>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-01T22:05:01.680', 'CommentCount': '5', 'AcceptedAnswerId': '554', 'CreationDate': '2012-03-20T20:51:22.867', 'Id': '553'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $\\Sigma$ be the set of terminal and $N$ the set of non-terminal symbols of some context-free grammar $G$.</p>\n\n<p>Say I have a string $a \\in (\\Sigma \\cup N)^+$ such that $x a y \\in \\mathcal{S}(G)$ where $x,y\\in (\\Sigma \\cup N)^*$ and $\\mathcal{S}(G)$ are the sentential forms of $G$.</p>\n\n<p>Given $G$, I'd like to determine a set $C = \\{ b \\mid wabz \\in \\mathcal{S}(G), b \\in \\Sigma \\cup N \\}$. </p>\n\n<p>To clarify, in this case, $w, x, y, z, a, b$ are strings of terminals and non-terminals and $b$ is of length one.</p>\n\n<p>I can see how to do this if $a$ is also of length one; each $b$ is a member of the follow set of $a$ (including non-terminals).</p>\n\n<p>However, I am curious if it's possible for a sequence of characters. For my application, the string $a$ is not much longer than the right hand side of the productions in $G$.</p>\n\n<p>The distinction between terminals and non-terminals is somewhat mute in my application because I am using a generative grammar; and I believe that this won't lead to much trouble since $b$ is of length one.</p>\n", 'ViewCount': '285', 'Title': 'Given a string and a CFG, what characters can follow the string (in the sentential forms of the CFG)?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-27T03:04:17.907', 'LastEditDate': '2012-03-27T03:04:17.907', 'AnswerCount': '1', 'CommentCount': '12', 'AcceptedAnswerId': '556', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '237', 'Tags': '<algorithms><context-free><formal-grammars><compilers>', 'CreationDate': '2012-03-20T22:55:17.150', 'Id': '555'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '472', 'Title': 'Creating a Self Ordering Binary Tree', 'LastEditDate': '2012-04-12T05:55:04.773', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '79', 'FavoriteCount': '1', 'Body': "<p>I have an assignment where I need to make use a binary search tree and alter it to self order itself such that items that are accessed the most (have a higher priority) are at the top of the tree, the root being the most accessed node.</p>\n\n<p>The professor gave me the BST and node struct to work with, but trying to get my head around the algorithm to update the tree as things are being inserted is confusing me.</p>\n\n<p>I know that as the insert is happening, it checks if the current node's data is less or greater than the current node, then recursively goes in the correct direction until it finds a null pointer and inserts itself there. and after it is inserted it increases the priority by 1.</p>\n\n<pre><code>template &lt;class Type&gt;\nvoid BinarySearchTree&lt;Type&gt; ::  insert( const Type &amp; x, BinaryNode&lt;Type&gt; * &amp; t )\n{\n    if( t == NULL )\n        t = new BinaryNode&lt;Type&gt;( x, NULL, NULL );\n    else if( x &lt; t-&gt;element )\n        insert( x, t-&gt;left );\n    else if( t-&gt;element &lt; x )\n        insert( x, t-&gt;right );\n    else\n        t-&gt;priority++;  // Duplicate; do nothing for right now\n}\n</code></pre>\n\n<p>Now I need to figure out when the node is equal, how to re-order the tree so that the current node (who is equal to an already existing node) finds the existing node, increases that node's priority, then shifts it up if the root is a lower priority.</p>\n\n<p>I think I have the idea down that the AVL logic would work, and when a shift would take place, it would be a single rotation right or a single rotation left.</p>\n\n<p>Here's where I'm confused,  don't really know where to start with creating an algorithm to solve the problem. Since the AVL algorithm works with keeping track of the balance of a tree, then rotating nodes left or right accordingly, this tree doesn't need to worry about being balanced, just that the nodes with the highest priority not have children with a higher priority.</p>\n", 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-12T05:55:04.773', 'CommentCount': '0', 'AcceptedAnswerId': '610', 'CreationDate': '2012-03-21T00:05:18.697', 'Id': '559'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What are some good online resources that will help me better understand BigO notation, running time &amp; invariants?</p>\n\n<p>I'm looking for lectures, interactive examples if possible. </p>\n", 'ViewCount': '464', 'Title': 'BigO, Running Time, Invariants - Learning Resources', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-29T08:45:22.170', 'LastEditDate': '2012-03-26T05:30:58.720', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '326', 'Tags': '<algorithms><landau-notation><education><runtime-analysis>', 'CreationDate': '2012-03-21T03:05:09.847', 'FavoriteCount': '2', 'Id': '569'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>You have one coin. You may flip it as many times as you want. </p>\n\n<p>You want to generate a random number $r$ such that $a \\leq r &lt; b$ where $r,a,b\\in \\mathbb{Z}^+$. </p>\n\n<p>Distribution of the numbers should be uniform. </p>\n\n<p>It is easy if $b -a = 2^n$:</p>\n\n<pre><code>r = a + binary2dec(flip n times write 0 for heads and 1 for tails) \n</code></pre>\n\n<p>What if $b-a \\neq 2^n$?</p>\n', 'ViewCount': '2142', 'Title': 'Generating uniformly distributed random numbers using a coin', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-24T21:55:20.950', 'LastEditDate': '2012-04-29T20:51:09.547', 'AnswerCount': '7', 'CommentCount': '0', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '244', 'Tags': '<algorithms><probability-theory><randomness><random-number-generator>', 'CreationDate': '2012-03-21T03:12:00.883', 'FavoriteCount': '4', 'Id': '570'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have two strings. Call them $A$ and $B$. Neither string has any repeated characters.</p>\n\n<p>How can I find the shortest sequence of insert, move, and delete operation that turns $A$ into $B$, where:</p>\n\n<ul>\n<li><code>insert(char, offset)</code> inserts <code>char</code> at the given <code>offset</code> in the string</li>\n<li><code>move(from_offset, to_offset)</code> moves the character currently at offset <code>from_offset</code> to a new position so that it has offset <code>to_offset</code></li>\n<li><code>delete(offset)</code> deletes the character at <code>offset</code></li>\n</ul>\n\n<p>Example application: You do a database query and show the results on your website. Later, you rerun the database query and discover that the results have changed. You want to change what is on the page to match what is currently in the database using the minimum number of DOM operations. There are two reasons why you'd want the shortest sequence of operations. First, efficiency. When only a few records change, you want to make sure that you do $\\mathcal{O}(1)$ rather than $\\mathcal{O}(n)$ DOM operations, since they are expensive. Second, correctness. If an item moved from one position to another, you want to move the associated DOM nodes in a single operation, without destroying and recreating them. Otherwise you will lose focus state, the content of <code>&lt;input&gt;</code> elements, and so forth.</p>\n", 'ViewCount': '238', 'Title': 'Expressing an arbitrary permutation as a sequence of (insert, move, delete) operations', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:22:27.077', 'LastEditDate': '2012-05-15T20:22:27.077', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '330', 'Tags': '<algorithms><combinatorics><string-metrics>', 'CreationDate': '2012-03-21T03:25:44.267', 'Id': '576'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a balanced binary tree, which represents a recursive partitioning of a set of $N$ points into nested subsets.  Each node of the tree represents a subset, with the following properties: subsets represented by two children nodes of the same parent are disjoint, and their union is equal to the subset represented by the parent.  The root represents the full set of points, and each leaf represents a single distinct point.  So there are $\\log N$ levels to the tree, and each level of the tree represents a partitioning of the points into increasingly fine levels of granularity.</p>\n\n<p>Now suppose we have two algorithms, each of which operates on all of the subsets of the tree.  The first does $O(D^2)$ operations at each node, where $D$ is the size of the subset represented by the node.  The second does $O(D \\log D)$ operations at each node.  What is the worst case runtime of these two algorithms?</p>\n\n<p>We can easily bound the first algorithm as $O(N^2 \\log N)$, because it does $O(N^2)$ work at each of $\\log N$ levels of the tree.  Similarly, we can bound the second algorithm as $O(N \\log ^2 N)$, by similar reasoning.</p>\n\n<p>The question is, are these bounds tight, or can we do better?  How do we prove it?</p>\n', 'ViewCount': '394', 'Title': 'What is the complexity of these tree-based algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-21T15:26:40.690', 'LastEditDate': '2012-03-21T06:35:12.333', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '579', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '363', 'Tags': '<algorithms><time-complexity><binary-trees>', 'CreationDate': '2012-03-21T04:43:41.727', 'Id': '578'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2045', 'Title': 'Intuition for logarithmic complexity', 'LastEditDate': '2012-06-18T13:16:23.193', 'AnswerCount': '7', 'Score': '44', 'PostTypeId': '1', 'OwnerUserId': '385', 'FavoriteCount': '16', 'Body': "<p>I believe I have a reasonable grasp of complexities like $\\mathcal{O}(1)$, $\\Theta(n)$ and $\\Theta(n^2)$.</p>\n\n<p>In terms of a list, $\\mathcal{O}(1)$ is a constant lookup, so it's just getting the head of the list.\n$\\Theta(n)$ is where I'd walk the entire list, and $\\Theta(n^2)$ is walking the list once for each element in the list.</p>\n\n<p>Is there a similar intuitive way to grasp $\\Theta(\\log n)$ other than just knowing it lies somewhere between $\\mathcal{O}(1)$ and $\\Theta(n)$?</p>\n", 'Tags': '<algorithms><complexity-theory><time-complexity><intuition>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-18T13:16:23.193', 'CommentCount': '6', 'AcceptedAnswerId': '582', 'CreationDate': '2012-03-21T05:51:41.653', 'Id': '581'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to improve the worst case scenario for a depth first search on an Euler graph, starting at some point and ending at that same point?</p>\n\n<p>I need to do the whole search but it is not fast enough for large amounts of data. I have tried <a href="https://en.wikipedia.org/wiki/Bidirectional_search" rel="nofollow">bidirectional search</a> but I can not keep the result numerically ordered. Therefore I wonder if there is any other good method to smooth the worst case scenario for the depth first search.</p>\n', 'ViewCount': '251', 'Title': 'Improve worst case time of depth first search on Euler graphs', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-03-20T20:09:11.067', 'LastEditDate': '2013-03-20T20:09:11.067', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '591', 'Tags': '<algorithms><graphs><graph-traversal><eulerian-paths>', 'CreationDate': '2012-03-21T16:04:53.030', 'Id': '615'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '710', 'Title': 'Find shortest paths in a weighed unipathic graph', 'LastEditDate': '2012-03-21T23:24:31.727', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '512', 'FavoriteCount': '1', 'Body': '<p>A directed graph is said to be <em>unipathic</em> if for any two vertices $u$ and $v$ in\nthe graph $G=(V,E)$, there is at most one simple path from $u$ to $v$. </p>\n\n<p>Suppose I am given a unipathic graph $G$ such that each edge has a positive or negative weight, but contains no negative weight cycles.</p>\n\n<p>From this I want to find a $O(|V|)$ algorithm that finds all the the shortest paths to all nodes from a source node $s$.</p>\n\n<p>I am not sure how I would go about approaching this problem. I am trying to see how I could use the fact that it contains no negative weight cycles and of course at most one simple path between any node $u$ to $v$.</p>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-03-23T00:52:25.950', 'CommentCount': '3', 'AcceptedAnswerId': '679', 'CreationDate': '2012-03-21T19:37:01.033', 'Id': '625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1066', 'Title': 'Deciding on Sub-Problems for Dynamic Programming', 'LastEditDate': '2012-03-25T16:36:58.410', 'AnswerCount': '3', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '106', 'FavoriteCount': '8', 'Body': '<p>I have used the technique of dynamic programming multiple times however today a friend asked me how I go about defining my sub-problems, I realized I had no way of providing an objective formal answer. How do you formally define a sub-problem for a problem that you would solve using dynamic programming?</p>\n', 'Tags': '<algorithms><dynamic-programming>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-25T16:36:58.410', 'CommentCount': '0', 'AcceptedAnswerId': '647', 'CreationDate': '2012-03-22T03:30:10.270', 'Id': '645'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '893', 'Title': 'Logarithmic vs double logarithmic time complexity', 'LastEditDate': '2012-04-12T05:54:48.283', 'AnswerCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '652', 'FavoriteCount': '2', 'Body': '<p>In real world applications is there a concrete benefit when using $\\mathcal{O}(\\log(\\log(n))$ instead of $\\mathcal{O}(\\log(n))$ algorithms ?</p>\n\n<p>This is the case when one use for instance van Emde Boas trees instead of more conventional binary search tree implementations. \nBut for example, if we take $n &lt; 10^6$ then in the best case the double logarithmic algorithm outperforms the logarithmic one by (approximately) a factor of $5$. And also in general the implementation is more tricky and complex. </p>\n\n<p>Given that I personally prefer BST over VEB-trees, what do you think ?</p>\n\n<p><em>One could easily demonstrate that :</em></p>\n\n<p>$\\qquad \\displaystyle \\forall n &lt; 10^6.\\ \\frac{\\log n}{\\log(\\log(n))} &lt; 5.26146$</p>\n', 'Tags': '<algorithms><complexity-theory><binary-trees><algorithm-analysis><search-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T03:54:23.937', 'CommentCount': '3', 'AcceptedAnswerId': '661', 'CreationDate': '2012-03-22T14:23:03.533', 'Id': '654'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '658', 'Title': 'How to convert an NFA with overlapping cycles into a regular expression?', 'LastEditDate': '2014-01-25T16:51:44.933', 'AnswerCount': '3', 'Score': '10', 'OwnerDisplayName': 'zell', 'PostTypeId': '1', 'OwnerUserId': '694', 'Body': '<p>If I understand correctly, NFA have the same expressive power as regular expressions. Often, reading off equivalent regular expressions from NFA is easy: you translate cycles to stars, junctions as alternatives and so on. But what to do in this case: </p>\n\n<p><img src="http://i.stack.imgur.com/yCGnv.png" alt="enter image description here"><br>\n<sup>[<a href="https://github.com/akerbos/sesketches/blob/gh-pages/src/cs_689.tikz" rel="nofollow">source</a>]</sup></p>\n\n<p>The overlapping cycles make it hard to see what this automaton accepts (in terms of regular expressions). Is there a trick?</p>\n', 'Tags': '<algorithms><formal-languages><finite-automata><regular-expressions>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-25T16:51:44.933', 'CommentCount': '7', 'AcceptedAnswerId': '692', 'CreationDate': '2012-03-23T07:35:51.517', 'Id': '689'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The simplex algorithm walks greedily on the corners of a polytope to find the optimal solution to the linear programming problem. As a result, the answer is always a corner of the polytope. Interior point methods walk the inside of the polytope. As a result, when a whole plane of the polytope is optimal (if the objective function is exactly parallel to the plane), we can get a solution in the middle of this plane.</p>\n\n<p>Suppose that we want to find a corner of the polytope instead. For example if we want to do maximum matching by reducing it to linear programming, we don\'t want to get an answer consisting of "the matching contains 0.34% of the edge XY and 0.89% of the edge AB and ...". We want to get an answer with 0\'s and 1\'s (which simplex would give us since all corners consist of 0\'s and 1\'s). Is there a way to do this with an interior point method that guarantees to find exact corner solutions in polynomial time? (for example perhaps we can modify the objective function to favor corners)</p>\n', 'ViewCount': '414', 'Title': 'Finding exact corner solutions to linear programming using interior point methods', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-05T12:33:33.760', 'LastEditDate': '2012-03-23T22:39:23.917', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '771', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '700', 'Tags': '<algorithms><optimization><linear-programming>', 'CreationDate': '2012-03-23T20:54:01.227', 'Id': '706'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '229', 'Title': 'Research on evaluating the performance of cache-obliviousness in practice', 'LastEditDate': '2012-03-26T20:58:01.420', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '2', 'Body': u'<p><a href="http://en.wikipedia.org/wiki/Cache-oblivious_algorithm" rel="nofollow">Cache-oblivious algorithms and data structures</a> are a rather new thing, introduced by Frigo et al. in <a href="http://userweb.cs.utexas.edu/~pingali/CS395T/2009fa/papers/coAlgorithms.pdf" rel="nofollow">Cache-oblivious algorithms, 1999</a>. Prokop\'s <a href="http://www.google.fi/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http%3A%2F%2Fsupertech.csail.mit.edu%2Fpapers%2FProkop99.pdf&amp;ei=Dc1tT-aLI8bm4QSC4YjAAg&amp;usg=AFQjCNHWhtzqOQqUonQWHduna8_nbQYx2g&amp;sig2=Nf_YDGY3NZLj7q0FY6TZgw" rel="nofollow">thesis</a> from the same year introduces the early ideas as well.</p>\n\n<p>The paper by Frigo et al. present some experimental results showing the potential of the theory and of the cache-oblivious algorithms and data structures. Many cache-oblivious data structures are based on static search trees. Methods of storing and navigating these trees have been developed quite a bit, perhaps most notably by Bender et al. and also by Brodal et al. Demaine gives a nice <a href="http://www.cs.uwaterloo.ca/~imunro/cs840/DemaineCache.pdf" rel="nofollow">overview</a>.</p>\n\n<p>The experimental work of investigating the cache behaviour in practice was done at least by Ladner et al. in <a href="http://www.cs.amherst.edu/~ccm/cs34/papers/ladnerbst.pdf" rel="nofollow">A Comparison of Cache Aware and Cache Oblivious Static Search Trees Using Program Instrumentation, 2002</a>. Ladner et al. benchmarked the cache behaviour of algorithms solving the binary search problem, using the classic algorithm, cache-oblivious algorithm and cache-aware algorithm. Each algorithm was benchmarked with both implicit and explicit navigation methods. In addition to this, the thesis by <a href="http://www.diku.dk/forskning/performance-engineering/frederik/thesis.pdf" rel="nofollow">R\xf8nn, 2003</a> analyzed the same algorithms to quite high detail and also performed even more thorough testing of the same algorithms as Ladner et al.</p>\n\n<p><strong>My question is</strong></p>\n\n<blockquote>\n  <p>Has there been any newer research on <em>benchmarking</em> the cache behaviour of cache-oblivious algorithms in <em>practice</em> since? I\'m especially interested in the performance of the static search trees, but I would also be happy with any other cache-oblivious algorithms and data structures.</p>\n</blockquote>\n', 'Tags': '<algorithms><data-structures><computer-architecture><reference-request><cpu-cache>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-03-26T20:58:01.420', 'CommentCount': '1', 'AcceptedAnswerId': '741', 'CreationDate': '2012-03-24T13:51:44.963', 'Id': '740'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to understand what is wrong with the following proof of the following recurrence </p>\n\n<p>$$\r\nT(n) = 2\\,T\\!\\left(\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\right)+n \r\n$$\n$$\r\nT(n) \\leq 2\\left(c\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\right)+n \\leq cn+n = n(c+1) =O(n)\r\n$$</p>\n\n<p>The documentation says it's wrong because of the inductive hypothesis that\n$$\r\nT(n) \\leq cn\r\n$$\nWhat Am I missing?</p>\n", 'ViewCount': '418', 'Title': 'Error in the use of asymptotic notation', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-26T18:24:32.487', 'LastEditDate': '2012-03-26T18:24:32.487', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '777', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '736', 'Tags': '<algorithms><landau-notation><asymptotics><recurrence-relation>', 'CreationDate': '2012-03-25T21:16:46.657', 'Id': '772'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '635', 'Title': 'How To Best Learn About Algorithms In Depth', 'LastEditDate': '2012-03-29T15:01:07.467', 'AnswerCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '106', 'FavoriteCount': '5', 'Body': '<p>I have been reading this site with a great deal of interest, but I find a lot of it goes over my head. This has made me wish to learn a lot more about algorithms and CS in general. As far as I can tell from my research, there are 2 main ways of doing this. </p>\n\n<ol>\n<li><p>I can by a nice thick heavy book and work my way through it slowly but surely.</p></li>\n<li><p>I can "learn by doing" and by a nice book, but instead of reading it cover to cover, move to parts that interest me and work on implementing and applying algorithms I like.</p></li>\n<li><p>?</p></li>\n</ol>\n\n<p>My question is, which of the above did the you use and would you recommend the same approach to someone else?</p>\n', 'Tags': '<algorithms><education>', 'LastEditorUserId': '5', 'LastActivityDate': '2012-09-17T10:09:54.860', 'CommentCount': '1', 'AcceptedAnswerId': '819', 'CreationDate': '2012-03-27T13:12:49.160', 'Id': '818'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I <a href="http://blog.computationalcomplexity.org/2004/04/blum-complexity-measures.html" rel="nofollow">read</a> that the number of coin tosses of a probabilistic Turing machine (PTM) is not a <a href="http://en.wikipedia.org/wiki/Blum_axioms" rel="nofollow">Blum complexity measure</a>. Why?</p>\n\n<p>Clarification:</p>\n\n<p>Note that since the execution of the machine is not deterministic, one should be careful about defining the number of coin tosses for a PTM $M$ on input $x$ in a way similar to the time complexity for NTMs and PTMs. One way is to define it as the maximum number of coin tosses over possible executions of $M$ on $x$.</p>\n\n<p>We need the definition to satisfy the axiom about decidability of $m(M,x)=k$. We can define it as follows:</p>\n\n<p>$$\nm(M,x) =\n\\begin{cases}\nk &amp; \\text{all executions of $M$ on $x$ halt, $k=\\max$ #coin tosses} \\\\\n\\infty &amp; o.w. \\\n\\end{cases}\n$$</p>\n\n<p>The number of random bits that an algorithm uses is a complexity measure that appears in papers, e.g. "algorithm $A$ uses only $\\lg n$ random bits, whereas algorithm $B$ uses $n$ random bits".</p>\n', 'ViewCount': '119', 'Title': 'Is the number of coin tosses of a probabilistic Turing machine a Blum complexity measure?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-28T17:32:38.167', 'LastEditDate': '2012-03-28T17:32:38.167', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<computability><complexity-theory><randomness><probabilistic-algorithms>', 'CreationDate': '2012-03-27T20:35:16.207', 'Id': '835'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In algorithm analysis you often have to solve recurrences. In addition to Master Theorem, substitution and iteration methods, there is one using <em>characteristic polynomials</em>.</p>\n\n<p>Say I have concluded that a characteristic polynomial $x^2 - 2x + 2$ has <em>imaginary</em> roots, namely $x_1 = 1+i$ and $x_2 =1-i$. Then I cannot use</p>\n\n<p>$\\qquad c_1\\cdot x_1^n + c_2\\cdot x_2^n$</p>\n\n<p>to obtain the solution, right? How should I proceed in this case?</p>\n', 'ViewCount': '438', 'Title': 'Solving Recurrences via Characteristic Polynomial with Imaginary Roots', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-19T00:28:07.237', 'LastEditDate': '2012-04-01T07:20:09.083', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '852', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation>', 'CreationDate': '2012-03-31T10:13:58.077', 'FavoriteCount': '1', 'Id': '915'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1181', 'Title': 'Examples of sophisticated recursive algorithms', 'LastEditDate': '2012-03-31T22:18:18.110', 'AnswerCount': '6', 'Score': '10', 'OwnerDisplayName': 'elektronaj', 'PostTypeId': '1', 'OwnerUserId': '859', 'FavoriteCount': '1', 'Body': '<p>I was explaining the famous deterministic <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Linear_general_selection_algorithm_-_Median_of_Medians_algorithm">linear-time selection algorithm</a> (median of medians algorithm) to a friend.</p>\n\n<p>The recursion in this algorithm (while being very simple) is quite sophisticated. There are two recursive calls, each with different parameters.</p>\n\n<p>I was trying to find other examples of such interesting recursive algorithms, but could not find any. All of the recursive algorithms I could come up with are either simple tail-recursions or simple divide and conquer (where the two calls are "the same").</p>\n\n<p>Can you give some examples of sophisticated recursion?</p>\n', 'Tags': '<algorithms><recursion>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-02T09:17:34.660', 'CommentCount': '10', 'CreationDate': '2012-03-31T19:31:29.977', 'Id': '923'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '6955', 'Title': 'Adding elements to a sorted array', 'LastEditDate': '2012-04-01T07:07:56.053', 'AnswerCount': '5', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '1', 'Body': '<p>What would be the fastest way of doing this (from an algorithmic perspective, as well as a practical matter)?</p>\n\n<p>I was thinking something along the following lines.</p>\n\n<p>I could add to the end of an array and then use bubblesort as it has a best case (totally sorted array at start) that is close to this, and has linear running time (in the best case).</p>\n\n<p>On the other hand, if I know that I start out with a sorted array, I can use a binary search to find out the insertion point for a given element.</p>\n\n<p>My hunch is that the second way is nearly optimal, but curious to see what is out there.</p>\n\n<p>How can this best be done?</p>\n', 'Tags': '<algorithms><efficiency><arrays><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-06T17:17:29.770', 'CommentCount': '3', 'AcceptedAnswerId': '931', 'CreationDate': '2012-04-01T01:49:35.277', 'Id': '930'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a CSP model where changing the value of a particular variable is expensive. Is there any work where the objective function also considers the number of changes in the value of the variable during the search process?</p>\n\n<p>Update: An example: The expensive-to-change variable may be in the control of some other agent and there is some overhead of involving that agent to change the variable. Another example: The variable participates in one of the constraints, and the satisfaction of this constraint involves calling an expensive function (such as, a simulator), e.g. z = f(x, y) is the constraint, and f is an expensive-to-compute function. x and y are therefore expensive-to-change variables.</p>\n', 'ViewCount': '104', 'Title': 'In Constraint Programming, are there any models that take into account the number of variable changes?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-05T14:48:45.240', 'LastEditDate': '2012-04-03T02:05:51.637', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '893', 'Tags': '<algorithms><constraint-programming>', 'CreationDate': '2012-04-02T06:56:55.063', 'FavoriteCount': '0', 'Id': '987'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://dictionary.reference.com/browse/Codd%27s+reduction+algorithm">Codd\'s Algorithm</a> converts an expression in tuple relational calculus to Relational Algebra.</p>\n\n<ol>\n<li>Is there a standard implementation of the algorithm? </li>\n<li>Is this algorithm used anywhere? (It seems that the industry only needs SQL and variants, I\'m not sure about database theorists in academia.)</li>\n<li>What\'s the complexity of the reduction?</li>\n</ol>\n\n<p><sub> This was posted on <a href="http://stackoverflow.com/questions/4149840/about-codds-reduction-algorithm">SO</a> over a year ago, but it didn\'t receive a good answer. </sub>  </p>\n', 'ViewCount': '197', 'Title': "About Codd's reduction algorithm", 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-03T09:39:42.760', 'LastEditDate': '2012-04-03T02:16:02.700', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1003', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '898', 'Tags': '<algorithms><database-theory>', 'CreationDate': '2012-04-02T17:24:45.813', 'Id': '996'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an efficient algorithm that lets me process the minimax search tree for chess with <a href="http://en.wikipedia.org/wiki/Alpha-beta_pruning" rel="nofollow">alpha-beta pruning</a> on a distributed architecture. The algorithms I have found (PVS, YBWC, DTS see below) are all quite old (1990 being the latest). I assume there have been many substantial advancements since then. What is the current standard in this field?</p>\n\n<p>Also please point me to an idiot\'s explanation of DTS as I can\'t understand it from the research papers that I have read.</p>\n\n<p>The algorithms mentioned above:</p>\n\n<ul>\n<li>PVS: Principle Variation Splitting</li>\n<li>YBWC: Young Brothers Wait Concept</li>\n<li>DTS: Dynamic Tree Splitting</li>\n</ul>\n\n<p>are all are discussed <a href="http://chessprogramming.wikispaces.com/Parallel+Search" rel="nofollow">here</a>.</p>\n', 'ViewCount': '250', 'Title': 'distributed alpha beta pruning', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-03T02:19:30.217', 'LastEditDate': '2012-04-03T02:19:30.217', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '905', 'Tags': '<algorithms><distributed-systems><board-games>', 'CreationDate': '2012-04-02T21:00:57.743', 'Id': '998'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '371', 'LastEditorDisplayName': 'user742', 'Title': 'Optimal algorithm for finding the girth of a sparse graph?', 'LastEditDate': '2012-04-05T14:18:34.090', 'AnswerCount': '1', 'Score': '16', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': '<p>I wonder how to find the <a href="http://en.wikipedia.org/wiki/Girth_%28graph_theory%29">girth</a> of a sparse undirected graph. By sparse I mean $|E|=O(|V|)$. By optimum I mean the lowest time complexity.</p>\n\n<p>I thought about some modification on <a href="http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm">Tarjan\'s algorithm</a> for undirected graphs, but I didn\'t find good results. Actually I thought that if I could find a 2-connected components in $O(|V|)$, then I can find the girth, by some sort of induction which can be achieved from the first part. I may be on the wrong track, though. Any algorithm asymptotically better than $\\Theta(|V|^2)$ (i.e. $o(|V|^2)$) is welcome.</p>\n', 'Tags': '<algorithms><time-complexity><graph-theory>', 'LastActivityDate': '2013-04-15T08:22:17.407', 'CommentCount': '9', 'AcceptedAnswerId': '1087', 'CreationDate': '2012-04-02T23:30:30.770', 'Id': '1001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider two 2D-Array $B_{ij} $ (the buy array) and $S_{ij}$ (the sell array) where each $i^{th}$ element is associated with an array of floating-point values and each of the floating point value, in turn, is associated with an array of integers.</p>\n\n<p>For example</p>\n\n<pre><code>B = [ \n  0001 =&gt; [ 32.5 =&gt; {10, 15, 20}, \n            45.2 =&gt; {48, 16, 19}, \n            ...,\n            k1\n          ], \n  0002 =&gt; [ 35.6 =&gt; {17, 35, 89}, \n            68.7 =&gt; {18, 43, 74}, \n            ...,\n            k2\n          ] \n] \n</code></pre>\n\n<p>and similiarly for the sell array.</p>\n\n<p>This is akin to an order association system of a stock/commodity exchange </p>\n\n<pre><code>BuyOrderBook = [\n                 CompanyName =&gt; [\n                                 Price1 =&gt; [Qty1, Qty2...],\n                                 Price2 =&gt; [Qty1, Qty2...]\n                                ]\n                 SecondCompany = [...]\n               ]\n</code></pre>\n\n<p>What is the fastest way known to solve the following problem:</p>\n\n<blockquote>\n  <p><strong>Input:</strong> Buy array $B$, Sell array $S$<br>\n  <strong>Problem:</strong> Decide wether there are $(c_1 \\Rightarrow p_1 \\Rightarrow q_1) \\in B$ and $(c_2 \\Rightarrow p_2 \\Rightarrow q_2) \\in S$ with $q_1, q_2 &gt; 0$ and $p_2 \\geq p_1$.</p>\n</blockquote>\n\n<p>In short, what is the fatest way of matching orders for an exchange?</p>\n\n<p><strong>Update in response to comments</strong></p>\n\n<p>Lets say, MSFT has 25 shares @ \\$60 to be sold and there is buyer who is willing to offer \\$61 for 10 shares of MSFT. Then the buyer gets 10 shares @ \\$60 and the buy order book becomes empty while the sell order book now is updated with new quantity - 15 shares @ \\$60.</p>\n\n<p>Now take the reverse case, MSFT has 25 shares @ \\$60 to be bought and there is seller who is willing to receive \\$61 for 10 shares of MSFT. Then the trade will not be executed because seller is demanding a minimum of \\$61 and the buyer is offering a maximum of \\$60. The orders are now stored and await until new orders are received.</p>\n\n<p>(This is the <a href="http://en.wikipedia.org/wiki/Order_%28exchange%29#Limit_order" rel="nofollow">limit order principle</a>, where the seller specifies minimum price at which he is willing to sell at and buyer specifies maximum price at which he is willing to buy). </p>\n\n<p>Post execution, the sell order book will be (25-10)=15@86.5 while buy order book will be empty (10-10)=0.</p>\n', 'ViewCount': '495', 'Title': 'Most efficient know way to match orders', 'LastEditorUserId': '191', 'LastActivityDate': '2012-04-04T13:19:02.627', 'LastEditDate': '2012-04-04T13:19:02.627', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '191', 'Tags': '<algorithms>', 'CreationDate': '2012-04-03T14:28:53.430', 'FavoriteCount': '2', 'Id': '1004'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a test about the <a href="http://en.wikipedia.org/wiki/Branch_and_bound">branch and bound</a> algorithm. I understand theoretically how this algorithm works but I couldn\'t find examples that illustrates how this algorithm can be implemented practically. </p>\n\n<p>I found some examples such as <a href="http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html">this one</a>\nbut I\'m still confused about it. I also looked for travelling salesman problem and I couldn\'t understand it.</p>\n\n<p>What I need is some problems and how can these problems solved by using branch and bound.</p>\n', 'ViewCount': '2222', 'Title': 'Branch and Bound explanation', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-25T17:00:01.013', 'LastEditDate': '2012-10-14T09:28:44.170', 'AnswerCount': '2', 'CommentCount': '11', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '701', 'Tags': '<algorithms><optimization><branch-and-bound>', 'CreationDate': '2012-04-04T00:31:20.227', 'FavoriteCount': '4', 'Id': '1016'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have noticed that different data structures are used when we implement search algorithms. For example, we use queues to implement breadth first search, stacks to implement depth-first search and min-heaps to implement the <a href="https://en.wikipedia.org/wiki/A%2a_algorithm">A* algorithm</a>. In these cases, we do not need to construct the search tree explicitly.</p>\n\n<p>But I can not find a simple data structure to simulate the searching process of the <a href="http://www.cs.cf.ac.uk/Dave/AI2/node26.html">AO* algorithm</a>. I would like to know if constructing the search tree explicitly is the only way to implement AO* algorithm? Can anybody provide me an efficient implementation?</p>\n', 'ViewCount': '994', 'Title': 'How to implement AO* algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T20:12:52.987', 'LastEditDate': '2012-09-22T09:04:58.943', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '931', 'Tags': '<algorithms><graphs><data-structures><search-algorithms>', 'CreationDate': '2012-04-04T03:05:59.537', 'FavoriteCount': '2', 'Id': '1020'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '245', 'Title': 'Supporting data structures for SAT local search', 'LastEditDate': '2012-04-05T17:15:02.153', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/WalkSAT">WalkSAT and GSAT</a> are well-known and simple local search algorithms for solving the Boolean satisfiability problem. The pseudocode for the GSAT algorithm is copied from the question <a href="http://cs.stackexchange.com/questions/219/implementing-the-gsat-algorithm-how-to-select-which-literal-to-flip">Implementing the GSAT algorithm - How to select which literal to flip?</a> and presented below.</p>\n\n<pre><code>procedure GSAT(A,Max_Tries,Max_Flips)\n  A: is a CNF formula\n  for i:=1 to Max_Tries do\n    S &lt;- instantiation of variables\n    for j:=1 to Max_Iter do\n      if A satisfiable by S then\n        return S\n      endif\n      V &lt;- the variable whose flip yield the most important raise in the number of satisfied clauses;\n      S &lt;- S with V flipped;\n    endfor\n  endfor\n  return the best instantiation found\nend GSAT\n</code></pre>\n\n<p>Here we flip the variable that maximizes the number of satisfied clauses. How is this done efficiently? The naive method is to flip every variable, and for each step through all clauses and calculate how many of them get satisfied. Even if a clause could be queried for satisfiability in constant time, the naive method would still run in $O(VC)$ time, where $V$ is the number of variables and $C$ the number of clauses. I\'m sure we can do better, hence the question:</p>\n\n<blockquote>\n  <p>Many local search algorithms flip the variable\'s assignment that maximizes the number of satisfied clauses. In practice, with what data structures is this operation supported efficiently?</p>\n</blockquote>\n\n<p>This is something I feel like textbooks often omit. One example is even the famous <a href="http://aima.cs.berkeley.edu/">Russell &amp; Norvig book</a>.</p>\n', 'Tags': '<algorithms><data-structures><satisfiability>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-15T16:21:12.517', 'CommentCount': '4', 'AcceptedAnswerId': '1262', 'CreationDate': '2012-04-05T16:08:17.820', 'Id': '1058'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '315', 'Title': 'How many shortest distances change when adding an edge to a graph?', 'LastEditDate': '2012-04-06T09:55:35.533', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '92', 'FavoriteCount': '2', 'Body': '<p>Let $G=(V,E)$ be some complete, weighted, undirected graph. We construct a second graph $G&#39;=(V, E&#39;)$ by adding edges one by one from $E$ to $E&#39;$. We add $\\Theta(|V|)$ edges to $G&#39;$ in total.</p>\n\n<p>Every time we add one edge $(u,v)$ to $E&#39;$, we consider the shortest distances between all pairs in $(V, E&#39;)$ and $(V, E&#39; \\cup \\{ (u,v) \\})$. We count how many of these shortest distances have changed as a consequence of adding $(u,v)$. Let $C_i$ be the number of shortest distances that change when we add the $i$th edge, and let $n$ be the number of edges we add in total.</p>\n\n<blockquote>\n  <p>How big is $C = \\frac{\\sum_i C_i}{n}$?</p>\n</blockquote>\n\n<p>As $C_i = O(|V|^2)=O(n^2)$, $C=O(n^2)$ as well. Can this bound be improved? Note that I define $C$ to be the average over all edges that were added, so a single round in which a lot of distances change is not that interesting, though it proves that $C = \\Omega(n)$.</p>\n\n<p>I have an algorithm for computing a geometric t-spanner greedily that works in $O(C n \\log n)$ time, so if $C$ is $o(n^2)$, my algorithm is faster than the original greedy algorithm, and if $C$ is really small, potentially faster than the best known algorithm (though I doubt that).</p>\n\n<p>Some problem-specific properties that might help with a good bound: the edge $(u,v)$ that is added always has larger weight than any edge already in the graph (not necessarily strictly larger). Furthermore, its weight is shorter than the shortest path between $u$ and $v$.</p>\n\n<p>You may assume that the vertices correspond to points in a 2d plane and the distances between vertices are the Euclidian distances between these points. That is, every vertex $v$ corresponds to some point $(x,y)$ in the plane, and for an edge $(u,v)=((x_1,y_1),(x_2,y_2))$ its weight is equal to $\\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2.}$</p>\n', 'Tags': '<algorithms><graphs><graph-theory><shortest-path>', 'LastEditorUserId': '92', 'LastActivityDate': '2014-01-19T02:02:00.257', 'CommentCount': '11', 'AcceptedAnswerId': '1063', 'CreationDate': '2012-04-05T19:15:54.200', 'Id': '1062'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '736', 'Title': 'Is there an algorithm which finds sorted subsequences of size three in $O(n)$ time?', 'LastEditDate': '2013-05-05T20:08:44.940', 'AnswerCount': '3', 'Score': '17', 'OwnerDisplayName': 'Christopher Done', 'PostTypeId': '1', 'OwnerUserId': '969', 'FavoriteCount': '2', 'Body': u"<p>I want to prove or disprove the existence of an algorithm which, given an array $A$ of integers, finds three indices $i, j$ and $k$ such that $i &lt; j &lt; k$ and $A[i] &lt; A[j] &lt; A[k]$ (or finds that there is no such triple) in linear time.</p>\n\n<p>This is not a homework question; I saw it on a programming forum framed as \u201ctry to implement such an algorithm.\u201d I suspect that it is impossible after various experiments. My intuition tells me so, but that does not really count for anything.</p>\n\n<p>I would like to prove it formally. How do you do it? I would ideally like to see a proof laid out step-by-step, and then if you are so inclined, some explanation of how to go about proving/disproving simple questions like this in general. If it helps, some examples:</p>\n\n<pre><code>[1,5,2,0,3] \u2192 (1,2,3)\n[5,6,1,2,3] \u2192 (1,2,3)\n[1,5,2,3] \u2192 (1,2,3)\n[5,6,1,2,7] \u2192 (1,2,7)\n[5,6,1,2,7,8] \u2192 (1,2,7)\n[1,2,999,3] \u2192 (1,2,999)\n[999,1,2,3] \u2192 (1,2,3)\n[11,12,8,9,5,6,3,4,1,2,3] \u2192 (1,2,3)\n[1,5,2,0,-5,-2,-1] \u2192 (-5,-2,-1)\n</code></pre>\n\n<p>I supposed that one could iterate over $A$, and each time there is an $i &lt; j$ (our current $j$, that is), we make a new triple and push it onto an array. We continue stepping and comparing each triple until one of our triples is complete. So it's like <code>[1,5,2,0,-5,-2,-1] \u2192 1..2.. -5.. -2.. -1</code>, <code>[1,5,2,0,-5,-2,3,-1] \u2192 1..2.. -5.. -2.. 3</code>! But I think this is more complex than mere $\\mathcal{O}(n)$ as the number of triples on our triple array would in the worst case correspond to the size of the input list.</p>\n", 'Tags': '<algorithms><arrays><subsequences>', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-22T17:22:29.563', 'CommentCount': '2', 'AcceptedAnswerId': '1073', 'CreationDate': '2012-04-05T20:56:01.307', 'Id': '1071'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This problem is taken from <a href="https://www.interviewstreet.com/challenges/dashboard/#problem/4eed18ded76fe">interviewstreet.com</a></p>\n\n<p>We are given an array of integers $Y=\\{y_1,...,y_n\\}$ that represents $n$ line segments such that endpoints of segment $i$ are $(i, 0)$ and $(i, y_i)$. Imagine that from the top of each segment a horizontal ray is shot to the left, and this ray stops when it touches another segment or it hits the y-axis. We construct an array of n integers, $v_1, ..., v_n$, where $v_i$ is equal to length of ray shot from the top of segment $i$. We define $V(y_1, ..., y_n)\r\n= v_1 + ... + v_n$.</p>\n\n<p>For example, if we have $Y=[3,2,5,3,3,4,1,2]$, then $[v_1, ..., v_8] = [1,1,3,1,1,3,1,2]$, as shown in the picture below:</p>\n\n<p><img src="http://i.stack.imgur.com/bZ04e.png" alt="enter image description here"></p>\n\n<p>For each permutation $p$ of $[1,...,n]$, we can calculate $V(y_{p_1}, ..., y_{p_n})$. If we choose a uniformly random permutation $p$ of $[1,...,n]$, what is the expected value of $V(y_{p_1}, ..., y_{p_n})$?</p>\n\n<p>If we solve this problem using the naive approach it will not be efficient and run practically forever for $n=50$. I believe we can approach this problem by indepdently calculating the expected value of $v_i$ for each stick but I still need to know wether there is another efficient approach for this problem. On what basis can we calculate the expected value for each stick independently?</p>\n', 'ViewCount': '899', 'Title': 'How to approach Vertical Sticks challenge', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-10T11:00:33.150', 'LastEditDate': '2012-04-06T17:28:56.150', 'AnswerCount': '5', 'CommentCount': '1', 'Score': '16', 'OwnerDisplayName': 'Anantha Krishnan', 'PostTypeId': '1', 'Tags': '<algorithms><probability-theory>', 'CreationDate': '2012-04-06T07:44:24.677', 'FavoriteCount': '2', 'Id': '1076'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am currently working on a solution to a problem for which (after a bit of research) the use of a hill climbing, and more specificly a <em>shotgun</em> (or <em>random-restart</em>) <a href="http://en.wikipedia.org/wiki/Hill_climbing" rel="nofollow">hill climbing</a> algorithmic idea seems to be the best fit, as I have no clue how the best start value can be found.</p>\n\n<p>But there is not a lot of information about this type of algorithm except the <a href="http://en.wikipedia.org/wiki/Hill_climbing#Variants" rel="nofollow">rudimentary idea</a> behind it:</p>\n\n<blockquote>\n  <p>[Shotgun] hill climbing is a meta-algorithm built on top of the hill climbing algorithm. It iteratively does hill-climbing, each time with a random initial condition $x_0$. The best $x_m$ is kept: if a new run of hill climbing produces a better $x_m$ than the stored state, it replaces the stored state.</p>\n</blockquote>\n\n<p>If I understand this correctly, this means something like this (assuming maximisation):</p>\n\n<pre><code>x = -infinity;\nfor ( i = 1 .. N ) {\n  x = max(x, hill_climbing(random_solution()));\n}\nreturn x;\n</code></pre>\n\n<p>But how can you make this really effective, that is better than normal hill climbing? It is hard to believe that using random start values helps a lot, especially for huge search spaces. More precisely, I wonder:</p>\n\n<ul>\n<li>Is there a good strategy for choosing the $x_0$ (that is implementing <code>random_solution</code>), in particular knowing (intermediate) results of former iterations?</li>\n<li>How to choose $N$, that is how many iterations are needed to be quite certain that the perfect solution is not missed (by much)?</li>\n</ul>\n', 'ViewCount': '396', 'Title': 'How to implement the details of shotgun hill climbing to make it effective?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-02T20:52:19.537', 'LastEditDate': '2012-09-02T15:43:00.100', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1089', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '78', 'Tags': '<algorithms><optimization><heuristics>', 'CreationDate': '2012-04-06T19:13:39.907', 'Id': '1084'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to implement bidirectional search in a graph. I am using two breadth first searches from the start node and the goal node. The states that have been checked are stored in two hash tables (closed lists).\nHow can I get the solution (path from the start to the goal), when I find that a state that is checked by one of the searches is in the closed list of the other?</p>\n\n<p>EDIT </p>\n\n<p>Here are the explanations from the book:\n<em>"Bidirectional search is implemented by having one or both of the searches check each\nnode before it is expanded to see if it is in the fringe of the other search tree; if so, a solution has been found... Checking a node for membership in the other search tree can be done in constant time with a hash table..."</em></p>\n\n<p>Some pages before: \n<em>"A node is a boolkkeeping data structure used to represent the search tree. A state corresponds to a configuration of the world... two different nodes can contain the same world state, if that state is generated via two different search paths."</em> So from that I conclude that if nodes are kept in the hash tables than a node from the BFS started from the start node would not match a node constructed from the other BFS started from the goal node.</p>\n\n<p>And later in general Graph search algorithm the states are stored in the closed list, not the nodes, but it seems to me that even that the states are saved in the hash tables after that the nodes are retrieved from there.</p>\n', 'ViewCount': '546', 'Title': 'How to construct the found path in bidirectional search', 'LastEditorUserId': '770', 'LastActivityDate': '2012-05-01T21:04:41.210', 'LastEditDate': '2012-04-07T21:08:01.660', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '1620', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '770', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2012-04-07T16:12:39.053', 'Id': '1113'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On Alpha-Beta pruning, <a href="http://en.wikipedia.org/wiki/Negascout" rel="nofollow">NegaScout</a> claims that it can accelerate the process by setting [Alpha,Beta] to [Alpha,Alpha-1].</p>\n\n<p>I do not understand the whole process of NegaScout.</p>\n\n<p>How does it work? What is its recovery mechanism when its guessing failed?</p>\n', 'ViewCount': '714', 'Title': 'How does the NegaScout algorithm work?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-08T21:24:54.280', 'LastEditDate': '2012-04-08T21:24:54.280', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '240', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2012-04-08T16:07:29.703', 'Id': '1134'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am interested in calculating the $n$'th power of a $n\\times n$ matrix $A$. Suppose we have an algorithm for matrix multiplication which runs in $\\mathcal{O}(M(n))$ time. Then, one can easily calculate $A^n$ in $\\mathcal{O}(M(n)\\log(n))$ time. Is it possible to solve this problem in lesser time complexity?</p>\n\n<p>Matrix entries can, in general, be from a semiring but you can assume additional structure if it helps.</p>\n\n<p>Note: I understand that in general computing $A^m$ in $o(M(n)\\log(m))$ time would give a $o(\\log m)$ algorithm for exponentiation. But, a number of interesting problems reduce to the special case of matrix exponentiation where m=$\\mathcal O(n)$, and I was not able to prove the same about this simpler problem.</p>\n", 'ViewCount': '975', 'Title': 'Complexity of computing matrix powers', 'LastEditorUserId': '984', 'LastActivityDate': '2012-04-10T09:49:46.710', 'LastEditDate': '2012-04-10T09:49:46.710', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '984', 'Tags': '<algorithms><complexity-theory><time-complexity><computer-algebra>', 'CreationDate': '2012-04-09T00:05:00.413', 'FavoriteCount': '2', 'Id': '1147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am implementing a set of path finding algorithms such as Dijkstra's, Depth First, etc.</p>\n\n<p>At first I used a couple of self made graphs, but now I'd like to take the challenge a bit further and thus I'm looking for either</p>\n\n<ol>\n<li>graphs used in benchmarks;</li>\n<li>graphs of real world cities (or a way to download that kind of info off google maps, or any other kind of source, if possible).</li>\n</ol>\n\n<p>I'd like those sources to either have or allow me to easily create frontiers such that I can try my algorithms for different sized sets of graphs, if possible.</p>\n\n<p>I'm looking for simple solutions, as I'd prefer not to be diverted from main goal (compare a set of different algorithms), so I'd need a quick way to convert that graph data into my own format (basically, a set of connected <code>(x, y)</code> points).</p>\n\n<p>To be more concrete, what I'm looking for are 2D cyclic graphs. If those graphs reflect real world city streets (taking into consideration one-way streets, two-way streets, etc, better yet!).</p>\n", 'ViewCount': '327', 'Title': 'Where to get graphs to test my search algorithms against?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-08T23:01:11.637', 'LastEditDate': '2012-04-09T11:16:45.717', 'AnswerCount': '2', 'CommentCount': '10', 'Score': '19', 'PostTypeId': '1', 'OwnerUserId': '8073', 'Tags': '<algorithms><graphs><data-sets><benchmarking>', 'CreationDate': '2012-04-09T03:37:40.150', 'FavoriteCount': '5', 'Id': '1151'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My question goes to those who are concerned with computational biology algorithmics. I'm going to take a course on bioinformatics this fall; the problem, however, is that I have too little background in biology and chemistry to feel prepared for that cycle of lections (I was rather weak at these subjects at school).</p>\n\n<p>Could you recommend any books that would provide a good introduction to the questions of natural sciences that bioinformatics focuses on?</p>\n", 'ViewCount': '196', 'Title': 'Introductory books on nature sciences behind bioinformatics', 'LastEditorUserId': '6416', 'LastActivityDate': '2013-01-14T23:06:35.373', 'LastEditDate': '2013-01-14T20:32:01.657', 'AnswerCount': '4', 'CommentCount': '8', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><reference-request><education><bioinformatics>', 'CreationDate': '2012-04-09T09:12:41.920', 'FavoriteCount': '3', 'Id': '1156'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have developed the following pseudocode for the sum of pairs problem:</p>\n\n<blockquote>\n  <p>Given an array $A$ of integers and an integer $b$, return YES if there are positions $i,j$ in $A$ with $A[i] + A[j] = b$, NO otherwise.</p>\n</blockquote>\n\n<p>Now I should state a loop invariant that shows that my algorithm is correct. Can someone give me a hint of a valid loop invariant? </p>\n\n<pre><code>PAIRSUM(A,b):\nYES := true;\nNO := false;\nn := length(A);\nif n&lt;2 then\n  return NO;\n\nSORT(A);\ni := 1;\nj := n;\nwhile i &lt; j do  // Here I should state my invariant\n   currentSum := A[i] + A[j];\n   if currentSum = b  then\n      return YES;\n   else \n    if currentSum &lt; b then\n      i := i + 1;\n    else\n      j := j \u2013 1;\nreturn NO;\n</code></pre>\n', 'ViewCount': '428', 'Title': 'Loop invariant for an algorithm', 'LastEditorUserId': '635', 'LastActivityDate': '2012-04-10T08:37:32.370', 'LastEditDate': '2012-04-10T03:29:03.627', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><proof-techniques><loop-invariants>', 'CreationDate': '2012-04-09T10:03:11.910', 'Id': '1157'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I would like to write a simple program that accepts a set of windows (width+height) and the screen resolution and outputs an arrangement of those windows on the screen such that the windows take the most space. Therefore it is possible to resize a window, while maintaining  <code>output size &gt;= initial size</code> and the aspect ratio. So for window $i$, I'd like the algorithm to return a tuple $(x, y, width, height)$.</p>\n\n<p>I believe this is might be a variation of 2D Knapsack. I've tried going over results around the web but they mostly had a lot of background (and no implementation) that made it hard for me to follow.</p>\n\n<p>I'm less interested in the fastest possible algorithm, but more in something that is practical for my specific need.</p>\n", 'ViewCount': '458', 'Title': 'How to devise an algorithm to arrange (resizable) windows on the screen to cover as much space as possible?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-14T12:28:42.977', 'LastEditDate': '2012-04-11T21:05:32.490', 'AnswerCount': '2', 'CommentCount': '12', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '1042', 'Tags': '<algorithms><computational-geometry><packing><user-interface>', 'CreationDate': '2012-04-10T21:20:21.423', 'Id': '1217'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have the following algorithmic problem:</p>\n\n<blockquote>\n  <p>Determine the space Turing complexity of recognizing DNA strings that are Watson-Crick palindromes. </p>\n</blockquote>\n\n<p>Watson-Crick palindromes are strings whose reversed complement is the original string. The <em>complement</em> is defined letter-wise, inspired by DNA: A is the complement of T and C is the complement of G. A simple example for a WC-palindrome is ACGT.</p>\n\n<p>I've come up with two ways of solving this.</p>\n\n<p><strong>One requires $\\mathcal{O}(n)$ space.</strong></p>\n\n<ul>\n<li>Once the machine is done reading the input. The input tape must be copied to the work tape in reverse order. </li>\n<li>The machine will then read the input and work tapes from the left and compare each entry to verify the cell in the work tape is the compliment of the cell in the input. This requires $\\mathcal{O}(n)$ space. </li>\n</ul>\n\n<p><strong>The other requires $\\mathcal{O}(\\log n)$ space.</strong></p>\n\n<ul>\n<li>While reading the input. Count the number of entries on the input tape.</li>\n<li>When the input tape is done reading\n<ul>\n<li>copy the complement of the letter onto the work tape</li>\n<li>copy the letter L to the end of the work tape</li>\n</ul></li>\n<li>(Loop point)If the counter = 0, clear the worktape and write yes, then halt</li>\n<li>If the input tape reads L\n<ul>\n<li>Move the input head to the left by the number of times indicated by the counter  (requires a second counter)</li>\n</ul></li>\n<li>If the input tape reads R \n<ul>\n<li>Move the input head to the right by the number of times indicated by the counter (requires a second counter)</li>\n</ul></li>\n<li>If the cell that holds the value on the worktape matches the current cell on the input tape\n<ul>\n<li>decrement the counter by two</li>\n<li>Move one to the left or right depending if R or L is on the worktape respectively</li>\n<li>copy the Complement of L or R to the worktape in place of the current L or R</li>\n<li>continue the loop</li>\n</ul></li>\n<li>If values dont match, clear the worktape and write no, then halt</li>\n</ul>\n\n<p>This comes out to about $2\\log n+2$ space for storing both counters, the current complement, and the value L or R.</p>\n\n<p><strong>My issue</strong></p>\n\n<p>The first one requires both linear time and space. The second one requires $\\frac{n^2}{2}$ time and $\\log n$ space. I was given the problem from the quote and came up with these two approaches, but I don't know which one to go with. I just need to give the space complexity of the problem. </p>\n\n<p><strong>The reason I'm confused</strong></p>\n\n<p>I would tend to say the second one is the best option since it's better in terms of time, but that answer only comes from me getting lucky and coming up with an algorithm. It seems like if I want to give the space complexity of something, it wouldn't require luck in coming up with the right algorithm. Am I missing something? Should I even be coming up with a solution to the problem to answer the space complexity?</p>\n", 'ViewCount': '406', 'Title': 'The space complexity of recognising Watson-Crick palindromes', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T07:35:10.873', 'LastEditDate': '2012-04-11T20:15:46.800', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '1224', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '596', 'Tags': '<algorithms><algorithm-analysis><turing-machines><space-complexity>', 'CreationDate': '2012-04-11T15:21:09.507', 'Id': '1223'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '855', 'Title': 'Why does the splay tree rotation algorithm take into account both the parent and grandparent node?', 'LastEditDate': '2012-08-10T11:00:07.597', 'AnswerCount': '1', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '1056', 'FavoriteCount': '2', 'Body': '<p>I don\'t quite understand why the rotation in the splay tree data structure is taking into account not only the parent of the rating node, but also the grandparent (zig-zag and zig-zig operation). Why would the following not work:</p>\n\n<p>As we insert, for instance, a new node to the tree, we check whether we insert into the left or right subtree. If we insert into the left, we rotate the result RIGHT, and vice versa for right subtree. Recursively it would be sth like this</p>\n\n<pre><code>Tree insert(Tree root, Key k){\n    if(k &lt; root.key){\n        root.setLeft(insert(root.getLeft(), key);\n        return rotateRight(root);\n    }\n    //vice versa for right subtree\n}\n</code></pre>\n\n<p>That should avoid the whole "splay" procedure, don\'t you think?</p>\n', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'LastEditorUserId': '187', 'LastActivityDate': '2012-08-10T11:00:07.597', 'CommentCount': '0', 'AcceptedAnswerId': '1230', 'CreationDate': '2012-04-11T21:37:01.123', 'Id': '1229'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a set $S = \\{ a_1,a_2,a_3,\\ldots , a_N \\}$ of $N$ coders.</p>\n\n<p>Each Coders has rating $R_i$ and the number of gold medals $E_i$, they had won so far.</p>\n\n<p>A Software Company wants to hire exactly three  coders to develop an application.</p>\n\n<p>For hiring three coders, they developed the following strategy:</p>\n\n<ol>\n<li>They first arrange the coders in ascending order of ratings and descending order of gold medals.</li>\n<li>From this arranged list, they select the three of the middle coders.\nE.g., if the arranged list is $(a_5,a_2,a_3,a_1,a_4)$ they select $(a_2,a_3,a_1)$ coders.</li>\n</ol>\n\n<p>Now we have to help company by writing a program  for this task.</p>\n\n<p><strong>Input:</strong></p>\n\n<p>The first line contains $N$, i.e. the number of coders.</p>\n\n<p>Then the second line contains the ratings $R_i$ of $i$th coder.</p>\n\n<p>The third line contains the number of gold medals bagged by the $i$th coder.</p>\n\n<p><strong>Output:</strong></p>\n\n<p>Display only one line that contains the sum of gold medals earned by the three coders the company will select.</p>\n', 'ViewCount': '169', 'Title': 'Efficiently selecting the median and elements to its left and right', 'LastEditorUserId': '31', 'LastActivityDate': '2012-04-13T08:30:58.127', 'LastEditDate': '2012-04-13T08:30:58.127', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1232', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '1001', 'Tags': '<algorithms><algorithm-design>', 'CreationDate': '2012-04-12T17:12:28.240', 'Id': '1231'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '214', 'Title': "Ordering elements so that some elements don't come between others", 'LastEditDate': '2012-05-30T07:24:52.507', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '5', 'Body': '<p>Given an integer $n$ and set of triplets of distinct integers\n$$S \\subseteq \\{(i, j, k) \\mid 1\\le i,j,k \\le n, i \\neq j, j \\neq k, i \\neq k\\},$$\nfind an algorithm which either finds a permutation $\\pi$ of the set $\\{1, 2, \\dots, n\\}$ such that\n$$(i,j,k) \\in S \\implies (\\pi(j)&lt;\\pi(i)&lt;\\pi(k)) ~\\lor~ (\\pi(i)&lt;\\pi(k)&lt;\\pi(j))$$\nor correctly determines that no such permutation exists.  Less formally, we want to reorder the numbers 1 through $n$; each triple $(i,j,k)$ in $S$ indicates that $i$ must appear before $k$ in the new order, but $j$ must not appear between $i$ and $k$.</p>\n\n<p><strong>Example 1</strong></p>\n\n<p>Suppose $n=5$ and $S = \\{(1,2,3), (2,3,4)\\}$.  Then</p>\n\n<ul>\n<li><p>$\\pi = (5, 4, 3, 2, 1)$ is <em>not</em> a valid permutation, because $(1, 2, 3)\\in S$, but $\\pi(1) &gt; \\pi(3)$.</p></li>\n<li><p>$\\pi = (1, 2, 4, 5, 3)$ is <em>not</em> a valid permutation, because $(1, 2, 3) \\in S$ but $\\pi(1) &lt; \\pi(3) &lt; \\pi(5)$.</p></li>\n<li><p>$(2, 4, 1, 3, 5)$ is a valid permutation.</p></li>\n</ul>\n\n<p><strong>Example 2</strong></p>\n\n<p>If $n=5$ and $S = \\{(1, 2, 3), (2, 1, 3)\\}$, there is no valid permutation.  Similarly, there is no valid permutation if $n=5$ and $S = \\{(1,2,3), (3,4,5), (2,5,3), (2,1,4)\\}$  (I think; may have made a mistake here).</p>\n\n<p><em>Bonus: What properties of $S$ determine whether a feasible solution exists?</em></p>\n', 'Tags': '<algorithms><optimization><scheduling>', 'LastEditorUserId': '72', 'LastActivityDate': '2012-05-30T07:24:52.507', 'CommentCount': '5', 'AcceptedAnswerId': '1275', 'CreationDate': '2012-04-13T19:26:19.010', 'Id': '1255'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am writing a Program, solving the <a href="http://en.wikipedia.org/wiki/Route_inspection_problem" rel="nofollow">Chinese Postman Problem</a> (also known as route inspection problem) in an undirected draph and currently facing the problem to find the best additional edges to connect the nodes with odd degree, so I can compute an Eulerian circuit.</p>\n\n<p>There might be (considering the size of the graph that wants to be solved) an enormous combination of edges which need to be computed and evaluated.</p>\n\n<p>As an example there are the odd-degree nodes $A, B, C, D, E, F, G, H$. The best combinations could be:</p>\n\n<ol>\n<li>$AB$, $CD$, $EF$, $GH$</li>\n<li>$AC$, $BD$, $EH$, $FG$</li>\n<li>$AD$, $BC$, $EG$, $FH$</li>\n<li>$AE$ ....</li>\n</ol>\n\n<p>where $AB$ means "edge between node $A$ and node $B$".</p>\n\n<p>Therefore my question is: is there a known algorithm to solve that problem in a complexity better than pure brute force (computing and evaluating them all)?</p>\n\n<p>\u20ac:After some research effort I found <a href="http://web.mit.edu/urban_or_book/www/book/chapter6/6.4.4.html" rel="nofollow">this</a> article, speaking about the "Edmonds\' minimum-length matching algorithm" but I cannot find any pseudo-code or learners-descriptions of this algorithm (or at least I do not recognize them, as Google offers a lot of hits an matching algorithms by J. Edmonds)</p>\n', 'ViewCount': '1120', 'Title': 'Chinese Postman Problem: finding best connections between odd-degree nodes', 'LastEditorUserId': '78', 'LastActivityDate': '2014-01-02T12:01:32.207', 'LastEditDate': '2012-04-15T00:21:19.243', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '78', 'Tags': '<algorithms><graphs><graph-theory>', 'CreationDate': '2012-04-13T21:05:09.293', 'FavoriteCount': '1', 'Id': '1257'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>(this is related to my other question, see <a href="http://cs.stackexchange.com/questions/1217/how-to-devise-an-algorithm-to-arrange-resizable-windows-on-the-screen-to-cover">here</a>)</p>\n\n<p>Imagine a screen, with 3 windows on it:</p>\n\n<p><img src="http://i.stack.imgur.com/vVUl3.jpg" alt="enter image description here"></p>\n\n<p>I\'d like to find an efficient data structure to represent this, while supporting these actions:</p>\n\n<ul>\n<li>return a list of coordinates where a given window can be positioned without overlapping with others\n<ul>\n<li>for the above example, if we want to insert a window of size 2x2, possible positions will be (8, 6), (8, 7), ..</li>\n</ul></li>\n<li>resizing a window on the screen without overlapping other windows while maintaining aspect ratio</li>\n<li>insert window at position x, y (assuming it doesn\'t overlap)</li>\n</ul>\n\n<p>Right now my naive approach is keeping an array of windows and going over all points on the screen, checking for each one if it\'s in any of the windows. This is $O(n\\cdot m\\cdot w)$ where $n, m$ are the width, height of the screen and $w$ is the number of windows in it. Note that in general $w$ will be small (say &lt; 10) where each window is taking a lot of space.</p>\n', 'ViewCount': '93', 'Title': 'Efficient queriable data structure to represent a screen with windows on it', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T11:39:14.260', 'LastEditDate': '2012-04-22T11:39:14.260', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1042', 'Tags': '<algorithms><computational-geometry><user-interface><modelling>', 'CreationDate': '2012-04-14T12:08:18.573', 'Id': '1268'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For the following jobs: </p>\n\n<p><img src="http://i.stack.imgur.com/rwOBN.png" alt="job table"></p>\n\n<p>The <strong>average wait time</strong> would be using a FCFS algorithm:</p>\n\n<p>(6-6)+(7-2)+(11-5)+(17-5)+(14-1) -> 0+5+6+10+13 -> 34/5 = 7 (6.8)</p>\n\n<p>What would the <strong>average turnaround time</strong> be? </p>\n', 'ViewCount': '8299', 'Title': 'What is the average turnaround time?', 'LastActivityDate': '2014-04-30T15:21:50.527', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '1279', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '935', 'Tags': '<algorithms><operating-systems><process-scheduling><scheduling>', 'CreationDate': '2012-04-14T13:25:49.020', 'Id': '1270'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We are given a set $F=\\{f_1, f_2, f_3, \u2026, f_N\\}$ of $N$ Fruits. Each Fruit has price $P_i$ and vitamin content $V_i$; we associated fruit $f_i$ with the ordered pair $(P_i, V_i)$. Now we have to arrange these fruits in such a way that the sorted list contains prices in ascending order and vitamin contents in descending order.</p>\n\n<p><strong>Example 1</strong>: $N = 4$ and $F = \\{(2, 8), (5, 11), (7, 9), (10, 2)\\}$.</p>\n\n<p>If we arrange the list such that all price are in ascending order and vitamin contents in descending order, then the valid lists are the following:</p>\n\n<ul>\n<li>$[(2, 8)]$</li>\n<li>$[(5, 11)]$</li>\n<li>$[(7, 9)]$</li>\n<li>$[(10, 2)]$</li>\n<li>$[(2, 8), (10, 2)]$</li>\n<li>$[(5, 11), (7, 9)]$</li>\n<li>$[(5, 11), (10, 2)]$</li>\n<li>$[(7, 9), (10, 2)]$</li>\n<li>$[(5, 11), (7, 9), (10, 2)]$</li>\n</ul>\n\n<p>From the above lists, I want to choose the list of maximal size. If more than one list has maximal size, we should choose the list of maximal size whose sum of prices is least. The list which should be chosen in the above example is $\\{(5, 11), (7, 9), (10, 2)\\}$.</p>\n\n<p><strong>Example 2</strong>: $N = 10$ and $$F = \\{(99,10),(12,23),(34,4),(10,5),(87,11),(19,10), \\\\(90,18), (43,90),(13,100),(78,65)\\}$$</p>\n\n<p>The answer to this example instance is $[(13,100),(43,90),(78,65),(87,11),(99,10)]$.</p>\n\n<p>Until now, this is what I have been doing:</p>\n\n<ol>\n<li>Sort the original list in ascending order of price;</li>\n<li>Find all subsequences of the sorted list;</li>\n<li>Check whether the subsequence is valid, and compare all valid subsequences.</li>\n</ol>\n\n<p>However, this takes exponential time; how can I solve this problem more efficiently?</p>\n', 'ViewCount': '139', 'Title': 'Find subsequence of maximal length simultaneously satisfying two ordering constraints', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T21:21:42.877', 'LastEditDate': '2012-10-11T21:21:42.877', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1289', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1001', 'Tags': '<algorithms><arrays><constraint-programming><subsequences>', 'CreationDate': '2012-04-15T11:03:26.633', 'Id': '1287'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have an array of integers having length $N$. How can I output all longest decreasing sequences? (A subsequence consists of elements of the array that do not have to be consecustive, for example $(3,2,1)$ is a decreasing subsequence of $(7,3,5,2,0,1)$.) I know how to calculate the length of longest decreasing sequences, but don't know how to report all longest decreasing sequences.</p>\n\n<p>Pseudocode will be helpful.</p>\n", 'ViewCount': '421', 'Title': 'How to output all longest decreasing sequences', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T21:22:02.847', 'LastEditDate': '2012-10-11T21:22:02.847', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '1313', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1001', 'Tags': '<algorithms><arrays><subsequences>', 'CreationDate': '2012-04-15T12:49:34.590', 'Id': '1290'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m looking for a list of informed search algorithms, also known as heuristic search algorithms. </p>\n\n<p>I\'m aware of: </p>\n\n<ol>\n<li><p><a href="http://en.wikipedia.org/wiki/Best-first_search" rel="nofollow">best-first search</a></p>\n\n<ul>\n<li>Greedy best-first search</li>\n<li><a href="http://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* search</a></li>\n</ul></li>\n</ol>\n\n<p>Are there more best-first algorithm or other informed searches that are not best-first?</p>\n', 'ViewCount': '289', 'Title': 'Survey of informed search algorithms?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-09T08:02:07.933', 'LastEditDate': '2012-04-16T20:08:11.290', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '935', 'Tags': '<algorithms><reference-request><artificial-intelligence><search-algorithms>', 'CreationDate': '2012-04-16T09:53:58.100', 'FavoriteCount': '2', 'Id': '1300'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $M$ be a $(0, 1)$ matrix. We say two entries are neighbors if they are adjacent horizontal or vertically, and both entries are $1$'s. One wants to find minimum number of $1$'s to add, so every $1$ can reach another one through a sequence of neighbors. </p>\n\n<p>Example:</p>\n\n<pre><code>100\n000\n001\n</code></pre>\n\n<p>Here we need 3 $1$'s:</p>\n\n<pre><code>100\n100\n111\n</code></pre>\n\n<p>How can we efficiently find the minimum number of $1$'s to add, and where?</p>\n", 'ViewCount': '131', 'Title': "Find minimum number 1's so the matrix consist of 1 connected region of 1's", 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-17T08:18:45.140', 'LastEditDate': '2012-04-16T19:58:48.527', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1303', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms><graph-theory><matrices>', 'CreationDate': '2012-04-16T10:16:09.357', 'Id': '1301'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So I thought this (though somewhat basic) question belonged here:</p>\n\n<p>Say I have a graph of size 100 nodes arrayed in a 10x10 pattern (think chessboard). The graph is undirected, and unweighted. Moving through the graph involves moving three spaces forward and one space to either right or left (similar to how a chess knight moves across a board).</p>\n\n<p>Given a fixed beginning node, how would one find the shortest path to any other node on the board?</p>\n\n<p>I imagined that there would only be an edge between nodes that are viable moves. So, given this information, I would want to find the shortest path from a starting node to an ending node.</p>\n\n<p>My initial thought was that each edge is weighted with weight 1. However, the graph is undirected, so Djikstras would not be an ideal fit. Therefore, I decided to do it using an altered form of a depth first search.</p>\n\n<p>However, I couldn't for the life of me visualize how to get the shortest path using the search.</p>\n\n<p>Another thing I tried was putting the graph in tree form with the starting node as the root, and then selecting the shallowest (lowest row number) result that gave me the desired end node... this worked, but was incredibly inefficient, and thus would not work for a larger graph.</p>\n\n<p>Does anyone have any ideas that might point me in the right direction on this one?</p>\n\n<p>Thank you very much.</p>\n\n<p>(I tried to put in a visualization of the graph, but was unable to due to my low reputation)</p>\n", 'ViewCount': '2595', 'Title': 'Shortest Path on an Undirected Graph?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-01T22:38:48.113', 'LastEditDate': '2012-04-18T05:56:13.197', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '1330', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1132', 'Tags': '<algorithms><graphs><graph-theory><search-algorithms><shortest-path>', 'CreationDate': '2012-04-18T04:23:36.273', 'Id': '1329'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So, I'm trying to conceptualize something:</p>\n\n<p>Say we have a weighed graph of size N. A and B are nodes on the graph. You want to find the shortest path from A to B, given a few caveats:</p>\n\n<ol>\n<li><p>movements on the graph are regulated by a circular cycle of length 48, in such a manner that:</p>\n\n<blockquote>\n  <p>cycle{</p>\n\n<pre><code>     0 &lt;= L &lt;= 24  movement IS possible\n\n    25 &lt;= L &lt;= 48 movement IS NOT possible\n</code></pre>\n  \n  <p>}</p>\n</blockquote>\n\n<p>For simplicity's sake, we will call this cycle 'time'.</p></li>\n<li><p>The distance between nodes A and B is equal to:</p>\n\n<blockquote>\n  <p>shortest_distance(A to B) - 1 OR shortest_distance(A to B) + 1</p>\n</blockquote>\n\n<p>Depending on their orientation</p></li>\n<li><p>the weight of the edges represents the 'time' it takes to travel between nodes.</p></li>\n</ol>\n\n<p>I'd like to create an algorithm that will give me the shortest path with these constraints in mind, assuming one is leaving from node A at time(cycle) = 12, traveling towards node B. The shortest path would be defined as the path which takes the least 'time'.</p>\n\n<p>Step one would obviously be to take into account the orientation affecting the shortest distance (i.e. which way are they oriented by above), which would be a simple addition or substraction to the result of djikstra's algorithm</p>\n\n<p>What I'm having trouble figuring out is how to account for the cycle in the algorithm... could it be as simple as just an if statement checking to see if the current cycle time is within the constraints that allow movement?</p>\n\n<p>Would my idea be viable? If not, does anyone h ave any suggestions at different ways I should look at this problem?</p>\n\n<p>I know this question seems really basic, but I just can't wrap my head around it.</p>\n", 'ViewCount': '155', 'Title': "Modified Djikstra's algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-18T15:00:39.803', 'LastEditDate': '2012-04-18T05:48:56.107', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '1341', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1132', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2012-04-18T05:30:45.100', 'Id': '1332'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1359', 'Title': 'Randomized Selection', 'LastEditDate': '2012-04-18T19:51:56.180', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1134', 'FavoriteCount': '2', 'Body': u'<p>The randomized selection algorithm is the following:</p>\n\n<p>Input: An array $A$ of $n$ (distinct, for simplicity) numbers and a number $k\\in [n]$</p>\n\n<p>Output: The the "rank $k$ element" of $A$ (i.e., the one in position $k$ if $A$ was sorted)</p>\n\n<p>Method:</p>\n\n<ul>\n<li>If there is one element in $A$, return it</li>\n<li>Select an element $p$ (the "pivot") uniformly at random</li>\n<li>Compute the sets $L = \\{a\\in A : a &lt; p\\}$ and $R = \\{a\\in A : a &gt; p\\}$</li>\n<li>If $|L| \\ge k$, return the rank $k$ element of $L$.</li>\n<li>Otherwise, return the rank $k - |L|$ element of $R$</li>\n</ul>\n\n<p>I was asked the following question:</p>\n\n<blockquote>\n  <p>Suppose that $k=n/2$, so you are looking for the median, and let $\\alpha\\in (1/2,1)$\n  be a constant.  What is the probability that, at the first recursive call, the \n  set containing the median has size at most $\\alpha n$?</p>\n</blockquote>\n\n<p>I was told that the answer is $2\\alpha - 1$, with the justification "The pivot selected should lie between $1\u2212\\alpha$ and $\\alpha$ times the original array"</p>\n\n<p>Why? As $\\alpha \\in (0.5, 1)$, whatever element is chosen as pivot is either larger or smaller than more than half the original elements. The median always lies in the larger subarray, because the elements in the partitioned subarray are always less than the pivot. </p>\n\n<p>If the pivot lies in the first half of the original array (less than half of them), the median will surely be in the second larger half, because once the median is found, it must be in the middle position of the array, and everything before the pivot is smaller as stated above. </p>\n\n<p>If the pivot lies in the second half of the original array (more than half of the elements), the median will surely first larger half, for the same reason, everything before the pivot is considered smaller. </p>\n\n<p>Example:</p>\n\n<p>3 4 5 8 7 9 2 1 6 10</p>\n\n<p>The median is 5.</p>\n\n<p>Supposed the chosen pivot is 2. So after the first iteration, it becomes:</p>\n\n<p>1 2 ....bigger part....</p>\n\n<p>Only <code>1</code> and <code>2</code> are swapped after the first iteration. Number 5 (the median) is still in the first greater half (accroding to the pivot 2). The point is, median always lies on greater half, how can it have a chance to stay in a smaller subarray?</p>\n', 'Tags': '<algorithms><algorithm-analysis><probability-theory><randomized-algorithms>', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-18T19:51:56.180', 'CommentCount': '5', 'AcceptedAnswerId': '1343', 'CreationDate': '2012-04-18T08:21:27.190', 'Id': '1334'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm doing some exam (Java-based algorithmics) revision and have been given the question:</p>\n\n<blockquote>\n  <p>Describe how you might extend your implementation [of a queue using a circular array] to support the expansion of the Queue to allow it to store more data items.</p>\n</blockquote>\n\n<p>The Queue started off implemented as an array with a fixed maximum size. I've got two current answers to this, but I'm not sure either are correct:</p>\n\n<ol>\n<li><p>Implement the Queue using the Java Vector class as the underlying array structure. The Vector class is similar to arrays, but a Vector can be resized at any time whereas an array's size is fixed when the array is created.</p></li>\n<li><p>Copy all entries into a larger array.</p></li>\n</ol>\n\n<p>Is there anything obvious I'm missing?</p>\n", 'ViewCount': '898', 'Title': 'Extending the implementation of a Queue using a circular array', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-21T02:26:16.323', 'LastEditDate': '2012-04-18T22:53:03.927', 'AnswerCount': '4', 'CommentCount': '7', 'AcceptedAnswerId': '1361', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1137', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-04-18T10:07:18.590', 'Id': '1336'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The usual simple algorithm for finding the median element in an array $A$ of $n$ numbers is:</p>\n\n<ul>\n<li>Sample $n^{3/4}$ elements from $A$ with replacement into $B$</li>\n<li>Sort $B$ and find the rank $|B|\\pm \\sqrt{n}$ elements $l$ and $r$ of $B$</li>\n<li>Check that $l$ and $r$ are on opposite sides of the median of $A$ and that there are at most $C\\sqrt{n}$ elements in $A$ between $l$ and $r$ for some appropriate constant $C &gt; 0$.  Fail if this doesn\'t happen.</li>\n<li>Otherwise, find the median by sorting the elements of $A$ between $l$ and $r$</li>\n</ul>\n\n<p>It\'s not hard to see that this runs in linear time and that it succeeds with high probability. (All the bad events are large deviations away from the expectation of a binomial.)</p>\n\n<p>An alternate algorithm for the same problem, which is more natural to teach to students who have seen quick sort is the one described here: <a href="http://cs.stackexchange.com/questions/1334/randomized-selection/1343">Randomized Selection</a></p>\n\n<p>It is also easy to see that this one has linear expected running time: say that a "round" is a sequence of recursive calls that ends when one gives a 1/4-3/4 split, and then observe that the expected length of a round is at most 2.  (In the first draw of a round, the probability of getting a good split is 1/2 and then after actually increases, as the algorithm was described so round length is dominated by a  geometric random variable.)</p>\n\n<p>So now the question: </p>\n\n<blockquote>\n  <p>Is it possible to show that randomized selection runs in linear time with high probability?</p>\n</blockquote>\n\n<p>We have $O(\\log n)$ rounds, and each round has length at least $k$ with probability at most $2^{-k+1}$, so a union bound gives that the running time is $O(n\\log\\log n)$ with probability $1-1/O(\\log n)$.</p>\n\n<p>This is kind of unsatisfying, but is it actually the truth?</p>\n', 'ViewCount': '98', 'Title': 'Sharp concentration for selection via random partitioning?', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-19T19:52:47.013', 'LastEditDate': '2012-04-19T19:52:47.013', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1350', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '657', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms>', 'CreationDate': '2012-04-18T20:22:01.597', 'Id': '1346'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for algorithms to optimize a strictly monotonic function $f$ such that $f(x) &lt; y$ </p>\n\n<p>$f : [a,b] \\longrightarrow [c,d]\r\n\\qquad \\text{where } [a,b] \\subset {\\mathbb N},  [c,d] \\subset {\\mathbb N}$<br>\nsuch that $\\arg\\max{_x} f(x) &lt; y$</p>\n\n<p>My first idea was to use a variant of binary search, pick a point $x$ in $[a,b]$ at random; if $f(x) &gt; y$ then we eliminate $[x, b]$, and if $f(x) &lt; y$ we eliminate $[a, x]$. We repeat this procedure until the solution is found.</p>\n\n<p>Do you have any other ideas to maximize the function $f$ ?</p>\n', 'ViewCount': '236', 'Title': 'Optimizing a strictly monotone function', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-29T16:10:36.877', 'LastEditDate': '2012-04-19T21:53:36.550', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '6888', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '652', 'Tags': '<algorithms><optimization>', 'CreationDate': '2012-04-19T10:27:02.390', 'Id': '1353'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have written a program to sort Linked Lists and I noticed that my insertion sort works much better than my quicksort algorithm. \nDoes anyone have any idea why this is?\nInsertion sort has a complexity of $\\Theta(n^2)$ and quicksort $O(n\\log n)$ so therefore quicksort should be faster. I tried for random input size and it shows me the contrary. Strange...</p>\n\n<p>Here the code in Java:</p>\n\n\n\n<pre><code>public static LinkedList qSort(LinkedList list) {\n\n    LinkedList x, y;\n    Node currentNode;\n    int size = list.getSize();\n\n    //Create new lists x smaller equal and y greater\n    x = new LinkedList();\n    y = new LinkedList();\n\n    if (size &lt;= 1)\n        return list;\n    else {\n\n        Node pivot = getPivot(list);\n        // System.out.println("Pivot: " + pivot.value);     \n        //We start from the head\n        currentNode = list.head;\n\n        for (int i = 0; i &lt;= size - 1; i++) {\n            //Check that the currentNode is not our pivot\n            if (currentNode != pivot) {\n                //Nodes with values smaller equal than the pivot goes in x\n                if (currentNode.value &lt;= pivot.value) {\n                    {\n                        x.addNode(currentNode.value);\n                        // System.out.print("Elements in x:");\n                        // x.printList();\n                    }\n\n                } \n                //Nodes with values greater than the pivot goes in y\n                else if (currentNode.value &gt; pivot.value) {\n                    if (currentNode != pivot) {\n                        y.addNode(currentNode.value);\n                        // System.out.print("Elements in y:");\n                        // y.printList();\n                    }\n                }\n            }\n            //Set the pointer to the next node\n            currentNode = currentNode.next;\n        }\n\n        //Recursive calls and concatenation of the Lists and pivot\n        return concatenateList(qSort(x), pivot, qSort(y));\n\n    }\n}\n</code></pre>\n', 'ViewCount': '3274', 'Title': 'Quicksort vs. insertion sort on linked list: performance', 'LastEditorUserId': '1011', 'LastActivityDate': '2013-02-01T13:07:24.713', 'LastEditDate': '2012-04-20T07:08:17.467', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '1386', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><algorithm-analysis><sorting><lists>', 'CreationDate': '2012-04-19T12:05:13.983', 'Id': '1354'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '4509', 'Title': 'Quicksort explained to kids', 'LastEditDate': '2012-04-19T21:11:51.423', 'AnswerCount': '4', 'Score': '10', 'OwnerDisplayName': 'd555', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '3', 'Body': u'<p>Last year, I was reading a fantastic <a href="http://arxiv.org/abs/quant-ph/0510032">paper on \u201cQuantum Mechanics for Kindergarden\u201d</a>. It was not easy paper.</p>\n\n<p>Now, I wonder how to explain quicksort in the simplest words possible. How can I prove (or at least handwave) that the average complexity is $O(n \\log n)$, and what the best and the worst cases are, to a kindergarden class? Or at least in primary school?</p>\n', 'Tags': '<algorithms><education><algorithm-analysis><didactics><sorting>', 'LastEditorUserId': '5', 'LastActivityDate': '2012-04-20T14:53:59.763', 'CommentCount': '8', 'AcceptedAnswerId': '1369', 'CreationDate': '2012-04-19T20:23:14.023', 'Id': '1367'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In many texts a lower bound for finding $k$th smallest element is derived making use of arguments using medians. How can I find one using an adversary argument?</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Selection_algorithm">Wikipedia</a> says that tournament algorithm runs in $O(n+k\\log n)$, and $n - k + \\sum_{j = n+2-k}^{n} \\lceil{\\operatorname{lg}\\, j}\\rceil$ is <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Lower_bounds">given</a> as lower bound.</p>\n', 'ViewCount': '489', 'Title': 'Lower bound for finding kth smallest element using adversary arguments', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-20T17:33:33.450', 'LastEditDate': '2012-04-20T07:47:54.503', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1378', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '947', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2012-04-20T04:11:34.473', 'Id': '1377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '250', 'Title': 'Algorithm to chase a moving target', 'LastEditDate': '2012-04-20T22:53:41.330', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '4', 'Body': "<p>Suppose that we have a black-box $f$ which we can query and reset. When we reset $f$, the state $f_S$ of $f$ is set to an element chosen uniformly at random from the set $$\\{0, 1, ..., n - 1\\}$$ where $n$ is fixed and known for given $f$. To query $f$, an element $x$ (the guess) from $$\\{0, 1, ..., n - 1\\}$$ is provided, and the value returned is $(f_S - x) \\mod n$. Additionally, the state $f_S$ of$f$ is set to a value $f_S&#39; = f_S \\pm k$, where $k$ is selected uniformly at random from $$\\{0, 1, 2, ..., \\lfloor n/2 \\rfloor - ((f_S - x) \\mod n)\\} $$</p>\n\n<p>By making uniformly random guesses with each query, one would expect to have to make $n$ guesses before getting $f_S = x$, with variance $n^2 - n$ (stated without proof).</p>\n\n<p>Can an algorithm be designed to do better (i.e., make fewer guesses, possibly with less variance in the number of guesses)? How much better could it do (i.e., what's an optimal algorithm, and what is its performance)?</p>\n\n<p>An efficient solution to this problem could have important cost-saving implications for shooting at a rabbit (confined to hopping on a circular track) in a dark room.</p>\n", 'Tags': '<algorithms><probability-theory><randomized-algorithms>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-10T13:45:31.237', 'CommentCount': '3', 'AcceptedAnswerId': '1976', 'CreationDate': '2012-04-20T14:48:58.600', 'Id': '1392'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '743', 'Title': 'Rectangle Coverage by Sweep Line', 'LastEditDate': '2012-04-22T11:19:33.100', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1170', 'FavoriteCount': '0', 'Body': '<p>I am given an exercise unfortunately I didn\'t succeed by myself.</p>\n\n<blockquote>\n  <p>There is a set of rectangles $R_{1}..R_{n}$ and a rectangle $R_{0}$. Using plane sweeping algorithm determine if $R_{0}$ is completely covered by the set of $R_{1}..R_{n}$.</p>\n</blockquote>\n\n<p>For more details about the principle of sweep line algorithms see <a href="http://en.wikipedia.org/wiki/Sweep_line_algorithm" rel="nofollow">here</a>.</p>\n\n<p>Let\'s start from the beginning. Initially we know sweep line algorithm as the algorithm for finding <a href="http://en.wikipedia.org/wiki/Line_segment_intersection" rel="nofollow">line segment intersections</a>which requires two data structures:</p>\n\n<ul>\n<li>a set $Q$ of event points (it stores endpoints of segments and intersections points)</li>\n<li>a status $T$ (dynamic structure for the set of segments the sweep line intersecting)</li>\n</ul>\n\n<p><strong>The General Idea:</strong> assume that sweep line $l$ is a vertical line that starts approaching the set of rectangles from the left. Sort all $x$ coordinates of rectangles and store them in $Q$ in increasing order - should take $O(n\\log n)$. Start from the first event point, for every point determine the set of rectangles that intersect at given $x$ coordinate, identify continuous segments of intersection rectangles and check if they cover $R_{0}$ completely at current $x$ coordinate. With $T$ as a binary tree it\'s gonna take $O(\\log n)$. If any part of $R_{0}$ remains uncovered that $R_{0}$ is not completely covered.</p>\n\n<p><strong>Details:</strong> The idea of segment intersection algorithm was that only adjacent segments intersect. Based on this fact we built status $T$ and maintained it throughout the algorithm. I tried to find a similar idea in this case and so far with no success, the only thing I can say is two rectangles intersect if their corresponding $x$ and $y$ coordinates overlap. </p>\n\n<p>The problem is how to build and maintain $T$, and what the complexity of building and maintain $T$ is. I assume that <a href="http://en.wikipedia.org/wiki/R_Trees" rel="nofollow">R trees</a> can be very useful in this case, but as I found it\'s very difficult to determine the minimum bounding rectangle using R trees. </p>\n\n<p>Do you have any idea about how to solve this problem, and particularly how to build $T$?</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T11:19:33.100', 'CommentCount': '4', 'AcceptedAnswerId': '1396', 'CreationDate': '2012-04-20T15:04:57.143', 'Id': '1393'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to use a color camera to track multiple objects in space. Each object will have a different color and in order to be able to distinguish well between each objects I'm trying to make sure that each color assigned to an object is as different from any color on any other object as possible.</p>\n\n<p>In RGB space, we have three planes, all with values between 0 and 255. In this cube $(0,0,0) / (255,255,255)$, I would like to distribute the $n$ colors so that there is as much distance between themselves and others as possible. An additional restriction is that $(0, 0, 0)$ and $(255, 255, 255)$ (or as close to them as possible) should be included in the $n$ colors, because I want to make sure that none of my $(n-2)$ objects takes either color because the background will probably be one of these colors.</p>\n\n<p>Probably, $n$ (including black and while) will not be more than around 14.</p>\n\n<p>Thanks in advance for any pointers on how to get these colors.  </p>\n", 'ViewCount': '253', 'Title': 'Distribute objects in a cube so that they have maximum distance between each other', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T11:55:42.867', 'LastEditDate': '2012-04-22T11:55:42.867', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '1411', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1176', 'Tags': '<algorithms><optimization><computational-geometry>', 'CreationDate': '2012-04-20T20:56:07.157', 'Id': '1399'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3834', 'Title': 'When to use recursion?', 'LastEditDate': '2014-01-13T09:05:58.140', 'AnswerCount': '5', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '1157', 'FavoriteCount': '5', 'Body': '<p>When are some (relatively) basic (think first year college level CS student) instances when one would use recursion instead of just a loop? </p>\n', 'Tags': '<algorithms><recursion>', 'LastEditorUserId': '1157', 'LastActivityDate': '2014-01-13T09:05:58.140', 'CommentCount': '1', 'AcceptedAnswerId': '1435', 'CreationDate': '2012-04-21T20:57:29.993', 'Id': '1418'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '172', 'Title': 'Overflow safe summation', 'LastEditDate': '2012-04-23T22:15:35.557', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '1', 'Body': '<p>Suppose I am given $n$ fixed width integers (i.e. they fit in a register of width $w$), $a_1, a_2, \\dots a_n$ such that their sum $a_1 + a_2 + \\dots + a_n = S$ also fits in a register of width $w$.</p>\n\n<p>It seems to me that we can always permute the numbers to $b_1, b_2, \\dots b_n$ such that each prefix sum $S_i = b_1 + b_2 + \\dots + b_i$ also fits in a register of width $w$.</p>\n\n<p>Basically, the motivation is to compute the sum $S = S_n$ on fixed width register machines without having to worry about integer overflows at any intermediate stage.</p>\n\n<p>Is there a fast (preferably linear time) algorithm to find such a permutation (assuming the $a_i$ are given as an input array)? (or say if such a permutation does not exist).</p>\n', 'Tags': '<algorithms><arrays><integers><numerical-analysis>', 'LastEditorUserId': '139', 'LastActivityDate': '2012-04-23T22:15:35.557', 'CommentCount': '5', 'AcceptedAnswerId': '1425', 'CreationDate': '2012-04-21T23:39:36.593', 'Id': '1424'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Suppose I am given an array of $n$ fixed width integers (i.e. they fit in a register of width $w$), $a_1, a_2, \\dots a_n$. I want to compute the sum $S = a_1 + \\ldots + a_n$ on a machine with 2\'s complement arithmetic, which performs additions modulo $2^w$ with wraparound semantics. That\'s easy \u2014 but the sum may overflow the register size, and if it does, the result will be wrong.</p>\n\n<p>If the sum doesn\'t overflow, I want to compute it, and to verify that there is no overflow, as fast as possible. If the sum overflows, I only want to know that it does, I don\'t care about any value.</p>\n\n<p>Naively adding numbers in order doesn\'t work, because a partial sum may overflow. For example, with 8-bit registers, $(120, 120, -115)$ is valid and has a sum of $125$, even though the partial sum $120+120$ overflows the register range $[-128,127]$.</p>\n\n<p>Obviously I could use a bigger register as an accumulator, but let\'s assume the interesting case where I\'m already using the biggest possible register size.</p>\n\n<p>There is a well-known technique to <a href="http://cs.stackexchange.com/a/1425">add numbers with the opposite sign as the current partial sum</a>. This technique avoids overflows at every step, at the cost of not being cache-friendly and not taking much advantage of branch prediction and speculative execution.</p>\n\n<p>Is there a faster technique that perhaps takes advantage of the permission to overflow partial sums, and is faster on a typical machine with an overflow flag, a cache, a branch predictor and speculative execution and loads?</p>\n\n<p>(This is a follow-up to <a href="http://cs.stackexchange.com/questions/1424/overflow-safe-summation">Overflow safe summation</a>)</p>\n', 'ViewCount': '203', 'Title': 'Detecting overflow in summation', 'LastEditorUserId': '39', 'LastActivityDate': '2014-01-31T16:51:25.577', 'LastEditDate': '2012-04-22T15:03:49.187', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<algorithms><arrays><integers><numerical-analysis>', 'CreationDate': '2012-04-22T01:16:19.560', 'FavoriteCount': '1', 'Id': '1426'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '201', 'Title': 'What is the name of this logistic variant of TSP?', 'LastEditDate': '2012-04-23T14:25:32.307', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '1', 'Body': '<p>I have a logistic problem that can be seen as a variant of $\\text{TSP}$. It is so natural, I\'m sure it has been studied in Operations research or something similar. Here\'s one way of looking at the problem.</p>\n\n<p>I have $P$ warehouses on the Cartesian plane. There\'s a path from a warehouse to every other warehouse and the distance metric used is the Euclidean distance. In addition, there are $n$ different items. Each item $1 \\leq i \\leq n$ can be present in any number of warehouses. We have a collector and we are given a starting point $s$ for it, say the origin $(0,0)$. The collector is given an order, so a list of items. Here, we can assume that the list only contains distinct items and only one of each. We must determine the shortest tour starting at $s$ visiting some number of warehouses so that the we pick up every item on the order.</p>\n\n<p>Here\'s a visualization of a randomly generated instance with $P = 35$. Warehouses are represented with circles. Red ones contain item $1$, blue ones item $2$ and green ones item $3$. Given some starting point $s$ and the order ($1,2,3$), we must pick one red, one blue and one green warehouse so the order can be completed. By accident, there are no multi-colored warehouses in this example so they all contain exactly one item. This particular instance is a case of <a href="http://en.wikipedia.org/wiki/Set_TSP_problem" rel="nofollow">set-TSP</a>.</p>\n\n<p><img src="http://i.stack.imgur.com/5kKsj.png" alt="An instance of the problem."></p>\n\n<p>I can show that the problem is indeed $\\mathcal{NP}$-hard. Consider an instance where each item $i$ is located in a different warehouse $P_i$. The order is such that it contains every item. Now we must visit every warehouse $P_i$ and find the shortest tour doing so. This is equivalent of solving an instance of $\\text{TSP}$.</p>\n\n<p>Being so obviously useful at least in the context of logistic, routing and planning, I\'m sure this has been studied before. I have two questions:</p>\n\n<ol>\n<li>What is the name of the problem?</li>\n<li>How well can one hope to approximate the problem (assuming $\\mathcal{P} \\neq \\mathcal{NP}$)? </li>\n</ol>\n\n<p>I\'m quite happy with the name and/or reference(s) to the problem. Maybe the answer to the second point follows easily or I can find out that myself.</p>\n', 'Tags': '<algorithms><optimization><reference-request><approximation>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-23T18:22:16.660', 'CommentCount': '4', 'AcceptedAnswerId': '1464', 'CreationDate': '2012-04-22T15:35:56.930', 'Id': '1440'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3560', 'Title': 'What is most efficient for GCD?', 'LastEditDate': '2012-12-01T00:11:19.023', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '1', 'Body': "<p>I know that Euclid's algorithm is the best algorithm for get the GCD (great common divisor) for a list the positive integer numbers. \nBut, in the practice, you can write two codes por evaluate the gcd (for my case, i decided use java, but c/c++ may be another option).</p>\n\n<p>I need to get the most efficient code of two possibilities form to programming. </p>\n\n<p>Recursive Mode, you can write...</p>\n\n<pre><code>static long gcd(long a, long b){\n    a = Math.abs(a); b = Math.abs(b);\n    return (a==0)?b:gcd(b, a%b);\n  }\n</code></pre>\n\n<p>And, iterative mode, looks like ...</p>\n\n<pre><code>static long gcd(long a, long b) {\n  long r, i;\n  while(b!=0){\n    r = a % b;\n    a = b;\n    b = r;\n  }\n  return a;\n}\n</code></pre>\n\n<p>Regards, </p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>We can do that with the Binary GCD, and the easy code is like that</p>\n\n<pre><code>int gcd(int a, int b)\n{\n    while(b) b ^= a ^= b ^= a %= b;\n    return a;\n}\n</code></pre>\n\n<p>Great Discussion.</p>\n", 'Tags': '<algorithms><efficiency>', 'LastEditorUserId': '1152', 'LastActivityDate': '2013-12-31T12:09:28.093', 'CommentCount': '7', 'AcceptedAnswerId': '1449', 'CreationDate': '2012-04-22T18:18:25.867', 'Id': '1447'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '660', 'Title': 'How to use adversary arguments for selection and insertion sort?', 'LastEditDate': '2012-04-23T06:41:39.370', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '947', 'FavoriteCount': '1', 'Body': '<p>I was asked to find the adversary arguments necessary for finding the lower bounds for selection and insertion sort. I could not find a reference to it anywhere.</p>\n\n<p>I have some doubts regarding this. I understand that adversary arguments are usually used for finding lower bounds for certain "problems" rather than "algorithms".</p>\n\n<p>I understand the merging problem. But how could I write one for selection and insertion sort?</p>\n', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-23T17:27:14.653', 'CommentCount': '4', 'AcceptedAnswerId': '1465', 'CreationDate': '2012-04-23T03:40:40.260', 'Id': '1455'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there any algorithm that works better than $\\Theta(n^2)$ to verify whether a square matrix is a magic one? (E.g. such as sum of all the rows, cols and diagonally are equal to each other). \nI did see someone mention a $O(n)$ time on a website a few days ago but could not figure out how.</p>\n', 'ViewCount': '380', 'Title': 'Magic Square Check for NxN Matrix - with Minimum Complexity?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-23T17:16:30.050', 'LastEditDate': '2012-04-23T17:15:09.943', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '852', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2012-04-23T09:51:52.267', 'Id': '1460'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2524', 'Title': 'Circle Intersection with Sweep Line Algorithm', 'LastEditDate': '2012-04-23T19:38:51.163', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1170', 'FavoriteCount': '1', 'Body': u'<p>Unfortunately I am still not so strong in understanding <a href="http://en.wikipedia.org/wiki/Sweep_line_algorithm">Sweep Line Algorithm</a>. All papers and textbooks on the topic are already read, however understanding is still far away. Just in order to make it clearer I try to solve as many exercises as I can. But, really interesting and important tasks are still a challenge for me.</p>\n\n<p>The following exercise I found in lecture notes of <a href="http://theory.cs.uiuc.edu/~jeffe/teaching/algorithms/notes/xo-sweepline.pdf">Line Segment Intersection</a> by omnipotent Jeff Erickson.</p>\n\n<blockquote>\n  <p><strong>Exercise 2.</strong> Describe and analyze a sweepline algorithm to determine, given $n$ circles in the plane, whether any two intersect, in $O(n \\log n)$ time. Each circle is speci\ufb01ed by its center and its radius, so the input consists of three arrays $X[1.. n], Y [1.. n]$, and $R[1.. n]$. Be careful to correctly implement the low-level primitives.</p>\n</blockquote>\n\n<p>Let\'s try to make a complex thing easier. What do we know about intersection of circles? What analogue can be found with intersection of lines. Two lines might intersect if they adjacent, which property two circle should have in order to intersect? Let $d$ be the distance between the center of the circles, $r_{0}$ and $r_{1}$ centers of the circles. Consider few cases:</p>\n\n<ul>\n<li><p>Case 1: If $d &gt; r_{0} + r_{1}$ then there are no solutions, the circles are separate.</p></li>\n<li><p>Case 2: If $d &lt; |r_{0} - r_{1}|$ then there are no solutions because one circle is contained within the other.</p></li>\n<li><p>Case 3: If $d = 0$ and $r_{0} = r_{1}$ then the circles are coincident and there are an infinite number of solutions.</p></li>\n</ul>\n\n<p>So, it looks like conditions of intersection are ready, of course it may be wrong conditions. Please correct if it\'s so.</p>\n\n<p><strong>Algorithm.</strong> Now we need to find something in common between two intersecting circles. With analogue to line intersection, we need to have insert condition and delete condition to event queue. Let\'s say event point are x coordinate of the first and the last points which vertical sweep line touches. On the first point we insert circle to <em>status</em>\n and check for intersection (3 cases for checking are mentioned above) with nearest circles, on the last point we delete circle from <em>status</em>.</p>\n\n<p>It looks like is enough for sweep line algorithm. If there is something wrong, or may be there is something what should be done different, feel free to share your thoughts with us.</p>\n\n<p><strong>Addendum</strong>:</p>\n\n<p>I insert a circle when vertical sweep line touches the circle for the first time, and remove a circle from the status when sweep line touches it for the last time. The check for intersection should be done for the nearest previous circle. If we added a circle to <em>status</em> and there was already  another circle which we added before and it was still there, therefore the pervious circle was not "closed", so there might be an intersection.</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '1170', 'LastActivityDate': '2013-02-27T20:58:21.923', 'CommentCount': '7', 'AcceptedAnswerId': '1468', 'CreationDate': '2012-04-23T18:50:58.747', 'Id': '1466'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '101', 'Title': 'Looking for a ranking algorithm that favors newer entries', 'LastEditDate': '2012-04-23T22:18:33.990', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '339', 'FavoriteCount': '2', 'Body': "<p>I'm working on a ranking system that will rank entries based on votes that have been cast over a period of time.  I'm looking for an algorithm that will calculate a score which is kinda like an average, however I would like it to favor newer scores over older ones.  I was thinking of something along the line of: </p>\n\n<p>$$\\frac{\\mathrm{score}_1 +\\ 2\\cdot \\mathrm{score}_2\\ +\\ \\dots +\\ n\\cdot \\mathrm{score}_n}{1 + 2 + \\dots + n}$$</p>\n\n<p>I was wondering if there were other algorithms which are usually used for situations like this and if so, could you please explain them?</p>\n", 'Tags': '<algorithms><data-mining>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-24T05:59:38.350', 'CommentCount': '3', 'AcceptedAnswerId': '1472', 'CreationDate': '2012-04-23T19:52:08.590', 'Id': '1471'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '665', 'Title': 'Dealing with intractability: NP-complete problems', 'LastEditDate': '2013-06-06T14:11:05.583', 'AnswerCount': '6', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '1219', 'FavoriteCount': '8', 'Body': '<p>Assume that I am a programmer and I have an NP-complete problem that I need to solve it. What methods are available to deal with NPC problems? Is there a survey or something similar on this topic?</p>\n', 'Tags': '<algorithms><reference-request><np-complete><efficiency><reference-question>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-06T14:11:05.583', 'CommentCount': '4', 'AcceptedAnswerId': '1481', 'CreationDate': '2012-04-24T03:28:23.417', 'Id': '1477'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know that the 2D and 3D Knapsack problems are NPC, but is there any way to solve them in reasonable time if the instances are not very complicated? Would dynamic programming work?</p>\n\n<p>By 2D (3D) Knapsack I mean I have a square (cube) and a I have list of objects, all data are in centimeters and are at most 20m.</p>\n', 'ViewCount': '1006', 'Title': 'Algorithms for two and three dimensional Knapsack', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-29T10:36:33.367', 'LastEditDate': '2012-04-24T05:50:51.137', 'AnswerCount': '2', 'CommentCount': '9', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1219', 'Tags': '<algorithms><complexity-theory><np-complete><computational-geometry><knapsack-problems>', 'CreationDate': '2012-04-24T03:35:25.657', 'FavoriteCount': '1', 'Id': '1478'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '702', 'Title': 'Complexity of finding the largest $m$ numbers in an array of size $n$', 'LastEditDate': '2012-04-24T20:40:13.420', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '1', 'Body': "<p>What follows is my algorithm for doing this in what I believe to be $O(n)$ time, and my proof for that. My professor disagrees that it runs in $O(n)$ and instead thinks that it runs in $\\Omega(n^2)$ time. Any comments regarding the proof itself, or the style (i.e. my ideas may be clear but the presentation not).</p>\n\n<p>The original question:</p>\n\n<blockquote>\n  <p>Given $n$ numbers, find the largest $m \\leq n$ among them in time $o(n \\log n)$. You may not assume anything else about $m$.</p>\n</blockquote>\n\n<p>My answer:</p>\n\n<ol>\n<li>Sort the first $m$ elements of the array. This takes $O(1)$ time, as this is totally dependent on $m$, not $n$.</li>\n<li>Store them in a linked list (maintaining the sorted order). This also takes $O(1)$ time, for the same reason as above.</li>\n<li>For every other element in the array, test if it is greater than the least element of the linked list. This takes $O(n)$ time as $n$ comparisons must be done.</li>\n<li>If the number is in fact greater, then delete the first element of the linked list (the lowest one) and insert the new number in the location that would keep the list in sorted order. This takes $O(1)$ time because it is bounded by a constant ($m$) above as the list does not grow.</li>\n<li>Therefore, the total complexity for the algorithm is $O(n)$.</li>\n</ol>\n\n<p>I am aware that using a red-black tree as opposed to linked list is more efficient in constant terms (as the constant upper bound is $O(m\\cdot \\log_2(m))$ as opposed to $m$ and the problem of keeping a pointer to the lowest element of the tree (to facilitate the comparisons) is eminently doable, it just didn't occur to me at the time.</p>\n\n<p>What is my proof missing? Is there a more standard way of presenting it (even if it is incorrect)?</p>\n", 'Tags': '<algorithms><time-complexity><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-24T03:35:40.420', 'CommentCount': '9', 'AcceptedAnswerId': '1489', 'CreationDate': '2012-04-24T18:03:35.983', 'Id': '1485'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a <a href="https://www.iis.se/docs/DNS-bok-sid-14.jpg">tree</a> ( in graph theory sense) such as this:</p>\n\n<p><img src="http://i.stack.imgur.com/sK90D.jpg" alt="enter image description here"></p>\n\n<p>This is a directed tree with one starting node and many ending nodes. Each of the edge has a length assigned to it.</p>\n\n<p>My question is, how to find the longest path connecting from the starting node to any of the ending node? The brute force approach is to check all the root-leaf paths and taking the one with maximal length, but I would prefer a more efficient algorithm if there is one. </p>\n', 'ViewCount': '4475', 'Title': 'Find the longest path in a tree', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-25T07:50:26.063', 'LastEditDate': '2012-04-25T07:48:32.063', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '1497', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '304', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-04-25T03:22:55.250', 'Id': '1494'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Do you know any algorithm that calculates the factorial after modulus efficiently?</p>\n\n<p>For example, I want to program:</p>\n\n<pre><code>for(i=0; i&lt;5; i++)\n  sum += factorial(p-i) % p;\n</code></pre>\n\n<p>But, <code>p</code> is a big number (prime) for applying factorial directly $(p \\leq 10^ 8)$.</p>\n\n<p>In Python, this task is really easy, but i really want to know how to optimize.</p>\n', 'ViewCount': '2032', 'Title': 'What is the most efficient way to compute factorials modulo a prime?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-15T22:02:22.787', 'LastEditDate': '2012-10-06T22:43:33.660', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><efficiency><integers>', 'CreationDate': '2012-04-25T03:24:45.137', 'FavoriteCount': '6', 'Id': '1495'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a system of linear equations $Ax=0$, where $A$ is a $n\\times n$ matrix with rational entries. Assume that the rank of $A$ is $&lt;n$. What is the complexiy to check\nwhether it has a solution $x$ such that all entries of $x$ are stricly greater than 0 (namely, $x$ is a positive vector)? Of course, one can use Gauss elimination, but this seems not to be optimal.</p>\n', 'ViewCount': '573', 'Title': 'Complexity of checking whether linear equations have a positive solution', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-28T10:35:29.087', 'LastEditDate': '2012-04-26T00:21:41.243', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1236', 'Tags': '<algorithms><complexity-theory><linear-algebra>', 'CreationDate': '2012-04-25T12:24:02.827', 'FavoriteCount': '1', 'Id': '1500'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My goal is to solve the following problem, which I have described by its input and output:</p>\n\n<p><strong>Input:</strong></p>\n\n<p>A directed acyclic graph $G$ with $m$ nodes, $n$ sources, and $1$ sink ($m > n \\geq 1$).</p>\n\n<p><strong>Output:</strong></p>\n\n<p>The <a href="https://en.wikipedia.org/wiki/Vc_dimension">VC-dimension</a> (or an approximation of it) for the neural network with topology $G$.</p>\n\n<p><strong>More specifics</strong>: </p>\n\n<ul>\n<li>Each node in $G$ is a sigmoid neuron. The topology is fixed, but the weights on the edges can be varied by the learning algorithm.</li>\n<li>The learning algorithm is fixed (say backward-propagation).</li>\n<li>The $n$ source nodes are the input neurons and can only take strings from $\\{-1,1\\}^n$ as input.</li>\n<li>The sink node is the output unit. It outputs a real value from $[-1,1]$ that we round up to $1$ or down to $-1$ if it is more than a certain fixed threshold $\\delta$ away from $0$. </li>\n</ul>\n\n<p>The naive approach is simply to try to break more and more points, by attempting to train the network on them. However, this sort of simulation approach is not efficient.</p>\n\n<hr>\n\n<h3>Question</h3>\n\n<p>Is there an efficient way (i.e. in $\\mathsf{P}$ when changed to the decision-problem: is VC-dimension less than input parameter $k$?) to compute this function? If not, are there hardness results?</p>\n\n<p>Is there a works-well-in-practice way to compute or approximate this function? If it is an approximation, are there any guarantees on its accuracy?</p>\n\n<h3>Notes</h3>\n\n<p>I asked a <a href="http://stats.stackexchange.com/q/25952/4872">similar question</a> on stats.SE but it generated no interest.</p>\n', 'ViewCount': '204', 'Title': 'Efficiently computing or approximating the VC-dimension of a neural network', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T02:12:14.187', 'LastEditDate': '2012-04-25T16:58:07.127', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><complexity-theory><machine-learning><neural-networks><vc-dimension>', 'CreationDate': '2012-04-25T15:21:46.690', 'FavoriteCount': '1', 'Id': '1504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '263', 'Title': 'Runtime of the optimal greedy $2$-approximation algorithm for the $k$-clustering problem', 'CommunityOwnedDate': '2012-05-23T23:00:33.923', 'LastEditDate': '2012-05-23T23:00:33.923', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '1', 'Body': '<p>We are given a set 2-dimensional points $|P| = n$ and an integer $k$. We must find a collection of $k$ circles that enclose all the $n$ points such that the radius of the largest circle is as large as possible. In other words, we must find a set $C = \\{ c_1,c_2,\\ldots,c_k\\}$ of $k$ center points such that the cost function $\\text{cost}(C) = \\max_i \\min_j D(p_i, c_j)$ is minimized. Here, $D$ denotes the Euclidean distance between an input point $p_i$ and a center point $c_j$. Each point assigns itself to the closest cluster center grouping the vertices into $k$ different clusters.</p>\n\n<p>The problem is known as the (discrete) $k$-clustering problem and it is $\\text{NP}$-hard. It can be shown with a reduction from the $\\text{NP}$-complete dominating set problem that if there exists a $\\rho$-approximation algorithm for the problem with $\\rho &lt; 2$ then $\\text{P} = \\text{NP}$. </p>\n\n<p>The optimal $2$-approximation algorithm is very simple and intuitive. One first picks a point $p \\in P$ arbitrarily and puts it in the set $C$ of cluster centers. Then one picks the next cluster center such that is as far away as possible from all the other cluster centers. So while $|C| &lt; k$, we repeatedly find a point $j \\in P$ for which the distance $D(j,C)$ is maximized and add it to $C$. Once $|C| = k$ we are done.</p>\n\n<p>It is not hard to see that the optimal greedy algorithm runs in $O(nk)$ time. This raises a question: can we achieve $o(nk)$ time? How much better can we do?</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-05-23T23:05:25.433', 'CommentCount': '0', 'AcceptedAnswerId': '1675', 'CreationDate': '2012-04-25T19:34:33.163', 'Id': '1507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have two problems related to paths in a directed graph. Let $G=(V,E)$ be a directed graph with source $s \\in V$ and target $t \\in V$. Let $v \\in V \\setminus \\{s,t\\}$ be another vertex in $G$. </p>\n\n<ol>\n<li><p>Find a simple directed path\xb9 from $s$ to $t$ through $v$. </p></li>\n<li><p>Find a simple directed path from $s$ to $t$ that goes through two fixed edges in $G$.</p></li>\n</ol>\n\n<p>I do not know if there are polynomial time algorithms for them. Does anyone have solutions or references for them?</p>\n\n<hr>\n\n<ol>\n<li>A simple directed path does not allow any vertex to appear more than once. </li>\n</ol>\n', 'ViewCount': '193', 'Title': 'Simple paths with halt in between in directed graphs', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T13:30:06.013', 'LastEditDate': '2012-04-26T21:08:51.690', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '7', 'OwnerDisplayName': 'Bin Fu', 'PostTypeId': '1', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-04-26T18:01:42.210', 'FavoriteCount': '1', 'Id': '1516'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I asked this <a href="http://stackoverflow.com/questions/10326446/how-to-approach-dynamic-graph-related-problems">question</a> at generic stackoverflow and I was directed here.</p>\n\n<p>It will be great if some one can explain how to approach partial or fully dynamic graph problems in general.</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Find Shortest Path between two vertices $(u,v)$ in a undirected weighted graph for $n$ instances, when an edge is removed at each instance.</li>\n<li>Find number of connected components in an undirected graph for n instances when an edge is remove at each instance, etc.</li>\n</ul>\n\n<p>I recently encountered this genre of problems in a programming contest. I searched through the web and I found lot of research papers concerning with dynamic graphs [1,2]. I read couple of them and and I couldnt find anything straight forward (clustering, sparsification etc). Sorry for being vague.</p>\n\n<p>I really appreciate if some can provide pointers to understand these concepts better.</p>\n\n<hr>\n\n<ol>\n<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8372"><em>Dynamic Graph Algorithms</em></a> by D. Eppstein , Z. Galil , G. F. Italiano (1999)</li>\n<li><a href="http://www.lix.polytechnique.fr/~liberti/sppsurvey.pdf"><em>Shortest paths on dynamic graphs</em></a> by G. Nannicini, L. Liberti (2008)</li>\n</ol>\n', 'ViewCount': '453', 'Title': 'How to approach Dynamic graph related problems', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-23T15:24:36.313', 'LastEditDate': '2012-04-27T07:08:12.330', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1262', 'Tags': '<algorithms><data-structures><graphs>', 'CreationDate': '2012-04-27T01:05:51.733', 'FavoriteCount': '1', 'Id': '1521'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m working on problem H in the <a href="http://neerc.ifmo.ru/past/2004/problems/problems.pdf" rel="nofollow">ACM ICPC 2004\u20132005 Northeastern European contest</a>.</p>\n\n<p>The problem is basically to find the worst case that produces a maximal number of exchanges in the algorithm (sift down) to build the heap.</p>\n\n<ul>\n<li>Input: Input \ufb01le contains $n$ ($1 \\le n \\le 50{,}000$).</li>\n<li>Output:  Output the array containing $n$ different integer numbers from $1$ to $n$, such that it is a heap, and when converting it to a sorted array, the total number of exchanges in sifting operations is maximal possible.</li>\n</ul>\n\n<p>Sample input: <code>6</code><br>\nCorresponding output: <code>6 5 3 2 4 1</code></p>\n\n<p>And the basics outputs:</p>\n\n<pre><code>[2, 1]   \n[3, 2, 1]   \n[4, 3, 1, 2] \n[5, 4, 3, 2, 1] \n[6, 5, 3, 4, 1, 2]\n</code></pre>\n', 'ViewCount': '2275', 'Title': 'Finding a worst case of heap sort', 'LastEditorUserId': '1152', 'LastActivityDate': '2013-10-28T16:39:52.673', 'LastEditDate': '2012-04-28T13:55:03.753', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><data-structures><algorithm-analysis><sorting>', 'CreationDate': '2012-04-27T21:28:44.810', 'FavoriteCount': '1', 'Id': '1540'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '299', 'Title': 'Approximation algorithm for TSP variant, fixed start and end anywhere but starting point + multiple visits at each vertex ALLOWED', 'LastEditDate': '2012-04-28T09:30:21.653', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1282', 'FavoriteCount': '1', 'Body': '<p>NOTE: Due to the fact that the trip does not end at the same place it started and also the fact that every point can be visited more than once as long as I still visit all of them, this is not really a TSP variant, but I put it due to lack of a better definition of the problem.</p>\n\n<p>This problem was originally posted on StackOverflow, but I was told that this would be a better place. I got one pointer, which converted the problem from non-metric to a metric one.</p>\n\n<p>So..</p>\n\n<p>Suppose I am going on a hiking trip with n points of interest. These points are all connected by hiking trails. I have a map showing all trails with their distances, giving me a directed graph.</p>\n\n<p>My problem is how to approximate a tour that starts at a point A and visits all n points of interest, while ending the tour anywhere but the point where I started and I want the tour to be as short as possible.</p>\n\n<p>Due to the nature of hiking, I figured this would sadly not be a symmetric problem (or can I convert my asymmetric graph to a symmetric one?), since going from high to low altitude is obviously easier than the other way around.</p>\n\n<p>Since there are no restrictions regarding how many times I visit each point, as long as I visit all of them, it does not matter if the shortest path from a to d goes through b and c. Is this enough to say that triangle inequality holds and thus I have a metric problem?</p>\n\n<p>I believe my problem is easier than TSP, so those algorithms do not fit this problem. I thought about using a minimum spanning tree, but I have a hard time applying it to this problem, which under the circumstances, should be a metric asymmetric directed graph?</p>\n\n<p>What I really want are some pointers as to how I can come up with an approximation algorithm that will find a near optimal tour through all n points</p>\n', 'Tags': '<algorithms><complexity-theory><graphs><graph-theory><approximation>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-28T21:59:47.610', 'CommentCount': '5', 'AcceptedAnswerId': '1551', 'CreationDate': '2012-04-28T07:45:11.773', 'Id': '1542'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Are there techniques and/or software tools that can be used to\n  construct hard instances of a simple puzzle game (or a simple planning\n  problem)?</p>\n</blockquote>\n\n<p>With "hard" I mean that any solution of the instance is "long" with respect to the input size.</p>\n\n<p>What I have in mind:</p>\n\n<ul>\n<li>model the puzzle game using a <a href="http://en.wikipedia.org/wiki/Constraint_programming" rel="nofollow">constraint programming</a> language (or even <a href="http://en.wikipedia.org/wiki/STRIPS" rel="nofollow">STRIPS</a>);</li>\n<li>the tool starts with assigning some random values to the model parameters to construct an instance;</li>\n<li>solve the instance and if solutions are "easy" (shorter than a fixed length) or no solution is found in a specified amount of time, try to adjust it using some heuristics (or other techniques such as GA or simulated annealing).</li>\n</ul>\n', 'ViewCount': '105', 'Title': 'Techniques/tools for constructing hard instances of a puzzle game', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-29T16:47:56.540', 'LastEditDate': '2012-04-28T22:27:36.283', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '140', 'Tags': '<algorithms>', 'CreationDate': '2012-04-28T10:46:46.763', 'Id': '1548'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have developed two algorithms and now they are asking me to find their running time.\nThe problem is to develop a singly linked list version for manipulating polynomials. The two main operations are <em>addition</em> and <em>multiplication</em>.</p>\n\n<p>In general for lists the running for these two operations are ($x,y$ are the lists lengths):</p>\n\n<ul>\n<li>Addition: Time $O(x+y)$, space $O(x+y)$</li>\n<li>Multiplication: Time $O(xy \\log(xy))$, space $O(xy)$</li>\n</ul>\n\n<p>Can someone help me to find the running times of my algorithms?\nI think for the first algorithm it is like stated above $O(x+y)$, for the second one I have two nested loops and two lists so it should be $O(xy)$, but why the $O(xy \\log(xy))$ above?</p>\n\n<p>These are the algorithms I developed (in Pseudocode):</p>\n\n<pre><code> PolynomialAdd(Poly1, Poly2):\n Degree := MaxDegree(Poly1.head, Poly2.head);\n while (Degree &gt;=0) do:\n      Node1 := Poly1.head;\n      while (Node1 IS NOT NIL) do:\n         if(Node1.Deg = Degree) then break;\n         else Node1 = Node1.next;\n      Node2 := Poly2.head;\n      while (Node2 IS NOT NIL) do:\n         if(Node2.Deg = Degree) then break;\n         else Node2 = Node2.next;\n      if (Node1 IS NOT NIL AND Node2 IS NOT NIL) then\n         PolyResult.insertTerm( Node1.Coeff + Node2.Coeff, Node1.Deg);\n      else if (Node1 IS NOT NIL) then\n         PolyResult.insertTerm(Node1.Coeff, Node1.Deg);\n      else if (Node2 IS NOT NIL) then\n         PolyResult.insertTerm(Node2.Coeff, Node2.Deg);\n      Degree := Degree \u2013 1;\n return PolyResult; \n\n PolynomialMul(Poly1, Poly2): \n Node1 := Poly1.head;\n while (Node1 IS NOT NIL) do:\n      Node2 = Poly2.head;\n      while (Node2 IS NOT NIL) do:\n           PolyResult.insertTerm(Node1.Coeff * Node2.Coeff, \n                              Node1.Deg + Node1.Deg);\n           Node2 = Node2.next;                 \n      Node1 = Node1.next;\n return PolyResult;\n</code></pre>\n\n<p><code>InsertTerm</code> inserts the term in the correct place depending on the degree of the term. </p>\n\n<pre><code> InsertTerm(Coeff, Deg):\n NewNode.Coeff := Coeff;\n NewNode.Deg := Deg;\n if List.head = NIL then\n    List.head := NewNode;\n else if NewNode.Deg &gt; List.head.Deg then\n    NewNode.next := List.head;\n    List.head := NewNode;\n else if NewNode.Deg = List.head.Deg then \n    AddCoeff(NewNode, List.head);\n else\n    Go through the List till find the same Degree and summing up the coefficient OR\n    adding a new Term in the right position if Degree not present;\n</code></pre>\n', 'ViewCount': '1224', 'Title': 'Running time - Linked Lists Polynomial', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-09T09:08:38.773', 'LastEditDate': '2012-05-09T09:04:04.817', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '1584', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-04-29T10:01:54.643', 'Id': '1567'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It\'s well known that <a href="http://en.wikipedia.org/wiki/Monotone_polygon">Monotone polygon</a> plays a crucial role in <a href="http://en.wikipedia.org/wiki/Polygon_triangulation">Polygon triangulation</a>. </p>\n\n<blockquote>\n  <p><strong>Definiton:</strong> monotone polygon - a polygon $P$ in the plane is called monotone with respect to a straight line $L$, if every line orthogonal to $L$ intersects $P$ at most twice.</p>\n</blockquote>\n\n<p>I am interested in building an algorithm for testing any given polygon for monotonicity.</p>\n\n<p>In my opinion, we should consider every vertex with inner angle $&gt;180^{\\circ}$, because a perpendicular from $L$ might intersect only adjacent edges of reflex vertex ($>180^{\\circ}$). In addition, slope of $L$ also should be taken into account, and should be between the slopes of adjacent edges of reflex vertex. </p>\n\n<p>It looks like the above theory should be enough for constructing an algorithm. </p>\n\n<p>What\'s your opinion? How to test polygon for monotonicity?</p>\n', 'ViewCount': '168', 'Title': 'Testing Polygon for Monotonicity', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-30T12:47:55.563', 'LastEditDate': '2012-04-29T18:29:49.203', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1588', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1170', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-04-29T18:01:16.830', 'Id': '1577'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m trying to find the best algorithm for converting an \u201cordinary\u201d linked list into an \u201cideal" skip list. </p>\n\n<p>The definition of an \u201cideal skip list\u201d is that in the first level we\'ll have all the elements, half of them in the next level, a quarter of them in the level after that, and so on.</p>\n\n<p>I\'m thinking about a $\\mathcal{O}(n)$ run-time algorithm involving throwing a coin for each node in the original linked-list, to determine for any given node whether it should be placed in a higher or lower level, and create a duplicate node for the current node at a higher level. This algorithm should work in $\\mathcal{O}(n)$; is there any better algorithm? </p>\n', 'ViewCount': '302', 'Title': 'Building ideal skip lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-16T14:45:04.503', 'LastEditDate': '2012-05-09T10:47:32.457', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '776', 'Tags': '<algorithms><data-structures><randomized-algorithms><lists>', 'CreationDate': '2012-04-30T13:03:01.867', 'FavoriteCount': '1', 'Id': '1589'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to search for an algorithm that can tell me which node has the highest download (or upload) capacity given a weighted directed graph, where weights correspond to individual link bandwidths. I have looked at the maximal flow problem and at the Edmond-Karp algorithm. My questions are the following: </p>\n\n<ol>\n<li>Edmond-Karp just tells us how much throughput we can get (at the sink) from source to sink if any of the paths were used. Correct?</li>\n<li>Edmond-Karp does not tell us which path can give us the maximum flow. Correct?</li>\n</ol>\n', 'ViewCount': '473', 'Title': 'Finding the maximum bandwidth along a single path in a network', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T15:52:42.887', 'LastEditDate': '2012-06-10T11:42:16.480', 'AnswerCount': '2', 'CommentCount': '8', 'AcceptedAnswerId': '1639', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1289', 'Tags': '<algorithms><graphs><network-flow>', 'CreationDate': '2012-04-30T14:17:06.293', 'Id': '1591'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A <a href="http://stackoverflow.com/questions/10378738/maximise-sum-of-non-overlapping-numbers-from-matrix">question was posted on Stack Overflow</a> asking for an algorithm to solve this problem:</p>\n\n<blockquote>\n  <p>I have a matrix (call it A) which is nxn. I wish to select a subset\n  (call it B) of points from matrix A. The subset will consist of n\n  elements, where one and only one element is taken from each row and\n  from each column of A. The output should provide a solution (B) such\n  that the sum of the elements that make up B is the maximum possible\n  value, given these constraints (eg. 25 in the example below). If\n  multiple instances of B are found (ie. different solutions which give\n  the same maximum sum) the solution for B which has the largest minimum\n  element should be selected.</p>\n  \n  <p>B could also be a selection matrix which is nxn, but where only the n\n  desired elements are non-zero.</p>\n  \n  <p>For example: if A =</p>\n\n<pre><code>|5 4 3 2 1|\n|4 3 2 1 5|\n|3 2 1 5 4|\n|2 1 5 4 3|\n|1 5 4 3 2|\n</code></pre>\n  \n  <p>=> B would be</p>\n\n<pre><code> |5 5 5 5 5|\n</code></pre>\n</blockquote>\n\n<p>I <a href="http://stackoverflow.com/a/10387455/1191425">proposed a dynamic programming solution</a> which I suspect is as efficient as any solution is going to get. I\'ve copy-pasted my proposed algorithm below.</p>\n\n<hr>\n\n<ul>\n<li>Let $A$ be a square array of $n$ by $n$ numbers.</li>\n<li>Let $A_{i,j}$ denote the element of $A$ in the <code>i</code>th row and <code>j</code>th column.</li>\n<li>Let $S( i_1:i_2, j_1:j_2 )$ denote the optimal sum of non-overlapping numbers for a square subarray of $A$ containing the intersection of rows $i_1$ to $i_2$ and columns $j_1$ to $j_2$.</li>\n</ul>\n\n<p>Then the optimal sum of non-overlapping numbers is denoted <code>S( 1:n , 1:n )</code> and is given as follows:</p>\n\n<p>$$S( 1:n , 1:n ) = \\max \\left \\{ \\begin{array}{l}  S(   2:n , 2:n   ) + A_{1,1} \\\\\r\n                             S(   2:n , 1:n-1 ) + A_{1,n} \\\\\r\n                            S( 1:n-1 , 2:n   ) + A_{n,1} \\\\\r\n                            S( 1:n-1 , 1:n-1 ) + A_{n,n} \\\\\r\n                            \\end{array} \\right.$$</p>\n\n<pre><code>Note that S( i:i, j:j ) is simply Aij.\n</code></pre>\n\n<p>That is, the optimal sum for a square array of size <code>n</code> can be determined by separately computing the optimal sum for each of the four sub-arrays of size <code>n-1</code>, and then maximising the sum of the sub-array and the element that was "left out".</p>\n\n<pre><code>S for |# # # #|\n      |# # # #|\n      |# # # #|\n      |# # # #|\n\nIs the best of the sums S for:\n\n|#      |      |      #|      |# # #  |       |  # # #|\n|  # # #|      |# # #  |      |# # #  |       |  # # #|\n|  # # #|      |# # #  |      |# # #  |       |  # # #|\n|  # # #|      |# # #  |      |      #|       |#      |\n</code></pre>\n\n<hr>\n\n<p>This is a very elegant algorithm and I strongly suspect that it is correct, but I can\'t come up with a way to <strong>prove</strong> it is correct.</p>\n\n<p>The main difficulty I am having it proving that the problem displays optimal substructure. I believe that if the four potential choices in each calculation are the <em>only</em> four choices, then this is enough to show optimal substructure. That is, I need to prove that this:</p>\n\n<pre><code>|   #    |\n| #   # #|\n| #   # #| \n| #   # #|\n</code></pre>\n\n<p>Is not a valid solution, either because it\'s impossible (i.e. proof by contradiction) or because this possibility is already accounted for by one of the four "<code>n-1</code> square" variations.</p>\n\n<p>Can anyone point out any flaws in my algorithm, or provide a proof that it really does work?</p>\n', 'ViewCount': '341', 'Title': 'Maximise sum of "non-overlapping" numbers in square array - help with proof', 'LastEditorUserId': '1320', 'LastActivityDate': '2012-05-02T08:20:36.307', 'LastEditDate': '2012-05-01T00:17:31.290', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '1600', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1320', 'Tags': '<algorithms><dynamic-programming><check-my-algorithm>', 'CreationDate': '2012-04-30T19:20:52.473', 'Id': '1597'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm making a graduate thesis about proving correctness of program for multiplying 2 matrices using Hoare logic. For doing this, I need to generate the invariant for nested loop for this program:</p>\n\n<pre><code>for i = 1:n\n    for j = 1:n\n        for k = 1:n\n            C(i,j) = A(i,k)*B(k,j) + C(i,j);\n        end\n    end\nend\n</code></pre>\n\n<p>I've tried to find the invariant for inner loop first, but I can't find the true one until now. Is there someone can help me for finding the invariant for above program?</p>\n", 'ViewCount': '514', 'Title': 'Invariant For Nested Loop in Matrix Multiplication Program', 'LastEditorUserId': '472', 'LastActivityDate': '2012-05-06T15:19:40.093', 'LastEditDate': '2012-05-02T10:33:52.290', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '7', 'OwnerDisplayName': 'Anggha Nugraha', 'PostTypeId': '1', 'OwnerUserId': '1352', 'Tags': '<algorithms><loop-invariants><correctness-proof>', 'CreationDate': '2012-05-01T20:55:52.537', 'Id': '1625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '411', 'Title': 'All soldiers should shoot at the same time', 'LastEditDate': '2012-05-02T22:20:43.843', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '0', 'Body': '<p>When I was a student, I saw a problem in a digital systems/logic design textbook, about N soldiers standing in a row, and want to shoot at the same time. A more difficult version of the problem was that the soldiers stand in a general network instead of a row. I am sure this is a classical problem, but I cannot remember its name. Can you remind me?</p>\n', 'Tags': '<algorithms><distributed-systems><clocks>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-02T22:20:43.843', 'CommentCount': '0', 'AcceptedAnswerId': '1633', 'CreationDate': '2012-05-02T10:03:40.080', 'Id': '1632'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>[<em>previously appearing in cstheory, it was closed there and introduced here instead</em>]</p>\n\n<p>Given an edge-weighted graph $G=(V,E)$ the problem of finding the shortest path is known to be in P ---and indeed a simple approach would be Dijkstra\'s algorithm which can solve this problem in $O(V^2)$. A similar problem is to find the maximum path in $G$ from a source node to a target node and this can be solved with Integer Programming so that, as far as I know, this is not known to be in P.</p>\n\n<p>Now, the problem of finding a path in $G$ such that it deviates the minimum from a given target value (typically larger than the optimal distance but less than the maximum distance that separates the source and target nodes) has been conjectured to be in EXPTIME (see section "Conventions" of <a href="http://search-conference.org/index.php/Main/SOCS09program" rel="nofollow">A depth-first approach to target-value search</a> in the proceedings of SoCS 2009). In particular, this paper addresses this particular problem for  directed acyclic graphs (DAGs). A previous work is <a href="http://www.uwosh.edu/faculty_staff/furcyd/search_symposium_2008/schedule.html" rel="nofollow">Heuristic Search for Target-Value Path Problem</a>. There is event a US Patent of this algorithm <a href="http://www.google.es/patents?hl=es&amp;lr=&amp;vid=USPATAPP12497353&amp;id=gojwAAAAEBAJ&amp;oi=fnd&amp;dq=%22depth-first+search+for+target+value+problems%22&amp;printsec=abstract#v=onepage&amp;q=%22depth-first%20search%20for%20target%20value%20problems%22&amp;f=false" rel="nofollow">US 2011/0004625</a>.</p>\n\n<p>I\'ve been searching for related problems in other fields of Computer Science and Mathematics and strikingly, I have found none though this problem is clearly relevant in practice ---there are tons of opportunities to look for a specific target value instead of the minimum or the maximum path.</p>\n\n<p>Do you know related problems to this or additional bibliographical references to this problem? Any information on this problem including studies of their complexity would be very welcome</p>\n\n<p><strong>Note</strong>: as already pointed out by Jeffe in cstheory, proving this problem to be in EXPTIME is trivial and the authors probably meant EXPTIME-complete.</p>\n', 'ViewCount': '121', 'Title': 'Target-Value Search (& II)', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T20:16:41.033', 'LastEditDate': '2012-05-09T15:50:10.143', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1337', 'Tags': '<algorithms><complexity-theory><reference-request><search-algorithms>', 'CreationDate': '2012-05-02T12:21:24.380', 'Id': '1634'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I have a weighted undirected complete graph $G = (V, E)$. Each edge $e = (u, v, w)$ is assigned with a positive weight $w$. I want to calculate the minimum-weighted $(d, h)$-tree-decomposition. By $(d, h)$-tree-decomposition, I mean to divide the vertices $V$ into $k$ trees, such that the height of each tree is $h$, and each non-leaf node has $d$ children. </p>\n\n<p>I know it is definitely $\\text{NP}$-Hard, since minimum $(1, |V|-1)$-tree-decomposition is the minimum Hamilton path. But are there any good approximation algorithms?</p>\n', 'ViewCount': '214', 'Title': 'Approximate minimum-weighted tree decomposition on complete graphs', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T11:59:46.367', 'LastEditDate': '2012-05-10T11:59:46.367', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '11', 'OwnerDisplayName': 'Geni', 'PostTypeId': '1', 'OwnerUserId': '1354', 'Tags': '<algorithms><complexity-theory><graphs><graph-theory><approximation>', 'CreationDate': '2012-05-02T21:38:35.253', 'Id': '1640'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1389', 'Title': 'How can we assume that basic operations on numbers take constant time?', 'LastEditDate': '2013-09-10T22:18:05.507', 'AnswerCount': '6', 'Score': '31', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'FavoriteCount': '15', 'Body': "<p>Normally in algorithms we do not care about comparison, addition, or subtraction of numbers -- we assume they run in time $O(1)$.  For example, we assume this when we say that comparison-based sorting is $O(n\\log n)$, but when numbers are too big to fit into registers, we normally represent them as arrays so basic operations require extra calculations per element.</p>\n\n<p>Is there a proof showing that comparison of two numbers (or other primitive arithmetic functions) can be done in $O(1)$? If not why are we saying that comparison based sorting is $O(n\\log n)$?</p>\n\n<hr>\n\n<p><em>I encountered this problem when I answered a SO question and I realized that my algorithm is not $O(n)$ because sooner or later I should deal with big-int, also it wasn't pseudo polynomial time algorithm, it was $P$.</em></p>\n", 'Tags': '<algorithms><complexity-theory><algorithm-analysis><time-complexity><reference-question>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T22:18:05.507', 'CommentCount': '2', 'AcceptedAnswerId': '1661', 'CreationDate': '2012-05-03T00:06:31.453', 'Id': '1643'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Let $A_P = (Q,\\Sigma,\\delta,0,\\{m\\})$ the <em>string matching automaton</em> for pattern $P \\in \\Sigma^m$, that is </p>\n\n<ul>\n<li>$Q = \\{0,1,\\dots,m\\}$</li>\n<li>$\\delta(q,a) = \\sigma_P(P_{0,q}\\cdot a)$ for all $q\\in Q$ and $a\\in \\Sigma$</li>\n</ul>\n\n<p>with $\\sigma_P(w)$ the length of the longest prefix of $P$ that is a Suffix of $w$, that is</p>\n\n<p>$\\qquad \\displaystyle \\sigma_P(w) = \\max \\left\\{k \\in \\mathbb{N}_0 \\mid P_{0,k} \\sqsupset w \\right\\}$.</p>\n\n<p>Now, let $\\pi$ the <em>prefix function</em> from the <a href="https://secure.wikimedia.org/wikipedia/en/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm" rel="nofollow">Knuth-Morris-Pratt algorithm</a>, that is</p>\n\n<p>$\\qquad \\displaystyle \\pi_P(q)= \\max \\{k \\mid k &lt; q \\wedge P_{0,k} \\sqsupset P_{0,q}\\}$.</p>\n\n<p>As it turns out, one can use $\\pi_P$ to compute $\\delta$ quickly; the central observation is:</p>\n\n<blockquote>\n  <p>Assume above notions and $a \\in \\Sigma$. For $q \\in \\{0,\\dots,m\\}$ with $q = m$ or $P_{q+1} \\neq a$, it holds that</p>\n  \n  <p>$\\qquad \\displaystyle \\delta(q,a) = \\delta(\\pi_P(q),a)$</p>\n</blockquote>\n\n<p>But how can I prove this?</p>\n\n<hr>\n\n<p>For reference, this is how you compute $\\pi_P$:</p>\n\n<pre><code>m \u2190 length[P ]\n\u03c0[0] \u2190 0\nk \u2190 0\nfor q \u2190 1 to m \u2212 1 do\n  while k &gt; 0 and P [k + 1] =6 P [q] do\n    k \u2190 \u03c0[k]\n    if P [k + 1] = P [q] then\n       k \u2190 k + 1\n    end if\n    \u03c0[q] \u2190 k\n end while\nend for\n\nreturn \u03c0\n</code></pre>\n', 'ViewCount': '1117', 'Title': 'Connection between KMP prefix function and string matching automaton', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T15:53:30.563', 'LastEditDate': '2012-05-17T23:59:30.850', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '1900', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1378', 'Tags': '<algorithms><finite-automata><strings><searching>', 'CreationDate': '2012-05-05T09:56:27.257', 'Id': '1669'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '550', 'Title': 'Line separates two sets of points', 'LastEditDate': '2012-05-10T13:54:35.603', 'AnswerCount': '3', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '1170', 'FavoriteCount': '0', 'Body': '<p>If there is a way to identify if two sets of points can be separated by a line?</p>\n\n<blockquote>\n  <p>We have two sets of points $A$ and $B$ if there is a line that separates $A$ and $B$ such that all points of $A$ and only $A$ on the one side of the line, and all points of $B$ and only $B$ on the other side.</p>\n</blockquote>\n\n<p>The most naive algorithm I came up with is building convex polygon for $A$ and $B$ and test them for intersection. It looks time the time complexity for this should be $O(n\\log h)$ as for constructing a convex polygon. Actually I am not expecting any improvements in time complexity, I am not sure it can be improved at all. But al least there should be a more beautiful way to determine if there is such a line.</p>\n', 'Tags': '<algorithms><machine-learning><computational-geometry>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T13:54:35.603', 'CommentCount': '0', 'AcceptedAnswerId': '1687', 'CreationDate': '2012-05-05T15:18:04.503', 'Id': '1672'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I found the P vs NP problem some time ago and I have recently worked on the subset sum problem. I have read <a href="http://en.wikipedia.org/wiki/Subset_sum_problem" rel="nofollow">Wikipedia article</a> on the Subset Sum problem as well as the question <a href="http://stackoverflow.com/questions/4355955/subset-sum-algorithm">Subset Sum Algorithm</a> </p>\n\n<p>I have looked at the problem and found  some solutions  but so far they seem to be NP, \nI believe I can make a sufficiently fast algorithm in NP time.</p>\n\n<p>My problem is I am not good in theory so it doesn\'t help me much to talk about the Cook-Levin Theorem or Non-Deterministic Turing Machines.</p>\n\n<p>What I would like is an explanation of the pseudo-polynomial time dynamic programming subset sum that on Wikipedia.</p>\n\n<p>I have read it and I believe I understand the general concept of why it is NP instead of P (related to the size of the input rather than the operations with it),\nbut I do not understand the algorithm.</p>\n\n<p>I would appreciate if someone would put provide an example with some numbers and how it works. It would help me a lot because it would:</p>\n\n<ul>\n<li>Give me ideas to improve my future algorithm</li>\n<li>Help me understand intuitively when an algorithm is pseudo-polyonmial instead of NP.</li>\n</ul>\n', 'ViewCount': '807', 'Title': 'Subset sum, pseudo-polynomial time dynamic programming solution?', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-03-15T01:32:27.107', 'LastEditDate': '2013-03-15T01:32:27.107', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'OwnerDisplayName': 'user1094566', 'PostTypeId': '1', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2011-12-12T21:32:08.280', 'Id': '1689'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I am trying to break all the numbers from 1 to N down into their prime factors. Once I have the factors from 1 to N-1, is there an algorithm to give me the factors of 1 to N using dynamic programming?</p>\n", 'ViewCount': '280', 'Title': 'Is it possible to use dynamic programming to factor numbers', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-06T20:41:21.420', 'LastEditDate': '2012-05-06T19:56:49.690', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'Will Den', 'PostTypeId': '1', 'Tags': '<algorithms><dynamic-programming><factoring>', 'CreationDate': '2012-05-06T16:42:27.970', 'FavoriteCount': '1', 'Id': '1694'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an algorithm to find a minimal traversal of a directed graph of the following type. Two vertices are given, a start vertex and a terminating vertex. The traversal consists of several runs; each run is a path from the start vertex to the terminating vertex. A run may visit a node more than once. The length of a traversal is the total number of vertices traversed by the runs, with multiplicity; in other words, the length of a traversal is the number of runs plus the sum of the lengths of the runs.</p>\n\n<p>If there are edges that are not reachable (i.e. the origin of the edge is not reachable from the start vertex, or the terminating vertex is not reachable from the target of the edge), they are ignored.</p>\n\n<p>To illustrate my needs, I give a simple graph and post the result, I would like to receive by the algorithm (start vertex $1$, terminating vertex $4$):</p>\n\n<p>Graph edges:</p>\n\n<ul>\n<li>$1 \\to 2,3$</li>\n<li>$2 \\to 1,3,4$</li>\n<li>$3 \\to 4$</li>\n</ul>\n\n<p>Result:</p>\n\n<ul>\n<li>Run A: $1, 2, 1, 3, 4$</li>\n<li>Run B: $1, 2, 4$</li>\n<li>Run C: $1, 2, 3, 4$</li>\n</ul>\n\n<p>Each edge (also each direction) has been covered. Each run begins with vertex $1$ and ends with vertex $4$. The minimum total number of visited vertices is searched. In the given example, the minimum number is $5+3+4=12$. There is no unreachable edge in this example.</p>\n', 'ViewCount': '364', 'Title': 'Find the minimal number of runs to visit every edge of a directed graph', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-08T18:36:00.010', 'LastEditDate': '2012-05-06T23:29:42.950', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1397', 'Tags': '<algorithms><graphs><graph-theory>', 'CreationDate': '2012-05-06T22:00:02.643', 'FavoriteCount': '3', 'Id': '1698'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '114', 'Title': 'Randomized String Searching', 'LastEditDate': '2012-05-10T14:52:46.263', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '2', 'Body': "<p>I need to detect whether a binary pattern $P$ of length $m$ occurs in a binary text $T$ of length $n$ where $m &lt; n$.</p>\n\n<p>I want to state an algorithm that runs in time $O(n)$ where we assume that arithmetic operations on $O(\\log_2 n)$ bit numbers can be executed in constant time. The algorithm should accept with probability $1$ whenever $P$ is a substring of $T$ and reject with probability of at least $1 - \\frac{1}{n}$ otherwise.</p>\n\n<p>I think fingerprinting could help here. But I can't get it.</p>\n", 'Tags': '<algorithms><strings><searching><probabilistic-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T14:52:46.263', 'CommentCount': '3', 'AcceptedAnswerId': '1718', 'CreationDate': '2012-05-07T07:56:27.833', 'Id': '1712'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $A[1...N]$ be an Array of size $N$ with maximum element $\\max$.</p>\n\n<p>I want to transform array $A$ such that after transformations all elements of $A$ contain $\\max$, i.e. after transformation $A = [\\max,\\max,\\max,\\max,\\dots,\\max]$.</p>\n\n<p>In one step, I can apply the following operation to any consecutive sub-array $A[x..y]$:</p>\n\n<blockquote>\n  <p>Assign to all $A[i]$ with $x \\leq i \\leq y$ the <a href="https://en.wikipedia.org/wiki/Median#The_sample_median" rel="nofollow">median</a> of subarray $A[x..y]$.</p>\n</blockquote>\n\n<p>We consider as <em>median</em> always the $\\left\\lceil \\frac{n+1}{2} \\right\\rceil$-th element in an increasingly sorted version of $A$.</p>\n\n<p>What is the minimum number of steps needed to transform $A$ as desired? If it helps, assume that $N\\leq 30$.</p>\n\n<hr>\n\n<p><strong>Example 1:</strong></p>\n\n<p>Let $A = [1, 2, 3]$. We need to change it to $[3, 3, 3]$. The  minium number of steps is two, first for subarray $A[2..3]$ (after that $A$ equals to $[1, 3, 3]$), then operation to $A[1..3]$.</p>\n\n<p><strong>Example 2:</strong></p>\n\n<p>$A=[2,1,1,2]$.The  min step is  two. The median of subarray $A[1..4]$ is $2$ (3rd element in $[1,1,2,2]$. Apply the operation to $A[1..4]$ once and we get $[2,2,2,2]$.</p>\n', 'ViewCount': '179', 'Title': 'array median transformation using the min number of steps', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-14T20:32:19.343', 'LastEditDate': '2012-05-14T13:50:08.610', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1001', 'Tags': '<algorithms><arrays>', 'CreationDate': '2012-05-09T06:05:41.193', 'Id': '1748'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am a novice(total newbie to computational complexity theory) and I have a question.</p>\n\n<p>Lets say we have 'Traveling Salesman Problem' ,will the following application of Dijkstra's Algorithms solve it?</p>\n\n<p>From a start point we compute the shortest distance between two points. We go to the point. We delete the source point. Then we compute the next shortest distance point from the current point and so on...</p>\n\n<p>Every step we make the graph smaller while we move the next available shortest distance point. Until we visit all the points.</p>\n\n<p>Will this solve the traveling salesman problem.</p>\n", 'ViewCount': '5562', 'Title': "Dijsktra's algorithm applied to travelling salesman problem", 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-12T18:30:15.427', 'LastEditDate': '2012-05-12T18:30:15.427', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '5', 'OwnerDisplayName': 'Kamaal', 'PostTypeId': '1', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-05-09T06:55:45.493', 'FavoriteCount': '1', 'Id': '1749'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Having extracted the data-flow in some rather large programs as directed, acyclic graphs, I'd now like to optimize the order of evaluation to minimze the maximum amount of memory used.</p>\n\n<p>That is, given a graph {1 -> 3, 2 -> 3, 4 -> 5, 3 -> 5}, I'm looking for an algorithm that will decide the order of graph reduction to minimize the number of 'in-progress' nodes, in this particular case to decide that it should be reduced in the order 1-2-3-4-5; avoiding the alternative ordering, in this case 4-1-2-3-5, which would leave the output from node 4 hanging until 3 is also complete.</p>\n\n<p>Naturally, if there are two nodes using the output from a third, then it only counts once; data is not copied unnecessarily, though it does hang around until both of those nodes are reduced.</p>\n\n<p>I would also quite like to know what this problem is called, if it has a name. It looks similar to the graph bandwidth problem, only not quite; the problem statement may be defined in terms of path/treewidth, but I can't quite tell, and am unsure if I should prioritize learning that branch of graph theory right now.</p>\n", 'ViewCount': '126', 'Title': 'Optimizing order of graph reduction to minimize memory usage', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-10T03:23:26.373', 'LastEditDate': '2012-05-10T03:23:26.373', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1425', 'Tags': '<algorithms><graphs><optimization><software-engineering><program-optimization>', 'CreationDate': '2012-05-09T12:07:24.633', 'FavoriteCount': '1', 'Id': '1752'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '132', 'Title': 'Explain $\\log_2(n)$ squared asymptotic run-time for naive nested parallel CREW PRAM mergesort', 'LastEditDate': '2012-05-09T15:58:31.630', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'Paul Caheny', 'PostTypeId': '1', 'OwnerUserId': '1427', 'Body': '<p>On from Page 1 of <a href="http://www.inf.ed.ac.uk/teaching/courses/dapa/note3.pdf" rel="nofollow">these lecture notes</a> it is stated in the final paragraph of the section titled CREW Mergesort:</p>\n\n<blockquote>\n  <p>Each such step (in a sequence of $\\Theta(\\log_2\\ n)$ steps) takes\n  time $\\Theta(\\log_2\\ s)$ with a sequence length of $s$. Summing these, we\n  obtain an overall run time of $\\Theta((\\log_2\\  n)^2)$ for $n$\n  processors, which is not quite (but almost!) cost-optimal.</p>\n</blockquote>\n\n<p>Can anyone show explicitly how the sum mentioned is calculated and the squared log result arrived at?</p>\n', 'Tags': '<algorithms><complexity-theory><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-09T15:59:10.290', 'CommentCount': '0', 'AcceptedAnswerId': '1755', 'CreationDate': '2012-05-09T13:35:17.113', 'Id': '1754'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>This question has been prompted by <a href="http://cs.stackexchange.com/questions/1626/efficient-data-structures-for-building-a-fast-spell-checker">Efficient data structures for building a fast spell checker</a>.</p>\n\n<p>Given two strings $u,v$, we say they are <em>$k$-close</em> if their <a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance" rel="nofollow">Damerau\u2013Levenshtein distance</a>\xb9 is small, i.e. $\\operatorname{LD}(u,v) \\geq k$ for a fixed $k \\in \\mathbb{N}$. Informally, $\\operatorname{LD}(u,v)$ is the minimum number of deletion, insertion, substitution and (neighbour) swap operations needed to transform $u$ into $v$. It can be computed in $\\Theta(|u|\\cdot|v|)$ by dynamic programming. Note that $\\operatorname{LD}$ is a <a href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29" rel="nofollow">metric</a>, that is in particular symmetric.</p>\n\n<p>The question of interest is:</p>\n\n<blockquote>\n  <p>Given a set $S$ of $n$ strings over $\\Sigma$ with lengths at most $m$, what is the cardinality of </p>\n  \n  <p>$\\qquad \\displaystyle S_k := \\{ w \\in \\Sigma^* \\mid \\exists v \\in S.\\ \\operatorname{LD}(v,w) \\leq k \\}$?</p>\n</blockquote>\n\n<p>As even two strings of the same length have different numbers of $k$-close strings\xb2 a general formula/approach may be hard (impossible?) to find. Therefore, we might have to compute the number explicitly for every given $S$, leading us to the main question:</p>\n\n<blockquote>\n  <p>What is the (time) complexity of finding the cardinality of the set $\\{w\\}_k$ for (arbitrary) $w \\in \\Sigma^*$?</p>\n</blockquote>\n\n<p>Note that the desired quantity is exponential in $|w|$, so explicit enumeration is not desirable. An efficient algorithm would be great.</p>\n\n<p>If it helps, it can be assumed that we have indeed a (large) set $S$ of strings, that is we solve the first highlighted question.</p>\n\n<hr>\n\n<ol>\n<li>Possible variants include using the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> instead.</li>\n<li>Consider $aa$ and $ab$. The sets of $1$-close strings over $\\{a,b\\}$ are $\\{ a, aa,ab,ba,aaa,baa,aba,aab \\}$ (8 words) and $\\{a,b,aa,bb,ab,ba,aab,bab,abb,aba\\}$ (10 words), respectively .</li>\n</ol>\n', 'ViewCount': '268', 'Title': 'How many strings are close to a given set of strings?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:21:01.853', 'LastEditDate': '2012-05-15T20:21:01.853', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><time-complexity><strings><word-combinatorics><string-metrics>', 'CreationDate': '2012-05-09T15:48:12.173', 'FavoriteCount': '1', 'Id': '1758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to turn one string into another with only single letter substitions.  What is a good way to do this, passing through only valid words in between (<a href="http://www.wuzzlesandpuzzles.com/wordchange/" rel="nofollow">this</a> website has some examples)?</p>\n\n<p>Valid here means "a word in English" as this is the domain I consider.</p>\n\n<p>My current idea is that I could use a shortest path algorithm with the Hamming distance for edge weights. The problem is that it will take a long time to build the graph, and even then the weight is not so precise in terms of distance (though it will never underestimate it) unless the weight is one, so I would probably have to find a to build a graph that only had weights of one.</p>\n\n<p>What would be the easiest way to build the graph? Am I taking entirely the wrong approach?</p>\n', 'ViewCount': '320', 'Title': 'Turn one string into another with single letter substitions', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:21:41.307', 'LastEditDate': '2012-05-15T20:21:41.307', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '863', 'Tags': '<algorithms><strings><string-metrics>', 'CreationDate': '2012-05-10T23:30:48.357', 'Id': '1785'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1427', 'Title': 'Quicksort to find median?', 'LastEditDate': '2012-05-14T15:04:59.000', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': '<p>Why is the worst scenario $\\mathcal{O}\\left(n^2\\right)$ when using quicksort to find the median of a set of numbers?</p>\n\n<ul>\n<li><p>If your algorithm continually picks a number larger than or smaller than <em>all</em> numbers in the list wouldn\'t your algorithm fail? For example if the list of numbers are:</p>\n\n<p>$S = (12,75,82,34,55,15,51)$</p>\n\n<p>and you keep picking numbers greater than $82$ or less than $12$ to create sublists with, wouldn\'t your set   always remain the same size?</p></li>\n<li><p>If your algorithm continually picks a number that creates sublists of $1$ why is the worst case scenario $\\mathcal{O}\\left(n^2\\right)$? Wouldn\'t efficiency be linear considering that according to the <a href="http://en.wikipedia.org/wiki/Master_theorem" rel="nofollow">Master Theorem</a>,  $d&gt;\\log_b a$?* (and therefore be $\\mathcal{O}\\left(n^d\\right)$ or specifically in this case $\\mathcal{O}\\left(n\\right)$)</p></li>\n</ul>\n\n<p>*Where $d$ is the efficiency exponent (i.e. linear, exponential etc.), $b$ is the factor the size of problem is reduced by at each iteration, $a$ is the number of subproblems and $k$ is the level. Full ratio: $T(n) = \\mathcal{O}\\left(n^d\\right) * (\\frac{a}{b^d})^k$</p>\n', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-14T15:06:04.740', 'CommentCount': '0', 'AcceptedAnswerId': '1791', 'CreationDate': '2012-05-11T01:27:39.883', 'Id': '1789'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have trouble understanding the cyclic coordinate method. How does it differ with the <a href="http://en.wikipedia.org/wiki/Pattern_search_%28optimization%29" rel="nofollow">Hook and Jeeves method</a> and the <a href="http://en.wikipedia.org/wiki/Rosenbrock_methods" rel="nofollow">Rosenbrock method</a>?</p>\n\n<p>From a past exam text:</p>\n\n<blockquote>\n  <p>Describe the cyclic coordinate method and outline the similarities and the \n  differences between the Cyclic Coordinate method, the Hooke and Jeeves \n  method, and the Rosenbrock method.</p>\n</blockquote>\n\n<p>I would appreciate a good reference, I\'m having trouble finding any.</p>\n', 'ViewCount': '258', 'Title': 'Cyclic coordinate method: how does it differ from Hook & Jeeves and Rosenbrock?', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-27T17:35:21.360', 'LastEditDate': '2012-11-27T17:35:21.360', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'qurty', 'PostTypeId': '1', 'Tags': '<algorithms><reference-request><optimization><numerical-analysis>', 'CreationDate': '2012-05-06T17:32:23.577', 'Id': '1792'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am seeking a fast algorithm to compute the following function, a quantile of the <a href="http://en.wikipedia.org/wiki/Poisson_distribution" rel="nofollow">Poisson distribution</a>:\n$$f(n, \\lambda) = e^{-\\lambda} \\sum_{k=0}^{n} \\frac{\\lambda^k}{k!} $$</p>\n\n<p>I can think of an algorithm in $O(n)$, but considering the structure of the series, there is probably a $O(1)$ solution (or at least a good $O(1)$ approximation). Any take?</p>\n', 'ViewCount': '222', 'Title': 'Fast Poisson quantile computation', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-11T22:53:46.723', 'LastEditDate': '2012-05-11T22:33:12.843', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1457', 'Tags': '<algorithms><numerical-analysis>', 'CreationDate': '2012-05-11T14:33:03.150', 'Id': '1796'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If the weights of the weighted 3-DIMENSIONAL-MATCHING problem are restricted to let\'s say, 1 and 2, is there a possibility to reduce this case to the unweighted 3-DIMENSIONAL-MATCHING problem?\n(Because for the unweighted version, there is a (1.5+$\\epsilon$)-approximation<sup>1</sup> algorithm, for the weighted version, there is only a 2-approx<sup>2,3</sup> algorithm)</p>\n\n<hr>\n\n<p>References:</p>\n\n<ol>\n<li><p><a href="http://www.nada.kth.se/~viggo/wwwcompendium/node275.html#HurSch89" rel="nofollow">unweighted ($1.5+\\epsilon$-approx)</a> </p></li>\n<li><p><a href="http://www.nada.kth.se/~viggo/wwwcompendium/node275.html#ArkHas97" rel="nofollow">weighted ($2+\\epsilon$-approx)</a> </p></li>\n<li><p><a href="http://www.cs.umd.edu/~yhchan/thesis.pdf" rel="nofollow">weighted ($2$-approx)</a> by CHAN, Yuk Hei, 2009</p></li>\n</ol>\n', 'ViewCount': '256', 'Title': 'Weighted Maximum 3-DIMENSIONAL-MATCHING with restricted weights (Approx Algo)', 'LastEditorUserId': '1464', 'LastActivityDate': '2012-05-14T19:27:33.897', 'LastEditDate': '2012-05-14T19:27:33.897', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1464', 'Tags': '<algorithms><approximation>', 'CreationDate': '2012-05-12T13:43:09.090', 'Id': '1806'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given three matrices $A, B,C \\in \\mathbb{Z}^{n \\times n}$ we want to test whether $AB  \\neq C$. Assume that the arithmetic operations $+$ and $-$ take constant time when applied to numbers from $\\mathbb{Z}$.</p>\n\n<p>How can I state an algorithm with one-sided error that runs in $O(n^2)$ time and prove its correctness?</p>\n\n<p>I tried it now for several hours but I can't get it right. I think I have to use the fact that for any $x \\in \\mathbb{Z}^n$ at most half of the vectors $s \\in S = \\left\\{1, 0\\right\\}^n$  satisfy $x \\cdot s = 0$, where $x \\cdot s$ denotes the scalar product$\\sum_{i=1}^{n} x_is_i$.</p>\n", 'ViewCount': '232', 'Title': 'Probabilistic test of matrix multiplication with one-sided error', 'LastEditorUserId': '41', 'LastActivityDate': '2013-05-24T03:04:37.160', 'LastEditDate': '2012-05-12T20:19:48.560', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1811', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><probabilistic-algorithms><matrices><linear-algebra>', 'CreationDate': '2012-05-12T19:15:56.887', 'Id': '1809'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am doing FFT-based multiplication of polynomials with integer coefficients (long integers, in fact). The coefficients have a maximum value of $BASE-1, \\quad BASE \\in \\mathbb{n},\\quad BASE &gt; 1$. </p>\n\n<p>I would like to put forward a formal argument that if we use complex DFT for computing a convolution on a physical machine, it will yield incorrect results at some transform length $n\\in \\mathbb{N}$. </p>\n\n<p>What was easy to prove was the fact that at some big $n$ computing the convolution with DFT will not at all be possible, since, for example, the following difference of primitive roots modulo $n$:  $\\omega_n^1 - \\omega_n^2 \\rightarrow 0$ when $n \\rightarrow \\infty$, and if we are restricted by some machine epsilon $\\epsilon$, at some $n$ it will make the values indistinguishable and interpolation impossible.</p>\n\n<p>But the boundary I\'ve received using such an argument was way too big: only for $n=2^{60}$ I\'ve received $\\omega_n^1 - \\omega_n^2$ that had both components, $Re$ and $Im$, less than representable by $double$-precision type. This certainly is a boundary, but not very practical one.</p>\n\n<p>What I would like to show (if it is possible), is that much earlier than interpolation becomes theoretically impossible, the round-off errors will start to give wrong coefficients in the convolution, so that</p>\n\n<p>$$a\\cdot b \\neq IDFT(DFT(a)\\times DFT(b)),$$</p>\n\n<p>where $DFT$ and $IDFT$ are algorithm implementations that I use to calculate the Fourier transform. </p>\n\n<p>Maybe it is possible to make use of the fact that the value of the primitive root modulo $n$, $\\omega_n = \\exp(-2\\pi i / n)$, is an irrational number for the majority of $n$\'s. It will thereby be computed with inevitable error $\\psi$, defined as the value needed to "round off" everything that\'s less than the machine epsilon $\\epsilon$. Thus all the values used for DFT,</p>\n\n<p>$$\\omega_n^0, \\omega_n^1, ..., \\omega_n^{n-1},$$</p>\n\n<p>except for $\\omega_n^0$ will also be computed with errors. </p>\n\n<p>Since I\'m not a good mathematician at all, I don\'t know if and how I could use this fact to prove that the situation is going to worsen with increasing $n$ and that eventually the convolution is going to be computed incorrectly. </p>\n\n<p>I would also like to have and argument for OR against the following claim: for fixed $n$, the maximal error will be produced when all the coefficients of both polynomials are $BASE-1$.</p>\n\n<p>Thank you very much in advance!</p>\n', 'ViewCount': '101', 'Title': 'An argument for error accumulation during complex DFT', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-14T16:26:36.650', 'LastEditDate': '2012-05-14T15:45:22.677', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1198', 'Tags': '<algorithms><proof-techniques><numerical-analysis>', 'CreationDate': '2012-05-12T21:44:19.307', 'Id': '1814'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2080', 'Title': 'dynamic programming exercise on cutting strings', 'LastEditDate': '2012-05-13T18:35:49.400', 'AnswerCount': '2', 'Score': '8', 'OwnerDisplayName': 'Mark', 'PostTypeId': '1', 'OwnerUserId': '1556', 'FavoriteCount': '1', 'Body': '<p>I have been working on the following problem from this <a href="http://www.cs.berkeley.edu/~vazirani/algorithms/chap6.pdf">book</a>.</p>\n\n<blockquote>\n  <p>A certain string-processing language offers a primitive operation which splits a string into two\n  pieces. Since this operation involves copying the original string, it takes n units of time for a\n  string of length n, regardless of the location of the cut. Suppose, now, that you want to break a\n  string into many pieces. The order in which the breaks are made can affect the total running\n  time. For example, if you want to cut a 20-character string at positions $3$ and $10$, then making\n  the first cut at position $3$ incurs a total cost of $20 + 17 = 37$, while doing position 10 first has a\n  better cost of $20 + 10 = 30$.</p>\n</blockquote>\n\n<p>I need a dynamic programming algorithm that given $m$ cuts, finds the minimum cost of cutting a string into $m +1$ pieces.</p>\n', 'Tags': '<algorithms><combinatorics><strings><dynamic-programming>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-31T14:56:19.823', 'CommentCount': '0', 'CreationDate': '2012-04-09T03:17:15.270', 'Id': '1822'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '396', 'Title': 'Maximum Enclosing Circle of a Given Radius', 'LastEditDate': '2012-05-13T19:14:54.947', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '1170', 'FavoriteCount': '1', 'Body': '<p>I try to find an approach to the following problem:</p>\n\n<blockquote>\n  <p>Given the set of point $S$ and radius $r$, find the center point of circle, such that the circle contains the maximum number of points from the set. The running time should be $O(n^2)$.</p>\n</blockquote>\n\n<p>At first it seemed to be something similar to smallest enclosing  circle problem, that easily can be solved in $O(n^2)$. The idea was to set an arbitrary center and encircle all point of $S$. Next, step by step, replace the circle to touch the left/rightmost points and shrink the circle to the given radius, obviously, this is not going to work.</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '31', 'LastActivityDate': '2012-05-14T03:24:33.840', 'CommentCount': '0', 'AcceptedAnswerId': '1832', 'CreationDate': '2012-05-13T18:33:29.710', 'Id': '1825'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Some frontmatter: I\'m a recreational computer scientist and employed software engineer. So, pardon if this prompt seems somewhat out of left field -- I routinely play with mathematical simulcra and open problems when I have nothing better to do. </p>\n\n<p>While playing with the <a href="http://en.wikipedia.org/wiki/Riemann_hypothesis" rel="nofollow">Riemann hypothesis</a>, I determined that the <a href="http://en.wikipedia.org/wiki/Prime_gap" rel="nofollow">prime gap</a> can be reduced to a recurrence relation based on the intersection of all $n-1$ complementary functions formed by the multiples of each previous prime number (keen observers will note this is a generalization of the <a href="http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes" rel="nofollow">Sieve of Eratosthenes</a>). If this makes absolutely no sense to you, don\'t worry -- it\'s still frontmatter.</p>\n\n<p>Seeing how these functions related, I realized that the next instance of each prime can be reduced to the first intersection of these functions, recurring forward infinitely. However, I could not determine if this is tractable in polytime and polyspace. Thus: <strong>what I\'m looking for is an algorithm that can determine the first intersection of $n$ discrete (and, if applicable, monotonic) functions in polynomial time and space. If no such algorithm currently exists or can exist, a terse proof or reference stating so is sufficient.</strong> </p>\n\n<p>The closest I can find so far is <a href="http://en.wikipedia.org/wiki/Dykstra%27s_projection_algorithm" rel="nofollow">Dykstra\'s projection algorithm</a> (yes, that\'s R. L. Dykstra, not <a href="http://en.wikipedia.org/wiki/Edsger_Dijkstra" rel="nofollow">Edsger Dijkstra</a>), which I believe reduces itself to a problem of <a href="http://en.wikipedia.org/wiki/Linear_programming#Integer_unknowns" rel="nofollow">integer programming</a> and is, therefore, NP-hard. Similarly, if one performs a transitive set intersection of all of the applicable points (as they\'re currently understood to be bounded), we must still constrain ourselves to exponential space for our recurrence due to the current weak bound of $\\ln(m)$ primes for any real $m$ (and therefore, $e^n$ space for each prime $n$).</p>\n\n<p>Globally, I\'m wondering if my understanding of the reduction of the problem is wrong. I don\'t expect to solve the Riemann hypothesis (or any deep, open problem in this space) any time soon. Rather, I\'m seeking to learn more about it by playing with the problem, and I\'ve hit a snag in my research.</p>\n', 'ViewCount': '109', 'Title': 'Polytime and polyspace algorithm for determining the leading intersection of n discrete monotonic functions', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-23T16:24:14.963', 'LastEditDate': '2013-03-23T16:24:14.963', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '10705', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '958', 'Tags': '<algorithms><reference-request><discrete-mathematics>', 'CreationDate': '2012-05-13T23:01:03.153', 'Id': '1828'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am still fighting with hashing and I am ask myself: what is the most efficient way to count the number of different words in a text using a hash table?</p>\n\n<p>My intuition says that applying the hashcode function to every word in the text, as result we will have words with different hash values in different buckets and the same words will have the same bucket and therefore we will have a collision problem which we can resolve using the chaining method.</p>\n\n<p>Does it work like that?</p>\n', 'ViewCount': '773', 'Title': 'Counting different words in text using hashing', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:29:24.713', 'LastEditDate': '2012-05-14T16:54:51.270', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><strings><hash-tables>', 'CreationDate': '2012-05-14T16:41:35.327', 'Id': '1838'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder if somebody could quickly and briefly outline some of the similarities and differences between the line search methods <a href="http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Golden_section_search" rel="nofollow">Golden Section Search</a>, <a href="http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Fibonacci_search" rel="nofollow">Fibonacci Search</a> and <a href="https://en.wikipedia.org/wiki/Dichotomic_search" rel="nofollow">Dichotomic Search</a>.</p>\n\n<p>I know Dichotomous has two functional evaluations per iteration whereas the other two only one, and that the Fibonacci search tends to the Golden Section as the number of functional evaluates tends to infinity. I know also that you have to predetermine the number of functional evaluates for Fibonacci. Are there other, similar techniques?</p>\n', 'ViewCount': '371', 'Title': 'Golden Section, Fibonacci and Dichotomic Searches', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T09:22:55.040', 'LastEditDate': '2012-05-15T09:22:55.040', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1495', 'Tags': '<algorithms><optimization>', 'CreationDate': '2012-05-14T20:03:20.293', 'Id': '1843'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Brzozowski's DFA minimization algorithm builds a minimal DFA for DFA $G$ by:</p>\n\n<ol>\n<li>reversing all the edges in $G$, making the initial state an accept state, and the accept states initial, to get an NFA $N&#39;$ for the reverse language, </li>\n<li>using powerset construction to get $G&#39;$ for the reverse language, </li>\n<li>reversing the edges (and initial-accept swap) in $G&#39;$ to get an NFA $N$ for the original language, and</li>\n<li>doing powerset construction to get $G_{\\min}$.</li>\n</ol>\n\n<p>Of course, since some DFA's have an exponential large reverse DFA, this algorithm runs in exponential time in worst case in terms of the size of the input, so lets keep track of the size of the reverse DFA. </p>\n\n<p>If $N$ is the size of the input DFA, $n$ is the size of the minimal DFA, and $m$ the size of the minimal reverse DFA, then <strong>what is the run time of Brzozowski's algorithm in terms of $N$,$n$, and $m$?</strong></p>\n\n<p>In particular, <strong>under what relationship between $n$ and $m$ does Brzozowski's algorithm outperform Hopcroft's or Moore's algorithms?</strong></p>\n\n<p>I have heard that on typical examples in <em>practice/application</em>, Brzozowski's algorithm outperforms the others. <strong>Informally, what are these typical examples like?</strong></p>\n", 'ViewCount': '1327', 'Title': "Brzozowski's algorithm for DFA minimization", 'LastEditorUserId': '55', 'LastActivityDate': '2012-11-09T13:05:46.000', 'LastEditDate': '2012-05-16T18:52:28.467', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><finite-automata><runtime-analysis>', 'CreationDate': '2012-05-16T16:43:10.513', 'FavoriteCount': '2', 'Id': '1872'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a degree distribution, how fast can we construct a graph that follows the given degree distribution? A link or algorithm sketch would be good. The algorithm should report a "no" incase no graph can be constructed and any one example if multiple graphs can be constructed.</p>\n', 'ViewCount': '270', 'Title': 'Reconstructing Graphs from Degree Distribution', 'LastActivityDate': '2012-05-17T14:43:13.887', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '1885', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '661', 'Tags': '<algorithms><graphs><graph-theory>', 'CreationDate': '2012-05-17T12:53:27.080', 'FavoriteCount': '1', 'Id': '1883'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The identities used in multiplication algorithms by</p>\n\n<ul>\n<li><p><a href="http://en.wikipedia.org/wiki/Karatsuba_algorithm#The_basic_step">Karatsuba</a> (integers)</p></li>\n<li><p><a href="http://en.wikipedia.org/wiki/Multiplication_algorithm#Gauss.27s_complex_multiplication_algorithm">Gauss</a> (complex numbers)</p></li>\n<li><p><a href="http://en.wikipedia.org/wiki/Strassen_algorithm">Strassen</a> (matrices)</p></li>\n</ul>\n\n<p>seem very closely related. Is there a common abstract framework/generalization?</p>\n', 'ViewCount': '251', 'Title': 'Common idea in Karatsuba, Gauss and Strassen multiplication', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-20T00:18:56.647', 'LastEditDate': '2012-05-20T00:18:56.647', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '667', 'Tags': '<algorithms><matrices>', 'CreationDate': '2012-05-18T03:52:53.033', 'Id': '1901'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am learning the <a href="http://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* search algorithm</a> on an 8-puzzle problem.</p>\n\n<p>I don\'t have questions about A*, but I have some for the heuristic score -  Nilsson\'s sequence score.</p>\n\n<p><a href="http://www.heyes-jones.com/astar.html" rel="nofollow">Justin Heyes-Jones web pages - A* Algorithm</a> explains A* very clearly. It has a picture for Nilsson\'s sequence scores.</p>\n\n<p><img src="http://i.stack.imgur.com/Wbl63.jpg" alt="Nilsson\'s sequence scores"></p>\n\n<p>It explains:</p>\n\n<p><strong>Nilsson\'s sequence score</strong></p>\n\n<blockquote>\n  <p>A tile in the center scores 1 (since it should be empty)</p>\n  \n  <p>For each tile not in the center, if the tile clockwise to it is not the one that should be clockwise to it then score 2. </p>\n  \n  <p>Multiply this sequence by three and finally add the total distance you need to move each tile back to its correct position. </p>\n</blockquote>\n\n<p>I can\'t understand the steps above for calculating the scores.</p>\n\n<p>For example, for the start state, what h = 17?</p>\n\n<blockquote>\n  <p>0 A C </p>\n  \n  <p>H B D </p>\n  \n  <p>G F E</p>\n</blockquote>\n\n<p>So, by following the description, </p>\n\n<p><code>B</code> is in the center, so we have 1</p>\n\n<p>Then <code>for each title not in the center, if the **tile** clockwise to **it** is not the one that should be clockwise to it then score 2.</code> I am not sure what this statement means. </p>\n\n<p>What does the double starred <code>title</code> refer to? </p>\n\n<p>What does the double starred <code>it</code> refer to?</p>\n\n<p>Does the double starred <code>it</code> refer to the center title (B in this example)? Or does it refer to each title not in the center?</p>\n\n<p>Is the next step that we start from <code>A</code>? So <code>C</code> should not be clockwise to <code>A</code>, then we have 2. And then <code>B</code> should be clockwise to <code>A</code>, then we ignore, and so on and so forth?</p>\n', 'ViewCount': '1466', 'Title': "Nilsson's sequence score for 8-puzzle problem in A* algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-18T22:19:54.293', 'LastEditDate': '2012-05-18T07:52:23.553', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'Jackson Tale', 'PostTypeId': '1', 'Tags': '<algorithms><machine-learning><search-algorithms><heuristics>', 'CreationDate': '2012-05-14T15:41:04.650', 'Id': '1904'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '677', 'Title': 'Recurrences and Generating Functions in Algorithms', 'LastEditDate': '2012-09-22T18:14:36.253', 'AnswerCount': '4', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '5', 'Body': '<p>Combinatorics plays an important role in computer science. We frequently utilize combinatorial methods in both analysis as well as design in algorithms. For example one method for finding a $k$-vertex cover set in a graph might just inspect all $\\binom{n}{k}$ possible subsets. While the binomial functions grows exponentially, if $k$ is some fixed constant we end up with a polynomial time algorithm by asymptotic analysis.</p>\n\n<p>Often times real-life problems require more complex combinatorial mechanisms which we may define in terms of recurrences. One famous example is the <a href="http://en.wikipedia.org/wiki/Fibonacci_number" rel="nofollow">fibonacci sequence</a> (naively) defined as:</p>\n\n<p>$f(n) = \\begin{cases}\n   1 &amp; \\text{if } n = 1 \\\\\n   0 &amp; \\text{if } n = 0 \\\\\n   f(n-1) + f(n-2) &amp; \\text{otherwise}\n  \\end{cases}\n$</p>\n\n<p>Now computing the value of the $n$th term grows exponentially using this recurrence, but thanks to dynamic programming, we may compute it in linear time. Now, not all recurrences lend themselves to DP (off hand, the factorial function), but it is a potentially exploitable property when defining some count as a recurrence rather than a generating function.</p>\n\n<p>Generating functions are an elegant way to formalize some count for a given structure. Perhaps the most famous is the binomial generating function defined as:</p>\n\n<p>$(x + y)^\\alpha = \\sum_{k=0}^\\infty \\binom{\\alpha}{k}x^{\\alpha - k}y^k$</p>\n\n<p>Luckily this has a closed form solution. Not all generating functions permit such a compact description. </p>\n\n<blockquote>\n  <p>Now my question is this: how often are generating functions used in <em>design</em> of algorithms? It is easy to see how they may be exploited to understand the rate of growth required by an algorithm via analysis, but what can they tell us about a problem when creating a method to solve some problem?</p>\n</blockquote>\n\n<p>If many times the same count may be reformulated as a recurrence it may lend itself to dynamic programming, but again perhaps the same generating function has a closed form. So it is not so evenly cut.</p>\n', 'Tags': '<algorithms><algorithm-analysis><combinatorics>', 'LastEditorUserId': '699', 'LastActivityDate': '2013-01-24T03:59:38.067', 'CommentCount': '9', 'AcceptedAnswerId': '1937', 'CreationDate': '2012-05-18T17:13:56.063', 'Id': '1913'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '8101', 'Title': 'To Find the median of an unsorted array', 'LastEditDate': '2012-05-18T17:53:24.993', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1545', 'FavoriteCount': '2', 'Body': '<p>To fine the median of an unsorted array, we can make a min-heap in $O(n\\log n)$ time for $n$ elements, and then we can extract one by one $n/2$ elements to get the median. But this approach would take $O(n \\log n)$ time.</p>\n\n<p>Can we do the same by some method in $O(n)$ time? If we can, then please tell.</p>\n', 'Tags': '<algorithms><time-complexity>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-20T00:23:13.460', 'CommentCount': '4', 'AcceptedAnswerId': '1938', 'CreationDate': '2012-05-18T17:40:24.557', 'Id': '1914'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '287', 'Title': 'What is postorder traversal on this simple tree?', 'LastEditDate': '2012-05-19T08:26:17.000', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'Jorge Zapata', 'PostTypeId': '1', 'OwnerUserId': '1547', 'Body': '<p>Given the following tree: </p>\n\n<p><img src="http://i.stack.imgur.com/GbJzO.png" alt="tree"></p>\n\n<p>Which traversal method would give as result the following output: CDBEA?</p>\n\n<p>The answer in my study guide is <em>Postorder</em>, but I think postorder would output: DEBCA. Am I wrong?</p>\n', 'Tags': '<algorithms><trees>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-19T08:26:17.000', 'CommentCount': '4', 'AcceptedAnswerId': '1916', 'CreationDate': '2012-05-18T18:03:56.970', 'Id': '1915'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array $a_1,\\ldots,a_n$ of natural numbers $\\leq k$, where $k$ is a constant, I want to answer in $O(1)$ queries of the form: "how many times does $m$ appear in the array between indices $i$ and $j$"?</p>\n\n<p>The array should be preprocessed in linear time. In particular I\'d like to know if there\'s a reduction to Range Minimum Query.</p>\n\n<hr>\n\n<p>This is equivalent to RMQ in the case where $k=1$ and you want to query the number of ones within an interval. So we can use <a href="http://en.wikipedia.org/wiki/Range_Queries#Statement_Of_The_Problem">it</a>.<br>\n<sup>I couldn\'t answer my own question because of limits of SE.</sup></p>\n', 'ViewCount': '187', 'Title': 'Preprocess an array for counting an element in a slice (reduction to RMQ?)', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-20T05:42:23.280', 'LastEditDate': '2012-05-19T10:49:02.503', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '1942', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1504', 'Tags': '<algorithms><arrays><algorithm-design>', 'CreationDate': '2012-05-18T22:36:42.753', 'Id': '1918'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '79', 'Title': 'Making random sources uniformly distributed', 'LastEditDate': '2012-05-19T14:50:50.590', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '0', 'Body': '<p>How do I build a random source that outputs the bits 0 and 1 with $prob(0) = prob(1) = 0.5$. We have access to another random source $S$ that outputs $a$ or $b$ with independent probabilities $prob(a)$ and $prob(b) = 1 - prob(a)$ that are unknown to us.</p>\n\n<p>How do I state an algorithm that does the job and that does not consume more than an expected number of\n$(prob(a) \\cdot prob(b))^{-1}$ symbols of $S$ between two output bits and prove its correcteness?</p>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-19T19:13:23.613', 'CommentCount': '1', 'AcceptedAnswerId': '1934', 'CreationDate': '2012-05-19T14:14:55.337', 'Id': '1921'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '174', 'Title': 'Deterministic and randomized communication complexity of set equality', 'LastEditDate': '2012-06-06T13:47:19.663', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '1', 'Body': u'<p>Two processors $A, B$ with inputs $a \\in  \\{0, 1\\}^n$ (for $A$) and $b \\in  \\{0, 1\\}^n$\n(for $B$) want to decide whether $a = b$. $A$ does not know $B$\u2019s input and vice versa.</p>\n\n<p>A can send a message $m(a) \\in  \\{0, 1\\}^n$ which $B$ can use to decide $a = b$. The communication and computation rules are called a <em>protocol</em>.</p>\n\n<ul>\n<li>Show that every deterministic protocol must satisfy $|m(a)| \\ge  n$.</li>\n<li>State a randomized protocol that uses only $O(\\log_2n)$ Bits. The protocol should always accept if $a = b$ and accept with probability at most $1/n$ otherwise. Prove its correctness.</li>\n</ul>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-06-06T13:47:19.663', 'CommentCount': '5', 'AcceptedAnswerId': '1978', 'CreationDate': '2012-05-19T14:30:57.650', 'Id': '1922'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A source provides a stream of items $x_1, x_2,\\dots$ . At each step $n$ we want to save a random sample $S_n \\subseteq \\{ (x_i, i)|1 \\le i \\le n\\}$ of size $k$, i.e. $S_n$ should be a uniformly chosen sample from all $\\tbinom{n}{k}$ possible samples consisting of seen items. So at each step $n \\ge k$ we must decide whether to add the next item to $S$ or not. If so we must also decide which of the current items to remove from $S$ .</p>\n\n<p>State an algorithm for the problem. Prove its correctness.</p>\n', 'ViewCount': '158', 'Title': 'Online generation of uniform samples', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-03T01:02:26.967', 'LastEditDate': '2012-05-19T15:31:20.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1931', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness><online-algorithms>', 'CreationDate': '2012-05-19T14:38:52.510', 'Id': '1923'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The exercise is </p>\n\n<blockquote>\n  <p>Given a set of point $S$ and a point $p$. Decide in $O(n)$ time if $p$ is a vertex  of convex polygon formed from points of $S$.</p>\n</blockquote>\n\n<p>The problem is I am a little bit confused with time complexity $O(n)$. The more naive solution would be to construct convex polygon in $O(n\\log n)$ and test if $p$ is one of the vertices. </p>\n', 'ViewCount': '272', 'Title': 'If a point is a vertex of convex hull', 'LastEditorUserId': '472', 'LastActivityDate': '2012-05-25T13:05:40.863', 'LastEditDate': '2012-05-20T14:41:43.617', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '1941', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-05-20T03:47:59.837', 'Id': '1940'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '370', 'Title': 'How to go from a recurrence relation to a final complexity', 'LastEditDate': '2012-05-22T17:52:22.850', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1562', 'FavoriteCount': '1', 'Body': u'<p>I have an algorithm, shown below, that I need to analyze. Because it\'s recursive in nature I set up a recurrence relation.  </p>\n\n<pre><code>//Input: Adjacency matrix A[1..n, 1..n]) of an undirected graph G  \n//Output: 1 (true) if G is complete and 0 (false) otherwise  \nGraphComplete(A[1..n, 1..n]) {\n  if ( n = 1 )\n    return 1 //one-vertex graph is complete by definition  \n  else  \n    if not GraphComplete(A[0..n \u2212 1, 0..n \u2212 1]) \n      return 0  \n    else \n      for ( j \u2190 1 to n \u2212 1 ) do  \n        if ( A[n, j] = 0 ) \n          return 0  \n      end\n      return 1\n}\n</code></pre>\n\n<p>Here is what I believe is a valid and correct recurrence relation:  </p>\n\n<p>$\\qquad \\begin{align}\r\n  T(1) &amp;= 0 \\\\\r\n  T(n) &amp;= T(n-1) + n - 1 \\quad \\text{for } n \\geq 2\r\n\\end{align}$</p>\n\n<p>The "$n - 1$" is how many times the body of the for loop, specifically the "if A[n,j]=0" check, is executed.</p>\n\n<p>The problem is, where do I go from here? How do I convert the above into something that actually shows what the resulting complexity is?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-22T17:52:22.850', 'CommentCount': '1', 'AcceptedAnswerId': '1960', 'CreationDate': '2012-05-20T21:24:29.637', 'Id': '1959'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I'm learning for the exam and have problems with this task:</p>\n\n<blockquote>\n  <p>Describe an algorithm that transforms a given NFA $A = (Q, \\Sigma, \\delta, q_0, F)$ (which may have $\\epsilon$-transitions) into an equivalent NFA without $\\epsilon$-transitions with the same condition number. And then determine the maturity of the algorithm. The algorithm should have a running time $O(|Q| \xb7 |\\delta|)$ where\n  $$|\\delta| := \\sum_{\\substack{q\\in Q\\\\ a\\in\\Sigma\\cup\\{\\epsilon\\}}} |\\delta(q,a)|$$</p>\n</blockquote>\n", 'ViewCount': '710', 'Title': 'Transforming an NFA into an NFA of similar size but without $\\epsilon$-transitions', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-23T21:24:51.643', 'LastEditDate': '2012-05-22T19:49:52.230', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1593', 'Tags': '<algorithms><automata><finite-automata>', 'CreationDate': '2012-05-21T22:26:40.027', 'Id': '1983'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Who can help me with this topic: <a href="https://en.wikipedia.org/wiki/Open_addressing" rel="nofollow">Probing</a> with a step width that is a prime number.</p>\n\n<p>I am struggling with this question about defining a hashing function $h(k, i)$ for open addressing on a table of length m, that is, with slots numbers $0, 1, 2, \\dots ,m \u2212 1$.</p>\n\n<p>We know that a function $h(k, i) = h_1(k) + i \\cdot h_2(k) \\mod m$ produces a permutation for every $k$ if $h_2(k)$ and $m$ are relatively prime, that is, if $\\operatorname{gcd}(h_2(k),m) = 1$. </p>\n\n<p>We can assume that $m, w$ be integers such that the greatest common divisor $\\operatorname{gcd}(m,w) = 1$. </p>\n\n<p>How can I prove that the function above</p>\n\n<p>$\\qquad f : \\{ 0, \\dots,m \u2212 1 \\} \\to \\{ 0, \\dots,m \u2212 1 \\}\\\\\n \\qquad f(i) = i \\cdot w \\mod m$</p>\n\n<p>is a permutation, in other words, a <a href="https://en.wikipedia.org/wiki/Bijection" rel="nofollow">bijective function</a>? </p>\n', 'ViewCount': '116', 'Title': 'Is open adressing with prime steps bijective?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-23T14:00:48.277', 'LastEditDate': '2012-05-22T08:32:55.930', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><hash-tables><hash>', 'CreationDate': '2012-05-22T08:09:20.643', 'Id': '1988'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I am new and I have to develop an algorithm with a 2d integer array as input to compute the best career path.\nLets consider a network with n nodes, which are numbered from $1$ to $n$.\nAll are connected one to the other. </p>\n\n<p>To move from node $i$ to node $j$ can have different costs (classical example job changing):</p>\n\n<ul>\n<li>positive value indicates a benefit</li>\n<li>negative value indicates a loss </li>\n<li>some steps have a cost of $0$</li>\n</ul>\n\n<p>Each entry of $B[i, j]$  indicates the bene\ufb01t (or the cost, if negative), of a step from node $i$ to node $j$.</p>\n\n<p>I need to find out the maximal gain of a path from i to j is</p>\n\n<p>$$\nG(i, j) = \\max { \\{ g(p) \\mid \\text{ p is a path from i to j} \\}  }\n$$</p>\n\n<p>It is not possible to gain from walking in a cycle.\nThis means, the values in B must be such that for any path p from a node i to itself, we have $g(p) \\leq 0$.</p>\n\n<p>Ex of matrix B: </p>\n\n<p>$$\n\\begin{array}{cc}\n 0 &amp; 1 &amp; 0 &amp; 1 \\\\\\\\\n\u22122 &amp; 0 &amp; 0 &amp; \u22122 \\\\\\\\\n 0 &amp; 2 &amp; 0 &amp; 1 \\\\\\\\\n\u22123 &amp; \u22121 &amp; \u22123 &amp; 0\n\\end{array}\n$$</p>\n\n<p>Hints:</p>\n\n<p>1) Cycles bring no gain. Therefore, the greatest possible bene\ufb01t of moving\nfrom $i$ to $j$ can be achieved by visiting any intermediate node at most once.</p>\n\n<p>2) Consider the following variant of the problem. Let $G_{aux}(i, j, k)$ be the maximal gain that can be achieved by walking from $i$ to $j$ along a path that uses only the nodes $1, \\ldots, k$ as intermediate points. </p>\n\n<p>The tasks asked:</p>\n\n<ol>\n<li><p>Explain how $G_{aux}(i, j, k)$ can be used to compute $G(i, j)$. Develop a recurrence for $G_{aux}(i, j, k)$.</p></li>\n<li><p>Write pseudo-code for algorithms that compute arrays with all values of\n$G_{aux}$ and $G$ and explain why your algorithms are correct.</p></li>\n</ol>\n\n<p>Who can help me with this tasks?\nIs the basis of all this task the Floyd-Warshall's algorithm ? right?</p>\n", 'ViewCount': '139', 'Title': 'Optimal path - best career', 'LastEditorUserId': '851', 'LastActivityDate': '2012-05-22T21:38:01.030', 'LastEditDate': '2012-05-22T21:38:01.030', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1602', 'Tags': '<algorithms>', 'CreationDate': '2012-05-22T19:29:52.510', 'Id': '2005'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '14352', 'Title': 'How to convert finite automata to regular expressions?', 'LastEditDate': '2014-03-22T11:29:26.503', 'AnswerCount': '4', 'Score': '24', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '18', 'Body': '<p>Converting regular expressions into (minimal) NFA that accept the same language is easy with standard algorithms, e.g. <a href="http://en.wikipedia.org/wiki/Thompson%27s_construction_algorithm" rel="nofollow">Thompson\'s algorithm</a>. The other direction seems to be more tedious, though, and sometimes the resulting expressions are messy.</p>\n\n<p>What algorithms are there for converting NFA into equivalent regular expressions? Are there advantages regarding time complexity or result size?</p>\n\n<p><sup>This is supposed to be a reference question. Please include a general decription of your method as well as a non-trivial example.</sup></p>\n', 'Tags': '<algorithms><formal-languages><finite-automata><regular-expressions><reference-question>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-22T11:29:26.503', 'CommentCount': '2', 'AcceptedAnswerId': '2389', 'CreationDate': '2012-05-23T08:19:27.003', 'Id': '2016'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to solve the following computational geometry problem. </p>\n\n<blockquote>\n  <p>Let $S$ be a set of $n$ axis-parallel rectangles in the plane, so that the bottom edge of each rectangle in $S$ lies on the $x$-axis.</p>\n  \n  <ol>\n  <li>What is (an upper bound on) the combinatorial complexity of the union $K$ of the rectangles in $S$?</li>\n  <li>Give an efficient algorithm for computing the union and its area.</li>\n  </ol>\n</blockquote>\n\n<p>I suggest using sweep line algorithm for the purpose of computing union of areas. \nFirst we should consider queue of events. Events are just the leftmost and the rightmost $x$'s of rectangle. As in standard interpretation all $x$'s should be sorted. </p>\n\n<p>Start iterations on event queue (like in standard algorithm). On every new event we can compute an area we've already covered. When two or more rectangles intersect (can be identified by data structure) we should pick the rectangle with the biggest $y$-coordinate until the next event.</p>\n\n<p>That's a general idea. The main difference from the classic sweep line algorithm is that we don't have to compute intersection and inserting them to queue.  All we are interested in is intersection of rectangles which occur on vertical lines of leftmost $x$ and rightmost $x$.</p>\n\n<p>I am not completely sure that the solution I presented is the correct one. This exercice was marked with high complexity grade. Maybe I missed something?</p>\n\n<p>In addition, I don't know how to answer the first question. </p>\n", 'ViewCount': '488', 'Title': 'Area of the union of rectangles anchored on the x-axis', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-23T20:23:50.673', 'LastEditDate': '2012-05-23T20:23:50.673', 'AnswerCount': '0', 'CommentCount': '17', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1170', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-05-23T11:25:08.457', 'Id': '2019'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In a directed graph with a starting node and an ending node, how to find a small (doesn't have to be smallest. &lt;10 for example) set S of nodes such that every possible path from the starting node to the ending node contains at least one member of set S. The graph may have loops. This may be NP hard. Is there an approximate method to find one or several such S from the graph? Enumerating and testing every candidate seems not work. thanks.</p>\n", 'ViewCount': '58', 'Title': 'Finding small node sets that can not be avoided on paths from source to sink', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-23T22:05:04.273', 'LastEditDate': '2012-05-23T22:05:04.273', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2032', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1620', 'Tags': '<algorithms><graphs><graph-theory>', 'CreationDate': '2012-05-23T20:39:40.190', 'Id': '2028'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a "smarter" version of Bellman-Ford here; this version is more clever about choosing the edges to relax.</p>\n\n<pre><code>//Queue Q; source s; vertices u, v; distance to v d(v)\nQ \u2190 s // Q holds vertices whose d(v) values have been updated recently.\nWhile (Q !empty) {\n  u \u2190 Dequeue(Q)\n  for each neighbor v of u {\n    Relax(u, v)\n    if d(v) was updated by Relax and v not in Q\n      Enqueue(v)\n  }\n}\n</code></pre>\n\n<p>But, can anyone explain why this improved version correctly finds the shortest path from $s$ to every other vertex in a directed graph with no negative cycles?</p>\n\n<p>Also, what is the <em>worst-case</em> runtime if every shortest path uses at most $v$ edges?</p>\n', 'ViewCount': '458', 'Title': 'Bellman-Ford variation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-25T05:17:22.893', 'LastEditDate': '2012-05-24T07:55:34.530', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1624', 'Tags': '<algorithms><graphs><graph-theory><runtime-analysis><shortest-path>', 'CreationDate': '2012-05-24T01:44:55.237', 'FavoriteCount': '2', 'Id': '2039'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the usual definition of probabilistic poly-time machine it is said that the machine halts in polynomial time for all inputs. </p>\n\n<p>Is the intention really to say that the machine halts for all inputs, or that if it halts it must be in polynomial time?</p>\n', 'ViewCount': '124', 'Title': 'Probabilistic poly-time machine always halts on all inputs?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-25T20:13:41.407', 'LastEditDate': '2012-05-25T11:15:04.400', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'OwnerDisplayName': 'user583311', 'PostTypeId': '1', 'Tags': '<complexity-theory><terminology><turing-machines><probabilistic-algorithms>', 'CreationDate': '2012-05-24T14:21:16.507', 'Id': '2047'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Solving the <a href="https://en.wikipedia.org/wiki/Maximum_flow_problem" rel="nofollow">maximum flow problem</a> yields one qualified minimal cut. But I want several (maybe hundreds) small cuts as candidates. The cuts don\'t have to be minimum cuts, as long as they are small (in weight). How do I do that?</p>\n', 'ViewCount': '154', 'Title': 'In s-t directed graph, how to find many small cuts?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-02T17:30:29.300', 'LastEditDate': '2012-05-25T11:18:35.083', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1620', 'Tags': '<algorithms><graphs><graph-theory><optimization><approximation>', 'CreationDate': '2012-05-24T20:19:25.250', 'FavoriteCount': '2', 'Id': '2052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Dynamic programming can reduce the time needed to perform a recursive algorithm. I know that dynamic programming can help reduce the time complexity of algorithms. Are the general conditions such that if satisfied by a recursive algorithm would imply that using dynamic programming will reduce the time complexity of the algorithm? When should I use dynamic programming?</p>\n', 'ViewCount': '1373', 'Title': 'When can I use dynamic programming to reduce the time complexity of my recursive algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-25T11:10:59.523', 'LastEditDate': '2012-05-25T11:10:59.523', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1634', 'Tags': '<algorithms><dynamic-programming><efficiency><algorithm-design>', 'CreationDate': '2012-05-24T22:26:27.710', 'FavoriteCount': '1', 'Id': '2057'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I need to recover a data block from a repeated stream of data. I'm looking to see what algorithms may already exist for this as it does not feel like a novel situation.</p>\n\n<p>Here are the specifics:</p>\n\n<ol>\n<li>There is an N-length block of data contained in a stream</li>\n<li>The block is repeated many times in the stream</li>\n<li>the data is highly corrupted, some bytes could just be wrong, where as others can be detected as missing (erasures)</li>\n<li>There is a function <code>F(data)</code> which can say if a block represents valid data (the probability of a false positive is virtually zero)</li>\n<li><code>F</code> can also provide a probability value that even if the block is not valid data whether the block itself is valid (but just has too much corruption to be recovered)</li>\n<li>The chance of corrupted data is very low compared to missing data</li>\n</ol>\n\n<p>For example, say I have this data stream and wish to recover the 10 length sequence <code>1234567890</code>. The data is just a rough visual example (I can't guarantee recovery is actually possible from this bit). A <code>.</code> represents a missing byte, and <code>&lt;break&gt;</code> indicates an unknown block of data (no data  and not length known). Note also the <code>Q</code>s as an example of corrupt data.</p>\n\n<p><code>23.5678901.3456789&lt;break&gt;2345678..1..4567QQ012345678..3456</code></p>\n\n<p>How can I take such a stream of data and recovery probably blocks of N data? As the actual data includes forward error recovery the block recovery need not be perfect. All it needs to do is give probable reconstructed blocks of data and the <code>F</code> function will attempt to do error recovery.  Thus I expect <code>F</code> fill have to be called several times. </p>\n\n<p>I'd like to find something better than simply calling <code>F</code> at each point in the stream since the error rate could be high enough that no single run block of N can be recovered -- the repetitions in the stream must be used somehow.</p>\n", 'ViewCount': '41', 'Title': 'Block detection in repeated stream', 'LastEditorUserId': '1642', 'LastActivityDate': '2012-05-25T11:53:58.550', 'LastEditDate': '2012-05-25T11:53:58.550', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2072', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1642', 'Tags': '<algorithms><online-algorithms><communication-protocols>', 'CreationDate': '2012-05-25T04:33:48.567', 'Id': '2064'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have to make a quick clustering program but the following formula is gibberish to me:</p>\n\n<blockquote>\n  <p>$\\operatorname{Perf}(X,C) = \\sum\\limits_{i=1}^n\\min\\{||X_i-C_l||^2  \\mid  l = 1,...,K\\}$</p>\n  \n  <p>where $X$ is a set of multi-dimensional data and $C$ is a set of centroids for each data cluster.</p>\n</blockquote>\n\n<p>This formula is a fitness function for an <a href="https://en.wikipedia.org/wiki/Artificial_bee_colony_algorithm" rel="nofollow">artificial bee colony clustering algorithm</a> as a substitute for <a href="https://en.wikipedia.org/wiki/K-means_clustering_algorithm" rel="nofollow">k-means clustering algorithm</a>. It is described as a total\nwithin-cluster variance or the total mean-square quantization error (MSE).</p>\n\n<p>Can anyone translate it to <em>pseudo-code</em>, normal human <em>English</em>, or at least enlighten me?</p>\n', 'ViewCount': '117', 'Title': 'What does this performance formula mean?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-25T10:19:26.157', 'LastEditDate': '2012-05-25T10:19:26.157', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '2068', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1643', 'Tags': '<algorithms><terminology><evolutionary-computing>', 'CreationDate': '2012-05-25T05:45:24.833', 'Id': '2067'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to solve the following coverage problem.</p>\n\n<blockquote>\n  <p>There are $n$ transmitters with coverage area of 1km and $n$ receivers. Decide in $O(n\\log n)$ that all receivers  are covered by any transmitter. All reveivers and transmiters are represented by their $x$ and $y$ coordinates.</p>\n</blockquote>\n\n<p>The most advanced solution I can come with takes $O(n^2\\log n)$. For every receiver sort all transmitter by it distance to this current receiver, then take the transmitter with shortest distance and this shortest distance should be within 0.5 km.</p>\n\n<p>But the naive approach looks like much better in time complexity $O(n^2)$. Just compute all distance between all pairs of transmitter and receiver.</p>\n\n<p>I am not sure if I can apply range-search algorithms in this  problem. For example kd-trees allow us to find such ranges, however I never saw an example, and I am not sure if there are kind of range-search for circles. </p>\n\n<p>The given complexity $O(n\\log n)$ assumes that the solution should be somehow similar to sorting.</p>\n', 'ViewCount': '220', 'Title': 'Coverage problem (transmitter and receiver)', 'LastEditorUserId': '157', 'LastActivityDate': '2012-09-14T14:03:11.483', 'LastEditDate': '2012-05-25T06:24:54.113', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '1170', 'Tags': '<algorithms><computational-geometry><search-problem>', 'CreationDate': '2012-05-25T06:06:23.963', 'FavoriteCount': '3', 'Id': '2069'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For my pet project I need to cluster some data which could be easily represented as graph, so I want to use this as an opportunity to educate myself and play with various algorithms. I'd prefer the book on graph clustering as it often more self contained but articles are fine too. Back in the days I used to work in the field of numerical linear algebra so I'd also prefer algebraical view on things (so books which view graph as a matrix with specific properties are more accessible to me).</p>\n\n<p>p.s. I've tried scholar.google.com but was overwhelmed by vast number of results.  </p>\n", 'ViewCount': '81', 'Title': 'Could someone suggest me a good introductory book or an article on graph clustering?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-25T14:29:21.953', 'LastEditDate': '2012-05-25T14:19:42.963', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '2077', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1647', 'Tags': '<algorithms><graphs><graph-theory><reference-request><books>', 'CreationDate': '2012-05-25T13:00:28.593', 'Id': '2076'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '597', 'Title': 'Determine missing number in data stream', 'LastEditDate': '2012-05-26T03:55:57.317', 'AnswerCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1402', 'FavoriteCount': '2', 'Body': '<p>We receive a stream of $n-1$ pairwise different numbers from the set $\\left\\{1,\\dots,n\\right\\}$.</p>\n\n<p>How can I determine the missing number with an algorithm that reads the stream once and uses a memory of only $O(\\log_2 n)$ bits?</p>\n', 'Tags': '<algorithms><integers><online-algorithms>', 'LastEditorUserId': '157', 'LastActivityDate': '2012-05-28T17:15:07.057', 'CommentCount': '4', 'AcceptedAnswerId': '2090', 'CreationDate': '2012-05-25T17:40:04.097', 'Id': '2079'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been looking around for an algorithm that would optimize the distance between 2 list of coordinates and choose which coordinate should go together.</p>\n\n<p>Say I have List 1:</p>\n\n<pre><code>205|200\n220|210\n200|220\n200|180\n</code></pre>\n\n<p>List 2:</p>\n\n<pre><code>210|200\n207|190\n230|200\n234|190\n</code></pre>\n\n<p>Calculated Distance between Coords:</p>\n\n<pre><code>205|200 to 210|200 == 5.00\n205|200 to 207|190 == 10.20\n205|200 to 230|200 == 25.00\n205|200 to 234|190 == 30.68\n\n220|210 to 210|200 == 14.14\n220|210 to 207|190 == 23.85\n220|210 to 230|200 == 14.14\n220|210 to 234|190 == 24.41\n\n200|220 to 210|200 == 22.36\n200|220 to 207|190 == 30.81\n200|220 to 230|200 == 36.06\n200|220 to 234|190 == 45.34\n\n200|180 to 210|200 == 22.36\n200|180 to 207|190 == 12.21\n200|180 to 230|200 == 36.06\n200|180 to 234|190 == 35.44\n</code></pre>\n\n<p>This Algorithm would pick:</p>\n\n<pre><code>205|200 to 230|200 == 25.00\n220|210 to 207|190 == 23.85\n200|220 to 210|200 == 22.36\n200|180 to 234|190 == 35.44\n</code></pre>\n\n<p>The Algorithm would pick these numbers as they would be the group that would have the littlest variance between the distance.\nConditions:</p>\n\n<ol>\n<li>A Coordinate may only be used ones from each list</li>\n<li>If List 1 or List2 is larger than it still only uses each coordinate once, but it tries to get the smallest distance variance and does nothing with the unused coordinates.</li>\n</ol>\n\n<p>If you need more clarification please ask.</p>\n\n<p>P.S. I've looked at the Hungarian algorithm and it seems like it will sort of do the job, but not exactly how I was expecting. The Hungarian algorithm will only try and make the least distance from all the coordinates, which can mean the smallest variance, but not every time as variance is more important here then least distance optimization.</p>\n\n<p><strong>Additional Information</strong></p>\n\n<p>I will have an array of List1, List2, and then the distances:</p>\n\n<pre><code>Distance[List1_item_0][List2_item_0] = 5;\nDistance[List1_item_0][List2_item_1] = 10.20;\nDistance[List1_item_0][List2_item_2] = 25.00;\nDistance[List1_item_0][List2_item_3] = 30.68;\n\nDistance[List1_item_1][List2_item_0] = 14.14;\nDistance[List1_item_1][List2_item_1] = 23.85;\nDistance[List1_item_1][List2_item_2] = 14.14;\nDistance[List1_item_1][List2_item_3] = 24.41;\n\nDistance[List1_item_2][List2_item_0] = 22.36;\nDistance[List1_item_2][List2_item_1] = 30.81;\nDistance[List1_item_2][List2_item_2] = 36.06;\nDistance[List1_item_2][List2_item_3] = 45.34;\n\nDistance[List1_item_3][List2_item_0] = 22.36;\nDistance[List1_item_3][List2_item_1] = 12.21;\nDistance[List1_item_3][List2_item_2] = 36.06;\nDistance[List1_item_3][List2_item_3] = 35.44;\n</code></pre>\n\n<p>From the Distance['List1_item_#] I would need to pick a distance. Once that distance is picked the [List2_item_#] CANNOT be picked by a different [List1_item_#]. The distances picked for each [List1_item_#] element would need to be picked in a way that the variance between them all is minimal. So distance for each [List1_item_#] should be as close as possible to each other without reusing a [List2_item_#] more than once.</p>\n", 'ViewCount': '232', 'Title': 'Algorithm to minimize distance variance between 2D coordinates', 'LastEditorUserId': '1654', 'LastActivityDate': '2012-11-04T16:06:16.810', 'LastEditDate': '2012-05-26T09:49:46.280', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1654', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-05-25T20:52:40.690', 'FavoriteCount': '1', 'Id': '2082'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Has anyone seen this problem before? It\'s suppose to be NP-complete.</p>\n\n<blockquote>\n  <p>We are given vertices $V_1,\\dots ,V_n$ and possible parent sets for each vertex. Each parent set has an associated cost. Let $O$ be an ordering (a permutation) of the vertices. We say that a parent set of a vertex $V_i$ is consistent with an ordering $O$ if all of the parents come before the vertex in the ordering. Let $mcc(V_i, O)$ be the minimum cost of the parent sets of vertex $V_i$ that are consistent with ordering $O$. I need to find an ordering $O$ that minimizes the total cost: $mcc(V_1, O), \\dots ,mcc(V_n, O)$.</p>\n</blockquote>\n\n<p>I don\'t quite understand the part "...if all of the parents come before the vertex in the ordering." What does it mean?</p>\n', 'ViewCount': '94', 'Title': 'Need help understanding this optimization problem on graphs', 'LastEditorUserId': '1123', 'LastActivityDate': '2012-05-27T18:05:16.293', 'LastEditDate': '2012-05-27T18:05:16.293', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2104', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1556', 'Tags': '<algorithms><graph-theory><terminology><optimization>', 'CreationDate': '2012-05-27T07:00:31.647', 'Id': '2100'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '141', 'Title': 'Finding the point nearest to the x-axis over some segment', 'LastEditDate': '2012-05-27T12:09:06.050', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1379', 'FavoriteCount': '1', 'Body': "<p>I have problem with solving the following exercise</p>\n\n<blockquote>\n  <p>Given the set $P$ on $n$ points in two dimensions, build in time $O(n\\log n)$ a data structure of $P$ such that given a horizontal segment $s$ find the first point that $s$ touches when moving upwards from the x-axis in time $O(\\log^2n)$.</p>\n</blockquote>\n\n<p>The preprocessing time is equivalent to sorting, so we can perform sorting by one dimension.</p>\n\n<p>The query time is a little bit confusing - $\\log^2$n. I would say it's $\\log n$ binary searchs but it doesn't make sense.</p>\n", 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-13T23:48:36.627', 'CommentCount': '1', 'AcceptedAnswerId': '2105', 'CreationDate': '2012-05-27T07:29:21.960', 'Id': '2101'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If I were to let the variables be the propositions and, constraint be all clauses being satisfied, which technique would be more effective in solving 3-SAT? <a href="http://en.wikipedia.org/wiki/Look-ahead_%28backtracking%29#Look_ahead_techniques" rel="nofollow">Forward checking</a> or <a href="http://en.wikipedia.org/wiki/Arc_consistency#Arc_consistency" rel="nofollow">arc consistency</a>? From what I gathered forward-checking is $O(n)$, while Arc consistency is about $O(8c)$ where c is the number of constraints (According to this <a href="http://www.cs.ubc.ca/~kevinlb/teaching/cs322%20-%202006-7/Lectures/lect11.pdf" rel="nofollow">page</a>). So perhaps forward -checking is faster somehow? How should I determine which to use?</p>\n', 'ViewCount': '591', 'Title': 'Forward checking vs arc consistency on 3-SAT', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-28T18:01:07.223', 'LastEditDate': '2012-05-28T18:01:07.223', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1556', 'Tags': '<algorithms><satisfiability><heuristics><3-sat><sat-solvers>', 'CreationDate': '2012-05-28T07:03:31.467', 'Id': '2120'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'d like to understand the <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" rel="nofollow">Baum-Welch algorithm</a>. I liked <a href="http://www.youtube.com/watch?v=7zDARfKVm7s&amp;feature=related" rel="nofollow">this video</a> on the Forward-Backward algorithm so I\'d like a similar one for Baum-Welch.</p>\n\n<p>I\'m having trouble coming up with good resources for Baum-Welch. Any ideas?</p>\n', 'ViewCount': '275', 'Title': 'Any very user friendly resources on the Baum-Welch algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-29T18:34:46.793', 'LastEditDate': '2012-05-28T23:52:50.387', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '3364', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1680', 'Tags': '<algorithms><reference-request><hidden-markov-models>', 'CreationDate': '2012-05-28T23:43:11.757', 'Id': '2149'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '785', 'Title': 'How to prove correctness of a shuffle algorithm?', 'LastEditDate': '2012-05-30T08:13:00.493', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': "<p>I have two ways of producing a list of items in a random order and would like to determine if they are equally fair (unbiased).</p>\n\n<p>The first method I use is to construct the entire list of elements and then do a shuffle on it (say a Fisher-Yates shuffle). The second method is more of an iterative method which keeps the list shuffled at every insertion. In pseudo-code the insertion function is:</p>\n\n<pre><code>insert( list, item )\n    list.append( item )\n    swap( list.random_item, list.last_item )\n</code></pre>\n\n<p>I'm interested in how one goes about showing the fairness of this particular shuffling. The advantages of this algorithm, where it is used, are enough that even if slightly unfair it'd be okay. To decide I need a way to evaluate its fairness.</p>\n\n<p>My first idea is that I need to calculate the total permutations possible this way versus the total permutations possible for a set of the final length. I'm a bit at a loss however on how to calculate the permutations resulting from this algorithm. I also can't be certain this is the best, or easiest approach.</p>\n", 'Tags': '<algorithms><proof-techniques><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-01T11:23:35.340', 'CommentCount': '5', 'AcceptedAnswerId': '2156', 'CreationDate': '2012-05-29T07:11:15.180', 'Id': '2152'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a directed acyclic graph where edge (A,B) means that vertex A depends on vertex B. </p>\n\n<p>Vertex deletions have the following restrictions:</p>\n\n<ol>\n<li>When vertex B is removed, all dependent vertexes should also be removed. </li>\n<li>When vertex  A is removed and vertex A was the only vertex that depends on B, vertex B should also be removed.</li>\n</ol>\n\n<p><img src="http://i.stack.imgur.com/qeDQd.png" alt="enter image description here"></p>\n\n<p>I need to list the vertixes which are deleted when</p>\n\n<ol>\n<li><p>Vertex B is deleted. My solution is B, E and J because</p>\n\n<ul>\n<li>B -- deleted</li>\n<li>E -- because of condition 2, B is removed and B was the only vertex that depends on E</li>\n<li>J -- because of condition 2</li>\n</ul></li>\n<li><p>Vertex C is deleted. My solution is C, F, A, G, ... ?</p>\n\n<ul>\n<li>C -- deleted</li>\n<li>F -- because of condition 2 (C is the only vertex to F)</li>\n<li>A -- condition 1 (depends on C)</li>\n<li>G -- condition 2 (C is the only vertex to G)</li>\n<li>I think here the process goes on and cascades. Is that correct?</li>\n</ul></li>\n</ol>\n\n<p>What could be an algorithm for such vertexes dependency network which allows for the vertex deletion?</p>\n\n<p>PS: this is an old exam exercise (2008/09); I use it as exercise for my one middle of June.</p>\n', 'ViewCount': '454', 'Title': 'Dependency Graph - Acyclic graph', 'LastEditorUserId': '31', 'LastActivityDate': '2012-06-01T20:50:34.107', 'LastEditDate': '2012-06-01T08:54:17.570', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><graphs><graph-theory>', 'CreationDate': '2012-06-01T07:35:22.473', 'Id': '2186'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '761', 'Title': 'How to use a greedy algorithm to find the non-decreasing sequence closest to the given one?', 'LastEditDate': '2012-10-11T21:23:25.513', 'AnswerCount': '3', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '1718', 'FavoriteCount': '1', 'Body': "<p>You are given n integers $a_1, \\ldots, a_n$ all between $0$ and $l$. Under each integer $a_i$ you should write an integer $b_i$ between $0$ and $l$ with the requirement that the $b_i$'s form a non-decreasing sequence. Define the deviation of such a sequence to be $\\max(|a_1-b_1|, \\ldots, |a_n-b_n|)$. Design an algorithm that finds the $b_i$'s with the minimum deviation in runtime $O(n\\sqrt[4]{l})$.</p>\n\n<p>I honestly have no clue whatsoever how to even begin to solve this question. It looks like a dynamic programming question to me, but the professor said that this should be solved using a greedy algorithm. It would be much appreciated if someone can point me in the right direction by giving a small hint.</p>\n", 'Tags': '<algorithms><optimization><greedy-algorithms><subsequences>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T21:23:25.513', 'CommentCount': '6', 'AcceptedAnswerId': '2242', 'CreationDate': '2012-06-01T15:43:28.810', 'Id': '2188'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For a given undirected graph $G$, a <a href="http://en.wikipedia.org/wiki/Gomory%E2%80%93Hu_tree" rel="nofollow">Gomory-Hu tree</a> is a graph which has the same nodes as $G$, but its edges represent the minimal cut between each pair of nodes in $G$. The Gomory-Hu algorithm finds such a tree for a graph. A representant pair of nodes is defined as follows: if $R$ and $S$ are two components of the Gomory-Hu tree, and there is an edge $e$ between them, then the nodes $r \\in R$ and $s \\in S$ are representants if the weight of the edge $(r,s)$ is the same as the weight of $e$. </p>\n\n<p>I have to learn not only the algorithm, but also all the lemmas needed to prove that it works. For this specific lemma, there is a proof given in my learning materials, but I am afraid I don\'t understand how it works. </p>\n\n<p>It starts by picking two components of the Gomory-Hu tree, $A$ and $B$, with an edge $h$ between them, $a \\in A$ and $b \\in B$ being the representants. In the next iteration, nodes $x$ and $y$ in $A$ are picked, and a new minimal $(x,y)$-cut is calculated (dividing $A$ into the subsets $X$ and $Y$), such that now $h$ connects $X$ and $B$. If $a \\in X$, then $a$ and $b$ are still representants. But if $a \\in Y$, the proof claims that $x$ and $b$ are the new representants of $h$. </p>\n\n<p>For this, it states that </p>\n\n<blockquote>\n  <p>The cut which created $h$ divides $x$ and $b$. From that, it follows that $f(x,b) \\le f(a,b)$. </p>\n</blockquote>\n\n<p>[It uses $f(a,b)$ to denote the flow in the minimal cut between nodes $a$ and $b$.] Then it goes on to prove that also $f(x,b) \\ge f(a,b)$. And then the two flows must be equal, so the flow between $x$ and $b$ is the same as the flow in the minimal $(a,b)$-cut, so $x$ and $b$ are representants. </p>\n\n<p>But as I understand the algorithm, the cut which created $h$ was a minimal cut between the nodes $a$ and $b$. The node $x$ wasn\'t even a special node at the time the graph was divided into components $A$ and B$.$ Yes, this cut happens to divide $x$ and $b$ too, but there is no guarantee that it is the minimal cut between $x$ and $b$ (this is exactly what we are trying to prove here). So I think that we can follow that $f(x,b) \\ge f(a,b)$, but not that $f(x,b) \\le f(a,b)$. I suspect that there is an error in my reasoning and not in the reasoning of the prof who wrote the learning materials, but where is it? </p>\n\n<p>And if there actually is an error in this proof, what is the correct proof? </p>\n', 'ViewCount': '79', 'Title': 'What is the proof for the lemma "For every iteration of the Gomory-Hu algorithm, there is a representant pair for each edge"?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-02T04:53:12.310', 'LastEditDate': '2012-06-02T04:53:12.310', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1717', 'Tags': '<algorithms><graph-theory><algorithm-analysis>', 'CreationDate': '2012-06-01T15:45:55.923', 'Id': '2189'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Definition</strong>: monotone polygon - a polygon $P$ in the plane is called monotone with respect to a straight line $L$, if every line orthogonal to $L$ intersects $P$ at most twice.</p>\n</blockquote>\n\n<p>I am wondering if and how it is possible to test whether there is a line $L$ for a given polygon $P$ so that $P$ is monotone with respect to $L$.</p>\n\n<p>Previously I\'ve asked for help with the <a href="http://cs.stackexchange.com/questions/1577/testing-polygon-for-monotonicity">similar problem when $L$ is the x-axis</a>, and now I am interested in the case when $L$ is not given in advance.</p>\n', 'ViewCount': '460', 'Title': 'Testing Polygon for Monotonicity with respect to any arbitrary line', 'LastEditorUserId': '1170', 'LastActivityDate': '2012-06-06T09:13:40.597', 'LastEditDate': '2012-06-04T13:30:00.020', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2227', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1170', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-06-02T07:45:32.713', 'Id': '2197'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider an array $X$ of $n$ cells, each containing a number from $\\{1,..., n\\}$. There is at least\none duplicate number, i.e., a number that appears at least twice. I want output <em>some</em> duplicate number. When streaming we may pass over $X$ more than once. The inspection of a cell generates cost $1$. The cost of a run of an algorithm is the sum of all individual costs. I can at most store $\\log_2n$ bit numbers.\nI tried to do that with a streaming algorithm that uses additional memory $O(1)$ with costs $O(n^2)$. Is it possible to state a ramdom access algorithm that uses additional memory $O(\\log_2n)$ with costs $O(n)$?.</p>\n\n<p>Which algorithm solves the problem by using additional memory $O(1)$ with costs $O(n^2)$?.\nWhich algorithm solves the problem by using additional memory $O(\\log_2n)$ with costs $O(n)$?.</p>\n\n<p>My problem is similar to the cycle detection problem, but I don't know how to use the cycle detection problem to solve mine. Is there maybe a simpler way that I can't see now?</p>\n", 'ViewCount': '115', 'Title': 'Streaming algorithm and random access', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-03T22:21:04.637', 'LastEditDate': '2012-06-02T18:27:50.347', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><randomized-algorithms><random><streaming-algorithm>', 'CreationDate': '2012-06-02T14:46:56.317', 'FavoriteCount': '2', 'Id': '2199'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '303', 'Title': 'Weighted subset sum problem', 'LastEditDate': '2012-06-03T12:19:58.383', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '851', 'FavoriteCount': '1', 'Body': '<p>Given an integer sequence $\\{ a_1, a_2, \\ldots, a_N  \\}$ that has length $N$ and a fixed integer $M\\leq N$, the <a href="http://opc.iarcs.org.in/public/WEIGHTED-SUM.pdf" rel="nofollow">problem</a> is to find a subset $A =\\{i_1, \\dots, i_M\\} \\subseteq [N]$ with $1 \\leq i_1 \\lt i_1 \\lt \\dots \\lt i_M \\leq N$ such that</p>\n\n<p>$\\qquad \\displaystyle \\sum_{j=1}^M j \\cdot a_{i_j}$ </p>\n\n<p>is maximized.</p>\n\n<hr>\n\n<p>For instance, if the given sequence is $-50; 100; -20; 40; 30$ and $M = 2$, the best weighted sum arises when we choose positions 2 and 4. </p>\n\n<p>So that we get a value $1 \\cdot 100 + 2 \\cdot 40 = 180$.</p>\n\n<p>On the other hand, if the given sequence is $10; 50; 20$ and $M$ is again 2, the best option is to choose positions 1 and 2 that we get a value $1 \\cdot 10 + 2 \\cdot 50 = 110$.</p>\n\n<hr>\n\n<p>To me it looks similar to the <a href="http://en.wikipedia.org/wiki/Maximum_subarray_problem" rel="nofollow">maximum subarray problem</a>, but I can think of many examples in which the maximum subarray is not the best solution.</p>\n\n<p>Is this problem an instance of a well studied problem? What is the best algorithm to solve it?</p>\n\n<p>This question was inspired by <a href="http://stackoverflow.com/questions/10861642/find-maximum-weighted-sum-over-all-m-subsequences">this StackOverflow question</a>.</p>\n', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-03T12:19:58.383', 'CommentCount': '9', 'AcceptedAnswerId': '2202', 'CreationDate': '2012-06-02T15:07:15.513', 'Id': '2200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a game I\'m building some ai for that has 2 players making simultaneous moves.  In this game there is exactly one move where, if they both make it at the same time, the outcome is different than if they\'d made it separately (all other moves are pretty independent).</p>\n\n<p>Anyway, I\'m trying to find a good algorithm to throw at it.  Minimax with alpha-beta pruning seems like it would be a good candidate if the players were making alternating moves, but not for simultaneous ones.  I found <a href="http://www.lamsade.dauphine.fr/~saffidine/Papers/2012/Alpha-Beta%20Pruning%20for%20Games%20with%20Simultaneous%20Moves.pdf">a paper(pdf)</a> on the topic, but it\'s a little over my head- I\'m having trouble reading the pseduocode.</p>\n\n<p>So, can someone either help clarify that approach, suggest another way to accomplish alpha-beta pruning on such a game, or suggest a better algorithm entirely?</p>\n', 'ViewCount': '188', 'Title': 'Alpha-Beta Pruning with simultaneous moves?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-04T08:42:59.177', 'LastEditDate': '2012-06-04T08:42:59.177', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1737', 'Tags': '<algorithms><artificial-intelligence><search-algorithms><game-theory>', 'CreationDate': '2012-06-04T00:45:22.650', 'Id': '2215'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I consider <a href="http://en.wikipedia.org/wiki/Point_in_polygon" rel="nofollow">Point Location Problem in Polygon</a> in repetitive mode in the case of simple polygon.</p>\n\n<p>In computational geometry,Point Location Problem in Polygon problem asks whether a given point in the plane lies inside, outside, or on the boundary of a polygon.</p>\n\n<p>There are few method that work in Single-Shot approach, where the input is a polygon $P$ and a single point $q$ (no preprocessing time). Ray casting algorithm is the famous algorithm for single-shot, it takes $O(n)$ to determine whether a point $q$ belongs to polygon $P$. </p>\n\n<p>In addition, there is a repetitive approach, where instead of single point $q$ we should check the sequence of points, therefore the preprocessing is required. Division wedge is a algorithm that works in repetitive mode. Query time of division wedge is $O(\\log n)$ and preprocessing time is $O(n)$. Division wedge assumes that there is a central point in polygon, visible from every vertex of polygon (part of the kernel of the polygon). The problem is a central point can be easily determined in convex polygon as well as in star-shaped polygon, but what to do in the case of simple polygon.</p>\n\n<p>If division wedge is applied in the case of simple polygon how we can determine a central point in simple polygon? If division edge in not applied if there is the more efficient way to solve a problem in simple polygon than in arbitrary planar subdivision.</p>\n', 'ViewCount': '251', 'Title': 'Point Location Problem in Polygon in Repetitive Mode for a Simple Polygon', 'LastEditorUserId': '472', 'LastActivityDate': '2012-06-05T14:46:01.177', 'LastEditDate': '2012-06-04T16:02:22.587', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '2228', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-06-04T15:56:42.037', 'Id': '2225'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a graph G. How can we find a spanning tree that minimizes the maximum weight of all the edges in the tree? I am convinced that by simply finding an MST of G would suffice, but I am having a lot of trouble proving that my idea is actually correct. Can anyone show me a proof sketch or give me some hints as to how to construct the proof? Thanks!</p>\n', 'ViewCount': '991', 'Title': 'How to find spanning tree of a graph that minimizes the maximum edge weight?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-10-08T20:18:19.143', 'LastEditDate': '2012-06-04T23:30:42.360', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1718', 'Tags': '<algorithms><graph-theory><optimization>', 'CreationDate': '2012-06-04T16:24:30.567', 'Id': '2226'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '276', 'Title': 'Generating number of possibilites of popping two stacks to two other stacks', 'LastEditDate': '2012-06-13T15:33:40.447', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1784', 'FavoriteCount': '1', 'Body': '<p>Context: I\'m working on <a href="http://stackoverflow.com/questions/10875675/how-to-find-out-all-the-popping-out-possibilities-of-two-stacks">this problem</a>:</p>\n\n<blockquote>\n  <p>There are two stacks here:</p>\n\n<pre><code>A: 1,2,3,4 &lt;- Stack Top\n  B: 5,6,7,8\n</code></pre>\n  \n  <p>A and B will pop out to other two stacks: C and D. For example: </p>\n\n<pre><code> pop(A),push(C),pop(B),push(D).\n</code></pre>\n  \n  <p>If an item have been popped out , it must be pushed to C or D immediately.</p>\n</blockquote>\n\n<p>The goal is to enumerate all possible stack contents of C and D after moving all elements.</p>\n\n<p>More elaborately, the problem is this: If you have two source stacks with $n$ unique elements (all are unique, not just per stack) and two destination stacks and you pop everything off each source stack to each destination stack, generate all unique destination stacks - call this $S$.</p>\n\n<p>The stack part is irrelevant, mostly, other than it enforces a partial order on the result. If we have two source stacks and one destination stack, this is the same as generating all permutations without repetitions for a set of $2N$ elements with $N$ \'A\' elements and $N$ \'B\' elements. Call this $O$.</p>\n\n<p>Thus</p>\n\n<p>$\\qquad \\displaystyle |O| = (2n)!/(n!)^2$</p>\n\n<p>Now observe all possible bit sequences of length 2n (bit 0 representing popping source stack A/B and bit 1 pushing to destination stack C/D), call this B. |B|=22n. We can surely generate B and check if it has the correct number of pops from each destination stack to generate |S|. It\'s a little faster to recursively generate these to ensure their validity. It\'s even faster still to generate B and O and then simulate, but it still has the issue of needing to check for duplicates.</p>\n\n<p>My question</p>\n\n<p>Is there a more efficient way to generate these?</p>\n\n<p>Through simulation I found the result follows <a href="http://oeis.org/A084773" rel="nofollow">this sequence</a> which is related to Delannoy Numbers, which I know very little about if this suggests anything.</p>\n\n<p>Here is my Python code</p>\n\n<pre><code>def all_subsets(list):\n    if len(list)==0:\n        return [set()]\n    subsets = all_subsets(list[1:])\n\n    return [subset.union(set([list[0]])) for subset in subsets] + subsets\n\ndef result_sequences(perms):\n    for perm in perms:\n        whole_s = range(len(perm))\n        whole_set = set(whole_s)\n        for send_to_c in all_subsets(whole_s):\n            send_to_d = whole_set-set(send_to_c)\n            yield [perm,send_to_c,send_to_d]\n\nn = 4\nperms_ = list(unique_permutations([n,n],[\'a\',\'b\'])) # number of unique sequences                                                                                                               \nresult = list(result_sequences(perms_))\n</code></pre>\n', 'Tags': '<algorithms><combinatorics><efficiency>', 'LastEditorUserId': '1784', 'LastActivityDate': '2012-06-14T22:35:54.300', 'CommentCount': '2', 'AcceptedAnswerId': '2305', 'CreationDate': '2012-06-07T16:36:42.153', 'Id': '2257'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '753', 'Title': 'Finding interesting anagrams', 'LastEditDate': '2012-06-07T22:08:29.743', 'AnswerCount': '4', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '1786', 'FavoriteCount': '0', 'Body': "<p>Say that $a_1a_2\\ldots a_n$ and $b_1b_2\\ldots b_n$ are two strings of the same length.  An <strong>anagramming</strong> of two strings is a bijective mapping $p:[1\\ldots n]\\to[1\\ldots n]$ such that $a_i = b_{p(i)}$ for each $i$.</p>\n\n<p>There might be more than one anagramming for the same pair of strings.  For example, If $a=$<code>abcab</code> and $b=$<code>cabab</code> we have $p_1[1,2,3,4,5]\\to[4,5,1,2,3]$ and $p_2[1,2,3,4,5] \\to [2,5,1,4,3]$, among others.</p>\n\n<p>We'll say that the <strong>weight</strong> $w(p)$ of an anagramming $p$ is the number of values of $i\\in[1\\ldots n-1]$ for which $p(i)+1\\ne p(i+1)$. That is, it is the number of points at which $p$ does <em>not</em> increase by exactly 1.For example, $w(p_1) = 1$ and $w(p_2) = 4$.</p>\n\n<p>Suppose there exists an anagramming for two strings $a$ and $b$. Then at least one  anagamming must have least weight. Let's say this this one is <strong>lightest</strong>. (There might be multiple lightest anagrammings; I don't care because I am interested only in the weights.)</p>\n\n<h2>Question</h2>\n\n<p>I want an algorithm which, given two strings for which an anagramming exists, efficiently <strong>yields the exact weight of the lightest anagramming</strong> of the two strings. It is all right if the algorithm yields a lightest anagramming, but it need not.</p>\n\n<p>It is a fairly simple matter to generate all anagrammings and weigh them, but there may be many, so I would prefer a method that finds light anagrammings directly.</p>\n\n<hr>\n\n<h2>Motivation</h2>\n\n<p>The reason this problem is of interest is as follows.  It is very easy to make the computer search the dictionary and find anagrams, pairs of words that contain exactly the same letters.  But many of the anagrams produced are uninteresting.  For instance, the longest examples to be found in Webster's Second International Dictionary are:</p>\n\n<blockquote>\n  <p>cholecystoduodenostomy<br>\n  duodenocholecystostomy</p>\n</blockquote>\n\n<p>The problem should be clear: these are uninteresting because they admit a very light anagramming that simply exchanges the <code>cholecysto</code>, <code>duedeno</code>, and <code>stomy</code> sections, for a weight of 2. On the other hand, this much shorter example is much more surprising and interesting:</p>\n\n<blockquote>\n  <p>coastline<br>\n  sectional</p>\n</blockquote>\n\n<p>Here the lightest anagramming has weight 8.</p>\n\n<p>I have a program that uses this method to locate interesting anagrams, namely those for which all anagrammings are of high weight. But it does this by generating and weighing all possible anagrammings, which is slow.</p>\n", 'Tags': '<algorithms><strings><search-algorithms><natural-lang-processing>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-21T14:18:17.910', 'CommentCount': '2', 'AcceptedAnswerId': '2265', 'CreationDate': '2012-06-07T18:31:28.390', 'Id': '2259'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need an algorithm to calculate the number of ways of expressing a number N as sum of numbers inside the interval [a, b] </p>\n', 'ViewCount': '135', 'Title': 'Numbers of ways of expressing the sum of a number between [a,b]', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-08T12:27:19.363', 'LastEditDate': '2012-06-08T12:17:33.120', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'vikt0r', 'PostTypeId': '1', 'Tags': '<algorithms><combinatorics>', 'CreationDate': '2012-06-07T05:43:12.957', 'Id': '2273'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading something about the concept of walks in a graph b/w a start vertex and a terminating vertex in a graph and then suddenly a problem struck me, is there any algorithm or a method that can be used to enumerate all the distinct walks from a start vertex to a terminal vertex in a graph, if so can you all point me to some relevant links to study this problem and what are some applications of solving this problem?</p>\n', 'ViewCount': '233', 'Title': 'Enumerating all the walks in a graph between a start vertex and a terminal vertex?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-09T03:44:41.467', 'LastEditDate': '2012-06-08T12:29:16.630', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'Coder', 'PostTypeId': '1', 'Tags': '<algorithms><reference-request><graph-theory>', 'CreationDate': '2012-06-05T13:10:08.960', 'FavoriteCount': '2', 'Id': '2274'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We want to solve a minimal-cost-flow problem with a generic negative-cycle cancelling algorithm. That is, we start with a random valid flow, and then we do not pick any "good" negative cycles such as minimal average cost cycles, but use Bellman-Ford to discover a minimal cycle and augment along the discovered cycle. Let $V$ be the number of nodes in the graph, $A$ the number of edges, $U$ the maximal capacity of an edge in the graph, and $W$ the maximal costs of an edge in the graph. Then, my learning materials claim: </p>\n\n<ul>\n<li>The maximal costs at the beginning can be no more than $AUW$ </li>\n<li>The augmentation along one negative cycle reduces the costs by at least one unit </li>\n<li>The lower bound for the minimal costs is 0, because we don\'t allow negative costs </li>\n<li>Each negative cycle can be found in $O(VA)$ </li>\n</ul>\n\n<p>And they follow from it that the algorithm\'s complexity is $O(V\xb2AUW)$. I understand the logic behind each of the claims, but think that the complexity is different. Specifically, the maximal number of augmentations is given by one unit of flow per augmentation, taking the costs from $AUW$ to zero, giving us a maximum of $AUW$ augmentations. We need to discover a negative cycle for each, so we multiply the maximal number of augmentations by the time needed to discover a cycle ($VA$) and arrive at $O(A\xb2VUW)$ for the algorithm. </p>\n\n<p>Could this be an error in the learning materials (this is a text provided by the professor, not a student\'s notes from the course), or is my logic wrong? </p>\n', 'ViewCount': '258', 'Title': u'Why is the complexity of negative-cycle-cancelling $O(V\xb2AUW)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-06T00:47:27.197', 'LastEditDate': '2012-06-10T11:42:45.650', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1717', 'Tags': '<algorithms><graph-theory><algorithm-analysis><runtime-analysis><network-flow>', 'CreationDate': '2012-06-08T13:26:38.437', 'FavoriteCount': '1', 'Id': '2283'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $f$ and $g$ be two functions and $p$ a number. Consider the following program:</p>\n\n<pre><code>Recurs(v,p) :\n  find s &lt; v such that f(s,v) &lt; v/2 and g(s,v-s) &lt; p\n\n  if no such s exists then\n    return v\n  else if s &lt;= v/4 then \n    return v-s U Recurs(s,p)\n  else if s &gt; v/4 then \n    return Recurs(s,p) U Recurs(v-s,p)\nend\n</code></pre>\n\n<p>Can the recurrence for the running time of this recursion be $T(v)=T\\left(\\frac{v}{4}\\right)+T\\left(\\frac{3v}{4}\\right)+1$?</p>\n', 'ViewCount': '154', 'Title': "Is the following recurrence for this program's runtime correct?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-11T14:11:29.740', 'LastEditDate': '2012-06-11T10:38:39.823', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '3', 'OwnerDisplayName': 'raarava', 'PostTypeId': '1', 'OwnerUserId': '1818', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2012-06-04T03:17:56.457', 'Id': '2295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '27', 'Title': 'How to use greedy algorithm to solve this?', 'LastEditDate': '2012-06-09T07:28:40.890', 'AnswerCount': '0', 'Score': '1', 'OwnerDisplayName': 'Aden Dong', 'PostTypeId': '1', 'OwnerUserId': '1718', 'Body': u'<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/2188/how-to-use-greedy-algorithm-to-solve-this">How to use greedy algorithm to solve this?</a>  </p>\n</blockquote>\n\n\n\n<p>You are given $n$ integers $a_1, \\ldots, a_n$ all between $0$ and $l$. Under each integer $a_i$ you should write an integer $b_i$ between $0$ and $l$ with the requirement that the $b_i$\'s form a non-decreasing sequence (i.e. $b_i \\le b_{i+1}$ for all $i$). Define the deviation of such a sequence to be $\\max(|a_1\u2212b_1|,\\ldots,|a_n\u2212b_n|)$. Design an algorithm that finds the $b_i$\'s with the minimum deviation in runtime $O(n\\sqrt[4]{l})$.</p>\n\n<p>There were also two hints, one is to first find an algorithm in $O(nl)$ time, the other is that the runtime of the optimal algorithm is actually must less than $\\Theta(n\\sqrt[4]{l})$.</p>\n\n<p>I was able to find a solution that runs in $O(n^2)$ (without using any of the hints), but I have no idea how to find an algorithm that runs in $O(n\\sqrt[4]{l})$. Can anyone offer some insight into this? Maybe give a rough sketch of your algorithm? Thanks!</p>\n', 'ClosedDate': '2012-06-09T08:32:15.807', 'Tags': '<algorithms><optimization><greedy-algorithms>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-09T07:28:40.890', 'CommentCount': '0', 'CreationDate': '2012-06-06T04:06:54.050', 'Id': '2296'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am interested in the complexity of the restricted version of the vertex cover problem below:</p>\n\n<blockquote>\n  <p><strong>Instance:</strong> A bipartite graph $G =(L, R, E)$ and an integer $K$.</p>\n  \n  <p><strong>Question:</strong> Is there $S \\subset L$, $|S| \\leq K$ and every vertex in $R$ has a neighbor in $S$ $( S$ is vertex cover for $R)$</p>\n</blockquote>\n\n<p>Vertex cover is $\\mathsf{P}$ if $S \\subset L \\cup R$ and cover $L \\cup R$; and it is $\\mathsf{NP}$-complete for nonbipartite graphs. However, the problem I am looking at does not fit in either cases. Any pointers where I could find an answer will be appreciated.</p>\n', 'ViewCount': '215', 'Title': 'Restricted version of vertex cover', 'LastActivityDate': '2012-06-15T08:56:53.127', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'hbm', 'PostTypeId': '1', 'Tags': '<complexity-theory><algorithms><graph-theory>', 'CreationDate': '2012-06-09T12:55:36.820', 'FavoriteCount': '1', 'Id': '2302'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was trying to write some simple code for a "flow layout" manager and what I came up with initially was something like the following (semi-pseudocode):</p>\n\n<pre><code>int rowHeight = 0;\nRECT rect = parent.getClientRect();\nPOINT pos = rect.position;  // Start at top-left corner, row by row\n\nforeach (Window child in parent.children)\n{\n    // POINT is a tuple of: (x, y)\n    // SIZE is a tuple of: (width, height)\n    // RECT is a tuple of: (left, top, right, bottom)\n    RECT proposed1 = RECT(rect.left + pos.x, rect.top + pos.y, rect.right, rect.bottom),\n         proposed2 = RECT(rect.left, rect.top + pos.y + rowHeight, rect.right, rect.bottom);\n    SIZE size1 = child.getPreferredSize(proposed1),\n         size2 = child.getPreferredSize(proposed2);\n    if (size1.width &lt;= proposed1.width)\n    {\n        child.put(proposed1);  // same row\n        pos.x += size1.width;\n        rowHeight = max(rowHeight, size1.height);\n    }\n    else\n    {\n        child.put(proposed2);  // new row\n        pos.x = rect.left;\n        pos.y += rowHeight;\n        rowHeight = size2.height;\n    }\n}\n</code></pre>\n\n<p>In other words, the algorithm is very simple:<br>\nThe layout manager asks every component, "is the remaining portion of the row enough for you?" and, if the component says "no, my width is too long", it places the component on the next row instead.</p>\n\n<p>There are two major problems with this approach:</p>\n\n<ul>\n<li><p>This algorithm results in very long, thin components, because it is essentially greedy with the width of each component -- if a component wants the whole row, it will use the whole row (ugly), even if it could use a smaller width (but larger height).</p></li>\n<li><p>It only works if you already <em>know</em> what the parent\'s size is -- but you might not! Instead, you might simply have a restriction, "the parent\'s size must be between these two dimensions", but the rest might be open-ended.</p></li>\n</ul>\n\n<p>I am, however, at a loss of how to come up with a better algorithm -- how do I figure out what would be a good size to to \'propose\' to the component?   And even when I figure that out, what should I try to optimize, exactly? (The area, the width, the aspect ratio, the number of components on the screen, or something else?)</p>\n\n<p>Any ideas on how I should approach this problem?</p>\n', 'ViewCount': '280', 'Title': '"Flow layouts" inside a GUI -- how do I come up with a good algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-10T11:26:48.527', 'LastEditDate': '2012-06-10T11:26:48.527', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<algorithms><computational-geometry><greedy-algorithms><user-interface>', 'CreationDate': '2012-06-09T23:51:40.070', 'FavoriteCount': '0', 'Id': '2306'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am aware that for a problem to be considered NP-Hard, any problem in NP must be reduceable to your problem (problem which you are trying to prove is NP-Hard).</p>\n\n<p>Let's assume that you have proven that a problem <code>Y</code> is NP-Hard, and you have a problem <code>X</code> which you know is in NP, and you would like to solve.</p>\n\n<p>To solve <code>X</code>, which of the following reductions would be carried out?</p>\n\n<ol>\n<li>X -> Y</li>\n<li>Y -> X</li>\n</ol>\n\n<p>Which of the following? i.e. would you reduce <code>X</code> to <code>Y</code> or vice-versa, if you would like to solve <code>X</code> which is in NP, and <code>Y</code> which is NP-Hard?</p>\n", 'ViewCount': '222', 'Title': 'Solve a problem through reduction', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-10T12:45:26.697', 'LastEditDate': '2012-06-10T11:29:36.043', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '1814', 'Tags': '<algorithms><complexity-theory><reductions><np-hard>', 'CreationDate': '2012-06-10T08:06:59.850', 'Id': '2312'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '749', 'Title': 'Sorting algorithms which accept a random comparator', 'LastEditDate': '2012-06-12T20:52:01.087', 'AnswerCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': u'<p>Generic sorting algorithms generally take a set of data to sort and a comparator function which can compare two individual elements. If the comparator is an order relation\xb9, then the output of the algorithm is a sorted list/array.</p>\n\n<p>I am wondering though which sort algorithms would actually <em>work</em> with a comparator that is not an order relation (in particular one which returns a random result on each comparison). By "work" I mean here that they continue return a permutation of their input and run at their typically quoted time complexity (as opposed to degrading to the worst case scenario always, or going into an infinite loop, or missing elements). The ordering of the results would be undefined however. Even better, the resulting ordering would be a uniform distribution when the comparator is a coin flip.</p>\n\n<p>From my rough mental calculation it appears that a merge sort would be fine with this and maintain the same runtime cost and produce a fair random ordering. I think that something like a quick sort would however degenerate,  possibly not finish, and not be fair.</p>\n\n<p>What other sorting algorithms (other than merge sort) would work as described with a random comparator?</p>\n\n<hr>\n\n<ol>\n<li><p>For reference, a comparator is an order relation if it is a proper function (deterministic) and satisfies the axioms of an order relation:</p>\n\n<ul>\n<li>it is deterministic: <code>compare(a,b)</code> for a particular <code>a</code> and <code>b</code> always returns the same result.</li>\n<li>it is transitive: <code>compare(a,b) and compare(b,c) implies compare( a,c )</code></li>\n<li>it is antisymmetric <code>compare(a,b) and compare(b,a) implies a == b</code></li>\n</ul></li>\n</ol>\n\n<p>(Assume that all input elements are distinct, so reflexivity is not an issue.)</p>\n\n<p>A random comparator violates all of these rules. There are however comparators that are not order relations yet are not random (for example they might violate perhaps only one rule, and only for particular elements in the set).</p>\n', 'Tags': '<algorithms><randomized-algorithms><sorting>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-03-20T18:11:29.530', 'CommentCount': '11', 'AcceptedAnswerId': '2349', 'CreationDate': '2012-06-12T11:39:54.473', 'Id': '2336'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Are there any problems in $\\mathsf{P}$ that have randomized algorithms beating lower bounds on deterministic algorithms? More concretely, do we know any $k$ for which $\\mathsf{DTIME}(n^k) \\subsetneq \\mathsf{PTIME}(n^k)$? Here $\\mathsf{PTIME}(f(n))$ means the set of languages decidable by a randomized TM with constant-bounded (one or two-sided) error in $f(n)$ steps. </p>\n\n<blockquote>\n  <p>Does randomness buy us anything inside $\\mathsf{P}$?</p>\n</blockquote>\n\n<p>To be clear, I am looking for something where the difference is asymptotic (preferably polynomial, but I would settle for polylogarithmic), not just a constant.</p>\n\n<p><em>I am looking for algorithms asymptotically better in the worst case. Algorithms with better expected complexity are not what I am looking for. I mean randomized algorithms as in RP or BPP not ZPP.</em></p>\n', 'ViewCount': '332', 'Title': 'Problems in P with provably faster randomized algorithms', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-29T08:14:04.840', 'LastEditDate': '2013-07-29T08:08:49.313', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '208', 'Tags': '<algorithms><complexity-theory><randomized-algorithms>', 'CreationDate': '2012-06-13T13:44:19.383', 'FavoriteCount': '4', 'Id': '2362'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '815', 'Title': 'How to describe algorithms, prove and analyse them?', 'LastEditDate': '2012-06-16T12:29:53.190', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1715', 'FavoriteCount': '4', 'Body': "<p>Before reading <em>The Art of Computer Programming (TAOCP)</em>, I have not considered these questions deeply. I would use pseudo code to describe algorithms, understand them and estimate the running time only about orders of growth. The <em>TAOCP</em> thoroughly changes my mind.</p>\n\n<p><em>TAOCP</em> uses English mixed with steps and <em>goto</em> to describe the algorithm, and uses flow charts to picture the algorithm more readily. It seems low-level, but I find that there's some advantages, especially with flow chart, which I have ignored a lot. We can label each of the arrows with an assertion about the current state of affairs at the time the computation traverses that arrow, and make an inductive proof for the algorithm. The author says:</p>\n\n<blockquote>\n  <p>It is the contention of the author that we really understand why an algorithm is valid only when we reach the point that our minds have implicitly filled in all the assertions, as was done in Fig.4.</p>\n</blockquote>\n\n<p>I have not experienced such stuff. Another advantage is that, we can count the number of times each step is executed. It's easy to check with Kirchhoff's first law. I have not analysed the running time exactly, so some $\\pm1$ might have been omitted when I was estimating the running time.</p>\n\n<p>Analysis of orders of growth is sometimes useless. For example, we cannot distinguish quicksort from heapsort because they are all $E(T(n))=\\Theta(n\\log n)$, where $EX$ is the expected number of random variable $X$, so we should analyse the constant, say, $E(T_1(n))=A_1n\\lg n+B_1n+O(\\log n)$ and $E(T_2(n))=A_2\\lg n+B_2n+O(\\log n)$, thus we can compare $T_1$ and $T_2$ better. And also, sometimes we should compare other quantities, such as variances. Only a rough analysis of orders of growth of running time is not enough. As <em>TAOCP</em> translates the algorithms into assembly language and calculate the running time, It's too hard for me, so I want to know some techniques to analyse the running time a bit more roughly, which is also useful, for higher-level languages such as C, C++ or pseudo codes.</p>\n\n<p>And I want to know what style of description is mainly used in research works, and how to treat these problems.</p>\n", 'Tags': '<algorithms><proof-techniques><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T15:44:06.610', 'CommentCount': '9', 'AcceptedAnswerId': '2390', 'CreationDate': '2012-06-14T13:56:56.550', 'Id': '2374'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was trying to come up with a system that would evaluate bylaws for an organization as to determine their underlying logic.</p>\n\n<p>I think a first-order predicate system would work for representing the rules, which could be translated from the text via part-of-speech tagging and other NLP techniques.  </p>\n\n<p>Is there a systematic way to interpret the first-order logic rules as a whole, or some type of ML architecture that would work as a second layer to find similarities between the elements.</p>\n\n<p>For example,</p>\n\n<blockquote>\n  <p>List of fun activities:</p>\n  \n  <ul>\n  <li>golf</li>\n  <li>coffee break</li>\n  <li>pizza</li>\n  </ul>\n  \n  <p>Bylaws:</p>\n  \n  <ol>\n  <li><p>On Friday, we play golf</p></li>\n  <li><p>On Friday or Saturday, we take a quick coffee break, and if it's Saturday, we get pizza</p></li>\n  </ol>\n</blockquote>\n\n<p>Conclusion: our group has fun on weekends</p>\n\n<p>It sounds far fetched, but I'm curious if it's possible.  I also realize that perhaps more first-order logic would be a better fit for driving the conclusions of the second layer.  </p>\n", 'ViewCount': '80', 'Title': 'Methods to evaluate a system of written rules', 'LastActivityDate': '2012-08-13T22:49:33.717', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '3160', 'Score': '10', 'OwnerDisplayName': 'jonsca', 'PostTypeId': '1', 'OwnerUserId': '88', 'Tags': '<machine-learning><algorithms><pattern-recognition><logic>', 'CreationDate': '2012-02-16T07:32:59.457', 'Id': '2382'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>A graph is 2\u2200-connected if it remains connected even if any single edge is removed. Let G = (V, E) be a connected undirected graph. Develop an algorithm as fast as possible to check 2\u2200-connectness of G.</p>\n\n<p>I know the basic idea is to build a DFS searching tree and then check each edge is not on a circle with DFS. Any help would be appreciated.</p>\n\n<p>What I expect to see is a detailed algorithm description(especially the initialization of needed variables which is obscure sometimes), complexity analysis could be omitted.</p>\n', 'ViewCount': '100', 'Title': u'Algorithm to check the 2\u2200-connectness property of a graph', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-17T13:44:12.877', 'LastEditDate': '2012-06-17T13:39:54.627', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2397', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1875', 'Tags': '<algorithms><graphs><efficiency>', 'CreationDate': '2012-06-16T15:33:06.990', 'Id': '2394'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the book <a href="http://www.cs.uu.nl/geobook/">"Computational Geometry: Algorithms and Applications"</a> by Mark de Berg et al., there is a very simple brute force algorithm for computing Delaunay triangulations. The algorithm uses the notion of <em>illegal edges</em> -- edges that may not appear in a valid Delaunay triangulation and have to be replaced by some other edges. On each step, the algorithm just finds these illegal edges and performs required displacements (called <em>edge flips</em>) till there are no illegal edges.</p>\n\n<blockquote>\n  <p>Algorithm <strong>LegalTriangulation</strong>($T$)</p>\n  \n  <p><em>Input</em>. Some triangulation $T$ of a point set $P$.<br>\n  <em>Output</em>. A legal triangulation of $P$.</p>\n  \n  <p><strong>while</strong> $T$ contains an illegal edge $p_ip_j$<br>\n  <strong>do</strong><br>\n  $\\quad$ Let $p_i p_j p_k$ and $p_i p_j p_l$ be the two triangles adjacent to $p_ip_j$.<br>\n  $\\quad$ Remove $p_ip_j$ from $T$, and add $p_kp_l$ instead.<br/>\n  <strong>return</strong> $T$.</p>\n</blockquote>\n\n<p>I\'ve heard that this algorithm runs in $O(n^2)$ time in worst case; however, it is not clear to me whether this statement is correct or not. If yes, how can one prove this upper bound?</p>\n', 'ViewCount': '769', 'Title': 'Brute force Delaunay triangulation algorithm complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-21T13:44:11.817', 'LastEditDate': '2012-06-17T13:28:32.350', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><time-complexity><algorithm-analysis><computational-geometry><runtime-analysis>', 'CreationDate': '2012-06-16T22:01:33.543', 'FavoriteCount': '2', 'Id': '2400'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '912', 'Title': 'Shortest distance between a point in A and a point in B', 'LastEditDate': '2012-06-20T07:35:54.063', 'AnswerCount': '4', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1170', 'FavoriteCount': '1', 'Body': '<blockquote>\n  <p>Given two sets $A$ and $B$ each containing $n$ disjoint points\n  in the plane, compute the shortest distance between a point in $A$ and a point in $B$, i.e., $\\min \\space \\{\\mbox{ } \\text{dist}(p, q) \\mbox{ } | \\mbox{ } p \\in A \\land q \\in B \\space \\} $.</p>\n</blockquote>\n\n<p>I am not sure if I am right, but this problem very similar to problems that can be solved by linear programming in computational geometry. However, the reduction to LP is not straightforward. Also my problem looks related to finding the thinnest stip between two sets of points which obviously can be solved by LP in $O(n)$ in 2-dimensional space.</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-06-23T08:50:58.580', 'CommentCount': '4', 'AcceptedAnswerId': '2416', 'CreationDate': '2012-06-19T18:28:30.493', 'Id': '2415'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have one puzzle whose answer I have boiled down to finding the total number and which type of permutation they are.</p>\n\n<p>For example if the string is of length ten as $w = aabbbaabba$, the total number of permutations will be </p>\n\n<p>$\\qquad \\displaystyle \\frac{|w|}{|w|_a! \\cdot |w|_b!} = \\frac{10!}{5!\\cdot 5!}$</p>\n\n<p>Now had the string been of distinct characters, say $w'=abcdefghij$, I would have found the permutations by this algorithm : </p>\n\n<pre><code>for i = 1 to |w|\n  w = rotate(w)\nw = rotate(w)\nreturn w.head + rotate(w.tail)\n</code></pre>\n\n<p>Can some one throw new ideas on this - how to find the number of permutations for a string having repeated characters? Is there any other mathematical/scientific name of for what I am trying to do?</p>\n", 'ViewCount': '1544', 'Title': 'Finding the number of distinct permutations of length N with n different symbols', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-24T11:51:51.870', 'LastEditDate': '2012-06-22T09:14:31.023', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'softy', 'PostTypeId': '1', 'OwnerUserId': '1942', 'Tags': '<algorithms><combinatorics><strings><word-combinatorics>', 'CreationDate': '2012-06-21T16:17:23.717', 'Id': '2443'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/2302/restricted-version-of-vertex-cover">Restricted version of vertex cover</a>  </p>\n</blockquote>\n\n\n\n<p>Suppose we have a $(A,B,E)$ bipartite graph and a positive integer k. Suppose that k is smaller than $|A|$ and we want to find one of those k element subsets of $A$ which covers the most points from $B$. I can only come up wirh exponential algorithms. Is this in $P$?</p>\n\n<p>Also is mixed integer programming for maximal flows in $P$? It can be easily formulated as such.</p>\n', 'ViewCount': '36', 'ClosedDate': '2012-06-22T09:32:51.747', 'Title': 'constrained cover on biparite graphs', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-25T17:24:17.437', 'LastEditDate': '2012-06-25T17:24:17.437', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1931', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-06-21T21:36:08.990', 'Id': '2444'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can I prove that the cover time for a directed graph G can be exponential in the size of G?</p>\n', 'ViewCount': '161', 'Title': 'Prove: cover time for directed graph is exponential', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-27T02:51:16.120', 'LastEditDate': '2013-01-27T02:51:16.120', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><graph-theory><random-walks>', 'CreationDate': '2012-06-22T12:29:32.200', 'Id': '2449'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>For a given planar graph $G(V,E)$ embedded in the plane, defined by list of segments $E= \\left \\{ e_1,...,e_m \\right \\} $, each segment $e_i$ is represented by its endpoints $\\left \\{ L_i,R_i \\right \\}$. Construct a DCEL data structure for the planar subdivision, describe an algorithm, prove it\'s  correctness and show the complexity.</p>\n</blockquote>\n\n<p>More information about DCEL (double connected edge list) you can find on <a href="http://en.wikipedia.org/wiki/DCEL" rel="nofollow">wikipedia - DCEL</a>.</p>\n\n<p>According to description of DCEL and connections between different objects of DCEL (vertices, edges and faces) the required data structure must be complicated.</p>\n\n<p>I found that <em>doubly-linked lists</em> can be used as data structure for DCEL, I am not sure how to build and maintain connections between vertices - edges and edges - faces.</p>\n\n<p>I tried to find any hint in textbook, but the construction of DCEL wasn\'t described, map overlay is more popular topic.</p>\n\n<p>Regarding the algorithm, plane sweep algorithm with $O((n+l)\\log n)$ should do the job, but it seems to be overkill, because segments are intersected not in arbitrary points, but only in endpoints, therefore $O(n\\log n)$ seems more reasonable.</p>\n\n<p>The main problem is the data structure, so far I haven\'t seen any good example with at least similar complexity.</p>\n\n<p>Please, if you have any idea about a data structure for DCEL or about algorithm for constructing DCEL, share it with us.</p>\n', 'ViewCount': '1495', 'Title': 'Constructing of Double Connected Edge List (DCEL)', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-09-22T03:25:45.343', 'LastEditDate': '2012-06-22T15:37:35.383', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2516', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><data-structures><computational-geometry><lists>', 'CreationDate': '2012-06-22T12:58:26.957', 'Id': '2450'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Given a set $S$ of points $p_1,..,p_2$ give the most efficient algorithm for determining if any 3 points of the set are collinear.</p>\n</blockquote>\n\n<p>The problem is I started with general definition but I cannot continue to actually solving the problem.</p>\n\n<p>What can we say about collinear points in general, 3 points $a,b,c$ are collinear if the distance $d(a,c) = d(a,b)+d(b,c)$ in the case when $b$ is between $a$ and $c$.</p>\n\n<p>The naive approach has $O(n(n-1)(n-2))=O(n^3)$ time complexity.</p>\n\n<p>How to solve this problem, what should be the next step?</p>\n', 'ViewCount': '756', 'Title': 'If any 3 points are collinear', 'LastActivityDate': '2012-06-22T18:23:20.000', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2456', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-06-22T17:09:13.053', 'FavoriteCount': '1', 'Id': '2453'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Apologies for another Markov Chain question but this one is best given its own question to avoid confusion. I am using a Markov Chain to get the 10 best search results from the union of 3 different search engines. The top 10 results are taken from each engine to form a set of 30 results.</p>\n\n<p>The chain starts at State x, a uniform distribution of set S = {1,2,3,...30}. If the current state is page P, select page Q uniformly from the union of the results from each search engine. If the rank of Q &lt; rank of P in 2 of the 3 engines that rank both P and Q, move to Q. Else, remain at P. </p>\n\n<p>This results in a number of pairwise comparisons being carried out. result2 is compared with result1 and a count is made of each time result2 ranks better than 1. The results are sorted by the results of the pairwise comparisons, with the lowest score ranked first. e.g.</p>\n\n<pre>\nEngine Rankings:                       Pairwise Comparison:\n         eng1   eng2   eng3                    result1  result2  result3  result4  result5\nresult1   1      2      2              result1    0        1        0        0        1\nresult2   4      3      1              result2    2        0        1        2        2\nresult3   2      4      5              result3    3        2        0        1        2\nresult4   5      5      3              result4    3        1        2        0        1\nresult5   3      1      4              result5    2        1        1        2        0\n\n</pre>\n\n<p>The problem with this example is, if we add the total of each row in the pairwise comparison, we get {2,7,8,7,7}, leaving 3 different results with the same score. I'm wondering if there is a method to further sort these results in order to refine the results so that I'm not left with a number of results that have the same score? I've seen Keminization but I can't see how this would apply? Can someone please give me some guidance?</p>\n", 'ViewCount': '107', 'Title': 'Improve Markov Chain results', 'LastActivityDate': '2012-06-23T06:01:57.383', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1916', 'Tags': '<algorithms><machine-learning><markov-chains>', 'CreationDate': '2012-06-22T19:37:01.610', 'Id': '2457'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I've been given the following problem:</p>\n\n<p>Given a data structure $M$ that is based on comparisons and supports the following methods on a group of numbers $S$:</p>\n\n<ul>\n<li>$\\text{Insert}(x)$ \u2013 add $x$ to $S$</li>\n<li>$\\text{Extract_min}()$ \u2013 remove the minimal element in $S$ and return it </li>\n</ul>\n\n<p>We can implement with a heap the above methods in $O(\\log n)$, however, we're looking at \na bigger picture, a general case that we have no guarantee that $M$ is indeed a heap. Prove that \nno matter what kind of data structure $M$ is, that <strong>at least one</strong> of the methods that $M$ supports must take $\\Omega(\\log n )$.</p>\n\n<p><strong>My solution:</strong></p>\n\n<p>Each sorting algorithm that is based on comparisons must take at the worst case at least $\\Omega(n\\log n)$ \u2013 we'll prove that using a decision tree: if we look at any given algorithm that is based on comparisons, as a binary tree where each vertex is a <em>compare-method</em> between 2 elements: </p>\n\n<ul>\n<li>if the first is bigger than the second element \u2013 we'll go to the left child</li>\n<li>if the second is bigger than the first element \u2013 we'll go to the right child</li>\n</ul>\n\n<p>At the end, we'll have $n!$ leaves that are the options for sorting the elements.</p>\n\n<p>The height of the tree is $h$, then:</p>\n\n<p>$$2^h \\ge n! \\quad\\Longrightarrow\\quad \\log(2^h) &gt;= \\log(n!) \\quad\\Longrightarrow\\quad h \\ge \\log(n!) \\quad\\Longrightarrow\\quad h = \\Omega(n \\log n)$$</p>\n\n<p>Then, if we have a $\\Omega(n \\log n)$ worst case for $n$ elements, then we have a $\\Omega(\\log n)$ for a single element. </p>\n\n<p>I'm not sure regarding this solution, so I'd appreciate for corrections or anything else \nyou can come up with. </p>\n", 'ViewCount': '311', 'Title': 'Prove that for a general data structure - operations Extract_min() and Insert(x) cost $\\Omega(\\log n)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-26T12:42:03.597', 'LastEditDate': '2012-06-26T12:42:03.597', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '776', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'CreationDate': '2012-06-23T13:38:12.030', 'FavoriteCount': '0', 'Id': '2459'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>First of all we must read a word, and a desired size.<br>\nThen we need to find the longest palindrome created by characters in this word used in order.<br>\nFor example for size = 7 and word = "abcababac" the answer is 7 ("abababa").   </p>\n\n<p>Postscript: the size of the word is smaller than 3000.</p>\n', 'ViewCount': '3096', 'Title': 'Fastest algorithm for finding the longest palindrome subsequence', 'LastEditorUserId': '39', 'LastActivityDate': '2013-03-22T21:13:31.333', 'LastEditDate': '2012-10-11T21:21:11.817', 'AnswerCount': '3', 'CommentCount': '8', 'Score': '6', 'OwnerDisplayName': 'Lin Yon Xong', 'PostTypeId': '1', 'Tags': '<algorithms><strings><subsequences>', 'CreationDate': '2012-06-18T14:11:18.557', 'FavoriteCount': '1', 'Id': '2466'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '131', 'Title': 'Efficient bandwidth algorithm', 'LastEditDate': '2012-06-25T13:00:38.987', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1289', 'FavoriteCount': '0', 'Body': "<p>Recently I sort of stumbled on a problem of finding an efficient topology given a weighted directed graph. Consider the following scenario:</p>\n\n<ol>\n<li><p>Node 1 is connected to 2,3,4 at 50 Mbps. Node 1 has 100 Mbps network card.</p></li>\n<li><p>Node 3 is connected to 5 at 50 Mbps. Node 3 has 100 Mbps card.</p></li>\n<li><p>Node 4 is connected to Node 3 at 40 Mbps. Node 4 has 100 Mbps card.</p></li>\n</ol>\n\n<p>(Sorry about not having a picture)</p>\n\n<p>Problem: If Node 1 starts sending data to its immediate nodes (2 and 3), we can clearly see it's network card capacity will be drained out after Node 3. Whereas if it were to <em>skip</em> node 3 and start sending to node 4, the data will eventually reach to node 3 via 4 and hence, node 5 will be getting data via node 3.\nThe problem becomes more complicated if all the links were of 50 Mbps and we can clearly see that node 2 and node 4 are the only way to reach all nodes.</p>\n\n<p>Question: Is there an algorithm which gives the optimal path to ALL nodes keeping the network (card) capacity in mind? </p>\n\n<p>I read the shortest path algorithm,max flow algorithms but none of them seem to address my problems. perhaps,im missing something. I'll appreciate if someone can help me out.</p>\n", 'Tags': '<algorithms><graph-theory><optimization><linear-programming>', 'LastEditorUserId': '29', 'LastActivityDate': '2012-06-25T13:00:38.987', 'CommentCount': '2', 'AcceptedAnswerId': '2480', 'CreationDate': '2012-06-24T15:25:59.813', 'Id': '2470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $\\oplus$ be bitwise xor. Let $k,a,b$ be non-negative integers. $[a..b]=\\{x\\mid a\\leq x, x\\leq b\\}$, it is called a integer interval.</p>\n\n<p>What is a fast algorithm to find \n$\\{ k\\oplus x\\mid x\\in [a..b]\\}$ as a union of set of integer intervals.</p>\n\n<p>One can prove that $[a+k..b-k]\\subseteq \\{ k\\oplus x\\mid x\\in [a..b]\\}$ by showing that $x-y\\leq x\\oplus y \\leq x+y$.</p>\n\n<p><strong>Edit:</strong> I should specify the actually input and output to remove ambiguity.</p>\n\n<p>Input: $k, a, b$.</p>\n\n<p>Output: $a_1, b_1, a_2, b_2,\\ldots,a_m,b_m$. Such that:</p>\n\n<p>$$\n\\{ k\\oplus x\\mid x\\in [a..b]\\} = \\bigcup_{i=1}^m [a_i..b_i]\n$$</p>\n', 'ViewCount': '412', 'Title': 'What is the bitwise xor of an interval?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-06-25T09:12:09.557', 'LastEditDate': '2012-06-24T22:46:43.727', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '2473', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms><integers>', 'CreationDate': '2012-06-24T16:39:36.453', 'Id': '2471'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Most books explain the reason the algorithm doesn't work with negative edges as nodes are deleted from the priority queue after the node is arrived at since the algorithm assumes the shortest distance has been found. However since negative edges can reduce the distance, a future shorter distance might be found; but since the node is deleted it cannot be updated.</p>\n\n<p>Wouldn't an obvious solution to this be to <em>not delete the node</em>? Why not keep the node in the queue, so if a future <em>shorter</em> distance is found, it can be updated? If I am misunderstanding the problem, what <em>is</em> preventing the algorithm from being used with negative edges?</p>\n", 'ViewCount': '4441', 'Title': "Using Dijkstra's algorithm with negative edges?", 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-16T02:00:29.480', 'LastEditDate': '2012-06-25T17:22:17.767', 'AnswerCount': '4', 'CommentCount': '9', 'Score': '4', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2012-06-24T03:13:44.967', 'Id': '2482'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In Cormen et. al., <em>Introduction to Algorithms</em> (3rd ed.), I don't get a line in the proof of Lemma 26.1  which states that the augmented flow $f\\uparrow f'$ is a flow in $G$ and is s.t. $|f\\uparrow f'| =|f|+|f'|$ (this is pp. 717-718).</p>\n\n<p>My confusion: When arguing <em>flow-conservation</em> they use the definition of $f\\uparrow f'$ in the first line to say that for each $u\\in V\\setminus\\{s,t\\}$</p>\n\n<p>$$ \\sum_{v\\in V} (f\\uparrow f')(u,v) = \\sum_{v\\in V} (f(u,v)+f'(u,v) - f'(v,u)), $$</p>\n\n<p>where the augmented path is defined as</p>\n\n<p>$$ (f\\uparrow f')(u,v) = \\begin{cases} f(u,v)+f'(u,v) - f'(v,u) &amp; \\text{if $(u,v)\\in E$}, \\\\\n0 &amp; \\text{otherwise}. \\end{cases} $$</p>\n\n<p>Why can they ignore the 'otherwise' clause in the summation? I don't think the first clause evaluates to zero in all such cases. Do they use flow-conservation of $f$ and $f'$ in some way?</p>\n", 'ViewCount': '273', 'Title': "CLRS - Maxflow Augmented Flow Lemma 26.1 - don't understand use of def. in proof", 'LastEditorUserId': '1964', 'LastActivityDate': '2012-06-25T23:11:59.127', 'LastEditDate': '2012-06-25T23:11:59.127', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2494', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1964', 'Tags': '<algorithms><network-flow>', 'CreationDate': '2012-06-25T17:14:37.713', 'Id': '2492'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve finished most of the material in Cormen\'s Intro to Algorithms book and I am looking for an algorithms book that covers material beyond Corman\'s book. Are there any recommendations?</p>\n\n<p>NOTE: I asked this on stackoverflow but wasn\'t all too happy with the answer. </p>\n\n<p>NOTE: Looking at most of the comments I think ideally I would like to find a book that would cover the material of the the 787 course in <a href="http://www.cs.wisc.edu/academic-programs/courses/cs-course-descriptions">this course description</a>.</p>\n', 'ViewCount': '918', 'Title': 'Book for algorithms beyond Cormen', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-27T12:45:45.290', 'LastEditDate': '2012-06-27T12:45:45.290', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '2505', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1790', 'Tags': '<algorithms><reference-request><books>', 'CreationDate': '2012-06-26T02:30:47.140', 'Id': '2495'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is the question: suppose we are given x cents, the amount we want to pay, and a 6-tuple (p, n, d, q, l, t) that represents respectively the number of pennies, nickels, dimes, quarters, loonies and toonies you have. Assume that you have enough coins to pay x cents. You do not have to pay exactly x cents; you can pay more. The cashier is assumed to be smart enough to give you back the optimal number of coins as change. We want to minimize the number of coins that changes hands, that is the number of coins you give to the cashier plus the number of coins the cashier gives back to you.</p>\n\n<p>For example, if we want to pay 99 cents and we have 99 pennies and 1 loonie, then the optimal solution would be to give the cashier the loonie and take back 1 penny.</p>\n\n<p>A particularly easy solution that occurs to me is to create a six-dimensional array. But in practice this is not feasible. So I am wondering if anyone can give me a small hint as to how to use dynamic programming to solve this (as this question looks intuitively to me like a DP problem). Once I have a hint, I can perhaps work out the remaining details myself. Thanks.</p>\n', 'ViewCount': '467', 'Title': 'How to use dynamic programming to solve this?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-27T13:23:56.797', 'LastEditDate': '2012-06-27T13:13:22.973', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1718', 'Tags': '<algorithms><optimization><dynamic-programming>', 'CreationDate': '2012-06-26T23:26:55.670', 'Id': '2507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a directed weighted graph $G = (V, E, W)$. There is always an edge from a vertex $i$ to another one $j$, the weight $w(i,j)$ could be positive infinity, and there does not exist any negative cycle. </p>\n\n<p>An execution of some algorithms will find the lengths (summed weights) of the shortest paths between all pairs of vertices though it does not return details of the paths themselves. For instance, <a href="http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm" rel="nofollow">Floyd\u2013Warshall algorithm</a> is straightforward, and it works. Let us denote the result by $G\' = (V, E, W\')$.</p>\n\n<p>In $G\'$, it is possible that for an edge from $i$ to $j$, $w\'(i,j) = w\'(i, k_0) + w\'(k_0, k_1) + \\dots + w\'(k_n, j)$. Let us make from $G\'$ another graph $G\'\'$ whose any element is same as $G\'$ except $w\'\'(i,j) = \\infty \\neq w\'(i,j)$. Therefore we know that an execution of a shortest paths algorithm on $G\'\'$ will give $G\'$.</p>\n\n<p>So given a $G\'$, I would like to find all the graphs like $G\'\'$, such that for all $i$ and $j$, $w\'\'(i,j) \\in \\{ w\'(i,j), \\infty\\}$, and $G\'\'$ can be reduced to $G\'$ via a shortest paths algorithm.</p>\n\n<p>Hope my question is clear... I do not know if an algorithm for this exists already, does anyone have any idea?</p>\n', 'ViewCount': '71', 'Title': 'Find all the special graphs which can reduced to the shortest paths graph', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-27T13:50:18.340', 'LastEditDate': '2012-06-27T13:02:58.253', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'OwnerDisplayName': 'SoftTimur', 'PostTypeId': '1', 'OwnerUserId': '5008', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2011-12-26T05:19:48.980', 'Id': '2511'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given two strings, $r$ and $s$, where $n = |r|$, $m = |s|$ and $m \\ll n$, find the minimum edit distance between $s$ for each beginning position in $r$ efficiently.</p>\n\n<p>That is, for each suffix of $r$ beginning at position $k$, $r_k$, find the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> of $r_k$ and $s$ for each $k \\in [0, |r|-1]$.  In other words, I would like an array of scores, $A$, such that each position, $A[k]$, corresponds to the score of $r_k$ and $s$.</p>\n\n<p>The obvious solution is to use the standard dynamic programming solution for each $r_k$ against $s$ considered separately, but this has the abysmal running time of $O(n m^2)$ (or $O(n d^2)$, where $d$ is the maximum edit distance).  It seems like you should be able to re-use the information that you\'ve computed for $r_0$ against $s$ for the comparison with $s$ and $r_1$.</p>\n\n<p>I\'ve thought of constructing a prefix tree and then trying to do dynamic programming algorithm on $s$ against the trie, but this still has worst case $O(n d^2)$ (where $d$ is the maximum edit distance) as the trie is only optimized for efficient lookup.</p>\n\n<p>Ideally I would like something that has worst case running time of $O(n d)$ though I would settle for good average case running time.  Does anyone have any suggestions?  Is $O(n d^2)$ the best you can do, in general?</p>\n\n<p>Here are some links that might be relevant though I can\'t see how they would apply to the above problem as most of them are optimized for lookup only:</p>\n\n<ul>\n<li><a href="http://stevehanov.ca/blog/index.php?id=114" rel="nofollow">Fast and Easy Levensthein distance using a Trie</a></li>\n<li><a href="http://stackoverflow.com/questions/3183149/most-efficient-way-to-calculate-levenshtein-distance">SO: Most efficient way to calculate Levenshtein distance</a></li>\n<li><a href="http://stackoverflow.com/questions/4057513/levenshtein-distance-algorithm-better-than-onm?rq=1">SO: Levenshtein Distance Algoirthm better than $O(n m)$</a></li>\n<li><a href="http://www.berghel.net/publications/asm/asm.php" rel="nofollow">An extension of Ukkonen\'s enhanced dynamic programming ASM algorithm</a></li>\n<li><a href="http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata" rel="nofollow">Damn Cool Algorithms: Levenshtein Automata</a></li>\n</ul>\n\n<p>I\'ve also heard some talk about using some type of distance metric to optimize search (such as a <a href="http://en.wikipedia.org/wiki/BK-tree" rel="nofollow">BK-tree</a>?) but I know little about this area and how it applies to this problem.</p>\n', 'ViewCount': '836', 'Title': 'Efficiently calculating minimum edit distance of a smaller string at each position in a larger one', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-28T15:40:06.263', 'LastEditDate': '2012-06-28T15:40:06.263', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '2526', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '67', 'Tags': '<algorithms><runtime-analysis><strings><dynamic-programming><string-metrics>', 'CreationDate': '2012-06-27T20:48:29.300', 'Id': '2519'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '383', 'Title': 'Getting parallel items in dependency resolution', 'LastEditDate': '2012-06-28T22:30:03.503', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1995', 'FavoriteCount': '2', 'Body': '<p>I have implemented a topological sort based on the <a href="http://en.wikipedia.org/wiki/Topological_sort" rel="nofollow">Wikipedia article</a> which I\'m using for dependency resolution, but it returns a linear list. What kind of algorithm can I use to find the independent paths?</p>\n', 'Tags': '<algorithms><graphs><parallel-computing><scheduling>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-06-28T22:30:03.503', 'CommentCount': '1', 'AcceptedAnswerId': '2525', 'CreationDate': '2012-06-28T09:12:35.827', 'Id': '2524'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an algorithm, which given a graph $G$ and a natural number $t$, determines if $G$ is <a href="http://en.wikipedia.org/wiki/Symmetric_graph">$t$-transitive</a>.</p>\n\n<p>I am also interested in knowing if this problem is in P, NP, NPC or some other interesting facts about its complexity class.</p>\n', 'ViewCount': '105', 'Title': 'Algorithm to test a graph for $t$-transitivity', 'LastEditorUserId': '1350', 'LastActivityDate': '2012-07-01T19:45:15.080', 'LastEditDate': '2012-06-28T13:52:50.560', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1350', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2012-06-28T12:59:46.923', 'Id': '2527'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So I have a problem understanding how the <a href="http://courses.cs.vt.edu/csonline/OS/Lessons/MemoryAllocation/index.html" rel="nofollow">worst-fit protocol for memory allocation</a> reacts to contiguous blocks of empty memory.  None of the examples I have found address this possibility.</p>\n\n<p>For example, say you have the following blocks (where \'O\' stands for occupied block and \'E\' stands for empty block) and are to allocate 10 MB via the worst-fit algorithm:</p>\n\n<pre><code>------------------------------------------------------------\n|10 MB O | 40 MB E | 10 MB O | 20 MB E | 30 MB E | 10 MB O |\n------------------------------------------------------------\n----0---------1---------2---------3---------4---------5-----\n</code></pre>\n\n<p>My question is does the worst-fit algorithm select block one leaving behind a 30 MB hole in block 1, or does it select block 3 leaving behind a cumulative 40 MB hole between blocks 3 and 4?     </p>\n', 'ViewCount': '1026', 'Title': 'How does worst-fit memory allocation react when encountering contiguous empty memory blocks?', 'LastEditorUserId': '29', 'LastActivityDate': '2014-03-23T07:51:23.327', 'LastEditDate': '2012-06-30T10:17:00.327', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '603', 'Tags': '<algorithms><operating-systems><memory-allocation>', 'CreationDate': '2012-06-28T19:47:01.640', 'Id': '2536'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My setup is something like this: I have a sequence of sets of integers $C_i (1\\leq i\\leq n)$, with $|C_i|$ relatively small - on the order of four or five items for all $i$.  I want to choose a sequence $x_i (1\\leq i\\leq n)$ with each $x_i\\in C_i$ such that the total variation (either $\\ell_1$ or $\\ell_2$, i.e. $\\sum_{i=1}^{n-1} |x_i-x_{i+1}|$ or  $\\sum_{i=1}^{n-1} \\left(x_i-x_{i+1}\\right)^2$) is minimized.  While it seems like the choice for each $x_i$ is 'local', the problem is that choices can propagate and have non-local effects and so the problem seems inherently global in nature.</p>\n\n<p>My primary concern is in a practical algorithm for the problem; right now I'm using annealing methods based on mutating short subsequences, and while they should be all right it seems like I ought to be able to do better.  But I'm also interested in the abstract complexity &mdash; my hunch would be that the standard query version ('is there a solution of total variation $\\leq k$?') would be NP-complete via a reduction from some constraint problem like 3-SAT but I can't quite see the reduction.  Any pointers to previous study would be welcome &mdash; it seems like such a natural problem that I can't believe it hasn't been looked at before, but my searches so far haven't turned up anything quite like it.</p>\n", 'ViewCount': '133', 'Title': 'Minimizing the total variation of a sequence of discrete choices', 'LastEditorUserId': '242', 'LastActivityDate': '2012-06-29T20:59:15.583', 'LastEditDate': '2012-06-28T22:07:25.153', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '2542', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '242', 'Tags': '<algorithms><complexity-theory><optimization>', 'CreationDate': '2012-06-28T21:57:56.210', 'Id': '2539'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '139', 'Title': 'Find string that minimizes the sum of the edit distances to all other strings in set', 'LastEditDate': '2012-06-30T10:54:21.683', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'jmvidal', 'PostTypeId': '1', 'OwnerUserId': '2015', 'FavoriteCount': '1', 'Body': '<p>I have a set of strings $S$ and I am using the edit-distance (<a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein</a>) to measure the distance between all pairs.</p>\n\n<p>Is there an algorithm for finding the string $x$ which minimizes the sum of the distances to all strings in $S$, that is</p>\n\n<p>$\\arg_x \\min \\sum_{s \\in S} \\text{edit-distance}(x,s)$</p>\n\n<p>It seems like there should, but I can\'t find the right reference.</p>\n', 'Tags': '<algorithms><reference-request><strings><string-metrics>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-30T10:54:21.683', 'CommentCount': '0', 'AcceptedAnswerId': '2551', 'CreationDate': '2012-06-29T15:25:33.753', 'Id': '2546'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There are obvious analogs (pardon the pun) between Boolean algebra and algebra. They have similar laws, operators and properties. I can't figure out why Karnaugh Maps and sum of products, which are used to derive a Boolean function from a truth table, doesn't have an equivalent in algebra.  Perhaps it does, but I haven't seen it. </p>\n\n<p>My only explanation is that if it were possible, you could theoretically find a function for any arbitrary series of numbers (0, 2, 4, 6, 8 f(n)=2n). Thus, you could solve a ton of very difficult problems. I'm not necessarily looking for a formal proof but an explanation. </p>\n\n<p>Right now, I am having fleeting ideas that it has something to do with infinite outputs and inputs, something to do with place values, or true or false equivalents in algebra. There's something here that's difficult to put my finger on.</p>\n", 'ViewCount': '435', 'Title': 'Can truth tables be used in non-boolean algebra to derive functions?', 'LastEditorUserId': '2029', 'LastActivityDate': '2012-09-16T01:05:18.613', 'LastEditDate': '2012-07-01T21:12:14.550', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2029', 'Tags': '<algorithms><computability><logic>', 'CreationDate': '2012-06-30T22:00:28.147', 'FavoriteCount': '2', 'Id': '2566'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having trouble finding good resources that give a worst case $O(n \\ln n)$ <a href="http://en.wikipedia.org/wiki/In-place_algorithm">in place</a> <a href="http://www.algorithmist.com/index.php/Stable_Sort">stable</a> sorting algorithm.  Does anyone know of any good resources?</p>\n\n<p>Just a reminder, in place means it uses the array passed in and the sorting algorithm is only allowed to use constant extra space.  Stable means that elements with the same key appear in the same order in the sorted array as they did in the original.</p>\n\n<p>For example, naive merge sort is worst case $O(n \\ln n)$ and stable but uses $O(n)$ extra space.  Standard quicksort can be made stable, is in place but is worst case $O(n^2)$.  Heapsort is in place, worst case $O(n \\ln n)$ but isn\'t stable.  <a href="http://en.wikipedia.org/wiki/Sorting_algorithm">Wikipedia</a> has a nice chart of which sorting algorithms have which drawbacks.  Notice that there is no sorting algorithm that they list that has all three conditions of stability, worst case $O(n \\ln n)$ and being in place.</p>\n\n<p>I have found a paper called <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.8523&amp;rep=rep1&amp;type=pdf">"Practical in-place mergesort"</a> by Katajainen, Pasanen and Teuhola, which claims to have a worst case $O(n \\ln n)$ in place stable mergesort variant.  If I understand their results correctly, they use (bottom-up?) mergesort recursively on the first $\\frac{1}{4}$ of the array and the latter $\\frac{1}{2}$ of the array and use the second $\\frac{1}{4}$ as scratch space to do the merge.  I\'m still reading through this so any more information on whether I\'m interpreting their results correctly is appreciated.</p>\n\n<p>I would also be very interested in a worst case $O(n \\ln n)$ in place stable quicksort.  From what I understand, modifying quicksort to be worst case $O(n \\ln n)$ requires <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Linear_general_selection_algorithm_-_Median_of_Medians_algorithm">selecting a proper pivot</a> which would destroy the stability that it would otherwise normally enjoy.</p>\n\n<p>This is purely of theoretical interest and I have no practical application.  I would just like to know the algorithm that has all three of these features.</p>\n', 'ViewCount': '1193', 'Title': 'Worst case $O(n \\ln n)$ in place stable sort?', 'LastActivityDate': '2013-10-27T16:22:39.590', 'AnswerCount': '3', 'CommentCount': '5', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '67', 'Tags': '<algorithms><reference-request><sorting>', 'CreationDate': '2012-07-01T14:50:37.140', 'FavoriteCount': '2', 'Id': '2569'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a run-time implementation question regarding the 3-dimensional (unweighted 2-)approximation algorithm below:\nHow can I construct the maximum matching M_r in S_r in linear time in line 8?</p>\n\n<p>$X, Y, Z $ are disjoint sets; a matching $M$ is a subset of $S$ s.t. no two triples in $M$ have the same coordinate at any dimension.</p>\n\n<p>$\n\\text{Algorithm: unweighted 3-dimensional matching (2-approximation)} \\\\\n\\text{Input: a set $S\\subseteq X \\times Y \\times Z$ of triples} \\\\\n\\text{Output: a matching M in S}\n$</p>\n\n<pre><code> 1) construct maximal matching M in S;  \n 2) change = TRUE;  \n 3) while (change) {  \n 4)   change = FALSE;  \n 5)   for each triple (a,b,c) in M {  \n 6)     M = M - {(a,b,c)};  \n 7)     let S_r be the set of triples in S not contradicting M;  \n 8)     construct a maximum matching M_r in S_r;  \n 9)     if (M_r contains more than one triple) {  \n10)       M = M \\cup M_r;  \n11)       change = TRUE;  \n12)     } else {  \n13)       M = M \\union {(a,b,c)};  \n14)     }  \n15) }  \n</code></pre>\n\n<hr>\n\n<p>[1] <a href="http://faculty.cse.tamu.edu/chen/courses/cpsc669/2011/notes/ch9.pdf" rel="nofollow">http://faculty.cse.tamu.edu/chen/courses/cpsc669/2011/notes/ch9.pdf</a>, p. 326</p>\n', 'ViewCount': '459', 'Title': '3-dimensional matching approximation algorithm (implementation details)', 'LastEditorUserId': '157', 'LastActivityDate': '2013-01-13T19:29:47.290', 'LastEditDate': '2013-01-13T19:29:47.290', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2574', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2037', 'Tags': '<algorithms><graphs><approximation><matching>', 'CreationDate': '2012-07-01T17:27:37.793', 'Id': '2571'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Basically, the problem I am solving is this. Initially, the array $A$ is empty. Then I am given data to fill the array and at any time I have to make a query to print the $|A|/3$-th largest element inserted so far.</p>\n\n<p>I was solving the problem with segment trees, but I am not able to make a little modification to the query function of the segment tree. The query function that I wrote returns the largest element between indices $a_{\\text{begin}}$ and $a_{\\text{end}}$:</p>\n\n<pre><code>int query(int Nodenumber,int t_begin,int t_end,int a_begin,int a_end)\n{\n    if (t_begin&gt;=a_begin &amp;&amp; t_end&lt;=a_end)\n        return Tree[Nodenumber];\n    else\n    {\n        int mid=((t_begin+t_end)/2);\n        int res = -1;\n\n        if (mid&gt;=a_begin &amp;&amp; t_begin&lt;=a_end)\n            res = max(res,query(2*Nodenumber,t_begin,mid,a_begin,a_end));\n\n        if (t_end&gt;=a_begin &amp;&amp; mid+1&lt;=a_end)\n            res = max(res,query(2*Nodenumber+1,mid+1,t_end,a_begin,a_end));\n\n        return res;\n    }\n} \n</code></pre>\n\n<p>Note to make a query, I call the query function as <code>query(1,0,N-1,QA,QB)</code>.</p>\n\n<p>But I want to return the $|A|/3$-th largest element between indices $a_{\\text{begin}}$ and $a_{\\text{end}}$. So how should I modify the function to do this?</p>\n\n<p>So updating, queries, updating, queries, updating, queries and so on are done randomly and several (upto $10^5$) times.</p>\n\n<p>So, for solving the problem, did I pick the right data structure? I thought of using heaps, but that will be too slow, as I would have to pop $|A|/3$ elements from the top and reinsert them for every query.</p>\n', 'ViewCount': '329', 'Title': 'Finding the $k$th largest element in an evolving query data structure', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-10T17:59:07.600', 'LastEditDate': '2012-07-02T10:04:27.083', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '2579', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-07-02T03:52:34.127', 'Id': '2575'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '961', 'Title': 'Most efficient algorithm to print 1-100 using a given random number generator', 'LastEditDate': '2012-07-02T19:46:10.570', 'AnswerCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2042', 'FavoriteCount': '4', 'Body': u'<p>We are given a random number generator <code>RandNum50</code> which generates a random integer uniformly in the range 1\u201350.\nWe may use only this random number generator to generate and print all integers from 1 to 100 in a random order. Every number must come exactly once, and the probability of any number occurring at any place must be equal.</p>\n\n<p>What is the most efficient algorithm for this?</p>\n', 'Tags': '<algorithms><integers><randomness><random-number-generator>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-24T20:32:31.727', 'CommentCount': '6', 'AcceptedAnswerId': '2578', 'CreationDate': '2012-07-02T05:57:26.373', 'Id': '2576'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m having a very hard time trying to figure out how to solve this problem efficiently. Let me describe how it goes:</p>\n\n<p>"A hard working mom bought several fruits with different nutritional values for her 3 kids, Amelia, Jessica and Bruno. Both girls are overweight, and they are very vicious and always leave poor Bruno with nothing, so their mother decided to share the food in the following manner:</p>\n\n<ul>\n<li><p>Amelia being the heaviest one gets the most amount of Nutritional Value.</p></li>\n<li><p>Jessica gets an amount equal or less than Amelia</p></li>\n<li><p>Bruno gets an amount equal or less than Jessica, but you need to find a way to give him the highest possible nutritional value while respecting the rule ( $A \\geq J \\geq B$ )"</p></li>\n</ul>\n\n<p>One of the test cases given by my teacher is the following:</p>\n\n<pre><code>The fruit list has the following values { 4, 2, 1, 8, 11, 5, 1\n\nInput:\n7   -----&gt; Number of Fruits\n4 2 1 8 11 5 1 ----&gt; Fruits Nutritional Values\n\nOutput:\n1 11  ----&gt;  One fruit, their nutritional values sum for Amelia\n5     ----&gt;  Position of the fruit in the list\n3 11  ----&gt;  Three fruits, their nutritional values sum for Jessica\n1 2 6 ----&gt;  Position of the fruits in the list\n3 10  ----&gt;  Three fruits, their nutritional values sum for Bruno\n3 4 7 ----&gt;  Position of the fruits in the list\n</code></pre>\n\n<p>Note: I am aware that there are several ways of diving the fruits among the kids, but it doesn\'t really matter as long as it follows the rule $A \\geq J \\geq B$.</p>\n\n<p>I\'m trying to make a program in C# that solves this kind of problems but I need an efficient formula to make this work. Generating all the subsets is out of the question because it is very consuming task. The list of fruits can have up to $50$ elements, $2^{50}$ is a huge number.</p>\n', 'ViewCount': '739', 'Title': 'Partition of a set of integer into 3 subsets of approximately equal sum', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-02T20:17:28.170', 'LastEditDate': '2012-07-02T19:42:50.433', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'Julian J. Tejera', 'PostTypeId': '1', 'Tags': '<algorithms><integers><approximation><linear-programming>', 'CreationDate': '2012-07-02T06:06:44.417', 'Id': '2580'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '197', 'Title': 'How to detect stack order?', 'LastEditDate': '2012-07-02T19:24:50.887', 'AnswerCount': '1', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1895', 'FavoriteCount': '2', 'Body': "<p>We take the sequence of integers from $1$ to $n$, and we push them onto a stack one by one in order. Between each push, we can choose to pop any number of items from the stack (from 0 to the current stack size).</p>\n\n<p>Every time we pop a value from the stack, we will print it out.</p>\n\n<p>For example, $1,2,3$ is printed out when we do <code>push, pop, push, pop, push, pop</code>. $3,2,1$ comes from <code>push, push, push, pop, pop, pop</code>. </p>\n\n<p>However, $3,1,2$ is not a possible printout, because it is not possible to have $3$ printed followed by $1$, without seeing $2$ in between.</p>\n\n<p>Question: <strong>How can we detect impossible orders like $3,1,2$?</strong></p>\n\n<p>In fact, based on my observation, I have come out a potential solution. But the problem is I can't prove my observation is complete.</p>\n\n<p>The program that I wrote with the following logic:</p>\n\n<p>When the current value minus the next value is larger than 1, a value between current and next cannot appear after next. For example, if current=3 and next=1, then the value between current (3) and next (1) is 2 which cannot appear after next(1), hence $3,1,2$ violates the rule.</p>\n\n<p>Does this cover all cases?</p>\n", 'Tags': '<algorithms><stack>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-04T13:50:57.453', 'CommentCount': '0', 'AcceptedAnswerId': '2594', 'CreationDate': '2012-07-02T12:01:14.353', 'Id': '2582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we are given an array of positive integers $P = [p_1, p_2, \\dots, p_N]$ where each $p_i$ represents the price of a product on a different day $i = 1 \\dots N$. </p>\n\n<p>I would like to design an algorithm to find the maximum profit that you can given this array of prices. Profit is made by buying at a given date $i$ and selling at a later date $j$ so that $i \\leq j$.</p>\n\n<p>One easy solution is the following "exhaustive algorithm":</p>\n\n<pre><code>profit = 0\nfor i = 1 to N-1 \n  for j = i+1 to N\n    if P(j) - P(i) &gt; profit    \n      profit = P(j) - P(i) \n</code></pre>\n\n<p>The issue with this however is that it takes time $\\Omega(N^2)$. </p>\n\n<p>Can anyone think of something faster?</p>\n', 'ViewCount': '879', 'Title': 'Finding the Largest "Ordered" Difference in Elements of an Array', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-16T16:10:38.710', 'LastEditDate': '2012-07-04T08:15:13.833', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '2584', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2046', 'Tags': '<algorithms><arrays>', 'CreationDate': '2012-07-02T14:27:49.000', 'Id': '2583'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '337', 'Title': 'How can I find minimum number required to add to sequence such that their xor becomes zero', 'LastEditDate': '2012-07-04T08:50:11.117', 'AnswerCount': '3', 'Score': '5', 'OwnerDisplayName': 'ArG0NaUt', 'PostTypeId': '1', 'OwnerUserId': '2368', 'FavoriteCount': '1', 'Body': '<p>Given a sequence of natural numbers, you can add any natural number to any number in the sequence such that their xor becomes zero. My goal is to minimize the sum of added numbers.</p>\n\n<p>Consider the following examples :</p>\n\n<ol>\n<li><p>For $1, 3$ the answer is $2$;  adding $2$ to $1$ we get $3 \\oplus 3=0$.</p></li>\n<li><p>For $10, 4, 5, 1$ the answer is $6$;  adding $3$ to $10$ and $3$ to $8$ we get $13 \\oplus 4 \\oplus 8 \\oplus 1 = 0$.</p></li>\n<li><p>For $4, 4$ the answer is $0$, since $4 \\oplus 4 = 0$.</p></li>\n</ol>\n\n<p>I tried working on binary representations of sequence number but it got so complex. I want to know if there is any simple and efficient way to solve this problem.</p>\n', 'Tags': '<algorithms><integers><xor>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-03T08:55:47.557', 'CommentCount': '1', 'CreationDate': '2012-06-30T05:10:32.913', 'Id': '2590'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a <a href="https://en.wikipedia.org/wiki/Cactus_graph" rel="nofollow">cactus</a>, we want to weight its edges in such a way that</p>\n\n<ol>\n<li>For each vertex, the sum of the weights of edges incident to the vertex is no more than 1.</li>\n<li>The sum of all edge weights is maximized.</li>\n</ol>\n\n<p>Clearly the answer is no more than $\\frac{n}{2}$ for $n$ vertices ($\\sum d_i = 2D$ where $d_i$ is the sum for one vertex and $D$ is the sum over every edge). This bound is achievable for cycle graphs by weighting each edge 1/2.</p>\n\n<p>I found a greedy algorithm for trees. Just assign 1 to edges incident to leaves and remove them and their neighbors from the graph in repeated passes. This prunes the cactus down to a bunch of interconnected cycles. At this point I assumed the remaining cycles were not interconnected and weighted each edge 1/2. This got 9/10 test cases but is, of course, incomplete.</p>\n\n<p>So, how might we solve this problem for cacti in general? I would prefer hints to full solutions, but either is fine.</p>\n\n<p><sub>\nThis question involves a problem from <a href="https://genesys.interviewstreet.com" rel="nofollow">an InterviewStreet CompanySprint</a>. I already competed but I\'d like some thoughts on a problem (solutions aren\'t released, and I\'ve been banging my head against the wall over this problem).\n</sub></p>\n', 'ViewCount': '265', 'Title': 'Balanced weighting of edges in cactus graph', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-04T21:47:55.087', 'LastEditDate': '2012-07-04T21:47:55.087', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2038', 'Tags': '<algorithms><graph-theory><greedy-algorithms>', 'CreationDate': '2012-07-03T16:09:43.020', 'FavoriteCount': '1', 'Id': '2598'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have read that the degree of nodes in a "knowledge" graph of people roughly follows a power law distribution, and more exactly can be approximated with a Pareto-Lognormal distribution.</p>\n\n<p>Where can I find a kind of algorithm that will produce a random graph with this distribution?</p>\n\n<p>See for example the paper <a href="http://www.cs.ucsb.edu/~alessandra/papers/ba048f-sala.pdf" rel="nofollow">Revisiting Degree Distribution Models for Social Graph Analysis</a> (page 4, equation 1) for a mathematical description (distribution function) of the kind of distribution I\'m interested in.</p>\n', 'ViewCount': '226', 'Title': 'How to random-generate a graph with Pareto-Lognormal degree nodes?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T04:40:16.137', 'LastEditDate': '2012-07-05T07:42:21.527', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2808', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2060', 'Tags': '<algorithms><graph-theory><probability-theory><randomness>', 'CreationDate': '2012-07-04T11:43:54.893', 'Id': '2608'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Semi clustering algorithm is mentioned in the <a href="http://people.apache.org/~edwardyoon/documents/pregel.pdf" rel="nofollow">Google Pregel paper</a>. The score of a semi cluster is calculated using the below formula</p>\n\n<p>$\\qquad \\displaystyle S_c =\\frac{I_c - f_BB_c}{\\frac{1}{2}V_c(V_c - 1)}$</p>\n\n<p>where</p>\n\n<ul>\n<li>$I_c$ is sum of the weights of all the internal edges,</li>\n<li>$B_c$ is the sum of the weights of all the boundary edges,</li>\n<li>$V_c$ is the number of edges in the semi cluster and</li>\n<li>$f_b$ is the boundary edge score factor (user defined between 0 and 1).</li>\n</ul>\n\n<p>The algorithm is pretty straight forward, but I could not understand how the above formula was derived. Note that the denominator is the number of edges possible between $V_c$ number of vertices.</p>\n\n<p>Could someone please explain it?</p>\n', 'ViewCount': '94', 'Title': 'What is the significance of the semi clustering formula in the Google Pregel paper?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-05T21:43:57.403', 'LastEditDate': '2012-07-05T08:03:25.720', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2074', 'Tags': '<algorithms><graph-theory><terminology>', 'CreationDate': '2012-07-05T02:48:54.200', 'Id': '2621'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array $A$ of $N$ integers, each element in the array can be increased by a fixed number $b$ with some probability $p[i]$, $0 \\leq i &lt; n$. I have to find the expected number of swaps that will take place to sort the array using <a href="http://en.wikipedia.org/wiki/Bubble_sort" rel="nofollow">bubble sort</a>.</p>\n\n<p>I\'ve tried the following:</p>\n\n<ol>\n<li><p>The probability for an element $A[i] &gt; A[j]$ for $i &lt; j$ can be calculated easily from the given probabilities.</p></li>\n<li><p>Using the above, I have calculated the expected number of swaps as:</p>\n\n<pre><code>double ans = 0.0;\nfor ( int i = 0; i &lt; N-1; i++ ){\n    for ( int j = i+1; j &lt; N; j++ ) {\n        ans += get_prob(A[i], A[j]); // Computes the probability of A[i]&gt;A[j] for i &lt; j.\n</code></pre></li>\n</ol>\n\n<p>Basically I came to this idea because the expected number of swaps can be calculated by the number of inversions of the array. So by making use of given probability I am calculating whether a number $A[i]$ will be swapped with a number $A[j]$.</p>\n\n<p>Note that the initial array elements can be in any order, sorted or unsorted. Then each number can change with some probability. After this I have to calculate the expected number of swaps.</p>\n\n<p>I have posted <a href="http://stackoverflow.com/questions/11331314/number-of-swaps-in-bubble-sort">a similar question</a> before but it did not had all the constraints.</p>\n\n<p>I did not get any good hints on whether I am even on the right track or not, so I listed all the constraints here. Please give me some hints if I am thinking of the problem in an incorrect way.</p>\n', 'ViewCount': '2139', 'Title': 'Expected number of swaps in bubble sort', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:48:38.960', 'LastEditDate': '2012-07-09T08:40:49.420', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '9', 'OwnerDisplayName': 'TheRock', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><sorting><average-case>', 'CreationDate': '2012-07-05T08:44:10.770', 'FavoriteCount': '3', 'Id': '2630'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1279', 'Title': 'Top down, Bottom up Dynamic programing', 'LastEditDate': '2012-07-17T06:11:34.983', 'AnswerCount': '1', 'Score': '14', 'OwnerDisplayName': 'stefan', 'PostTypeId': '1', 'OwnerUserId': '4259', 'FavoriteCount': '2', 'Body': '<p>Is there a fundamental difference between Top down and Bottom up Dynamic programing? </p>\n\n<p>Meaning, is there a problem which can be solved bottom up but not top down?</p>\n\n<p>Or is the bottom up approach just an unwinding of the recurrence in the top down approach?</p>\n', 'Tags': '<algorithms><dynamic-programming>', 'LastEditorUserId': '41', 'LastActivityDate': '2013-05-04T01:10:18.433', 'CommentCount': '0', 'CreationDate': '2011-11-09T18:09:05.470', 'Id': '2644'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '202', 'Title': 'Is it possible to always construct a hamiltonian path on a tournament graph by sorting?', 'LastEditDate': '2012-07-09T09:07:50.400', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2105', 'FavoriteCount': '1', 'Body': '<p>Is it possible to always construct a hamiltonian path on a <a href="http://en.wikipedia.org/wiki/Tournament_%28graph_theory%29#Paths_and_cycles" rel="nofollow">tournament graph</a> $G=(V,E)$ by sorting (using any sorting algorithm) with the following total order:</p>\n\n<p>$\\qquad \\displaystyle a \\leq b \\iff (a,b) \\in E \\lor \\left(\\exists\\, c \\in V. a \\leq c \\land c \\leq b\\right)$</p>\n\n<p>For context, this came from an observation that the inductive construction in the above page seems to be equivalent to insertion sort using the given order. Is it possible to use other sorting algorithms?</p>\n', 'Tags': '<algorithms><graph-theory><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-21T22:52:26.573', 'CommentCount': '2', 'AcceptedAnswerId': '2652', 'CreationDate': '2012-07-08T04:56:49.143', 'Id': '2646'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '541', 'Title': 'Collectively pay the bill problem', 'LastEditDate': '2013-07-11T19:54:15.473', 'AnswerCount': '2', 'Score': '20', 'PostTypeId': '1', 'OwnerUserId': '220', 'FavoriteCount': '10', 'Body': "<p>There are $n$ people at a table. The $i$th person has to pay $p_i$ dollars. </p>\n\n<p>Some people don't have the right bills to pay exactly $p_i$, so they come up with the following algorithm.</p>\n\n<blockquote>\n  <p>First, everyone puts some of their money on the table. Then each individual takes back the money they overpaid. </p>\n</blockquote>\n\n<p>The bills have a fixed set of denominations (not part of the input).</p>\n\n<p>An example:\nSuppose there are two people, Alice and Bob.  Alice owes \\$5 and has five \\$1 bills.  Bob owes \\$2 and has one \\$5 bill.  After Alice and Bob put all their money on the table, Bob takes back \\$3, and everyone is happy.</p>\n\n<p>Of course, there are times where one doesn't have to put <em>all</em> his money on the table. For example, if Alice had a thousand \\$1 bills, it's not necessary for her to put them all on the table and then take most of them back.</p>\n\n<p>I want to find an algorithm with the following properties: </p>\n\n<ol>\n<li><p>The input specifies the number of people, how much each person owes and how many bills of each denomination each person has.</p></li>\n<li><p>The algorithm tells each person which bills to put on the table in the first round.</p></li>\n<li><p>The algorithm tells each person which bills to remove from the table in the second round.</p></li>\n<li><p>The number of bills put on the table + the number of bills removed from the table is minimized. </p></li>\n</ol>\n\n<p>If there is no feasible solution, the algorithm just return an error.</p>\n", 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '220', 'LastActivityDate': '2013-07-11T19:54:15.473', 'CommentCount': '8', 'AcceptedAnswerId': '4808', 'CreationDate': '2012-07-08T20:31:24.413', 'Id': '2648'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '604', 'Title': 'How hard is finding the discrete logarithm?', 'LastEditDate': '2012-07-09T23:57:38.123', 'AnswerCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1667', 'FavoriteCount': '2', 'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Discrete_logarithm">discrete logarithm</a> is the same as finding $b$ in $a^b=c \\bmod N$, given $a$, $c$, and $N$.</p>\n\n<p>I wonder what complexity groups (e.g. for classical and quantum computers) this is in, and what approaches (i.e. algorithms) are the best for accomplishing this task.</p>\n\n<p>The wikipedia link above doesn\'t really give very concrete runtimes.  I\'m hoping for something more like what the best known methods are for finding such.</p>\n', 'Tags': '<algorithms><complexity-theory><time-complexity><discrete-mathematics>', 'LastEditorUserId': '1667', 'LastActivityDate': '2012-07-17T13:43:38.477', 'CommentCount': '1', 'AcceptedAnswerId': '2765', 'CreationDate': '2012-07-09T17:33:10.047', 'Id': '2658'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to understand the approach for this problem:</p>\n\n<blockquote>\n  <p>"If all streets are one way, there is still a legal way to drive from\n  one intersection to another"</p>\n</blockquote>\n\n<p>The question is to prove that it can be done in linear time. I am not looking for direct answers but the approach to this problem.</p>\n\n<p>How can I think about this problem in terms of graph theory? AFAI understand, this will result in a DAG. But then should I choose BFS or DFS and why to prove it? (both are liner time algos)</p>\n', 'ViewCount': '122', 'Title': 'Existence of a route following one-way streets', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-23T23:19:37.897', 'LastEditDate': '2012-07-17T05:56:03.863', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '271', 'Tags': '<algorithms><graphs><algorithm-analysis>', 'CreationDate': '2012-07-10T17:53:32.663', 'Id': '2674'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '150', 'Title': 'Algorithm for type conversion / signature matching', 'LastEditDate': '2012-07-13T11:10:36.447', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '1', 'Body': u"<p>I'm working on an expression typing system and looking for insights on what algorithms may be available which solve my problem -- or a proof that its complexity is too high to be reasonable to implement. The problem is defined below.</p>\n\n<p>I have a set of types which form a directed graph $T = (V,E)$ (assume no cycles). This graph represents the allowed type conversions in a language. For example an edge $e_i = v_1 \\rightarrow v_2$ indicates that $v_1$ can be implicitly converted to type $v_2$.</p>\n\n<p>I have a set of parameter types for a function expressed as a set $P = { p_1 ... p_n : p_i \u2208 V }$. I also have a list of functions $F$ that might be applicable at this point. Each function has a signature (the types it accepts) $F_j = { f_1 ... f_n : t_i \u2208 V }$.</p>\n\n<p>The goal is to use a series of type conversions allowed by $T$ to convert $P$ into a signature compatible with any function in $F$. Conversion means moving along an edge in the graph to another type. Compatible means the converted parameter types match the function types.</p>\n\n<p>If each conversion has a cost of 1, which function, if selected, has the minimum total conversion cost for all parameters?</p>\n\n<hr>\n\n<p><em>A very simple example</em>: Assume we have a graph of types <code>integer -&gt; real -&gt; complex</code>. Our parameters have the types <code>{ integer, real }</code>. We have a function with types <code>{ complex, complex }</code>. The first integer takes two conversion to match complex, and the real takes one conversion, for a total cost of three. We have another function with types <code>{ real, real }</code>. This has a cost of one and is thus the better match.</p>\n\n<hr>\n\n<p>My initial idea is to treat the search as a path through a graph and use a modified A* algorithm. Each of the possible functions is a goal in that graph, and each path between nodes represents the conversion of a single parameter type. With even a modest number of allowed type conversions however this becomes very inefficient.</p>\n", 'Tags': '<algorithms><typing>', 'LastEditorUserId': '29', 'LastActivityDate': '2012-07-13T11:10:36.447', 'CommentCount': '0', 'AcceptedAnswerId': '2685', 'CreationDate': '2012-07-10T18:28:31.157', 'Id': '2677'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the set of graphs in which the maximum degree of the vertices is a constant number $\\Delta$ independent of the number of vertices. Is the vertex coloring problem (that is, color the vertices with minimum number of colors such that no pair of adjacent nodes have the same color) on this set still NP-hard? Why?</p>\n', 'ViewCount': '92', 'Title': 'Vertex coloring with an upper bound on the degree of the nodes', 'LastActivityDate': '2012-07-17T14:39:49.543', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2692', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1672', 'Tags': '<algorithms><complexity-theory><graph-theory><graphs><np-complete>', 'CreationDate': '2012-07-11T10:26:02.150', 'Id': '2690'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '123', 'Title': 'Why is solving of diagonal quadratic equations over $\\mathbb R$ and $\\mathbb C$ in $P$?', 'LastEditDate': '2012-07-11T18:02:08.427', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2103', 'FavoriteCount': '1', 'Body': '<p>Let $\\mathbb F\\in\\{\\mathbb R, \\mathbb C\\}$ the field of real or complex numbers. Then [1, page 22 in the middle] claims that the following equation can easily be solved in deterministic polynomial time:\n$$ \\sum_{i=1}^n a_ix_i^2=b$$\nwith $a_i, b\\in\\mathbb F$. Some discussion suggests, that the algorithmical model assumed in the paper is that of a machine that can perform field operations in one step. </p>\n\n<p>My question is: What is the algorithm? Is it multidimensional Newton? That would be weird because this algorithm only converges (in some cases) and does not give an exact solution. I\'m quite unused to computational models over fields like the reals or complex numbers and maybe for someone who is more experienced this is cristal-clear?</p>\n\n<p>[1] <a href="http://www.math.uni-bonn.de/~saxena/papers/cubic-forms.pdf" rel="nofollow">Agrawal &amp; Saxena, On the complexity of cubic forms, 2006.</a></p>\n', 'Tags': '<algorithms><real-numbers>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-07-12T14:49:59.593', 'CommentCount': '5', 'AcceptedAnswerId': '2709', 'CreationDate': '2012-07-11T17:32:48.143', 'Id': '2697'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '320', 'Title': 'Polygons generated by a set of segments', 'LastEditDate': '2012-07-18T11:08:48.960', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Antoine', 'PostTypeId': '1', 'OwnerUserId': '2151', 'FavoriteCount': '1', 'Body': '<p>Given a set of segments, I would like to compute the set of closed polygons inside the convex hull of the set of the end of those segments. The vertices of the polygons are the intersections of the segments. For example, if you draw the 6 lines restricted which equations are: $x=-1$, $x=0$, $x=1$, $y=-1$, $y=0$, $y=1$, I would like the algorithm to output the four unit squares around the origin.<img src="http://i.stack.imgur.com/I6qmZ.png" alt="The polygons I\'m trying to compute"></p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '2151', 'LastActivityDate': '2012-07-18T11:08:48.960', 'CommentCount': '5', 'AcceptedAnswerId': '2732', 'CreationDate': '2012-04-16T17:15:21.840', 'Id': '2717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a bivariate function like</p>\n\n<p>$\nf(x,y) = \\frac{1}{x^3 \\sqrt{\\pi}}. e^{\\frac{2-x}{x^2}} . y^3 . e^{3.y \\over 3-y}\n$</p>\n\n<p>and I want to find its global maximum over a range of \n$\nx \\in [0, 200] \\text{, and } y \\in [300,50000]\n$</p>\n\n<p>What kind of algorithms I can use to find the global maximum. I want to have keywords for searching and finding materials. </p>\n\n<p>Are there any java library which I can use to solve these kind of problems? </p>\n', 'ViewCount': '406', 'Title': 'Function Maximization in Java', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T09:36:15.783', 'LastEditDate': '2012-07-18T01:36:31.670', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2156', 'Tags': '<algorithms><optimization><mathematical-analysis><mathematical-software>', 'CreationDate': '2012-07-13T15:18:16.297', 'Id': '2727'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Mark lives in a tiny country populated by people who tend to over-think things. One day, the king of the country decides to redesign the country's currency to make giving change more efficient. The king wants to minimize the expected number of coins it takes to exactly pay any amount up to (but not including) the amount of the smallest paper bill.</p>\n\n<p>Suppose that the smallest unit of currency is the Coin. The smallest paper bill in the kingdom is worth $n$ Coins. The king decides that there should not be more than $m$ different coin denominations in circulation. The problem, then, is to find a $m$-set $\\{d_1, d_2, ..., d_m\\}$ of integers from $\\{1, 2, ..., n - 1\\}$ which minimizes $\\frac{1}{n-1}\\sum_{i = 1}^{n-1}{c_1(i) + c_2(i) + ... + c_m(i)}$ subject to $c_1(i)d_1 + c_2(i)d_2 + ... c_m(i)d_m = i$.</p>\n\n<p>For instance, take the standard USD and its coin denominations of $\\{1, 5, 10, 25, 50\\}$. Here, the smallest paper bill is worth 100 of the smallest coin. It takes 4 coins to make 46 cents using this currency; we have $c_1(46) = 1, c_2(46) = 0, c_3(46) = 2, c_4(46) = 1, c_5(46) = 0$. However, if we had coin denominations of $\\{1, 15, 30\\}$, it would take only 3 coins: $c_1(46) = 1, c_2(46) = 1, c_3(46) = 1$. Which of these denomination sets minimizes the average number of coins to make any sum up to and including 99 cents?</p>\n\n<p>More generally, given $n$ and $m$, how might one algorithmically determine the optimal set? Clearly, one might enumerate all viable $m$-subsets and compute the average number of coins it takes to make sums from 1 to $n - 1$, keeping track of the optimal one along the way. Since there are around $C(n - 1, m)$ $m$-subsets (not all of which are viable, but still), this would not be terribly efficient. Can you do better than that?</p>\n", 'ViewCount': '1098', 'Title': 'Algorithm to find optimal currency denominations', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:16:54.940', 'LastEditDate': '2012-07-18T01:01:59.327', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '2806', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '69', 'Tags': '<algorithms><optimization><combinatorics><integers>', 'CreationDate': '2012-07-13T18:00:47.967', 'Id': '2734'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '314', 'Title': 'Minimize the maximum component of a sum of vectors', 'LastEditDate': '2012-07-21T21:36:25.613', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1689', 'FavoriteCount': '1', 'Body': '<p>I\'d like to learn something about this optimization problem: For given non-negative whole numbers $a_{i,j,k}$,\nfind a function $f$ minimizing the expression</p>\n\n<p>$$\\max_k \\sum_i a_{i,f(i),k}$$</p>\n\n<p>An example using a different formulation might make it clearer:\nYou\'re given a set of sets of vectors like</p>\n\n<pre><code>{\n    {(3, 0, 0, 0, 0), (1, 0, 2, 0, 0)},\n    {(0, 1, 0, 0, 0), (0, 0, 0, 1, 0)},\n    {(0, 0, 0, 2, 0), (0, 1, 0, 1, 0)}\n}\n</code></pre>\n\n<p>Choose one vector from each set, so that the maximum component of their sum is minimal.\nFor example, you may choose</p>\n\n<pre><code>(1, 0, 2, 0, 0) + (0, 1, 0, 0, 0) + (0, 1, 0, 1, 0) = (1, 1, 2, 1, 0)\n</code></pre>\n\n<p>with the maximum component equal to 2, which is clearly optimal here.</p>\n\n<p>I\'m curious if this is a well-known problem and what problem-specific approximate solution methods are available. It should be fast and easy to program (no <a href="http://en.wikipedia.org/wiki/Linear_programming#Integer_unknowns" rel="nofollow">ILP</a> solver, etc.). No exact solution is needed as it\'s only an approximation of the real problem.</p>\n\n<hr>\n\n<p>I see that I should have added some details about the problem instances I\'m interested in:</p>\n\n<ul>\n<li>$i \\in \\{0, 1, \\ldots, 63\\}$, i.e., there\'re always 64 rows (when written as in the above example).</li>\n<li>$j \\in \\{0, 1\\}$, i.e., there\'re only 2 vectors per row.</li>\n<li>$k \\in \\{0, 1, \\ldots, N-1\\}$ where $N$ (the vector length) is between 10 and 1000.</li>\n</ul>\n\n<p>Moreover, on each row the sum of the elements of all vectors is the same, i.e.,</p>\n\n<p>$$\\forall i, j, j\':\\quad \\sum_k a_{i,j,k} = \\sum_k a_{i,j\',k}$$</p>\n\n<p>and the sum of the elements of the sum vector is less than its length, i.e.,</p>\n\n<p>$$\\sum_k \\sum_i a_{i,f(i),k} &lt; N$$</p>\n', 'Tags': '<algorithms><optimization><linear-programming>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-21T21:36:25.613', 'CommentCount': '5', 'AcceptedAnswerId': '2758', 'CreationDate': '2012-07-14T03:22:03.487', 'Id': '2741'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '130', 'Title': 'Assign m agents to N points by minimizing the total distance', 'LastEditDate': '2012-07-16T20:18:03.193', 'AnswerCount': '2', 'Score': '4', 'OwnerDisplayName': 'd. th. man', 'PostTypeId': '1', 'OwnerUserId': '2192', 'Body': '<p>Suppose we have $N$ fixed points (set $S$ with $|S|=N$) on the plane and $m$ agents with fixed, known initial positions ($m&lt;N$) outside $S$. We should transfer the agents so that in our final configuration they are all positioned to different points of $S$. How could we achieve it by minimizing the total distance covered by the agents? </p>\n', 'Tags': '<algorithms><graphs><optimization>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-18T01:55:49.667', 'CommentCount': '3', 'AcceptedAnswerId': '2772', 'CreationDate': '2012-07-15T16:18:37.857', 'Id': '2771'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '106', 'Title': 'Determining how similar a given string is to a collection of strings', 'LastEditDate': '2012-07-22T09:43:50.247', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2185', 'FavoriteCount': '2', 'Body': '<p>I\'m not sure if this question belongs here and I apologize if not.  What I am looking to do is to develop a programmatic way in which I can probabilistically determine whether a given string "belongs" in a bag of strings.  For example, if I have bag of 10,000 US city names, and then I have the string "Philadelphia", I would like some quantitative measure of how likely \'Philadelphia\' is a US city name based on the US city names I already know.  While I know I won\'t be able to separate real city names from fake city names in this context, I would at least expect to have strings such as "123.75" and "The quick red fox jumped over the lazy brown dogs" excluded given some threshold.</p>\n\n<p>To get started, I\'ve looked at Levenshtein Distance and poked around a bit on how that\'s been applied to problems at least somewhat similar to the one I\'m trying to solve.  One interesting application I found was plagiarism detection, with one paper describing how Levenshtein distance was used with a modified Smith-Waterman algorithm to score papers based on how likely they were a plagarized version of a given base paper.  My question is if anyone could point me in the right direction with other established algorithms or methodologies that might help me.  I get the feeling that this may be a problem someone in the past has tried to solve but so far my Google-fu has failed me.</p>\n', 'Tags': '<algorithms><reference-request><string-metrics>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-22T09:43:50.247', 'CommentCount': '3', 'AcceptedAnswerId': '2780', 'CreationDate': '2012-07-16T21:29:07.047', 'Id': '2777'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>At my current project I had a network problem come up for which I could not find a solution. In a peer-to-peer network I needed to send an action to all peers, and each peer was to act on it only if it could verify that all other peers would also act on it.</p>\n\n<p>That is, given a network of peers $P = { P_1, ..., P_n }$. We wish to send, from some source peer $P_s$ a message to all other peers. This message contains an action which must be performed. The peer should perform this action if and only if every other peer will perform the action. That is, it performs the action if it can verify that all other peers will also have receipt of the action and can perform the same verification.</p>\n\n<p>The problem is subject to these conditions:</p>\n\n<ol>\n<li>There is no implicit message delivery guarantee: if $P_x$ sends a message to $P_y$ there is no way for $P_x$ to know if $P_y$ gets the message. (Of course $P_y$ can send a receipt, but that receipt is subject to the same constraint)</li>\n<li>Additional messages with any payload may be created.</li>\n<li>There is no total ordering on the messages received by peers. Messages can arrive in a different time-order than which they were sent. This time-order may be unique per peer. <em>Two messages sent in order from $P_x$ to $P_y$ are very unlikely to arrive out of order.</em></li>\n<li>Messages can arrive at any point in the future (so not only are they not ordered, they can be indefintely delayed). A message cannot inherently be detected as lost. <em>Most messages will be delivered quickly, or truly lost.</em></li>\n<li>Each peer has a synchronized clock. It is accurate enough in the domain of scheduling an action and to approximately measure transmission delays. It is however not accurate enough to establish a total ordering on messages using timestamps.</li>\n</ol>\n\n<p>I was not able to find a solution. I'm interested in a <em>guarantee</em> and not simply a high probability of being correct (which can be done simply be repeatedly sending confirmations from peer to peer and rejections upon any likely loss.) My stumbling block is the inability to verify that any particular message actually arrived. So even if $P_x$ determines there is an erorr, there is no guaranteed way to tell the other peers about it.</p>\n\n<p>A negative confirmation is also acceptable. I have a suspicion that a guarantee cannot actually be achieved, only an arbitrarily high probability.</p>\n", 'ViewCount': '71', 'Title': 'Message receipt verification in a cluster', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-03T09:56:09.247', 'LastEditDate': '2012-08-03T09:56:09.247', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '2802', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1642', 'Tags': '<algorithms><distributed-systems><computer-networks><fault-tolerance>', 'CreationDate': '2012-07-17T07:56:19.143', 'Id': '2784'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1273', 'Title': 'Removing Left Recursion from Context-Free Grammars - Ordering of nonterminals', 'LastEditDate': '2014-01-23T17:17:28.390', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'clebert', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': u'<p>I have recently implemented the Paull\'s algorithm for removing left-recursion from context-free grammars:</p>\n\n<blockquote>\n  <p>Assign an ordering $A_1, \\dots, A_n$ to the nonterminals of the grammar.</p>\n  \n  <p>for $i := 1$ to $n$ do begin<br>\n  $\\quad$ for $j:=1$ to $i-1$ do begin<br>\n  $\\quad\\quad$ for each production of the form $A_i \\to A_j\\alpha$ do begin<br>\n  $\\quad\\quad\\quad$ remove $A_i \\to A_j\\alpha$ from the grammar<br>\n  $\\quad\\quad\\quad$ for each production of the form $A_j \\to \\beta$ do begin<br>\n  $\\quad\\quad\\quad\\quad$ add $A_i \\to \\beta\\alpha$ to the grammar<br>\n  $\\quad\\quad\\quad$ end<br>\n  $\\quad\\quad$ end<br>\n  $\\quad$ end<br>\n  $\\quad$ transform the $A_i$-productions to eliminate direct left recursion<br>\n  end</p>\n</blockquote>\n\n<p>According to <a href="http://research.microsoft.com/pubs/68869/naacl2k-proc-rev.pdf" rel="nofollow" title="Removing Left Recursion from Context-Free Grammars">this document</a>, the efficiency of the algorithm crucially depends on the ordering of the nonterminals chosen in the beginning; the paper discusses this issue in detail and suggest optimisations.</p>\n\n<p>Some notation:</p>\n\n<blockquote>\n  <p>We will say that a symbol $X$ is a <em>direct left corner</em> of\n  a nonterminal $A$, if there is an $A$-production with $X$ as the left-most symbol on the right-hand side. We define the <em>left-corner relation</em> to be the reflexive transitive closure of the direct-left-corner relation, and we define the <em>proper-left-corner relation</em> to be the transitive closure of\n  the direct-left-corner relation. A nonterminal is <em>left recursive</em> if it is a proper left corner of itself; a nonterminal is <em>directly left recursive</em> if it is a direct left corner of itself; and a nonterminal is <em>indirectly left recursive</em> if it is left recursive, but not directly left recursive.</p>\n</blockquote>\n\n<p>Here is what the authors propose:</p>\n\n<blockquote>\n  <p>In the inner loop of Paull\u2019s algorithm, for nonterminals $A_i$ and $A_j$, such that $i &gt; j$ and $A_j$ is a direct left corner of $A_i$, we replace all occurrences of $A_j$ as a direct left corner of $A_i$ with all possible expansions of $A_j$.</p>\n  \n  <p>This only contributes to elimination of left recursion from the grammar if $A_i$ is a left-recursive nonterminal, and $A_j$ lies on a path that makes $A_i$ left recursive; that is, if $A_i$ is a left corner of $A_j$ (in addition to $A_j$ being a left corner of $A_i$).</p>\n  \n  <p>We could eliminate replacements that are useless in removing left recursion if we could order the nonterminals of the grammar so that, if $i &gt; j$ and $A_j$ is a direct left corner of $A_i$, then $A_i$ is also a left corner of $A_j$.</p>\n  \n  <p>We can achieve this by ordering the nonterminals in decreasing order of the number of distinct left corners they have.</p>\n  \n  <p>Since the left-corner relation is transitive, if C is a direct left corner of B, every left corner of C is also a left corner of B.</p>\n  \n  <p>In addition, since we defined the left-corner relation to be reflexive, B is a left corner of itself.</p>\n  \n  <p>Hence, if C is a direct left corner of B, it must follow B in decreasing order of number of distinct left corners, unless B is a left corner of C.</p>\n</blockquote>\n\n<p>All I want is to know how to order the nonterminals in the beginning, but I don\'t get it from the paper. Can someone explain it in a simpler way? Pseudocode would help me to understand it better.</p>\n', 'Tags': '<algorithms><context-free><formal-grammars><efficiency><left-recursion>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-23T17:17:28.390', 'CommentCount': '0', 'AcceptedAnswerId': '2793', 'CreationDate': '2012-05-23T12:50:45.647', 'Id': '2792'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The problem is as follows:</p>\n\n<p>Given a finite set of rectangles ($S\\subset\\mathbb{R}\\times\\mathbb{R}$), build a data structure that will support the following operations:</p>\n\n<ul>\n<li>Check, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and returns true iff there is a rectangle $c\\in S$ so that $r_x \\leq c_x$ and $r_y \\leq c_y$.</li>\n<li>Get, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and returns the minimal member of $S$ (that is, a member with a minimal area - $c_x c_y$) that contains $r$ (i.e., $r_x \\leq c_x$ and $r_y \\leq c_y$).</li>\n<li>Insert, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and adds it to $S$.</li>\n<li>Remove, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and removes it from $S$.</li>\n</ul>\n\n<p>The four procedures have to be efficient, measuring efficiency by $m$ and $n$, where $m$ is the number of different widths in $S$ and $n$ is the number of different heights in $S$.</p>\n\n<p>Space efficiency has to be no worse than $O(mn)$ (that is, linear to the number of elements in $S$).</p>\n', 'ViewCount': '215', 'Title': 'Finding a minimal containing rectangle from a given set of rectangles', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T11:09:53.763', 'LastEditDate': '2012-07-18T10:18:12.310', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2197', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2012-07-18T08:41:16.803', 'FavoriteCount': '1', 'Id': '2810'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p><a href="http://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm">Bor\u016fvka\'s algorithm</a> is one of the standard algorithms for calculating the minimum spanning tree for a graph $G = (V,E)$, with $|V| = n, |E| = m$.</p>\n\n<p>The pseudo-code is:</p>\n\n<pre><code>MST T = empty tree\nBegin with each vertex as a component\nWhile number of components &gt; 1\n    For each component c\n       let e = minimum edge out of component c\n       if e is not in T\n           add e to T  //merging the two components connected by e\n</code></pre>\n\n<p>We call each iteration of the outer loop a round. In each round, the inner loop cuts the number of components at least in half. Therefore there are at most $O(\\log n)$ rounds. In each round, the inner loop looks at each edge at most twice (once from each component). Therefore the running time is at most $O(m \\log n)$.</p>\n\n<p>Now suppose after each round, we remove all the edges which only connect vertices within the same component and also remove duplicate edges between components, so that the inner loop only looks at some number of edges m\' &lt; m which are the minimum weight edges which connect two previously disconnected components. </p>\n\n<p><strong>How does this optimization affect the running time?</strong></p>\n\n<p>If we somehow knew that in each round, it would cut the number of edges in half, then the running time would be significantly improved:\n$T(m) = T(m /2) + O(m) = O(m)$.</p>\n\n<p>However, while the optimization will dramatically reduce the number of edges examined, (only 1 edge by the final round, and at most # of components choose 2 in general), it\'s not clear how/if we can use this fact to tighten the analysis of the run-time. </p>\n', 'ViewCount': '374', 'Title': u"Tighter analysis of modified Bor\u016fvka's algorithm", 'LastActivityDate': '2012-07-20T08:15:16.420', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '2829', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<algorithms><algorithm-analysis><spanning-trees>', 'CreationDate': '2012-07-18T18:53:31.420', 'FavoriteCount': '3', 'Id': '2816'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there a way to reconstruct a binary tree just from its in-order representation?</p>\n\n<p>I've searched the internet, but I could only find solutions for reconstructing a binary tree from inorder and preorder representations, but none for only inorder.</p>\n", 'ViewCount': '316', 'Title': 'From in-order representation to binary tree', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-19T21:10:44.540', 'LastEditDate': '2012-07-19T08:49:01.357', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'papen', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><binary-trees>', 'CreationDate': '2012-07-18T19:23:00.627', 'Id': '2817'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know why and how the <a href="https://en.wikipedia.org/wiki/Push-relabel_algorithm" rel="nofollow">push relabel algorithm</a> works for solving the max-flow problem. But why is a global update step required? </p>\n', 'ViewCount': '224', 'Title': 'Reason for global update steps in the push-relabel algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-20T15:50:51.657', 'LastEditDate': '2012-07-20T15:50:51.657', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'user997704', 'PostTypeId': '1', 'OwnerUserId': '2212', 'Tags': '<algorithms><graph-theory><network-flow>', 'CreationDate': '2012-07-18T09:27:04.577', 'Id': '2819'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there a way to create a single edge on a graph that connects 3 or more nodes? For example, let's say that the probability of Y occurring after X is 0.1, and the probability of Z occurring after Y is 0.001, but the probability of Z occurring after <em>both</em> X and Y occur is 0.95. If the probabilities are assigned to each edge as weights, how can I make this happen?</p>\n\n<p>$$X _\\overrightarrow{0.1} Y$$</p>\n\n<p>$$Y _\\overrightarrow{0.001} Z$$</p>\n\n<p>$$\\overrightarrow{X \\underrightarrow{} Y \\underrightarrow{0.95}} Z$$</p>\n", 'ViewCount': '110', 'Title': 'An edge that connects more than two nodes in a graph?', 'LastEditorUserId': '2214', 'LastActivityDate': '2012-07-20T01:26:43.280', 'LastEditDate': '2012-07-19T21:55:12.563', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2214', 'Tags': '<algorithms><graphs><probabilistic-algorithms><weighted-graphs>', 'CreationDate': '2012-07-19T21:42:57.053', 'Id': '2826'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '209', 'Title': 'Greedy choice and matroids (greedoids)', 'LastEditDate': '2012-07-20T14:19:38.057', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2220', 'FavoriteCount': '3', 'Body': '<p>As I was going through the material about the greedy approach, I came to know that a knowledge on matroids (greedoids) will help me approaching the problem properly. After reading about matroids I have roughly understood what matroids are. But how do you use the concept of a matroid for solving a given optimisation problem? </p>\n\n<p>Take, for example, the <a href="https://en.wikipedia.org/wiki/Activity_selection_problem" rel="nofollow">activity selection problem</a>. What are the steps to use matroid theory for solving the problem?</p>\n', 'Tags': '<algorithms><graph-theory><greedy-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-20T14:23:46.633', 'CommentCount': '2', 'AcceptedAnswerId': '2841', 'CreationDate': '2012-07-20T09:34:21.970', 'Id': '2840'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '464', 'Title': 'Standard or Top Text on Applied Graph Theory', 'LastEditDate': '2012-07-23T07:12:08.633', 'AnswerCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1022', 'FavoriteCount': '3', 'Body': '<p>I am looking for a reference text on applied graph theory and graph algorithms.  Is there a standard text used in most computer science programs?  If not, what are the most respected texts in the field? I have Cormen et al.</p>\n', 'Tags': '<algorithms><graph-theory><reference-request><education><books>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-27T05:17:04.987', 'CommentCount': '6', 'AcceptedAnswerId': '2911', 'CreationDate': '2012-07-20T20:50:07.910', 'Id': '2845'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been looking for a way to represent the <a href="http://en.wikipedia.org/wiki/Golden_ratio_base" rel="nofollow">golden ratio ($\\phi$) base</a> more efficiently in binary.  The standard binary golden ratio notation works but is horribly space inefficient.  The  Balanced Ternary Tau System (BTTS) is the best I\'ve found but is quite obscure.  The paper describing it in detail is <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.880" rel="nofollow">A. Stakhov, Brousentsov\'s Ternary Principle, Bergman\'s Number System and Ternary Mirror-symmetrical Arithmetic, 2002</a>.  It is covered in less depth by <a href="http://neuraloutlet.wordpress.com/tag/ternary-tau-system/" rel="nofollow">this blog post</a>.</p>\n\n<p>BTTS is a <a href="http://en.wikipedia.org/wiki/Balanced_ternary" rel="nofollow">balanced ternary representation</a> that uses $\\phi^2 = \\phi + 1$ as a base and 3 values of $\\bar 1$ ($-1$), $0$, and $1$ to represent addition or subtraction of powers of $\\phi^2$.  The table on page 6 of the paper lists integer values from 0 up to 10, and it can represent any $\\phi$-based number as well.</p>\n\n<p>BTTS has some fascinating properties, but being ternary, I didn\'t think I\'d be able to find a compact bit representation for it.</p>\n\n<p>Then I noticed that because of the arithmetic rules, the pattern $\\bar 1 \\bar 1$ never occurs as long as you only allow numbers $\\ge 0$.  This means that the nine possible combinations for each pair of trits ($3^2$) only ever has 8 values, so we can encode 2 trits with 3 bits ($2^3$, a.k.a octal).  Also note that the left-most bit (and also right-most for integers because of the mirror-symmetric property) will only ever be $0$ or $1$ (again for positive numbers only), which lets us encode the left-most trit with only 1 bit.</p>\n\n<p>So a $2^n$-bit number can store $\\lfloor 2^n/3\\rfloor * 2 + 1$ balanced trits, possibly with a bit left over (maybe a good candidate for a sign bit).  For example, we can represent $10 + 1 = 11$ balanced trits with $15 + 1 = 16$  bits, or $20 + 1 = 21$ balanced trits with $30 + 1 = 31$ bits, with 1 left over (32-bit).  This has much better space density than ordinary golden ratio base binary encoding.</p>\n\n<p>So my question is, what would be a good octal (3-bit) encoding of trit pairs such that we can implement the addition and other arithmetic rules of the BTTS with as little difficulty as possible.  One of the tricky aspects of this system is that carries happen in both directions, i.e. <br/>\n$1 + 1 = 1 \\bar 1 .1$ and $\\bar 1 + \\bar 1 = \\bar 1 1.\\bar 1$.</p>\n\n<p>This is my first post here, so please let me know if I need to fix or clarify anything.</p>\n\n<p>--<strong>Edit</strong>--</p>\n\n<p>ex0du5 asked for some clarification of what I need from a binary representation:</p>\n\n<ol>\n<li>I want to be able to represent positive values of both integers and powers of $\\phi$.  The range of representable values need not be as good as binary, but it should be better than phinary per bit.  I want to represent the largest possible set of phinary numbers in the smallest amount of space possible.  Space takes priority over operation count for arithmetic operations.</li>\n<li>I need addition to function such that carries happen in both directions.  Addition will be the most common operation for my application.  Consequently it should require as few operations as possible.  If a shorter sequence of operations are possible using a longer bit representation (conflicting with goal 1), then goal 1 takes priority.  Space is more important than speed.</li>\n<li>Multiplication only needs to handle integers > 0 multiplied to a phinary number, not arbitrary phinary number multiplication, and so can technically be emulated with a series of additions, though a faster algorithm would be helpful.</li>\n<li>I\'m ignoring division and subtraction for now, but having algorithms for them would be a bonus.</li>\n<li>I need to eventually convert a phinary number to a binary floating point approximation of it\'s value, but this will happen only just prior to output.  There will be no converting back and forth.</li>\n</ol>\n', 'ViewCount': '294', 'Title': 'What is a good binary encoding for $\\phi$-based balanced ternary arithmetic algorithms?', 'LastEditorUserId': '2230', 'LastActivityDate': '2012-11-18T06:37:19.827', 'LastEditDate': '2012-07-26T14:22:13.457', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2230', 'Tags': '<algorithms><data-structures><efficiency><coding-theory>', 'CreationDate': '2012-07-20T21:32:29.663', 'Id': '2847'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need a super-fast method for ordinary differential equations. Should I use the <a href="http://en.wikipedia.org/wiki/Midpoint_method" rel="nofollow">midpoint method</a>? I need this for a <a href="http://en.wikipedia.org/wiki/Reaction%E2%80%93diffusion_system" rel="nofollow">reaction-diffusion system</a> (Gray-Scott).</p>\n', 'ViewCount': '81', 'Title': "Which method for ODE instead of Euler's?", 'LastEditorUserId': '472', 'LastActivityDate': '2012-07-21T20:37:26.090', 'LastEditDate': '2012-07-21T20:37:26.090', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2240', 'Tags': '<algorithms><numerical-analysis>', 'CreationDate': '2012-07-21T20:02:43.227', 'Id': '2854'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>We are given a set of objects, say integers, $S$. In addition, we are given a predicate $P$, for example $P(i): \\Leftrightarrow i \\geq 0$. We don't know in advance how many elements of $S$ satisfy the predicate $P$, but we would like to sample or choose an element uniformly at random from $S' = \\{ i \\mid i \\in S \\wedge P(i) \\}$.</p>\n\n<p>The naive approach is to scan $S$ and for example record all the integers or indices for which $P$ holds, then choose one of them uniformly at random. The downside is that in the worst-case, we need $|S|$ space.</p>\n\n<p>For large sets or in say a streaming environment the naive approach is not acceptable. Is there an in-place algorithm for the problem?</p>\n", 'ViewCount': '112', 'Title': 'Choosing an element from a set satisfying a predicate uniformly at random in $O(1)$ space', 'LastActivityDate': '2012-07-21T21:45:31.217', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2856', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><randomized-algorithms><streaming-algorithm><in-place>', 'CreationDate': '2012-07-21T21:45:31.217', 'Id': '2855'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '182', 'Title': 'Determining the particular number in $O(n)$ time and space (worst case)', 'LastEditDate': '2012-07-25T03:17:34.603', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1715', 'FavoriteCount': '1', 'Body': "<p>$\\newcommand\\ldotd{\\mathinner{..}}$Given that $A[1\\ldotd n]$ are integers such that $0\\le A[k]\\le m$ for all $1\\le k\\le n$, and the occurrence of each number except a particular number in $A[1\\ldotd n]$ is an odd number. Try to find the number whose occurrence is an even number.</p>\n\n<p>There is an $\\Theta(n\\log n)$ algorithm: we sort $A[1\\ldotd n]$ into $B[1\\ldotd n]$, and break $B[1\\ldotd n]$ into many pieces, whose elements' value are the same, therefore we can count the occurrence of each element.</p>\n\n<p>I want to find a worst-case-$O(n)$-time-and-$O(n)$-space algorithm.</p>\n\n<p>Supposing that $m=\\Omega(n^{1+\\epsilon})$ and $\\epsilon&gt;0$, therefore radix sort is not acceptable.\n$\\DeclareMathOperator{\\xor}{xor}$\nBinary bitwise operations are acceptable, for example, $A[1]\\xor A[2]$.</p>\n", 'Tags': '<algorithms><search-algorithms>', 'LastEditorUserId': '1715', 'LastActivityDate': '2012-07-25T03:17:34.603', 'CommentCount': '15', 'AcceptedAnswerId': '2867', 'CreationDate': '2012-07-23T02:31:17.933', 'Id': '2863'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '175', 'Title': 'Bound on space for selection algorithm?', 'LastEditDate': '2012-07-24T08:41:54.167', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '67', 'FavoriteCount': '2', 'Body': '<p>There is a well known worst case $O(n)$ <a href="http://en.wikipedia.org/wiki/Selection_algorithm" rel="nofollow">selection algorithm</a> to find the $k$\'th largest element in an array of integers.  It uses a <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Properties_of_pivot" rel="nofollow">median-of-medians</a> approach to find a good enough pivot, partitions the input array in place and then recursively continues in it\'s search for the $k$\'th largest element.</p>\n\n<p>What if we weren\'t allowed to touch the input array, how much extra space would be needed in order to find the $k$\'th largest element in $O(n)$ time?  Could we find the $k$\'th largest element in $O(1)$ extra space and still keep the runtime $O(n)$?  For example, finding the maximum or minimum element takes $O(n)$ time and $O(1)$ space.  </p>\n\n<p>Intuitively, I cannot imagine that we could do better than $O(n)$ space but is there a proof of this?</p>\n\n<p>Can someone point to a reference or come up with an argument why the $\\lfloor n/2 \\rfloor$\'th element would require $O(n)$ space to be found in $O(n)$ time?</p>\n', 'Tags': '<algorithms><algorithm-analysis><space-complexity><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-26T18:36:48.300', 'CommentCount': '1', 'AcceptedAnswerId': '2896', 'CreationDate': '2012-07-24T03:19:59.790', 'Id': '2893'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a very specific question about semantic clustering.</p>\n\n<p>I have a list of words/phrases. I want to run an intelligent semantic clustering algorithm on this list. Please let me know what the available options are. Definitely I am looking for NLP based algorithms.</p>\n\n<p>Simple, open-source, easy-to-use solutions will be highly appreciated. The semantic part is extremely important here.</p>\n', 'ViewCount': '523', 'Title': 'Semantic clustering', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-27T18:49:17.373', 'LastEditDate': '2012-07-27T14:31:08.407', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2288', 'Tags': '<algorithms><strings><string-metrics><natural-lang-processing><ontologies>', 'CreationDate': '2012-07-27T10:56:33.110', 'FavoriteCount': '1', 'Id': '2922'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<h3>Background</h3>\n\n<p>$\\newcommand\\ldotd{\\mathinner{..}}$Last month, I heard about a new linear-time algorithm to determine the <a href="https://en.wikipedia.org/wiki/Longest_palindromic_substring" rel="nofollow">longest palindromic substring</a> called Jeuring\'s algorithm. It seemed interesting, therefore I had a try to analyze the algorithm. It\'s not too difficult to show that it\'s an $\\Theta(n)$-time-algorithm, but I want to observe more closely.</p>\n\n<p>I wrote a C++ implementation, and an essay to prove and analyze the algorithm. It\'s somewhat long, therefore I will only post the critical part of the code, where you can learn the algorithm by heart, see the succeeding section. You can get all my previous work from <a href="https://github.com/FrankEular/pldrm" rel="nofollow">github</a>. pldrm.nw is the literate programming source file, which produces the pldrm.cc and pldrm.pdf. pldrm.pdf is the essay I\'ve written.</p>\n\n<h3>Problem</h3>\n\n<p>Let $A=$ the number of times <em>goto</em> statement is executed in <em>move loop</em>, and $B=$ the number of times <em>move loop</em> is executed where <em>goto</em> statement is <strong>NOT</strong> executed, $C=$ the number of times a[++j]=min(a[p],l) is executed in <em>move loop</em>. You can see the code in the following section.\nI found that $A+B+C=2n$ and $B=\\sum_{k=2}^{n+1}[b_k=1]$, where $b_k$ is the length of longest tail palindrome of $s[0\\ldotd k]$, and $[P]$ is <a href="http://en.wikipedia.org/wiki/Iverson_bracket" rel="nofollow">Iverson bracket</a>. For details, you can read my pdf from <a href="https://github.com/FrankEular/pldrm" rel="nofollow">github</a>.\nI\'m looking for somebody to help me analyze the quantity $A,C$. Any help? Thanks!</p>\n\n<h3>The critical part of the algorithm</h3>\n\n<p>Given that $s[1\\ldotd n]$ is the string inputted, and $n$ is the length of $s$, where $s[0]=1,s[n+1]=0$.\nWe say $l+r$ is the center of substring $s[l\\ldotd r]$. For example, $4$ is the center of $s[2\\ldotd2]$ or $s[1\\ldotd3]$.\n$a_k$ is the length of the longest palindrome whose center is $k$, and $a[0\\ldotd2n]$ is the array to save $\\langle a_k\\rangle$.\n<strong>The algorithm is used to determine $a_k$.</strong>\nWe call some string $A$ is a tail palindrome of the other string $B$ if and only if $A$ is a palindromic tail substring of $B$. For example, $A=aba$ and $B=aaaababa$, where $A$ is palindromic and $A$ is a tail substring of $B$.</p>\n\n<p>Here\'s the critical part of the code:</p>\n\n<pre><code>&lt;&lt;main loop&gt;&gt;=\nj = 1;\nl = 1;\na[0] = 1;\na[1] = 0;\nfor (int k=2; k&lt;=n+1; k++) {\n  &lt;&lt;process&gt;&gt;\n  advance:\n  ;\n}\n@\n</code></pre>\n\n<p>Process is made up of an infinite loop, which is used to find the longest tail palindrome of $s[0\\ldotd k]$. There are two exits of it. One is in extension subroutine, while the other one is in move loop. The way of exit is <em>goto advance;</em>.</p>\n\n<pre><code>&lt;&lt;process&gt;&gt;=\nfor (;;) {\n  &lt;&lt;check&gt;&gt;\n  &lt;&lt;move loop&gt;&gt;\n}\n@\n</code></pre>\n\n<p>The check subroutine checks whether a tail palindrome of $s[0\\ldotd k-1]$ could be extended to that of $s[0\\ldotd k]$. If so, exit from the process loop and advance $k$, otherwise start the move loop.</p>\n\n<pre><code>&lt;&lt;check&gt;&gt;=\nif (s[k] == s[k-l-1]) {\n  l += 2;\n  goto advance;\n}\n@\n</code></pre>\n\n<p>Here\'s the move loop, which looks short and easy. It\'s used to find a shorter tail palindrome of $s[0\\ldotd k-1]$.</p>\n\n<pre><code>&lt;&lt;move loop&gt;&gt;=\na[++j] = l;\nfor (p=j-1; --l&gt;=0&amp;&amp;l!=a[p]; p--) {\n  a[++j] = min(a[p], l);\n}\nif (l &lt; 0) {\n  l = 1;\n  goto advance;\n}\n@\n</code></pre>\n', 'ViewCount': '510', 'Title': 'Analysis of a linear-time algorithm for longest palindromic substring', 'LastEditorUserId': '29', 'LastActivityDate': '2012-08-12T13:37:49.487', 'LastEditDate': '2012-08-12T13:37:49.487', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1715', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-07-28T04:29:45.943', 'Id': '2936'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been studying the three and I\'m stating my inferences from them below. Could someone tell me if I have understood them accurately enough or not? Thank you.</p>\n\n<ol>\n<li><p><a href="https://en.wikipedia.org/wiki/Dijkstra_algorithm" rel="nofollow">Dijkstra algorithm</a> is used only when you have a single source and you want to know the smallest path from one node to another, but fails in cases like <a href="http://i.stack.imgur.com/rmowk.png" rel="nofollow">this</a>.</p></li>\n<li><p><a href="https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm" rel="nofollow">Floyd-Warshall algorithm</a> is used when any of all the nodes can be a source, so you want the shortest distance to reach any destination node from any source node. This only fails when there are negative cycles.</p></li>\n<li><p><a href="https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm" rel="nofollow">Bellman-Ford</a> is used like Dijkstra, when there is only one source. This can handle negative weights and its working is the same as Floyd-Warshall except for one source, right? (This is the one I am least sure about.)</p></li>\n</ol>\n', 'ViewCount': '1458', 'Title': 'Am I right about the differences between Floyd-Warshall, Dijkstra and Bellman-Ford algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-29T19:58:24.517', 'LastEditDate': '2012-07-29T13:58:33.600', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'Programming Noob', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-07-28T21:07:40.740', 'FavoriteCount': '1', 'Id': '2942'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have trouble understanding how to calculate the depth of a sorting network on $n$ inputs.</p>\n\n<p>For example, in case of selection sort, we have:</p>\n\n<p>$\\qquad \\displaystyle D(n)=D(n-1)+2\\\\\\qquad D(2)=1$</p>\n\n<p>which leads to</p>\n\n<p>$\\qquad \\displaystyle D(n)=2n-3=\\Theta(n)$</p>\n\n<p>I have confirmed that the depth of selection sort is equal to $2n-3$ by hand, but I can't understand how the recurrence $D(n)=D(n-1)+2$ is derived.</p>\n", 'ViewCount': '292', 'Title': 'How to calculate the depth of sorting networks?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-03T04:32:08.207', 'LastEditDate': '2012-07-30T07:08:14.057', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'Steven', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation><sorting>', 'CreationDate': '2012-07-29T11:18:33.773', 'FavoriteCount': '1', 'Id': '2950'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '415', 'Title': 'Generating inputs for random-testing graph algorithms?', 'LastEditDate': '2012-08-04T16:01:43.273', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '2', 'Body': '<p>When testing algorithms, a common approach is random testing: generate a significant number of inputs according to some distribution (usually uniform), run the algorithm on them and verify correctness. Modern testing frameworks can generate inputs automatically given the algorithms signature, with some restrictions.</p>\n\n<p>If the inputs are numbers, lists or strings, generating such inputs in straight-forward. Trees are harder, but still easy (using stochastic context-free grammars or similar approaches).</p>\n\n<p>How can you generate random graphs (efficiently)? Usually, picking graphs uniformly at random is not what you want: they should be connected, or planar, or cycle-free, or fulfill any other property. Rejection sampling seems suboptimal, due to the potentially huge set of undesirable graphs.</p>\n\n<p>What are useful distributions to look at? Useful here means that</p>\n\n<ul>\n<li>the graphs are likely to test the algorithm at hand well and</li>\n<li>they can be generated effectively and efficiently.</li>\n</ul>\n\n<p>I know that there are many models for random graphs, so I\'d appreciate some insight into which are best for graph generation in this regard.</p>\n\n<p>If "some algorithm" is too general, please use shortest-path finding algorithms as a concrete class of algorithms under test. Graphs for testing should be connected and rather dense (with high probability, or at least in expectation). For testing, the optimal solution would be to create random graphs around a shortest path so we <em>know</em> the desired result (without having to employ another algorithm).</p>\n', 'Tags': '<algorithms><graphs><randomness><software-testing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T16:33:06.190', 'CommentCount': '1', 'AcceptedAnswerId': '2953', 'CreationDate': '2012-07-30T21:36:29.087', 'Id': '2952'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there a relatively simple way of telling if two pieces of text are semantically similar?</p>\n\n<p>Some assumptions that are valid:</p>\n\n<ul>\n<li>It is all english</li>\n<li>I have a list of all the <em>important</em> nouns</li>\n</ul>\n\n<p>Are there any strategies that I should pursue? Looking for something that is relatively computationally cheap, though something that could be scaled to improve accuracy at the expense of computational power would be a bonus.</p>\n\n<p><strong>Note:</strong></p>\n\n<p>Assume that there are not enough posts for some type of probabilistic analysis, but some type of NN might be feasible (I think, just don't know enough about it).</p>\n", 'ViewCount': '144', 'Title': 'Semantic similarity in text', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-01T01:56:11.157', 'LastEditDate': '2012-07-31T09:09:35.170', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '863', 'Tags': '<algorithms><natural-lang-processing><ontologies>', 'CreationDate': '2012-07-30T23:28:34.217', 'FavoriteCount': '2', 'Id': '2955'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to make a sequence of numbers, where I pick the numbers $a_{0}, a_{1},..,a_{n}$. The length of the sequence is $n+1$.</p>\n\n<p>Now I want the product of any pair of two numbers in the sequence modulo $k$ to be guaranteed to be unique and $k$ has to be as small as it can be in such a way that the resulting set has to be $\\{r_{0},r_{1},\\ldots,r_{k-1\\}}$. So</p>\n\n<p>$$a_{0}a_{1} \\bmod k \\rightarrow r_{0} \\\\\na_{0}a_{2} \\bmod k \\rightarrow r_{1} \\\\\na_{1}a_{2} \\bmod k \\rightarrow r_{2} \\\\\n\\vdots\\\\\na_{n-1}a_{n} \\bmod k \\rightarrow r_{k-1}$$</p>\n\n<p>First I was thinking about using prime numbers, so that the products could be unique, but I have to find a property so that every product is not congruent to every other product. </p>\n\n<p>To be more general: I am looking for a sequence of length $n$ and minimal $k$ such that </p>\n\n<p>$\\qquad \\displaystyle |\\{a_ia_j \\mod k \\mid 0 \\leq i &lt; j \\leq n\\}| = n(n+1)$.</p>\n\n<p>So a mapping from every product mod k to "a value" filling up every number from $0$ to $k-1$.</p>\n', 'ViewCount': '171', 'Title': 'Finding a sequence of numbers where every product of two mod k is unique', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-01T21:59:34.120', 'LastEditDate': '2012-08-01T21:59:34.120', 'AnswerCount': '0', 'CommentCount': '19', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2317', 'Tags': '<algorithms><number-theory>', 'CreationDate': '2012-07-31T08:03:37.330', 'FavoriteCount': '1', 'Id': '2957'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '880', 'Title': 'Example using Penetrance & Branching Factor in State-space Heuristic Search', 'LastEditDate': '2012-08-02T06:43:50.807', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2321', 'FavoriteCount': '2', 'Body': u'<p>I need an example for how to calculate penetrance and branching factor of the search tree in in state-space heuristic search. The definitions are as following. <em>Penetrance</em> $P$ is defined by</p>\n\n<p>$\\qquad \\displaystyle P = \\frac{L}{T}$</p>\n\n<p>and <em>branching factor</em> $B$ is defined by</p>\n\n<p>$\\qquad \\displaystyle \\frac{B}{(B-1)} \\cdot (B^L \u2013 1) = T$ </p>\n\n<p>where $L$ is the length of the path from the root to the solution and $T$ the total number of nodes expanded.</p>\n', 'Tags': '<artificial-intelligence><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-26T03:14:19.410', 'CommentCount': '0', 'AcceptedAnswerId': '2964', 'CreationDate': '2012-07-31T12:54:12.277', 'Id': '2961'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On <a href="https://en.wikipedia.org/wiki/Levenshtein_distance#Computing_Levenshtein_distance">Wikipedia</a>, an implementation for the bottom-up dynamic programming scheme for the edit distance is given. It does not follow the definition completely; inner cells are computed thus:</p>\n\n<pre><code>if s[i] = t[j] then  \n  d[i, j] := d[i-1, j-1]       // no operation required\nelse\n  d[i, j] := minimum\n             (\n               d[i-1, j] + 1,  // a deletion\n               d[i, j-1] + 1,  // an insertion\n               d[i-1, j-1] + 1 // a substitution\n             )\n}\n</code></pre>\n\n<p>As you can see, the algorithm <em>always</em> chooses the value from the upper-left neighbour if there is a match, saving some memory accesses, ALU operations and comparisons. </p>\n\n<p>However, deletion (or insertion) may result in a <em>smaller</em> value, thus the algorithm is locally incorrect, i.e. it breaks with the optimality criterion. But maybe the mistake does not change the end result -- it might be cancelled out.</p>\n\n<p>Is this micro-optimisation valid, and why (not)?</p>\n', 'ViewCount': '306', 'Title': 'Micro-optimisation for edit distance computation: is it valid?', 'LastActivityDate': '2012-08-02T07:32:35.867', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2997', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><dynamic-programming><string-metrics><correctness-proof><program-optimization>', 'CreationDate': '2012-08-01T15:41:33.670', 'Id': '2985'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '748', 'Title': 'How fast can we find all Four-Square combinations that sum to N?', 'LastEditDate': '2012-08-02T16:54:45.280', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2334', 'FavoriteCount': '2', 'Body': '<p>A question was asked at Stack Overflow (<a href="http://stackoverflow.com/questions/11732555/how-to-find-all-possible-values-of-four-variables-when-squared-sum-to-n#comment15599644_11732555">here</a>):</p>\n\n<blockquote>\n  <p>Given an integer $N$, print out all possible\n  combinations of integer values of $A,B,C$ and $D$ which solve the equation $A^2+B^2+C^2+D^2 = N$.</p>\n</blockquote>\n\n<p>This question is of course related to <a href="http://en.wikipedia.org/wiki/Lagrange%27s_four-square_theorem" rel="nofollow">Bachet\'s Conjecture</a> in number theory (sometimes called Lagrange\'s Four Square Theorem because of his proof).  There are some papers that discuss how to find a single solution, but I have been unable to find anything that talks about how fast we can find <em>all</em> solutions for a particular $N$ (that is, all <em>combinations</em>, not all <em>permutations</em>).</p>\n\n<p>I have been thinking about it quite a bit and it seems to me that it can be solved in $O(N)$ time and space, where $N$ is the desired sum. However, lacking any prior information on the subject, I am not sure if that is a significant claim on my part or just a trivial, obvious or already known result.</p>\n\n<p>So, the question then is, how fast can we find all of the Four-Square Sums for a given $N$?</p>\n\n<hr>\n\n<p>OK, here\'s the (nearly) O(N) algorithm that I was thinking of.  First two supporting functions, a nearest integer square root function:</p>\n\n<pre><code>    // the nearest integer whose square is less than or equal to N\n    public int SquRt(int N)\n    {\n        return (int)Math.Sqrt((double)N);\n    }\n</code></pre>\n\n<p>And a function to return all TwoSquare pairs summing from 0 to N:</p>\n\n<pre><code>    // Returns a list of all sums of two squares less than or equal to N, in order.\n    public List&lt;List&lt;int[]&gt;&gt; TwoSquareSumsLessThan(int N)\n    {\n        //Make the index array\n        List&lt;int[]&gt;[] Sum2Sqs = new List&lt;int[]&gt;[N + 1];\n\n        //get the base square root, which is the maximum possible root value\n        int baseRt = SquRt(N);\n\n        for (int i = baseRt; i &gt;= 0; i--)\n        {\n            for (int j = 0; j &lt;= i; j++)\n            {\n                int sum = (i * i) + (j * j);\n                if (sum &gt; N)\n                {\n                    break;\n                }\n                else\n                {\n                    //make the new pair\n                    int[] sumPair = { i, j };\n                    //get the sumList entry\n                    List&lt;int[]&gt; sumLst;\n                    if (Sum2Sqs[sum] == null)\n                    {   \n                        // make it if we need to\n                        sumLst = new List&lt;int[]&gt;();\n                        Sum2Sqs[sum] = sumLst;\n                    }\n                    else\n                    {\n                        sumLst = Sum2Sqs[sum];\n                    }\n                    // add the pair to the correct list\n                    sumLst.Add(sumPair);\n                }\n            }\n        }\n\n        //collapse the index array down to a sequential list\n        List&lt;List&lt;int[]&gt;&gt; result = new List&lt;List&lt;int[]&gt;&gt;();\n        for (int nn = 0; nn &lt;= N; nn++)\n        {\n            if (Sum2Sqs[nn] != null) result.Add(Sum2Sqs[nn]);\n        }\n\n        return result;\n    }\n</code></pre>\n\n<p>Finally, the algorithm itself:</p>\n\n<pre><code>    // Return a list of all integer quads (a,b,c,d), where:\n    //      a^2 + b^2 + c^2 + d^2 = N,\n    // and  a &gt;= b &gt;= c &gt;= d,\n    // and  a,b,c,d &gt;= 0\n    public List&lt;int[]&gt; FindAllFourSquares(int N)\n    {\n        // get all two-square sums &lt;= N, in descending order\n        List&lt;List&lt;int[]&gt;&gt; Sqr2s = TwoSquareSumsLessThan(N);\n\n        // Cross the descending list of two-square sums &lt;= N with\n        // the same list in ascending order, using a Merge-Match\n        // algorithm to find all combinations of pairs of two-square\n        // sums that add up to N\n        List&lt;int[]&gt; hiList, loList;\n        int[] hp, lp;\n        int hiSum, loSum;\n        List&lt;int[]&gt; results = new List&lt;int[]&gt;();\n        int prevHi = -1;\n        int prevLo = -1;\n\n        //  Set the Merge sources to the highest and lowest entries in the list\n        int hi = Sqr2s.Count - 1;\n        int lo = 0;\n\n        //  Merge until done ..\n        while (hi &gt;= lo)\n        {\n            // check to see if the points have moved\n            if (hi != prevHi)\n            {\n                hiList = Sqr2s[hi];\n                hp = hiList[0];     // these lists cannot be empty\n                hiSum = hp[0] * hp[0] + hp[1] * hp[1];\n                prevHi = hi;\n            }\n            if (lo != prevLo)\n            {\n                loList = Sqr2s[lo];\n                lp = loList[0];     // these lists cannot be empty\n                loSum = lp[0] * lp[0] + lp[1] * lp[1];\n                prevLo = lo;\n            }\n\n            // do the two entries\' sums together add up to N?\n            if (hiSum + loSum == N)\n            {\n                // they add up, so cross the two sum-lists over each other\n                foreach (int[] hiPair in hiList)\n                {\n                    foreach (int[] loPair in loList)\n                    {\n                        // make a new 4-tuple and fill it\n                        int[] quad = new int[4];\n                        quad[0] = hiPair[0];\n                        quad[1] = hiPair[1];\n                        quad[2] = loPair[0];\n                        quad[3] = loPair[1];\n\n                        // only keep those cases where the tuple is already sorted\n                        //(otherwise it\'s a duplicate entry)\n                        if (quad[1] &gt;= quad[2]) //(only need to check this one case, the others are implicit)\n                        {\n                            results.Add(quad);\n                        }\n                        //(there\'s a special case where all values of the 4-tuple are equal\n                        // that should be handled to prevent duplicate entries, but I\'m\n                        // skipping it for now)\n                    }\n                }\n                // both the HI and LO points must be moved after a Match\n                hi--;\n                lo++;\n            }\n            else if (hiSum + loSum &lt; N)\n            {\n                lo++;   // too low, so must increase the LO point\n            }\n            else    // must be &gt; N\n            {\n                hi--;   // too high, so must decrease the HI point\n            }\n        }\n        return results;\n    }\n</code></pre>\n\n<p>As I said before, it should be pretty close to O(N), however, as Yuval Filmus points out, as the number of Four Square solutions to N can be of order (N ln ln N), then this algorithim could not be less than that.</p>\n', 'Tags': '<algorithms><complexity-theory><performance><number-theory>', 'LastEditorUserId': '2334', 'LastActivityDate': '2012-08-02T16:54:45.280', 'CommentCount': '5', 'AcceptedAnswerId': '3003', 'CreationDate': '2012-08-01T20:29:11.273', 'Id': '2988'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve just begun this stage 2 Compsci paper on algorithms, and stuff like this is not my strong point. I\'ve come across this in my lecture slides.</p>\n\n<pre><code>int length = input.length();\nfor (int i = 0; i &lt; length - 1; i++) {\n    for (int j = i + 1; j &lt; length; j++) {\n        System.out.println(input.substring(i,j));\n    }\n}\n</code></pre>\n\n<p>"In each iteration, the outer loop executes $\\frac{n^{2}-(2i-1)n-i+i^{2}}{2}$ operations from the inner loop for $i = 0, \\ldots, n-1$."</p>\n\n<p>Can someone please explain this to me step by step?</p>\n\n<p>I believe the formula above was obtained by using Gauss\' formula for adding numbers... I think...</p>\n', 'ViewCount': '1969', 'Title': 'Time complexity formula of nested loops', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-02T08:36:54.223', 'LastEditDate': '2012-08-02T08:36:54.223', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '2996', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-02T04:56:03.523', 'Id': '2994'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m currently looking at network clustering algorithms (we\'re currently looking at both directed and undirected, unweighted networks).  The algorithms we\'ve tried produce visually nice clusters.  However, we would like to evaluate them against some "gold standard" -- a collection of networks in which the clusters have already been identified (and are more-or-less regarded as <em>the</em> clusters by the complex network\'s research community).</p>\n\n<blockquote>\n  <p><strong>Question</strong>: What are the gold standards for network clustering algorithms?  Where can I access them?</p>\n</blockquote>\n', 'ViewCount': '91', 'Title': 'Seeking "gold standard" to evaluate accuracy of network clustering algorithm', 'LastActivityDate': '2012-08-05T13:36:02.027', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2254', 'Tags': '<algorithms><data-sets><network-topology>', 'CreationDate': '2012-08-03T03:24:45.980', 'Id': '3013'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '11355', 'Title': 'What is the novelty in MapReduce?', 'LastEditDate': '2012-08-04T09:11:58.820', 'AnswerCount': '4', 'Score': '39', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '16', 'Body': u'<p>A few years ago, <a href="https://en.wikipedia.org/wiki/Mapreduce">MapReduce</a> was hailed as revolution of distributed programming. There have also been <a href="http://craig-henderson.blogspot.de/2009/11/dewitt-and-stonebrakers-mapreduce-major.html">critics</a> but by and large there was an enthusiastic hype. It even got patented! [1]</p>\n\n<p>The name is reminiscent of <code>map</code> and <code>reduce</code> in functional programming, but when I read (Wikipedia)</p>\n\n<blockquote>\n  <p><strong>Map step:</strong> The master node takes the input, divides it into smaller sub-problems, and distributes them to worker nodes. A worker node may do this again in turn, leading to a multi-level tree structure. The worker node processes the smaller problem, and passes the answer back to its master node.</p>\n  \n  <p><strong>Reduce step:</strong> The master node then collects the answers to all the sub-problems and combines them in some way to form the output \u2013 the answer to the problem it was originally trying to solve.</p>\n</blockquote>\n\n<p>or [2] </p>\n\n<blockquote>\n  <p><strong>Internals of MAP:</strong> [...] MAP splits up the input value into words. [...] MAP is meant to associate each given key/value pair of the input with potentially many intermediate key/value pairs.</p>\n  \n  <p><strong>Internals of REDUCE:</strong> [...] [REDUCE] performs imperative aggregation (say, reduction): take many values, and reduce them to a single value.</p>\n</blockquote>\n\n<p>I can not help but think: this is <a href="https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm">divide &amp; conquer</a> (in the sense of Mergesort), plain and simple! So, is there (conceptual) novelty in MapReduce somewhere, or is it just a new implementation of old ideas useful in certain scenarios?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PALL&amp;p=1&amp;u=/netahtml/PTO/srchnum.htm&amp;r=1&amp;f=G&amp;l=50&amp;s1=7,650,331.PN.&amp;OS=PN/7,650,331&amp;RS=PN/7,650,331"> US Patent 7,650,331: "System and method for efficient large-scale data processing "</a> (2010)</li>\n<li><a href="http://dx.doi.org/10.1016/j.scico.2007.07.001">Google\u2019s MapReduce programming model \u2014 Revisited</a> by R. L\xe4mmel (2007)</li>\n</ol>\n', 'Tags': '<algorithms><distributed-systems><parallel-computing><algorithm-design>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-11T04:57:53.957', 'CommentCount': '4', 'AcceptedAnswerId': '3020', 'CreationDate': '2012-08-03T14:04:19.350', 'Id': '3019'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In the general case finding a Maximum Independent Subset of a Graph is NP-Hard.</p>\n\n<p>However consider the following subset of graphs:</p>\n\n<ul>\n<li>Create an $N \\times N$ grid of unit square cells.</li>\n<li>Build a graph $G$ by creating a vertex corresponding to every cell.  Notice that there are $N^2$ vertices.</li>\n<li>Create an edge between two vertices if their cells share a side.  Notice there are $2N(N-1)$ edges.</li>\n</ul>\n\n<p>A Maximum Independent Subset of $G$ is obviously a checker pattern.  A cell at the $R$th row and $C$th column is part of it if $R+C$ is odd.</p>\n\n<p>Now we create a graph $G'$ by copying $G$ and removing some vertices and edges.  (If you remove a vertex also remove all edges it ended of course. Also note you can remove an edge without removing one of the vertices it ends.)</p>\n\n<p>By what algorithm can we find a Maximum Independent Subset of $G'$?</p>\n", 'ViewCount': '331', 'Title': 'Maximum Independent Subset of 2D Grid Subgraph', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-23T11:58:45.633', 'LastEditDate': '2012-08-03T20:24:34.503', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '3301', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1577', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-08-03T16:23:56.503', 'Id': '3022'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to find the Maximum Independent Set of a Biparite Graph.</p>\n\n<p>I found this in some notes <strong>"May 13, 1998 - University of Washington - CSE 521 - Applications of network flow"</strong>:</p>\n\n<blockquote>\n  <p><strong>Problem:</strong></p>\n  \n  <p>Given a bipartite graph $G = (U,V,E)$, find an independent set $U\' \\cup V\'$ which is as large as possible, where $U\' \\subseteq U$ and $V\'\n&gt; \\subseteq V$. A set is independent if there are no edges of $E$ between\n  elements of the set.</p>\n  \n  <p><strong>Solution:</strong></p>\n  \n  <p>Construct a flow graph on the vertices $U \\cup B \\cup \\{s,t\\}$. For\n  each edge $(u,v) \\in E$ there is an infinite capacity edge from $u$ to\n  $v$. For each $u \\in U$, there is a unit capacity edge from $s$ to $u$,\n  and for each $v \\in V$, there is a unit capacity edge from $v$ to\n  $t$.</p>\n  \n  <p>Find a finite capacity cut $(S,T)$, with $s \\in S$ and $t \\in T$. Let\n  $U\' = U \\cap S$ and $V\' = V \\cap T$. The set $U\' \\cup V\'$ is\n  independent since there are no infinite capacity edges crossing the\n  cut.  The size of the cut is $|U - U\'| + |V - V\'| = |U| + |V| - |U\' \\cup V\'|$. This, in order to make the independent set as large as\n  possible, we make the cut as small as possible.</p>\n</blockquote>\n\n<p>So lets take this as the graph:</p>\n\n<pre><code>A - B - C\n    |\nD - E - F\n</code></pre>\n\n<p>We can split this into a bipartite graph as follows $(U,V)=(\\{A,C,E\\},\\{B,D,F\\})$</p>\n\n<p>We can see by brute force search that the sole Maximum Independent Set is $A,C,D,F$. Lets try and work through the solution above:</p>\n\n<p>So the constructed flow network adjacency matrix would be:</p>\n\n<p>$$\\begin{matrix}\n      &amp; s &amp; t &amp; A &amp; B &amp; C &amp; D &amp; E &amp; F \\\\\n    s &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\\n    t &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\\n    A &amp; 1 &amp; 0 &amp; 0 &amp; \\infty &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n    B &amp; 0 &amp; 1 &amp; \\infty &amp; 0 &amp; \\infty &amp; 0 &amp; \\infty &amp; 0 \\\\\n    C &amp; 1 &amp; 0 &amp; 0 &amp; \\infty &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\n    D &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\infty &amp; 0 \\\\\n    E &amp; 1 &amp; 0 &amp; 0 &amp; \\infty &amp; 0 &amp; \\infty &amp; 0 &amp; \\infty \\\\\n    F &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\infty &amp; 0 \\\\\n\\end{matrix}$$</p>\n\n<p>Here is where I am stuck, the smallest finite capacity cut I see is a trivial one: $(S,T) =(\\{s\\},\\{t,A,B,C,D,E,F\\})$ with a capacity of 3.</p>\n\n<p>Using this cut leads to an incorrect solution of:</p>\n\n<p>$$ U\' = U \\cap S = \\{\\}$$\n$$ V\' = V \\cap T = \\{B,D,F\\}$$\n$$ U\' \\cup V\' = \\{B,D,F\\}$$</p>\n\n<p>Whereas we expected $U\' \\cup V\' = \\{A,C,D,F\\}$?  Can anyone spot where I have gone wrong in my reasoning/working?</p>\n', 'ViewCount': '2163', 'Title': 'Maximum Independent Set of a Bipartite Graph', 'LastActivityDate': '2013-08-27T14:52:52.030', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '3033', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1577', 'Tags': '<algorithms><graph-theory><network-flow>', 'CreationDate': '2012-08-03T19:21:39.403', 'FavoriteCount': '1', 'Id': '3027'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Wondering about any known relations between <a href="http://qwiki.stanford.edu/index.php/Complexity_Zoo%3aR#rl" rel="nofollow">$\\mathsf{RL}$</a> complexity class (one sided error with logarithmic space) and its complementary class, $\\mathsf{coRL}$.</p>\n\n<p>Are they the same class?</p>\n\n<p>What are $\\mathsf{coRL}$\'s relation to $\\mathsf{NL}$, $\\mathsf{P}$?</p>\n', 'ViewCount': '99', 'Title': 'What is known about coRL and RL?', 'LastActivityDate': '2012-08-03T21:42:01.333', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3032', 'Score': '4', 'OwnerDisplayName': 'Uri', 'PostTypeId': '1', 'OwnerUserId': '2356', 'Tags': '<complexity-theory><complexity-classes><probabilistic-algorithms>', 'CreationDate': '2012-08-01T23:47:44.633', 'Id': '3031'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $M$ denote a set of $n$ positive integers, each less than $n^c$.</p>\n\n<p>What is the runtime of computing $\\prod_{m \\in M} m$ with a deterministic Turing machine?</p>\n', 'ViewCount': '74', 'Title': 'Run time of product of polynomially bounded numbers', 'LastEditorUserId': '2376', 'LastActivityDate': '2012-08-07T01:54:25.730', 'LastEditDate': '2012-08-07T01:54:25.730', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3047', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2372', 'Tags': '<algorithms><time-complexity><integers>', 'CreationDate': '2012-08-05T02:23:25.833', 'Id': '3039'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There are a lot of algorithms out there that solve this particular problem. My main problem is in trying to understand how they work; where to start. Most of the algorithms are academic in nature.</p>\n\n<p>There is a master list of product names. We feed in an input file into the program with product names, but the names of the products may be partial or incomplete. We would need to match the correct product from the master list.</p>\n\n<p>There are solutions like clustering algorithms, naive-bayes, etc. But I am looking at some working examples to actually understand the concept. Would want to review the computations step-by-step for a small model.</p>\n', 'ViewCount': '319', 'Title': 'name matching algorithms from partial input', 'LastActivityDate': '2012-08-05T07:50:29.757', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2374', 'Tags': '<algorithms>', 'CreationDate': '2012-08-05T06:02:08.100', 'FavoriteCount': '1', 'Id': '3040'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm not even sure if this is the right StackExchange to post this, but it seems like sentiment analysis would go here.</p>\n\n<p>What would be the best approach to determine if two people on Twitter are actually friends?</p>\n\n<p>I think that the first criteria would be that they are following each other. This would eliminate all of the celebrities, companies, etc. that a person follows.</p>\n\n<p>The second criteria could be physical proximity to one another. If two people tweet from the same general areas, then there's probably a higher chance that they actually know each other.</p>\n\n<p>After that, I'm not sure what criteria I should look for. Would sentiment analysis of their tweets to each other be a viable option? With that friendly sentiment could indicate that they actually know each other.</p>\n", 'ViewCount': '133', 'Title': 'Analyzing Twitter Relationships', 'LastEditorUserId': '41', 'LastActivityDate': '2012-10-25T01:49:05.057', 'LastEditDate': '2012-08-06T03:48:54.740', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2214', 'Tags': '<algorithms><machine-learning><computer-networks><social-networks>', 'CreationDate': '2012-08-05T17:56:11.877', 'Id': '3050'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Call a family of sets $\\mathcal{F} = \\{S_1, \\dotsc, S_k\\}$ "diverse" if each set $S_i \\in \\mathcal{F}$ has at least one unique element.  What are possible approaches for finding the largest diverse set $S$ in a family of sets $\\mathcal{F}$?</p>\n\n<p>One approach is to solve a modified set packing problem.  Suppose $\\mathcal{F}=\\{S_1,\\dotsc,S_k\\}$. Let $K$ be a subset of elements, $K \\subset \\bigcup S_i$, and let $\\mathcal{F}_{-K}=\\{S_1 \\setminus K,\\dotsc, S_k \\setminus K\\}$.  Then the maximal diverse set $S$ corresponds to the largest maximal set packing obtained from $\\mathcal{F}_{-L}$ where $L$ is the set of all non-unique elements in $\\mathcal{F}$.</p>\n\n<p>But, what\'s a good heuristic for choosing $K$?  Or are there better approaches altogether?</p>\n', 'ViewCount': '158', 'Title': 'Problem similar to set packing', 'LastEditorUserId': '19', 'LastActivityDate': '2012-09-04T04:54:39.640', 'LastEditDate': '2012-08-10T18:01:45.850', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1937', 'Tags': '<algorithms><combinatorics><heuristics>', 'CreationDate': '2012-08-05T21:50:03.113', 'FavoriteCount': '2', 'Id': '3052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '325', 'Title': 'Compression of domain names', 'LastEditDate': '2012-08-08T06:51:13.347', 'AnswerCount': '1', 'Score': '11', 'OwnerDisplayName': 'eggyal', 'PostTypeId': '1', 'OwnerUserId': '2380', 'FavoriteCount': '1', 'Body': '<p>I am curious as to how one might <em>very compactly</em> compress the domain of an arbitrary <a href="http://en.wikipedia.org/wiki/Internationalized_domain_name">IDN</a> hostname (as defined by <a href="http://tools.ietf.org/html/rfc5890">RFC5890</a>) and suspect this could become an interesting challenge. A Unicode host or domain name (U-label) consists of a string of Unicode characters, typically constrained to one language depending on the top-level domain (e.g. Greek letters under <code>.gr</code>), which is encoded into an ASCII string beginning with <code>xn--</code> (the corresponding A-label).</p>\n\n<p>One can build data models not only from the formal requirements that</p>\n\n<ul>\n<li><p>each non-Unicode label be a string matching <code>^[a-z\\d]([a-z\\d\\-]{0,61}[a-z\\d])?$</code>;</p></li>\n<li><p>each A-label be a string matching <code>^xn--[a-z\\d]([a-z\\d\\-]{0,57}[a-z\\d])?$</code>; and</p></li>\n<li><p>the total length of the entire domain (A-labels and non-IDN labels concatenated with \'.\' delimiters) not exceed 255 characters</p></li>\n</ul>\n\n<p>but also from various heuristics, including:</p>\n\n<ul>\n<li><p>lower-order U-labels are often lexically, syntactically and semantically valid phrases in some natural language including proper nouns and numerals (unpunctuated except hyphen, stripped of whitespace and folded per <a href="http://tools.ietf.org/html/rfc3491">Nameprep</a>), with a preference for shorter phrases; and</p></li>\n<li><p>higher-order labels are drawn from a dictionary of SLDs and TLDs and provide context for predicting which natural language is used in the lower-order labels.</p></li>\n</ul>\n\n<p>I fear that achieving good compression of such short strings will be difficult without considering these specific features of the data and, furthermore, that existing libraries will produce unnecessary overhead in order to accomodate their more general use cases.</p>\n\n<p>Reading Matt Mahoney\'s online book <a href="http://mattmahoney.net/dc/dce.html">Data Compression Explained</a>, it is clear that a number of existing techniques could be employed to take advantage of the above (and/or other) modelling assumptions which ought to result in far superior compression versus less specific tools.</p>\n\n<p>By way of context, this question is an offshoot from a <a href="http://stackoverflow.com/questions/7792624/producing-compact-ciphertext-of-short-strings">previous one on SO</a>.</p>\n\n<hr>\n\n<p><strong>Initial thoughts</strong></p>\n\n<p>It strikes me that this problem is an excellent candidate for offline training and I envisage a compressed data format along the following lines:</p>\n\n<ul>\n<li><p>A Huffman coding of the "<a href="http://publicsuffix.org/">public suffix</a>", with probabilities drawn from some published source of domain registration or traffic volumes;</p></li>\n<li><p>A Huffman coding of which (natural language) model is used for the remaining U-labels, with probabilities drawn from some published source of domain registration or traffic volumes given context of the domain suffix;</p></li>\n<li><p>Apply some dictionary-based transforms from the specified natural language model; and</p></li>\n<li><p>An arithmetic coding of each character in the U-labels, with probabilities drawn from contextually adaptive natural language models derived from offline training (and perhaps online too, although I suspect the data may well be too short to provide any meaningful insight?).</p></li>\n</ul>\n', 'Tags': '<algorithms><strings><natural-lang-processing><data-compression>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-31T13:51:19.050', 'CommentCount': '5', 'CreationDate': '2011-10-18T02:19:42.587', 'Id': '3056'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Of large sparse biparite graphs (say degree 4) with N verticies, roughly speaking, which of them cause the worst case running time of the <a href="https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm" rel="nofollow">Hopcroft-Karp algorithm</a>? What is their general structure and architecture, and why does it cause a problem?</p>\n\n<p>Further, in many implementations the DFS part is implemented using recursion, eg from Wikipedia:</p>\n\n<pre><code>function DFS (v)\n    if v != NIL\n        for each u in Adj[v]\n            if Dist[ Pair_G2[u] ] == Dist[v] + 1\n                if DFS(Pair_G2[u]) == true\n                    Pair_G2[u] = v\n                    Pair_G1[v] = u\n                    return true\n        Dist[v] = \u221e\n        return false\n    return true\n</code></pre>\n\n<p>What is the approximate maximum depth of the recursion in the worst case?</p>\n', 'ViewCount': '227', 'Title': 'Worst-case sparse graphs for Hopcroft-Karp Algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-08T06:54:35.197', 'LastEditDate': '2012-08-08T06:54:35.197', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1577', 'Tags': '<algorithms><graph-theory><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-07T05:31:16.787', 'Id': '3064'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3695', 'Title': 'Algorithm that finds the number of simple paths from $s$ to $t$ in $G$', 'LastEditDate': '2012-08-07T19:18:18.717', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '778', 'FavoriteCount': '8', 'Body': '<p>Can anyone suggest me a linear time algorithm that takes as input a directed acyclic graph $G=(V,E)$ and two vertices $s$ and $t$ and returns the number of simple paths from $s$ to $t$ in $G$.<br>\nI have an algorithm in which I will run a DFS(Depth First Search) but if DFS finds $t$ then it will not change the color(from white to grey) of any of the nodes which comes in the path $s \\rightsquigarrow t$ so that if this is the subpath of any other path then also DFS goes through this subpath again.For example consider the adjacency list where we need to find the number of paths from $p$ to $v$.<br>\n$$\\begin{array}{|c|c c c|}\n\\hline \np &amp;o &amp;s &amp;z \\\\ \\hline\no &amp;r &amp;s &amp;v\\\\ \\hline\ns &amp;r \\\\ \\hline\nr &amp;y \\\\ \\hline\ny &amp;v \\\\ \\hline\nv &amp;w \\\\ \\hline\nz &amp; \\\\ \\hline\nw &amp;z \\\\ \\hline\n\\end{array}$$\nHere DFS will start with $p$ and then lets say it goes to $p \\rightsquigarrow z$ since it doesnot encounter $v$ DFS will run normally.Now second path is $psryv$ since it encounter $v$ we will not change the color of vertices $s,r,y,v$ to grey.Then the path $pov$ since color of $v$ is still white.Then the path $posryv$ since color of $s$ is white and similarly of path $poryv$.Also a counter is maintained which get incremented when $v$ is encountered.</p>\n\n<p>Is my algorithm correct? if not, what modifications are needed to make it correct or any other approaches will be greatly appreciated.</p>\n\n<p><strong>Note</strong>:Here I have considered the DFS algorithm which is given in the book <em>"Introduction to algorithms by Cormen"</em> in which it colors the nodes according to its status.So if the node is unvisited , unexplored and explored then the color will be white,grey and black respectively.All other things are standard.</p>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '778', 'LastActivityDate': '2012-08-09T22:57:36.403', 'CommentCount': '2', 'AcceptedAnswerId': '3087', 'CreationDate': '2012-08-07T19:11:55.220', 'Id': '3078'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m using the book Introduction to Computer Science by John Zelle and at the end of Chapter 3 (Computing with numbers), I\'m asked to find the nth term of a Fibonacci sequence presumably using a definitive for loop, as no other decision structure has been introduced yet. </p>\n\n<p>Is this possible? I\'ve tried everything I could think of.</p>\n\n<p>**I know how to solve it using if statements and such. But the book hasn\'t yet covered decision structures, yet it asks me to find the nth term(given by the user). So I can only presume to know how to do this using "for" loops as this is all that has been covered so far</p>\n', 'ViewCount': '3602', 'Title': 'Is it possible to find the nth term  of a Fibonacci sequence using a definitive for loop?', 'LastEditorUserId': '2398', 'LastActivityDate': '2013-03-06T21:25:52.477', 'LastEditDate': '2012-08-07T20:56:52.317', 'AnswerCount': '5', 'CommentCount': '6', 'Score': '5', 'OwnerDisplayName': 'qzxt', 'PostTypeId': '1', 'OwnerUserId': '2398', 'Tags': '<algorithms><imperative-programming>', 'CreationDate': '2012-08-07T20:14:42.113', 'Id': '3079'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '202', 'Title': 'How to compute linear recurrence using matrix with fraction coefficients?', 'LastEditDate': '2012-08-09T21:16:25.387', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '991', 'FavoriteCount': '1', 'Body': '<p>What I\'m trying to do is generate <a href="http://en.wikipedia.org/wiki/Motzkin_number" rel="nofollow">Motzkin numbers</a> mod a large number $10^{14} + 7$ (not prime), and it needs to compute the $n$th Motzkin number as fast as possible. From Wikipedia, the formula for the $n$th Motzkin number is defined as following:</p>\n\n<p>$\\qquad \\displaystyle \\begin{align}\n  M_{n+1} &amp;= M_n + \\sum_{i=0}^{n-1} M_iM_{n-1-i} \\\\\n          &amp;= \\frac{2n+3}{n+3}M_n + \\frac{3n}{n+3}M_{n-1}\n\\end{align}$ </p>\n\n<p>My initial approach is to use the second formula which is obviously faster, but the problem I ran into is the division since modular arithmetic rule doesn\'t apply.</p>\n\n<pre><code>void generate_motzkin_numbers() {\n    motzkin[0] = 1;\n    motzkin[1] = 1;\n    ull m0 = 1;\n    ull m1 = 1;\n    ull numerator;\n    ull denominator;\n    for (int i = 2; i &lt;= MAX_NUMBERS; ++i) {\n        numerator = (((2*i + 1)*m1 + 3*(i - 1)*m0)) % MODULO;\n        denominator = (i + 2);\n        motzkin[i] = numerator/denominator;\n        m0 = m1;\n        m1 = motzkin[i];\n    }\n}\n</code></pre>\n\n<p>Then I tried the second formula, but the running time is horribly slow because the summation:</p>\n\n<pre><code>void generate_motzkin_numbers_nested_recurrence() {\n    mm[0] = 1;\n    mm[1] = 1;\n    mm[2] = 2;\n    mm[3] = 4;\n    mm[4] = 9;\n    ull result;\n    for (int i = 5; i &lt;= MAX_NUMBERS; ++i) {\n        result = mm[i - 1];\n        for (int k = 0; k &lt;= (i - 2); ++k) {\n            result = (result + ((mm[k] * mm[i - 2 - k]) % MODULO)) % MODULO;\n        }\n        mm[i] = result;\n    }\n}\n</code></pre>\n\n<p>Next, I\'m thinking of using matrix form which eventually can be speed up using exponentiation squaring technique, in other words $M_{n+1}$ can be computed as follows:\n$$M_{n+1} = \\begin{bmatrix} \\dfrac{2n+3}{n+3} &amp; \\dfrac{3n}{n+3} \\\\ 1 &amp; 0\\end{bmatrix}^n \\cdot \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}$$ \nWith exponentiation by squaring, this method running time is $O(\\log(n))$ which I guess the fastest way possible, where <code>MAX_NUMBERS = 10,000</code>. Unfortunately, again the division with modular is killing me. After apply the modulo to the numerator, the division is no longer accurate. So my question is, is there another technique to compute this recurrence modulo a number? I\'m think of a dynamic programming approach for the summation, but I still think it\'s not as fast as this method.  Any ideas or suggestions would be greatly appreciated. </p>\n', 'Tags': '<algorithms><recurrence-relation><efficiency><integers>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-10T13:32:37.340', 'CommentCount': '6', 'AcceptedAnswerId': '3116', 'CreationDate': '2012-08-09T19:06:02.450', 'Id': '3109'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I notice that in a few CS research papers, to compare the efficiency of two algorithms, the total number of key comparison in the algorithms is used rather than the real computing times themselves. Why can't we compare which one is better by running both programs and counting the total time needed to run the algorithms? </p>\n", 'ViewCount': '310', 'Title': 'Why use comparisons instead of runtime for comparing two algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-12T01:53:58.833', 'LastEditDate': '2012-08-12T00:14:15.607', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2460', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-11T20:41:56.243', 'FavoriteCount': '3', 'Id': '3126'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '691', 'Title': "How Does Populating Pastry's Routing Table Work?", 'LastEditDate': '2012-08-13T09:14:36.520', 'AnswerCount': '1', 'Score': '15', 'OwnerDisplayName': 'Paddy Foran', 'PostTypeId': '1', 'OwnerUserId': '2653', 'FavoriteCount': '2', 'Body': u'<p>I\'m trying to implement the Pastry Distributed Hash Table, but some things are escaping my understanding. I was hoping someone could clarify.</p>\n\n<p><strong>Disclaimer</strong>: I\'m not a computer science student. I\'ve taken precisely two computer science courses in my life, and neither dealt with anything remotely complex. I\'ve worked with software for years, so I feel I\'m up to the implementation task, if I could just wrap my head around the ideas. So I may just be missing something obvious.</p>\n\n<p>I\'ve read the paper that the authors published [1], and I\'ve made some good progress, but I keep getting hung up on this one particular point in how the routing table works:</p>\n\n<p>The paper claims that</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into $\\lceil \\log_{2^b} N\\rceil$ \n  rows with $2^b - 1$ entries each. The $2^b - 1$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>The $b$ stands for an application-specific variable, usually $4$. Let\'s use $b=4$, for simplicity\'s sake. So the above is</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into  $\\lceil \\log_{16} N\\rceil$ rows \n  with $15$ entries each. The $15$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>I understand that much. Further, $N$ is the number of servers in the cluster. I get that, too.</p>\n\n<p>My question is, if the row an entry is placed into depends on the shared length of the key, why the seemingly random limit on the number of rows? Each nodeId has 32 digits, when $b=4$ (128 bit nodeIds divided into digits of b bits). So what happens when $N$ gets high enough that $\\lceil\\log_{16} N\\rceil &gt; 32$? I realise it would take 340,282,366,920,938,463,463,374,607,431,768,211,457 (if my math is right) servers to hit this scenario, but it just seems like an odd inclusion, and the correlation is never explained.</p>\n\n<p>Furthermore, what happens if you have a small number of servers? If I have fewer than 16 servers, I only have one row in the table. Further, under no circumstances would every entry in the row have a corresponding server. Should entries be left empty? I realise that I\'d be able to find the server in the leaf set no matter what, given that few servers, but the same quandary is raised for the second row--what if I don\'t have a server that has a nodeId such that I can fill every possible permutation of the nth digit? Finally, if I have, say, four servers, and I have two nodes that share, say, 20 of their 32 digits, by some random fluke... should I populate 20 rows of the table for that node, even though that is far more rows than I could even come close to filling?</p>\n\n<p>Here\'s what I\'ve come up with, trying to reason my way through this:</p>\n\n<ol>\n<li>Entries are to be set to a null value if there is not a node that matches that prefix precisely.</li>\n<li>Empty rows are to be added until enough rows exist to match the shared length of the nodeIds.</li>\n<li>If, and only if, there is no matching entry for a desired message ID, fall back on a search of the routing table for a nodeId whose shared length is greater than or equal to the current nodeId\'s and whose entry is mathematically closer than the current nodeId\'s to the desired ID.</li>\n<li>If no suitable node can be found in #3, assume this is the destination and deliver the message.</li>\n</ol>\n\n<p>Do all four of these assumptions hold up? Is there somewhere else I should be looking for information on this?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1007/3-540-45518-3_18" rel="nofollow">Pastry: Scalable, decentralized object location and routing for large-scale peer-to-peer systems</a> by A. Rowstrong and P. Druschel (2001) -- <a href="http://research.microsoft.com/~antr/PAST/pastry.pdf" rel="nofollow">download here</a></li>\n</ol>\n', 'Tags': '<algorithms><data-structures><distributed-systems><hash-tables>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T17:34:24.577', 'CommentCount': '2', 'AcceptedAnswerId': '6069', 'CreationDate': '2012-05-23T22:02:33.000', 'Id': '3138'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For fun I am trying to make a wire-frame viewer for the <a href="http://0x10c.com/doc/dcpu-16.txt" rel="nofollow">DCPU-16</a>. I understand how do do everything except how to hide the lines that are hidden in the wire frame. All of the questions here on SO all assume you have access to OpenGL, unfortunately I do not have access to anything like that for the DCPU-16 (or any kind of hardware acceleration).</p>\n\n<p>I found a fairly good description of Appel\'s algorithm on <a href="http://books.google.com/books?id=aVQnUfL3yEwC&amp;lpg=PA251&amp;ots=zCOEvuKqve&amp;dq=Arthur%20Appel%27s%20algorithm.&amp;pg=PA252#v=onepage&amp;q&amp;f=true" rel="nofollow">Google Books</a>. However there is one issue I am having trouble figuring out.</p>\n\n<blockquote>\n  <p>Appel defined contour line as an edge shared by a front-facing and a\n  back-facing polygon, or unshared edge of a front facing polygon that\n  is not part of a closed polyhedron. An edge shared by two front-facing\n  polygons causes no change in visibility and therefore is not a contour\n  line. In Fig. 8.4, edges AB, EF, PC, GK and CH are contour lines,\n  whereas edges ED, DC and GI are not.</p>\n</blockquote>\n\n<p><img src="http://i.stack.imgur.com/Gajc7.png" alt="Fig. 8.4"></p>\n\n<p>I understand the rules of the algorithm and how it works once you have your contour lines, however I do not understand is what I need to do to determine if a edge is "<em>shared by a front-facing and a back-facing polygon, or unshared edge of a front facing polygon that is not part of a closed polyhedron</em>" from a coding point of view. I can look at a shape and I can know what lines are contour lines in my head but I don\'t have a clue on how to transfer that "understanding" in to a coded algorithm.</p>\n\n<hr>\n\n<h2>Update</h2>\n\n<p>I have made some progress in determining contour lines. I found <a href="http://www.eng.buffalo.edu/courses/mae573/handouts/lecture13.pdf" rel="nofollow">these</a> <a href="http://www.eng.buffalo.edu/courses/mae573/handouts/appel.pdf" rel="nofollow">two</a> lecture notes from a University of Buffalo class on computer graphics.</p>\n\n<p><img src="http://i.stack.imgur.com/xoe49.png" alt="enter image description here"></p>\n\n<blockquote>\n  <p>Consider the edges. These fall into three categories. </p>\n  \n  <ol>\n  <li>An edge joining two invisible faces is itself invisible. This will be deleted from the list and ignored. </li>\n  <li>An edge joining two potentially-visible faces is called a \'material edge\' and will require further processing. </li>\n  <li>An edge joining a potentially-visible face and an invisible face is a special case of a \'material edge\' and is also called a \'contour\n  edge\'.</li>\n  </ol>\n</blockquote>\n\n<p>Using the above two pieces of information I am able to get closer to being able to write this out as code, but I still have a long way to go.</p>\n', 'ViewCount': '299', 'Title': "How to find contour lines for Appel's Hidden Line Removal Algorithm", 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-03T16:18:12.797', 'LastEditDate': '2012-08-12T20:25:09.697', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '9', 'OwnerDisplayName': 'Scott Chamberlain', 'PostTypeId': '1', 'OwnerUserId': '2710', 'Tags': '<algorithms><computational-geometry><graphics>', 'CreationDate': '2012-04-22T21:16:34.033', 'Id': '3139'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm interested in a discrete max-convolution problem, which is to compute\n$$r(c) =  \\max_{x | x \\ge 0, \\sum_k x_k = c} \\left[ \\sum_{k=1} f_k(x_k) \\right] $$\nfor all values $c=0, \\ldots, C$, where $x=(x_1, \\ldots, x_k)$ is a vector of non-negative integers.</p>\n\n<p>If we assume that $f_k$ are all concave functions i.e., $f(i+1) - f(i) \\le f(i) - f(i-1)$, how efficiently can $r = (r(0), \\ldots, r(C))$ be computed?</p>\n\n<p>Follow-up: what if $J$ of the $f_k$ functions are not concave?</p>\n", 'ViewCount': '161', 'Title': 'Fast algorithm for max-convolution with concave functions?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-13T09:17:15.410', 'LastEditDate': '2012-08-13T09:17:15.410', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2475', 'Tags': '<algorithms><optimization><discrete-mathematics>', 'CreationDate': '2012-08-12T21:16:06.727', 'FavoriteCount': '2', 'Id': '3140'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an algorithm to perform batch processing in the increase-key operation? Let us say, a binary heap (min-heap) is used. In the normal increase-key function, if we perform increase key on one node, then we have to traverse paths from the node towards the children to re balance the heap. If we want to increase the keys of five nodes in the heap, we need to call the increase-key function five times. Is it possible to call only one increase-key function and perform increase-key on five nodes simultaneously?</p>\n', 'ViewCount': '217', 'Title': 'Batch processing in increase-key function using binary heap', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-15T11:00:10.347', 'LastEditDate': '2012-08-14T12:08:17.190', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2460', 'Tags': '<algorithms><data-structures><parallel-computing><concurrency><priority-queues>', 'CreationDate': '2012-08-14T00:39:37.677', 'FavoriteCount': '1', 'Id': '3163'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>There is an obvious similarity in workings between Prim\'s algorithm and Dijkstra\'s algorithm, however I see no reason for Prim\'s algorithm to keep track of  a node\'s parent.  In Dijkstra\'s algorithm, the parent of the node needs to be tracked in order to follow the chain back to the original node to determine the distance from the source, however since Prim\'s algorithm only requires knowledge of the distance from the minimal spanning tree, rather than a specific node, there would be no reason to track the parent.</p>\n\n<p>Please refer to the following <a href="http://www.personal.kent.edu/~rmuhamma/Algorithms/MyAlgorithms/GraphAlgor/primAlgor.htm" rel="nofollow">pseudocode</a> as reference:</p>\n\n<pre><code>PRIM(V, E, w, r )\n  Q \u2190 { }\n  for each u in V do\n    key[u] \u2190 \u221e\n    \u03c0[u] \u2190 NIL\n    INSERT(Q, u)\n  DECREASE-KEY(Q, r, 0)    \u25b7 key[r ] \u2190 0\n  while Q is not empty do \n    u \u2190 EXTRACT-MIN(Q)\n    for each v in Adj[u] do \n      if v in Q and w(u, v) &lt; key[v] then \n        \u03c0[v] \u2190 u\n        DECREASE-KEY(Q, v, w(u, v))\n</code></pre>\n', 'ViewCount': '202', 'Title': "Why does Prim's algorithm keep track of a node's parent?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-14T20:43:37.967', 'LastEditDate': '2012-08-14T20:43:37.967', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-08-06T04:01:28.780', 'Id': '3174'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A classic application of divide and conquer is to solve the following problem:</p>\n\n<p><em>Given an array $a[1\\dots n]$ of distinct, comparable elements, count the number of inversion pairs in the array: pairs $(i,j)$ such that $a[i] \\gt a[j]$ and $i \\lt j$.</em></p>\n\n<p>One approach to this is to do a Merge Sort, but also counting of the number of inversion pairs in the sub-problems. During the merge step, we count the number of inversion pairs that span across the (two) sub-problems and add to the counts of the sub-problems.</p>\n\n<p>While this is good, and gives an $O(n\\log n)$ time algorithm, this messes up array.</p>\n\n<p>If we have the additional constraint that the array is read-only, then we can make a copy and deal with the copy, or use an additional data-structure like an order statistics balanced binary tree to do the counting, both of which use $\\Theta(n)$ space.</p>\n\n<p>The current question is to try and better the space, while not affecting the run time. i.e.</p>\n\n<blockquote>\n  <p>Is there an $O(n\\log n)$ time algorithm to count the number of\n  inversion pairs, which works on a read-only array and uses sub-linear\n  (i.e. $o(n)$) space?</p>\n</blockquote>\n\n<p>Assume a uniform cost RAM model and that the elements take $O(1)$ space and comparison between them is $O(1)$.</p>\n\n<p>A reference will do, but an explanation will be better :-)</p>\n\n<p>I tried searching the web, but could not find any positive/negative answer for this. I suppose this is just a curiosity.</p>\n', 'ViewCount': '946', 'Title': 'Counting inversion pairs', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-04T17:41:34.980', 'LastEditDate': '2012-08-15T20:07:04.477', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '139', 'Tags': '<algorithms><reference-request><arrays>', 'CreationDate': '2012-08-15T17:52:59.523', 'FavoriteCount': '3', 'Id': '3200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Over at <a href="http://cs.stackexchange.com/questions/3200/counting-inversion-pairs">this question about inversion counting</a>, I <a href="http://cs.stackexchange.com/questions/3200/counting-inversion-pairs#comment8724_3200">found a paper</a> that proves a lower bound on space complexity for all (exact) <a href="https://en.wikipedia.org/wiki/Streaming_algorithm">streaming algorithms</a>. I have claimed that this bound extends to all linear time algorithms. This is a bit bold as in general, a linear time algorithm can jump around at will (random access) which a streaming algorithm can not; it has to investigate the elements in order. I may perform multiple passes, but only constantly many (for linear runtime).</p>\n\n<p>Therefore my question:</p>\n\n<blockquote>\n  <p>Can every linear-time algorithm be expressed as a streaming algorithm with constantly many passes?</p>\n</blockquote>\n\n<p>Random access seems to prevent a (simple) construction proving a positive answer, but I have not been able to come up with a counter example either.</p>\n\n<p>Depending on the machine model, random access may not even be an issue, runtime-wise. I would be interested in answers for these models:</p>\n\n<ul>\n<li>Turing machine, flat input</li>\n<li>RAM, input as array</li>\n<li>RAM, input as linked list</li>\n</ul>\n', 'ViewCount': '469', 'Title': 'Is every linear-time algorithm a streaming algorithm?', 'LastActivityDate': '2012-08-16T18:15:02.147', 'AnswerCount': '4', 'CommentCount': '7', 'AcceptedAnswerId': '3221', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><streaming-algorithm><simulation><lower-bounds>', 'CreationDate': '2012-08-16T08:26:31.460', 'FavoriteCount': '1', 'Id': '3214'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '780', 'Title': 'Uniform sampling from a simplex', 'LastEditDate': '2012-08-17T09:22:28.433', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2553', 'FavoriteCount': '1', 'Body': "<p>I am looking for an algorithm to generate an array of N random numbers, such that the sum of the N numbers is 1, and all numbers lie within 0 and 1. For example, N=3, the random point (x, y, z) should lie within the triangle:</p>\n\n<pre><code>x + y + z = 1\n0 &lt; x &lt; 1\n0 &lt; y &lt; 1\n0 &lt; z &lt; 1\n</code></pre>\n\n<p>Ideally I want each point within the area to have equal probability. If it's too hard, I can drop the requirement. Thanks.</p>\n", 'Tags': '<algorithms><randomness><random-number-generator><sampling>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-09-27T21:24:11.037', 'CommentCount': '2', 'AcceptedAnswerId': '3229', 'CreationDate': '2012-08-16T19:45:16.167', 'Id': '3227'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<h3>Abstract problem description</h3>\n\n<p>The way I see it, unparsing means to create a token stream from an AST, which when parsed again produces an equal AST, i.e. <code>parse(unparse(AST)) = AST</code> should hold.  </p>\n\n<p>This is the equal to finding a valid parse tree which would produce the same AST. </p>\n\n<p>The language is described by a context free S-attributed grammar using a eBNF variant. </p>\n\n<p>So the unparser has to find a valid \'path\' through the traversed nodes in which all grammar constraints hold. This bascially means to find a valid allocation of AST nodes to grammar production rules. This is a constraint satisfaction problem (CSP) in general and could be solved, like parsing, by backtracking in $O(e^n)$. </p>\n\n<p>Fortunately for parsing, this can be done in $O(n^3)$ using GLR (or better restricting the grammar). Because the AST structure is so close to the grammar production rule structure, I was really surprised seeing an implementation where the runtime is worse than parsing: XText uses ANTLR for parsing and backtracking for unparsing. </p>\n\n<h3>Questions</h3>\n\n<ol>\n<li>Is a context free S-attribute grammar everything a parser and unparser need to share or are there further constraints, e.g. on the parsing technique / parser implementation?</li>\n<li>I\'ve got the feeling this problem isn\'t $O(e^n)$ in general -- could some genius help me with this?</li>\n</ol>\n\n<p>I didn\'t receive an answer for this question on <a href="http://stackoverflow.com/questions/11918961/unparse-ast-oexpn">StackOverflow</a>. It was suggested to ask here, but I hate redundancy, so I hope you forgive me for asking you to <a href="http://stackoverflow.com/questions/11918961/unparse-ast-oexpn">answer here</a>. </p>\n', 'ViewCount': '148', 'Title': 'Can abstract syntax trees be unparsed in subexponential time?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-12-24T19:05:30.280', 'LastEditDate': '2012-12-23T05:26:29.457', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2465', 'Tags': '<algorithms><compilers><parsers>', 'CreationDate': '2012-08-16T21:52:15.340', 'FavoriteCount': '1', 'Id': '3233'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For some graphs, DFS and BFS search algorithms process nodes in the exact same order provided that they both start at the same node. Two examples are graphs that are paths and graphs that are star-shaped (trees of depth $1$ with an arbitrary number of children). Is there some way for categorizing graphs that satisfy this property?</p>\n', 'ViewCount': '478', 'Title': 'Graphs that cause DFS and BFS to process nodes in the exact same order', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-28T11:36:04.370', 'LastEditDate': '2012-09-24T11:25:55.257', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><graphs><graph-traversal>', 'CreationDate': '2012-08-20T04:22:07.603', 'FavoriteCount': '2', 'Id': '3263'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have two questions. Both are about finding any of the 3 largest among $n$ elements.</p>\n\n<ol>\n<li><p>How to show that $n-3$ comparisons <strong>suffice</strong> to find any of the $3$ largest among $n$ given numbers $(n \\geq 4)$?</p></li>\n<li><p>How to show that $n-3$ comparisons are <strong>necessary</strong> to find any of the $3$ largest among $n$ given numbers $(n \\geq 6)$?</p></li>\n</ol>\n\n<p><strong>My Idea</strong>: I have tried to solve 1. using a heap but could not derive any such bound.</p>\n\n<p>The place where I found this question gave one hint which said to analyze the number of connected components in a <em>comparison graph</em>. </p>\n\n<p>I shall appreciate if anyone can throw some light on a <em>comparison graph</em>.</p>\n', 'ViewCount': '214', 'Title': 'Find any of the 3 largest among $n$ elements', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-08-20T17:06:40.217', 'LastEditDate': '2012-08-20T17:01:30.757', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2556', 'Tags': '<algorithms>', 'CreationDate': '2012-08-20T07:13:19.997', 'Id': '3264'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Updated Algorithm:</strong> There was a major flaw in my original presentation of the algorithm which could have impacted the results. I apologize for the same. The correction has been posted underneath.\n<hr></p>\n\n<p>The original algorithm posted had a major flaw in its working. I tried my best but could not get the desired accuracy in presenting the algorithm in pseudo code and/or Set Theory notation. I am thus posting python code, which has been tested and produces the desired results.</p>\n\n<p>Note that my question, however, remains the same: What is the time complexity of the algorithm (assuming that powersets are already generated)?</p>\n\n<pre><code># mps is a set of powersets; below is a sample (test case)\nmps = [\n        [       [], [1], [2], [1,2]     ],\n        [       [], [3], [4], [3,4]     ],\n        [       [], [5], [6], [5,6]     ]\n      ]\n\n\n# Core algorithm\n# enumerate(mps) may not be required in languages like C which support indexed loops\nlen = mps.__len__()\nfor idx, ps in enumerate(mps):\n    if idx &gt; len - 2:\n            break;\n    mps[idx + 1] = merge(mps[idx], mps[idx+1])   # merge is defined below\n\n\n# Takes two powersets and merges them\ndef merge (psa, psb):\n    fs = []\n    for a in psa:\n            for b in psb:\n                    fs.append(list(set(a) | set(b)))\n    return fs\n</code></pre>\n\n<p><strong>Output</strong>: <code>mps[-1]   #Last item of the list</code></p>\n\n<p>Running the above example will result in listing out the powerset of $\\{1,2,3,4,5,6\\}$.</p>\n', 'ViewCount': '154', 'Title': 'What is the complexity of this subset merge algorithm?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-23T09:30:36.730', 'LastEditDate': '2012-08-23T09:30:36.730', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2597', 'Tags': '<algorithms><time-complexity><algorithm-analysis><check-my-algorithm>', 'CreationDate': '2012-08-21T15:00:26.797', 'Id': '3273'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '473', 'Title': 'Given a set of sets, find the smallest set(s) containing at least one element from each set', 'LastEditDate': '2012-08-21T17:01:25.033', 'AnswerCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2601', 'FavoriteCount': '1', 'Body': u'<p>Given a set $\\mathbf{S}$ of sets, I\u2019d like to find a set $M$ such that every set $S$ in $\\mathbf{S}$ contains at least one element of $M$. I\u2019d also like $M$ to contain as few elements as possible while still meeting this criterion, although there may exist more than one smallest $M$ with this property (the solution is not necessarily unique).</p>\n\n<p>As a concrete example, suppose that the set $\\mathbf{S}$ is the set of national flags, and for each flag $S$ in $\\mathbf{S}$, the elements are the colors used in that nation\u2019s flag. The United States would have $S = \\{red, white, blue\\}$ and Morocco would have $S = \\{red, green\\}$. Then $M$ would be a set of colors with the property that every national flag uses at least one of the colors in $M$. (<a href="https://secure.wikimedia.org/wikipedia/en/wiki/Olympic_rings#Symbol">The Olympic colors</a> blue, black, red, green, yellow, and white are an example of such an $M$, or at least were in 1920.)</p>\n\n<p>Is there a general name for this problem? Is there an accepted \u201cbest\u201d algorithm for finding the set $M$? (I\u2019m more interested in the solution itself than in optimizing the process for computational complexity.)</p>\n', 'Tags': '<algorithms>', 'LastEditorUserId': '2601', 'LastActivityDate': '2012-08-23T21:27:19.010', 'CommentCount': '2', 'AcceptedAnswerId': '3281', 'CreationDate': '2012-08-21T16:54:23.470', 'Id': '3276'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was solving this equation:\n$$\\text{key}=\\left(\\sum_{K=0}^n\\frac{1}{a^K}\\right)\\bmod{m}.$$</p>\n\n<h3>Given</h3>\n\n<p>$$ 1,000,000,000 &lt; a, n, m \\; &lt; 5,000,000,000, $$\n$$ a, m \\text{ are coprime}. $$</p>\n\n<p>I solved it by brute force, but it won't work in the given constrains so I need a faster algorithm or is there is something I can notice to make the formula easier to solve ?</p>\n", 'ViewCount': '49', 'Title': 'Solving $\\text{key}=(\\sum_{K=0}^n\\frac{1}{a^K})\\bmod m$ with High limits', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-08-22T10:30:53.557', 'LastEditDate': '2012-08-22T06:51:14.843', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2607', 'Tags': '<algorithms><discrete-mathematics><number-theory>', 'CreationDate': '2012-08-22T02:25:44.527', 'Id': '3283'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there any book or tutorial that teaches us how to efficiently apply the common algorithms (sorting, searching, etc.) on large data (i.e. data that cannot be fully loaded into main memory) and how to efficiently apply those algorithms considering the cost of block transfer from external memory ? For example, almost all algorithm textbooks say that B and B+-trees can be used to store data on disk. However, actually how this can be done, especially handling the pointers where the data is present on disk is not explained. Similarly, though many books teach searching techniques, they do not consider data present in secondary memory. </p>\n\n<p>I have checked Knuth's book. Although it discusses these ideas, I still did not understand how to actually apply them in a high-level language. Is there any reference that discusses these details?</p>\n", 'ViewCount': '261', 'Title': 'Applying algorithms on large data', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-18T21:47:06.810', 'LastEditDate': '2012-10-18T21:21:53.663', 'AnswerCount': '3', 'CommentCount': '10', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2596', 'Tags': '<algorithms><efficiency><memory-management><big-data>', 'CreationDate': '2012-08-22T11:10:59.273', 'Id': '3287'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm now doing exam revision, and from some past year exam papers, I noticed some questions that ask to write a recursive method with signature like</p>\n\n<pre><code>public void run(int n)\n</code></pre>\n\n<p>that must have a time complexity of like : $O(n^2), O(n^3), O(n^7), O(n^2!), O(2^n), O(9^n)$.</p>\n\n<p>Can anyone give some idea on how to solve this kind of recursion questions.</p>\n", 'ViewCount': '600', 'Title': 'How to write a recursive function that with certain time complexity', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-27T10:01:42.290', 'LastEditDate': '2012-08-23T09:12:36.930', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Tags': '<algorithms><time-complexity><recursion>', 'CreationDate': '2012-08-23T06:21:25.700', 'Id': '3297'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am coding a neural network implementation, but a I have problems in the design. I was wondering about how to compare the <em>output</em> with the <em>target</em>, my neural networks has three outputs</p>\n\n<p><code> groups = {'Iris-virginica':[0,0,1], 'Iris-setosa':[0,1,0], 'Iris-versicolor':[1,0,0]}</code></p>\n\n<p>I know I must translate each output to 0 and 1. </p>\n\n<p>I meant if my result is <code>Iris-virginica</code> and my output is more or less: <code>[0.999979082561091, 0.9999918549147135, 0.9998408912106317]</code>, the subtraction would yield the following result: </p>\n\n<p><code>[-0.999979082561091, -0.9999918549147135, 0.000159]</code></p>\n\n<p>Is that correct, or I need to follow a different approach. Is possible train my net with 0, 1 and 2 values. Do I need to know any more?</p>\n", 'ViewCount': '79', 'Title': 'How to compare the output of a neural network with his target?', 'LastActivityDate': '2012-08-23T19:30:48.420', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2615', 'Tags': '<algorithms><neural-networks>', 'CreationDate': '2012-08-23T17:29:31.833', 'Id': '3303'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Please consider the following triple-nested loop:</p>\n\n<pre><code>for (int i = 1; i &lt;= n; ++i)\n    for (int j = i; j &lt;= n; ++j)\n        for (int k = j; k &lt;= n; ++k)\n            // statement\n</code></pre>\n\n<p>The statement here is executed exactly $n(n+1)(n+2)\\over6$ times. Could someone please explain how this formula was obtained? Thank you.</p>\n', 'ViewCount': '3201', 'Title': 'Time complexity of a triple-nested loop', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-24T22:01:32.450', 'LastEditDate': '2012-08-24T14:24:26.183', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2620', 'Tags': '<algorithms><time-complexity><algorithm-analysis>', 'CreationDate': '2012-08-24T02:42:49.667', 'FavoriteCount': '5', 'Id': '3306'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The rules are that you can only build from an existing part, so in the example below, B is the only option for the first move = A.</p>\n\n<p>A mechanical assembly might be represented as follows:</p>\n\n<pre><code>  E\n  |\n  C\n  |\nA-B\n  |\n  D\n  |\n  F\n</code></pre>\n\n<p>Where the valid assembly paths when starting from A are:</p>\n\n<pre><code>A, B, C, E, D, F\nA, B, C, D, E, F\nA, B, C, D, F, E\nA, B, D, F, C, E\nA, B, D, C, F, E\nA, B, D, C, E, F\n</code></pre>\n\n<p>This is a fairly simple example, but providing an upper bound for an arbitrary assembly is difficult since it\'s related to the "connectivity" of the parts.</p>\n\n<p>n! would be an absolute upper bound I guess, but I\'m hopping to find something a little better.</p>\n\n<p>I\'ve also looked at representing the graph with the parts (A, B, C, etc) as the edges and doing Kirchhoff\'s theorem, but that doesn\'t work for sparsely connected graphs like the example above.</p>\n\n<p>Any information about the problem would help.  I\'m not sure if there\'s a formal description of this type of problem or not.</p>\n', 'ViewCount': '60', 'Title': 'Given a mechanical assembly as a graph, how to find an upper bound on number of assembly paths', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-18T07:33:36.180', 'LastEditDate': '2012-08-25T04:09:37.150', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '4602', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2569', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-08-24T16:26:19.600', 'Id': '3314'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is Huffman coding <strong>always</strong> optimal since it uses Shanon's ideas?\nWhat about text, image, video, ... compression?</p>\n\n<p>Is this subject still active in the field? What classical or modern references should I read?</p>\n", 'ViewCount': '552', 'Title': 'Is there any theoretically proven optimal compression algorithm?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-08-30T12:56:35.700', 'LastEditDate': '2012-08-30T12:56:35.700', 'AnswerCount': '4', 'CommentCount': '6', 'AcceptedAnswerId': '3317', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2609', 'Tags': '<algorithms><information-theory><data-compression>', 'CreationDate': '2012-08-24T17:34:44.707', 'Id': '3316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I found this problem while I was reading  an ACM  problem and it is about dynamic programming. The problem says that you have a square matrix $n\\times n$ filled with 1's or 0's, like this:</p>\n\n<h2>$$\\begin{bmatrix} 1 &amp;1 &amp;1 &amp;0\\\\ 1 &amp;1 &amp;1 &amp;1\\\\ 0 &amp;0 &amp;1 &amp;0\\\\ 1 &amp;1 &amp;1 &amp;1 \\end{bmatrix}$$</h2>\n\n<p>Now you have to find the biggest square matrix inside the original matrix which is filled with only 1's. \nIn my  example, take the matrix of $((1,1)$ to $(2,2)$\n$$\n\\begin{bmatrix}\n1 &amp; 1\\\\\n1 &amp;1\n\\end{bmatrix}\n$$</p>\n\n<p>But, we might have to deal with matrices of size 1000X1000 as well. So the algorithm should be efficient and use DP, although I don't know if there is other solution. My Teacher told me that it can be done by Dynamic Programming. But I didn't understand his method.</p>\n", 'ViewCount': '257', 'Title': 'Problem contest with matrix and DP', 'LastEditorUserId': '31', 'LastActivityDate': '2012-08-29T09:16:00.017', 'LastEditDate': '2012-08-29T09:16:00.017', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '3342', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2012-08-26T21:43:37.540', 'Id': '3340'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a connected and directed graph $G=(V,E)$ with positive weights on the edges. for every $t&gt;0$ we define $E(t)$ to be the group of edges with weight lower or equal than $t$. I need to find an efficient algorithm which computes the minimal $t$ such that $G(t)=(V,E(t))$ is connected. </p>\n\n<p>I can sort all the edges with $|E|\\log|E|$ complexity and try to take edges out of the graph from the heaviest to the easiet one, and to check with DFS if it is still connected, but its not effiecnt enough,</p>\n\n<p>Any suggestions? </p>\n', 'ViewCount': '81', 'Title': 'For a graph $G$ find the minimal $t$ such $G(t)=(V,E(t))$ is connected', 'LastEditorUserId': '2499', 'LastActivityDate': '2012-08-28T15:43:34.987', 'LastEditDate': '2012-08-28T12:33:38.540', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3349', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-08-28T10:14:23.247', 'Id': '3347'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a directed and strongly connected graph $G=(V,E)$, weight function $w: E \\to \\mathbb{R}$ and two distinct vertices $u,v \\in V$. We know that there aren't negative cycles.\nI need to find algorithm, efficient as possible,such that for every value of $k$, $2 \\leq k \\leq |V|-1$, it will find the lightest weight of a path between $u$ to $v$ that contains no more then $k$ edges (If there's one).</p>\n\n<p>I don't know what to do. I want to use Bellman Ford somehow, I just don't sure how.</p>\n\n<p>Thanks a lot. </p>\n", 'ViewCount': '100', 'Title': "Find the lightest weight of a path between $u$ to $v$ that contains no more then $k$ edges (If there's one)", 'LastActivityDate': '2012-08-28T23:11:30.527', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3358', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-08-28T17:00:35.717', 'Id': '3352'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For the branch-and-cut method, it is essential to know many facets of the polytopes generated by the problem. However, it is currently one of the hardest problems to actually calculate all facets of such polytopes as they rapidly grow in size.</p>\n\n<p>For an arbitrary optimization problem, the polytope used by branch-and-cut or also by cutting-plane-methods is the convex hull of all feasible vertices. A vertex is an assignment of all variables of the model. As a (very simple) example: if one would maximize $2\\cdot x+y$ s.t. $x+y \\leq 1$ and $0\\leq x,y\\leq 1.5$ then the vertices $(0,0)$, $(0,1)$ and $(1,0)$ are feasible vertices. $(1,1)$ violates the inequality $x+y\\leq 1.5$ and is therefore not feasible. The (combinatorical) optimization problem would be to choose among the feasible vertices. (In this case, obviously $(1,0)$ is the optimum). The convex hull of these vertices is the triangle with exactly these three vertices. The facets of this simple polytope are $x\\geq0$, $y\\geq 0$ and $x+y\\leq 1$. Note that the description through facets is more accurate than the model. In most hard problems - such as the TSP - the number of facets exceeds the number of model inequalities by several orders of magnitude.</p>\n\n<p>Considering the Travelling Salesman Problem, for which number of nodes is the polytope fully known and how much facets are there. if it is not complete, what are lower bounds on the number of facets?</p>\n\n<p>I'm particularly interested in the so-called hamiltonian path formulation of the TSP:</p>\n\n<p>$$min \\sum_{i=0}^{n-1}(\\sum_{j=0}^{i-1}c_{i,j}\\cdot x_{i,j}+\\sum_{j=i+1}^{n-1}c_{i,j}\\cdot x_{i,j})$$ s.t.</p>\n\n<p>$$\\forall i \\neq j:\\ \\ 0 \\leq x_{i,j}\\leq 1$$\n$$\\forall i \\neq j\\ \\ \\ x_{i,j}+x_{j,i}\\leq 1$$\n$$\\forall j \\ \\ \\sum_{i=0}^{j-1}x_{i,j}+\\sum_{i=j+1}^{n-1}x_{i,j}\\leq 1$$\n$$\\forall j \\ \\ \\sum_{i=0}^{j-1}x_{j,i}+\\sum_{i=j+1}^{n-1}x_{j,i}\\leq 1$$\n$$\\sum_{i=0}^{n-1}(\\sum_{j=0}^{i-1}x_{i,j}+\\sum_{j=i+1}^{n-1}x_{i,j})=n-1$$</p>\n\n<p>If you have any information about polytopes of other formulations of the TSP, feel free to share that too.</p>\n", 'ViewCount': '458', 'Title': 'Known facets of the Travelling Salesman Problem polytope', 'LastEditorUserId': '39', 'LastActivityDate': '2014-01-10T11:09:53.913', 'LastEditDate': '2013-06-06T15:04:53.917', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1227', 'Tags': '<algorithms><optimization><linear-programming><mathematical-programming><traveling-salesman>', 'CreationDate': '2012-08-29T21:07:54.547', 'FavoriteCount': '2', 'Id': '3367'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is a problem from CLRS 23-2 that I'm trying to solve. The problem assumes that given graph G is very sparse connected. It wants to improve further over Prim's algorithm $O(E + V \\lg V)$. The idea is to contract the graph, i.e. collapse two or more nodes into one node. So each reduction will reduce the graph by at least half nodes. The question is to come up with implementation such that time complexity of MST-REDUCE is $O(E)$. This uses set operations. MakeSet, Union and Find-Set. I've annotated my analysis in the picture along with algorithm. \nI'm thinking to implement the set as linked list here. So my make-set and find-set are $O(1)$. But Union sucks: $O(V)$. Since we are doing union for all the elements, we have total $O(V^2)$ time spent in union. Which gives amortized $O(V)$. Now the problem isn't clear whether it is expecting amortized time complexity or not. So I'm wondering if any better approach is possible. Note the algorithm is running for all nodes and all edges. Hence I think amortized makes sense.</p>\n\n<p>Here is my analysis (line, complexity)</p>\n\n<p>1-3 $V$ </p>\n\n<p>4-9 $\\frac{V}{2} \\cdot union = \\frac{V}{2} \\cdot \\frac{V}{2} = V^2 = V$ (amortized)</p>\n\n<p>10 $V \\cdot findset = V$</p>\n\n<p>12-21 $E \\cdot findset = E$</p>\n\n<p>Since $E &gt;= V - 1$, we have overall time complexity of $O(E)$. </p>\n\n<pre><code>0   MST-REDUCE(G, orig, c T)\n1   for each v in V[G]\n2       mark[v] &lt;- FALSE\n3       MAKE-SET(v)\n4   for each u in V[G]\n5       if mark[u] = FALSE\n6           choose v in Adj[u] such that c[u,v] is minimized.\n7           UNION(u,v)\n8           T &lt;- T union { orig(u,v) }\n9           mark[u] &lt;- mark[v] &lt;- TRUE\n10  V[G'] &lt;- { FIND-SET(v) : v in V[G] }\n11  E[G'] &lt;- { }\n12  for each (x,y) in E[G]\n13      u &lt;- FIND-SET(x)\n14      v &lt;- FIND-SET(y)\n15      if (u,v) doesn't belong E[G']\n16          E[G'] &lt;- E[G'] union {(u,v)}\n17          orig'[u,v] &lt;- orig[x,y]\n18          c'[u,v] &lt;- c[x,y]\n19      else if c[x,y] &lt; c'[u,v]\n20          orig'[u,v] &lt;- orig[x,y]\n21          c'[u,v] &lt;- c[x,y]\n22  construct adjacency list Adj for G'\n23  return G', orig', c', T\n</code></pre>\n", 'ViewCount': '671', 'Title': 'Show that the Minimum spanning tree Reduce Algorithm runs in O(E) on sparse graphs', 'LastEditorUserId': '2375', 'LastActivityDate': '2012-08-31T08:37:11.837', 'LastEditDate': '2012-08-31T08:37:11.837', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2375', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-30T22:19:05.153', 'Id': '3375'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If you have a quick-sort algorithm, and you always select the smallest (or largest) element as your pivot; am I right in assuming that if you provide an already sorted data set, you will always get worst-case performance regardless of whether your 'already sorted' list is in ascending or descending order? </p>\n\n<p>My thinking is that, if you always choose the smallest element for your pivot, then whether your 'already-sorted' input is sorted by ascending or descending doesn't matter because the subset chosen to be sorted relative to your pivot will always be the same size?</p>\n", 'ViewCount': '433', 'Title': 'Does Quicksort always have quadratic runtime if you choose a maximum element as pivot?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-27T12:58:30.307', 'LastEditDate': '2012-08-31T07:28:12.087', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '3379', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-08-31T04:24:50.107', 'Id': '3377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2398', 'Title': 'Finding the minimum cut of an undirected graph', 'LastEditDate': '2012-09-02T21:14:29.657', 'AnswerCount': '3', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1183', 'FavoriteCount': '0', 'Body': "<p>Here's a question from a past exam I'm trying to solve:</p>\n\n<p>For an undirected graph  $G$ with positive weights $w(e) \\geq 0$, I'm trying to find the minimum cut. I don't know other ways of doing that besides using the max-flow min-cut theorem. But the graph is undirected, so how should I direct it? I thought of directing edges on both ends, but then which vertex would be the source and which vertex would be the sink? Or is there another way to find the minimum cut?</p>\n", 'Tags': '<algorithms><graph-theory>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-10T02:56:02.367', 'CommentCount': '3', 'AcceptedAnswerId': '3408', 'CreationDate': '2012-09-02T08:47:15.917', 'Id': '3399'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Suppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure FORD-FULKERSON still correctly computes a maximum flow.  </p>\n</blockquote>\n\n<p>I was thinking that when we augment a path the residual capacity of reverse edge increases and can be used to decrease the flow in that edge (but overall increase the network flow) if needed. So if we disallow the edges into $s$ that means we are not allowing decrease in flow in edges $s\\to x$ ($x$ is the adjacent node to $s$). So in the case when we allow edges into $s$ we can have a cycle like </p>\n\n<p>$\\qquad \\displaystyle s \\to x_1 \\leadsto y \\leadsto x_2 \\to s \\to x_3 \\leadsto t$.  </p>\n\n<p>But if we disallow edges into $s$ again we can find the same path with out the cycle. All the above are intuitive ideas but I want a formal proof.  </p>\n\n<p>The question is from <em>Introduction to Algorithms</em> by Cormen et al.</p>\n', 'ViewCount': '356', 'Title': 'A variation in Ford-Fulkerson algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-02T13:17:51.197', 'LastEditDate': '2012-09-02T13:17:51.197', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2589', 'Tags': '<algorithms><graph-theory><network-flow><correctness-proof>', 'CreationDate': '2012-09-02T12:01:27.050', 'Id': '3400'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '129', 'Title': "Compute 'insertable' letters in a regular language", 'LastEditDate': '2012-09-03T19:54:55.990', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2714', 'FavoriteCount': '1', 'Body': '<p>Let $L$ a regular language and define the <em><a href="https://en.wikipedia.org/wiki/Subsequence" rel="nofollow">subsequence</a> closure</em> of $L$ as</p>\n\n<p>$\\qquad \\displaystyle S(L) = \\{ w \\mid \\exists w\' \\in L.\\ w \\text{ subsequence of } w\'\\}$.</p>\n\n<p>The problem I want to solve is to find for such subsequences $w \\in S(L)$ which letters can be inserted into them so that the result is also in $S(L)$. Formally:</p>\n\n<blockquote>\n  <p>Given $w_1\\dots w_n \\in S(L)$, output all pairs $(i,a) \\in \\{0,\\dots,n\\} \\times \\Sigma$ for which $w_1 \\dots w_{i} a w_{i+1} \\dots w_n \\in S(L)$.</p>\n</blockquote>\n\n<p>Consider, for instance, the language$\\{ab, abc, abcc\\}$. The string $b$ is in $S(L)$ and inserting $a$ at the beginning -- corresponding to $(0,a)$ -- yields $ab \\in S(L)$. On the other hand, the string $cb$ is not in $S(L)$; there is no way to convert it to a language string by insertion.</p>\n\n<p>Using this language, if the input string is $b$ the possible insertions I am looking for are $(0,a)$ and $(1,c)$ at the end. If the input string is $bc$ the possible insertions are $(0,a), (1,c)$ and $(2,c)$.</p>\n\n<p>The use of this algorithm is in a user interface: the user builds strings belonging to the language starting from an empty string and adding one character at a time in different positions. At each step the UI prompts the user with all the possible valid letters in all the possible insertion positions.</p>\n\n<p>I have a working naive algorithm that involves a lot of back-tracking, and it is way too slow even in relatively simple cases. I was wondering if there is something better, or -- failing that -- if there are any available studies of this problem.</p>\n', 'Tags': '<algorithms><regular-languages><finite-automata>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-04T20:40:39.843', 'CommentCount': '5', 'AcceptedAnswerId': '3424', 'CreationDate': '2012-09-02T19:55:58.830', 'Id': '3404'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '751', 'Title': 'Removing duplicates efficiently and with a low memory overhead', 'LastEditDate': '2012-10-19T08:35:43.330', 'AnswerCount': '1', 'Score': '7', 'OwnerDisplayName': 'doc', 'PostTypeId': '1', 'OwnerUserId': '2729', 'FavoriteCount': '3', 'Body': '<p>I want to filter efficiently a list of integers for duplicates in a way that only the resulting set needs to be stored.</p>\n\n<p>One way this can be seen:</p>\n\n<ul>\n<li>we have a range of integers $S = \\{1, \\dots{}, N\\}$ with $N$ big (say $2^{40}$)</li>\n<li>we have a function $f : S \\to S$ with, supposedly, many collisions (the images are uniformly distributed in $S$)</li>\n<li>we then need to store $f[S]$, that is $\\{f(x) | x \\in S\\}$</li>\n</ul>\n\n<p>I have a quite accurate (probabilistic) estimation of what $|f[S]|$ is, and can therefore allocate data structures in advance (say $|f[S]| \\approx 2^{30}$).</p>\n\n<p>I have had a few ideas, but I am not sure what would be the best approach:</p>\n\n<ul>\n<li>a bitset is out of the question because the input set does not fit into memory.</li>\n<li>a hash table, but (1) it requires some memory overhead, say 150% of $|f[S]|$ and (2) the table has to be explored when built which requires additional time because of the memory overhead.</li>\n<li>an "on the fly" sort, preferably with $O(N)$ complexity (non-comparison sort). Regarding that, I am not sure what is the major difference between <a href="http://en.wikipedia.org/wiki/Bucket_sort" rel="nofollow">bucket sort</a> and <a href="http://en.wikipedia.org/wiki/Flashsort" rel="nofollow">flashsort</a>.</li>\n<li>a simple array with a binary search tree, but this requires $O(N \\log |f[S]|)$ time.</li>\n<li>maybe using <a href="http://en.wikipedia.org/wiki/Bloom_filter" rel="nofollow">Bloom filters</a> or a similar data structure could be useful in a relaxation (with false positives) of the problem.</li>\n</ul>\n\n<p>Some questions on stackoverflow seem to tackle with this sort of things (<a href="http://stackoverflow.com/questions/12240997/sorting-array-in-on-run-time">http://stackoverflow.com/questions/12240997/sorting-array-in-on-run-time</a>, <a href="http://stackoverflow.com/questions/3951547/java-array-finding-duplicates">http://stackoverflow.com/questions/3951547/java-array-finding-duplicates</a>), but none seems to match my requirements.</p>\n', 'Tags': '<algorithms><data-structures><sorting>', 'LastEditorUserId': '2729', 'LastActivityDate': '2012-11-21T23:43:10.673', 'CommentCount': '17', 'CreationDate': '2012-09-03T10:11:36.747', 'Id': '3420'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am going to attempt to write a random number generator using exisiting randomize algorithms. Can you suggest which algorithm has the biggest sequence that never repeats? I don't care if they are fast or slow.</p>\n", 'ViewCount': '128', 'Title': 'Random algorithm with biggest sequence that never repeats', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-08T05:42:39.437', 'LastEditDate': '2012-09-08T05:42:39.437', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3429', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2731', 'Tags': '<algorithms><random>', 'CreationDate': '2012-09-04T13:19:14.110', 'Id': '3421'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder whether there is a simple algorithm to do this work:</p>\n\n<p>Say I have a collection of objects $C$, and a binary relation $R: C\\times C$ that is auto-reflective ($\\forall c\\in C: c R c$)   , and symmetric ($\\forall b,c\\in C: b R c \\implies c R b$). The relation is not necessarily transitive.   </p>\n\n<p>I want to find an integer function $$h: C\\rightarrow \\text{Integer}$$ so that\n$$\\forall b,c\\in C:   h(b)\\neq h(c) \\implies R(b,c) $$   </p>\n\n<p><strong>Example</strong>: \nLet $$C=\\{1,2,3\\}$$  $$R=\\{(1,1), (2,2), (3,3), (1,2), (1,3), (2,1), (2,3)\\}$$\n$$h_1 = \\{1\\rightarrow 5, 2\\rightarrow 6, 3\\rightarrow 7\\}$$ \n$$h_2 = \\{1\\rightarrow 5, 2\\rightarrow 7, 3\\rightarrow 7\\}$$\nthen $h_2$ is a correct answer. $h_1$ is not, because $h_1(2)\\neq h(3)$ yet $R(2,3)$ does not hold.</p>\n\n<p>More concretely, my question comes from the following real concern: I have a collection of sets $C$, between some of them being disjoint. I want to find a hash function $h$ that is able to distinguish the sets by their disjointness, so that for any  sets $b,c$ of $C$ belonging to different hash buckets, I can conclude that they are disjoint. This is motivating because the underlined $C$ may have a huge cardinal.  </p>\n', 'ViewCount': '101', 'Title': 'Algorithm that hashes a collection of sets following their disjointness relation', 'LastEditorUserId': '472', 'LastActivityDate': '2012-09-09T19:47:24.087', 'LastEditDate': '2012-09-09T19:47:24.087', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '694', 'Tags': '<algorithms><hash>', 'CreationDate': '2012-09-09T06:29:35.440', 'FavoriteCount': '1', 'Id': '3480'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a function to count upper bits of a 32 bit value. So if a number is 11100011111..., the result is 3 as there are 3 ones in the most significant place before a 0 is hit.</p>\n\n<p>I need to convert the function to use only bitwise operations (no if statements or while loops) and the total number of operations should be lesser than 50.</p>\n\n<p>Question: How can this be converted to bitwise operations only while keeping less than 50 ops?</p>\n\n<p>Here is the Code:</p>\n\n<pre><code>  int count = 0;\n  int i = 28;\n\n  while(i &gt;= 0) {\n    int temp = (x&gt;&gt;i) &amp; 0xF;\n    i-=4;\n    if(temp == 0xF) count+=4;\n    else {\n      int mask = 0x1;\n      int a = (temp&gt;&gt;3) &amp; mask;\n      int b = (temp&gt;&gt;2) &amp; mask;\n      int c = (temp&gt;&gt;1) &amp; mask;\n      int d = temp &amp; mask;\n\n      if (a != 1) break;\n        count+=1;\n      if (b != 1) break;\n        count+=1;\n      if (c != 1) break;\n        count+=1;\n      if (d != 1) break;\n        count+=1;\n    }\n</code></pre>\n', 'ViewCount': '441', 'Title': 'Converting function to bitwise only?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-12T12:11:10.530', 'LastEditDate': '2012-09-11T11:43:56.560', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2771', 'Tags': '<algorithms><integers><binary-arithmetic>', 'CreationDate': '2012-09-10T02:05:39.293', 'FavoriteCount': '2', 'Id': '3484'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a directed graph $G=(V,E)$ and a node $r\\in V$, I need to grow a tree $T$ rooted at $r$ that has a minimum weight and spans all reachable nodes in $G$.</p>\n\n<p>The weight function assigns a non-negative weight to each node, which depends on the node's ancestors in $T$.  Specifically, for some fixed sets of nodes $S_1, S_2, \\dots, S_k \\subseteq V$, the weight of node $v$ is the number of sets $S_i$ that contain $v$ and all its ancestors in $T$.</p>\n\n<p>Any suggestion how to approach this problem?</p>\n", 'ViewCount': '141', 'Title': 'Minimum vertex-weight directed spanning tree where the weight function depends on the tree', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-13T06:27:26.233', 'LastEditDate': '2012-09-13T06:27:26.233', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2774', 'Tags': '<algorithms><graph-theory><optimization><spanning-trees>', 'CreationDate': '2012-09-10T07:57:07.253', 'FavoriteCount': '1', 'Id': '3486'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '253', 'Title': 'Complexity inversely propotional to $n$', 'LastEditDate': '2012-09-11T01:36:36.780', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1272', 'FavoriteCount': '1', 'Body': '<p>Is it possible an algorithm complexity decreases by input size? Simply $O(1/n)$ possible?</p>\n', 'Tags': '<algorithms><time-complexity><asymptotics><landau-notation>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-11T01:36:36.780', 'CommentCount': '2', 'AcceptedAnswerId': '3497', 'CreationDate': '2012-09-10T15:09:25.010', 'Id': '3495'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My question is related to the <a href="http://en.wikipedia.org/wiki/Integer_relation_algorithm" rel="nofollow">Integer Relation Detection Problem</a> which can be formulated as:</p>\n\n<p>$\\qquad a_1x_1 + a_2x_2 + \\cdots + a_nx_n = 0$</p>\n\n<p>Where $\\forall i. a_i\\in\\mathbb{Z} \\land a_i&lt;c \\land x\\in \\mathbb{R}$, and $\\exists i. a_i\\neq 0$. $c$ and vector $\\mathbf{x}$ are given, and the problem is to find a valid vector $\\mathbf{a}$ that satisfies these constraints.</p>\n\n<p>There are a few algorithms to solve this problem, listed on the wikipedia page linked.</p>\n\n<p>My question: are there algorithms for a solution to the same problem with the modification that</p>\n\n<p>$\\qquad a_1x_1 + a_2x_2 + \\cdots + a_nx_n = 1$?</p>\n\n<p>Or equivalently (I believe),</p>\n\n<p>$\\qquad a_1x_1 + a_2x_2 + \\cdots + a_nx_n = b$?</p>\n\n<p>The constant $b\\in \\mathbb R$ is a given.</p>\n\n<p>On <a href="http://math.stackexchange.com/questions/191545/integer-relation-that-equals-one">math.se</a> I ask for a polynomial time algorithm or proof that none exist, with not much luck. Here I ask if a solution to this is equivalent to a solution the knapsack problem (can the knapsack problem be reduced to this), and this would thus be NP-hard.</p>\n', 'ViewCount': '145', 'Title': 'Reduction from knapsack problem to Integer relation that equals one', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-12T22:38:34.833', 'LastEditDate': '2012-09-12T22:38:34.833', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><complexity-theory><np-hard><number-theory><knapsack-problems>', 'CreationDate': '2012-09-11T02:29:27.720', 'FavoriteCount': '0', 'Id': '3503'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>General satisfiability (with a few exceptions such as Horn Clauses) is not believed to have an algorithmic solution. However, the following algorithm appears to be a solution for general satisfiability. What exactly is the flaw with the following algorithm?      </p>\n\n<ol>\n<li>Let $W$ be an empty set which will contain all variables that necessarily have to be true or false.</li>\n<li>Let $L$ be the set of clauses.</li>\n<li>Loop through $L$.</li>\n<li>Every time a non-conditional variable<sup>\u2020</sup> is found, remove it from $L$ and insert it into $W$.</li>\n<li>If this leaves an empty AND implication<sup>\u2021</sup>, remove all variables in that empty implication from $L$ and insert into $W$.</li>\n<li>If this leaves an empty OR implication<sup>\u2021</sup>, create new instances of the algorithm, where each instance deals with one variable in the implication (i.e. if the implication is: $x V \\implies y$, create one instance where $x$ is inserted into $W$, one where $y$ is inserted into $W$ and one where $x$ and $y$ are inserted into $W$).</li>\n<li>Set all variables in $W$ to the value they necessarily have to be.</li>\n<li>Reinsert the variables in $W$ in $L$ with their changed values and check if all clauses  are satisfied.</li>\n<li>If satisfiability is met, then return $L$, else return "Not Satisfiable".</li>\n</ol>\n\n<p><sup>\u2020</sup> A non conditional variable is defined as a variable that is necessary true or false, e.g. $\\implies x$ or $\\implies \\neg y$.</p>\n\n<p><sup>\u2021</sup> An empty implication is defined as an implication where one side is empty (e.g. $\\implies x \\wedge y$) or the other side is necessarily true (e.g. $\\mathrm{true} \\vee a \\implies b$.</p>\n\n<p>To get a more intuitive understanding of the algorithm consider the following set of clauses $L$:</p>\n\n<p>$$\\begin{align}\n  a \\wedge b &amp;\\implies c &amp; \\text{(i)} \\\\\n  &amp;\\implies f \\wedge g &amp; \\text{(ii)} \\\\\n  f &amp;\\implies \\neg a &amp; \\text{(iii)} \\\\\n  f \\vee a &amp;\\implies b &amp; \\text{(iv)} \\\\\n  &amp;\\implies c &amp; \\text{(v)} \\\\\n\\end{align}$$</p>\n\n<p>The algorithm will do the following:</p>\n\n<p>1) Since $c$, $f$, $g$ are non-conditional variables, the algorithm will insert them into $W$. $W = \\{c, f, g\\}$.</p>\n\n<p>2) Removing $c$, $f$ and $g$ will leave the empty clauses: $\\implies \\neg a, a \\wedge b, b$. These will be added to $W$. $W = \\{c, f, g, b, \\neg a\\}$.</p>\n\n<p>3) Reinserting the variables into $L$ will result in the first clauses being violated: $a \\wedge b \\implies c$. Since $a$ is false, $c$ is false, meaning clause (v) is violated. The algorithm will return "Not Satisfiable"</p>\n\n<p>I am aware that the algorithm appears confusing. Please feel free to ask for clarification.</p>\n\n<hr>\n\n<p>From comments I now realize that there is no known <em>efficient</em> general satisfiability algorithm. I\'m still interested in feedback about my algorithm. Does it work? How does it compare with common algorithms?</p>\n', 'ViewCount': '238', 'Title': 'A tentative satisfiability algorithm', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-19T00:28:48.400', 'LastEditDate': '2012-09-12T19:31:09.140', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '7', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'Tags': '<algorithms><logic><satisfiability><constraint-programming>', 'CreationDate': '2012-08-15T02:42:26.293', 'FavoriteCount': '1', 'Id': '3516'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to write an algorithm to find whether a directed circuit whose length is odd exists in a strongly connected digraph.</p>\n\n<p>Can anyone help me how to proceed with this problem???</p>\n', 'ViewCount': '476', 'Title': 'Finding odd directed circuit', 'LastEditorUserId': '72', 'LastActivityDate': '2012-09-12T22:41:45.780', 'LastEditDate': '2012-09-12T22:41:45.780', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2803', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-09-12T18:42:39.927', 'FavoriteCount': '1', 'Id': '3517'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In algorithms and complexity we focus on the asymptotic complexity of algorithms, i.e. the amount of resources an algorithm uses as the size of the input goes to infinity. </p>\n\n<p>In practice, what is needed is an algorithm that would work fast on a finite (although possibly very large) number of instances.</p>\n\n<p>An algorithm which works well in practice on the finite number of instances that we are interested in doesn't need to have good asymptotic complexity (good performance on a finite number of instances doesn't imply anything regarding the asymptotic complexity). Similarly, an algorithm with good asymptotic complexity may not work well in practice on the finite number of instances that we are interested in (e.g. because of large constants).</p>\n\n<p>Why do we use asymptotic complexity? How do these asymptotic analysis related to design of algorithms in practice?</p>\n", 'ViewCount': '799', 'Title': 'Explaining the relevance of asymptotic complexity of algorithms to practice of designing algorithms', 'LastEditorUserId': '41', 'LastActivityDate': '2013-01-16T10:19:05.873', 'LastEditDate': '2012-09-13T01:08:53.377', 'AnswerCount': '4', 'CommentCount': '1', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<algorithms><complexity-theory><education>', 'CreationDate': '2012-09-13T01:02:59.517', 'FavoriteCount': '8', 'Id': '3523'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have created the algorithm below...</p>\n\n<pre><code>        String A = v[0];\n        int val = 1;\n\n        for (int i = 1; i &lt; v.length; i++) {\n            if (val == 0) {\n                A = v[i];\n                val++;\n            } else if (v[i].equals(A))\n                val++;\n            else\n                val--;\n        }\n</code></pre>\n\n<p>The goal of the algorithm is to find the item that occurs in more than half the array.</p>\n\n<p>Let v = {"one", "two", "one", "three", "one", "two", "two", "one", "one"}</p>\n\n<p>The string "one" occurs 5 out of 9 times.  So, at the end of the loop, the String A will be equal to "one". </p>\n\n<p>I\'m lost as to how to derive a loop invariant from this.  Could someone provide me with some direction?</p>\n', 'ViewCount': '218', 'Title': 'Help Finding Loop Invariant From For Loop', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-15T09:39:40.800', 'LastEditDate': '2012-09-13T21:53:54.173', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2816', 'Tags': '<algorithms><loop-invariants><correctness-proof>', 'CreationDate': '2012-09-13T19:06:30.303', 'FavoriteCount': '2', 'Id': '3532'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I saw a RAM model diagram that displayed an input tape, output tape, the program (read-only), the instruction pointer, and the memory registers. However, when I look at questions of time complexity, it is relevant how much time the program needs to spend on one action. Say you want to read one integer symbol from the read tape, add it to an integer from a memory register, and then squish the result into the write tape cell, and then move the read head one to the right and the write head one to the left. How much time or how many moves did I just waste?</p>\n', 'ViewCount': '82', 'Title': 'What constitutes one operation/cycle/move in the RAM model?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-16T21:23:57.300', 'LastEditDate': '2012-09-15T13:06:08.960', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '4574', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2835', 'Tags': '<algorithms><time-complexity><algorithm-analysis><machine-models>', 'CreationDate': '2012-09-15T04:38:07.633', 'Id': '3557'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1040', 'Title': 'Why does heapsort run in $\\Theta(n \\log n)$ instead of $\\Theta(n^2 \\log n)$ time?', 'LastEditDate': '2012-09-16T22:11:13.370', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1758', 'FavoriteCount': '1', 'Body': '<p>I am reading section 6.4 on Heapsort algorithm in CLRS, page 160.</p>\n\n<pre><code>HEAPSORT(A)  \n1 BUILD-MAX-HEAP(A)  \n2 for i to A.length downto 2  \n3   exchange A[i] with A[i]\n4   A.heap-size = A.heap-size-1  \n5   MAX-HEAPIFY(A,1)\n</code></pre>\n\n<p>Why is the running time, according to the book is $\\Theta (n\\lg{n})$ rather than $\\Theta (n^2\\lg{n})$ ? <code>BUILD-MAX-HEAP(A)</code> takes $\\Theta(n)$, <code>MAX-HEAPIFY(A,1)</code> takes $\\Theta(\\lg{n})$ and repeated $n-1$ times (line 3).</p>\n', 'Tags': '<algorithms><algorithm-analysis><landau-notation><sorting>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-09-16T22:11:13.370', 'CommentCount': '0', 'AcceptedAnswerId': '4579', 'CreationDate': '2012-09-16T20:47:58.343', 'Id': '4578'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m confused to conclude the recursion tree method a guess for the next recurrence:\n$$T(n)=3T\\left (\\left\\lfloor \\frac{n}{2}\\right \\rfloor\\right) +n$$\nI write some costs for the levels of tree, you can see, but I\'m confusing in the final guess. I know for the master theorem that the answer for the guess is like that \n$$\\Theta(n^{\\log_2 3}) $$ but in the steps by tree and I don\'t belive, you can see my error (?). How can I finished or know the outcome for this method to calculate $T(n)$? \nthanks,\n<img src="http://i.stack.imgur.com/T7vlE.png" alt="enter image description here">\n<img src="http://i.stack.imgur.com/V00qK.png" alt="Recursion Tree by the my problem">\nAnd the final conclusion is (?)\n$$=2 n^{\\log_2 3}+\\Theta(n^{\\log_2 3})\\leq cn^{\\log_2 3} \\rightarrow \\ \\ T(n)\\in\\Theta(n^{\\log_2 3})$$ with $c=3$ ?</p>\n', 'ViewCount': '124', 'Title': 'Doubt with a problem of grown functions and recursion tree', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-22T15:42:55.307', 'LastEditDate': '2012-09-17T17:48:41.503', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2012-09-17T02:09:19.717', 'Id': '4583'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '66', 'Title': 'Computing the running time of a divide-by-4-and-conquer algorithm', 'LastEditDate': '2012-09-18T22:12:40.627', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '1', 'Body': "<p>I write this code in python:</p>\n\n<pre><code>def sub(ma):\n    n = len(ma); m = len(ma[0])\n    if n != m : return\n    n2 = int(ceil(n/2))\n    a = []; b = []; c = []; d = [] \n    for i in range(n2):\n        a.append(ma[i][0:n2])\n        b.append(ma[i][n2:n])\n        c.append(ma[n2+i][0:n2])\n        d.append(ma[n2+i][n2:n])\n    return [a,b,c,d] \n\ndef sum(ma):\n        if len(ma) == 1 : return ma[0][0]\n        div = sub(ma)       \n        return sum(div[0])+sum(div[1])+sum(div[2])+sum(div[3]) \n</code></pre>\n\n<p>Do you know what is a possibly recurrence equation $T(n)$ to the 'sum' method? \nI suppose that is like that\n$$T(n) = 4T(n/2) + f(n)$$\nwhat it is $f(n)$ ?\nThanks, </p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-18T22:12:40.627', 'CommentCount': '0', 'AcceptedAnswerId': '4586', 'CreationDate': '2012-09-17T06:29:26.353', 'Id': '4584'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was given a homework assignment with Big O. I'm stuck with nested for loops that are dependent on the previous loop. Here is a changed up version of my homework question, since I really do want to understand it:</p>\n\n<pre><code>sum = 0;\nfor (i = 0; i &lt; n; i++ \n    for (j = 0; j &lt; i; j++) \n        sum++;\n</code></pre>\n\n<p>The part that's throwing me off is the <code>j &lt; i</code> part. It seems like it would execute almost like a factorial, but with addition. Any hints would be really appreciated.</p>\n", 'ViewCount': '2880', 'Title': 'Big O: Nested For Loop With Dependence', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-18T20:24:32.557', 'LastEditDate': '2012-09-17T17:55:35.750', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1000229', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><landau-notation>', 'CreationDate': '2012-09-07T23:38:26.383', 'Id': '4590'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Stephen Cook's proof of the NP-completeness of SAT is constructive. Given a Turing machine $M$, one can create a logical formula that is satisfiable if and only if $M$'s computation halts in an accepting state. This suggests that we could take a logical formula and create a Turing machine $M'$ whose computation is described by that formula, thereby creating an artificial problem solved by $M'$. Is it possible to use existing NP-complete problems to create other NP-complete problems? Can this be automated?</p>\n", 'ViewCount': '151', 'Title': 'Creating artificial NP-Complete problems', 'LastActivityDate': '2012-12-15T13:54:32.517', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '7410', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><np-complete>', 'CreationDate': '2012-09-17T13:43:44.447', 'FavoriteCount': '1', 'Id': '4592'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So we have the following table of processes , where A, B and $\\Gamma$ are the resources.</p>\n\n<p>Here is a pic that i drew with the processes and the resources.</p>\n\n<p><img src="http://i.stack.imgur.com/MzpAz.jpg" alt="Table"></p>\n\n<p>So the exact question is this: Using banker\'s algorithm, calculate the minimum values of $x$ and $y$ in order the system is Deadlock free.</p>\n\n<p>I have done pretty much <em>huge</em> paper work and found that $x,y$ should be the numbers 2 and 3. But in order to find this I run the algorithm on paper several times ; for $[x,y] = [0,0],[0,1],[1,0],[1,1],[1,2]$ etc. until I found that for pair $[x=2, y=3]$ the system is deadlock free!</p>\n\n<p>So, I think that I am missing the point. All this took me like 1 hour or so. Is there a simple method with less paperwork? </p>\n\n<p>Thanks a lot in advance!</p>\n', 'ViewCount': '680', 'Title': "Banker's Algorithm and deadlocks", 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-27T17:17:21.627', 'LastEditDate': '2012-09-17T17:54:25.173', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2865', 'Tags': '<algorithms><reference-request><concurrency>', 'CreationDate': '2012-09-17T16:34:15.583', 'Id': '4593'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The source of this question comes from an undergraduate course I am taking, which covers an introduction to the analysis of algorithms. This is not for homework, but rather a question asked in CLRS.</p>\n\n<p>You have a slow machine running at $x$ MIPS, and a fast machine running at $y$ MIPS. You also have two algorithms of the same class, but different running time complexities: one "slow" algorithm runs at $T(n) = c_1n^2$ whereas a "fast" algorithm runs at $T(n) = c_2n \\log n$.</p>\n\n<p>You execute the slow algorithm on the fast machine, and the fast algorithm on the slow machine. What is the largest value of n such that the fast machine running the slow algorithm beats the slow machine running the fast algorithm?</p>\n\n<p><strong>My solution so far:</strong></p>\n\n<p>Find the set of all $n$ such that $$\\frac{c_2n\\log n}{x} &gt; \\frac{c_1n^2}{y}$$ where $n$ is a natural number.</p>\n\n<p>This is my work thus far:</p>\n\n<p>$$\\{n : \\frac{c_2 n \\log_2 n}{x} &gt; \\frac{c_1 n^2}{y}, n \\in \\mathbb{N}\\} = \\{n : n &lt; \\frac{c_2 y}{c_1 x} \\log_2 n, n \\in \\mathbb{N}\\}$$</p>\n\n<p>The only solution that comes to mind now is to plug-n-chug all values of $n$ until I find the first n where </p>\n\n<p>$$n &lt; \\frac{c_2y}{c_1x}\\log(n)$$ </p>\n\n<p>no longer holds.</p>\n', 'ViewCount': '288', 'Title': 'Given a fast and a slow computer, at what sizes does the fast computer running a slow algorithm beat the slow computer running a fast algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-19T10:41:35.640', 'LastEditDate': '2012-09-19T10:07:43.623', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2867', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><mathematical-analysis>', 'CreationDate': '2012-09-18T04:09:39.707', 'Id': '4600'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '66', 'Title': 'Anonymization of dataset preserving unique identities', 'LastEditDate': '2012-09-19T12:12:46.070', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2826', 'FavoriteCount': '1', 'Body': "<p>The $k$-anonymization paradigm (and its refinements) means to create datasets where every tuple is identical with $k-1$ others.</p>\n\n<p>However I'm in a situation where people are in the dataset many times. And I want to follow their progress through the health care system, so I need to know who is who. If I give each person a unique ID, which is necessary in this situation, a linking attack from within the table is possible!</p>\n\n<p>Does anyone know of any relevant theory or have attempted to deal with similar problems?</p>\n\n<p>I'm inclined to think it is impossible to give any good guarantee of anonymity in this situation.</p>\n\n<p>This will possibly be used for my MSc thesis topic.</p>\n", 'Tags': '<algorithms><security><databases>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-19T14:59:00.877', 'CommentCount': '6', 'AcceptedAnswerId': '4615', 'CreationDate': '2012-09-19T06:31:34.257', 'Id': '4611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was watching the <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-16-greedy-algorithms-minimum-spanning-trees/" rel="nofollow">video lecture from MIT on Prim\'s algorithm for minimum spanning trees</a>.\nWhy do we need to do the swap step for proving the theorem that if we choose a set of vertices  in minimum spanning tree of $G(V,E)$and let us call that $A$ such  $A\\subset B$,  the edge with the least weight connecting $A$ to $V-A$ will always be in the minimum spanning tree ?  The professor has done the swap step at point 59:07 seconds in the video.</p>\n', 'ViewCount': '179', 'Title': "Why do the swap step in Prim's algorithm for minimum spanning trees?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-20T06:52:26.807', 'LastEditDate': '2012-09-19T21:19:36.247', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '4624', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><graph-theory><algorithm-analysis><greedy-algorithms><spanning-trees>', 'CreationDate': '2012-09-19T13:11:11.090', 'Id': '4614'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to create a bloom filter of 208 million URLs. What would be a good choice of bit vector size and number of hash functions? I tried a bit vector of size 1 GB and 4 hash functions, but it resulted in too many false positives while reading.</p>\n\n<p>I have a huge web corpus containing web content of billions of URLs. I need to process the web content of URLs satisfying certain criteria: the URL should have appeared in web search results in the past 7 days at least 5 times. This is represented by a list of 208 million URLs. Joining the list directly with the web corpus is not feasible because of volume. So I am considering creation of a bloom filter out of the list and then using the bloom filter to prune out unnecessary URLs from the web corpus.</p>\n', 'ViewCount': '387', 'Title': 'Bloom Filter for 208 million URLs', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:17:46.310', 'LastEditDate': '2013-06-26T13:17:46.310', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2881', 'Tags': '<data-structures><probabilistic-algorithms><searching><big-data><bloom-filters>', 'CreationDate': '2012-09-19T15:58:11.313', 'Id': '4616'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>At my local squash club, there is a <a href="http://en.wikipedia.org/wiki/Ladder_tournament" rel="nofollow">ladder</a> which works as follows.</p>\n\n<ol>\n<li>At the beginning of the season we construct a table with the name of each\nmember of the club on a separate line.</li>\n<li>We then write the number\nof games won and the number of games played next to each name (in the form: player wins/games).</li>\n</ol>\n\n<p>Thus at the beginning of the season the table looks like this:</p>\n\n<pre><code>Carol 0/0\nBilly 0/0\nAlice 0/0\nDaffyd 0/0\n</code></pre>\n\n<p>Any two players may play a match, with one player winning. If the player nearest the bottom of the table wins, then the position of the players is switched. We then repeat step 2., updating the number of wins and games next to each player. For example, if Alice beats Billy, we have</p>\n\n<pre><code>Carol 0/0\nAlice 1/1\nBilly 0/1\nDaffyd 0/0\n</code></pre>\n\n<p>These matches go on throughout the season and eventually result in players being listed in approximate strength order.</p>\n\n<p>Unfortunately, the updating happens in a rather haphazard way, so mistakes are made. Below are some examples of invalid tables, that is, tables which could not be produced by correctly following the above steps for some starting order (we have forgotten the order we used at the beginning of the season) and sequence of matches and results:</p>\n\n<pre><code>Alice 0/1\nBilly 1/1\nCarol 0/1\nDaffyd 0/0\n\nAlice 2/3\nBilly 0/1\nCarol 0/0\nDaffyd 0/0\n\nAlice 1/1\nBilly 0/2\nCarol 2/2\nDaffyd 0/1\n</code></pre>\n\n<p>Given a table, how can we efficiently determine whether it is valid? We could start by noting the following:</p>\n\n<ol>\n<li><p>The order of the names doesn\'t matter, since we have forgotten the original starting order.</p></li>\n<li><p>The total number of wins should be half the sum of the number of games played. (This shows that the first example above is invalid.)</p></li>\n<li>Suppose the table is valid. Then there is a <a href="http://en.wikipedia.org/wiki/Multigraph" rel="nofollow">multigraph</a> - a graph admitting multiple edges but no loops - with each vertex corresponding to a player and each edge to a match played. Then the total number of games played by each player corresponds to the degree of the player\'s vertex in the multigraph. So if there\'s no multigraph with the appropriate vertex degrees, then the table must be invalid. For example, there is no multigraph with one vertex of degree one and one of degree three, so the second example is invalid. [We can efficiently check for the existence of such a multigraph.]</li>\n</ol>\n\n<p>So we have two checks we can apply to start off with, but this still allows invalid tables, such as the third example. To see that this table is invalid, we can work backwards, exhausting all possible ways the table could have arisen.</p>\n\n<p>I was wondering whether anyone can think of a polynomial time (in the number of players and the number of games) algorithm solving this decision problem?</p>\n', 'ViewCount': '398', 'Title': 'How to efficiently determine whether a given ladder is valid?', 'LastEditorUserId': '2883', 'LastActivityDate': '2013-08-19T09:10:40.150', 'LastEditDate': '2013-07-04T18:19:58.500', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '2883', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-09-19T17:14:22.300', 'FavoriteCount': '5', 'Id': '4617'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an algorithm for finding the shortest path in an undirected weighted graph?</p>\n', 'ViewCount': '1289', 'Title': 'Finding the Shortest path in undirected weighted graph', 'LastEditorUserId': '72', 'LastActivityDate': '2012-09-23T14:05:51.613', 'LastEditDate': '2012-09-23T14:05:51.613', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2890', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2012-09-20T13:36:31.857', 'Id': '4629'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a graph $G(V,E)$. Each edge $e$ has two weights $A_e$ and $B_e$. Find a spanning tree that minimizes the product $\\left(\\sum_{e \\in T}{A_e}\\right)\\left(\\sum_{e \\in T}{B_e}\\right)$. The algorithm should run in polynomial time with respect to $|V|, |E|$.</p>\n\n<p>I find it difficult to adapt any of the traditional algorithms on spanning trees(Kruskal, Prim, Edge-Deletion). How to solve it? Any hints?</p>\n', 'ViewCount': '250', 'Title': 'Minimal Spanning Tree With Double Weight Parameters', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-14T05:06:23.300', 'LastEditDate': '2012-09-21T13:05:45.600', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<algorithms><graph-theory><spanning-trees>', 'CreationDate': '2012-09-20T14:50:02.843', 'FavoriteCount': '6', 'Id': '4635'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a sequence of $n$ integers in a small range $[0, k)$ and all the integers have the same frequency $f$ (so the size of the sequence is $n = f * k$). What I'm trying to do now is to compress this sequence while providing random access (what is the $i$-th integer). I'm more interested in achieving high compression at the expense of higher random access times.</p>\n\n<p>I haven't tried with Huffman coding since it assigns codes based on frequencies (and all my frequencies are the same). Perhaps I'm missing some simple encoding for this particular case.</p>\n\n<p>Any help or pointers would be appreciated.</p>\n\n<p>Thanks in advance.  </p>\n", 'ViewCount': '89', 'Title': 'Compression of sequence with Direct Access', 'LastActivityDate': '2012-09-20T22:11:22.337', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4642', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2892', 'Tags': '<algorithms><data-compression>', 'CreationDate': '2012-09-20T16:41:15.050', 'Id': '4639'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '292', 'Title': 'complexity of decision problems vs computing functions', 'LastEditDate': '2012-09-22T17:24:26.267', 'AnswerCount': '2', 'Score': '-8', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '1', 'Body': u'<p>this is an area that admittedly Ive always found subtle about CS and occasionally trips me up, and clearly others. recently on tcs.se a user asked an apparently innocuous question about <a href="http://cstheory.stackexchange.com/questions/12682/is-the-n-queens-problem-np-hard">N-Queens being NP hard</a>, but it got downvoted there &amp; goes unanswered because the audience felt it was poorly phrased. </p>\n\n<p>the issue seems to be that some questions in CS have to be phrased as decision problems to measure their complexity, and it is not always obvious how to do that. also other questions are studied from the pov of computing a function with integer or string inputs and outputs, in which case eg their time or space complexity can be studied. for some NP complete problems eg SAT its proven that the decision problem (eg determining whether an answer exists) is roughly equivalent in time/space complexity to solving the function problem (finding a satisfying assignment), but of course thats not always the case.</p>\n\n<p>moreover much of CS theory is focused on NP complete problems [esp that presented to undergraduates in textbooks], which are nec. decision problems, which might lead to a mistaken impression that other types of problem formats are not as central to the theory. also NP complete is sometimes shown in a hierarchy that includes other complexity classes such as PSpace which can be used to measure <em>function</em> complexity in addition to <em>decision</em> complexity. </p>\n\n<p>think that the N-Queens example question &amp; related comments also shows that a bunch of related questions about the same problem in this case N-Queens can vary widely/dramatically in their complexity depending on slight variations of the phrasing. eg in this case:</p>\n\n<ul>\n<li>is there a solution to the N-Queens problem on a $n \\times n$ chessboard? this problem turns out to have O(1) time complexity\u2014 the answer is always Y.</li>\n<li>what is the complexity of finding a solution to the N-Queens problem given $n$ as the input referring to a square $n \\times n$ chessboard? as that question notes in comments by Peter Shor, an advanced and subtle thm from CS called "mahaney\'s" thm applies because it might refer to a "sparse input". also JeffE found a paper that says its in P.</li>\n<li>there is an NP complete or NP hard version of this problem if the squares are <em>irregularly specified</em> as the input. ie some kind of input that specifies the square allowed/unallowed locations (havent looked up the details).</li>\n</ul>\n\n<p><hr>\nwhile advanced theorists may find all this verging on trivial, feel its a legitimate area of focus which sometimes gets glossed over in theoretical treatments. my question</p>\n\n<blockquote>\n  <p>is there a reference somewhere that points/sorts out these kinds of subtleties/difficulties in formulating CS problems?</p>\n</blockquote>\n\n<p>it would be helpful if it also talks about how the issue relates to the complexity hierarchy. know that this is covered in some textbooks, but even then havent seen a nice concise discussion of that and wonder if anyone has a favorite ref for this type of issue. (suppose Garey and Johnson might have some discussion of this although dont have a copy close to check.)</p>\n\n<p>am not specifically focused on the N-Queens problem with this question, but an answer that sketches out the distinctions wrt N-Queens might be helpful. [eg an expanded explanation of Shor\'s comment how Mahaneys thm applies, the irregular board construction input format, etc]</p>\n\n<p>fyi here are two other example problems Ive noticed that can vary widely in complexity depending on various restrictions on the input</p>\n\n<p>[1] <a href="http://cs.stackexchange.com/questions/701/decidable-restrictions-of-the-post-correspondence-problem/4638">Post correspondence problem.</a> it can go from undecidable to NP complete or even in P depending on various restrictions on the solution.</p>\n\n<p>[2] Finding whether a regular expression is equivalent to all strings over the alphabet. with squaring this was shown to be in ExpSpace by Stockmeyer/Meyer, but restrictions on length lead to it to being in NP complete or P. see eg Chee Yap, <a href="http://cs.nyu.edu/yap/book/complexity/" rel="nofollow">Intro to Complexity classes</a>, ch5 on complete problems.</p>\n', 'ClosedDate': '2014-04-29T11:56:36.853', 'Tags': '<algorithms><complexity-theory><reference-request><time-complexity><np-complete>', 'LastEditorUserId': '699', 'LastActivityDate': '2012-10-01T00:23:22.727', 'CommentCount': '7', 'AcceptedAnswerId': '4674', 'CreationDate': '2012-09-22T16:57:55.790', 'Id': '4667'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have this simple 'assignment' problem:</p>\n\n<p>We have a set of agents $A = \\{a_1, a_2, \\dotso, a_n\\}$ and set of tasks $T= \\{t_1, 1_2, \\dotso, t_m\\}$. Note that $m$ is not necessarily equal to $n$. Unlike the general assignment formulation in Wikipedia, a task $t_c$ can only be assigned to an agent based on the task's preferred agents $ta_c \\subseteq A$. For example, if we have $ta_1= \\{a_1, a_3\\}$, that means that task $t_1$ can only be assigned to either agents $a_1$ or $a_3$. Now, each agent $t_d$ has a qouta $q_d$ where $q_d$ is positive integer. This means that $a_d$ must be assigned with $q_d$ number of tasks. </p>\n\n<p><strong>The Problem</strong></p>\n\n<p>Given above and a set of qoutas $\\{q_1, q_2, \\dotso, q_n\\}$, is there an assignment of tasks to agents such that all agents meet their respective qouta $q$. Note that it is not necessarily that all tasks be assigned to an agent. </p>\n\n<p><strong>Possible Solution</strong></p>\n\n<p>I have tried reformulating this problem in terms of a bipartite graph $G(A, T, E = \\cup ta_c)$ and expressed as a form of matching problem where given a matching $M$, a vertex agent $a_d\\in A$ is matched up to $q_d$ times or is incident to $q_d$ edges in $M$ but the vertices in $T$ is incident to only one edge in $M$. Not really like the usual matching problem which requires that the edges in $M$ are pairwise non-adjacent.</p>\n\n<p>However, it was suggested by someone (from cstheory, he knows who he is) that I could actually work this out as a maximum matching problem, by replicating an agent $a_d$ into $q_d$ vertices and 'conceptually' treat them as different vertices as input to the matching algorithm. The set of edges $E$ is also modified accordingly. Call the modified graph as $G'$</p>\n\n<p>It is possible to have more than 1 maximum matchings from graph $G'$. Now, if I understand this correctly, I still have to check each of the resulting maximum matchings and see that at least one of them satisfies the $qouta$ constraint of each $agent$ to actually provide the solution to the problem.</p>\n\n<p>Now, I want to prove that not finding one maximum matching $M$ $\\in$ set of all maximum matchings of the graph $G'$ that satisfies the $qouta$ constraint of the problem guarantees that there really exists no solution to the problem instance, otherwise a solution exist which is $M$.</p>\n\n<p><strong>I want to show that this algorithm always give correct result.</strong> </p>\n\n<p><strong>Question</strong></p>\n\n<p>Can you share to me some intuition on how I might go to show this?</p>\n", 'ViewCount': '294', 'Title': 'Simple Task-Assignment Problem', 'LastEditorUserId': '2922', 'LastActivityDate': '2012-09-23T00:13:44.433', 'LastEditDate': '2012-09-23T00:13:44.433', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4684', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2922', 'Tags': '<algorithms><graph-theory><proof-techniques><assignment-problem>', 'CreationDate': '2012-09-22T23:22:25.637', 'Id': '4683'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I see <strong>structural induction</strong> the usual way for proving an algorithm's <strong>termination</strong> property, but it's not that easy to prove by means of induction on a <strong>tree</strong> algorithm. Now I am struggling on proving that the pre-order tree traversal algorithm is terminable:</p>\n\n<pre><code>preorder(node)\n  if node == null then return\n  visit(node)\n  preorder(node.left) \n  preorder(node.right)\n</code></pre>\n\n<p>How should I prove?</p>\n", 'ViewCount': '220', 'Title': 'How to prove that the pre-order tree traversal algorithm terminates?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-26T20:14:24.327', 'LastEditDate': '2012-09-26T20:14:24.327', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4726', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2954', 'Tags': '<algorithms><data-structures><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2012-09-24T15:52:44.860', 'Id': '4719'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '326', 'Title': 'How to prove that BFS directed-graph traversal algorithm terminates?', 'LastEditDate': '2012-09-27T00:17:49.203', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2954', 'FavoriteCount': '0', 'Body': u'<p>How to prove that BFS directed-graph traversal algorithm terminates?\n(I copy the pseudocode from <a href="http://en.wikipedia.org/wiki/Breadth-first_search" rel="nofollow">here</a>) Input: A graph G and a root v of G.</p>\n\n<pre><code>  procedure BFS(G,v):\n      create a queue Q\n      enqueue v onto Q\n      mark v\n      while Q is not empty:\n          t \u2190 Q.dequeue()\n          if t is what we are looking for:\n              return t\n          for all edges e in G.incidentEdges(t) do\n             o \u2190 G.opposite(t,e)\n             if o is not marked:\n                  mark o\n                  enqueue o onto Q\n</code></pre>\n', 'Tags': '<algorithms><algorithm-analysis><correctness-proof>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-09-27T00:17:49.203', 'CommentCount': '1', 'AcceptedAnswerId': '4749', 'CreationDate': '2012-09-26T18:21:07.543', 'Id': '4746'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a directed graph and two distinct vertices S and T, is there a polynomial-time algorithm which finds every vertex which is on at least one simple path from S to T?</p>\n\n<p>It is not difficult to find all vertices that are both successors of S and predecessors of T but this is only a superset of the set above. For example, consider the following graph:\nS -> a; a -> b; b -> c; b-> T; c -> a</p>\n\n<p>While a, b and c are all successors of S and predecessors of T, there is no simple path from S to T going through c (because every path from S to T going through c contains twice a and b).</p>\n\n<p>A closely related problem is the following:\nGiven a directed graph and three distinct vertices S and T and I, is there a polynomial-time algorithm to decide if there exist a simple path from S to T going through I.</p>\n\n<p>A polynomial-time algorithm to this latter problem can be use to build a polynomial algorithm to the former since we can apply it succesively by replacing I by every node in the graph (or more efficiently to every node that is both a succesor of S and a predecessor of T).</p>\n', 'ViewCount': '428', 'Title': 'How to find the vertices on simple path between two given vertices in a directed graph', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-29T11:34:22.223', 'LastEditDate': '2012-09-28T14:36:39.057', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2989', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2012-09-27T21:53:33.433', 'FavoriteCount': '1', 'Id': '4771'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '155', 'Title': 'Reconstructing a data table from cross-tabulation frequencies', 'LastEditDate': '2013-04-05T21:42:23.620', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2847', 'FavoriteCount': '1', 'Body': '<p>Say there is a data table $D$ that we cannot see, with $M$ columns. We are given exact <a href="https://en.wikipedia.org/wiki/Cross_tabulation" rel="nofollow">cross-tabulation</a> frequencies for all ${M \\choose 2}$ pairs of columns, that is how often each combination of two values occurs.</p>\n\n<p>From the cross-tabulations, we can derive a set of possible rows $R$ of $D$ and maximum frequencies for each possible row.</p>\n\n<p>How can we reconstruct the original table $D$? If there is not enough information to do so, how can we construct a possible table $D\'$ that has the same cross-tab frequencies? In this case, is it possible to count the number of possible tables?</p>\n\n<p>(Edit: As Vor noted, define a table as an unordered collection of rows. A permutation of the rows of a table yields the same table.)</p>\n\n<hr>\n\n<p>For example, if $D$ has rows:</p>\n\n<pre><code>X  A  j\nY  A  k\nX  B  j\nX  B  j\n</code></pre>\n\n<p>We have three sets of cross-tab frequencies:</p>\n\n<pre><code>X  A  1\nX  B  2\nY  A  1\nY  B  0\n\nX  j  3\nX  k  0\nY  j  0\nY  k  1\n\nA  j  1\nA  k  1\nB  j  2\nB  k  2\n</code></pre>\n\n<p>I would like an algorithm which will take the cross-tab frequencies as input and output the original table or a possible original table.</p>\n', 'Tags': '<algorithms><complexity-theory><combinatorics><statistics>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-07T14:24:45.853', 'CommentCount': '5', 'AcceptedAnswerId': '11102', 'CreationDate': '2012-09-28T20:16:24.500', 'Id': '4784'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am dealing with the problem of computing $ s = \\lfloor sqrt(x)\\rfloor$ with $x \\in [0,30000^2]$. The common <code>sqrtf(x)</code> on C language is too slow for this case, however it always gives me the correct result. I've tried with the Newton's method but I get very small errors when the square root of a number is exact. This leads to an uncertain pattern of $s-1$ results along the interval. If I increase the number of iterations the method becomes too slow but more exact.</p>\n\n<p>Does anyone know of faster methods or directions on the latest research done in the area?</p>\n\n<p>note to clarify: input is idealy a real number (i.e floating point) but i also accept solutions with integer as input.</p>\n", 'ViewCount': '600', 'Title': 'Fastest square root method with exact integer result?', 'LastEditorUserId': '2588', 'LastActivityDate': '2012-10-02T15:37:03.877', 'LastEditDate': '2012-10-02T13:53:27.143', 'AnswerCount': '1', 'CommentCount': '11', 'AcceptedAnswerId': '4841', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2588', 'Tags': '<algorithms><integers><discrete-mathematics>', 'CreationDate': '2012-09-29T05:17:23.397', 'Id': '4789'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '266', 'Title': 'Time complexity of an enumeration of SUBSET SUM instances', 'LastEditDate': '2012-10-04T17:08:49.840', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '140', 'FavoriteCount': '1', 'Body': '<p>An instance of the <strong><a href="http://en.wikipedia.org/wiki/Subset_sum_problem" rel="nofollow">SUBSET SUM</a></strong> problem (given $y$ and $A = \\{x_1,...,x_n\\}$ is there a non-empty subset of $A$ whose sum is $y$) can be represented on a one-tape Turing Machine with a list of comma separated numbers in binary format.\nIf $\\Sigma = \\{0,1,\\#\\}$ a reasonable format could be:</p>\n\n<p>$( 1 \\; (0|1)^* \\; \\#)^* \\#$</p>\n\n<p>Where the first required argument is the value $y$ and $\\#\\#$ encodes the end of the input. For example:</p>\n\n<pre><code> 1  0  0  #  1  0  #  1  #  #\n^^^^^^^^     ^^^^     ^\n   y          x1     x2\nInstance: y=4, A={2,1}\n</code></pre>\n\n<p>I would like to enumerate the SUBSET SUM instances.</p>\n\n<blockquote>\n<b>Question</b>: What is the (best) time complexity that can be achieved by a Turing Machine $TM_{Enum}$ that on input $m$ (which can be represented on the tape with a string of size $\\log m + 1$) - outputs the $m$-th SUBSET SUM instance in the above format?\n</blockquote>\n\n<p><strong>EDIT</strong>:</p>\n\n<p>Yuval\'s answer is fine, this is only a longer explanation.</p>\n\n<p>Without loss of  generality we set that $y &gt; 0$ and $0 &lt; x_1 \\leq x_2 \\leq ...  \\leq x_n$, $n \\geq 0$</p>\n\n<p>And we can represent an instance of subset sum using this encoding:</p>\n\n<p>$y \\# x_1\\# d_2\\# ...\\# d_{n} \\#\\#$ where $d_i \\geq 1, x_i = x_{i-1} + d_i - 1 \\; , i \\geq 2$</p>\n\n<p>Using a binary representation for $y,x_1, d_2, d_3, ...$ we have the following representation:</p>\n\n<p>$1 \\; ((0|1)^* \\# 1)^* \\; \\#\\#$</p>\n\n<p>Equivalent to $1 \\; (0|1|\\#1)^* \\; \\#\\#$. There is always a leading 1 and a trailing ## so we can consider only the $(0|1|\\#1)^*$ part.</p>\n\n<p>So the decoder TM on input $m$ in binary format should:</p>\n\n<ul>\n<li>output the leading 1</li>\n<li>convert $m$ to base 3 mapping digit 2 to $\\#1$</li>\n<li>when outputing the i-th intermediate $\\#$ calculate $x_i = d_i + x_{i-1}-1$</li>\n<li>output the trailing $\\#\\#$</li>\n</ul>\n\n<p>No duplicate instances are generated.</p>\n', 'Tags': '<algorithms><formal-languages><turing-machines><enumeration>', 'LastEditorUserId': '140', 'LastActivityDate': '2012-10-04T17:08:49.840', 'CommentCount': '7', 'AcceptedAnswerId': '4870', 'CreationDate': '2012-09-29T14:53:22.363', 'Id': '4794'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am learning algorithms. So, I came along with something very interesting.</p>\n\n<p>The asymptotic bound of linear function $an+b$ is $O(n^2)$ for all $a&gt;0$.</p>\n\n<p>This is same as for $an^2 + bn + c$. But shouldn't it be different?</p>\n", 'ViewCount': '82', 'Title': 'Why bound of linear function is same as that of quadratic equation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-29T22:12:00.863', 'LastEditDate': '2012-09-29T20:29:01.790', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3010', 'Tags': '<algorithms><asymptotics>', 'CreationDate': '2012-09-29T18:06:56.773', 'Id': '4796'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1105', 'Title': 'Fast k mismatch string matching algorithm', 'LastEditDate': '2012-09-29T20:37:46.053', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '3011', 'FavoriteCount': '2', 'Body': '<p>I am looking for a fast k-mismatch string matching algorithm. Given a pattern string P of length m, and a text string T of length n, I need a fast (linear time) algorithm to find all positions where P matches a substring of T with at most k mismatches. This is different from the k-differences problem (edit distance). A mismatch implies the substring and the pattern have a different letter in at most k positions. I really only require k=1 (at most 1 mismatch), so a fast algorithm for the specific case of k=1 will also suffice. The alphabet size is 26 (case-insensitive english text), so space requirement should not grow too fast with the size of the alphabet (eg., the FAAST algorithm, I believe, takes space exponential in the size of the alphabet, and so is suitable only for protein and gene sequences).</p>\n\n<p>A dynamic programming based approach will tend to be O(mn) in the worst case, which will be too slow. I believe there are modifications of the Boyer-Moore algorithm for this, but I am not able to get my hands on such papers. I do not have subscription to access academic journals or publications, so any references will have to be in the public domain.</p>\n\n<p>I would greatly appreciate any pointers, or references to freely available documents, or the algorithm itself for this problem. </p>\n', 'Tags': '<algorithms><reference-request><strings><string-metrics><substrings>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-04T09:58:32.560', 'CommentCount': '3', 'AcceptedAnswerId': '4855', 'CreationDate': '2012-09-29T19:47:41.390', 'Id': '4797'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '516', 'Title': 'Sorting as a linear program', 'LastEditDate': '2012-09-30T16:46:29.820', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '71', 'FavoriteCount': '3', 'Body': '<p>A surprising number of problems have fairly natural reductions to linear programming problems. See <a href="http://www.cs.berkeley.edu/~vazirani/algorithms/chap7.pdf">Chapter 7</a> of [1] for examples of network flows, bipartite matching, zero-sum games, shortest paths, a form of linear regression, and even circuit evaluation! </p>\n\n<p>Since circuit evaluation reduces to linear programming, any problem in $P$ must have a linear programming formulation. Therefore, we have a "new" sorting algorithm, via reduction to a linear program. So, my question is,\nwhat is the linear program that will sort an array of $n$ real numbers? What is the running time of the reduce-to-lp-and-solve sorting algorithm?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://www.cs.berkeley.edu/~vazirani/algorithms/">Algorithms</a> by S. Dasgupta, C. Papadimitriou and U. Vazirani (2006)</li>\n</ol>\n', 'Tags': '<algorithms><sorting><linear-programming>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-02T03:55:41.983', 'CommentCount': '6', 'AcceptedAnswerId': '4823', 'CreationDate': '2012-09-30T01:19:11.290', 'Id': '4805'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '375', 'Title': 'Using Funk SVD with SGD?', 'LastEditDate': '2012-09-30T18:00:23.330', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '106', 'FavoriteCount': '2', 'Body': '<p>I work on a recommender system framework which is implemented with a variant on Funk SVD (See his explanation of his algorithm <a href="http://sifter.org/~simon/journal/20061211.html" rel="nofollow" title="here">here</a>). </p>\n\n<p>However the framework that we are trying to integrate doesn\'t support Funk SVD, only SGD (<a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="nofollow">Stochastic Gradient Descent</a>). </p>\n\n<p>Since shouldn\'t these be compatible? In other words, I should be able to create the U and V matrices with SGD and then treat them like they were made via the Funk SVD process?</p>\n\n<p>Are there any disadvantages of using this versus the algorithm detailed by Funk?</p>\n', 'Tags': '<algorithms><machine-learning><matrices>', 'LastEditorUserId': '106', 'LastActivityDate': '2012-09-30T18:00:23.330', 'CommentCount': '5', 'AcceptedAnswerId': '4809', 'CreationDate': '2012-09-30T02:09:47.340', 'Id': '4806'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a set of tasks (execution time, deadline=period):</p>\n\n<ul>\n<li>$T_1(20,100)$</li>\n<li>$T_2(30,250)$</li>\n<li>$T_3(100,400) $</li>\n</ul>\n\n<p>Now I want to restrict the deadlines as $D_i = f \\cdot P_i$, where $D_i$ is new deadline for $i$-th task, $P_i$ is the original period for $i$-th task and $f$ is the factor I want to figure out. What is the smallest value of $f$ that the tasks will continue to meet their deadlines?</p>\n', 'ViewCount': '70', 'Title': 'Scheduling: advance deadline for implicit-deadline rate monotonic algorithm', 'LastEditorUserId': '31', 'LastActivityDate': '2012-10-01T18:14:07.263', 'LastEditDate': '2012-09-30T09:42:04.520', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2998', 'Tags': '<algorithms><scheduling>', 'CreationDate': '2012-09-30T07:04:11.410', 'FavoriteCount': '1', 'Id': '4810'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p><a href="http://en.wikipedia.org/wiki/Partition_refinement">Partition refinement</a> is a technique in which you start with a finite set of objects and progressively split the set. Some problems, like DFA minimization, can be solved using partition refinement quite efficiently. I don\'t know of any other problems that are usually solved using partition refinement other than the ones listed on the Wikipedia page. Out of all these problems, the Wikipedia page mentions two for which algorithms based on partition refinement run in linear time. There\'s the lexicographically ordered topological sort [1] and an algorithm for <a href="http://en.wikipedia.org/wiki/Lexicographic_breadth-first_search">lexicographic breadth-first search</a> [2].</p>\n\n<blockquote>\n  <p>Are there any other examples or references to problems that can be solved using partition refinement very efficiently, meaning something better than loglinear in terms of time?</p>\n</blockquote>\n\n<hr>\n\n<p>[1] <a href="http://epubs.siam.org/doi/abs/10.1137/0205005">Sethi, Ravi, "Scheduling graphs on two processors", SIAM Journal on Computing 5 (1): 73\u201382, 1976.</a></p>\n\n<p>[2] <a href="http://epubs.siam.org/doi/abs/10.1137/0205021">Rose, D. J., Tarjan, R. E., Lueker, G. S., "Algorithmic aspects of vertex elimination on graphs", SIAM Journal on Computing 5 (2): 266\u2013283, 1976.</a></p>\n', 'ViewCount': '157', 'Title': 'Problems for which algorithms based on partition refinement run faster than in loglinear time', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-05T01:32:41.660', 'LastEditDate': '2012-10-04T22:21:19.390', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><reference-request><data-structures><partitions><sets>', 'CreationDate': '2012-10-02T17:25:18.093', 'FavoriteCount': '1', 'Id': '4843'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to create an algorithm in linear time where if given a directed acyclic graph I can add edges to make it strongly connected components.</p>\n\n<p>I believe I have an algorithm to identify sources and sinks in the input list of edges in form</p>\n\n<pre><code>1 2\n2 5\n3 1 \netc\n</code></pre>\n\n<p>And I know that the minimum number of edges to be added in order to create connected components is equal to MAX(sources,sinks).</p>\n\n<p>My question is, is there a way to come up with where I should add edges so that I can have the minimum number and still be linear complexity?</p>\n\n<p>Here is an example of what I'm after.</p>\n\n<p>Given this input edges:</p>\n\n<pre><code>1 3\n1 4\n2 3\n2 4\n5 7\n5 8\n6 8\n6 9\n</code></pre>\n\n<p>Output these edges to be added:</p>\n\n<pre><code>3 1\n4 5\n7 6\n8 1\n9 2\n</code></pre>\n", 'ViewCount': '379', 'Title': 'If I have sources and sinks of a DAG can I find the minimum number of edges to be added to make it Strongly Connected?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-03T15:45:33.350', 'LastEditDate': '2012-10-03T11:58:04.703', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'ZAX', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2012-10-03T01:00:34.353', 'Id': '4848'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have been reading <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow"><em>Introduction to Algorithms</em> by Cormen et al.</a> and I\'m reading <a href="http://books.google.co.uk/books?id=NLngYyWFl_YC&amp;pg=PA73" rel="nofollow">the statement of the Master theorem starting on page 73</a>. In case 3 there is also a regularity condition that needs to be satisfied to use the theorem:</p>\n\n<blockquote>\n  <p>... 3. If </p>\n  \n  <p>$\\qquad \\displaystyle f(n) = \\Omega(n^{\\log_b a + \\varepsilon})$</p>\n  \n  <p>for some constant $\\varepsilon &gt; 0$ and if</p>\n  \n  <p>$\\qquad \\displaystyle af(n/b) \\leq cf(n)$ \xa0\xa0\xa0\xa0\xa0[<strong>this is the regularity condition</strong>]</p>\n  \n  <p>for some constant $c &lt; 1$ and for all sufficiently large $n$, then .. </p>\n</blockquote>\n\n<p>Can someone tell me why the regularity condition is needed? How does the theorem fail if the condition is not satisfied?</p>\n', 'ViewCount': '2056', 'Title': 'Why is there the regularity condition in the master theorem?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-04T09:41:58.807', 'LastEditDate': '2012-10-04T09:41:58.807', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '7', 'OwnerDisplayName': 'GrowinMan', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><mathematical-analysis><landau-notation><master-theorem>', 'CreationDate': '2012-10-02T03:54:52.210', 'FavoriteCount': '2', 'Id': '4854'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We got tower $T_1$ with $n$ odd disks (1,3,5,...) and tower $T_2$ with $n$ even disks (2,4,6,...).\nNow we want to move all $2n$ disks to tower $T_3$.\nIf $T(p,q)$ is a recurrence relation of minimum number of moves we make to move $p$ disks from $T_1$ and $q$ disks from $T_2$ to $T_3$, we must find $T(n,n)$.</p>\n\n<p>If anyone has any idea, please share.</p>\n', 'ViewCount': '179', 'Title': 'An Alternative Hanoi Tower problem', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-07T17:03:01.067', 'LastEditDate': '2012-10-05T11:00:19.797', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3046', 'Tags': '<algorithms><recurrence-relation><board-games>', 'CreationDate': '2012-10-04T05:27:08.737', 'FavoriteCount': '0', 'Id': '4872'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Computational_complexity_theory" rel="nofollow">Wikipedia</a></p>\n\n<blockquote>\n  <p><strong>a computational problem</strong> is understood to be a task that is in principle amenable to being solved by a computer (i.e. the problem can\n  be stated by a set of mathematical instructions). Informally, a\n  computational problem consists of <strong>problem instances</strong> and <strong>solutions</strong> to\n  these problem instances. For example, primality testing is the problem\n  of determining whether a given number is prime or not. The instances\n  of this problem are natural numbers, and the solution to an instance\n  is yes or no based on whether the number is prime or not.</p>\n  \n  <p>... A key distinction between analysis of algorithms and computational\n  complexity theory is that the former is devoted to <strong>analyzing the\n  amount of resources needed by a particular algorithm to solve a\n  problem</strong>, whereas the latter asks a more general question about <strong>all\n  possible algorithms that could be used to solve the same problem</strong>.</p>\n</blockquote>\n\n<p>So a problem can be solved by multiple algorithms. </p>\n\n<p>I was wondering if an algorithm can solve different problems, or can only solve one problem? Note that I distinguish a problem and its instances as in the quote.</p>\n', 'ViewCount': '96', 'Title': 'Relation between problems and algorithms', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-05T13:07:38.853', 'LastEditDate': '2012-10-05T11:02:18.093', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '4885', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><complexity-theory><terminology>', 'CreationDate': '2012-10-05T04:36:59.257', 'Id': '4880'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>G{V, E} is directed, <strong>cyclic</strong>, weighted graph. What is the algorithm of finding all paths between any given two nodes?<br>\nCan you suggest any good reading?</p>\n', 'ViewCount': '465', 'Title': 'The path between any two nodes in cyclic directed graph', 'LastEditorUserId': '41', 'LastActivityDate': '2012-10-09T03:17:05.620', 'LastEditDate': '2012-10-09T03:17:05.620', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3067', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2012-10-05T12:01:57.987', 'FavoriteCount': '1', 'Id': '4883'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '852', 'Title': 'Dijkstra to favor solution with smallest number of edges if several paths have same weight', 'LastEditDate': '2012-10-07T06:39:29.413', 'AnswerCount': '1', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2826', 'FavoriteCount': '1', 'Body': "<p>You can modify any graph $G$ so that Dijkstra's finds the solution with the minimal number of edges thusly:</p>\n\n<p>Multiply every edge weight with a number $a$, then add $1$ to the weight to penalize each additional edge in the solution, i.e. </p>\n\n<p>$w'(u,v)=a*w(u,v)+1$</p>\n\n<p>This does not work for all values of $a$; $a$ needs to be at least $x$ for this to work. If $a$ is not this minimum number, it might not choose the shortest path. How do I find this minimum value $x$?</p>\n\n<p>Ps. This was done recreationally, I'm done with homework long ago.</p>\n", 'Tags': '<algorithms><graph-theory><shortest-path>', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-10-07T22:16:23.373', 'CommentCount': '3', 'AcceptedAnswerId': '4936', 'CreationDate': '2012-10-05T15:07:58.803', 'Id': '4887'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is part of a larger problem, which I believe I have reduced to this. Given a tree $T$ having positive edge weights, and $k$ leaves (nodes which have exactly one connected node), I need to delete some edges in the tree so that no two leaves in the original tree are connected (by a path) in the newly formed forest (of trees). The total sum of the weights of the deleted edges needs to be minimized.</p>\n\n<p>My understanding is that atleast $k-1$ edges need to be deleted to separate out all the $k$ leaves. Any more deletions will unnecessarily increase the total cost. Thus, we need to perform exactly $k-1$ deletions.</p>\n\n<p>My hypothesis:\nFor every pair of leaf nodes $l_i$ and $l_j$, find the edge with the minimum weight in the (unique) path from $l_i$ to $l_j$. The $k-1$ least weight edges from this set of edges need to be deleted. This will minimize the sum of weights of the edges to be deleted in order to disconnect all leaves from each other.</p>\n\n<p>I am unable to prove or disprove this hypothesis. Can someone please prove the correctness of this hypothesis, or give a counter-example along with the correct algorithm to solve this problem? If this is indeed correct, is there a faster way (asymptotic complexity wise) to solve this problem? This approach will take $\\Theta({k^2})$ time. Thanks in advance!</p>\n', 'ViewCount': '210', 'Title': 'Separate all leaves of a weighted tree with minimum weight cuts', 'LastActivityDate': '2014-01-23T09:20:39.300', 'AnswerCount': '2', 'CommentCount': '8', 'AcceptedAnswerId': '6396', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '3011', 'Tags': '<algorithms><graph-theory><trees>', 'CreationDate': '2012-10-06T05:53:00.637', 'FavoriteCount': '1', 'Id': '4898'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Foo and Bar are playing a game of strategy. At the start of the game, there are $N$ apples, placed in a row (in straight line). The apples are numbered from $1$ to $N$. Each apple has a particular price value.</p>\n\n<p>The price of $i$th apple is $p_i$.</p>\n\n<p>In this game, the players Foo and bar make an alternative move.</p>\n\n<p>In each move, the player does the following:</p>\n\n<ul>\n<li>If there is more than one apple left, the player tosses an unbiased coin. If  the outcome is head, the player takes the first apple among the apples that are  currently present in a row in a straight line.</li>\n<li>If there is a single apple left, te player takes it.</li>\n</ul>\n\n<p>The goal here is to calculate the expected total price value that Foo will get if Foo plays first.</p>\n\n<pre><code>Example 1:\nN=5\nApple price val: \n5 2 3 1 5 \n\nAnswer is : 11.00\n\nExample 2:\nN=6\n7 8 2 3 7 8\n\nAnswer : 21.250\n\n\nExample 3:\nN=3\n1 4 9\n\nFirst           Second      Third          Foo Total Val\nFoo gets 1  Bar gets 4  Foo gets 9          10\nFoo gets 1  Bar gets 9  Foo gets 4          5\nFoo gets 9  Bar gets 1  Foo gets 4          13\nFoo gets 9  Bar gets 4  Foo gets 1          10\n\nprobability 0.5 \u2022 0.5 = 0.25. \nExpected value (Foo)= (0.25 *10 )+ (0.25 *5) + (0.25*13)+ (0.25*10) = 9.500\n</code></pre>\n\n<p>I wrote the following code:</p>\n\n<pre><code>#include&lt;iostream&gt;\nusing namespace std;\ndouble calculate(int start,int end,int num,int current);\nint arr[2010];\nint main()\n{\n    int T;\n    scanf("%d",&amp;T);\n    for(int t=0;t&lt;T;t++)\n    {\n        int N;\n        scanf("%d",&amp;N);\n        for(int i=0;i&lt;N;i++)\n        {\n            scanf("%d",&amp;arr[i]);\n        }\n        printf("%.3lf\\n",calculate(0,N-1,N/2+N%2,0));   \n    }\n\n    return 0;\n}\ndouble calculate(int start,int end,int num,int current)\n{\n    if(num==current)\n        return 0;\n    double  value=0;\n    value=.5*arr[start]+.5*arr[end]+.5*calculate(start+1,end,num,current+1)+.5*calculate(start,end-1,num,current+1);\n    return value;\n}\n</code></pre>\n\n<p>But the above code is quite slow. The constraints are: price of apples $p_i \\le 1000$; $1 \\le N \\le 2000$; there are 500 test cases. How can I solve this more efficiently?</p>\n', 'ViewCount': '115', 'Title': 'Expected gain of a game of chance with differently-priced tokens', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-06T22:32:58.253', 'LastEditDate': '2012-10-06T20:43:51.750', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '4902', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><game-theory>', 'CreationDate': '2012-10-06T10:02:48.777', 'Id': '4899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3252', 'Title': "Why can't DFS be used to find shortest paths in unweighted graphs?", 'LastEditDate': '2012-10-07T13:03:05.600', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2826', 'FavoriteCount': '2', 'Body': '<p>I understand that using DFS "as is" will not find a shortest path in an unweighted graph. </p>\n\n<p>But why is tweaking DFS to allow it to find shortest paths in unweighted graphs such a hopeless prospect? All texts on the subject simply state that it cannot be done. I\'m unconvinced (without having tried it myself). </p>\n\n<p>Do you know any modifications that will allow DFS to find the shortest paths in  unweighted graphs? If not, what is it about the algorithm that makes it so difficult?</p>\n', 'Tags': '<algorithms><graph-theory><shortest-path>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-23T09:21:07.067', 'CommentCount': '2', 'AcceptedAnswerId': '4927', 'CreationDate': '2012-10-07T08:07:18.370', 'Id': '4914'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the runtime complexity of the following implementation of Bubblesort (for integers)?</p>\n\n<pre><code>    #define SWAP(a,b)   { int t; t=a; a=b; b=t; }\n\n    void bubble( int a[], int n )\n    /* Pre-condition: a contains n items to be sorted */\n    {\n       int i, j;\n      /* Make n passes through the array */\n      for(i=0;i&lt;n-1;i++)\n      {\n     /* From the first element to the end\n       of the unsorted section */\n        for(j=1;j&lt;(n-i);j++)\n        {\n        /* If adjacent items are out of order, swap them */\n       if( a[j-1]&gt;a[j] ) SWAP(a[j-1],a[j]);\n       }\n    }\n}  \n</code></pre>\n', 'ViewCount': '282', 'Title': 'Complexity of optimized bubblesort', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-07T15:49:43.613', 'LastEditDate': '2012-10-07T15:49:43.613', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3087', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-10-07T09:52:54.087', 'Id': '4915'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $A$ be an adjacency matrix of a directed graph. What's the meaning of the $(i,j)-$entry of the matrix $((A^T)^{7} \\cdot (A^{7}))$ ? </p>\n\n<p>My initial interpretation is that $(i,j)$ of this matrix is zero whenever nodes $i$ and $j$ have no 7-length in-coming paths from a common source. Is that right? Any attention is appreciated! </p>\n", 'ViewCount': '125', 'Title': 'Meaning of the adjacency matrix product', 'LastActivityDate': '2012-10-07T19:58:49.100', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '5', 'OwnerDisplayName': 'John Smith', 'PostTypeId': '1', 'OwnerUserId': '3091', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2012-10-07T14:44:26.083', 'FavoriteCount': '1', 'Id': '4923'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '611', 'Title': 'Dynamic programming with large number of subproblems', 'LastEditDate': '2012-10-09T12:39:47.157', 'AnswerCount': '2', 'Score': '7', 'OwnerDisplayName': 'Alexandre', 'PostTypeId': '1', 'OwnerUserId': '3101', 'FavoriteCount': '2', 'Body': '<p>Dynamic programming with large number of subproblems. So I\'m trying to solve this problem from Interview Street:</p>\n\n<blockquote>\n  <p><strong>Grid Walking</strong> (Score 50 points)<br>\n  You are situated in an $N$-dimensional grid at position $(x_1,x_2,\\dots,x_N)$. The dimensions of the grid are $(D_1,D_2,\\dots,D_N$). In one step, you can walk one step ahead or behind in any one of the $N$ dimensions. (So there are always $2N$ possible different moves). In how many ways can you take $M$ steps such that you do not leave the grid at any point? You leave the grid if for any $x_i$, either $x_i \\leq 0$ or $x_i &gt; D_i$.</p>\n</blockquote>\n\n<p>My first try was this memoized recursive solution:</p>\n\n<pre><code>def number_of_ways(steps, starting_point):\n    global n, dimensions, mem\n    #print steps, starting_point\n    if (steps, tuple(starting_point)) in mem:\n        return mem[(steps, tuple(starting_point))]\n    val = 0\n    if steps == 0:\n        val = 1\n    else:\n        for i in range(0, n):\n            tuple_copy = starting_point[:]\n            tuple_copy[i] += 1\n            if tuple_copy[i] &lt;= dimensions[i]:\n                val += number_of_ways(steps - 1, tuple_copy)\n            tuple_copy = starting_point[:]\n            tuple_copy[i] -= 1\n            if tuple_copy[i] &gt; 0:\n                val += number_of_ways(steps - 1, tuple_copy)\n    mem[(steps, tuple(starting_point))] = val\n    return val\n</code></pre>\n\n<p>Big surprise: it fails for a large number of steps and/or dimensions due to a lack of memory.</p>\n\n<p>So the next step is to improve my solution by using dynamic programming. But before starting, I\'m seeing a major problem with the approach. The argument <code>starting_point</code> is an $n$-tuple, where $n$ is as large as $10$. So in fact, the function could be <code>number_of_ways(steps, x1, x2, x3, ... x10)</code> with  $1 \\leq x_i \\leq 100$.</p>\n\n<p>The dynamic programming problems I\'ve seen in textbooks almost all have twp variables, so that only a two-dimensional matrix is needed. In this case, a ten-dimensional matrix would be needed. So $100^{10}$ cells in total.</p>\n\n<p>With 2-D matrixes in dynamic programming, usually only the previous row of calculations is needed for the next calculation, hence reducing the spatial complexity from $mn$ to $\\min(m,n)$. I\'m not sure how I would do the same in this case. Visualizing a table isn\'t feasible, so the answer would have to come directly from the recursion above. </p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Using Peter Shor\'s suggestions, and making some minor corrections, notably the need to keep track of position in the $W(i, t_i)$ function, and rather than only splitting dimensions into two sets A and B, doing the splitting recursively, effectively using a divide-and-conquer method, until a base case is reached where only one dimension is in the set.</p>\n\n<p>I came up with the following implementation, which passed all tests below the maximum execution time:</p>\n\n<pre><code>def ways(di, offset, steps):\n    global mem, dimensions\n    if steps in mem[di] and offset in mem[di][steps]:\n        return mem[di][steps][offset]\n    val = 0\n    if steps == 0:\n        val = 1\n    else:\n        if offset - 1 &gt;= 1:\n            val += ways(di, offset - 1, steps - 1)\n        if offset + 1 &lt;= dimensions[di]:\n            val += ways(di, offset + 1, steps - 1)\n    mem[di][steps][offset] = val\n    return val\n\n\ndef set_ways(left, right, steps):\n    # must create t1, t2, t3 .. ti for steps\n    global mem_set, mem, starting_point\n    #print left, right\n    #sleep(2)\n    if (left, right) in mem_set and steps in mem_set[(left, right)]:\n        return mem_set[(left, right)][steps]\n    if right - left == 1:\n        #print \'getting steps for\', left, steps, starting_point[left]\n        #print \'got \', mem[left][steps][starting_point[left]], \'steps\'\n        return mem[left][steps][starting_point[left]]\n        #return ways(left, starting_point[left], steps)\n    val = 0\n    split_point =  left + (right - left) / 2 \n    for i in xrange(steps + 1):\n        t1 = i\n        t2 = steps - i\n        mix_factor = fact[steps] / (fact[t1] * fact[t2])\n        #print "mix_factor = %d, dimension: %d - %d steps, dimension %d - %d steps" % (mix_factor, left, t1, split_point, t2)\n        val += mix_factor * set_ways(left, split_point, t1) * set_ways(split_point, right, t2)\n    mem_set[(left, right)][steps] = val\n    return val\n\nimport sys\nfrom time import sleep, time\n\nfact = {}\nfact[0] = 1\nstart = time()\naccum = 1\nfor k in xrange(1, 300+1):\n    accum *= k\n    fact[k] = accum\n#print \'fact_time\', time() - start\n\ndata = sys.stdin.readlines()\nnum_tests = int(data.pop(0))\nfor ignore in xrange(0, num_tests):\n    n_and_steps = data.pop(0)\n    n, steps = map(lambda x: int(x), n_and_steps.split())\n    starting_point = map(lambda x: int(x), data.pop(0).split())\n    dimensions = map(lambda x: int(x), data.pop(0).split())\n    mem = {}\n    for di in xrange(n):\n        mem[di] = {}\n        for i in xrange(steps + 1):\n            mem[di][i] = {}\n            ways(di, starting_point[di], i)\n    start = time()\n    #print \'mem vector is done\'\n    mem_set = {}\n    for i in xrange(n + 1):\n        for j in xrange(n + 1):\n            mem_set[(i, j)] = {}\n    answer = set_ways(0, n, steps)\n    #print answer\n    print answer % 1000000007\n    #print time() - start\n</code></pre>\n', 'Tags': '<algorithms><efficiency><dynamic-programming>', 'LastEditorUserId': '3101', 'LastActivityDate': '2012-12-11T02:10:15.510', 'CommentCount': '4', 'AcceptedAnswerId': '4949', 'CreationDate': '2012-10-07T23:07:21.413', 'Id': '4941'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '963', 'Title': 'Finding paths with smallest maximum edge weight', 'LastEditDate': '2012-10-08T09:00:08.523', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Jiew Meng', 'PostTypeId': '1', 'OwnerUserId': '3099', 'FavoriteCount': '1', 'Body': '<p>I need to find the easiest cost path between two vertices of a graph. Easiest here means the path with the smallest maximum-weigth edge. </p>\n\n<p><img src="http://i.stack.imgur.com/WWszV.png" alt="enter image description here"></p>\n\n<p>In the above graph, the easiest path from 1 to 2 is: </p>\n\n<pre><code>1 &gt; 3 &gt; 4 &gt; 2\n</code></pre>\n\n<p>Because the maximum edge weight is only 2. On the other hand, the shortest path <code>1 -&gt; 2</code> has maximum weight 4. </p>\n\n<p>So it\'s an MST problem. I am thinking I will use Kruskal\'s algorithm to build the tree, but I\'m not sure how exactly. I will know the edges but how do I "reconstruct" the path? For example, given vertices <code>3</code> and <code>2</code>, how do I know to go left (top) of right in the tree? Or do I try both ways? </p>\n', 'Tags': '<algorithms><graph-theory><shortest-path>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T09:00:08.523', 'CommentCount': '3', 'AcceptedAnswerId': '4943', 'CreationDate': '2012-10-08T03:50:33.827', 'Id': '4942'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I got no responses on stackoverflow, so I\'m asking here:</p>\n\n<p>How useful is the LIS (<a href="http://www.geeksforgeeks.org/archives/9591" rel="nofollow">Longest Increasing Subsequence</a>) problem in tackling other CS problems?  There are a few algorithms, using patience sorting, dynamic programming or with decision trees.  How are these used in real life -- maybe to data streams or something?</p>\n\n<p>To remind you, I put in bold the longest increasing sequence</p>\n\n<p>{<strong>0</strong>, 8, 4, 12, <strong>2</strong>, 10, <strong>6</strong>, 14, 1, <strong>9</strong>, 5, 13, 3, <strong>11</strong>, 7, <strong>15</strong>}.</p>\n\n<p>As a bonus, is there any way to use the result that <a href="http://mathworld.wolfram.com/Erdos-SzekeresTheorem.html" rel="nofollow">a sequence of length mn + 1 will have an increasing subsequence of length m or a decreasing subsequence of length n</a>? E.g. Our list as length 16, so there should be an increasing sequence of length 5 or decreasing sequence of length 5.  In our case <em>0,2,6,9,11,15</em>.</p>\n\n<p>Also an increasing sequence of length 8 or a decreasing sequence of length 3: in our case <em>12,10,1</em>.</p>\n\n<p>CF: <a href="http://stackoverflow.com/q/12458641/737051">http://stackoverflow.com/q/12458641/737051</a></p>\n', 'ViewCount': '242', 'Title': 'Longest Increasing Subsequence', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T21:06:52.490', 'LastEditDate': '2012-10-11T21:06:52.490', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'OwnerDisplayName': 'john mangual', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><arrays><subsequences>', 'CreationDate': '2012-10-09T18:03:23.037', 'Id': '4985'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>This is a similar algorithm to one I used in a previous question, but I\'m trying to illustrate a different problem here.</p>\n\n<pre><code>for (int i = 0; i &lt; numbers.length - 1; i++) {\n    for (int j = i + 1; j &lt; numbers.length; j++) {\n        if (numbers[i] + numbers[j] == 10) {\n            System.out.println(i+" and "+j+" add up to 10!");\n            return;\n        }\n    }\n}\nSystem.out.println("None of these numbers add up to 10!");\nreturn;\n</code></pre>\n\n<p>Basically, I realised, that I have a decent understanding of how to work out the best case run time here. I.e. the first two numbers which are fed in, add up to 10, and therefore our best-case runtime is constant time. Also, I understand that in the worst-case, none of the numbers we provide add up to 10, so we must then iterate through all loops, giving us a quadratic runtime. However, in the slides I was going through (for this particular course) I noticed that I had missed this:</p>\n\n<p>Average case</p>\n\n<p>\u2022 Presume that we\'ve just randomly given\nintegers between 1 and 9 as input in\nnumbers to our previous example</p>\n\n<p>\u2022 Exercise: Work out the average running time</p>\n\n<p>I realise I don\'t have the first clue as to how to go about calculating average case run-time. I\'m not asking for the answer to: "what is the average-case runtime for this algorithm?", what I need to know is, how do you work something like this out? </p>\n', 'ViewCount': '253', 'Title': 'How to go about working the average case run time of this trivial algorithm (and other algorithms)?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T16:02:27.433', 'LastEditDate': '2012-10-28T16:02:27.433', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2012-10-10T10:06:09.990', 'Id': '4993'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an algorithm to raise a matrix to the $n$th power in $O(\\log n)$ time? I have been searching online, but have been unsuccessful thus far.</p>\n', 'ViewCount': '448', 'Title': 'Matrix powering in $O(\\log n)$ time?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-11T05:37:25.543', 'LastEditDate': '2012-10-10T21:13:06.220', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '4999', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '110', 'Tags': '<algorithms><matrices>', 'CreationDate': '2012-10-10T13:37:38.843', 'Id': '4998'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What differences and relationships are between randomized algorithms and nondeterministic algorithms?</p>\n\n<p>From Wikipedia</p>\n\n<blockquote>\n  <p>A <strong>randomized algorithm</strong> is an algorithm which employs a degree of\n  randomness as part of its logic. The algorithm typically uses\n  uniformly random bits as an auxiliary input to guide its behavior, in\n  the hope of achieving good performance in the "average case" over all\n  possible choices of random bits. Formally, the algorithm\'s performance\n  will be a random variable determined by the random bits; thus either\n  the running time, or the output (or both) are random variables.</p>\n  \n  <p>a <strong>nondeterministic algorithm</strong> is an algorithm that can exhibit\n  different behaviors on different runs, as opposed to a deterministic\n  algorithm. There are several ways an algorithm may behave differently\n  from run to run. A <strong>concurrent algorithm</strong> can perform differently on\n  different runs due to a race condition. A <strong>probabalistic algorithm</strong>\'s\n  behaviors depends on a random number generator. An algorithm that\n  solves a problem in nondeterministic polynomial time can run in\n  polynomial time or exponential time depending on the choices it makes\n  during execution.</p>\n</blockquote>\n\n<p>Are randomized algorithms and probablistic algorithms the same concept? </p>\n\n<p>If yes, so are randomized algorithms just a kind of nondeterministic algorithms?</p>\n', 'ViewCount': '1555', 'Title': 'Differences and relationships between randomized and nondeterministic algorithms?', 'LastEditorUserId': '31', 'LastActivityDate': '2014-02-02T18:42:47.173', 'LastEditDate': '2014-01-23T10:11:12.273', 'AnswerCount': '4', 'CommentCount': '7', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><terminology><randomized-algorithms><machine-models><nondeterminism>', 'CreationDate': '2012-10-11T05:19:33.520', 'FavoriteCount': '5', 'Id': '5008'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>$$\\begin{align*}\n  T[1] &amp;= 1 \\\\\n  T[2] &amp;= 2 \\\\\n  T[i] &amp;= T[i-1] + T[i-3] + T[i-4] &amp; \\text{for \\(i \\gt 2\\)} \\\\\n\\end{align*}$$</p>\n\n<p>I have to calculate $T[N]$, but $N$ is too big ($\\approx 10^9$), how can I optimize it?</p>\n', 'ViewCount': '142', 'Title': 'Optimize a linear recurrence', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-11T09:35:56.713', 'LastEditDate': '2013-03-11T09:35:56.713', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '3154', 'Tags': '<algorithms><dynamic-programming><recursion>', 'CreationDate': '2012-10-11T10:19:54.720', 'Id': '5018'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I like to implement an arc-length Parameterization of a cubic bezier curve. So far I have implemented the method of calculating the arc length of the curve and now I\'m stuck at calculating the times to divide the original curve into equal arc length segments.</p>\n\n<p>What i have:</p>\n\n<ul>\n<li>$m = 5$ The number of segments to create</li>\n<li>$i = 0,1, ...., m$ </li>\n<li>$s = \\text{arc length}$</li>\n<li>$l = s / m$</li>\n<li>$t_0,t_1,...,t_n$ The parameter values of the original bezier curve.</li>\n</ul>\n\n<p>The formula I have:\n$$\\int_{t_0}^{\\tilde{t_i}} ds/dt = i * \\tilde{l}$$</p>\n\n<p>To calculate the value of $\\tilde{t_i}$ I would have to go through 2 steps:</p>\n\n<ol>\n<li>Calculate a spline segment indexed by $j$ which satisfies $\\sum_{p=0}^{j-1} l_p \\le i * \\tilde{l} &lt; \\sum_{p=0}^{j}l_p$</li>\n<li>Compute $\\tilde{t_i}$ such that $\\int_{t_j}^{\\tilde{t_i}}ds/dt * dt = i  * \\tilde{l} - \\sum_{p=0}^{j-1}l_p$</li>\n</ol>\n\n<p>To my questions:</p>\n\n<ol>\n<li><p>What is $\\tilde{l}$? I know it is an approximated value but is it neccesery to be different or could just use $l$?</p></li>\n<li><p>The first calculation of $j$ makes sense, but how would I solve the second to $\\tilde{t_i}$?</p></li>\n</ol>\n\n<p>Maybe I just don\'t understand correctly what the integral $\\int_a^bds/dt*dt$ is or how I can calculate it programmatically.</p>\n\n<p>I am following <a href="http://homepage.cs.uiowa.edu/~hank/publications/images/ArcLength03.pdf" rel="nofollow">this paper</a>.</p>\n', 'ViewCount': '282', 'Title': 'Arc-Length parameterization of a cubic bezier curve', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T20:52:56.190', 'LastEditDate': '2012-10-11T20:52:56.190', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '3156', 'Tags': '<algorithms><computational-geometry><numerical-analysis>', 'CreationDate': '2012-10-11T15:33:30.873', 'Id': '5021'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a forest, i.e., nodes with directed edges and no cycles (directed or undirected). I define the height of a vertex $v$ as 0 if it does not have any incoming edges, or the maximum number of edges to traverse in reverse to reach a vertex of height 0. \n<img src="http://i.stack.imgur.com/WzfAx.png" alt="enter image description here"></p>\n\n<p>I also know that the average degree of a node is a small constant, say 2 or so. To find the height of all vertices, I can think of two algorithms:</p>\n\n<p>Walking Algorithm</p>\n\n<ol>\n<li>Go through and mark $h=0$ for vertices with no incoming edges.</li>\n<li>For each vertex with $h=0$, follow the outgoing edges, updating the height of each encountered vertex if it\'s previous height is smaller.</li>\n</ol>\n\n<p>Frontier Algorithm</p>\n\n<ol>\n<li>Go through and mark $h=0$ for vertices with no incoming edges, and mark these as the frontier.</li>\n<li>For every frontier vertex, see if it\'s parent has children at or below the frontier, If it does, mark the parent as having $1$ plus the largest height among its children. Mark the parent as being on the frontier.</li>\n<li>Repeat 2 until there is nothing beyond the frontier.</li>\n</ol>\n\n<p>My questions:</p>\n\n<ol>\n<li>Is there a name for this problem, and a well known fastest solution?</li>\n<li>I tend to think simply walking up from all the $h=0$ vertices is the fastest solution. Am I right?</li>\n</ol>\n', 'ViewCount': '206', 'Title': 'Finding the height of all nodes in a forest', 'LastEditorUserId': '3159', 'LastActivityDate': '2012-10-12T17:47:57.643', 'LastEditDate': '2012-10-11T22:03:43.253', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '8', 'OwnerDisplayName': 'highBandWidth', 'PostTypeId': '1', 'OwnerUserId': '3159', 'Tags': '<algorithms><trees><graph-traversal>', 'CreationDate': '2012-10-11T18:13:18.083', 'Id': '5027'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In radix sort we first sort by least significant digit then we sort by second least significant digit and so on and end up with sorted list. </p>\n\n<p>Now if we have list of $n$ numbers we need $\\log n$ bits to distinguish between those number. So number of radix sort passes we make will be $\\log n$. Each pass takes $O(n)$ time and hence running time of radix sort is $O(n \\log n)$ </p>\n\n<p>But it is well known that it is linear time algorithm. Why? </p>\n', 'ViewCount': '3133', 'Title': 'Why is Radix Sort $O(n)$?', 'LastActivityDate': '2012-10-15T00:06:04.160', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '244', 'Tags': '<algorithms><sorting>', 'CreationDate': '2012-10-11T21:25:28.443', 'FavoriteCount': '1', 'Id': '5030'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Could anybody please explain what the difference between "bounding" and "pruning" in branch and bound algorithms is?</p>\n\n<p>I\'d also appreciate references (preferably books), where this distinction is made clear.</p>\n', 'ViewCount': '276', 'Title': 'What is the difference between bounding and pruning in branch-and-bound algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-13T07:40:01.563', 'LastEditDate': '2012-10-14T09:26:51.660', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2914', 'Tags': '<algorithms><terminology><branch-and-bound>', 'CreationDate': '2012-10-13T10:20:34.733', 'Id': '5040'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Two problems: Given a known table (boolean or int), convert it to a function that returns the same value using only simple operations (and, or, xor, sum...). For example:</p>\n\n<p><code>bool table1[] = {0,1}</code> converted to <code>fun (int index) {return index;}</code></p>\n\n<p><code>bool table2[] = {0,1,1,0}</code> converted to <code>fun (int index) {return index ^ (index&gt;&gt;1);}</code></p>\n\n<p><code>int table3[] = {4,5,7}</code> converted to <code>fun(int index) {return index+4+(index&gt;&gt;1);}</code></p>\n', 'ViewCount': '77', 'Title': 'Convert table look-up into function', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T11:00:07.563', 'LastEditDate': '2012-10-14T09:50:06.500', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'Alain', 'PostTypeId': '1', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-10-14T01:55:10.093', 'Id': '6053'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '56', 'Title': 'Compute relational composition in $O(|E||V|)$', 'LastEditDate': '2012-10-14T09:37:36.983', 'AnswerCount': '2', 'Score': '7', 'OwnerDisplayName': 'Johannes', 'PostTypeId': '1', 'OwnerUserId': '3181', 'Body': '<p>Definitions: Let $G=(V,E)$ be a DAG without self-loops, and $X \\subseteq G$ and $Y \\subseteq G$ be graphs.</p>\n\n<p>Input: $X,Y$. Output: The relational composition <a href="http://en.wikipedia.org/wiki/Relation_composition">relational composition</a> $X \\circ Y$ in $\\mathcal{O}(|E||V|)$.</p>\n\n<ul>\n<li>Case 1: $|E| \\le |V|$. Two for loops over $E(X)$ and $E(Y)$: Runtime $ \\le \\mathcal{O}(|E|^2) \\le \\mathcal{O}(|E||V|)$.</li>\n<li>Case 2: $|V| \\le |E|$\n<ol>\n<li>Draw the graph $(V(G),E(X) \\cup E(Y))$: $(O(|V|)+\\mathcal{O}(|2E|)))$. We call edges from $E(X)$ black and from $E(Y)$ red.</li>\n<li>Topologically sort it (Kahn: $\\mathcal{O}(|V|) + \\mathcal{O}(|E|)$). Let the first level be $0$, and edges go from a level to a higher level.</li>\n<li>Draw this graph twice.</li>\n<li>In the first copy, remove every red edge beginning at even level, and every black edge beginning at odd level: $\\mathcal{O}(E)$.</li>\n<li>In the second copy, remove every "black even" and "red odd": $\\mathcal{O}(E)$.</li>\n<li>For the first copy:\n<ul>\n<li>for all nodes $u$ on level $2i$</li>\n<li>for all nodes $v$ on level $2i+1$</li>\n<li>report edge $(u,v)$ (Runtime $\\mathcal{O}(V^2) \\le \\mathcal{O}(EV)$).</li>\n</ul></li>\n<li>For the second copy: The same for "$2i+1$".</li>\n<li>Union the reported nodes, throw out duplicates  $\\mathcal{O}(V^2) &lt;= \\mathcal{O}(EV)$ (I hope the graph representation allows this).</li>\n</ol></li>\n</ul>\n\n<p>Could some of you please look over my algorithm and check whether </p>\n\n<ul>\n<li>it is correct</li>\n<li>it is in $\\mathcal{O}(|E||V|)$</li>\n</ul>\n\n<p>If it\'s correct, does my algorithm already "exist"? If not, could you provide an alternative? I\'ll accept the first answer, but upvote if some more people are so kind to check.</p>\n\n<p>EDIT: Step 6 Seems to be in $O(E^2)$ sometimes. I wish this would not be true. Has anyone a working algorithm?</p>\n', 'Tags': '<algorithms><graph-theory><algorithm-analysis><check-my-algorithm>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T09:41:40.783', 'CommentCount': '0', 'AcceptedAnswerId': '6057', 'CreationDate': '2012-10-13T08:23:35.550', 'Id': '6055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We are given an interval $I$ and several points $p_1,p_2,...,p_n$. We are also given a set of sensors. Each sensor can be represented by an interval on the same line, which means all points lie within the interval can be monitored by the corresponding sensor. The sensors may not have the equal range.</p>\n\n<p>Given the current positions of points and sensors, some (or maybe none) points in $I$ may not be monitored by any sensor. We would like to ask the following question: </p>\n\n<blockquote>\n  <p>Given a distance $\\delta$, is it possible to shift each sensor (to the left or to the right) by a distance at most $\\delta$, such that every point in $I$ can be covered by some sensor?</p>\n</blockquote>\n\n<p>PS: I tried to solve this by greedy algorithm. But there is always an exception to any greedy paradigm I came up with. If we want to cover the whole interval with the sensors, I am sure it can be solved by greedy algorithm. But if we only want to "monitor" finite discrete points, is there an efficient algorithm?</p>\n', 'ViewCount': '136', 'Title': 'Sensor Cover Problem', 'LastEditorUserId': '848', 'LastActivityDate': '2012-10-17T23:24:33.717', 'LastEditDate': '2012-10-15T01:35:07.993', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-10-14T13:26:04.793', 'FavoriteCount': '1', 'Id': '6062'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>By <a href="http://math.stackexchange.com/questions/112734/in-what-sense-is-uniform-cost-search-uniform">this</a> possibly good attempt to explain this, the "uniformity" in the Uniform cost search <em>is actually the uniformity of the heuristic function</em>.</p>\n\n<p>Is this explanation correct ? If yes, then don\'t all <code>un-informed</code> cost searches (like <code>BFS</code>, <code>DFS</code>, etc) have no heurstic and thus, be called as "<code>uniform cost</code> searches" ?</p>\n', 'ViewCount': '265', 'Title': 'Why is Uniform cost search called "uniform" cost search?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-15T06:30:49.180', 'LastEditDate': '2012-10-14T21:24:18.123', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6084', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3032', 'Tags': '<terminology><artificial-intelligence><search-algorithms><heuristics>', 'CreationDate': '2012-10-14T19:43:54.853', 'Id': '6072'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '188', 'Title': 'Represent string as concatenations', 'LastEditDate': '2012-10-18T02:50:57.417', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '220', 'FavoriteCount': '1', 'Body': "<p>If $S_1,S_2$ are set of strings, then $S_1S_2 = \\{s_1s_2|s_1\\in S_1, s_2\\in S_2\\}$. $S^0=\\{\\epsilon\\}$, $\\epsilon$ is the empty string. $S^n = S^{n-1}S$. </p>\n\n<p>Two related problems about represent string as concatenation of other strings. </p>\n\n<ol>\n<li><p>Given a finite set $S$ of strings, how to decide if there exist a string can be written as concatenations of elements in $S$ in two different ways?</p></li>\n<li><p>Given a finite set $S$ of strings and $n$, how can one compute the smallest set of strings $T$, such that $S\\subset T^n$?</p></li>\n</ol>\n\n<p>(Bonus: what about infinite $S$, at least when it's regular? For the second problem when $S$ is infinite, we might ask to find a minimal $T$ under set inclusion.)</p>\n", 'Tags': '<algorithms><formal-languages><strings>', 'LastEditorUserId': '220', 'LastActivityDate': '2012-10-20T12:24:21.363', 'CommentCount': '6', 'AcceptedAnswerId': '6190', 'CreationDate': '2012-10-16T22:30:32.147', 'Id': '6114'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given an adjacency matrix $A_G$ of an undirected graph $G$, it is easy and straightforward to compute the characteristic polynomial $\\chi_G(\\lambda)$. What about the other way around? The problem can be formulated as follows.</p>\n\n<blockquote>\n  <p><strong>Problem</strong> Given a polynomial $P$, decide whether there is a graph $G$ with the corresponding adjacency matrix $A_G$ such that its characteristic polynomial $\\chi_G(\\lambda)$ equals the given $P$.</p>\n</blockquote>\n\n<p>For an arbitrary $P$, it is not always the case that there is a corresponding $A_G$. The naive exhaustive algorithm for the problem uses a basic theorem in algebraic graph theory: </p>\n\n<blockquote>\n  <p><strong>Theorem</strong> Let $G=(V,E)$ be a graph with adjacency matrix $A_G$ and $\\chi_G(\\lambda) = \\lambda^n+c_1\\lambda^{n-1}+c_2\\lambda^{n-2}+\\cdots+c_{n-1}\\lambda+c_n$, then </p>\n  \n  <p><strong>(1)</strong> $c_1=0$, </p>\n  \n  <p><strong>(2)</strong> $-c_2 = |E|$, and </p>\n  \n  <p><strong>(3)</strong> $-c_3 = \\text{twice the # of triangles in G}$.</p>\n</blockquote>\n\n<p>Now, given $P$, the algorithm goes through every candidate $A_G$ of a corresponding $G$ with $|E|=-c_2$ and number of triangles ($K_3$) equal to $-c_3$. For each $A_G$, compute $\\text{det}(A_G - \\lambda I)$ and see if it equals the given $P$. If none match, return false. Otherwise, return the $A_G$.</p>\n\n<p>This works, but is clearly not fast. The exhaustive algorithm would work even without the above theorem. Its use makes the search space smaller. What's a fast and more clever algorithm?</p>\n", 'ViewCount': '173', 'Title': "What's a fast algorithm to decide whether there is an $A_G$ corresponding to a given $\\chi_G(\\lambda)$?", 'LastEditorUserId': '3092', 'LastActivityDate': '2012-10-25T20:02:19.457', 'LastEditDate': '2012-10-25T19:53:16.970', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6314', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><graph-theory><discrete-mathematics><matrices>', 'CreationDate': '2012-10-16T22:50:02.250', 'Id': '6115'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am studying <a href="http://en.wikipedia.org/wiki/Best-first_search" rel="nofollow">best first search</a> as it compares to BFS (breadth-first search) and DFS (depth-first search), but I don\'t know when BFS is better than best-first search. So, my question is </p>\n\n<blockquote>\n  <p>When would best-first search be worse than breadth-first search?</p>\n</blockquote>\n\n<p>This question is one of the back exercises in <em>Artificial Intelligence</em> by Rich &amp; Knight, and it asks for an answer in terms of time &amp; space complexity and allows you to define any heuristic function.</p>\n', 'ViewCount': '2151', 'Title': 'When would best first search be worse than breadth first search?', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-04T08:12:23.923', 'LastEditDate': '2012-11-04T06:23:27.297', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '6461', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1267', 'Tags': '<graphs><artificial-intelligence><search-algorithms>', 'CreationDate': '2012-10-17T15:52:14.500', 'Id': '6125'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some difficulties performing the worst case analysis on this algorithm.<br>\nThe outermost loop is executed $2N$ times.<br>\nThe while loop, in the worst case, will increase by $2$ each time, so it performs $i/2$ basic operations ($*2$ because double call)</p>\n\n<pre><code>for (i=1; i&lt;=2*N; i++) {\n        j = 0;\n        while (j &lt;= i) {\n            a[i] = function (function (a[i]));\n            if (c[i][j] != 0)\n                j = j + 6;\n            else\n                j = j + 2;\n        }\n    }\n</code></pre>\n\n<p><code>function</code> is the basic operation.<br>\nAm I going the right way? </p>\n', 'ViewCount': '525', 'Title': 'Runtime analysis of a nested loop', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-17T09:19:56.000', 'LastEditDate': '2012-11-17T09:19:56.000', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'eouti', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2012-10-17T16:35:50.160', 'Id': '6129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been trying to implement Brzozowski's algorithm but I've just discovered that it creates suboptimal automata for a certain class of inputs, having one more state than what is really needed in the result. I can show it on a trivial automaton:</p>\n\n<pre><code>   a b           a   b           a b            a     b            a b\n&gt;0 0 1  rev  *0 0,2  -   det  &gt;0 - 1  rev  *0   -     -    det  &gt;0 1 2\n 1 1 2  --&gt;   1  1   0   --&gt;   1 2 5  --&gt;   1   -    0,4   --&gt;   1 1 2\n*2 0 2       &gt;2  -  1,2        2 2 3        2  1,2    -          2 2 3\n                              *3 4 -        3   -     2         *3 1 3\n                              *4 4 1        4  3,4    -          \n                              *5 5 5        5   5    1,5         \n                                           &gt;6 3,4,5 1,2,5        \n</code></pre>\n\n<p>Here <em>rev</em> is the edge reversal part, where I'd already removed the transitions on epsilon, and <em>det</em> is determinization through powerset construction, creating new states as soon as it discovers them, recursively.</p>\n\n<p>The problem here is this: once I add the extra state to make up for the three different start states after the first edge reversal and powerset construction, nothing ever returns to that state and thus I can't get rid of it later for being equivalent to the original start state.</p>\n\n<p>Is there something wrong with the way I'm doing it? Am I missing something?</p>\n", 'ViewCount': '136', 'Title': "Problem with implementing Brzozowski's algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-18T15:29:53.410', 'LastEditDate': '2012-10-18T06:29:58.037', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6152', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4237', 'Tags': '<algorithms><automata><finite-automata>', 'CreationDate': '2012-10-17T22:23:04.570', 'Id': '6138'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I found  a 2-approximation algorithm for a certain problem and I want to show that the analysis is tight. </p>\n\n<p>Do I now need to come up with an example of generic size $n$ or does it suffice to show that I   have an example of size $10$ for which the algorithm yields $2OPT$?</p>\n", 'ViewCount': '398', 'Title': 'Providing Tight Example in Approximation Algorithm Analysis', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-18T18:33:07.623', 'LastEditDate': '2012-10-18T06:35:18.780', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4239', 'Tags': '<algorithms><proof-techniques><approximation>', 'CreationDate': '2012-10-17T23:07:10.890', 'FavoriteCount': '2', 'Id': '6140'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="https://en.wikipedia.org/wiki/Radon_transform" rel="nofollow">Radon transform</a> is used to take 2d projections of an object and create a 3d representation.</p>\n\n<p>It seems like it would be possible to apply such a transform in 3d graphics in games (although possibly too slow to be practical).</p>\n\n<p>For example, a very simple way to display an object is to use a 3d rectangle and texture map each side. This is relatively fast but the 3d detail is limited. When a side is parallel with the visual plane it will represent the detail 100% (so the visual detail would be limited to that of the texture map). Of course it won\'t represent external 3d effects properly, like lighting.</p>\n\n<p>But by using the Radon transform one could gain a true 3d approximation of the object from the six textures/projections used. By increasing the number of textures/projections the approximation is better.</p>\n\n<p>I\'m curious if the idea has potential. Possibly for high-quality 3d models it might pay off in performance and size. Of course 3d models can be optimized to limit their size which also increases speed but visually doesn\'t change much.</p>\n', 'ViewCount': '207', 'Title': 'Radon transform for advanced 3d graphics and games?', 'LastEditorUserId': '4244', 'LastActivityDate': '2013-07-17T02:17:03.387', 'LastEditDate': '2012-10-19T06:48:32.120', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4244', 'Tags': '<algorithms><computational-geometry><efficiency><graphics>', 'CreationDate': '2012-10-18T05:20:19.123', 'FavoriteCount': '1', 'Id': '6147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="https://www.interviewstreet.com/challenges/dashboard/#problem/4f1c88e0dec8a" rel="nofollow">https://www.interviewstreet.com/challenges/dashboard/#problem/4f1c88e0dec8a</a></p>\n\n<blockquote>\n  <p>Fairy Chess (35 Points)</p>\n  \n  <p>You have a $N \\times N$ chess board. An $S$-leaper is a chess piece which can move from square $(x_1,y_1)$ on the board to any other square $(x_2,y_2)$ if $|x_1 - x_2| + |y_1 - y_2| \\le S$. The chess board may also contain some pawns. The leaper cannot land on the same square as a pawn. In how many ways can a leaper move $M$ times on the board?</p>\n  \n  <p>Input:\n  The first line contains the number of test cases $T$. $T$ cases follow. Each case contains integers $N$, $M$ and $S$ on the first line. The next $N$ lines contains $N$ characters each. The $i$th character on the $j$th line is a <code>.</code> if the corresponding chess square is empty, <code>P</code> if there is a pawn, or <code>L</code> if the leaper is situated on that square.</p>\n  \n  <p>Output:\n  For each case, output the number of ways the leaper can make $M$ moves. Output each answer modulo 1000000007.</p>\n  \n  <p>Constraints:\n  $$\\begin{gather}\n1 \\le T \\le 10 \\\\\n1 \\le S \\le N \\le 200 \\\\\n1 \\le M \\le 200 \\\\\n\\end{gather}$$\n  There will be exactly one <code>L</code> character on the board.</p>\n  \n  <p>Sample Input:</p>\n\n<pre><code>3\n4 1 1\n....\n.L..\n.P..\n....\n3 2 1\n...\n...\n..L\n4 3 2\n....\n...L\n..P.\nP...\n</code></pre>\n  \n  <p>Sample Output:</p>\n\n<pre><code>4\n11\n385\n</code></pre>\n</blockquote>\n\n<p>I wrote a DP solution but with O(N^5).</p>\n\n<blockquote>\n  <p>Psuedocode for ways function:</p>\n</blockquote>\n\n<pre><code>Memoize[Xmax][Ymax][Nmax];\n\nways(int X, int Y, int M) // (X,Y) current co-ordinates and M is number of moves to make\n{\n    if(Memoize[X][Y][M] != -1) // != -1 means we already have the result\n        return Memoize[X][Y][M];\n    sum=0;\n    for all (u,v) such that |X-u| + |Y-v| &lt;= S\n        sum += ways(u, v, M-1);\n\n    Memoize[X][Y][M] = sum;\n    return sum;\n\n}\n</code></pre>\n\n<blockquote>\n  <p>Code:</p>\n</blockquote>\n\n<pre><code>/* Enter your code here. Read input from STDIN. Print output to STDOUT */\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nlong ways(long ***DP, char **Chess, int X, int Y, int S, int N, int M);\n\nint main()\n{\n    int T;\n    scanf("%d", &amp;T);\n    while(T&gt;0)\n    {\n        int N, M, S;\n        scanf("%d", &amp;N);\n        scanf("%d", &amp;M);\n    scanf("%d", &amp;S);\n        long ***DP = (long ***) malloc(sizeof(long **) * N);\n        int i,j,k;\n        char **Chess = (char **)malloc(sizeof(char *) *N);\n        int Xstart;\n        int Ystart; //printf("N=%dM=%dS=%d\\n", N, M, S);\n        for(i=0;i&lt;N;i++)\n        {\n        Chess[i] = (char *)malloc(sizeof(char) *N);\n        DP[i] = (long **)malloc(sizeof(long *) * N);\n            for(j=0;j&lt;N;j++)\n            {   //printf("i=%dj=%d\\n", i, j);\n        DP[i][j] = (long *)malloc(sizeof(long) *(M+1));\n                char a;\n                scanf(" %c", &amp;a);\n                if(a==\'L\')\n                {\n                    Xstart = i;\n                    Ystart = j;\n                }\n                Chess[i][j] = a;\n        //printf("jf=%d\\n", j);\n        }\n        }\n\n        for(i=0;i&lt;N;i++)\n        {\n            for(j=0;j&lt;N;j++)\n            {\n                for(k=1;k&lt;M+1;k++)\n                {\n                    DP[i][j][k] = -1;\n                }\n                DP[i][j][0] = 1;\n            }\n        }\n\n       printf("%ld\\n", ways(DP, Chess, Xstart, Ystart, S, N, M));\n       T--;\n    }\n}\n\nlong ways(long ***DP, char **Chess, int X, int Y, int s, int N, int M)\n{\n    if(DP[X][Y][M] !=-1)\n    {\n    //printf("X=%d Y=%d M=%d Val=%ld\\n", X, Y, M, DP[X][Y][M]);\n    return DP[X][Y][M];\n    }\n    else\n    {\n        long sum1 = 0;\n\n\n        int S,k;\n    sum1 += ways(DP, Chess, X, Y, s, N, M-1);\n\n        for(S=1;S&lt;=s;S++)\n    {\n        for(k=0;k&lt;=S;k++)\n            {\n                if(k!=0 &amp;&amp; (S-k)!=0)\n                {\n                    if(X+k&lt;N &amp;&amp; Y+(S-k) &lt;N &amp;&amp; Chess[X+k][Y+(S-k)] != \'P\')\n                    {\n                        Chess[X+k][Y+(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X+k, Y+(S-k), s, N, M-1);\n                Chess[X+k][Y+(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(X+k&lt;N &amp;&amp; Y-(S-k)&gt;=0 &amp;&amp; Chess[X+k][Y-(S-k)] != \'P\')\n                    {\n                        Chess[X+k][Y-(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X+k, Y-(S-k), s, N, M-1);\n                Chess[X+k][Y-(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';  \n                    }\n                    if(X-k&gt;=0 &amp;&amp; Y+(S-k) &lt;N &amp;&amp; Chess[X-k][Y+(S-k)] != \'P\')\n                    {\n                        Chess[X-k][Y+(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X-k, Y+(S-k), s, N, M-1);\n                Chess[X-k][Y+(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(X-k&gt;=0 &amp;&amp; Y-(S-k) &gt;=0 &amp;&amp; Chess[X-k][Y-(S-k)] != \'P\')\n                    {\n                        Chess[X-k][Y-(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X-k, Y-(S-k), s, N, M-1);\n                            Chess[X-k][Y-(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n\n\n                }\n                else if(k==0 &amp;&amp; (S-k)!=0)\n                {\n                    if(Y+(S-k)&lt;N &amp;&amp; Chess[X][Y+(S-k)] != \'P\')\n                    {\n                        Chess[X][Y+(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X, Y+(S-k), s, N, M-1);\n                Chess[X][Y+(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(Y-(S-k)&gt;=0 &amp;&amp; Chess[X][Y-(S-k)] != \'P\')\n                    {\n                        Chess[X][Y-(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X, Y-(S-k), s, N, M-1);\n                Chess[X][Y-(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n\n\n                }\n                else if(k!=0 &amp;&amp; (S-k)==0)\n                {\n                    if(X+k&lt;N &amp;&amp; Chess[X+k][Y] != \'P\')\n                    {\n                        Chess[X+k][Y] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X+k, Y, s, N, M-1);\n                Chess[X+k][Y] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(X-k&gt;=0 &amp;&amp; Chess[X-k][Y] != \'P\')\n                    {\n                        Chess[X-k][Y] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X-k, Y, s, N, M-1);\n                Chess[X-k][Y] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n\n\n                }\n            }\n    }\n    //printf("X=%d Y=%d M=%d Val=%ld\\n", X, Y, M, sum);\n    DP[X][Y][M] = sum1;\n        return sum1;\n    }   \n}\n</code></pre>\n', 'ViewCount': '396', 'Title': 'How to do Fairy Chess problem in O(N^3)?', 'LastEditorUserId': '4272', 'LastActivityDate': '2014-02-11T06:33:58.330', 'LastEditDate': '2012-10-19T19:07:55.293', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4272', 'Tags': '<algorithms><data-structures><dynamic-programming>', 'CreationDate': '2012-10-19T16:29:04.340', 'FavoriteCount': '1', 'Id': '6170'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Inputs.</strong> I am given a finite set $S$ of symbols.  I know there should exist some total order $&lt;$ on $S$, but I\'m not given this ordering and it could be anything.</p>\n\n<p>I am also given a collection of assertions.  Each assertion takes the form $s_1&lt;s_2&lt;\\cdots&lt;s_m$, where $s_1,\\dots,s_m$ form a subset of the symbols of $S$.  The assertion probably won\'t mention all of the symbols of $S$, just a subset.  Each assertion will probably cover a different subset.</p>\n\n<p><strong>Warmup problem.</strong> The starter problem is: Given $n$ assertions, identify whether they are all internally self-consistent, i.e., whether there exists a total order on $S$ that is consistent with all of the assertions, and if so, output an example of such a total order.</p>\n\n<p><strong>The real problem.</strong> In practice, a few assertions might be faulty.  Almost all of them should be correct, though.  So, the real problem is: if the assertions are not all internally self-consistent, find a minimal subset of assertions to label as "probably-erroneous", such that if you remove the probably-erroneous assertions, the remainder are all self-consistent.</p>\n\n<p><strong>What I know.</strong> I know how to solve the warmup problem (just compute the transitive closure of the union of the partial orders given by each assertion, and check that the result is antisymmetric; or, in other words, create a graph with $S$ as vertex set and an edge $s\\to t$ if $s&lt;t$ appears in any assertion, then check for cycles).  However, I don\'t know how to solve the real problem.  Any ideas?</p>\n\n<p><strong>Real-world parameters.</strong> In the application domain where I\'ve run into this, $S$ might have up to a few hundred symbols, and I might have up to a few thousand assertions, with each assertion typically mentioning dozens of symbols.</p>\n', 'ViewCount': '112', 'Title': 'Given many partial orders, check them for consistency and report any that are not consistent', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-19T22:21:54.297', 'LastEditDate': '2012-10-19T18:36:26.050', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><graphs><approximation><finite-sets><partial-order>', 'CreationDate': '2012-10-19T17:25:00.620', 'FavoriteCount': '1', 'Id': '6173'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>From <a href="http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/lect12.pdf" rel="nofollow">Van Emde Boas trees lecture</a>:</p>\n\n<blockquote>\n  <p>We will use the idea of superimposing a tree of degree ${u^{1/2}}$ on top of\n  a bit vector, but <strong>shrink the universe size recursively</strong> by a square\n  root at each tree level. The ${u^{1/2}}$ items on the \ufb01rst level each hold\n  structures of ${u^{1/4}}$ items, which hold structures of ${u^{1/8}}$ items, and\n  so on, down to size 2.\n  I have a  question regarding van Emde Boas trees :</p>\n</blockquote>\n\n<ol>\n<li>How is the universe size getting reduced ? Aren\'t we just spreading the universe keys which is always constant at $u$ to different levels ? I can not understand the idea of "<strong>shriniking</strong>" the universe size .  I find similar language is used in defining the recursive structure for Van Emde Boas tree in <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/" rel="nofollow">Introduction to Algorithms</a> by CLRS also .</li>\n</ol>\n', 'ViewCount': '563', 'Title': 'Explanation of recursive structure of Van Emde Boas Tree', 'LastEditorUserId': '2223', 'LastActivityDate': '2013-01-18T15:27:02.663', 'LastEditDate': '2012-10-22T09:04:38.213', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6197', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><data-structures><algorithm-analysis><search-trees><trees>', 'CreationDate': '2012-10-20T14:15:05.167', 'Id': '6192'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In order to achieve the time complexity of $O(\\log \\log u)$ for van Emde Boas trees I read in <a href="http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/lect12.pdf" rel="nofollow">this lecture</a> that the the universe size  $u$  is chosen as $u = 2^{2^k}$ for some integer $k$ for van Emde Boas trees. Why choose $u$ to be of this specific form ?</p>\n', 'ViewCount': '116', 'Title': 'Size of the universe for van Emde Boas Trees', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-10-20T14:34:19.040', 'LastEditDate': '2012-10-20T14:34:19.040', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6195', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><data-structures><algorithm-analysis><binary-trees><trees>', 'CreationDate': '2012-10-20T14:22:39.857', 'Id': '6193'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>This is a homework problem for my introduction to algorithms course.</p>\n\n<blockquote>\n  <p>Recall the scheduling problem from Section 4.2 in which we sought to\n  minimize the maximum lateness. There are $n$ jobs, each with a deadline\n  $d_i$ and a required processing time $t_i$, and all jobs are available to be\n  scheduled starting at time $s$. For a job $i$ to be done, it needs to be assigned\n  a period from $s_i \\geq s$ to $f_i$ = $s_i + t_i$, and different jobs should be assigned\n  nonoverlapping intervals. As usual, such an assignment of times will be\n  called a schedule.</p>\n  \n  <p>In this problem, we consider the same setup, but want to optimize a\n  different objective. In particular, we consider the case in which each job\n  must either be done by its deadline or not at all. We\u2019ll say that a subset $J$ of\n  the jobs is schedulable if there is a schedule for the jobs in $J$ so that each\n  of them finishes by its deadline. Your problem is to select a schedulable\n  subset of maximum possible size and give a schedule for this subset that\n  allows each job to finish by its deadline.</p>\n  \n  <p>(a) Prove that there is an optimal solution $J$ (i.e., a schedulable set of\n  maximum size) in which the jobs in $J$ are scheduled in increasing\n  order of their deadlines.</p>\n  \n  <p>(b) Assume that all deadlines $d_i$ and required times $t_i$ are integers. Give\n  an algorithm to find an optimal solution. Your algorithm should\n  run in time polynomial in the number of jobs $n$, and the maximum\n  deadline $D = \\max_i d_i$.</p>\n</blockquote>\n\n<p>I've solved the problem as worded with the recurrence </p>\n\n<p>$Opt(i, d) = \\max\\left \\{ \n\\begin{array}\n \\\\ Opt(i-1, d-t_i) + 1 \\hspace{20 mm} d\\leq d_i\n \\\\ Opt(i-1, d) \n\\end{array}\n\\right \\}$</p>\n\n<p>but our instructor added a new requirement that our algorithm must not be dependent on D. This recurrence seems like it would produce an $O(nD)$ running time if implemented with dynamic programming.</p>\n\n<p>I can't figure out how to reduce its running time from $O(nD)$ to $O(n^k)$. To me it seems like it's a variation on the knapsack problem with all values equal to 1. In which case it seems like this is the best that can be done.</p>\n\n<p>If I'm doing something wrong could someone point me in the right direction, or if I've done everything right so far, could someone at least give me a hint as to how I can make an $O(n^k)$ recurrence or algorithm.</p>\n", 'ViewCount': '560', 'Title': 'Maximum Schedulable Set Zero-Lateness Deadline Scheduling', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-30T18:55:05.237', 'LastEditDate': '2012-10-31T09:59:21.227', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4294', 'Tags': '<algorithms><time-complexity><dynamic-programming><efficiency><scheduling>', 'CreationDate': '2012-10-20T23:13:27.230', 'FavoriteCount': '2', 'Id': '6202'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m a none-computer-science-student and get some knowledge on AI by taking the CS188.1x Course (Artificial Intelligence) on www.edx.org .</p>\n\n<p>Currently, I am working on the "Search in Pacman" Project; the sources can be found online at <a href="http://www-inst.eecs.berkeley.edu/~cs188/pacman/projects/search/search.html" rel="nofollow">Berkley CS188</a> . I have problems finding an good solution for "Finding All the Corners", so I need a good Multiple Goal Heuristic.</p>\n\n<p>I allready tried the simple approach described in <a href="http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html" rel="nofollow">here</a>. I used the minimum of all manhattan distances to all goals. This works, but is considered a rather poor heuistic, because my A-Star Algorithm expands 2606 nodes for the given maze. Using the same with euclidean distance expands even 103081 nodes. A good heuristic should expand 1600 nodes or less. A very good one 1200 nodes, an excellent one even 800 or less.</p>\n\n<p>I got a hint by other students who use minimum spanning trees created with Kruskal\'s Algorithm. I wanted to investigate into that direction, but I am somehow confused how the Kruskal Algorithm can be used to get a Heuristic?\nAs far as I understood, this Algorithm returns a minimum spanning tree (MST) which is a path, right? So it is a solution to the Traveling Salesman Problem (TSP); it returns a sequence of nodes. But I need a heuristic, so a cost function which can be applied to this problem and called by an Algorithm (like A*).</p>\n\n<p>Can anyone of you give me a hint on how to proceed? Every help is highly appreciated!</p>\n', 'ViewCount': '1068', 'Title': 'Heuristic for Finding Multiple Goals in Graph - e.g. using Kruskals Algorithm', 'LastActivityDate': '2013-01-15T21:48:58.973', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'OwnerDisplayName': 'EliteTUM', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><heuristics><search-problem>', 'CreationDate': '2012-10-09T13:24:05.047', 'Id': '6208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '313', 'Title': 'Assignment problem for multiple days', 'LastEditDate': '2012-10-21T09:32:46.587', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '4298', 'FavoriteCount': '2', 'Body': '<p>I have a problem that can be reduced to an assignment problem.\n(In a previous <a href="http://cstheory.stackexchange.com/questions/12850/matching-on-bipartite-graph-multiple-edges">question</a> i found out how to do that.)</p>\n\n<p>Which means we have a set $A$ of agents and a set $T$ of tasks as well as a cost function $c(i,j)$. We need to find an assignment so that the total cost is minimal.</p>\n\n<p>The <a href="http://en.wikipedia.org/wiki/Hungarian_algorithm">hungarian algorithm</a> can find an optimal solution in at least $O(n^4)$. Which sounds good to me.</p>\n\n<p>My new Problem is:\nThere is a given number of days. I have to solve the assignment problem for each day so that <strong>every task is done every day</strong> and <strong>no agent does the same task twice</strong>.</p>\n\n<p>What I have tried:\nWe could run the hungarian algorithm separately for each day and limit the number of possible combinations based on the result of the previous day. But this would get us into trouble at some of the later days, where most likely it will be impossible to find a feasibly solution.</p>\n\n<p>Another idea is to somehow integrate local search to change decisions made at a previous day. But I think we can\'t rely on this.</p>\n\n<p>The problem instances I have to face will be somewhere around $|A| = |T| = 500$. The cost matrix $C(i,j)$ will have lots of same values (E.g. mostly 1 or infinity, only some 2 or 3). So during the hungarian algorithm there is a lot of space to create different optimal solutions for a single day.</p>\n\n<p>I\'d be glad to hear some ideas or advises how to find a good solution for the problem.\nThanks in advance.</p>\n', 'Tags': '<algorithms><graph-theory><assignment-problem>', 'LastEditorUserId': '4298', 'LastActivityDate': '2012-10-21T16:52:13.963', 'CommentCount': '1', 'AcceptedAnswerId': '6216', 'CreationDate': '2012-10-21T08:44:43.650', 'Id': '6210'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '868', 'Title': 'How can I improve my Algorithm?', 'LastEditDate': '2012-10-23T13:43:40.853', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4272', 'FavoriteCount': '1', 'Body': '<blockquote>\n  <p>This is a problem from Interview Street in Dynamic Programming section.\n  <a href="https://www.interviewstreet.com/challenges/dashboard/#problem/4f2c2e3780aeb" rel="nofollow">https://www.interviewstreet.com/challenges/dashboard/#problem/4f2c2e3780aeb</a></p>\n  \n  <p>Billboards(20 points)</p>\n  \n  <p>ADZEN is a very popular advertising firm in your city. In every road you can see their advertising billboards. Recently they are facing a serious challenge , MG Road the most used and beautiful road in your city has been almost filled by the billboards and this is having a negative effect on the natural view.</p>\n  \n  <p>On people\'s demand ADZEN has decided to remove some of the billboards in such a way that there are no more than K billboards standing together in any part of the road.</p>\n  \n  <p>You may assume the MG Road to be a straight line with N billboards.Initially there is no gap between any two adjecent billboards.</p>\n  \n  <p>ADZEN\'s primary income comes from these billboards so the billboard removing process has to be done in such a way that the billboards remaining at end should give maximum possible profit among all possible final configurations.Total profit of a configuration is the sum of the profit values of all billboards present in that configuration.</p>\n  \n  <p>Given N,K and the profit value of each of the N billboards, output the maximum profit that can be obtained from the remaining billboards under the conditions given.</p>\n</blockquote>\n\n<pre><code>Constraints\n1 &lt;= N &lt;= 1,00,000(10^5)\n1 &lt;= K &lt;= N\n0 &lt;= profit value of any billboard &lt;= 2,000,000,000(2*10^9)\n</code></pre>\n\n<blockquote>\n  <p>My Solution (Psuedocode):</p>\n</blockquote>\n\n<pre><code>Let Profit[i] denote the Profit from ith billboard.\n(i, j) denotes the range of billboards\nMaxProfit(i, j) for all (i, j) such that i&lt;=j and i-j+1 &lt;= K is:\n    MaxProfit(i, j) = Profit[i] + Profit[i+1] + ... + Profit[j];\n\nFor other (i,j) MaxProfit equals,\n\nMaxProfit(i, j)\n{\n        if(MaxProfit(i, j) is already calculated)\n            then return its value;\n    max = 0;\n    for all k such that i&lt;=k&lt;=j // k denotes that, that position has no   billboard\n    {\n        temp = MaxProfit(i, k-1) + MaxProfit(k+1, j);\n        if(temp &gt; max)\n        max = temp;\n    }\nreturn max;\n}\n</code></pre>\n\n<p>My solution is of order $$N^2$$. So I get TLE and Segmentation fault for larger N. I have already passed 6/10 test cases. I need to pass remaining 4. Help needed.</p>\n', 'Tags': '<algorithms><data-structures><dynamic-programming>', 'LastEditorUserId': '4272', 'LastActivityDate': '2012-10-25T10:57:30.120', 'CommentCount': '10', 'AcceptedAnswerId': '6286', 'CreationDate': '2012-10-21T09:36:39.707', 'Id': '6211'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '172', 'Title': 'Classfication of randomized algorithms', 'LastEditDate': '2012-10-22T00:56:03.857', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '3', 'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Randomized_algorithm">Wikipedia</a> about randomized algorithms</p>\n\n<blockquote>\n  <p>One has to distinguish between <strong>algorithms</strong> that use the random\n  input to reduce the expected running time or memory usage, but always\n  terminate with a correct result in a bounded amount of time, and\n  <strong>probabilistic algorithms</strong>, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo\n  algorithms) or fail to produce a result (Las Vegas algorithms) either\n  by signalling a failure or failing to terminate.</p>\n</blockquote>\n\n<ol>\n<li>I was wondering how the first kind of "<strong>algorithms</strong> use the random\ninput to reduce the expected running time or memory usage, but\nalways  terminate with a correct result in a bounded amount of time?</li>\n<li>What differences are between it and Las Vegas algorithms which may\nfail to produce a result?</li>\n<li>If I understand correctly,  probabilistic algorithms and randomized algorithms are not the same concept. Probabilistic algorithms are just one\nkind of randomized algorithms, and the other kind is those use the\nrandom  input to reduce the expected running time or memory usage,\nbut always  terminate with a correct result in a bounded amount of\ntime?</li>\n</ol>\n', 'Tags': '<algorithms><terminology><randomized-algorithms><nondeterminism><machine-models>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-04-07T14:10:29.767', 'CommentCount': '0', 'AcceptedAnswerId': '6222', 'CreationDate': '2012-10-22T00:53:00.617', 'Id': '6221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1566', 'Title': 'What is tail recursion?', 'LastEditDate': '2013-05-24T03:17:30.907', 'AnswerCount': '4', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2223', 'FavoriteCount': '3', 'Body': '<p>I know the general concept of recursion.  I came across the concept of <strong>tail recursion</strong> while studying the quicksort algorithm.  In this <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-4-quicksort-randomized-algorithms/" rel="nofollow">video of quick sort algorithm from MIT</a> at 18:30 seconds the professor says that this is a tail recursive algorithm.  It is not clear to me what tail recursion really means.</p>\n\n<p>Can someone explain the concept with a proper example?</p>\n\n<p><em>Some answers provided by the SO community <a href="http://stackoverflow.com/questions/11864006/why-is-quick-sort-called-a-tail-recursive-algorithm">here</a>.</em></p>\n', 'Tags': '<algorithms><reference-request><recursion>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:17:30.907', 'CommentCount': '6', 'AcceptedAnswerId': '7814', 'CreationDate': '2012-10-22T08:58:53.803', 'Id': '6230'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '494', 'Title': 'Minimum space needed to sort a stream of integers', 'LastEditDate': '2012-10-22T19:53:01.833', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4315', 'FavoriteCount': '1', 'Body': '<p>This question has gotten a lot of attention on SO:<br>\n<a href="http://stackoverflow.com/questions/12748246/sorting-1-million-8-digit-numbers-in-1mb-of-ram">Sorting 1 million 8-digit numbers in 1MB of RAM</a></p>\n\n<p>The problem is to sort a stream of 1 million 8-digit numbers (integers in the range $[0,\\: 99\\mathord{,}999\\mathord{,}999]$) using only 1 MB of memory ($2^{20}$ bytes = $2^{23}$ bits) and no external storage. The program must read values from an input stream and write the sorted result to an output stream.</p>\n\n<p>Obviously the entire input can\'t fit into memory, but clearly the result can be represented in under 1 MB since $2^{23} \\geq \\log_2 \\binom{10^8}{10^6} \\approx 8079302$ (it\'s a tight fit).</p>\n\n<p>So, what is the minimum amount of space needed to sort n integers with duplicates in this streaming manner, and is there an algorithm to accomplish the specified task?</p>\n', 'Tags': '<algorithms><sorting><space-complexity><data-compression><streaming-algorithm>', 'LastEditorUserId': '4315', 'LastActivityDate': '2012-10-23T10:13:54.180', 'CommentCount': '5', 'AcceptedAnswerId': '6246', 'CreationDate': '2012-10-22T18:13:46.890', 'Id': '6236'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '335', 'Title': 'Do "inductively" and "recursively" have very similar meanings?', 'LastEditDate': '2012-10-23T12:39:08.900', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '1', 'Body': '<p>Do "inductively" and "recursively" mean very similar?</p>\n\n<p>For example, if there is an algorithm that determines a n-dim vector by determine its first k+1 components based on its first k components having been determined, and is initialized with the first component, would you call it works recursively or inductively? I have been using "recursively", but today someone said it "inductively".</p>\n', 'Tags': '<algorithms><terminology><recursion><induction>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-19T15:56:33.453', 'CommentCount': '7', 'AcceptedAnswerId': '6551', 'CreationDate': '2012-10-23T02:13:50.443', 'Id': '6247'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given $a,b,c,d \\in \\mathbb N$ and $b,d \\notin \\{0\\}$,</p>\n\n<p>$$\n\\begin{eqnarray*}\n\\frac a b &lt; \\frac c d &amp;\\iff&amp; ad &lt; cb\n\\end{eqnarray*}\n$$</p>\n\n<p>My questions are:</p>\n\n<p>Given $a,b,c,d$</p>\n\n<ol>\n<li>Assuming we can decide $x &lt; y \\in \\mathbb Z$ in $\\mathcal{O}(|x| +|y|)$, is there any way of deciding $ad&lt;cb$ without having to preform the multiplications (or divisions), $a\\cdot d$ and $c \\cdot b$. Or is there some sort of proof that there is no way.</li>\n<li>Is there a faster method to compare rational numbers than multiplying out the denominators.</li>\n</ol>\n', 'ViewCount': '455', 'Title': 'Comparing rational numbers', 'LastEditorUserId': '2755', 'LastActivityDate': '2012-11-25T14:45:37.623', 'LastEditDate': '2012-10-24T23:46:41.550', 'AnswerCount': '3', 'CommentCount': '14', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><integers>', 'CreationDate': '2012-10-23T17:48:28.160', 'FavoriteCount': '2', 'Id': '6266'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '157', 'Title': 'Are randomized algorithms constructive?', 'LastEditDate': '2012-10-25T11:05:48.563', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '1', 'Body': '<p>From , the proofs by the probabilistic method are often said to be non-constructive.</p>\n\n<p>However,  a proof by probabilistic method indeed designs a randomized algorithm and uses it for proving existence. Quoted from p103 of <a href="http://books.google.com/books?id=QKVY4mDivBEC&amp;pg=PR5&amp;lpg=PP1&amp;dq=randomized%20algorithms#v=onepage&amp;q&amp;f=false">Randomized Algorithms\n By Rajeev Motwani, Prabhakar Raghavan</a>:</p>\n\n<blockquote>\n  <p>We could view the proof  by the probabilistic method as a randomized\n  algorithm. This would then require a further analysis bounding the\n  probability that the algorithm fails to find a good partition on a\n  given execution. The main difference between a thought experiment in\n  the probabilistic method and a randomized algorithm is the end that\n  each yields. When we use the probabilistic method, we are only\n  concerned with showing that a combinatorial object exists; thus, we\n  are content with showing that a favorable event occurs with non-zero\n  probability. With a randomized algorithm, on the other hand,\n  efficiency is an important consideration - we cannot tolerate a\n  miniscule success probability.</p>\n</blockquote>\n\n<p>So I wonder if randomized algorithms are viewed as not constructive, although they do output a solution at the end of each run, which may or may not be an ideal solution.</p>\n\n<p>How is an algorithm or proof being "constructive" defined?</p>\n\n<p>Thanks!</p>\n', 'Tags': '<algorithms><terminology><randomized-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-30T16:24:48.083', 'CommentCount': '5', 'AcceptedAnswerId': '6289', 'CreationDate': '2012-10-24T12:19:26.950', 'Id': '6288'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the time complexity of computing $\\frac{1}{2^n} {{n}\\choose{(n+2)/2}}$?</p>\n\n<p>$$\\frac{1}{2^n} {{n}\\choose{(n+2)/2}} = \\frac{1}{2^n} \\frac{n(n-1)\\cdots ((n-2)/2)}{((n+2)/2) (n/2) \\cdots 1}$$</p>\n\n<p>The numerator and denominator in $\\frac{n(n-1)\\cdots ((n-2)/2)}{((n+2)/2) (n/2) \\cdots 1}$ will take $O(n)$ multiplications.</p>\n\n<p>$2^n$ will take $n$ multiplications.</p>\n\n<p>So in total, there will be $O(n)$ multiplications. Is it the correct time complexity?</p>\n', 'ViewCount': '124', 'Title': 'What is the time complexity of computing $\\frac{1}{2^n} {{n}\\choose{(n+2)/2}}$', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-25T11:00:25.887', 'LastEditDate': '2012-10-25T11:00:25.887', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><time-complexity><discrete-mathematics>', 'CreationDate': '2012-10-24T18:37:04.533', 'FavoriteCount': '0', 'Id': '6295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know that the disjoint set datastructure is used   to keep track of the connected components of an undirected graph when the edges are added to the graph dynamically . I also know that is is used in <a href="http://en.wikipedia.org/wiki/Kruskal%27s_algorithm" rel="nofollow">Kruskal\'s  algorithm for minimum spanning trees</a> . What are the other possible applications of this datastructure ?</p>\n', 'ViewCount': '487', 'Title': 'Practical applications of disjoint set datastructure', 'LastActivityDate': '2012-10-25T21:10:04.077', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6318', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><graph-theory><data-structures><graphs>', 'CreationDate': '2012-10-25T12:40:07.410', 'FavoriteCount': '3', 'Id': '6308'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm in a first year discrete math course and we started algorithms. I created a recursive algorithm to multiply two numbers together:</p>\n\n<pre><code>function multiply($n, $r) {\n    if($n == 1)\n return $r;\n    else if($r == 1)\n return $n;\n    else\n        return $r + multiply($n - 1, $r);\n}\n</code></pre>\n\n<p>How do I prove my algorithm is correct?</p>\n\n<p>A quick google search tells me I have to prove that it works for $n + 1$ and I have to prove that it terminates. Unfortunately I'm still incredibly new to this and haven't the faintest clue as to where to start for proving my algorithm correct, so I would really appreciate some help here. I think maybe I have to do some sort of proof by induction but as this is an algorithm, I wouldn't know where to start.</p>\n", 'ViewCount': '643', 'Title': 'Prove correctness of recursive multiplication algorithm', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-31T05:56:55.600', 'LastEditDate': '2013-08-31T05:56:55.600', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4359', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2012-10-25T15:37:15.703', 'Id': '6311'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '194', 'Title': 'Algorithmic consequences of algebraic formula for partition function?', 'LastEditDate': '2012-10-27T11:21:20.700', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '667', 'FavoriteCount': '2', 'Body': '<p><a href="http://www.aimath.org/news/partition/brunier-ono">Bruinier and Ono</a> have found an algebraic formula for the <a href="http://en.wikipedia.org/wiki/Partition_function_%28number_theory%29#Partition_function">partition function</a>, which was widely reported to be a breakthrough. I am unable to understand the paper, but does it have any algorithmic consequences for fast computation of the partition function?</p>\n', 'Tags': '<algorithms><complexity-theory><number-theory>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-27T15:47:28.097', 'CommentCount': '2', 'AcceptedAnswerId': '6338', 'CreationDate': '2012-10-26T11:33:57.743', 'Id': '6323'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the procedure for computing the rank of a <a href="http://mathworld.wolfram.com/Multiset.html" rel="nofollow">multiset</a> after inserting an element?</p>\n\n<p>For instance, lets say we have a set $S = (0,1)$ containing $n = 2$ distinct elements.</p>\n\n<p>The multiset $M = (1,1)$ has rank $5$ because there are $4$ multisets less than it based on lexicographic ordering: $(0), (1), (0,0), (0,1)$.</p>\n\n<p>If we insert $0$, we get $(0,1,1)$ which has rank $8$. If $1$ were inserted instead we\'d have $(1,1,1)$ with rank $9$.</p>\n\n<p>Is there a function $f(r,x,n)$ which takes a rank $r$, an element $x$, and $n$, and returns the new rank after inserting $x$?</p>\n', 'ViewCount': '93', 'Title': 'Computing the rank of a multiset after inserting another element', 'LastEditorUserId': '4223', 'LastActivityDate': '2012-10-27T15:58:34.620', 'LastEditDate': '2012-10-27T07:27:16.467', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '6339', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4223', 'Tags': '<algorithms><combinatorics><sets><binary-arithmetic>', 'CreationDate': '2012-10-27T01:53:27.787', 'Id': '6325'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '128', 'Title': 'What are some applications of binary finite fields in CS?', 'LastEditDate': '2012-10-27T22:33:00.597', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '1', 'Body': '<p>I was looking at details on <a href="http://en.wikipedia.org/wiki/Finite_field" rel="nofollow">finite fields</a>. Finite binary fields, e.g. $\\mathbb{F_2}$, are used in CS in some places such as circuit theory [1]. </p>\n\n<blockquote>\n  <p>What are some key applications of finite fields in CS?</p>\n</blockquote>\n\n<p>I am also looking for uses of $\\mathbb{F_{2}^n}$ which <a href="http://mathworld.wolfram.com/FiniteField.html" rel="nofollow">Mathworld</a> shows can be represented as binary vectors.</p>\n\n<hr>\n\n<p>[1] <a href="http://eccc.hpi-web.de/report/2012/133/download/" rel="nofollow">Noga Alon and Gil Cohen. On Rigid Matrices and Subspace Polynomials. Electronic Colloquium on Computational Complexity, Report No. 133 (2012)</a>.</p>\n', 'Tags': '<algorithms><reference-request><discrete-mathematics><applied-theory>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-27T22:33:00.597', 'CommentCount': '4', 'AcceptedAnswerId': '6331', 'CreationDate': '2012-10-27T02:40:18.207', 'Id': '6326'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>One can compress data with straight-line grammars. An algorithm that employs this technique is called <em>Sequitur</em>. If I understood correctly, Sequitur basically starts with one rule representing the input and then does these three steps till the grammar does not change anymore:</p>\n\n<ol>\n<li>For each rule, try to find any sequences of symbols in any other rule that match the rule's right hand side and replace these sequences by the rules left hand side.</li>\n<li>For each pair of adjacent symbols in any right hand side, find all non-overlapping other pairs of adjacent symbols that are equal to the original pair. If there are any other pairs, add a new nonterminal, replace all occurrences of these pairs by the new nonterminal and add a new rule that defines the nonterminal.</li>\n<li>For each nonterminal that appears exactly once on all right-hand sides of all rules, replace its occurrence by its definition, remove the nonterminal and the rule that defines it.</li>\n</ol>\n\n<p>For each (non-empty) input, can one guarantee that the above algorithm terminates?</p>\n", 'ViewCount': '251', 'Title': 'Will this algorithm terminate on any input?', 'LastEditorUserId': '2280', 'LastActivityDate': '2013-01-28T22:04:37.830', 'LastEditDate': '2012-10-29T06:14:04.147', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2280', 'Tags': '<algorithms><algorithm-analysis><formal-grammars><data-compression><correctness-proof>', 'CreationDate': '2012-10-28T21:15:53.433', 'FavoriteCount': '1', 'Id': '6360'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I got a n*m matrix updated in realtime (i.e. about every 10ms) with values between 0 and 1024, and I want to work out from that matrix a multitouch trackpad behaviour, which is:</p>\n\n<ul>\n<li>generate one or more points on the surface given the values on the matrix,</li>\n<li>make this or those point as big as the value can be.</li>\n</ul>\n\n<p>For example here is a few lines of a 9x9 matrix updates, and we can consider the following matrix as an example (with a touch in the middle):</p>\n\n<pre><code>[ [ 12,  7,12 ],\n  [ 12,129,19 ],\n  [ 12, 11,22 ] ]\n</code></pre>\n\n<p>The goal is to mimic the behaviour of a common touchpad (like on every smartphone, or laptop). So, I\'m getting values from a evenly distributed matrix of capacitive sensors on a physical object, which are processed by a microcontroller into a matrix, and I want to get coordinates and weight of one or several points.</p>\n\n<p>The idea would be to get something like <a href="https://www.youtube.com/watch?v=SiC-EfQ1fh4" rel="nofollow">this</a> (of course, I don\'t expect to have more than 2 or 3 detected points, and that level of precision with a matrix that small).</p>\n\n<p>Here are a few example raw logs:</p>\n\n<ul>\n<li><a href="http://m0g.net/~guyzmo/touch_diag.log" rel="nofollow">http://m0g.net/~guyzmo/touch_diag.log</a> </li>\n<li><a href="http://m0g.net/~guyzmo/touch_double.log" rel="nofollow">http://m0g.net/~guyzmo/touch_double.log</a></li>\n</ul>\n\n<p><strong>Edits</strong>:</p>\n\n<p>Thinking about my problematic made me consider this idea: I think I should make some kind of interpolation to augment the definition of the matrix, and in some way make the new values additive.</p>\n\n<p>i.e. imagine we have the following matrix :</p>\n\n<pre><code>[ [ 200, 200, 150 ],\n  [ 150, 150,  80 ],\n  [  80,  80,  40 ] ]\n</code></pre>\n\n<p>and we want to interpolate it somehow into something that would look like (I\'m inventing the values, but it\'s to expose the idea):</p>\n\n<pre><code>[ [ 200, 400, 200, 175, 150 ],\n  [ 175, 200, 175, 150, 125 ],\n  [ 150, 170, 150, 125,  80 ],\n  [ 100, 125, 100,  80,  60 ],\n  [  80,  80,  80,  60,  40 ] ]\n</code></pre>\n\n<p>I\'ve looked at interpolation algorithms, and it looks like the one we want that is the closer to our needs is the hermite interpolation. But though I have <a href="http://paulbourke.net/miscellaneous/interpolation/" rel="nofollow">RTFM</a> on interpolation methods, I don\'t know how I can apply it to a matrix.</p>\n', 'ViewCount': '73', 'Title': 'How to correlate a matrix of values to get a coordinated point?', 'LastEditorUserId': '4389', 'LastActivityDate': '2012-11-01T17:25:17.297', 'LastEditDate': '2012-11-01T17:24:10.297', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '6429', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4389', 'Tags': '<algorithms><matrices>', 'CreationDate': '2012-10-29T20:53:56.093', 'Id': '6374'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am working on problem (15-11) Inventory planning from <em>Introduction to Algorithms</em> (CLRS, 3rd Ed).</p>\n\n<blockquote>\n  <p><strong>15-11: Inventory Planning, p.411</strong></p>\n  \n  <p>The Rinky Dink Company makes machines that resurface ice rinks. The demand for such products varies from month to month, and so the company needs to develop a strategy to plan its manufacturing given the fluctuating, but predictable, demand. The company wishes to design a plan for the next $n$ months. For each month $i$, the company knows the demand $d_i$, that is, the number of machines that it will sell. Let $D  = \\displaystyle\\sum_{i=1}^{n} d_i $ be the total demand over the next $n$ months. The company keeps a full-time staff who provide labor to manufacture up to m machines in a given month, it can hire additional, part-time labor, at a cost that works out to c dollars per machine. Furthermore, if, at the end of a month, the company is holding any unsold machines, it must pay inventory costs. The cost for holding j machines is given as a function $h(j)$ for $u = 1, 2, ... , D$, where $h(j) \\ge 0$ for $1 \\le j \\le D$ and $h(j) \\le h(j+1)$ for $j \\le 1 \\le D - 1$.<br>\n  Give an algorithm that calculates a plan for the company that minimizes its costs while fulfilling all the demand. The running time should be polynomical in $n$ and $D$.</p>\n</blockquote>\n\n<p>In other words, problem asks to create dynamic programming algorithm that solves this problem.</p>\n\n<p>So far, I came up the the following solution, and not sure if it is any good.<br>\nOptimal sub-problem:\nLet $MinCost(i,j)$ be the function that returns minimized cost of operation for past $i$ months, $j$ is the number of unsold machines left at the end of the month $i$.(Goal, is to calculate $MinCost(n,0)$, in other words, at the end of planning period(month $n$), there are no unsold machines.)  </p>\n\n<p>So, the DP recurrence is given by $MinCost(0,0) = 0$ and</p>\n\n<p>$\\quad MinCost(i,j) = \\min\\{MinCost(i-1,j-k) + c(k,j,i) + h(j) \\mid 1 \\le k \\le D \\}$</p>\n\n<p>for $i+j &gt; 0$; here, $c(k,j,i)$ is the function that calculates the costs of the production.</p>\n\n<p>If my optimal sub-problem is correct, how do I create an algorithm to solve it?</p>\n', 'ViewCount': '1024', 'Title': 'Inventory planning problem solved through dynamic programming', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-30T14:49:19.330', 'LastEditDate': '2012-10-30T14:46:07.273', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1758', 'Tags': '<algorithms><dynamic-programming><check-my-algorithm>', 'CreationDate': '2012-10-30T04:11:00.550', 'Id': '6379'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/6404/runtime-of-the-binary-gcd-state-machine">Runtime of the binary-GCD state machine</a>  </p>\n</blockquote>\n\n\n\n<p><strong>Hello I am doing self study from MIT OCW exercises and I could not understand this question. Can anyone explain me,</strong></p>\n\n<ul>\n<li><p>First, why does this state machine assume it halves the $(max(a,b))$ at every two transitions beacuse we can follow steps 4 and 7 and one extra step to halve the $(max(a,b))$, which is more than 2 transitions/steps. I know it is not a common case but a case is a case unless proved.</p></li>\n<li><p>Next I dont see where an extra 3 comes from in $3+2log(max(a,b))$  </p></li>\n</ul>\n\n<p><strong>Question:</strong>\nThe binary-GCD state machine computes the GCD of a and b using only division\nby 2 and subtraction, which makes it run very efficiently on hardware that uses binary\nrepresentation of numbers. In practice, it runs more quickly than the Euclidean\nalgorithm state machine.</p>\n\n<p><strong>Prove that the machine reaches a final state in at most $3+2log(max(a,b))$\ntransitions.</strong></p>\n\n<p><strong>Hint:</strong> Strong induction on $max(a,b)$ </p>\n\n<p>states: $\\mathbb{N}^3$</p>\n\n<p>start state: $(a,b,1)$, where $a &gt;b&gt;0$</p>\n\n<p>transitions: if $min(x,y) &gt;0$, then $(x,y,e) \\rightarrow $</p>\n\n<p>the first possible state according to rules </p>\n\n<p><strong>1-</strong> $(1,0,ex)$ if $(x=y)$</p>\n\n<p><strong>2-</strong> $(1,0,e)$ if $(y=1)$</p>\n\n<p><strong>3-</strong> $(x/2,y/2,2e)$ if $(2|x \\ and \\ 2|y)$</p>\n\n<p><strong>4-</strong> $(y,x,e)$ if $(y&gt;x)$</p>\n\n<p><strong>5-</strong> $(x,y/2,e)$ if $(2|y)$</p>\n\n<p><strong>6-</strong> $(x/2,y,e)$ if $(2|x)$</p>\n\n<p><strong>7-</strong> $(x-y,y,e)$ if $(otherwise)$</p>\n', 'ViewCount': '27', 'ClosedDate': '2012-10-31T23:37:58.493', 'Title': 'The binary-GCD algorithm state machine', 'LastActivityDate': '2012-10-31T13:43:07.573', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4412', 'Tags': '<algorithms><proof-techniques>', 'CreationDate': '2012-10-31T13:43:07.573', 'FavoriteCount': '0', 'Id': '6402'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am doing self study from <a href="http://www.myoops.org/twocw/mit/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-042JFall-2005/3970D151-BFD3-4181-81CD-804F109B3AC2/0/ln7.pdf" rel="nofollow">MIT OCW exercises</a> and I could not understand this question.</p>\n\n<blockquote>\n  <p>The following rules define the <em>binary-GCD state machine</em> working on states in $\\mathbb{N}^3$ with start state $(a,b,1)$ for $a&gt;b&gt;0$. If multiple rules apply, smaller numbers have precedence.</p>\n  \n  <p>Provided $\\min(x,y) &gt;0$, then $(x,y,e) \\to $</p>\n  \n  <ol>\n  <li>$(1,0,ex)$ if $x=y$</li>\n  <li>$(1,0,e)$ if $y=1$</li>\n  <li>$(x/2,y/2,2e)$ if $2|x \\land 2|y$</li>\n  <li>$(y,x,e)$ if $y&gt;x$</li>\n  <li>$(x,y/2,e)$ if $2|y$</li>\n  <li>$(x/2,y,e)$ if $2|x$</li>\n  <li>$(x-y,y,e)$ otherwise</li>\n  </ol>\n</blockquote>\n\n<p>The binary-GCD state machine computes the GCD of $a$ and $b$ using only division by $2$ and subtraction, which makes it run very efficiently on hardware that uses binary representation of numbers. In practice, it runs more quickly than the Euclidean algorithm state machine.</p>\n\n<p>Each execution of a command (one of rules 1-7 according to algorithm) is a transition and the current state $(x,y,e)$ is stored in registers $A,B,E$. At first the values in the registers $a,b,1$</p>\n\n<p>Here is the question I am having trouble with:</p>\n\n<blockquote>\n  <p>Prove that the machine reaches a final state in at most $3+2\\log(\\max(a,b))$\n  transitions.</p>\n  \n  <p><strong>Hint:</strong> Strong induction on $\\max(a,b)$ </p>\n</blockquote>\n\n<ul>\n<li><p>First, why does this state machine assume it halves the $\\max(a,b)$ at every two transitions beacuse we can apply rule 4 and 7 and one extra rule to halve the $\\max(a,b)$, which is more than two transitions/steps. I know it is not a common case but a case is a case unless proved.</p></li>\n<li><p>Next I don\'t see where an extra 3 comes from in $3+2\\log(\\max(a,b))$.  </p></li>\n</ul>\n', 'ViewCount': '129', 'Title': 'Runtime of the binary-GCD state machine', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T04:01:55.783', 'LastEditDate': '2012-10-31T23:35:29.357', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4413', 'Tags': '<algorithms><finite-automata><proof-techniques>', 'CreationDate': '2012-10-31T17:37:32.330', 'FavoriteCount': '0', 'Id': '6404'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '985', 'Title': "Base of logarithm in runtime of Prim's and Kruskal's algorithms", 'LastEditDate': '2012-11-01T22:48:50.390', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'CodeKingPlusPlus', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Body': "<p>For Prim's and Kruskal's Algorithm there are many implementations which will give different running times. However suppose our implementation of Prim's algorithm has runtime $O(|E| + |V|\\cdot \\log(|V|))$ and Kruskals's algorithm has runtime $O(|E|\\cdot \\log(|V|))$.</p>\n\n<p>What is the base of the $\\log$?</p>\n", 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T22:48:50.390', 'CommentCount': '3', 'AcceptedAnswerId': '6436', 'CreationDate': '2012-10-27T20:48:47.810', 'Id': '6435'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am currently working on a project where I\'m using an implementation of Hoffman and Pavley\'s "<a href="http://dl.acm.org/citation.cfm?doid=320998.321004" rel="nofollow">Method for the Solution of the Nth Best Path Problem</a>" to find n-th best path through a directed graph. The implementation is based on <a href="http://quickgraph.codeplex.com/wikipage?title=Ranked%20Shortest%20Path" rel="nofollow">QuickGraph\'s Ranked Shortest Path implementation</a>.</p>\n\n<p>I have been trying to determine the complexity of Hoffman and Pavley\'s algorithm as well as QuickGraph\'s implementation, but without any luck -- so basically my question is if someone knows the complexity of the original method proposed by Hoffman and Pavley as well as the complexity of QuickGraph\'s implementation?</p>\n', 'ViewCount': '175', 'Title': "What is the complexity of Hoffman and Pavley's Nth best path algorithm?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-02T10:09:43.120', 'LastEditDate': '2012-11-02T10:09:43.120', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4433', 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><shortest-path>', 'CreationDate': '2012-11-02T08:43:34.007', 'FavoriteCount': '1', 'Id': '6444'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '199', 'Title': 'Modification of Hamilton Path', 'LastEditDate': '2012-11-03T04:47:29.347', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4436', 'FavoriteCount': '1', 'Body': '<p>Although I know that the <a href="http://en.wikipedia.org/wiki/Hamiltonian_path_problem" rel="nofollow">Hamilton Path problem</a> is ${\\sf NP}$-complete, I think the following variant can be solved in polynomial time:</p>\n\n<blockquote>\n  <p>Given a planar graph with vertex set $V$, edge set $E$, start node $S$ and target node $F$,\n  our task is to find the Hamiltonian path from $S$ to $F$ or write that the path doesn\'t exist.</p>\n  \n  <p><em>Last condition</em>: In the path, in addition to selecting the directly connected vertices, \n  we can also choose those connected to exactly one neighbor.</p>\n  \n  <p><strong>Edit</strong>: The degree of any vertex is at most four ($\\deg(v_i) \\le 4$).</p>\n</blockquote>\n\n<p>Does anyone have any ideas how to prove that this can be solved in polynomial time? </p>\n\n<p>It can be hard to understand, so I will give an example:  </p>\n\n<p><img src="http://i.stack.imgur.com/meTSp.png" alt="Examples"></p>\n\n<p>In the left example, for $S=1,F=12$, the solution is the path $1, 11, 8, 7, 5, 9, 2, 10, 4, 6, 3, 12$.  </p>\n\n<p>In the right example, for $S=1,F=15$, there is no Hamiltonian path.</p>\n', 'Tags': '<algorithms><complexity-theory><graph-theory><np-hard>', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-03T16:06:59.817', 'CommentCount': '9', 'AcceptedAnswerId': '6448', 'CreationDate': '2012-11-02T10:23:44.050', 'Id': '6446'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We are given a graph with $n$ vertices, $m$ edges, and path edge costs of $x$. For vertices without a direct path that are distant exactly one neighbor, we can add new edge with edge cost $y$. Our task is to find shortest path (i.e minimum cost) between the start vertex and all others vertices.</p>\n\n<p>I have developed an algorithm, but I would like to create something faster than adding edges to the graph (via breadth-first search) and <a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a>. Here are a couple examples:</p>\n\n<p><em>Example 1 Input:</em> For $x=3$, $y=1$</p>\n\n<p><img src="http://i.stack.imgur.com/RrBPA.png" alt="Example 1 Graph"></p>\n\n<p><em>Possible $y$ paths included:</em></p>\n\n<p><img src="http://i.stack.imgur.com/1lgvX.png" alt="Example 1 Graph with y"></p>\n\n<p><em>Output</em>: cost of shortest path from start node to node $i$ (assume that from start node to start node is 0)</p>\n\n<blockquote>\n  <p>1: 0<br>\n  2: 2<br>\n  3: 2<br>\n  4: 1<br>\n  5: 1<br>\n  6: 3  </p>\n</blockquote>\n\n<p><em>Example 2 Input</em>: For $x=3, y=2$</p>\n\n<p><img src="http://i.stack.imgur.com/Bsm7F.png" alt="Example 2 Graph"> </p>\n\n<p><em>Output</em>:  </p>\n\n<blockquote>\n  <p>1: 0<br>\n  2: 3<br>\n  3: 3<br>\n  4: 2<br>\n  5: 5  </p>\n</blockquote>\n', 'ViewCount': '264', 'Title': 'Shortest path in graph - upgrade an algorithm', 'LastEditorUserId': '4304', 'LastActivityDate': '2013-01-08T15:48:50.643', 'LastEditDate': '2012-11-03T04:47:38.037', 'AnswerCount': '0', 'CommentCount': '12', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4440', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2012-11-02T16:25:24.033', 'Id': '6453'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Assume that we have a set $D$ and each member of $D$ is a data and key pair. We want a data structure that would support the following operations:</p>\n\n<ul>\n<li>Insert $(d,k)$ into $D$,</li>\n<li>Delete member $e$, (no need to search to find $e$, e.g. $e$ points to a member in $D$),</li>\n<li>MostFrequent, which returns a member $e \\in D$ such that $e.key$ is one of the most frequent keys in $D$ (note that the most frequent key doesn't need to be unique).</li>\n</ul>\n\n<p>What would be an efficient implementation of this data structure?</p>\n\n<p>My solution is a heap for the keys and their frequencies prioritized by the frequencies plus a hash table where the hash function maps members with the same key to the same slot in the hash table (with pointers from each part to the other). </p>\n\n<p>This can give $\\Theta(\\lg n)$ for the first two operations and $\\Theta(1)$ for the third (worst case running time). </p>\n\n<p>I am wondering if there is more efficient solution? (or a simpler solution with the same efficiency?)</p>\n", 'ViewCount': '870', 'Title': 'An efficient data structure supporting Insert, Delete, and MostFrequent', 'LastEditorUserId': '41', 'LastActivityDate': '2013-01-23T16:20:22.333', 'LastEditDate': '2012-11-02T21:39:10.150', 'AnswerCount': '3', 'CommentCount': '10', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<algorithms><data-structures><abstract-data-types>', 'CreationDate': '2012-11-02T17:00:40.447', 'FavoriteCount': '5', 'Id': '6455'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '600', 'Title': 'Iterative binary search analysis', 'LastEditDate': '2012-11-04T15:41:46.187', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Hasan Tahsin', 'PostTypeId': '1', 'OwnerUserId': '4610', 'Body': '<p>I\'m a little bit confused about the analysis of <a href="http://en.wikipedia.org/wiki/Binary_search" rel="nofollow">binary search</a>.\nIn almost every paper, the writer assumes that the array size $n$ is always $2^k$.\nWell I truly understand that the time complexity becomes $\\log(n)$ (worst case) under this assumption. But what if $n \\neq 2^k$?</p>\n\n<p>For example if $n=24$, then we have\n5 iterations for 24<br>\n4 i. for 12<br>\n3 i. for 6<br>\n2 i. for 3<br>\n1 i. for 1</p>\n\n<p>So how do we get the result $k=\\log n$ in this example (I mean of course every similar example whereby $n\\neq2^k$)?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-04T15:41:46.187', 'CommentCount': '4', 'AcceptedAnswerId': '6471', 'CreationDate': '2012-11-03T10:15:40.587', 'Id': '6470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to understand summation for amortization analysis of a hash-table from a <a href="http://videolectures.net/mit6046jf05_leiserson_lec13/" rel="nofollow">MIT lecture video</a> (at time 16:09). </p>\n\n<p>Although you guys don\'t have to go and look at the video, I feel that the summation he does is wrong so I will attach the screenshot of the slide.</p>\n\n<p><img src="http://i.stack.imgur.com/EBRfs.jpg" alt="MIT Lecture Slide"></p>\n', 'ViewCount': '138', 'Title': 'Why is $\\sum_{j=0}^{\\lfloor\\log (n-1)\\rfloor}2^j$ in $\\Theta (n)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-05T17:15:47.033', 'LastEditDate': '2012-11-05T17:15:47.033', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '6474', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<algorithms><data-structures><algorithm-analysis><mathematical-analysis><discrete-mathematics>', 'CreationDate': '2012-11-04T16:32:11.537', 'Id': '6473'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I ran into the following problem:</p>\n\n<p>Given a directed acyclic graph with real-valued edge weights, and two vertices s and t, compute the minimum s-t cut.</p>\n\n<p>For general graphs this is NP-hard, since one can trivially reduce max-cut to it by simply reversing the edge weights (correct me if I'm wrong).</p>\n\n<p>What is the situation with DAGs? Can min-cut (or max-cut) be solved in polynomial time? Is it NP-hard and, if so, are there any known approximation algorithms?</p>\n\n<p>I tried to find work on this but wasn't able to (maybe I'm just using wrong keywords in my searches), so I was hoping somebody may know (or find) something about this.</p>\n", 'ViewCount': '521', 'Title': 'Minimum s-t cut in weighted directed acyclic graphs with possibly negative weights', 'LastActivityDate': '2012-11-05T21:05:51.660', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '6498', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '691', 'Tags': '<algorithms><complexity-theory><graph-theory><weighted-graphs>', 'CreationDate': '2012-11-04T18:19:49.463', 'Id': '6476'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://academia.stackexchange.com/questions/5079/are-there-hacks-or-smaller-scientific-search-engines-offering-you-context-sensit">I was asking on academia.se</a>, if anyone knows scientific search engines offering a <a href="http://en.wikipedia.org/wiki/Proximity_search_%28text%29" rel="nofollow">proximity operator</a> like Google Web Search does, while Google Scholar Search does not. That\'s sad, because this operator would be most useful for literature research offering you a nearly semantic/context-sensitive search and I\'ve seen requests for this feature in many blogs.</p>\n\n<p>The answer on my question linked above shows that 2 search engines offer something similar, but those operators also only work on titles and abstracts of papers, if I understand correctly. </p>\n\n<p>The wikipedia article doesn\'t explain what exactly limits the implementation of this kind of operator (exponentially rising indexing time, index size,... I\'m no search algorithm expert), but when Google Web Search offers it (the amount of web-text is much bigger), what possibly hinders the scientific search engines from offering it for full article text (cost-benefit ratio? I doubt this, as 99,9% of Google Web Search user don\'t know the AROUND(X) operator and the majority doesn\'t use <a href="http://www.googleguide.com/advanced_operators_reference.html" rel="nofollow">many operators</a> at all)?</p>\n\n<p>PS: If this question better fits SO, move it there, but I\'m more looking for a general explanation, what parameters determine and limit the implementation of an proximity operator.</p>\n', 'ViewCount': '96', 'Title': 'What limits the implementation of proximity operators for text indexing and searching?', 'LastEditorUserId': '298', 'LastActivityDate': '2012-11-10T15:30:49.783', 'LastEditDate': '2012-11-10T15:30:49.783', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '298', 'Tags': '<search-algorithms><data-mining><searching>', 'CreationDate': '2012-11-04T19:35:42.203', 'Id': '6477'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For a list of integers, of size n, where n is exponential, will merge-sort(n), run in poly-time or psuedo poly-time?</p>\n', 'ViewCount': '146', 'Title': 'Exponential input and poly-time algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-27T23:40:33.147', 'LastEditDate': '2012-11-05T08:06:37.387', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4365', 'Tags': '<algorithms><terminology><time-complexity><polynomial-time>', 'CreationDate': '2012-11-04T20:13:32.497', 'FavoriteCount': '1', 'Id': '6478'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have rank-deficient matrix $M \\in \\mathbb{R}^{n\\times m}$ with $\\text{rank}(M) = k$ and I want to find a <a href="http://en.wikipedia.org/wiki/Rank_factorization" rel="nofollow">rank factorization</a> $M = PQ$ with $P \\in \\mathbb{R}^{n \\times k}$ and $Q \\in \\mathbb{R}^{k \\times m}$. </p>\n\n<p>A popular approach is to compute the singular value decomposition (SVD) $M = UDV^*$ and keep the columns of $U$ and rows of $V$ corresponding to the non-zero singular values. This is a great approach, especially since it behaves nicely under noise. However, SVD seems to compute more than I need for just rank factorization (and the noise tolerance is cool, but not necessary). </p>\n\n<p><strong>What are the other approaches I can use?</strong> In particular, I am interested in algorithms that have <em>one</em> (or more) of the following properties:</p>\n\n<ol>\n<li>Outperform SVD asymptotically.</li>\n<li>Outperform SVD in practice, or on special inputs (for a reasonably interesting class of special inputs).</li>\n<li>Performance under small perturbation of $M$ is well understood.</li>\n</ol>\n\n<p>I am fine with giving $k$ to the algorithm ahead of time. Note that SVD does not require this (unless we are doing a perturbation analysis, but even then we usually give a bound on perturbation size and determine $k$ at run-time based on that).</p>\n', 'ViewCount': '202', 'Title': 'Alternatives to SVD for rank factorization', 'LastEditorUserId': '55', 'LastActivityDate': '2013-01-06T21:42:05.483', 'LastEditDate': '2013-01-06T21:42:05.483', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><machine-learning><numerical-analysis><linear-algebra>', 'CreationDate': '2012-11-05T20:45:27.737', 'Id': '6497'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am not sure how to draw parallel between the Wagner\u2013Fischer algorithm and dtw algo.\nIn both case we want to find the distance of each index combination (i,j).</p>\n\n<p>In Wagner\u2013Fischer, we initiate the distance by the number of insert we\'d have to do from one empty string to another.</p>\n\n<pre><code>let wagnerFischer (s: string) (t: string) =\n   let m, n = s.Length, t.Length\n   let d = Array2D.create (m + 1) (n + 1) 0\n\n   for i = 0 to m do d.[i, 0] &lt;- i\n   for j = 0 to n do d.[0, j] &lt;- j    \n\n   for j = 1 to n do\n       for i = 1 to m do\n          d.[i, j] &lt;- List.min [\n                           d.[i-1, j  ] + 1; \n                           d.[i  , j-1] + 1; \n                           d.[i-1, j-1] + if s.[i-1] = t.[j-1] then 0 else 1; ]\n   printfn "edit matrix \\n %A" d \n   d.[m,n]\n</code></pre>\n\n<p>in the DWT we initiate the boundary at +infinity because we dont want to \'skip\' any numbers of the sequence, we always want to match with another item.</p>\n\n<p>What I dont see is what changes between the DWT and the WF algo that prevent use to update the distance in homogeneous way. \nIn DWT we systematically add the cost, whereas in the WF algo, we have this non homegenous function wrt different cases </p>\n\n<p>I understand both algo, but dont make the connexion between those differences in the cost function update ..\nAny idea to understand the difference intuitively  ?</p>\n\n<pre><code>let sequencebacktrack (s: \'a seq) (t:\'a seq) (cost:\'a-&gt;\'a-&gt;double) (boundary:int-&gt;double)  =\n   let m, n = s |&gt; Seq.length, t |&gt; Seq.length\n   let d = Array2D.create (m + 1) (n + 1) 0.\n\n   for i = 0 to m do d.[i, 0] &lt;- boundary(i)\n   for j = 0 to n do d.[0, j] &lt;- boundary(j)\n\n   t |&gt; Seq.iteri( fun j tj -&gt;\n            s |&gt; Seq.iteri( fun i si -&gt; \n                        d.[1+i, 1+j] &lt;- cost tj si + List.min [d.[1+i-1, 1+j  ]; \n                                                               d.[1+i  , 1+j-1]; \n                                                               d.[1+i-1, 1+j-1]; ] ))\n   printfn "edit matrix \\n %A" d \n   d.[m,n]\n//does not work\nlet wagnerFischer2 (s: string) (t: string) =\n   sequencebacktrack s t (fun a b -&gt; if a = b then 0. else 1.) (id &gt;&gt; double)\n\nlet b = wagnerFischer2 "ll" "la"\n</code></pre>\n', 'ViewCount': '244', 'Title': 'Levenstein distance and dynamic time warp', 'LastActivityDate': '2012-11-06T12:43:09.843', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4469', 'Tags': '<algorithms><dynamic-programming><edit-distance>', 'CreationDate': '2012-11-05T22:39:54.580', 'Id': '6502'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '121', 'Title': 'BPP search: what does boosting correctness entail?', 'LastEditDate': '2012-11-07T10:22:52.953', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4488', 'FavoriteCount': '2', 'Body': u'<p>It is not really clear to me, how and if I can do boosting for correctness (or error reduction) on a <a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29" rel="nofollow">BPP</a> (bounded-error probabilistic polynomial-time) search problem. Can anyone of you explain me how it works?</p>\n\n<p>With BPP search, I mean a problem that can have false positive-negative, correct solution, and no-solution. Here\'s a definition:</p>\n\n<p>A probabilistic polynomial-time algorithm $A$ solves the search problem of the relation $R$ if</p>\n\n<ul>\n<li>for every $x \u2208 S$, $Pr[A(x) \u2208 R(x)] &gt; 1 - \u03bc(|x|)$</li>\n<li>for every $x \u2209 SR$, $Pr[A(x) = \\text{no-solution}] &gt; 1 - \u03bc(|x|)$</li>\n</ul>\n\n<p>were $R(x)$ is the set of solution for the problem and $\u03bc(|x|)$ is a negligible function (it is rare that it fails).</p>\n\n<p>So now I would like to increase my probability of getting a good answer, how can I do it?</p>\n\n<hr>\n\n<p>~ ".. boosting for correctness.." : a way to increase the probability of the algorithm (generally by multile runs of the probabilistic algorithm), i.e., when the problem have a solution then the algorithm likely return a valid one.</p>\n', 'Tags': '<probabilistic-algorithms><search-problem>', 'LastEditorUserId': '4221', 'LastActivityDate': '2012-11-08T06:49:27.833', 'CommentCount': '3', 'AcceptedAnswerId': '6537', 'CreationDate': '2012-11-06T08:05:34.063', 'Id': '6503'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We have a stochastic random source. This gives the bit $0$ (or $1$) with probability $1/2$. \nWe want to generate a uniform distribution on the set S = $\\{0, 1,..., n-1\\}$. </p>\n\n<p>Which algorithm gives with probability $1/n$ the value $i\\in S$. And how many bits are needed.</p>\n', 'ViewCount': '47', 'Title': 'Stochastical algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-06T10:58:11.537', 'LastEditDate': '2012-11-06T10:35:24.340', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><random><sampling>', 'CreationDate': '2012-11-06T09:57:05.557', 'Id': '6507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Looking for some tutorials / references that discuss Breadth First Search that takes into consideration the cost of paths, but could not find much information.</p>\n\n<p>Could someone refer a tutorial?</p>\n', 'ViewCount': '292', 'Title': 'Breadth First Search with cost', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-07T08:02:23.767', 'LastEditDate': '2012-11-06T14:28:43.053', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '6523', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4476', 'Tags': '<algorithms><reference-request><graphs><search-algorithms>', 'CreationDate': '2012-11-06T14:22:03.630', 'Id': '6514'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '439', 'Title': 'How asymptotically bad is naive shuffling?', 'LastEditDate': '2012-11-06T20:49:40.330', 'AnswerCount': '2', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '242', 'FavoriteCount': '3', 'Body': "<p>It's well-known that this 'naive' algorithm for shuffling an array by swapping each item with another randomly-chosen one doesn't work correctly:</p>\n\n<pre><code>for (i=0..n-1)\n  swap(A[i], A[random(n)]);\n</code></pre>\n\n<p>Specifically, since at each of $n$ iterations, one of $n$ choices is made (with uniform probability), there are $n^n$ possible 'paths' through the computation; because the number of possible permutations $n!$ doesn't divide evenly into the number of paths $n^n$, it's impossible for this algorithm to produce each of the $n!$ permutations with equal probability.  (Instead, one should use the so-called <em>Fischer-Yates</em> shuffle, which essentially changes out the call to choose a random number from [0..n) with a call to choose a random number from [i..n); that's moot to my question, though.)</p>\n\n<p>What I'm wondering is, how 'bad' can the naive shuffle be?  More specifically, letting $P(n)$ be the set of all permutations and $C(\\rho)$ be the number of paths through the naive algorithm that produce the resulting permutation $\\rho\\in P(n)$, what is the asymptotic behavior of the functions </p>\n\n<p>$\\qquad \\displaystyle M(n) = \\frac{n!}{n^n}\\max_{\\rho\\in P(n)} C(\\rho)$ </p>\n\n<p>and </p>\n\n<p>$\\qquad \\displaystyle m(n) = \\frac{n!}{n^n}\\min_{\\rho\\in P(n)} C(\\rho)$?  </p>\n\n<p>The leading factor is to 'normalize' these values: if the naive shuffle is 'asymptotically good' then </p>\n\n<p>$\\qquad \\displaystyle \\lim_{n\\to\\infty}M(n) = \\lim_{n\\to\\infty}m(n) = 1$.  </p>\n\n<p>I suspect (based on some computer simulations I've seen) that the actual values are bounded away from 1, but is it even known if $\\lim M(n)$ is finite, or if $\\lim m(n)$ is bounded away from 0?  What's known about the behavior of these quantities?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics><probability-theory><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-10T04:07:42.843', 'CommentCount': '13', 'AcceptedAnswerId': '6596', 'CreationDate': '2012-11-06T19:22:56.410', 'Id': '6519'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a group of n sets for which I need to calculate a sort of "uniqueness" or "similarity" value.  I\'ve settled on the <a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a> as a suitable metric.  Unfortunately, the Jaccard index only operates on two sets at a time.  In order to calculate the similarity between all $n$ sets, it will require in the order of $n^2$ Jaccard calculations.</p>\n\n<p>(If it helps, $n$ is usually between 10 and 10000, and each set contains on average 500 elements.  Also, in the end, I don\'t care how similar any two specific sets are - rather, I only care what the internal similarity of the whole group of sets is. (In other words, the mean (or at least a sufficiently accurate approximation of the mean) of all Jaccard indexes in the group))</p>\n\n<p>Two questions:</p>\n\n<ol>\n<li>Is there a way to still use the Jaccard index without the $n^2$ complexity?</li>\n<li>Is there a better way to calculate set similarity/uniqueness across a group of sets than the way I\'ve suggested above?</li>\n</ol>\n', 'ViewCount': '477', 'Title': 'Set Similarity - Calculate Jaccard index without quadratic complexity', 'LastActivityDate': '2013-01-24T12:02:54.673', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '7', 'OwnerDisplayName': 'rinogo', 'PostTypeId': '1', 'OwnerUserId': '4498', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2012-11-06T16:12:54.127', 'Id': '6526'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Most of the classic examples of dynamic programming algorithms have run-times such as $n$ or $n^2$. Are there any natural examples with a $O(n \\log n)$ run-time?</p>\n', 'ViewCount': '301', 'Title': 'Dynamic programming algorithms with log in the run-time', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-09T08:03:51.553', 'LastEditDate': '2012-11-09T08:03:51.553', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<algorithms><reference-request><dynamic-programming>', 'CreationDate': '2012-11-07T22:35:41.207', 'FavoriteCount': '1', 'Id': '6546'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a set of coins with different denominations $c1, ... , cn$ and a value v you want to find the least number of coins needed to represent the value v.</p>\n\n<p>E.g. for the coinset 1,5,10,20 this gives 2 coins for the sum 6 and 6 coins for the sum 19. </p>\n\n<p>My main question is: when can a greedy strategy be used to solve this problem?</p>\n\n<hr>\n\n<p>Bonus points: Is this statement plain incorrect? (From: <a href="http://stackoverflow.com/questions/6025076/how-to-tell-if-greedy-algorithm-suffices-for-the-minimum-coin-change-problem/6031625#6031625">How to tell if greedy algorithm suffices for the minimum coin change problem?</a>)</p>\n\n<blockquote>\n  <p>However, this paper has a proof that if the greedy algorithm works for the first largest denom + second largest denom values, then it works for them all, and it suggests just using the greedy algorithm vs the optimal DP algorithm to check it.\n  <a href="http://www.cs.cornell.edu/~kozen/papers/change.pdf">http://www.cs.cornell.edu/~kozen/papers/change.pdf</a></p>\n</blockquote>\n\n<p>Ps. note that the answers in that thread are incredibly crummy- that is why I asked the question anew.</p>\n', 'ViewCount': '3360', 'Title': 'When can a greedy algorithm solve the coin change problem?', 'LastActivityDate': '2012-11-12T03:16:46.283', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '6625', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><combinatorics><greedy-algorithms>', 'CreationDate': '2012-11-08T08:59:30.133', 'FavoriteCount': '0', 'Id': '6552'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<pre><code>Insertion-Sort (A) [where A is an array of numbers to be sorted]\n\n(1) for j = 2 to A.length\n(2)       key = A[j]\n(3)       i = j -1\n(4)       while i &gt; 0 and A[i] &gt; key\n(5)              A[i+1] = A[i]\n(6)              i = i - 1\n(7)       A[i + 1] = key\n</code></pre>\n\n<p>CLRS proves the correction of the above algorithm by using a loop invariant:</p>\n\n<blockquote>\n  <p><strong>Loop Invariant:</strong> At the start of each iteration of the for loop of lines 1\u20138, the subarray A[1... j - 1] consists of the elements originally in A[1... j - 1]  but in sorted order.</p>\n  \n  <p>We use loop invariants to help us understand why an algorithm is correct. We must show three things about a loop invariant:</p>\n  \n  <p>Initialization: It is true prior to the first iteration of the loop.</p>\n  \n  <p>Maintenance: If it is true before an iteration of the loop, it remains true before the</p>\n  \n  <p>Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.</p>\n</blockquote>\n\n<p>In the explanation of the maintenance aspect of the loop invariant, the following is mentioned:</p>\n\n<blockquote>\n  <p>Maintenance: A more formal treatment of the this property would require us to state and show a loop invariant for the while loop of lines 5\u20137. At this point, however, we prefer not to get bogged down in such formalism, and so we rely on our informal analysis to show that the second property holds for the outer loop.</p>\n</blockquote>\n\n<p>Why would a "formal treatment" require a loop invariant for the while loop? Having one invariant for the outer for loop is sufficient to prove the correctness of the algorithm- why would a "formal treatment" require a loop invariant?</p>\n', 'ViewCount': '153', 'Title': 'Loop invariants?', 'LastActivityDate': '2012-11-09T07:23:45.327', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4519', 'Tags': '<algorithms><algorithm-analysis><correctness-proof>', 'CreationDate': '2012-11-09T03:03:51.197', 'Id': '6567'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've got two log-space programs $F$ and $G$.</p>\n\n<ul>\n<li>Program $F$ will get input in array $A[1..n]$ and will create the output array $B[1..n]$.</li>\n<li>Program $G$ will get as input $B$ as created by $F$ and create from it the output array $C[1..n]$.</li>\n</ul>\n\n<p>I have to write a proof that there exist a log-space program $H$, which will get input Array $A$ and create from it corresponding array $C$. But I can't find the correct way to write it. How is this done?</p>\n\n<hr>\n\n<p>A log-space program is a program which uses $O(\\log n)$ bits of memory. Here are some conditions you have to keep:</p>\n\n<ol>\n<li><p>You have to use only variables which have simple integer type (for example <code>int</code> in C++, <code>longint</code> in Pascal).</p></li>\n<li><p>Allowed range of integer is defined: if $n$ is the size of the input we can save into variables only values which are polymonial sized based on $n$.</p>\n\n<p>For example: we can have variables which can takes on values in $[-n...n]$, $[-3n^5...3n^5]$ or also values $[-4...7]$, but we can't have variables which will take on values in $[ 0...2^n]$.\nNo other types of variables are allowed, neither are arrays and iterators.</p></li>\n<li><p>Exceptions from the rules about are input and output. Input will be available in special variables (mostly arrays) which your program can only read from, and the output can only be written to other special variables. So you can't read from output, and you can't increase values of input variables etc.</p></li>\n<li><p>Your programs can't use recursion.</p></li>\n</ol>\n\n<p>Example of log-space program written in Pascal (so everyone can understand it) which will find the largest number in the array of integer</p>\n\n<pre><code>    var n: integer;  //input variable the number of elements in A\n    A: array [1..n] of integer; //input variable - the array of integers\n    m: integer;      // output variable, the position of maximum\n    i, j: integer;   //working variables\n    begin\n      j := 1;\n      for i := 2 to n do\n        if A[i] &gt; A[j] then j := i;\n      m := j;\n    end;\n</code></pre>\n\n<p>The only two variables here are <code>j</code> and <code>i</code> and they evidently take values in $[1...n]$. Therefore all conditions are fulfilled and it really is a log-space program.</p>\n", 'ViewCount': '121', 'Title': 'Simulate the concatenation of two log-space programs in log-space', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-21T06:29:00.577', 'LastEditDate': '2012-11-09T07:49:44.290', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'OwnerDisplayName': 'user12392', 'PostTypeId': '1', 'Tags': '<algorithms><complexity-theory><simulation><space-complexity>', 'CreationDate': '2012-11-06T21:30:33.060', 'Id': '6571'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For directed graph $(G=(V, E),s,t,{Ce})$ in which we want to maximize max flow. All edge capacities are at least one. Define the capacity of an $s \\to t$ path to be the smallest capacities of constituent edges. The fastest path from $s$ to $t$ is the path with the most capcity.</p>\n\n<p>b) Show that the fastest path from $s$ to $t$ in a graph can be computed by Dijkstra's algorithm.</p>\n\n<p>c) Show that the maximum flow in $G$ is the sum of individual flows along at most $|E|$ paths from $s$ to $t$.</p>\n\n<p>It's one of the questions from my algorithms assignment, and I figured out (a), but can't get these two above.</p>\n", 'ViewCount': '418', 'Title': "Dijskstra's algorithm, maximum flow", 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-11T17:50:53.390', 'LastEditDate': '2012-11-09T22:10:28.233', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4523', 'Tags': '<algorithms><graphs><optimization><shortest-path><network-flow>', 'CreationDate': '2012-11-09T19:31:05.533', 'Id': '6586'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a container with a certain dimension. A number of small boxes that may be different in size is to be packed into the container. How to arrange the small boxes such that the container contains as many as possible?</p>\n\n<ul>\n<li>No rotation is allowed.</li>\n<li>The heavier boxes must not be on the top of the lighter ones.</li>\n<li>Approximation is allowed.</li>\n</ul>\n\n<p>I am looking for the algorithm so I can implement it in a software.</p>\n', 'ViewCount': '213', 'Title': 'Algorithm to pack any small boxes into a big box', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:25:11.543', 'LastEditDate': '2013-05-24T03:25:11.543', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4542', 'Tags': '<algorithms><combinatorics><efficiency><approximation><knapsack-problems>', 'CreationDate': '2012-11-10T19:18:01.317', 'Id': '6606'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have asked this exact question on <a href="http://stackoverflow.com/questions/13229722/need-a-practical-solution-for-creating-pattern-database5-5-5-for-15-puzzle">StackOverflow</a>. I did not get the answer that I was looking for. Please read this question fully before answering. Thank You.<br><br>\nFor static pattern database(5-5-5), see <a href="http://reference.kfupm.edu.sa/content/d/i/disjoint_pattern_database_heuristics_56916.pdf" rel="nofollow">this</a>(page 290 and 283) OR there is an explanation below. For <a href="http://en.wikipedia.org/wiki/Fifteen_puzzle" rel="nofollow">What is 15-puzzle?</a><br>\nI am creating a static patter database(5-5-5). This code to to fill entries into the first table. I am doing it via the recursive function <code>insertInDB()</code>. The first input to the recursive function is this (actually the input puzzle contains it in 1-D array. For better understanding I have represented it as 2-D below)<br></p>\n\n<hr>\n\n<p>1 &nbsp;2 &nbsp;3 &nbsp;4<br>\n0 &nbsp;6 &nbsp;0 &nbsp;0<br>\n0 &nbsp;0 &nbsp;0 &nbsp;0<br>\n0 &nbsp;0 &nbsp;0 &nbsp;0<br></p>\n\n<hr>\n\n<p>This is my code : <br></p>\n\n<pre><code>class DBClass\n{\n    public Connection connection;\n     public ResultSet rs;\n      public PreparedStatement ps1;\n    public PreparedStatement ps2;\n    public int k;\n      String read_statement,insert_statement;\n\n    public DBClass()\n    {\n        try {\n            Class.forName("com.mysql.jdbc.Driver");\n        } catch (ClassNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n            try {\n                connection = DriverManager\n                    .getConnection("jdbc:mysql://localhost/feedback?"\n                        + "user=ashwin&amp;password=ashwin&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf8&amp;validationQuery=Select 1");\n                insert_statement="insert into staticpdb1(hash,permutation,cost) values(?,?,?)";\n                read_statement="select SQL_NO_CACHE * from staticpdb1 where hash=? and permutation= ? LIMIT 1";\n                 ps1=connection.prepareStatement(read_statement, ResultSet.TYPE_SCROLL_SENSITIVE, \n                            ResultSet.CONCUR_UPDATABLE);\n                ps2=connection.prepareStatement(insert_statement);\n                k=0;\n            } catch (SQLException e) {\n                // TODO Auto-generated catch block\n                e.printStackTrace();\n            }\n    }\n    public int updateIfNecessary(FifteenPuzzle sub) \n       {\n           String str=sub.toDBString();\n           try\n           {\n\n               ps1.setInt(1, sub.hashcode());\n               ps1.setString(2,str);\n               rs=ps1.executeQuery();\n           if(rs.next())\n              {\n                  //if a row exists, check if the cost is greater than sub\'s\n                  int cost=rs.getInt(3);\n                  if(sub.g_n&lt;cost)  //if the cost of sub is less than db row\'s cost\n                  {\n                      //replace the cost\n                      rs.updateInt(3, sub.g_n);\n                      rs.updateRow();\n                      return 1;   //only examine - do not insert\n                  }\n                  else\n                      return 0;   //dont examine - return\n\n              }\n           else\n               return 2;      //insert and examine\n           }\n           catch(SQLException e)\n           {\n\n               System.out.println("here1"+e);\n               System.err.println("reported recursion level was "+e.getStackTrace().length);\n               return 0;\n           }\n           finally{\n\n               try{\n                   rs.close();}\n               catch(final Exception e1)\n               {\n                   System.out.println("here2"+e1);\n               }\n\n           }\n\n\n       }\n    public void insert(FifteenPuzzle sub)\n    {\n\n        try{\n        String str=sub.toDBString();\n\n\n         ps2.setInt(1,sub.hashcode());\n         ps2.setString(2, str);\n         ps2.setInt(3,sub.g_n);\n         ps2.executeUpdate();\n         ps2.clearParameters();\n        }catch(SQLException e)\n        {\n            System.out.println("here3"+e);\n        }\n    }\n\n    public void InsertInDB(FifteenPuzzle sub) throws SQLException\n       {\n\n           System.out.println(k++);\n\n           int i;\n\n           int p=updateIfNecessary(sub);\n          if(p==0)\n          {\n              System.out.println("returning");\n           return;\n          }\n          if(p==2)\n          {\n          insert(sub);\n          System.out.println("inserted");\n          }\n\n\n           //FifteenPuzzle temp=new FifteenPuzzle(sub.puzzle.clone(),2,sub.g_n);\n           for(i=0;i&lt;sub.puzzle.length;i++)\n           {\n               if(sub.puzzle[i]!=0)\n               {\n\n                   //check the positions it can be moved to\n                   if(i%4!=0 &amp;&amp; sub.puzzle[i-1]==0)  //left\n                   {\n                       //create another clone and increment the moves\n                       FifteenPuzzle temp_inner=new FifteenPuzzle(sub.puzzle.clone(),2,sub.g_n+1);\n                       //exchange positions\n                        int t=temp_inner.puzzle[i];\n                        temp_inner.puzzle[i]=temp_inner.puzzle[i-1];\n                        temp_inner.puzzle[i-1]=t;\n                        InsertInDB(temp_inner);\n                   }\n                   if(i%4!=3 &amp;&amp; sub.puzzle[i+1]==0)  //right\n                   {\n                       //create another clone and increment the moves\n                       FifteenPuzzle temp_inner=new FifteenPuzzle(sub.puzzle.clone(),2,sub.g_n+1);\n                       //exchange positions\n                        int t=temp_inner.puzzle[i];\n                        temp_inner.puzzle[i]=temp_inner.puzzle[i+1];\n                        temp_inner.puzzle[i+1]=t;\n                        InsertInDB(temp_inner);\n                   }\n                   if(i/4!=0 &amp;&amp; sub.puzzle[i-4]==0)  //up\n                   {\n                       //create another clone and increment the moves\n                       FifteenPuzzle temp_inner=new FifteenPuzzle(sub.puzzle.clone(),2,sub.g_n+1);\n                       //exchange positions\n                        int t=temp_inner.puzzle[i];\n                        temp_inner.puzzle[i]=temp_inner.puzzle[i-4];\n                        temp_inner.puzzle[i-4]=t;\n                        InsertInDB(temp_inner);\n                   }\n                   if(i/4!=3 &amp;&amp; sub.puzzle[i+4]==0)  //down\n                   {\n                       //create another clone and increment the moves\n                       FifteenPuzzle temp_inner=new FifteenPuzzle(sub.puzzle.clone(),2,sub.g_n+1);\n                       //exchange positions\n                        int t=temp_inner.puzzle[i];\n                        temp_inner.puzzle[i]=temp_inner.puzzle[i+4];\n                        temp_inner.puzzle[i+4]=t;\n                        InsertInDB(temp_inner);\n\n                  }\n             }   \n       }\n</code></pre>\n\n<p><br><br>\nThe function <strong>insertInDB(FifteenPuzzle fp)</strong> in the class is the recursive function and is called first from the main function with the array for the fifteen puzzle argument(<code>puzzle</code> is an integer array field of the Class <code>FifteenPuzzle</code>) being - <code>1,2,3,4,0,6,0,0,0,0,0,0,0,0,0,0</code>(same as the matrix shown above). Before explaining the other functions I will explain what static pattern database is; briefly(Because of the comments below)<br></p>\n\n<h2>What is a (5-5-5) static pattern database for 15-Puzzle?</h2>\n\n<p>Pattern databases are heuristics used to solve a fifteen puzzle(can be any puzzle. But here I will talk about only 15-Puzzle). A heuristic is a number used to determine which state to be expanded next. I is like cost of each state. Here state is a <em>permutation</em> of the 15-Puzzle. For simple puzzles like 8-Puzzle, the heuristic can be <strong>manhattan distance</strong>. It gives the minimum number of moves, for each misplaced tile, to reach <strong>it\'s</strong> goal position. Then manhattan distances for all the tiles are added up to give the cost for that tile. Manhattan distance gives the lower bound to the estimate of the number of moves required to reach the goal state i.e you cannot reach the goal state with moves, less than the manhattan distance. <strong>BUT</strong> manhattan distance is not a very good heuristic, though admissible,  because it does not consider other tiles near by it. If a tile has to be moved to it\'s goal position, the near by tiles also have to be moved and the number of moves increase. So, clearly for these puzzles, the actual cost is mostly much greater that\nthe manhattan distance.<br>\nTo <strong>overcome</strong> this(manhattan distance) and take into account the other tiles, pattern databases were introduced.\nA static patter database holds the heuristics for sub-problems or for a group of tiles to reach for their goal state. Since, you are calculating the number of moves to make these group of tiles reach their goal state, the other tiles in that group will be taken into account when a tiles is being moved. So, this is a better heuristic and mostly will always is greater than manhattan distance.<br>\n5-5-5 static pattern is just a form of static pattern database where the number of groups are 3, two of them containing 5 tiles each and the third one contains 6(6th isthe blank tile).</p>\n\n<h2>One of the groups is this matrix :<br></h2>\n\n<p>1 &nbsp;2 &nbsp;3 &nbsp;4<br>\n0 &nbsp;6 &nbsp;0 &nbsp;0<br>\n0 &nbsp;0 &nbsp;0 &nbsp;0<br>\n0 &nbsp;0 &nbsp;0 &nbsp;0<br></p>\n\n<hr>\n\n<p>I calculating the heuristics/number_of_moves for all permutations of this group to reach the above configuration and <strong>inserting them into my database</strong>. <br> The total number of combinations(also the no of rows in db) possible is \n<br></p>\n\n<pre><code>16!/(16-5)! = 524160\n</code></pre>\n\n<p><br>  Now, the other functions  - <code>updateIfNecessary(FifteenPuzzle)</code> - this function checks if the array of the passed FifteenPuzzle object is already present in the database. If already present in the database, it checks if the current object\'s cost is less than the cost in DB. If yes, it replaces it with the current cost else does nothing. The function -<code>insert(FifteenPuzzle)</code> inserts a new permutaion with the cost.<br><br>\n<strong>NOTE :</strong> <code>fifteenuzzle.g_n</code> is the cost for the puzzle. For the initial puzzle that represents the matrix above, the cost is <code>0</code> and for each move the cost is <code>incremented by1</code>.<br><br></p>\n\n<p>I have set the stack size to -<code>Xss128m</code>(1024, 512 and 256 were giving a fatal error) for stack size in run configurations. <br>\nCurrently the recursion number or the depth is <strong><code>7,500,000</code> and counting</strong>(value of <code>System.out.println(k++);</code>).\n<br> The total number of combinations possible is \n<br></p>\n\n<pre><code>16!/(16-5)! = 524160\n</code></pre>\n\n<p><br>\nBut the depth has already reached 7,500,000. This is because of generation of duplicate states. Currently the number of entries in the database is <strong>513423</strong>. You might think that there only 10,000 entries to fill up now. But now the rate at which entries are made has decreased drastically about <strong>1 entry every 30 min</strong>. This will never get over then. <br><br>\nI need a solution that is practical - <strong>with or without recursion</strong>. Is it possible?</p>\n', 'ViewCount': '314', 'Title': 'Need a practical solution for creating pattern database(5-5-5) for 15-Puzzle', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-24T13:56:58.500', 'LastEditDate': '2014-03-24T13:56:58.500', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '6675', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4341', 'Tags': '<algorithms><artificial-intelligence><recursion>', 'CreationDate': '2012-11-11T16:34:04.977', 'Id': '6616'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A binary counter is represented by an infinite array of 0 and 1.</p>\n\n<p>I need to implement the action $\\text{add}(k)$ which adds $k$ to the value represented in the array.</p>\n\n<p>The obvious way is to add 1, k times. Is there a more efficient way?</p>\n', 'ViewCount': '214', 'Title': 'Implementing addition for a binary counter', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-12T11:35:22.097', 'LastEditDate': '2012-11-12T11:35:22.097', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6628', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4561', 'Tags': '<algorithms><data-structures><efficiency>', 'CreationDate': '2012-11-12T08:21:57.977', 'Id': '6627'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Could anyone point me to simple tutorial on greedy algorithm  for Minimum Spanning tree - Kruskal's and Prims' Method</p>\n\n<p>I am looking for a tutorial which </p>\n\n<ul>\n<li>does not include all the mathematical notation  </li>\n<li>explains algorithm along with the analysis of the running time.</li>\n</ul>\n", 'ViewCount': '614', 'Title': 'Greedy algorithms tutorial', 'LastEditorUserId': '3004', 'LastActivityDate': '2012-11-13T07:33:33.163', 'LastEditDate': '2012-11-12T18:32:49.273', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6635', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<algorithm-analysis><greedy-algorithms>', 'CreationDate': '2012-11-12T15:02:47.490', 'Id': '6634'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is it about the structure of <a href="http://en.wikipedia.org/wiki/Counting_sort">counting sort</a> that only makes it work on integers?</p>\n\n<p>Surely strings can be counted?</p>\n\n<pre><code>\'\'\' allocate an array Count[0..k] ; initialize each array cell to zero ; THEN \'\'\'\nfor each input item x:\n    Count[key(x)] = Count[key(x)] + 1\ntotal = 0\nfor i = 0, 1, ... k:\n    c = Count[i]\n    Count[i] = total\n    total = total + c\n\n\'\'\' allocate an output array Output[0..n-1] ; THEN \'\'\'\nfor each input item x:\n    store x in Output[Count[key(x)]]\n    Count[key(x)] = Count[key(x)] + 1\nreturn Output\n</code></pre>\n\n<p>Where above does the linear time break down if you try to use counting sort on strings instead (assuming you have strings of fixed length)? </p>\n', 'ViewCount': '424', 'Title': 'Counting sort on non-integers - why not possible?', 'LastActivityDate': '2012-11-12T22:11:41.517', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '6642', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><sorting>', 'CreationDate': '2012-11-12T21:12:07.090', 'Id': '6641'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to find the most probable path (i.e., sequence of states) on an hidden Markov model (HMM) using the Viterbi algorithm. However, I don't know the transition and emission matrices, which I need to estimate from the observations (data).</p>\n\n<p>To estimate these matrices, which algorithm should I use: the Baum-Welch algorithm or the Viterbi Training algorithm? Why?</p>\n\n<p>In case I should use the Viterbi training algorithm, can anyone provide me a good pseudocode (it's not easy to find) ?</p>\n", 'ViewCount': '370', 'Title': 'Viterbi training vs. Baum-Welch algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-01-25T04:03:04.310', 'LastEditDate': '2012-11-14T14:20:41.380', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4590', 'Tags': '<algorithms><hidden-markov-models>', 'CreationDate': '2012-11-14T12:46:59.310', 'Id': '6664'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We can consider a <a href="http://en.wikipedia.org/wiki/Residue_number_system" rel="nofollow">Residue Number System (R.N.S.)</a> that has efficient operations for add, multiply, and compare between two numbers.</p>\n\n<p><strong>A more rigorous definition</strong></p>\n\n<p>We can suppose that we can perform the operations above between two non-negative integers $x$ and $y$ with $x \\ge y$ in time proportional to $O(x\\log(x))$ with $O(\\log(x^2))$ memory.  Or further, if it would possibly allow even more uses, if we could perform these operations in time linear with respect to $x$ and $y$.</p>\n\n<p><strong>What are the potential uses of this system?</strong></p>\n\n<p>For example, cryptology often uses fairly large numbers, and makes use of such a system.  Also, linear programming and systems of linear equations can make use of an efficient R.N.S.  Also, specialized processors can have R.N.S.\'s built in, and could potentially compete with standarized Boolean processors.</p>\n\n<p><strong>What additional uses would this efficient system allow?</strong></p>\n', 'ViewCount': '51', 'Title': 'What are the potential uses of a good R.N.S. system?', 'LastActivityDate': '2012-11-14T20:51:01.067', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1667', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-11-14T20:51:01.067', 'FavoriteCount': '1', 'Id': '6667'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There are $N$ number of people and $X$ amount of objects with different values. Each person will choose an object and will obtain that objects value. If multiple people choose the same object then the value of the object is shared among the people. For example, If there are 2 objects and 3 people, one of the objects will be chosen by at least 2 people, thus the value of the object is divided over 2. </p>\n\n<p>Each round the people make their decisions with new objects and new values, how can a person maximize their expected values accumulated from their choices in Q number of rounds, in order to win(i.e accumulated most value)? ($Q$ belongs to [1, +infinite).</p>\n\n<p>Note that keeping track of other player's decisions and their accumulated values can help in future decision making.</p>\n\n<p>Note: An approximation would be great as well, so far my strategy is to choose a random object at each round, I am looking for ways to maximize this strategy. </p>\n", 'ViewCount': '142', 'Title': 'Guessing the best choice to maximize returns', 'LastEditorUserId': '4365', 'LastActivityDate': '2012-11-16T09:25:51.250', 'LastEditDate': '2012-11-16T08:50:52.197', 'AnswerCount': '2', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4365', 'Tags': '<artificial-intelligence><randomized-algorithms>', 'CreationDate': '2012-11-15T21:33:25.423', 'Id': '6688'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We are given a labelled directed graph, where both vertices (or states) and edges (or transitions) have labels. Informally, two states are bisimilar when they have the same label and they can simulate each other\'s transitions. On the states the two states evolve to, the same again is true.</p>\n\n<p>More formally, a binary relation $R \\subseteq S \\times S$ is a <a href="http://en.wikipedia.org/wiki/Simulation_preorder" rel="nofollow">simulation</a> iff $\\forall (p,q) \\in R$</p>\n\n<ul>\n<li>$p$ and $q$ have the same label, and</li>\n<li>if $p \\overset{a}{\\longrightarrow} p\'$ then $\\exists q\', q \\overset{a}{\\longrightarrow} q\'$ and $(p\',q\') \\in R$.</li>\n</ul>\n\n<p>A relation $R$ is a <a href="http://en.wikipedia.org/wiki/Bisimulation" rel="nofollow">bisimulation</a> iff $R$ and $R^{-1}$ are simulations. The largest bisimulation on the given system is called the <em>bisimilarity relation</em>. </p>\n\n<p>There are algorithms for finding the bisimilarity relation, that run in $O(m \\log n)$ time and $O(m+n)$ space, where $n$ is the number of states and $m$ the number of transitions. An example is the Paige-Tarjan RCP algorithm from 1987.</p>\n\n<p>However, consider a simpler problem. Instead of finding all the bisimulations, I just want to find a few of them. Can it be done faster than in loglinear time? If so, how? For example, let\'s say one is given two states $p,q \\in S$ such that they have the same label and they can make the same transitions. What I find problematic is to check that the states they lead to are once again bisimulations. In other words, one could also ask if there is a quick way to decide if two given states are a bisimulation.</p>\n', 'ViewCount': '78', 'Title': 'How to quickly find a few bisimulations on a given labelled digraph?', 'LastActivityDate': '2012-11-16T00:20:57.640', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6691', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><graph-theory><process-algebras>', 'CreationDate': '2012-11-15T22:52:59.997', 'FavoriteCount': '1', 'Id': '6690'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1199', 'Title': 'Formalization of the shortest path algorithm to a linear program', 'LastEditDate': '2012-12-23T09:32:01.477', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2329', 'FavoriteCount': '2', 'Body': '<p>I\'m trying to understand a formalization of the shortest path algorithm to a linear programming problem:</p>\n\n<p>For a graph $G=(E,V)$, we defined  $F(v)=\\{e \\in E \\mid t(e)=v \\}$ and $B(v)=\\{ e \\in E \\mid h(e)=v\\}$ where $t(e)$ is a tail of a node, and  $h(e)$ is a head of a node.</p>\n\n<p>Also the solutions for the conditions for the linear problem was defined as $b_v=1$ for every node $v$ except of the root $r$ which from it we find all the shortest paths in the graph where $b_r=-(n-1)$. It is written here "We associate a flow (primal variable) $x_e$ with each arc $e \\in E$.</p>\n\n<p>The main linear program is to minimize $\\sum\\limits_{e\\in E }c_ex_e$, subject to $\\sum\\limits_{e\\in B(v)}x_e-\\sum\\limits_{e\\in F(v)}x_e=b_v$ for all $v \\in V$ and $x_e \\geq 0$ for all $e \\in E$, where $c_e$ is the length of arc $e$.</p>\n\n<p>I\'d really love your help with understanding what does $x_e$ represent. Is it the number of times I use $e$ in order to find all the shortest paths in the graph?</p>\n\n<p>I don\'t understand why does the above condition for this linear program is as at it, why does  $\\sum\\limits_{e\\in B(v)}x_e-\\sum\\limits_{e\\in F(v)}x_e=b_v$ for all $v \\in V$ should  be $1$ for every node and $-(n-1)$ for the all the root? If I think of a $3$ nodes tree for a graph,for  the middle node we get that the condition equals to $1$, which makes me think that I might be misunderstood what $x_e$ stands for.</p>\n', 'Tags': '<algorithms><graph-theory><shortest-path><linear-programming>', 'LastEditorUserId': '3016', 'LastActivityDate': '2012-12-23T12:30:04.713', 'CommentCount': '6', 'AcceptedAnswerId': '7562', 'CreationDate': '2012-11-17T15:59:11.327', 'Id': '6717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm just curious about the pseudocode (or real source code, doesn't matter) of the recursive version of this algorithm. In almost every book chapter/paper when describing this topic, they mention that the recursive version takes exponential time and then they give the code for the dynamic programming approach. I understand how the iterative version (dynamic programming ie. memoization) works. But i just wonder about the recursive version.\nFor the info, the key part in the iterative code is: <br/></p>\n\n<blockquote>\n  <p>$\\ell$ ... left <br/>\n  $r$ ... right <br/>\n  $a$ ... apex <br/>\n  $T$ ... triangulation </p>\n  \n  <p>$T_{\\ell,r}= \\min\\{T_{\\ell,a} + \\text{perimeter}_{\\ell,a,r} + T_{a,r}\\}$</p>\n</blockquote>\n\n<p>So how does the recursive function <code>findOT()</code> seem in <br/>\npseudocode or one of these languages (C#, Java, C/C++, PHP, Javascript, SML)?</p>\n", 'ViewCount': '240', 'Title': 'Minimum weight triangulation', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-18T16:49:03.613', 'LastEditDate': '2012-11-18T08:27:23.633', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '6743', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4610', 'Tags': '<algorithms><computational-geometry><recursion>', 'CreationDate': '2012-11-17T19:29:10.790', 'Id': '6720'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In  the theory of distributed algorithms, there are problems with lower bounds, as $\\Omega(n^2)$, that are "big" (I mean, bigger than $\\Omega(n\\log n)$), and nontrivial.\nI wonder if are there problems with similar bound in the theory of serial algorithm, I mean of order much greater than $\\Omega(n\\log n)$.</p>\n\n<p>With trivial, I mean "obtained just considering that we must read the whole input" and similarly.</p>\n', 'ViewCount': '172', 'Title': 'Is there any nontrivial problem in the theory of serial algorithms with a nontrivial polynomial lower bound of $\\Omega(n^2)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-24T08:01:48.460', 'LastEditDate': '2013-05-24T08:01:48.460', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1665', 'Tags': '<algorithms><complexity-theory><lower-bounds>', 'CreationDate': '2012-11-18T01:28:50.303', 'FavoriteCount': '1', 'Id': '6732'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was offered by a professor to give some tutoring in his course of Algorithm Design, based on Kleinberg and Tardos' book. He suggested me to prepare two exercise on dynamic programming, one exercise for polynomial reduction, and another one for approximation algorithms.The exercise should be not too advanced, but not just routine (they should fill two hour of tutoring).\nThen my question is: what are fascinating problems one can propose in dynamic programming, polynomial reductions and approximation algorithms for four nice tutoring sessions.</p>\n", 'ViewCount': '193', 'Title': 'Best a-little-advanced examples of dynamic programming, polynomial reduction and approximation algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-19T18:16:35.777', 'LastEditDate': '2012-11-19T18:16:35.777', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1665', 'Tags': '<algorithms><education><dynamic-programming>', 'CreationDate': '2012-11-18T01:47:15.080', 'Id': '6733'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There may be a large number of algorithms proposed for generating graphs satisfying some common properties (e.g., clustering coefficient, average shortest path length, degree distribution, etc).</p>\n\n<p>My question concerns a specific case: I want to generate a few <em>undirected regular</em> graphs (i.e., every node in these graphs has the same number of neighbors) with different clustering coefficients and average shortest path lengths. More generally, by fixing a degree distribution, I want to generate graphs with different clustering coefficients and average shortest path lengths.</p>\n\n<p>I wonder what are the well-known algorithms for doing this (or in fact, is there any?), and what are the recommended software for the same purpose?</p>\n', 'ViewCount': '358', 'Title': 'Algorithms for graph generation using given properties', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-21T10:21:38.160', 'LastEditDate': '2012-11-18T18:43:46.823', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4641', 'Tags': '<algorithms><graph-theory><graphs><sampling>', 'CreationDate': '2012-11-18T17:26:39.090', 'FavoriteCount': '1', 'Id': '6744'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A depth first search produces a spanning tree. If you perform DFS using all possible orderings of the adjacency list, wouldn't you find the minimum spanning tree? In other words, there is no example of a graph where a DFS won't find the minimum spanning tree regardless of how the adjacency list is ordered. Is this correct or not? I can't come up with a counter example and intuitively it seems correct...</p>\n", 'ViewCount': '703', 'Title': 'Depth First Search to find Minimum spanning tree', 'LastActivityDate': '2012-11-19T09:40:43.430', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4645', 'Tags': '<algorithms><graphs><spanning-trees>', 'CreationDate': '2012-11-19T00:37:02.580', 'FavoriteCount': '1', 'Id': '6749'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I recently came across the following interesting problem - one is given a sequence of <code>X</code>s and <code>Y</code>s such as <code>XXYXYXYYXYXXXYX</code>, and consider a sequence to be good if, as you start at the left and move right, the number of <code>X</code>s is greater than or equal to the number of <code>Y</code>s at any point except at the very end, at which the two quantities must be equal.</p>\n\n<p>One must determine the number of points at which changing either a single <code>X</code> to <code>Y</code> or a single <code>Y</code> to <code>X</code> in a given sequence will yield a good sequence.</p>\n\n<p>I initially considered traveling through the sequence linearly and checking if toggling the letter at that point would yield a good sequence, however that approach is on the order of $O(n\\cdot n)=O(n^2)$ in the worst case where n is the length of the sequence. However, I was wondering if there was some faster method to do it.</p>\n\n<p>EDIT: I made the observation that for any sequence, if the number of possible changes is greater than 0, than only one type of change will work (either changing an <code>X</code> to <code>Y</code> or <code>Y</code> to <code>X</code>) given the condition at the end that the number of <code>X</code> and <code>Y</code> must be equal.</p>\n', 'ViewCount': '103', 'Title': 'Improving Time Bound of this Algorithm past $O(n^2)$', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-19T05:59:02.350', 'LastEditDate': '2012-11-19T05:59:02.350', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'Rebecca Kuang', 'PostTypeId': '1', 'Tags': '<algorithms><subsequences>', 'CreationDate': '2012-11-18T16:26:51.083', 'Id': '6751'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a complex query $Q$ used to search a dataset $S$ to find $H_\\text{exact} = \\{s \\in S \\mid \\text{where $Q(s)$ is True}\\}$. Each query takes on average time $t$ so the overall time in the linear search is $t\\cdot |S|$. I can break a query down into simpler sub-queries q_i and find $H_\\text{approx} = \\{s\\in S \\mid \\forall q_j(s) \\text {is True}\\}$  and where $H_\\text{exact}\\subseteq H_\\text{approx}$. Each subquery $q_i$ is much faster to compute, so overall it is faster to find $H_\\text{approx}$ and then use $Q$ to find $H_\\text{exact}$.</p>\n\n<p>Each $Q$ has many $q_i$. The overlap between different $Q$ is high. I\'m looking for a way to determine a decision-tree-like set of fixed questions $q_j$ which minimize the average time to find a H_exact, based on a large sample of search queries.</p>\n\n<p>To make this more concrete, suppose the data set contains the 7 billion people in the world, and the complex queries are things like "the woman who lives in the red house on the corner of 5th and Lexington in a city starting with B."</p>\n\n<p>The obvious solution is to check every person in world and see who matches the query. There may be more than one such person. This method takes a long time. </p>\n\n<p>I could pre-compute this query exactly, in which case it would be very fast .. but only for this question. However, I know that other queries are for the woman who lives on the blue house on the same corner, the man who lives on the same corner, the same question but in a city starting with C, or something totally different, like \'the king of Sweden.\'</p>\n\n<p>Instead, I can break the complex question down into a set of easier but more general sets. For example, all of the above questions have a gender-role based query, so I can precompute the set of all people in the world who consider themselves a \'woman.\' This sub-query takes essentially no time, so the overall search time decreases by roughly 1/2. (Assuming that by other knowledge we know that a Swedish "king" cannot be a "woman." Hatshepsut was an Egyptian woman who was king.)</p>\n\n<p>However, there are sometimes queries which aren\'t gender-based, like "the person who lives on 8th street in a red house in a city starting with A." I can see that the subquery "lives in a red house" is common, and pre-compute a list of all those people who live in a red house.</p>\n\n<p>This gives me a decision tree. In the usual case, each branch of the decision tree contains different questions, and the methods to select the optimal terms for the decision tree are well known. However, I\'m building on an existing system which requires that all branches must ask the same questions.</p>\n\n<p>Here\'s an example of a possible final decision set: question 1 is \'is the person a woman?\', question 2 is \'does the person live in a red house?\', question 3 is \'does the person live in a city starting with A or does the person live in a city starting with B?\', and question 4 is \'does the person live on a numbered street?\'.</p>\n\n<p>When a query $Q$ comes in, I see if its $q_i$ match any of the pre-computed questions $q_j$ I\'ve determined. If so, then I get the intersection of those answers, and ask the question $Q$ on that intersection subset. Eg, if the question is "people who live in a red house on an island" then find that "person lives in a red house" is already precomputed, so it\'s only matter of finding the subset of those who also live on an island.</p>\n\n<p>I can get a cost model by looking at a set of many $Q$ and check to see the size of the corresponding $H_\\text{approx}$. I want to minimize the average size of $H_\\text{approx}$.</p>\n\n<p>The question is, how do I optimize the selection of possible $q_j$ to make this fixed decision tree? I tried a GA but it was slow to converge. Probably because my feature space has a few million possible $q_j$. I\'ve come up with a greedy method, but I\'m not happy with the result. It too is very slow, and I think I\'m optimizing the wrong thing.</p>\n\n<p>What existing research should I be looking at for ideas?</p>\n', 'ViewCount': '98', 'Title': 'Fixed-length decision-tree-like feature selection to minimize average search performance', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-19T09:26:37.540', 'LastEditDate': '2012-11-19T09:26:37.540', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4650', 'Tags': '<algorithms><optimization><machine-learning><greedy-algorithms>', 'CreationDate': '2012-11-19T09:17:01.957', 'Id': '6763'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1322', 'Title': 'Finding negative cycles for cycle-canceling algorithm', 'LastEditDate': '2012-11-20T09:31:55.460', 'AnswerCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4298', 'FavoriteCount': '4', 'Body': '<p>I am implementing the cycle-canceling algorithm to find an optimal solution for the min-cost flow problem. By finding and removing negative cost cycles in the residual network, the total cost is lowered in each round. To find a negative cycle I am using the bellman-ford algorithm.</p>\n\n<p>My Problem is:\nBellman-ford only finds cycles that are reachable from the source, but I also need to find cycles that are not reachable.</p>\n\n<p>Example: In the following network, we already applied a maximum flow. The edge $(A, B)$ makes it very expensive. In the residual network, we have a negative cost cycle with capacity $1$. Removing it, would give us a cheaper solution using edges $(A, C)$ and $(C, T)$, but we cannot reach it from the source $S$.</p>\n\n<p>Labels: Flow/Capacity, Cost</p>\n\n<p><img src="http://i.stack.imgur.com/jKtUd.png" alt="enter image description here"></p>\n\n<p>Of course, I could run Bellman-ford repeatedly with each node as source, but that does not sound like a good solution. I\'m a little confused because all the papers I read seem to skip this step.</p>\n\n<p>Can you tell me, how to use bellman-ford to find every negative cycle (reachable or not)?\nAnd if not possible, which other algorithm do you propose?</p>\n', 'Tags': '<algorithms><graph-theory><shortest-path><network-flow>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-20T17:17:58.753', 'CommentCount': '3', 'AcceptedAnswerId': '6789', 'CreationDate': '2012-11-19T20:56:27.043', 'Id': '6773'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for an efficient algorithm to find the longest repeated pattern in a string.</p>\n\n<p>For example, consider the following string of numbers:  </p>\n\n<p><code>5431428571428571428571428571427623874534</code>.</p>\n\n<p>As you can see, <code>142857142857</code> is the longest pattern which is repeated for a couple of times (at least twice) in this string.</p>\n\n<p>The repeated string should not contain any re\nany idea rather than brute-force?  </p>\n", 'ViewCount': '1033', 'Title': 'Find the longest repeated pattern in a string', 'LastEditorUserId': '472', 'LastActivityDate': '2012-11-22T03:33:54.443', 'LastEditDate': '2012-11-22T03:33:54.443', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'OwnerDisplayName': 'MBZ', 'PostTypeId': '1', 'Tags': '<algorithms><strings><subsequences>', 'CreationDate': '2012-11-08T19:28:56.950', 'FavoriteCount': '1', 'Id': '6776'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1104', 'Title': u"Modifying Dijkstra's algorithm for edge weights drawn from range $[1,\u2026,K]$", 'LastEditDate': '2012-11-21T08:01:32.647', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '2', 'Body': '<p>Suppose I have a directed graph with edge weights drawn from range $[1,\\dots, K]$ where $K$ is constant. If I\'m trying to find the shortest path using <a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a>, how can I modify the algorithm / data structure and improve the time complexity to $O(|V|+|E|)$?</p>\n', 'Tags': '<algorithms><data-structures><shortest-path><weighted-graphs>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-21T20:43:49.977', 'CommentCount': '6', 'AcceptedAnswerId': '6820', 'CreationDate': '2012-11-21T03:08:52.193', 'Id': '6797'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let us say you have a group of guys and and a group of girls. Each girl is either attracted to a guy or not, and vice versa. You want to match as many people as possible to a partner they like.</p>\n\n<p>Does this problem have a name? Is it feasibly solvable? Sounds hard to me...</p>\n\n<p>Ps. note that since the attraction is not neccessarily mutual the standard max-flow solution does not work.</p>\n', 'ViewCount': '120', 'Title': 'Matching girls with boys without mutual attraction (variant of maximum bipartite matching)', 'LastActivityDate': '2012-11-21T13:16:46.817', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6811', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><graphs><bipartite-matching>', 'CreationDate': '2012-11-21T10:50:19.373', 'Id': '6807'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We covered this in class today. I understand the mechanics of it, but aside from being a nice example of recursion does it serve any purpose? </p>\n\n<p><img src="http://i.stack.imgur.com/DOcD0.png" alt="enter image description here"></p>\n\n<p>Searching the web reveals lots of pages with the formula and it\'s implementation in code, some talk about the author, but nothing about it\'s purpose.</p>\n', 'ViewCount': '171', 'Title': 'What is the TAK function for?', 'LastActivityDate': '2012-11-23T12:42:32.547', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6822', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1548', 'Tags': '<algorithms><recursion>', 'CreationDate': '2012-11-21T14:20:48.557', 'FavoriteCount': '1', 'Id': '6815'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a bunch of databases, each having different access patterns, such that each puts a different amount of load on its database cluster. I would like to distribute them around my set of database clusters such that the workload for the clusters is evenly distributed. </p>\n\n<p>I looked at the <a href="http://en.wikipedia.org/wiki/3-partition_problem" rel="nofollow"><em>k</em>-partition problem</a>, which sounds close to what I want, except each of my database clusters has a different load capacity. That means I need an algorithm that minimizes what percent of load capacity is used on all clusters, whereas the <em>k</em>-partition problem minimizes the integer load on each cluster. </p>\n\n<p>Does such an algorithm exist? And can anyone point me to a sample implementation of it?</p>\n', 'ViewCount': '113', 'Title': 'How to distribute items of varying sizes into bins of varying sizes, such that percent utilization across all bins is minimized?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-12-23T05:49:49.143', 'LastEditDate': '2012-12-23T05:49:49.143', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4679', 'Tags': '<algorithms><partitions>', 'CreationDate': '2012-11-21T16:44:16.083', 'FavoriteCount': '2', 'Id': '6821'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider a straight highway in the plane which can be modelled by a horizontal strip in the plane. A finite set T of targets are located on the highway, and a finite set S of wireless sensors are located outside of the highway. A sensor s can monitor a target t if and only if the Euclidean distance between s and t is at most one. Suppose that each sensor s is an element of S has a positive cost c (s) and each target t is an element of T can be monitored by at least one sensor in S. Consider a subset S\' of sensors in S. S\' is said to be a cover if each target in T is covered by at least one sensor in S\'. The cost of S\' is the total costs of the sensors in S\'. The objective is to compute a cover S\' of minimum cost. This is a polynomial time algorithm</p>\n\n<p>Consider a 2D plane. There are targets that are randomly distributed between a $y$ upper bound and lower bound. This set is $T$. $T_1$ is marked with coordinates $(X,Y)$. There is a set $S$ of sensors that are guaranteed to cover every target. Each sensor has a radius $1$ and an $(X,Y)$ coordinate. Each target has a cost $c$ that is a weight. So my task is to find a minimum weight or cost for a set $S\'$ that covers each sensor.</p>\n\n<p>So I know that there is a recursive relation between the disks that "dominate" or "control" other disks but I\'m having trouble seeing a property that shows how to utilize the dominance of the disks. I got this far:</p>\n\n<p>Let $D^+$ be the set of disks whose center lies above the strip (upper disks).</p>\n\n<p>Let $D^-$ be the set of disks whose center lies below the strip (lower disks).</p>\n\n<p>Consider an upper disk $d$, and $d$ intersects a vertical line $L$. Another upper disk $d\'$ is said to be controlled or dominated by $d$, if one of the following holds: </p>\n\n<ol>\n<li>$d\'$ does not intersect $L$,</li>\n<li>the lower intersection endpoint of $d\'$ and $L$ is higher than the lower intersection endpoint of $d$ and $L$, and</li>\n<li>the lower intersection endpoint of $d\'$ and $L$ is identical to the lower intersection endpoint of $d$ and $L$, but the center of $d\'$ is on the right of the center of $d$.</li>\n</ol>\n\n<p>Similarly, for a lower disk $d$, and $d$ intersects a vertical line $L$. Another lower disk $d\'$ is said to be controlled or dominated by $d$, if one of the following holds:</p>\n\n<ol>\n<li>$d\'$ does not intersect $L$, </li>\n<li>the upper intersection endpoint of $d\'$ and $L$ is lower than the upper intersection endpoint of $d$ and $L$, and </li>\n<li>the upper intersection endpoint of $d\'$ and $L$ is identical to the upper intersection endpoint of $d$ and $L$, but the center of $d\'$ is on the right of the center of $d$.</li>\n</ol>\n\n<p>Howevern, I\'m having trouble finalizing the algorithm. Any help? Is that clear?</p>\n', 'ViewCount': '218', 'Title': 'Recursive relation help for dynamic programming 2D plane algorithm', 'LastEditorUserId': '4682', 'LastActivityDate': '2012-11-22T05:52:23.570', 'LastEditDate': '2012-11-22T05:52:23.570', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4682', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2012-11-21T19:54:08.923', 'Id': '6825'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am unsure if I have even identified the problem correctly, but reading up on <a href="http://en.wikipedia.org/wiki/Knapsack_problem" rel="nofollow">knapsack problem</a> seems the closest to what I am trying to solve:</p>\n\n<blockquote>\n  <p>A cook has $k$ ingredients of $p$ quantities. Given a\n  list of $n$ unique recipes, each consisting varying ingredients of varying\n  quantities. \n  Now, the cook would like to use all ingredients on ONE recipe with minimal leftovers.</p>\n</blockquote>\n\n<p>What is his solution? And can it be determined in $O(\\log n)$ time?</p>\n\n<p><code>Sample input</code></p>\n\n<p>500 pounds of flour\n300 mg sugar\n5 mg of vanilla pods\n20 eggs</p>\n\n<p><code>Database of possible recipes</code>:</p>\n\n<p>Thai Fried Noodles (doesn\'t contain vanilla or flour, but contains 1 tablespoon of sugar)</p>\n\n<p>Tiramisu (doesn\'t contain flour and vanilla but contains 3 tablespoons of sugar and 8 eggs)</p>\n\n<p>Anna\'s Special Tiramiu (doesn\'t contain flour and vanilla but contains 1 tablespoons of sugar and 8 eggs)</p>\n\n<p>Truffle Tagliatelle (doesn\'t contain any input ingredients)</p>\n\n<p><strong>EDIT</strong> Cost/benefit decision:</p>\n\n<p>Given the sample input, <code>Tiramisu</code> recipe is the most preferred because among the 4 recipes in the database, it contains the most number of input ingredient type (2 of 4 types), and the most number of input ingredient quantity.</p>\n\n<p>Expected result where 1) is the top search result of relevance:\n1) Tiramisu\n2) Anna\'s Special Tiramisu\n3) Thai Fried Noodles\n4) Truffle Tagliatelle</p>\n\n<p><strong>EDIT:</strong> I believe my question is a variant of the <a href="http://www.g12.cs.mu.oz.au/wiki/doku.php?id=simple_knapsack" rel="nofollow">integer knapsack problem</a></p>\n', 'ViewCount': '208', 'Title': 'Knapsack problem with multiple constraints', 'LastEditorUserId': '4694', 'LastActivityDate': '2012-11-27T07:25:31.180', 'LastEditDate': '2012-11-26T15:12:05.490', 'AnswerCount': '1', 'CommentCount': '18', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4694', 'Tags': '<algorithms><knapsack-problems>', 'CreationDate': '2012-11-22T17:34:47.507', 'Id': '6837'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '106', 'Title': 'Minimum cost subset of sensors covering targets', 'LastEditDate': '2012-11-23T06:40:54.747', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4695', 'FavoriteCount': '1', 'Body': "<p>I have a dynamic programming problem: </p>\n\n<blockquote>\n  <p>If I have a set of sensors covering targets (a target might be\n  covered by mutiple sensors) how can I find the minimum cost subset of\n  sensors covering all targets given each sensor has a cost?</p>\n</blockquote>\n\n<p>I have thought a lot about this, but I can't reach the recursive formula to write my program. The greedy algorithm does not always provide the correct minimum cost subset. My problem is that sensors overlap in covering targets. Any help?</p>\n\n<p><strong>Example:</strong> I have set of sensors $\\{s_1,s_2,s_3\\}$ with costs $\\{1,\\frac{5}{2},2\\}$ and 3 targets $\\{t_1,t_2,t_3\\}$. Sensors cover $\\{t_1 t_2,t_1 t_2 t_3,t_2 t_3\\}$ and I need to get minimum cost subset by dynamic programming. For the above example if I use greedy algorithm I would get $s_1,s_3$ but the right answer is $s_2$ only.</p>\n", 'Tags': '<algorithms><dynamic-programming>', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-23T06:40:54.747', 'CommentCount': '0', 'AcceptedAnswerId': '6850', 'CreationDate': '2012-11-22T19:01:32.803', 'Id': '6839'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For what size alphabet does it take longer to construct <a href="http://en.wikipedia.org/wiki/Suffix_tree" rel="nofollow">a suffix tree</a> - for a really small alphabet size (because it has to go deep into the tree) or for a large alphabet size? Or is it dependent on the algorithm you use? If it is dependent, how does the alphabet size affect <a href="http://en.wikipedia.org/wiki/Ukkonen%27s_algorithm" rel="nofollow">Ukkonen\'s algorithm</a>?</p>\n', 'ViewCount': '88', 'Title': 'What are the effects of the alphabet size on construct algorithms for suffix trees?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-23T09:43:36.243', 'LastEditDate': '2012-11-23T09:30:49.477', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4696', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings><efficiency>', 'CreationDate': '2012-11-22T22:14:10.630', 'Id': '6842'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm">Bellman-Ford algorithm</a> determines the shortest path from a source $s$ to all other vertices. Initially the distance between $s$ and all other vertices is set to $\\infty$. Then the shortest path from $s$ to each vertex is computed; this goes on for  $|V|-1$ iterations. My questions are:</p>\n\n<ul>\n<li>Why does there need to be $|V|-1$ iterations?</li>\n<li>Would it matter if I checked the edges in a different order?<br>\nSay, if I first check edges 1,2,3, but then on the second iteration I check 2,3,1. </li>\n</ul>\n\n<p>MIT  Prof. Eric said the order didn\'t matter, but this confuses me: wouldn\'t the algorithm incorrectly update a node based on edge $x_2$ if its value was dependent on the edge $x_1$ but $x_1$ is updated after $x_2$?</p>\n', 'ViewCount': '510', 'Title': 'Bellman-Ford algorithm - Why can edges be updated out of order?', 'LastEditorUserId': '4304', 'LastActivityDate': '2013-08-01T05:40:27.860', 'LastEditDate': '2012-11-25T21:17:42.783', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '6914', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<algorithms><shortest-path>', 'CreationDate': '2012-11-25T19:11:25.800', 'Id': '6894'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am stuck on this problem:</p>\n\n<blockquote>\n  <p>Given an array $A$ of the first $n$ natural numbers randomly permuted, an array $B$ is \n  constructed, such that\n  $B(k)$ is the number of elements from $A(1)$ to $A(k-1)$ which are smaller than $A(k)$.  </p>\n  \n  <p>i) Given $A$ can you find $B$ in $O(n)$ time?<br>\n  ii) Given $B$ can you find $A$ in $O(n)$ time?</p>\n</blockquote>\n\n<p>Here, $B(1) = 0$. For a concrete example:\n$$\\begin{vmatrix}\n  A &amp; 8 &amp; 4 &amp; 3 &amp; 1 &amp; 7 &amp; 2 &amp; 9 &amp; 6 &amp; 5 \\\\\n  B &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 3 &amp; 1 &amp; 6 &amp; 4 &amp; 4 \\\\\n\\end{vmatrix}$$</p>\n\n<p>Can anyone help me? Thanks.</p>\n', 'ViewCount': '177', 'Title': 'Finding number of smaller elements for each element in an array efficiently', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-30T19:49:37.933', 'LastEditDate': '2012-11-27T01:45:21.400', 'AnswerCount': '2', 'CommentCount': '9', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1972', 'Tags': '<algorithms><arrays><permutations>', 'CreationDate': '2012-11-25T23:13:06.470', 'Id': '6898'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3522', 'Title': 'Getting negative cycle using Bellman Ford', 'LastEditDate': '2012-11-26T15:11:04.630', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4298', 'FavoriteCount': '2', 'Body': '<p>I have to find a negative cycle in a directed weighted graph. I know how the Bellman Ford algorithm works, and that it tells me if there is a reachable negative cycle. But it does not explicitly name it.</p>\n\n<p>How can I get the actual path $v1, v2, \\ldots vk, v1$ of the cycle?</p>\n\n<p>After applying the standard algorithm we already did $n-1$ iterations and no further improvement should be possible. If we can still lower the distance to a node, a negative cycle exists.</p>\n\n<p>My idea is: Since we know the edge that can still improve the path and we know the predecessor of each node, we can trace our way back from that edge until we meet it again. Now we should have our cycle.</p>\n\n<p>Sadly, I did not find any paper that tells me if this is correct. So, does it actually work like that?</p>\n\n<p><strong>Edit:</strong> This example proofs that my idea is wrong.\nGiven the following graph, we run Bellman-Ford from node $1$.</p>\n\n<p><img src="http://i.stack.imgur.com/pNLTw.png" alt="enter image description here"></p>\n\n<p>We process edges in the order $a, b, c, d$. After $n-1$ iterations we get <strong>node distances:</strong><br>\n$1: -5$<br>\n$2: -30$<br>\n$3: -15$</p>\n\n<p>and <strong>parent table:</strong><br>\n$1$ has parent $3$<br>\n$2$ has parent $3$<br>\n$3$ has parent $2$<br></p>\n\n<p>Now, doing the $n$th iteration we see that the distance of node $1$ can still be improved using edge $a$. So we know that a negative cycle exists and $a$ is part of it.</p>\n\n<p>But, by tracing our way back through the parent table, we get stuck in another negative cycle $c, d$ and never meet $a$ again.</p>\n\n<p>How can we solve this problem?</p>\n', 'Tags': '<algorithms><graphs><shortest-path>', 'LastEditorUserId': '4298', 'LastActivityDate': '2013-05-22T22:30:41.320', 'CommentCount': '0', 'AcceptedAnswerId': '6921', 'CreationDate': '2012-11-26T14:27:26.067', 'Id': '6919'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '164', 'Title': 'Randomized Rounding of Solutions to Linear Programs', 'LastEditDate': '2012-11-30T04:10:17.983', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/Linear_programming#Integral_linear_programs" rel="nofollow">Integer linear programming</a> (ILP) is an incredibly powerful tool in combinatorial optimization. If we can formulate some problem as an instance of an ILP then solvers are guaranteed to find the global optimum. However, enforcing integral solutions has runtime that is exponential in the worst case. To cope with this barrier, several approximation methods related to ILPs can be used,</p>\n\n<ul>\n<li>Primal-Dual Schema</li>\n<li>Randomized Rounding</li>\n</ul>\n\n<p>The Primal-Dual Schema is a versatile method that gives us a "packaged" way to come up with a greedy algorithm and prove its approximation bounds using the relaxed dual LP. Resulting combinatorial algorithms tend to be very fast and perform quite well in practice. However its relation to linear programming is closer tied to the analysis. Further because of this analysis, we can easily show that constraints are not violated.</p>\n\n<p>Randomized rounding takes a different approach and solves the relaxed LP (using interior-point or ellipsoid methods) and rounds variables according to some probability distribution. If approximation bounds can be proven this method, like the Primal-Dual schema, is quite useful. However, one portion is not quite clear to me:</p>\n\n<blockquote>\n  <p>How do randomized rounding schemes show that constraints are not violated?</p>\n</blockquote>\n\n<p>It would appear that naively flipping a coin, while resulting in a 0-1 solution, could violate constraints! Any help illuminating this issue would be appreciated. Thank you.</p>\n', 'Tags': '<optimization><randomized-algorithms><linear-programming><approximation>', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-30T04:10:17.983', 'CommentCount': '4', 'AcceptedAnswerId': '6949', 'CreationDate': '2012-11-27T05:05:34.050', 'Id': '6941'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '160', 'Title': 'Algorithm exercise', 'LastEditDate': '2014-04-23T13:45:26.233', 'AnswerCount': '1', 'Score': '-1', 'OwnerDisplayName': 'newbie', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Body': u'<p>making exercises to prepare a test I\'m having problems to understand 2 questions, the questions are:</p>\n\n<pre><code> how many are the leafs of a decisional tree associated to any algorithm for \n  the search problem  in a ordered set?\n</code></pre>\n\n<p>for this question I have 2 set of answer where, for every set, 1 is right, looking at the solution I found this, but I\'m not able to understand why the right answers are all C.</p>\n\n<pre><code>    1.                 a) \u0398(n log n)    b) \u0398(log n) *c) \u03a9(n!)   d) O(n!)\n\n    2.                 a) \u0398(n log n)    b) \u0398(log n) *c) \u03a9(n)    d) \u0398(n!)\n</code></pre>\n\n<p>The other question, referred to the first one is:</p>\n\n<pre><code>  and in a non-ordered one?\n</code></pre>\n\n<p>And here I can\'t see what does it change with the number of leafs in the ordered case.</p>\n\n<p>I\'m sorry if this question violates the rules, in the faqs, I read here <a href="http://meta.stackexchange.com/questions/10811/how-to-ask-and-answer-homework-questions">http://meta.stackexchange.com/questions/10811/how-to-ask-and-answer-homework-questions</a> and it seems to be possible to make these kind of question.</p>\n\n<p>Thanks in advance.</p>\n', 'Tags': '<algorithms>', 'LastEditorUserId': '-1', 'LastActivityDate': '2012-11-27T21:18:09.883', 'CommentCount': '4', 'AcceptedAnswerId': '6967', 'CreationDate': '2012-11-27T16:29:04.553', 'Id': '6961'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a universe $U$ consisting of k sets of vectors with each vector $\\vec{v} \\in {\\mathbb{F}_{p^m}}^n $. Given also another vector $\\vec{c} \\in {\\mathbb{F}_{p^m}}^n$.\nNow decide if there is a set $X$ with $|X| = |U|$ and $X_i \\in U_i, i = 1,2,...,k$ such that $\\sum\\limits_i X_i = \\vec{c}$. If there is, output this set.</p>\n\n<p>In other words, I want to find a combination of one element out of each set that sums up to the given vector, given that the vectors' entries can only be the results of modulo operation with a given integer. I hope the problem becomes clear.</p>\n\n<p>I want to find an efficient algorithm to solve this problem. It seems to me that it is NP-complete, but I find no other NP-complete problem that I can reduce. If there is one, existing algorithm (if any) to that other problem could be used for this problem.<br>\nI looked at integer programming, but I did not find anything with respect to finite fields.</p>\n\n<p>Any ideas?</p>\n", 'ViewCount': '129', 'Title': 'Find vectors with elements of finite fields that sum up to given value', 'LastActivityDate': '2013-08-26T02:07:02.137', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '6989', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4769', 'Tags': '<algorithms><combinatorics><discrete-mathematics>', 'CreationDate': '2012-11-27T21:33:15.290', 'Id': '6968'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Data structures are seen as important, equal to algorithms. This view is especially encouraged in situations, where appropriate data structure is the main factor that allows an algorithm to exist and to perform at satisfying complexity.</p>\n\n<p>However, despite the additional features of data structures (e.g. organization, like in trees storing entries according to less-like relation), all that data structures do is keeping information unchanged between algorithm's actions. Can this generic feature of data structures be izolated and defined in abstract way?</p>\n", 'ViewCount': '132', 'Title': 'Generalized data structure', 'LastActivityDate': '2012-11-30T07:13:01.040', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4773', 'Tags': '<algorithms><data-structures><information-theory>', 'CreationDate': '2012-11-28T04:32:26.293', 'Id': '6980'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Problem:</strong> Suppose $V$ is an <a href="http://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL tree</a> (a self-balancing binary search tree) of $n$\n  elements. After the insertion of $n^2$ elements, what would be its\n  height?</p>\n</blockquote>\n\n<p><strong>My idea:</strong> the height of an AVL tree is originally $O(\\log(n))$ where $n$ is the number of elements. After insertion of $n^2$ elements, its height will be:$$O(\\log(n+n^2))=O(\\log(n^2))=O(2\\log(n))=O(\\log(n))$$</p>\n\n<p>My answer would be $O(\\log(n))$ but I\'m having doubts.</p>\n\n<p>Why is the asymptotic complexity of the result the same despite the fact that there are more elements?</p>\n', 'ViewCount': '82', 'Title': 'Height of AVL after entries', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-29T07:36:40.940', 'LastEditDate': '2012-11-29T06:08:02.270', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7007', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<algorithms><asymptotics><binary-trees><search-trees>', 'CreationDate': '2012-11-28T15:22:50.797', 'Id': '6995'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I experience a difficulty in solving exercises in distributed algorithm. Below is the the exercise I try to solve, it looks like I miss basic idea. </p>\n\n<p><strong>Exercise.</strong> Consider a 15-processor asynchronous network with processors 0,\u2026,14. The processors constantly run a synchronizer. Let $v$ and $v'$ be two processors in the network, and suppose that at a certain moment, the pulse counter at $v$ shows $p=27$. What is the range of possible pulse numbers at $v'$ in each of the following cases:</p>\n\n<p>a) The network is a ring, $v$ is processor number 11, $v'$ is processor number 2 and the synchronizer used is $\\alpha$.</p>\n\n<p><strong>Idea</strong>: if $v'$ hasn't sent any message up to pulse 27 of $v$, pulse of $v'$ is still 0, therefore lower bound of pulse of $v'$ is 0. The model of synchronizer is $\\alpha$ it means every node informs all nodes about it's safe(v,p) state, hence I assume that $v'$ might be 11-2=9 pulses before $v$. </p>\n\n<p>b) The network is a full balanced binary tree (4 levels), $v$ is the root, $v'$ is one of the leaves and the synchronizer used $\\beta$.</p>\n\n<p><strong>Idea</strong>: $v'$ also might have pulse 27, in this case $v$ sends at speed of $v'$.</p>\n\n<p>c) The same as in (b), except both $v$ and $v'$ are leaves.</p>\n\n<p>Honestly, I am completely confused by this exercise, I wrote few ideas, but I don't have any understanding and any intuition behind the answers.</p>\n\n<p>I will appreciate if someone show me the way how to solve such exercises.</p>\n", 'ViewCount': '145', 'Title': 'Distributed algorithms - $\\alpha, \\beta$ synchronizers', 'LastEditorUserId': '3094', 'LastActivityDate': '2012-11-29T04:18:41.997', 'LastEditDate': '2012-11-29T04:18:41.997', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7002', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><distributed-systems><synchronization>', 'CreationDate': '2012-11-28T20:58:24.443', 'Id': '7001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am investigating a convex hull algorithm that involves sorting. In fact, its running time is limited by sorting, so it is $O(n \\log n)$, where $n$ is the number of points on the plane.</p>\n\n<p>That algorithm first sorts the points by x-coordinates. It then includes the first and last points in the convex hull. From there on, I am confused. How could an algorithm like this get the other points of the hull?</p>\n', 'ViewCount': '143', 'Title': 'How does sorting come into play with convex hull?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-08-20T19:10:17.393', 'LastEditDate': '2012-11-30T07:53:04.263', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2012-11-30T07:42:55.877', 'Id': '7037'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to be able to locate a substring in a string allowing for a specified number of mismatches, insertions and deletions - and at the same time know how many mismatches, insertions and deletions were used for any match.</p>\n\n<p>Using brute force backtrack I can find the matches, but I cannot guarantee that the match was produced using the fewest permutations possible.</p>\n\n<p>Using dynamic programming I can find the matches and guarantee that the match was produced using the fewest permutations possible, but I cannot specify a number of allowed mismatches, insertion and deletions - only a total edit distance.</p>\n', 'ViewCount': '243', 'Title': 'Fuzzy string matching algorithm with allowed events?', 'LastEditorUserId': '19', 'LastActivityDate': '2013-01-27T23:12:52.733', 'LastEditDate': '2012-12-01T05:00:57.040', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4810', 'Tags': '<algorithms><dynamic-programming><strings><substrings>', 'CreationDate': '2012-11-30T10:52:00.527', 'FavoriteCount': '1', 'Id': '7040'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given an undirected Graph. I want to find a hamiltonian path with no restriction to starting or ending vertices. I know there are some smart algorithms for solving that.<br>\nNow let's make things interesting: sometimes, when I arrive at a vertex, the edges connected to that vertex change. I.e. some will be removed, others appear. It is only depending on the edge I used before which egdes will change.<br>\nThat means, if I go from vertex $v_1$ to $v_x$, edges connected to $v_x$ will change in the way $b_{1,x}$. if I arrived $v_x$ via $v_2$, edges will change in the way $b_{2,x}$ and so on.  </p>\n\n<p>I am looking for a efficient algorithm telling me if there is a hamiltonian path and output that if so. Has anybody ideas on how to approach that? Or has anyone an idea if this can be transformed into another better known problem?</p>\n", 'ViewCount': '133', 'Title': 'Hamiltonian path in dynamic graph', 'LastActivityDate': '2012-11-30T11:59:23.053', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4769', 'Tags': '<algorithms><graph-theory><reductions>', 'CreationDate': '2012-11-30T11:59:23.053', 'Id': '7046'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to understand a proof regarding radix-sort but to no avail.</p>\n\n<p>I'll first write down a brief summary of the proof and then assign some questions which I hope will be clarified enough.</p>\n\n<blockquote>\n  <p>Suppose you have an array of integer numbers in the range $\\{0,1,2,\\ldots n \\log n\\}$.\n  Show and explain an effective algorithm in the worst case which finds out if there are two elements with the same value.\n  note - You can use an extra memory in order of magnitude equals to $O(n)$.</p>\n</blockquote>\n\n<p>I don't really care about how the algorithm will look like. The idea to this proof is using radix-sort in $O(n)$ and then looking for two elements with the same value as the array is sorted.</p>\n\n<p><strong>The proof outline:</strong></p>\n\n<p>Let's suppose we are examining a larger domain which is $\\{0,1,2 ,\\ldots n^2 - 1\\}$.\nNow we'll treat each number according to its binary representation using radix-sort bit by bit.</p>\n\n<p>Right after this, comes the part I can't understand at all....</p>\n\n<p>As we know the order of magnitude of radix-sort is $\\Theta(d(n+k))$ and therefore all we have to decide is which a to choose to have order of magnitude equals to $O(n)$.</p>\n\n<p>using the formula $\\frac{2\\log n}{a}  (n + 2^a)$.</p>\n\n<p>After this step, you just choose $a = \\log n$ and you are done.</p>\n\n<p><strong>My questions are:</strong></p>\n\n<ol>\n<li><p>How did the writer concluded the following formula: \n$\\frac{2\\log n}{a}  (n + 2^a)$?</p></li>\n<li><p>Also, why do the writer prefer to work on the domain of $\\{0,1,2 ,\\ldots n^2 - 1\\}$ instead the one given in the question?</p></li>\n</ol>\n", 'ViewCount': '310', 'Title': 'Radix sort exercise', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-02T08:16:39.277', 'LastEditDate': '2012-12-01T09:36:18.030', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7100', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4514', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2012-11-30T18:38:03.813', 'Id': '7051'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can you perform the clique decision algorithm fewer than $ O(n) $ times to solve clique optimization?</p>\n\n<p>I\'m not sure if my approach is right but this is my thought process: you would pick vertices in a graph and see if they form a clique, then keep picking more vertices until you have the max possible clique.</p>\n\n<p>I\'m not sure how it can be done less than $ O(n) $ times.</p>\n\n<p>I can imagine an undirected graph such as:</p>\n\n<p><img src="http://i.stack.imgur.com/GptO1.png" alt="undirected graph"></p>\n\n<p>where $ \\{A, B, C\\} $ and $ \\{B, C, D\\} $ would be cliques. The number of vertices is 4, and the number of vertices in the cliques is 3, which is $ n - 1 $. Would this count as being done in less than $ O(n) $ times, or is this the wrong approach to this problem?</p>\n', 'ViewCount': '100', 'Title': 'Using Clique decision to solve Clique optimization', 'LastEditorUserId': '4689', 'LastActivityDate': '2012-12-02T03:39:34.423', 'LastEditDate': '2012-12-02T03:39:34.423', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7053', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4689', 'Tags': '<algorithms><graph-theory><optimization>', 'CreationDate': '2012-11-30T20:10:46.630', 'Id': '7052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have found an efficient algorithm for verifying if a string $\\omega$ is of the form $0^n1^n$, where $n \\in \\mathbb{N}$.</p>\n\n<ol>\n<li>Scan across $\\omega$. If a 1 appears before a 0, then reject.</li>\n<li><p>Repeat so long as some 0s and some 1s remain on the tape.</p>\n\n<ol>\n<li>Scan across $\\omega$. If the total number of 0s and 1s remaining is odd, reject.</li>\n<li>Scan across $\\omega$. Cross out every other 0 starting with the first 0.</li>\n<li>Scan across $\\omega$. Cross out every other 1 starting with the first 1.</li>\n</ol></li>\n<li><p>If no 0s and 1s remain in $\\omega$, accept. Otherwise, reject.</p></li>\n</ol>\n\n<p>I generally see how this algorithm is efficient. It gets rid of half of all 1s and 0s every iteration. How does it work though? Why must we reject if the total number of 0s and 1s remaining in $\\omega$ is odd?</p>\n', 'ViewCount': '55', 'Title': 'How does this algorithm for verifying if a string is $0^n1^n$ work?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-01T13:59:35.667', 'LastEditDate': '2012-12-01T13:59:35.667', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7057', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><formal-languages>', 'CreationDate': '2012-11-30T22:54:59.930', 'Id': '7055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The intervals are represented as two numbers, e.g. $(4.3, 5.6)$. The intervals are unique. </p>\n\n<p>If for $(x,y)$ and $(u,v)$, $x\u2264u$ and $v\u2264y$, $(u,v)$ is nested in $(x,y)$</p>\n\n<p>How do I find out which intervals are nested in others efficiently?</p>\n\n<p>The hint is to use two induction hypotheses: one to assume you can solve the problem for $n-1$ intervals, another that you can find the largest right endpoint for the $n-1$ intervals.</p>\n\n<p>How do I use this information to do the inductive step in constant time?</p>\n', 'ViewCount': '507', 'Title': 'Finding nested intervals efficiently', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-12-24T18:41:15.210', 'LastEditDate': '2012-12-02T06:12:43.807', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '7104', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><induction>', 'CreationDate': '2012-12-01T09:02:33.200', 'Id': '7073'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Inspired by this <a href="http://stackoverflow.com/questions/9452701/ukkonens-suffix-tree-algorithm-in-plain-english">question on Suffix Trees</a>, and the fabulous winning answer, I would hereby ask for a similar explanation of Myers\' <a href="http://www.gersteinlab.org/courses/452/09-spring/pdf/Myers.pdf" rel="nofollow">Fast Bit-Vector Algorithm for Approximate String\nMatching Based on Dynamic Programming</a>.</p>\n', 'ViewCount': '106', 'Title': "Myers' Fast bit-vector algorithm in plain English?", 'LastActivityDate': '2012-12-01T11:17:18.233', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4810', 'Tags': '<algorithms><strings>', 'CreationDate': '2012-12-01T11:17:18.233', 'Id': '7078'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can any body help me for new idea in AI (artificial intelligence) I can implemented in master thesis ?</p>\n\n<p>For example AI with Networks ?\nAI with Email System\nAI with GIS</p>\n', 'ViewCount': '331', 'ClosedDate': '2012-12-01T23:08:34.190', 'Title': 'Help in find new topic for master thesis', 'LastActivityDate': '2012-12-01T15:40:26.163', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4827', 'Tags': '<algorithms><programming-languages>', 'CreationDate': '2012-12-01T14:45:32.300', 'Id': '7082'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for O(V+E) algorithm for finding the <a href="http://en.wikipedia.org/wiki/Transitive_reduction">transitive reduction</a> given a DAG. </p>\n\n<p>That is remove as many edges as possible so that if you could reach v from u, for arbitrary v and u, you can still reach after removal of edges.</p>\n\n<p>If this is a standard problem, please point me to some model solution.</p>\n', 'ViewCount': '475', 'Title': 'Transitive reduction of DAG', 'LastEditorUserId': '4717', 'LastActivityDate': '2012-12-07T05:06:19.067', 'LastEditDate': '2012-12-02T06:11:56.987', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4717', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-12-02T05:18:30.017', 'FavoriteCount': '1', 'Id': '7096'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In famous Structure and Interretation of Computer Programs, there is an exercise (<a href="http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#%_thm_1.14" rel="nofollow">1.14</a>), that asks for the time complexity of the following algorithm - in Scheme - for counting change (the problem statement suggests drawing the tree for <code>(cc 11 5)</code> - which looks <a href="http://telegraphics.com.au/~toby/sicp/ex1-14.svg" rel="nofollow">like this</a>):</p>\n\n<pre><code> ; count change\n (define (count-change amount)\n   (define (cc amount kinds-of-coins)\n     (cond ((= amount 0) 1)\n           ((or (&lt; amount 0) (= kinds-of-coins 0)) 0)\n           (else (+ (cc (- amount\n                           (first-denomination kinds-of-coins))\n                        kinds-of-coins)\n                    (cc amount\n                        (- kinds-of-coins 1))))))\n   (define (first-denomination kinds-of-coins)\n     (cond ((= kinds-of-coins 1) 1)\n           ((= kinds-of-coins 2) 5)\n           ((= kinds-of-coins 3) 10)\n           ((= kinds-of-coins 4) 25)\n           ((= kinds-of-coins 5) 50)))\n   (cc amount 5))\n</code></pre>\n\n<p>Now... there are sites with solutions to the SICP problems, but I couldn\'t find any easy to understand proof for the <em>time</em> complexity of the algorithm - there is a mention somewhere that it\'s polynomial <code>O(n^5)</code></p>\n', 'ViewCount': '120', 'Title': 'Time complexity for count-change procedure in SICP', 'LastActivityDate': '2012-12-02T18:19:47.847', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7106', 'Score': '0', 'OwnerDisplayName': 'NeuronQ', 'PostTypeId': '1', 'OwnerUserId': '4844', 'Tags': '<algorithms><time-complexity><space-complexity>', 'CreationDate': '2012-11-22T20:09:19.883', 'Id': '7105'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>We are given two strings $x=x_1,x_2,x_3,\\ldots,x_m$ and $y=y_1,y_2,y_3,\\ldots,y_n$ over some finite alphabet.\n  We consider the problem of converting $x$ to $y$. Using the following operations:</p>\n  \n  <p>1.Substitution: replace one symbol by another one.</p>\n  \n  <p>2.Insertion: inserts one symbol</p>\n  \n  <p>3.Deletion: delete one symbol.</p>\n</blockquote>\n\n<p>For example, if $x$="logarithm" and $y$="algorithm", we convert $x$ to $y$ in the following way:</p>\n\n<ol>\n<li><p>start with "logarithm"</p></li>\n<li><p>inserting "a"at the front gives "alogarithm".</p></li>\n<li><p>deleting "o"gives "algarithm"</p></li>\n<li><p>replacing the second "a"by "o"gives "algorithm".</p></li>\n</ol>\n\n<p>The similarity problem between the string $x$ and $y$ is defined to be the minimum number of operations needed to convert $x$ to $y$.</p>\n\n<p>For example, the similarity between $x$="logarithm" and $y$="algorithm" is 3, because $x$ can be converted to $y$ using three operations. If the string $x$ has length $m$ and the string $y$ is empty, then the similarity between $x$ and $y$ is similar to $m$.</p>\n\n<p>Give a dynamic programming algorithm (in pseudocode) that computes, in $\\mathcal o(mn)$ time, the similarity between the string $x$ and $y$.</p>\n\n<p>It is as the edit distance problem but there is the corresponding minimization problem problem where we measure similarity instead of distance .  </p>\n', 'ViewCount': '400', 'Title': 'String similarity problem', 'LastEditorUserId': '3094', 'LastActivityDate': '2012-12-03T10:17:05.547', 'LastEditDate': '2012-12-02T20:38:06.193', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4828', 'Tags': '<algorithms><reference-request><dynamic-programming><strings>', 'CreationDate': '2012-12-02T19:47:18.867', 'Id': '7109'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The Clique problem takes a graph $G = (V,E)$ and an integer $k$ and asks if $G$ contains a clique of size $k$. (A clique is a set of vertices such that every pair of vertices in the set is adjacent.) The Independent-Set problem takes a graph $G\u2019 = (V\u2019,E\u2019)$ and an integer $k\u2019$ and asks if $G\u2019$ contains an independent set of size $k\u2019$. (An independent set is a set of vertices such that no pair of vertices in the set is adjacent.)</p>\n\n<ol>\n<li><p>Give a polynomial time algorithm that, given a graph $G$ and an integer $k$ produces a graph $G\u2019$ and an integer $k\u2019$ such that $G$ has a clique of size $k$ if and only if $G\u2019$ has an independent set of size $k\u2019$. Justify your answer.</p></li>\n<li><p>Use 1. to prove that the Independent-Set problem is NP-Complete given that the Clique problem is NP-Complete.</p></li>\n</ol>\n', 'ViewCount': '2074', 'Title': 'Reducing Clique to Independent Set', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-12-03T19:13:11.427', 'LastEditDate': '2012-12-03T18:47:25.250', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4828', 'Tags': '<algorithms><complexity-theory><graph-theory><np-complete>', 'CreationDate': '2012-12-03T10:46:11.947', 'Id': '7120'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve studied C++ for about a year now independently, having an initial goal of the usual "i want to program video games" to a new found goal of pursuing computer science as a full time career. Recently I\'ve been coding in C and seem to really enjoy the sytax and semantics a lot more, but i\'m worried that because of C\'s age it could be a dying language.</p>\n\n<p>The main area\'s of study that interest me would be Algorithms, Mobile Applications and OS development on relatively cross platform with high efficiency. What language would provide a more solid base to work off of in the future to accomplish some of these goals? What would be the advantages/disadvantages of each language?</p>\n', 'ViewCount': '152', 'ClosedDate': '2012-12-03T16:56:50.860', 'Title': 'Which would provide a stronger base for future learning? C vs C++', 'LastActivityDate': '2012-12-03T21:20:11.480', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4851', 'Tags': '<algorithms><programming-languages><operating-systems>', 'CreationDate': '2012-12-03T11:25:33.640', 'Id': '7122'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was practicing the following problem :</p>\n\n<blockquote>\n  <p>There are a total of $N$ operations.\n  At each operation, you can either add an element to the top or remove several elements as described below.</p>\n  \n  <p>Inputs are integers. An input other than $-1$ indicates that we have to add the elements in last in first out fashion (LIFO). If the input is $-1$, then we have to remove (pop) all the elements that lie above the current minimum element of the stack, and then pop that minimum element.</p>\n  \n  <p>For each $-1$ in the inputs, print $(m,p)$ where $m$ is the minimum element on the stack and $p$ counts how many total elements we popped from the stack.</p>\n</blockquote>\n\n<p>Example :</p>\n\n<pre><code>N=14 (Total 4 operations)\n9 \n6\n8\n-1\n2\n0\n6\n-1\n3\n1\n2\n10\n5\n-1\n\nIn above example : First three operation is inserting operation ,\nthat is we need to insert them in LIFO Fashion.\nSo the stack after third operation is : [9  6   8*] \n    PLEASE NOTE * represent the element at the top  of the stack.\nFourth Operation is -1 , that is we have to remove all elements\n(including the minimum ) , that lie above the minimun elements.\nThe Minimum Element is 6 ,so we remove 8 and 6 and Hence the stack now is :\n[9*]\nSo answer for fourth operation is (6,2)\n6 - the minimum element in the stack and 2 ,as we removed \ntotal 2 elements from the stack.\n\nOperation 5 th ,6 th and 7th are Inserting operations.\nAfter 4 th operation , the stack was [9*]\nAfter 7th operation , the stack looks like [9 2 0 6*]\n8th operation is -1 . Minimum element is 0 ,so we should remove 0,6 \n    from the stack\nHence Answer is (0,2)\nAs the minimum element is "0" and we removed total "2" elements from\n    the stack.\n\nThe  stack at the end of eighth operation is:\n[9 2]\n\nOperation 9 ,10 ,11 12,13  are inserting operations\nSo Stack after 13th operation is:\n[9 2 3 1 2 10 5* ]\noperation 14 is -1.\nThe minimum element  in the stack is 2. However 2 lies at two \n    different positions in the stack .\nBut we should remove that  2 , which is nearest to the top of the stack \n(In order word ,if the minimum lies at more than two positions in the \n    stack ,then the one which is closer to the top of the \nstack is considered).\nSo remove 2 at Index 5 (as it is closer to the top of stack) and element\nthat lies above it in stack.\nSo after removing 2(the minimum element ) and 10,5 (the elements above \n    the min element) \nThe stack looks like:\n[9 2 3 1*]\n    Answer is (2,3) //As 2 is the min element and we popped "3" elements\n\nPLEASE NOTE * represent the element at the top  of the stack.\n</code></pre>\n\n<h2>My approach</h2>\n\n<p>I am using a simple stack for the above problem. But the constraint is high: $1 \\le N \\le 10^6$. There are many $-1$\'s in the input, so a simple stack will work very slowly. The time limit for the problem is just 1 second.</p>\n', 'ViewCount': '345', 'Title': 'Efficient algorithm for a modified stack to pop the smallest element', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-03T20:51:13.180', 'LastEditDate': '2012-12-03T20:51:13.180', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7127', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><arrays><stack>', 'CreationDate': '2012-12-03T13:21:05.410', 'Id': '7124'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/6597/proof-of-linear-search">Proof of linear search?</a>  </p>\n</blockquote>\n\n\n\n<p>I\'m reading the MIT Press, Introduction to Algorithms textbook 3rd edition, and I am a bit confused by an exercise. </p>\n\n<p>2.1-3\nConsider the searching problem:\nInput: A sequence of n numbers A =  and a value v.\nOutput: An index i such that v = A[i] or the special value NIL if v does not appear in A.</p>\n\n<p>Write pseudocode for linear search, which scans through the sequence looking for v. Using a loop invariant, prove that your algorithm is correct. Make sure your loop invariant fulfills the three necessary properties.</p>\n\n<p>Earlier in the book I read that "We must show three things about a loop invariant:</p>\n\n<p>Initialization: It is true prior to the first iteration of the loop.</p>\n\n<p>Maintenance: If it is true before an iteration of the loop, it remains true before the\nnext iteration.</p>\n\n<p>Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.\n"</p>\n\n<p>I wrote this as my pseudocode</p>\n\n<pre><code>index = nil\nj = 1 \nwhile j &lt;= A.length and index == nil\n  if A[j] == v\n    index = j\n  j = j + 1\n</code></pre>\n\n<p>But when it comes to proving using a loop invariant I have no idea what to do. What do I even use as a loop invariant? To be honest I\'m not really clear about the entire loop invariant concept. Can someone please help me understand this? </p>\n\n<p>Thanks.</p>\n', 'ViewCount': '38', 'ClosedDate': '2012-12-04T17:03:05.747', 'Title': 'loop invariant proof', 'LastEditorUserId': '4648', 'LastActivityDate': '2012-12-04T03:56:29.240', 'LastEditDate': '2012-12-04T03:56:29.240', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'the.alch3m1st', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><loop-invariants>', 'CreationDate': '2012-12-03T20:11:11.017', 'Id': '7138'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://en.wikipedia.org/wiki/Fibonacci_number" rel="nofollow">Binet\'s formula</a> for the nth Fibonacci numbers is remarkable because the equation "converts" via a few arithmetic operations an irrational number $\\phi$ into an integer sequence. However, using finite precision arithmetic, one would always have some (small) roundoff error. </p>\n\n<blockquote>\n  <p>Is there a discussion/description somewhere of how to calculate the Fibonacci sequence using Binet\'s formula (ie <em>not</em> the recurrence relation) and floating point arithmetic which results in no roundoff error?</p>\n</blockquote>\n', 'ViewCount': '277', 'Title': "Calculating Binet's formula for Fibonacci numbers with arbitrary precision", 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-04T09:51:48.217', 'LastEditDate': '2012-12-04T04:02:54.733', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7150', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><discrete-mathematics><floating-point><arithmetic><mathematical-programming>', 'CreationDate': '2012-12-04T03:33:25.020', 'Id': '7145'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was solving the following problem. But I am not able to think of an efficient algorithm for this modified version of problem. The problem statement is:</p>\n\n<blockquote>\n  <p>We are given <strong>K Rectangles</strong>. The dimensions of $x$-th Rectangle is ($N_x \\times M_x$), where $1\\leq x \\leq K$.\n  From each rectangle $x$ , Alice Cuts  a rect. of dimension ($P_x \\times Q_x$), where $1 \\leq x \\leq K$ ,\n  from the <strong>Top-Right Corner</strong> and trashes the cut portion.</p>\n  \n  <p>Initially Alice placed a robot, at the <strong>top left corner</strong> of <strong>each</strong> rectangle. He is very interested to find the <em>number of ways, each robot</em> can reach the bottom-right corner (target) using the following rules:</p>\n  \n  <ul>\n  <li>The robot can only move 1 unit <em>right</em>* or the robot can only move 1 unit <em>downward</em>.</li>\n  <li>The robot <strong>cannot</strong> move upward, <strong>can't</strong> move even <em>left</em> and <strong>can't</strong> move even <em>diagonally</em>.</li>\n  <li>The robot can move on rectangle boundary.</li>\n  </ul>\n  \n  <p>The number of ways can be very large. Thus, Answer = (Number of ways) mod 10^9+7.</p>\n</blockquote>\n\n<p>Constraints is very large:</p>\n\n<pre><code>1&lt;=K&lt;=10\n2&lt;=(Nx,Mx)&lt;=5*10^5\n1&lt;=Px&lt;Nx\n1&lt;=Qx&lt;Mx\n</code></pre>\n\n<p>The Time Limit is just 1 second.</p>\n\n<p>Example:</p>\n\n<pre><code>K=1\n\nN1=2 M1=2\n\nP1=1 Q1=1\n</code></pre>\n\n<p>Answer: 5 ways</p>\n\n<p>I had <strong>solved the easier version</strong> of this problem (Using Pascal triangle + Combinatorics), when no portion of rectangle is cut/removed. But I don't know how to solve the above modified problem, where a small rectangle is cut from top-right Corner of the original rectangle.</p>\n", 'ViewCount': '325', 'Title': 'Modified paths Counting in a Rectangle', 'LastEditorUserId': '140', 'LastActivityDate': '2012-12-07T15:41:08.113', 'LastEditDate': '2012-12-04T18:01:37.250', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><combinatorics><dynamic-programming>', 'CreationDate': '2012-12-04T10:28:44.047', 'Id': '7152'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a graph $G = (V,E)$, where $|V| = n$. What is a fast algorithm for generating the collection of all 2-hop neighborhood lists of all nodes in $V$. </p>\n\n<p>Naively, you can do that in $O(n^3)$. With power of matrices, you can do that with $O(n^{2.8})$ using Strassen algorithm. You can do better than this using another matrix multiplication algorithm. Any better method ? Any Las Vegas algorithm ? </p>\n', 'ViewCount': '422', 'Title': 'Algorithm to find all 2-hop neighbors lists in a graph', 'LastActivityDate': '2012-12-06T15:48:01.947', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '7183', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '867', 'Tags': '<algorithms><algorithm-analysis><graphs><randomized-algorithms>', 'CreationDate': '2012-12-05T04:50:09.537', 'Id': '7169'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '326', 'Title': 'Determine whether the $k^{th}$ smallest element in max-heap is greater than a given number', 'LastEditDate': '2012-12-06T06:00:55.747', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4878', 'FavoriteCount': '1', 'Body': '<p>A set of numbers is stored in a <strong>max-heap</strong>. We want to find an algorithm with $O(k)$ time complexity to check if $k^{th}$ <strong>smallest</strong> element is greater than an arbitrary given number.</p>\n', 'Tags': '<algorithms><data-structures><heaps>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-24T13:40:07.560', 'CommentCount': '11', 'AcceptedAnswerId': '7660', 'CreationDate': '2012-12-05T09:27:05.190', 'Id': '7173'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the multiway cut problem, the input is an undirected graph $G= (V, E)$ and set of terminal nodes $s_1, s_2,\\ldots s_k$ are in $V$. The goal is to find a minimum\nset of edges in $E$ whose removal leaves all terminals in different components.</p>\n\n<ol>\n<li><p>How do we show that this problem can be solved exactly in polynomial time when\n$k= 2$?</p></li>\n<li><p>How do we get an approximation algorithm with ratio at most 2 for the case when $k \\geq 3$?</p></li>\n</ol>\n', 'ViewCount': '353', 'Title': '2OPT Approximation Algorithm for Multiway Cut Problem', 'LastEditorUserId': '19', 'LastActivityDate': '2013-04-02T15:47:46.743', 'LastEditDate': '2013-04-02T14:17:26.360', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'lam lae', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><approximation><network-flow>', 'CreationDate': '2012-12-02T22:34:14.450', 'Id': '7205'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I had the choice of choosing one out of the following two optimization problems which I could use to solve my problem. Which choice is the fastest? How much of a trade-off would it be?  Is the improvement in speed by many factors!?</p>\n\n<ol>\n<li><p>Minimizing a convex function $L(X)$ in one matrix variable with orthogonality constraints over the matrix-essentially in my case this ends up to solving an eigen-decomposition.</p></li>\n<li><p>Minimizing the same convex function $L(X)$ with linear constraints in $X$.</p></li>\n</ol>\n\n<p>I know that 2.) should be faster. But what is the direction of work I need to do- to compare the improvement in speed-especially in terms of using the fastest available eigen solver for 1.)-what would be the corresponding fastest approach to solve 2.)?</p>\n', 'ViewCount': '61', 'Title': 'Time - Complexity Convex Optimization and Eigen Decomposition', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T00:32:56.540', 'LastEditDate': '2012-12-06T10:17:22.360', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'VSPC', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity><optimization><efficiency><linear-algebra>', 'CreationDate': '2012-10-16T18:17:29.447', 'Id': '7206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '258', 'Title': 'Find maximum distance between elements given constraints on some', 'LastEditDate': '2013-01-14T22:06:36.387', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4896', 'FavoriteCount': '1', 'Body': '<p>I have a list of numbered elements 1 to N that fit into positions on a number line starting with 1. I also have constraints for these elements:</p>\n\n<ul>\n<li>The element 1 is in position 1, and element N must be at a position >= the position of element N-1. (i.e. element 2 could be at position 1, element 3 at position 7, and element 4 at position 8 (but not position 5))</li>\n<li>Some elements must be within a certain distance from each other on the line.</li>\n<li>Some elements must be at least a certain distance from other on the line.</li>\n</ul>\n\n<p>My objective is to return an integer that represents the maximum span between element 1 and element N. If no lineup is possible, return -1, and if the elements can be any distance apart, return -2. </p>\n\n<p>I am given:</p>\n\n<ul>\n<li>The number of elements</li>\n<li>A withinArray[][] where withinArray[x][y] = the distance elements x and y must be within on the line. Any zero values represent no constraints.</li>\n<li>An atLeastArray[][] where atLeastArray[x][y] = the distance elements x and y must be apart on the line. Any zero values represent no constraints.</li>\n</ul>\n\n<p>An example input would be: 4 elements, withinArray<a href="http://stackoverflow.com/questions/13714903/find-maximum-distance-between-elements-given-constraints-on-some">1</a>[3] = 10, withinArray[2][4] = 20, and atLeastArray[2][3] = 3. (all other array values are zero).</p>\n\n<p>The return value for this input would be 27. (element 1 at position 1, element 2 at position 8, element 3 at position 11, and element 4 at position 28)</p>\n\n<p>The problem was first posted <a href="http://stackoverflow.com/questions/13714903/find-maximum-distance-between-elements-given-constraints-on-some">here</a> by someone else. I\'d like to figure out an elegant solution to it programmatically. Though I\'ve been working on it for a whole day, I still have no luck coming up with a good solution. I feel that I need to use dynamic programming techniques, but have a hard time finding a good substructure. </p>\n\n<p>I am not able to locate the same example on the web. Any pointer to such materials online would be appreciated. It\'s even better if you are an expert for such kind of question and can outline the solution in detail. Executable code is a plus.</p>\n', 'Tags': '<algorithms><optimization><dynamic-programming><linear-programming>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-14T22:06:36.387', 'CommentCount': '0', 'AcceptedAnswerId': '7213', 'CreationDate': '2012-12-06T10:22:20.080', 'Id': '7208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let\'s say we have 10 people, each with a list of favorite books. For a given person X, I would like to find a special subset of X\'s books liked only by X, i.e. there is no other person that likes all of the books in X\'s special subset. I think of this special subset as a unique "fingerprint" for X. </p>\n\n<p>I would appreciate suggestions on an approach for finding such sets. (While this reads like a homework problem, it is related to a problem in my biology research that I am trying to solve.)</p>\n', 'ViewCount': '137', 'Title': 'Finding "fingerprint" sets', 'LastActivityDate': '2012-12-08T03:19:52.813', 'AnswerCount': '4', 'CommentCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '4927', 'Tags': '<algorithms><sets>', 'CreationDate': '2012-12-06T15:54:53.583', 'FavoriteCount': '1', 'Id': '7210'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to solve a graph problem (it's not for homework, just to practice my skills). A DAG $G(V,E)$ is given, where $V$ is the set of vertices and $E$ the edges. The graph is represented as an adjacency list, so $A_v$ is a set containing all the connections of $v$. My task is to find which vertices are reachable from each vertex $v\\in V$. The solution I use has a complexity of  $O(V^3)$, with transitive closure, but i read that in a blog it can be faster, although it didn't reveal how. Could anyone tell me another way (with better complexity) to solve the transitive closure problem in a DAG? </p>\n", 'ViewCount': '1090', 'Title': 'Efficient algorithm for retrieving the transitive closure of a directed acyclic graph', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-08T07:54:26.363', 'LastEditDate': '2012-12-07T17:07:54.007', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4916', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2012-12-07T16:11:31.303', 'FavoriteCount': '1', 'Id': '7231'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '904', 'Title': 'Retrieving the shortest path of a dynamic graph', 'LastEditDate': '2012-12-10T15:45:53.627', 'AnswerCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '4916', 'FavoriteCount': '3', 'Body': "<p>I'm studying shortest paths in directed graphs currently. There are many efficient algorithms for finding the shortest path in a network, like dijkstra's or bellman-ford's. But what if the graph is dynamic? By saying dynamic I mean that we can insert or remove vertices during the execution of the program. I'm trying to find an efficient algorithm for updating the shortest paths from a vertex $v$ to every other vertex $u$, after inserting an edge $e$, without needing to run the shortest path algorithm in the new graph again. How can I do this? Thanks in advance.</p>\n\n<ul>\n<li><em>Note:</em> the changes can be done after the first iteration of the algorithm</li>\n<li><em>Note[2]:</em> two nodes are given, $s$ the source and $t$ the target. I need to find the shortest path between these nodes. When the graph is updated I only have to update $\\pi(s,t)$, which is the shortest path between $s$ and $t$.</li>\n<li><em>Note[3]:</em> I'm only interested in the edge insertion case.</li>\n</ul>\n\n<blockquote>\n  <p><strong>A formal definition</strong>: Given a graph $G = (V,E)$. Define an <em>update operation</em> as 1) an insertion of an edge $e$ to $E$ or 2) a a deletion of an edge $e$ from $E$. The objective is to find efficiently the cost of all pairs shortest paths after an update operation. By efficiently, we mean at least better than executing an All-Pairs-Shortest-Path algorithm, such as Bellman-Ford algorithm, after each update operation.</p>\n</blockquote>\n\n<hr>\n\n<p><strong>Edit:</strong> Below there is a simplified version of the problem:</p>\n\n<blockquote>\n  <p>A weighted graph $G(V,E)$ is given, consisting of unidirectional edges, and two critical vertices $s$ and $t$. A set $C$ of candidate <em>bidirectional</em> edges is also given. I have to build an edge $(u,v) \\in C$ to minimize the distance from $s$ to $t$.</p>\n</blockquote>\n", 'Tags': '<algorithms><data-structures><graphs><efficiency><shortest-path>', 'LastEditorUserId': '4916', 'LastActivityDate': '2013-06-12T10:15:55.483', 'CommentCount': '7', 'AcceptedAnswerId': '7260', 'CreationDate': '2012-12-08T11:22:28.103', 'Id': '7250'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Count-Min_sketch">Count-Min Sketch</a> is an awesome data structure for estimating the frequencies of different elements in a data stream.  Intuitively, it works by picking a variety of hash functions, hashing each element with those hash functions, and incrementing the frequencies of various slots in various tables.  To estimate the frequency of an element, the Count-Min sketch applies the hash functions to those elements and takes the minimum value out of all the slots that are hashed to.</p>\n\n<p>The <a href="http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf">original paper on the Count-Min Sketch</a> mentions that the data structure requires pairwise independent hash functions in order to get the necessary guarantees on its expected performance.  However, looking over the structure, I don\'t see why pairwise independence is necessary.  Intuitively, I would think that all that would be required would be that the hash function be <a href="http://en.wikipedia.org/wiki/Universal_hashing">a universal hash function</a>, since universal hash functions are hash functions with low probabilities of collisions.  The analysis of the collision probabilities in the Count-Min Sketch looks remarkably similar to the analysis of collision probabilities in a chained hash table (which only requires a family of universal hash functions, not pairwise independent hash functions), and I can\'t spot the difference in the analyses.</p>\n\n<p>Why is it necessary for the hash functions in the Count-Min Sketch to be pairwise independent?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '467', 'Title': 'Why does the Count-Min Sketch require pairwise independent hash functions?', 'LastActivityDate': '2012-12-09T21:05:41.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7279', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><data-structures><randomized-algorithms><hash>', 'CreationDate': '2012-12-09T19:52:37.693', 'Id': '7275'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '175', 'Title': 'Smallest string length to contain all types of beads', 'LastEditDate': '2012-12-10T13:39:24.903', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4751', 'FavoriteCount': '1', 'Body': '<p>I read this question somewhere, and could not come up with an efficient answer.</p>\n\n<p>A string of some length has beads fixed on it at some given arbitrary distances from each other. There are $k$ different types of beads and $n$ beads in total on the string, and each bead is present atleast once. We need to find one consecutive section of the string, such that:</p>\n\n<ul>\n<li>that section contains all of the $k$ different types of beads atleast once.</li>\n<li>the length of this section is as small as possible, provided the first condition is met.</li>\n</ul>\n\n<p>We are given the positions of each bead on the string, or alternatively, the distances between each pair of consecutive beads.</p>\n\n<p>Of course, a simple brute force method would be to start from every bead (assume that the section starts from this bead), and go on till atleast on instance of all beads are found while keeping track of the length. Repeat for every starting position, and find the minimum among them. This will give a $O(n^2)$ solution, where $n$ is the number of beads on the string. I think a dynamic programming approach would also probably be $O(n^2)$, but I may be wrong. Is there a faster algorithm? Space complexity has to be sub-quadratic. Thanks!</p>\n\n<p>Edit: $k$ can be $O(n)$.</p>\n', 'Tags': '<algorithms><dynamic-programming><search-algorithms>', 'LastEditorUserId': '4751', 'LastActivityDate': '2012-12-10T15:19:21.183', 'CommentCount': '0', 'AcceptedAnswerId': '7278', 'CreationDate': '2012-12-09T20:04:48.587', 'Id': '7276'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've heard this interview question asked a lot and I was hoping to get some opinions on what good answers might be: You have a large file 10+ GB and you want to find out which element occurs the most, what is a good way to do this? </p>\n\n<p>Iterating and keeping track in a map is probably not a good idea since you use a lot of memory, and keeping track as entries come in isn't the greatest option since when this question is posed the file usually already exists.</p>\n\n<p>Other thoughts I had included splitting the file to be iterated through and processed by multiple threads and then have those results combined, but the memory issue for the maps is still there.</p>\n", 'ViewCount': '1595', 'Title': 'Finding the element that occurs the most in a very large file', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-12T16:03:59.557', 'LastEditDate': '2012-12-10T23:28:51.367', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4989', 'Tags': '<algorithms><arrays>', 'CreationDate': '2012-12-10T06:26:32.387', 'FavoriteCount': '2', 'Id': '7291'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1003', 'Title': 'Can there be a perfect chess algorithm?', 'LastEditDate': '2012-12-11T18:37:17.933', 'AnswerCount': '5', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '4947', 'FavoriteCount': '2', 'Body': "<p>Current chess algorithms go about 1 or maybe 2 levels down a tree of possible paths depending on the player's move's and the opponent's moves. Let's say that we have the computing power to develop an algorithm that predicts all possible movements of the opponent in a chess game. An algorithm that has all the possible paths that opponent can take at any given moment depending on the players moves. Can there ever be a perfect chess algorithm that will never lose? Or maybe an algorithm that will always win?\nI mean in theory someone who can predict all the possible moves must be able to find a way to defeat each and every one of them or simply choose a different path if a certain one will effeminately lead him to defeat.....</p>\n\n<p>edit--\nWhat my question really is. Let's say we have the computing power for a perfect algorithm that can play optimally. What happens when the opponent plays with the same optimal algorithm? That also will apply in all 2 player games with finite number (very large or not) of moves. Can there ever be an optimal algorithm that always wins?</p>\n\n<p>Personal definition: An optimal algorithm is a perfect algorithm that always wins... (not one that never loses, but one that always wins</p>\n", 'Tags': '<algorithms><turing-machines>', 'LastEditorUserId': '4947', 'LastActivityDate': '2012-12-14T02:21:57.283', 'CommentCount': '6', 'AcceptedAnswerId': '7335', 'CreationDate': '2012-12-10T20:36:39.337', 'Id': '7313'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Below is the general code for DFS with logic for marking back edges and tree edges. My doubt is that back edges from a vertex go back and point to an ancestor and those which point to the parent are not back edges (lets assume undirected graph). In an undirected graph we have an edge back and forth between 2 vertices $x$ and $y$. So after visiting $x$ when I process $y$, $y$ has $x$ as an adjacent vertex, but as its already visited, the code will mark it as a back edge. Am I right in saying that? Should we add any extra logic to avoid this, in case my assumption is valid?</p>\n\n<pre><code>DFS(G)\n    for v in vertices[G] do\n         color[v] = white       \n         parent[v]= nil\n         time = 0       \n\n    for v in vertices[G] do\n        if color[v] = white then\n            DFS-Visit(v)\n</code></pre>\n\n<p>Induce a depth-first tree on a graph starting at $v$.</p>\n\n<pre><code>DFS-Visit(v)\n    color[v]=gray\n    time=time + 1\n    discovery[v]=time\n    for a in Adj[v] do\n        if color[a] = white then\n            parent[a] = v\n            DFS-Visit(a)&lt;br&gt;\n            v-&gt;a is a tree edge\n        elseif color[a] = grey then     \n            v-&gt;a is a back edge\n    color[v] = black \n    time = time + 1\n</code></pre>\n\n<p>white means <code>unexplored</code>, gray means <code>frontier</code>, black means `processed' </p>\n", 'ViewCount': '199', 'Title': 'Does the DFS algorithm differentiate between an ancestor and a parent while computing back edges?', 'LastEditorUserId': '4254', 'LastActivityDate': '2012-12-12T15:24:36.360', 'LastEditDate': '2012-12-12T15:24:36.360', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7353', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5038', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-12-11T19:09:53.367', 'Id': '7331'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Facts:</strong> n points in the plane, each has one of k colors, all k colors are represented.</p>\n\n<p><strong>Problem:</strong> You wish to select k points, one of each color, such that the perimeter of the convex hull is as small as possible.</p>\n\n<p><strong>Greedy algorithm:</strong> For each point p, for each color c not equal to p, select the point of color c closest to p. In the end, choose the point set that has a convex hull with the smallest diameter (diameter is the distance between the two points furthest apart.)</p>\n\n<p>Why is the approximation ratio $\\pi/2$?</p>\n\n<p>This was an exercise on my graduate level algorithms exam. We were only given a few lines to answer so it should be simple enough, but I do not know where to start.</p>\n', 'ViewCount': '61', 'Title': 'Show that approximation ratio for a convex hull algorithm is $\\pi/2$', 'LastEditorUserId': '2826', 'LastActivityDate': '2012-12-11T20:16:15.093', 'LastEditDate': '2012-12-11T20:05:41.483', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><approximation><greedy-algorithms>', 'CreationDate': '2012-12-11T19:59:06.100', 'Id': '7333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '437', 'Title': 'Clarification with Kuhn-Munkres/Hungarian Algorithm', 'LastEditDate': '2012-12-12T20:25:07.587', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1025', 'FavoriteCount': '1', 'Body': '<p>I have been attempting to get my mind around the <a href="http://en.wikipedia.org/wiki/Hungarian_algorithm" rel="nofollow">Kuhn-Munkres/Hungarian Algorithm</a>. I have been using the following statement of the algorithm which I found <a href="http://www.math.uwo.ca/~mdawes/courses/344/kuhn-munkres.pdf" rel="nofollow">here</a>.</p>\n\n<p><img src="http://i.stack.imgur.com/sieBq.png" alt="Kuhn-Munkres Algorithm"></p>\n\n<p>Based on my readings on the algorithm, my understanding is that the improvement of the feasible vertex labeling in step 2. is supposed to be such that $G_l \\subset G_l\'$.</p>\n\n<p>The part I\'m getting stuck on is that it seems to me that it is possible for there to be an edge $xy$ in $G_l$ with $y \\in T$ but $x \\not \\in S$ resulting in the fact that\n$$\nl\'(x) + l\'(y) = l(x) + (l(y) + \\alpha_l) \\not = w(xy)\n$$\nand so $ xy \\not \\in G_l\'$ and $G_l \\not \\subset G_l\'$.</p>\n\n<p>Can anyone point out where I\'m going wrong?</p>\n', 'Tags': '<algorithms><bipartite-matching>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-12T20:25:07.587', 'CommentCount': '8', 'AcceptedAnswerId': '7363', 'CreationDate': '2012-12-11T22:26:40.083', 'Id': '7341'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I came across a couple of solutions to one of the problems that is in the CLRS textbook (pg. 637 23.2-5 edition 3).  I am wondering if anyone can make a clarification as to the stated running time of the solution.</p>\n\n<p>Q:  Given that we know the edge weights on a graph are between $1$ and some constant $W$, how fast can we make Prim's algorithm run?</p>\n\n<p>Solution:</p>\n\n<ul>\n<li>Uses an array of linked lists where each index corresponds to a given weight [$1\\dots W + 1$]</li>\n<li>Each link linked lists contains a series of vertices with the weight of the index as their key</li>\n</ul>\n\n<p>The run time given is $O(E)$</p>\n\n<p>They break it down as follows:</p>\n\n<p>$O(1)$ to find the vertex with smallest weight\n       - I understand this part since we scan at most $W$ array slots to find a non-empty list and $W$ is constant.</p>\n\n<p>$O(1)$ for decrease key\n     - This makes sense as far as the actual removing of a link from one list and inserting it into another</p>\n\n<p>My question is about the decrease key step.  Say for example we pull the node $A$ from the 3rd slot, that is the next node to be processed.  We look at $A$'s adjacency matrix and see that it has edges to $B, C, F$.  Since we are using linked lists we have no choice other than to look at all array slots from $4$ to $W$ and see if we can find the vertices listed in the linked list for that particular index. Then delete the vertex from its current list and add it to the correct list if we find it. Possible search time = $O(V)$ [since we store vertices in the linked lists and could search all before finding our desired vertex] not $O(1)$.</p>\n\n<p>Since each decrease key takes $O(V)$ and we process all vertices running time = $O(V^2)$.  </p>\n\n<p>If anyone could let me know where I am going astray in my analysis I would greatly appreciate it.</p>\n", 'ViewCount': '489', 'Title': 'Question about Prims algorithm where weights are between 1 and some constant W', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-14T22:17:34.860', 'LastEditDate': '2013-01-14T22:17:34.860', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'cpowel2', 'PostTypeId': '1', 'OwnerUserId': '5050', 'Tags': '<algorithms><graph-theory><runtime-analysis><spanning-trees>', 'CreationDate': '2012-12-11T18:43:12.797', 'Id': '7345'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given $n \\in \\mathbb{N}$ pairwise distinct numbers. I want to do as few comparisons per number as possible - and every number should experience roughly the same amount of comparisons as another number - and find the greatest $k \\leq n$ numbers, e.g. $k = \\frac{n}{10}$. In order to minimise the number of comparisons per number, I allow an error, say I am okay if I know that the result is correct with only a probability of $p$.</p>\n\n<p>I study mathematics and only had a very shallow introduction to algorithms so far, and the only relevant algorithms we discussed would be sorting algorithms. QuickSort comes to mind, but firstly I am not sure how I would implement my "tolerance of an error" and secondly, I think the fact that I only need the greatest $k$ numbers and I don\'t want to sort them, there might be more efficient algorithms.</p>\n\n<p>Any basic approach would be a great help already, I\'ll gladly do the maths on my own.</p>\n\n<p>Thanks in advance for any help.</p>\n', 'ViewCount': '137', 'Title': 'Finding the greatest $k$ numbers with a certainty of $p$', 'LastActivityDate': '2012-12-13T16:11:37.477', 'AnswerCount': '3', 'CommentCount': '5', 'AcceptedAnswerId': '7362', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5053', 'Tags': '<algorithms>', 'CreationDate': '2012-12-12T17:40:05.653', 'FavoriteCount': '2', 'Id': '7359'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Consider a directed graph $G$ on which one can dynamically add edges and make some specific queries.</p>\n\n<h3>Example: disjoint-set forest</h3>\n\n<p>Consider the following set of queries:</p>\n\n<pre><code>arrow(u, v)\nequiv(u, v)\nfind(u)\n</code></pre>\n\n<p>the first one adds an arrow $u\u2192v$ to the graph, the second one decides if $u\u2194^*v$, the last one finds a canonical representative of the equivalence class of $\u2194^*$, i.e. a $r(u)$ such that $u\u2194^*v$ implies $r(v)=r(u)$.</p>\n\n<p>There is a <a href="http://en.wikipedia.org/wiki/Disjoint-set_data_structure#Disjoint-set_forests">well-known algorithm using the disjoint-set forest data structure</a> implementing these queries in quasi-constant amortized complexity, namely $O(\u03b1(n))$. Note that in this case <code>equiv</code> is implemented using <code>find</code>.</p>\n\n<h3>More complex variant</h3>\n\n<p>Now I\'m interested in a more complex problem where the directions do matter:</p>\n\n<pre><code>arrow(u, v)\nconfl(u, v)\nfind(u)\n</code></pre>\n\n<p>the first adds an arrow $u\u2192v$, the seconds decides if there is a node $w$ reachable from both $u$ and $v$, i.e. $u\u2192^*\u2190^*v$. The last one should return an object $r(u)$ such that $u\u2192^*\u2190^*v$ implies $r(u) \\bullet r(v)$ where $\\bullet$ should be easily computable. (In order to, say, computes <code>confl</code>). The goal is to find a good data structure such that these operations are fast.</p>\n\n<h3>Cycles</h3>\n\n<p>The graph can contain cycles.</p>\n\n<p>I don\'t know if there is a way to efficiently <em>and incrementally</em> compute the strongly connected components, in order to only consider DAGs for the main problem.</p>\n\n<p>Of course I would appreciate a solution for DAGs, too. It would correspond to an incremental computation of the least common ancestor.</p>\n\n<h3>Naive approach</h3>\n\n<p>The disjoint-set forest data structure is not helpful here, since it disregards the direction of the edges. Note that $r(u)$ cannot be a single node, in the case the graph is not confluent.</p>\n\n<p>One can define $r(u)=\\{v \u2223 u\u2192^*v\\}$ and to define $\\bullet$ as $S_1\\bullet S_2$ when $S_1 \u2229 S_2\u2260\u2205$. But how to compute this incrementally?</p>\n\n<p>Probably that computing such a big set is not useful, a smaller set should be more interesting, as in the usual union-find algorithm.</p>\n', 'ViewCount': '401', 'Title': 'Directed union-find', 'LastEditorUserId': '82', 'LastActivityDate': '2013-01-15T11:49:18.190', 'LastEditDate': '2012-12-15T13:34:20.250', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '82', 'Tags': '<algorithms><graphs><search-algorithms>', 'CreationDate': '2012-12-12T17:52:23.180', 'FavoriteCount': '1', 'Id': '7360'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A question about the clique problem (specifically k-clique). Is there any algorithm that takes advantage of the properties of connected graphs to find cliques of a given size <code>k</code>, if such cliques exist?</p>\n', 'ViewCount': '308', 'Title': 'K-Clique in Connected Graphs', 'LastEditorUserId': '5063', 'LastActivityDate': '2012-12-15T09:48:55.380', 'LastEditDate': '2012-12-14T06:22:29.520', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5063', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-12-14T00:23:50.477', 'Id': '7393'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to find a polynomial time algorithm for finding the minimum vertex cover for a graph. I've written the algorithm below; I know this problem is $\\mathsf{NP}$-hard, which means there are probably some graphs for which this algorithm will not work.</p>\n\n<p>I need some help in finding the flaw in this algorithm and also, an indication for what restrictions should be imposed on graphs such that this algorithm works.</p>\n\n<p>In the algorithm below I have a graph $G=(V,E)$. I also define the $\\text{priority}(v)$ function; in rough terms, it is the number of edges that are covered by vertex $v$. The function has the property that</p>\n\n<p>$$\\sum_{v \\in V} \\text{priority}(v) = \\text{number of edges}.$$</p>\n\n<p>In other words, an edge is counted as covered by only one of its vertices, not both. </p>\n\n<pre><code>Define degree : V -&gt; NaturalNumbers\ndegree(v) = number of edges connected to v, for all v in V\n\nDefine priority : V -&gt; NaturalNumbers\nInitialize priority(v) = 0 for all v in V\n\nFor all (u, v) in E:\n    If degree(u) &gt;= degree(v):\n        priority(u) = priority(u) + 1\n    Else\n        priority(v) = priority(v) + 1\n\nDefine S as the solution to the vertex cover problem\nInitialize S to the empty set\n\nFor all v in V:\n    If priority(v) != 0:\n        Add v to the set S\n\nOutput S as the solution\n</code></pre>\n", 'ViewCount': '219', 'Title': 'For what special cases does this vertex cover algorithm fail or work?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-08-05T17:23:44.537', 'LastEditDate': '2013-03-23T17:44:07.647', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7401', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5063', 'Tags': '<algorithms><graphs><np-complete><np-hard>', 'CreationDate': '2012-12-14T20:39:37.110', 'Id': '7400'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1034', 'Title': 'Count unique increasing subsequences of length 3 in $O(n\\log n)$', 'LastEditDate': '2012-12-15T19:21:39.437', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4751', 'FavoriteCount': '3', 'Body': '<p><strong>Problem</strong>: Given an array of $n$ integers, $A[1 \\dots n]$, such that any integer occurs <em>at most</em> 2 times in the array, we have to find the number of <em>unique</em> increasing subsequences of length 3 (duplicate subsequences must be counted only once). In other words, we have to count the number of unique integers $A[i], A[j], A[k]$ such that $A[i] &lt; A[j] &lt; A[k]$ with $i &lt; j &lt; k$.</p>\n\n<p>I have been stuck on this for quite a while now. I did look up <a href="http://cs.stackexchange.com/questions/1071/is-there-an-algorithm-which-finds-sorted-subsequences-of-size-three-in-on-time">this</a> question which tests for the existence of such a triplet. But I think my question is different because it needs the count, and because it needs unique triplets (upto 1 repetition of any number is allowed).</p>\n\n<p><strong>Idea:</strong> $O(n^2)$ algorithm. For each number, we can scan the remaining array and find out how many unique numbers are greater than it and occur after it. This can be done in $O(n^2)$ by naive brute force. For every pair of numbers ($O(n^2)$), we now have the number of possible triplets that can be formed by using the pair as the first two numbers of the triplet (since we know how many are greater than the second number).</p>\n\n<p>Unfortunately, $O(n^2)$ is too slow, and I need a faster solution - $O(n\\log n)$ or $O(n)$. Space complexity also has to be sub-quadratic. I was thinking along the lines of sorting a copy of the array to find out the relative rank of each element, but could not go any further. Any help or hints are greatly appreciated!</p>\n', 'Tags': '<algorithms><arrays><subsequences>', 'LastEditorUserId': '4751', 'LastActivityDate': '2012-12-17T14:22:11.433', 'CommentCount': '10', 'AcceptedAnswerId': '7434', 'CreationDate': '2012-12-15T11:47:23.423', 'Id': '7409'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am seeking for some popular science videos on computer algorithms for my last lecture delivered in this term. The topic of the videos includes but are limited to algorithm design, algorithm analysis, computational complexity, the application of algorithms, the history of algorithms (even computer science), the story about the people working on the field, and so on.</p>\n\n<p>By ``popular science videos'', I am just ruling out the videos which are focusing on some specific theory and targeted at a special interest group. It will be better to meet the following specifications (not mandatory):</p>\n\n<ul>\n<li>produced by known university</li>\n<li>produced by known corporation, like BBC</li>\n<li>film or record, not just a speech given by one person </li>\n<li>lasts for more than half an hour</li>\n<li>amusing</li>\n</ul>\n\n<p>Any recommendation will be appreciated.</p>\n", 'ViewCount': '82', 'Title': 'Are there some popular science videos on computer algorithms to recommend?', 'LastActivityDate': '2012-12-15T14:28:46.113', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithms><history>', 'CreationDate': '2012-12-15T14:28:46.113', 'FavoriteCount': '1', 'Id': '7411'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '139', 'Title': 'How to find recurrences where Master formula cannot be applied', 'LastEditDate': '2012-12-17T05:27:40.540', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'ajmartin', 'PostTypeId': '1', 'OwnerUserId': '4975', 'Body': '<p>Given: $T(n) = T(\\sqrt{n}) + 1$ (base case $T(x) = 1$ for $x&lt;=2$) </p>\n\n<p>How do you solve such a recurrence?</p>\n', 'Tags': '<complexity-theory><algorithms>', 'LastEditorUserId': '4975', 'LastActivityDate': '2012-12-17T05:27:40.540', 'CommentCount': '2', 'AcceptedAnswerId': '7452', 'CreationDate': '2012-12-16T11:49:40.497', 'Id': '7445'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to understand why the modular exponentiation algorithm has a running time of about $\\log(n)$ where $n$ is the exponent. Here is the algorithm:</p>\n\n<pre><code>function modular_pow(base, exponent, modulus)\n    result := 1\n    while exponent &gt; 0\n        if (exponent mod 2 == 1):\n           result := (result * base) mod modulus\n        exponent := exponent &gt;&gt; 1\n        base = (base * base) mod modulus\n    return result\n</code></pre>\n\n<p>I think it has something to do with the number of bits for the binary representation of the exponent. What is also an intuitive numerical example?</p>\n', 'ViewCount': '165', 'Title': 'Running Time of Modular Exponentiation', 'LastActivityDate': '2012-12-16T20:07:08.770', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7449', 'Score': '1', 'OwnerDisplayName': 'CodeKingPlusPlus', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Tags': '<algorithms>', 'CreationDate': '2012-12-13T22:10:59.500', 'FavoriteCount': '1', 'Id': '7446'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider an undirected graph with a source and a sink vertex. We would like to remove minimum number of vertices in that graph to disconnect any path between source and sink.</p>\n\n<p>My intuition tells me that we can use max-flow, min-cut algorithm to solve this problem. I don't know whether my solution is correct or not. Please help me check it:</p>\n\n<ol>\n<li>Replace each of the undirected edges with a pair of directed edges.</li>\n<li>Replace each vertex $v$ with two vertices $v_\\text{in}$ and $v_\\text{out}$ connected by an edge. all the incoming edges of $v$ will be connected with $v_\\text{in}$, all the outgoing edges of $v$ will be connected with $v_\\text{out}$.</li>\n<li>Try to find a minimum cut $M$. The edges of $M$ refer to the vertices that we need to remove.</li>\n</ol>\n", 'ViewCount': '767', 'Title': 'Remove minimum number of vertices to disconnect the graph', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-17T09:25:17.337', 'LastEditDate': '2012-12-17T09:25:17.337', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5093', 'Tags': '<algorithms><graph-theory><network-flow>', 'CreationDate': '2012-12-17T07:12:38.157', 'Id': '7457'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am trying to understand the <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">distributed 6-color algorithm for vertex coloring</a> (on page 10).</p>\n\n<p>Here is a short description</p>\n\n<p>Idea of the algorithm: We start with color labels that have $\\log n$ bits. In each synchronous round we compute a new label with exponentially smaller size than the previous label, still guaranteed to have a valid vertex coloring.</p>\n\n<p>Let $i$ be the smallest index where $c_v$ and $c_p$ differ;\nthe new label is $i$ (as a bitstring) followed by the bit $c_v(i)$ itself</p>\n\n<pre><code>Grand-parent 0010110000 -&gt; 10010 -&gt; \u2026\nParent       1010010000 -&gt; 01010 -&gt; 111\nChild        0110010000 -&gt; 10001 -&gt; 001\n</code></pre>\n\n<p>The problem I cannot understand this example. Let\'s take Grand-parent($c_p$ = 0010110000) and parent($c_v$ = 1010010000), on the round when $c_v$ receives $c_p$, so we need to change $c_v$. They differ in 5th bit, counting from 0 (5 in binary is 101), so according to definition, $c_p$ is "$101$"+$c_p[5]=1010$, but in example it\'s 01010, what I get wrong?</p>\n', 'ViewCount': '167', 'Title': 'Distributed 6-color Vertex Coloring', 'LastEditorUserId': '1636', 'LastActivityDate': '2012-12-18T07:23:55.717', 'LastEditDate': '2012-12-18T06:01:55.390', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7460', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><distributed-systems><colorings>', 'CreationDate': '2012-12-17T11:30:23.563', 'Id': '7459'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to figure out a greedy algorithm that finds the optimum (minimum) dominating set for any tree in linear time.</p>\n\n<p>So a greedy algorithm to find a dominating set for a general graph is not optimum. It's an approximation of the optimum dominating set. But since this is a tree I am assuming that a greedy algorithm can give the optimum.</p>\n\n<p>What i have so far is:</p>\n\n<p>Select a vertex with the maximum number of adjacent vertices that are not dominated (that is, it's neighbors are either not dominated by one it its neighbors or they are not a dominated vertex themselves). We add this vertex to the dominated set.</p>\n\n<p>We repeat this proceudue until all vertices are either in the dominated set or are neighbors of one of the dominated vertices.</p>\n\n<p>But I am not sure this will give an optimum solution.</p>\n", 'ViewCount': '418', 'Title': 'Greedy Optimum Dominating Set For A Tree', 'LastActivityDate': '2012-12-23T08:11:29.983', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '512', 'Tags': '<graph-theory><greedy-algorithms>', 'CreationDate': '2012-12-17T19:13:27.847', 'Id': '7470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Summation of $O(n)$ from $1\\le k\\le n$.</p>\n\n<p>I think it should be $O(n)$ only. Because we are addition $O(k)$ and maximum order will be $O(n)$. But answer is given as $O(n^2)$.</p>\n\n<p>Correct me if I'm wrong.</p>\n", 'ViewCount': '126', 'Title': 'Summation of $O(n)$ from $1\\le k\\le n$', 'LastEditorUserId': '3016', 'LastActivityDate': '2012-12-19T03:47:23.047', 'LastEditDate': '2012-12-18T22:44:33.537', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '7500', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4763', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2012-12-17T22:29:28.933', 'Id': '7479'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a DAG representing strict partial order where each node is an assignment of variables $V$ to their values $v$. Each arc $(u,w)$ represents a change in one variable value such that $u\\succ w$. </p>\n\n<p>So if I have $n$ binary variables, I will end up with $2^n$ nodes which is exponential to the size of variables $n$. is there any method to store such DAG efficiently? </p>\n', 'ViewCount': '119', 'Title': 'Is there an efficient method to store large DAGs?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-18T18:31:40.867', 'LastEditDate': '2012-12-18T16:32:46.107', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<algorithms><graph-theory><data-structures><partial-order>', 'CreationDate': '2012-12-18T04:30:32.000', 'Id': '7484'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to create an <a href="http://en.wikipedia.org/wiki/Isochrone_map" rel="nofollow">isochrone</a> based on the OpenStreetMap data set. Everything works fine, I extracted data, processed it into a DAG, ran a Dijkstra algorithm over it. The result is a subset of the complete graph. This is an impression of the covered parts of the subset displayed over Google Maps:</p>\n\n<p><img src="http://i.stack.imgur.com/EwERW.png" alt="all vertices of graph"></p>\n\n<p>However, when the area gets larger, the number of reached vertices gets very large and displaying like this gets slow. What I would like to do is convert the set of edges and vertices into a polygon. Basically, this should be posible by removing all of the inner edges, leaving just the edges around the boundary of the area and the edges pointing out from it. I know coordinates for all vertices and approximating each edge as a line would be fine. Larger inner areas should become holes inside the polygon.</p>\n\n<p>My first attempt was to use an geospatial library (in my case the SqlServer spatial extensions), create a multiline from all of the edges and doing an ST_Buffer on it. Turns out to be very slow and memory consuming for large numbers of edges (> 1000)</p>\n\n<p>I was thinking along the lines of finding small polygons in the set (turning left at every turn?) and removing every edge that is part of 2 of these polygons. </p>\n\n<p>Extra image to use in comment below:\n<img src="http://i.stack.imgur.com/M2SWO.png" alt="sample graph"></p>\n', 'ViewCount': '526', 'Title': 'Algorithm for getting the outer boundary of a large graph', 'LastEditorUserId': '5116', 'LastActivityDate': '2013-01-09T12:18:08.840', 'LastEditDate': '2013-01-09T12:18:08.840', 'AnswerCount': '3', 'CommentCount': '3', 'AcceptedAnswerId': '7498', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5116', 'Tags': '<algorithms><computational-geometry><graph-traversal>', 'CreationDate': '2012-12-18T17:46:49.200', 'Id': '7491'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<blockquote>\n  <p>Strips \u2013Stands for STanford Research Institute Problem Solver (1971).</p>\n</blockquote>\n\n<p><strong>STRIPS Pseudo code -</strong></p>\n\n<pre><code>STRIPS(stateListstart, stateListgoals)\n1.Set state = start\n2.Set plan = []\n3.Set stack = goals\n4.while stack is not empty do\n     1.STRIPS-Step()\n5.Return plan\n\nSTRIPS-Step()\nswitch top of stack t:\n1.case this a goal that matches state:\n    1.pop stack\n2.case this an unsatisfied conjunctive-goal:\n    1.select an ordering for the sub-goals\n    2.push the sub-goals into stack\n3.case this a simple unsatisfied goal\n    1.choose an operator op whose add-list matches t\n    2.replace the twith op\n    3.push preconditions of op to stack\n4.case this an operator\n    1.pop stack\n    2.state = state + t.add-list -t.delete-list\n    3.plan = [plan | t]\n</code></pre>\n\n<p>Some explanations - </p>\n\n<p><code>state</code> - is a list of predicates.</p>\n\n<p><code>stack</code> - is a stack which includes both predicates or operations.  </p>\n\n<p><code>[plan | t]</code>  - add the opreration <code>t</code> to list <code>plan</code>. </p>\n\n<p>If the <code>stack</code> gets empty, it means that <code>plan</code> holds the solution plan.</p>\n\n<p>Since the algorithm is running <code>while stack is not empty do</code>, how can I recognize that there is no solution (i.e <code>plan</code>) which led to the <em>goal state</em> ?</p>\n\n<p>Those who are not familiar with this algorithm,  you can see its running example <a href="http://u.cs.biu.ac.il/~haimga/Teaching/AI/lessons/lesson7b-%20STRIPS_blockWorld.pdf" rel="nofollow">here</a>. Brief introduction: imagine a crane which picks boxes and put them each other , so <code>operation</code> can be <code>put box A on box B</code> and <code>state</code> can be <code>box A is on box B</code> .</p>\n', 'ViewCount': '171', 'Title': 'How to recognize a STRIPS planning problem has no solution?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-19T15:00:57.380', 'LastEditDate': '2012-12-19T15:00:57.380', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7507', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4409', 'Tags': '<algorithms><artificial-intelligence>', 'CreationDate': '2012-12-18T23:40:59.437', 'Id': '7495'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given $n$ jobs, schedule them such that the weighted sum is minimum.</p>\n\n<p>weighted minimum sum S  for the schedule $\\sigma = \\{ J_1, J_2, ... J_n \\}$ is given by :</p>\n\n<p>$S = \\sum_{1\\leqq i \\leqq n} w_i C_i$ where $C_i\\ = \\sum_{1\\leqq j \\leqq i} t_j$ and $w_i$ is the weight of job $J_i$,  $t_i$ is the time $J_i$ takes to complete.</p>\n\n<p>I think the solution is to schedule the jobs in shortest weighted processing time i.e. to arrange them in the increasing order of  $ t_i/w_i $.</p>\n\n<p>But how to prove this.</p>\n', 'ViewCount': '85', 'Title': 'Single machine job scheduling', 'LastEditorUserId': '41', 'LastActivityDate': '2012-12-23T09:25:30.377', 'LastEditDate': '2012-12-23T09:25:30.377', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4980', 'Tags': '<algorithms><scheduling>', 'CreationDate': '2012-12-20T07:28:33.517', 'Id': '7521'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There are polynomial time algorithms to find maximum weighted matching in a general graph. Is there any algorithm that also handles negative weights in the general graph and find maximum weighted matching for that graph with negative weights ?</p>\n', 'ViewCount': '148', 'Title': 'Maximum weight matching', 'LastEditorUserId': '157', 'LastActivityDate': '2013-01-13T05:16:27.090', 'LastEditDate': '2013-01-13T05:16:27.090', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'user12902', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><matching>', 'CreationDate': '2012-12-19T17:54:18.953', 'Id': '7526'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question about the stable marriage algorithm, for what I know it can only be used when I have arrays with the same number of elements for building the preference and the ranking matrices.</p>\n\n<p>For example if we have 10 students that should be assigned to 10 dorms, then I can build the preference and ranking matrix of this data. The question that I have is what to do in the case that I have, for example, only 5 students to assign and for example 10 dorms. Can I still apply the stable marriage algorithm?</p>\n\n<p>Maybe this question is a little bit foolish, but as I said I only saw this algorithm applied to quadratic arrays.</p>\n', 'ViewCount': '261', 'Title': 'The stable marriage algorithm with asymmetric arrays', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-25T21:15:03.620', 'LastEditDate': '2012-12-25T21:15:03.620', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '7546', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5164', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-12-22T20:33:36.160', 'Id': '7545'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wrote a function to check equality between integers <code>a</code> and <code>b</code> using bitwise shift operators. Return value of <code>1</code> means both are Equal and <code>0</code> means unequal inputs.</p>\n\n<pre><code>int foo(int a, int b)\n{\n        return ((a&gt;&gt;= b&lt;&lt;= a) ? 1 : 0);\n}\n</code></pre>\n\n<p>How can I prove theoretically that function <code>foo()</code> works for all values in subset of integer range including negative values. If it fails for certain values, can I theoretically find out for what set of values will foo() works correctly.</p>\n', 'ViewCount': '51', 'Title': 'Prove given equality check function works for all subset of integers', 'LastActivityDate': '2012-12-24T01:09:44.367', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'OwnerDisplayName': 'Manav', 'PostTypeId': '1', 'OwnerUserId': '5030', 'Tags': '<complexity-theory><algorithms>', 'CreationDate': '2012-12-19T12:50:26.990', 'Id': '7550'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I had an interview today, and the interviewer has told me about a theorem (of someone called Hill- or Hell-something) which states that for a non-deterministic algorithm there exists a deterministic algorithm of some time complexity and a space complexity of no more than the original space complexity times log(n).</p>\n\n<p>I am looking for that theorem (couldn't find it on Google). Thanks!</p>\n", 'ViewCount': '183', 'Title': 'Logarithmic space difference between deterministic and non-deterministic algorithms', 'LastActivityDate': '2012-12-23T08:37:29.267', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5165', 'Tags': '<algorithms><space-complexity><nondeterminism>', 'CreationDate': '2012-12-23T08:37:29.267', 'Id': '7559'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I  found the algorithm for finding the negative cycle in a graph after running <a href="http://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm" rel="nofollow">Bellman-Ford algorithm</a>.</p>\n\n<p>The algorithm is to perform another relax iteration over all the edges. Than if we find an edge to relax we stop and start to follow it parent vertex starting from $V$ that $u$ is its parent until we close a cycle.</p>\n\n<p>I\'m having little problem proving this. I want to prove now that no matter when I stop the algorithm - if $p[v] = u$ always $d[v]\\le d[u]+w(u,v)$ and to prove that a cycle in parent pointer is necessarily a negative cycle.</p>\n', 'ViewCount': '163', 'Title': 'Bellman-Ford algorthm and negative cycle proof', 'LastEditorUserId': '3094', 'LastActivityDate': '2012-12-24T02:51:57.977', 'LastEditDate': '2012-12-24T02:51:57.977', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5170', 'Tags': '<algorithms><graphs>', 'CreationDate': '2012-12-23T17:43:25.143', 'FavoriteCount': '1', 'Id': '7565'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a directed graph $G$, edge weights for each edge, source vertex $s$ and $d[v]$ for each vertex in the graph (the distance from $s$ to $v$), I need to find an algorithm that builds the shortest paths graph.</p>\n\n<p>I was thinking of going on edges by BFS, but then how can I know which edge should be in the tree and how to check that the $d[v]$ for each vertex is true?</p>\n', 'ViewCount': '139', 'Title': 'Shortest paths graph', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-26T05:57:06.023', 'LastEditDate': '2012-12-23T22:46:22.307', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5170', 'Tags': '<algorithms>', 'CreationDate': '2012-12-23T20:00:41.477', 'Id': '7569'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In their book <a href="http://www.amazon.co.uk/Randomized-Algorithms-Cambridge-International-Computation/dp/0521474655/ref=sr_1_1?ie=UTF8&amp;qid=1356380410&amp;sr=8-1"><em>Randomized Algorithms</em>,</a> Motwani and Raghavan open the introduction with a description of their RandQS function -- Randomized quicksort -- where the pivot, used for partitioning the set into two parts, is chosen at random.</p>\n\n<p>I have been racking my (admittedly somewhat underpowered) brains over this for some time, but I haven\'t been able to see what advantage this algorithm has over simply picking, say, the middle element (in index, not size) each time.</p>\n\n<p>I suppose what I can\'t see is this: if the initial set is in a random order, what is the difference between picking an element at a random location in the set and picking an element at a fixed position?</p>\n\n<p>Can someone enlighten me, in fairly simple terms? </p>\n', 'ViewCount': '1069', 'Title': 'What is the advantage of Randomized Quicksort?', 'LastActivityDate': '2014-05-03T22:52:13.613', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7583', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '5178', 'Tags': '<algorithm-analysis><sorting><randomized-algorithms>', 'CreationDate': '2012-12-24T20:26:07.713', 'FavoriteCount': '1', 'Id': '7582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '432', 'Title': 'Discrepancy between heads and tails', 'LastEditDate': '2012-12-26T14:53:11.573', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '5195', 'FavoriteCount': '4', 'Body': "<p>Consider a sequence of $n$ flips of an unbiased coin. Let $H_i$ denote the absolute value of the excess of the number of heads over tails seen in the first $i$ flips. Define $H=\\text{max}_i H_i$. Show that $E[H_i]=\\Theta ( \\sqrt{i} )$ and $E[H]=\\Theta( \\sqrt{n} )$. </p>\n\n<p>This problem appears in the first chapter of `Randomized algorithms' by Raghavan and Motwani, so perhaps there is an elementary proof of the above statement. I'm unable to solve it, so I would appreciate any help.</p>\n", 'Tags': '<probability-theory><randomized-algorithms>', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-31T01:55:08.807', 'CommentCount': '0', 'AcceptedAnswerId': '7601', 'CreationDate': '2012-12-26T07:03:36.157', 'Id': '7600'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to solve an exercise in <strong>Distributed Algorithm</strong> described as follow.</p>\n\n<p>Let\'s define a <em>label assignment problem</em> as follow. There is a anonymous network (vertices don\'t have unique ID\'s , nodes don\'t have any clue about $n$ - number of vertices nor about network topology ). The labels assigned to vertices must be distinct and there is no limitation on the length of the label. <em>The initiator is given.</em> Show the algorithm for solving the problem in asynchronous model with time complexity $\\text{O}(D)$ and using $\\text{O}(m)$ messages (where $D$ is the diameter of graph and $m$ number of edges), prove that the labels are indeed distinct. Show the changes in complexities if the algorithm is executed on the synchronous network.</p>\n\n<p>Let\'s say that the initiator is the node A, wlog assign it $A_{id} = 1$, broadcast $A_{id} = 1$ to all children of $A$ by <a href="http://en.wikipedia.org/wiki/Flooding_%28computer_networking%29" rel="nofollow">flooding algorithm</a>, any node $B$ by receiving message from it\'s parent for the first time will assign $B_{id} = PARENT_{id}+1$ and send $B_{id}$ to it\'s children, if $B_{id}$ is already defined, drop the message. $T=\\text{O}(D)$ - time complexity, $M=\\text{O}(m)$ - message complexity for broadcast.</p>\n\n<p>It seems like on synchronous network the complexities will be the same, just becasue <strong>broadcast</strong> and <strong>convergecast</strong> on synchronous and asynchronous network run in $T=\\text{O(D)}$ and $M=\\text{O}(m)$.</p>\n\n<p>If there are any change in complexities on the synchronous network? Does the above algorithm look good?</p>\n', 'ViewCount': '142', 'Title': 'Label Assignment Problem', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-04-26T09:08:40.590', 'LastEditDate': '2012-12-27T07:22:32.760', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><distributed-systems>', 'CreationDate': '2012-12-26T09:48:16.627', 'Id': '7603'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '213', 'Title': 'XOR-like behavior in flow networks', 'LastEditDate': '2013-03-03T11:44:38.840', 'AnswerCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4298', 'FavoriteCount': '2', 'Body': '<p><em>XOR is not the correct name, but I am looking for some kind of exclusive behavior.</em></p>\n\n<p>I am currently solving a set of different (assignment) problems by modeling flow networks and running a min-cost-max-flow algorithm. Flow networks are quite handy because a lot of problems can be reduced to them in an easy and understandable way. In my case these are matchings with some additional constraints. As these constraints are getting more complex I\'ve been wondering if there are some existing constructions to model specific behaviors.</p>\n\n<p>In this case I want to restrict the outgoing flow of a node to a single edge.</p>\n\n<p>Given a graph $G=(V, E)$, integral capacities $c(u,v)$ and costs $k(u,v)$. An arbitrary node is called $A$. It\'s direct neighbors are called $B_1, ..B_n$. Can we replace the edges $AB_1,...AB_n$ (red) with some construction so that <strong>only one edge can receive flow</strong>? Which means that if $AB_1$ gets some flow (e.g. $5/10$) no other (red) edge can receive flow.</p>\n\n<p><img src="http://i.stack.imgur.com/1Cli7.png" alt=""></p>\n\n<p>We could add intermediate nodes/edges and play with costs and capacities. The total capacity of our new construction has to stay the same and the cost of the different alternatives have to stay somehow proportional.</p>\n\n<p>So my questions are:</p>\n\n<ol>\n<li>Are there constructions like this in general? (Any keywords, links, papers)</li>\n<li>Can you suggest a solution to my specific problem?</li>\n</ol>\n', 'Tags': '<algorithms><graph-theory><graphs><network-flow><assignment-problem>', 'LastEditorUserId': '4298', 'LastActivityDate': '2013-10-29T15:14:57.683', 'CommentCount': '9', 'AcceptedAnswerId': '7623', 'CreationDate': '2012-12-26T22:37:50.973', 'Id': '7610'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In a bridge game, a deck of 52 cards (13 spades , 13 clubs, 13 diamonds, 13 spades) are dealt to 4 players (13 cards each) then game starts.Game session ends after 13 tricks each having 4 cards.There are 28561 possible non repeated 4 card groups.</p>\n\n<p>What is the best method of generating and storing all possible trick combinations (played according to the bridge rules just for one session) to be further processed by a computer program (i.e., data structures such as game trees, and any open source algorithms written for any computer language if any).</p>\n\n<p>All resources to read to get the theory behind or any references are welcome.\nThank you.</p>\n', 'ViewCount': '407', 'Title': 'Data structures and algorithms for bridge game play?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-30T09:54:03.547', 'LastEditDate': '2013-04-04T06:33:12.213', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5210', 'Tags': '<algorithms><data-structures><combinatorics><board-games>', 'CreationDate': '2012-12-27T12:12:28.953', 'Id': '7618'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '356', 'Title': 'Finding maximum and minimum of consecutive XOR values', 'LastEditDate': '2012-12-28T21:15:49.023', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5218', 'FavoriteCount': '1', 'Body': '<p>Given an integer array (maximum size 50000), I have to find the minimum and maximum $X$ such that $X = a_p \\oplus a_{p+1} \\oplus \\dots \\oplus a_q$ for some $p$, $q$ with $p \\leq q$.</p>\n\n<p>I have tried this process: $\\text{sum}_i = a_0 \\oplus a_1 \\oplus  \\dots \\oplus a_i$ for all $i$. I pre-calculated it in $O(n)$ and then the value of $X$ for some $p$, $q$ such that $(p\\leq q)$ is: $X = \\text{sum}_q \\oplus \\text{sum}_{p-1}$. Thus:</p>\n\n<p>$$\n\\mathrm{MinAns} = \\min_{(p,q) \\text{ s.t. } p\\le q}{\\text{sum}_q \\oplus \\text{sum}_{p-1}} \\\\\n\\mathrm{MaxAns} = \\max_{(p,q) \\text{ s.t. } p\\le q}{\\text{sum}_q \\oplus \\text{sum}_{p-1}} \\\\\n$$</p>\n\n<p>But this process is of $O(n^2)$. How can I do that more efficiently?</p>\n', 'Tags': '<algorithms><algorithm-analysis><performance><binary-arithmetic><arithmetic>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-29T05:03:58.663', 'CommentCount': '1', 'AcceptedAnswerId': '7640', 'CreationDate': '2012-12-28T03:12:04.687', 'Id': '7622'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Using the Shortest-Seek-Time-First (SSTF) disk scheduling algorithm (where we select a request with a minimum seek time from the current head position), what happens if the requests in both directions from the current head position are equal? </p>\n\n<p>For example, if the head position is at 25, and the nearest positions are 5 and 45, how do we determine which one to select? </p>\n\n<p>Thanks</p>\n', 'ViewCount': '487', 'Title': 'SSTF disk scheduling algorithm? What if lowest seek times are equal in either direction?', 'LastActivityDate': '2013-01-31T19:53:55.153', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7627', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2420', 'Tags': '<algorithms><operating-systems><storage>', 'CreationDate': '2012-12-28T14:45:26.713', 'Id': '7625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>my assumption:<br/>\n- we have an undirected graph with only positive edges<br/>\n- the edges are sorted alphabetically: <br/>\n&nbsp;&nbsp;&nbsp;&nbsp;e.g A-B, A-C, B-D <br/>\n&nbsp;&nbsp;&nbsp;&nbsp;and e.g not C-A, D-B, A-B </p>\n\n<p>I do not understand, why we need the first loop (line 1) here.\nI executed the algorithm on paper on 3-4 different undirected graphs.\nAnd everytime the first iteration of line1 ends, the algorithms finishes to find the shortest path and the remaining iterations are doing just garbage check.</p>\n\n<p>Can anyone give a concrete graph example where we need the first loop?\nDoes the first loop to do something with the negative edges or edge direction perhaps ?</p>\n\n<pre><code>1 for i=1 to vertices.length-1\n2    foreach e in edges\n3        if e.v2.cost &gt; e.v1.cost + e.weight\n4            e.v2.cost := e.v1.cost + e.weight\n5            e.v2.pre := e.v1  \n</code></pre>\n', 'ViewCount': '169', 'Title': 'Bellman-Ford: shortest path', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-29T00:42:39.700', 'LastEditDate': '2012-12-29T00:42:39.700', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7631', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5222', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2012-12-28T15:48:15.470', 'Id': '7628'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '206', 'Title': 'randomized algorithm for checking the satisfiability of s-formulas, that outputs the correct answer with probability at least $\\frac{2}{3}$', 'LastEditDate': '2013-01-05T20:18:01.783', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '1', 'Body': "<p>I'm trying to practice myself with random algorithms.</p>\n\n<p>Lets call a CNF formula over n variables s-formula if it is either unsatisable or it has at\nleast $\\frac{2^n}{n^{10}}$ satisfying assignments.</p>\n\n<p>I would like your help with show a randomized algorithm for checking the\nsatisfiability of s-formulas, that outputs the correct answer with probability at\nleast $\\frac{2}{3}$.</p>\n\n<p>I'm not really sure how to prove it. First thing that comes to my head is this thing- let's accept with probability $\\frac{2}{3}$ every input. Then if the input in the language, it was accepted whether in the initial toss($\\frac{2}{3}$) or it was not and then the probability to accept it is $\\frac{1}{3}\\cdot proability -to-accept$ which is bigger than $\\frac{2}{3}$. Is this the way to do that or should I use somehow Chernoff inequality which I'm not sure how.</p>\n", 'Tags': '<complexity-theory><time-complexity><randomized-algorithms>', 'LastEditorUserId': '1589', 'LastActivityDate': '2013-01-05T20:18:01.783', 'CommentCount': '0', 'AcceptedAnswerId': '7748', 'CreationDate': '2012-12-29T09:38:45.680', 'Id': '7641'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1495', 'Title': 'What is the fastest algorithm for finding all shortest paths in a sparse graph?', 'LastEditDate': '2013-05-24T03:12:57.407', 'AnswerCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '5233', 'FavoriteCount': '2', 'Body': '<p>In an unweighted, undirected graph with $V$ vertices and $E$ edges such that $2V \\gt E$, what is the fastest way to find all shortest paths in a graph? Can it be done in faster than Floyd-Warshall which is $O(V^3)$ but very fast per iteration?</p>\n\n<p>How about if the graph is weighted?</p>\n', 'Tags': '<algorithms><graphs><shortest-path>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-05-01T00:54:19.510', 'CommentCount': '0', 'AcceptedAnswerId': '7646', 'CreationDate': '2012-12-29T17:41:54.350', 'Id': '7644'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is the first step in solving a "tough" algorithmic problem always asking whether it\'s hard in the sense that other tough problems can be reduced to it? Not to make the scope of this question tight, what\'s good advice for approaching challenging algorithmic problems?</p>\n', 'ViewCount': '145', 'Title': 'Solving algorithmic problems', 'LastEditorUserId': '2499', 'LastActivityDate': '2013-01-29T16:55:23.057', 'LastEditDate': '2012-12-31T12:11:01.390', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><decision-problem><search-problem>', 'CreationDate': '2012-12-31T12:03:23.277', 'Id': '7663'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a question that I still struggle with.\nIt would be really appreciated if you guys could give me some hints.</p>\n\n<p>Here is the problem : \nAssume that $a[1\\dots n]$ is an array of $n$ positive real numbers.\nLet $\\alpha &gt;0$ and $\\beta &gt;0$</p>\n\n<ul>\n<li><p>a subarray $a_1$ with $m$ elements of $a[1 \\dots n]$ is called increasing if $\\frac{a_1[i]}{a_1[j]}\\geq \\alpha$, for all $i&gt;j$ and $1 \\leq i, j \\leq m$.</p></li>\n<li><p>a subarray $a_2$ with $k$ elements of $a[1 \\dots n]$ is called decreasing if $\\frac{a_2[i]}{a_2[j]}\\leq \\beta$, for all $i&gt;j$ and $1 \\leq i, j \\leq k$.</p></li>\n</ul>\n\n<p>Question : write a program to find all increasing/decreasing subarrays of $a[1 \\dots n]$ ?\nthanks so much for your help.</p>\n', 'ViewCount': '226', 'Title': 'Find all increasing/decreasing sub array', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-05-01T17:04:54.940', 'LastEditDate': '2013-01-01T14:07:41.843', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5258', 'Tags': '<algorithms><substrings>', 'CreationDate': '2013-01-01T03:12:53.483', 'Id': '7668'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we have a function on GPU which calculates elements of array from which we need to select top K elements. The number of elements can be quite big so we can't store them in memory and our algorithm should be online. Another requirement for algorithm is its good parallelization so it could be effectively run on GPU.</p>\n\n<p>Is there any classic approach for the problem? I've thought only of building heap/balanced tree with $k$ elements and inserting new elements into it if new elements is better than existing and popping smaller element. It gives O($n$ log$k$) but it fails with parallelisation because requires synchronization between GPU threads on modifying tree.</p>\n", 'ViewCount': '147', 'Title': 'An online parallel algorithm for finding top K elements', 'LastActivityDate': '2013-01-02T14:58:14.107', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5273', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-01-02T14:39:37.623', 'Id': '7698'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\u2019m trying to make a photo mosaic web application. I split the main picture in to a number of squares. I split this square up in to 9 squares (<a href="http://i49.tinypic.com/imryf6.jpg" rel="nofollow">See picture</a>) Then I calculate the mean color (RGB) of the entire square and the mean color of the 9 little squares. So I will get the following vector.</p>\n\n<p>Vector: Mean R; mean G; mean B; cell 1 R; cell 1 G; cell 1 B; cell 2 R; cell 2 G; cell 2 B; cell 3 R; cell 3 G; cell 3 B; cell 4 R; cell 4 G; cell 4 B; cell 5 R; cell 5 G; cell 5 B; cell 6 R; cell 6 G; cell 6 B; cell 7 R; cell 7 G; cell 7 B; cell 8 R; cell 8 G; cell 8 B; cell 9 R; cell 9 G; cell 9 B</p>\n\n<p>This will all be number between 0 and 255. And the mean RGB is more important than the other elements.</p>\n\n<p>I make the same vector for all pictures in my DB. So now I want to compare a square of the original too my DB and want to find the 10 closest pictures.</p>\n\n<p>My question is how do I compare these vectors??</p>\n', 'ViewCount': '116', 'Title': 'Compare vectors, photo mosaic', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-02T19:18:14.603', 'LastEditDate': '2013-01-02T19:18:14.603', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5274', 'Tags': '<algorithms><image-processing>', 'CreationDate': '2013-01-02T14:47:48.197', 'Id': '7699'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I have a set of sets of integers $A$, is there an efficient algorithm/data structure that will allow me to query for all sets of integers that include a given input set? That is, given input $I\\subset \\mathbb Z$, $\\forall x \\in I$ find $C$, the sets in $A$ that include all of $I$, that is $C=\\left\\{ B \\in A \\mid I\\subseteq B\\right\\}$.</p>\n\n<p>Looking for the best of a few solutions. </p>\n', 'ViewCount': '116', 'Title': 'Finding containing sets in a set of sets', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-01-02T20:21:07.680', 'LastEditDate': '2013-01-02T16:29:56.270', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><sets>', 'CreationDate': '2013-01-02T15:28:59.567', 'FavoriteCount': '1', 'Id': '7701'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m currently working on a paper describing a new algorithm in computational science. If all goes well, this algorithm will be around for a while (within the specific community). As such, I want to set notation conventions that will <em>not</em> drive other people insane. The primary difference is that this algorithm makes use (logically) of tree data structures while the traditional algorithms in the field have used linear arrays.</p>\n\n<p>The old algorithms therefore could denote specific data as $data_i$ (that is, using LaTex subscripts. Similarly, one could refer to $data_{i-1}$, and it would be clear that that is the "parent" data to $data_i$. Unfortunately, trees do not support this sort of indexing.</p>\n\n<p>Are there any notation conventions for trees that allow clear, concise descriptions of that sort? I want to be able to give talk about an arbitrary bit of data (i.e. $data_i$) and readily discuss parent and child data. The community is mathematician heavy and as such mathematical notation of the sub/superscript y operator sort is favored over the class.property CS style. Note also that these are arbitrary trees; they need not be binary or any other such structure.</p>\n\n<p>Does anyone know of a notation convention that would fit the bill? Alternatively, is there a better place to ask this? Thanks for the help.</p>\n', 'ViewCount': '193', 'Title': 'Notation Conventions for Tree Data Structures', 'LastEditorUserId': '5280', 'LastActivityDate': '2013-01-03T00:16:07.643', 'LastEditDate': '2013-01-02T22:22:01.857', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'Ethan', 'PostTypeId': '1', 'OwnerUserId': '5280', 'Tags': '<algorithms><trees>', 'CreationDate': '2013-01-02T19:14:32.530', 'Id': '7705'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to understand the approximation ratio for the <a href="http://mor.journal.informs.org/content/25/4/645.full.pdf" rel="nofollow">Kenyon-Remila</a> algorithm for the 2D cutting stock problem.</p>\n\n<p>The ratio in question is $(1 + \\varepsilon) \\text{Opt}(L) + O(1/\\varepsilon^2)$.</p>\n\n<p>The first term is clear, but the second doesn\'t mean anything to me and I can\'t seem to figure it out.</p>\n', 'ViewCount': '235', 'Title': 'What does big O mean as a term of an approximation ratio?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-08T06:24:22.580', 'LastEditDate': '2013-01-08T06:24:22.580', 'AnswerCount': '5', 'CommentCount': '0', 'AcceptedAnswerId': '7722', 'Score': '2', 'OwnerDisplayName': 'Jacob Fogner', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><approximation>', 'CreationDate': '2013-01-01T23:33:43.330', 'Id': '7720'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is an exercise in <strong>Distributed Algorithm</strong> I have some difficulties to solve. There are few ideas, however nothing useful at the time. I will appreciate any help with it.</p>\n\n<p>Graph $G$ is a $k$-tree if it\'s possible to divide all it\'s edges to $k$ trees ($k$ subgraphs without a loop). Given network $N$ every processor knows it\'s parent and it\'s children in every tree. Using algorithm for 3-coloring of an arbitrary tree (as describe in <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">Vertex Coloring</a>) show that the given graph $G$ is $3k$-colorable in the same time complexity $O(\\log^*n)$.</p>\n\n<p><strong>Ideas</strong>: every vertex $v$ could belong to one or more trees among $k$ trees. For every particular tree vertex $v$ belongs to, vertex $v$ knows it\'s parent in this tree and it\'s children in this tree. Exercise asks to color a graph $G$ in $3k$ colors.   Running the mentioned algorithm for any particular tree we will get correct 3-colorable $k$ trees, each vertex will have $m$ different color labels, where $m$ is number of trees, vertex $v$ belongs to.</p>\n\n<p>Now the difficult part, concatenation of all $m$ color labels in one color for vertex $v$ will result in $3^k$ different colors for graph $G$.</p>\n\n<p>In general, simplification of the problem is on the picture.</p>\n\n<p><img src="http://i.stack.imgur.com/uF8tv.png" alt="enter image description here"></p>\n\n<p>Vertices $v$ and $u$ are connected by an edge (on the picture the edge belongs to the 1 tree, in principle it doesn\'t matter, we could draw a parallel edge that belongs to 2 tree), $v$ and $u$ belong to 1 tree and 2 tree. After running 3-colorability algorithm for every tree we will get the correct colorability, now the problem is to make color assignments to vertices $v$ and $u$ as vertices of the graph $G$ based on the color assignment of these vertices for 1 tree and 2 tree, and of course because $u$ and $v$ are connected by an edge in graph $G$, the colors should be different.  </p>\n', 'ViewCount': '276', 'Title': 'k-Trees Graph Coloring', 'LastActivityDate': '2013-01-03T18:56:48.330', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><proof-techniques><distributed-systems>', 'CreationDate': '2013-01-03T18:56:48.330', 'Id': '7736'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m currently studying the book "Introduction to Algorithms - Cormen". Although a proof of correctness for the BFS algorithm is given, there isn\'t one any for the DFS in the book. So I was courious about how it can be shown that DFS visits all the nodes.\nI also googled for it. But it seems that every lecturer do some copy-paste work from this book in their pdf\'s, so I couldn\'t find anything useful. <br/><br/>\nBy DFS we had to show that it found the shortest path. But since DFS does not calculate something like that I have no idea how to prove it.<br/></p>\n\n<hr>\n\n<p>Off the topic, why are those proofs so important? Throughout the book there are so many lemmas and theorems which can be really boring sometimes. I understand how an algorithm works in 10 minutes, perhaps need another 5 to 10 minutes to understand how to analyse the running time, but then I\'m loosing 1 hour just for some useless lemmas. Besides, and worse even, I studied almost 50 proofs/lemmas of different algorithms till now, I never managed to solve one of them by myself. How can I gain the "proving ability"? Is there a systematical way to learn that? I don\'t mean the Hoare logic way with invariants, rather the informal way described in the book "Introduction to Algorithms". Is there any book which focuses on "how to prove algorithms" and show that in a systematical, introductory way ?</p>\n', 'ViewCount': '800', 'Title': 'DFS - Proof of Correctness', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-01-05T11:11:50.160', 'LastEditDate': '2013-01-05T10:16:22.507', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '7781', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5222', 'Tags': '<algorithms><graph-theory><graphs><correctness-proof>', 'CreationDate': '2013-01-04T05:53:53.593', 'Id': '7749'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am struggling in trying to figure out a non-deterministic algorithm for the following problem.</p>\n\n<p>Consider the following problem, called the \ufb01gure-of-eight problem (FOE). An instance is an undirected graph $G = (V,E)$ with vertices $V$ and edges $E$. $G$ is a yes-instance if there is a sequence of vertices $(v_{0},v_{1},...,v_{k+1})\\ (some\\ k \\geq 6)$ such that</p>\n\n<p>\u2022 Each pair $(v_{i},v_{i+1})$ is an edge $(each\\ i &lt; k \u2212 1)$ and $(v_{k\u22121},v_{0})$ is an edge. </p>\n\n<p>\u2022 Every vertex in $V$ occurs at least once in the sequence. </p>\n\n<p>\u2022 There is $j$ with $2 &lt; j &lt; k \u2212 2$ such that $v_{0} = v_{j}$. </p>\n\n<p>\u2022 No other vertex in the sequence is counted twice, i.e. if $v_{s} = v_{t} (any\\ s,t &lt; k)$ then either $s = t$ or ${s,t} = {0,j}$.</p>\n\n<p>If there is no such sequence of vertices then $G$ is a no-instance of FOE.</p>\n\n<p>Thanks</p>\n', 'ViewCount': '106', 'Title': 'Non-deterministic algorithm for solving figure of 8', 'LastActivityDate': '2013-01-04T14:17:12.447', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7757', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5300', 'Tags': '<algorithms><complexity-theory><graphs><nondeterminism>', 'CreationDate': '2013-01-04T13:03:33.720', 'Id': '7756'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm attempting to use the MT19937 variant of the Mersenne Twister PRNG to accomplish something.  Whether or not this something is feasible depends upon the answers to these two questions:</p>\n\n<p>What is the greatest value of <strong>m</strong> for which the following statements hold true:</p>\n\n<p>1 - For all seed values, the algorithm eventually produces every integer list of length <strong>m</strong>.</p>\n\n<p>2 - There exists a seed value for which the algorithm would eventually produce a given integer list of length <strong>m</strong>.</p>\n", 'ViewCount': '93', 'Title': 'Will the Mersenne Twister PRNG eventually produce all integer sequences of a certain length?', 'LastEditorUserId': '5308', 'LastActivityDate': '2013-01-05T18:42:00.607', 'LastEditDate': '2013-01-05T18:42:00.607', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5308', 'Tags': '<algorithms><pseudo-random-generators>', 'CreationDate': '2013-01-05T05:17:40.293', 'Id': '7775'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What would be a good book/resource that explains the basic idea behind those techniques, how to use (and maybe when to use them) and plenty of exercises with perhaps some worked examples (kind of like when for loops were introduced for the first time you were asked to use one to compute sums of odd numbers, even numbers etc.) ?</p>\n\n<p>If the explanation is both formal and "plain" (for dummies style explanation) that would be great. Thanks!</p>\n', 'ViewCount': '306', 'Title': 'Good explanatory resource for algorithm techniques such as greedy, backtracking and recursive functions', 'LastActivityDate': '2013-01-07T06:47:02.327', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '7813', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2530', 'Tags': '<algorithms>', 'CreationDate': '2013-01-06T08:15:59.517', 'Id': '7799'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '298', 'Title': 'Prove or refute: BPP(0.90,0.95) = BPP', 'LastEditDate': '2013-01-07T22:38:45.893', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '3', 'Body': '<p>I\'d really like your help with the proving or refuting the following claim: $BPP(0.90,0.95)=BPP$. In computational complexity theory, BPP, which stands for <a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29">bounded-error probabilistic polynomial time</a> is the class of decision problems solvable by a probabilistic Turing machine in polynomial time, with an error probability of at most $\\frac{1}{3}$ for all instances. $BPP=BPP(\\frac{1}{3},\\frac{2}{3})$.</p>\n\n<p>It is not immediate that any of the sets are subset of the other, since if the probability for an error is smaller than $0.9$ it doesn\'t have to be smaller than $\\frac{1}{3}$ and if it is bigger than $\\frac{2}{3}$ it doesn\'t have to be bigger than $0.905$. </p>\n\n<p>I\'m trying to use Chernoff\'s inequality for proving the claim, I\'m not sure exactly how.\nI\'d really like your help. Is there a general claim regarding these relations that I can use?</p>\n', 'Tags': '<complexity-theory><time-complexity><probabilistic-algorithms>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-08T19:20:38.810', 'CommentCount': '4', 'AcceptedAnswerId': '7829', 'CreationDate': '2013-01-07T21:40:43.267', 'Id': '7820'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have frequency data for different events under two conditions, resulting in sets of frequencies F1 and F2. I would like to normalize the frequencies of events under condition 1 by their frequencies under condition 2. However, there are events that occur in condition 1 but not condition 2, resulting in divide-by-zero problems when I attempt to normalize.</p>\n\n<p>For raw count data, I understand that there are a number of smoothing techniques (e.g. Witten-Bell) that can help sort this out, but I only have the frequencies, not the individual counts. In other words, I have frequencies like {0, 0.1, 0.2, 0.7} which could correspond to counts of {0, 1, 2, 7}, {0, 10, 20, 70}, etc. Are there any algorithms that are able to smooth this type of frequency data?</p>\n', 'ViewCount': '55', 'Title': 'Smoothing frequencies without count data', 'LastActivityDate': '2013-01-08T17:58:49.037', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7836', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4927', 'Tags': '<algorithms><probability-theory>', 'CreationDate': '2013-01-07T22:14:09.180', 'Id': '7821'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Let\'s consider distributed version of algorithm for finding MIS of any graph $A$.</p>\n\n<p>For details, MIS - <a href="http://en.wikipedia.org/wiki/Maximal_independent_set" rel="nofollow">Maximimal Independent Set</a>.</p>\n\n<p>Slow version of distributed algorithm for MIS, page 2 -  <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">Distributed algorithms. Maximal Independent Set</a></p>\n\n<p>In worst case, time complexity of the algorithm is $\\text{O}(n)$ and a message complexity is $\\text{O}(m)$. If nodes of the network are not unique than it\'s not possible to find MIS.</p>\n\n<p>I am interested in few special cases, when the graph is a <strong>path</strong> and <strong>ring</strong>.</p>\n\n<p><strong>Exercise</strong>: Consider a path graph $G$ with vertex UIDs to be a random permutation of $\\{1,\u2026,n\\}$, time complexity of MTS_Slow is $T \\leq c\\log n$ for a constant $c$ with probability $1-\\frac{1}{n}$.\nWhat is a time complexity and it\'s probability on a ring $G$ with vertex UIDs to be a random permutation of $\\{1,\u2026,n\\}$.</p>\n\n<p><strong>Ideas:</strong> Time complexity has $\\log n$ factor, therefore on each phase number of candidates for MIS nodes from a path graph $G$ should be divided by constant factor. Lets consider the worst case, when only one node is choicen to join MIS on each phase, it occurs when the nodes\' UID\'s are located in increasing or decreasing order, it happes with probability $P(B)=\\frac{1}{2^n}$, $P(A)=\\frac{1}{2}$, where B - nodes\' UID is placed in increasing order, A - the next node\'s UID is bigger the the current one. But how to show that the number of candidates is getting lower by any arbitrary constant factor. The problem is I have a difficulties in defining probability for taking arbitrary constant factor of nodes on each phase. </p>\n\n<p>Case with ring should be similar to a path graph case.</p>\n\n<p>I will appreciate any help in solving this exercise.</p>\n', 'ViewCount': '145', 'Title': 'Maximimal Independent Set on Ring and Path', 'LastActivityDate': '2013-01-15T01:57:56.640', 'AnswerCount': '1', 'CommentCount': '10', 'AcceptedAnswerId': '8940', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4799', 'Tags': '<algorithms><graphs><distributed-systems>', 'CreationDate': '2013-01-08T13:45:19.680', 'Id': '7832'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a tree of foods, which I\'d like to search to find relevant nodes. What\'s the best way to rank the results?</p>\n\n<p>Here\'s an example sub-set after a search for "apple":</p>\n\n<pre><code>|-- apple pie\n|-+ apples\n| |-- cooking\n| |-+ eating\n|   |-- average\n|   |-- Granny Smith\n|   |-- Golden Delicious\n|-- pork and apple casserole\n</code></pre>\n\n<p>Currently, I\'m ranking based on which results come earliest in the full name of each leaf, so a result set would look like:</p>\n\n<pre><code>0 Apple pie\n0 Apples, cooking\n0 Apples, eating, average\n0 Apples, eating, Granny Smith\n0 Apples, eating, Golden Delicious\n2 Pork and apple casserole\n</code></pre>\n\n<p>Where the rank is the index of the first token match, lower the rank the better.</p>\n\n<p>I\'d like to aggregate the matches, so that I don\'t display the full sub-tree on the initial search, for example:</p>\n\n<pre><code>Apples... (4)\nApple pie\nPork and apple casserole\n</code></pre>\n\n<p>The obvious way to rank these is to count the number of matching leaves, then the higher the rank, the better the match.</p>\n\n<p>But, I\'m not sure how to combine these rankings, since one is larger-is-better and one is smaller-is-better. Are there standard ways to combine rankings like this? (I\'m not sure what to search for, so Google is giving me results about search engine optimisation, and searching web pages, which doesn\'t seem to apply.)</p>\n', 'ViewCount': '46', 'Title': 'How to rank search results of short string lists', 'LastActivityDate': '2013-01-09T04:35:05.177', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7845', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5348', 'Tags': '<search-algorithms>', 'CreationDate': '2013-01-08T23:16:46.040', 'Id': '7842'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Consider the following problem:</p>\n\n<blockquote>\n  <p>Let a <em>$k$-wheel</em> be defined as an indexed circularly linked list of $k$ integers. For example\u2026</p>\n  \n  <blockquote>\n    <p>{3, 4, 9, -1, 6}  </p>\n  </blockquote>\n  \n  <p>\u2026is a 5-wheel with 3 at position 0, 4 at position 1, and so on. A wheel supports the operation of rotation, so that a one-step rotation would turn the above wheel into\u2026</p>\n  \n  <blockquote>\n    <p>{6, 3, 4, 9, -1}  </p>\n  </blockquote>\n  \n  <p>\u2026now with 6 at position 0, 3 at position 1, and so on. Let $W_{N_k}$ be an ordered set of $N$ distinct $k$-wheels. Given some $W_{N_k}$ and some integer $t$, find a series of rotations such that\u2026</p>\n  \n  <p>$$\\forall\\ 0 \\leq i &lt; k, \\sum_{N \\in W} N_i = t$$</p>\n  \n  <p>In other words, if you laid out the wheels as a matrix, the sum of every column would be $t$. Assume that $W_{N_k}$ is constructed so that the solution is unique up to rotations of every element (i.e., there are exactly $k$ unique solutions that consist of taking one solution, then rotating every wheel in $W$ by the same number of steps).</p>\n</blockquote>\n\n<p>The trivial solution to this problem involves simply checking every possible rotation. Here is some pseudocode for that:</p>\n\n<pre><code>function solve(wheels, index)\n    if wheels are solved:\n        return true\n    if index &gt;= wheels.num_wheels:\n        return false\n    for each position 1..k:\n        if solve(index + 1) is true:\n            return true\n        else:\n            rotate wheels[index] by 1\n\nsolve(wheels, 0)\n</code></pre>\n\n<p>This is a pretty slow solution (something like $O(k^n)$). I'm wondering if it is possible to do this problem faster, and also if there is a name for it.</p>\n", 'ViewCount': '81', 'Title': 'sum of like indices in circular lists', 'LastEditorUserId': '4517', 'LastActivityDate': '2013-02-08T21:11:55.413', 'LastEditDate': '2013-01-09T15:34:50.883', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4517', 'Tags': '<algorithms>', 'CreationDate': '2013-01-09T09:35:08.573', 'Id': '7846'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '241', 'Title': 'Concrete understanding of difference between PP and BPP definitions', 'LastEditDate': '2013-02-15T07:31:32.413', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5356', 'FavoriteCount': '3', 'Body': '<p>I am confused about how  <strong>PP</strong> and <strong>BPP</strong> are defined. Let us assume $\\chi$ is the characteristic function for a language $\\mathcal{L}$. <em>M</em> be the probabilistic Turing Machine. Are the following definitions correct:<br>\n$BPP =\\{\\mathcal{L} :Pr[\\chi(x) \\ne M(x)] \\geq \\frac{1}{2} + \\epsilon \\quad \\forall x \\in \\mathcal{L},\\ \\epsilon &gt; 0 \\}$<br>\n$PP =\\{\\mathcal{L} :Pr[\\chi(x) \\ne M(x)] &gt; \\frac{1}{2} \\}$  </p>\n\n<p>If the definition are wrong, please try to make minimal change to make them correct (i.e. do not give other equivalent definition which use counting machine or some modified model). I can not properly distinguish the conditions on probability on both the definitions.  </p>\n\n<p>Some concrete examples with clear insight into the subtle points would be very helpful. </p>\n', 'Tags': '<complexity-theory><terminology><randomized-algorithms><complexity-classes>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-15T07:31:32.413', 'CommentCount': '0', 'AcceptedAnswerId': '7849', 'CreationDate': '2013-01-09T11:28:04.633', 'Id': '7848'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I've designed an algorithm to find the longest common subsequence. these are steps:  </p>\n\n<p>Starts with <code>i = 0</code></p>\n\n<ol>\n<li>Picks the first letter from the first string start from ith letter.</li>\n<li>Go to the second string looking for that picked letter.</li>\n<li>If not found return to the first string and picks the next letter and repeat 1 to 3 until it finds a letter that is in the second string.</li>\n<li>Now that found a common letter in the second string, adds that to <code>$common_subsequence</code>.</li>\n<li>Store its position in <code>$index</code>.</li>\n<li>Picks next letter from the first string and do step 2 but this time starts from <code>$index</code>.</li>\n<li>Repeat 3 to 6 until reached end of string 1 or string 2.</li>\n<li>If length <code>$common_subsequence</code> is greater than length of common subsequence so far add that  change lcs to the <code>$common_subsequence</code>.</li>\n<li>Add 1 to the i and repeat 1 to 9 while i is less that length of the first string.</li>\n</ol>\n\n<p>This is an example:<br>\n\u202b\u202a</p>\n\n<pre><code>X=A, B, C, B, D, A, B\u202c\u202c  \n\u202b\u202aY=B, D, C, A, B, A\u202c\u202c \n</code></pre>\n\n<ol>\n<li>First pick <code>A</code>.</li>\n<li>Look for <code>A</code> in <code>Y</code>.</li>\n<li>Now that found <code>A</code> add that to the <code>$common_subsequence</code>.</li>\n<li>Then pick <code>B</code> from <code>X</code>.</li>\n<li>Look for <code>B</code> in <code>Y</code> but this time start searching from <code>A</code>.</li>\n<li>Now pick <code>C</code>. It isn't there in string 2, so pick the next letter in <code>X</code> that is <code>B</code>.<br>\n...<br>\n...<br>\n...  </li>\n</ol>\n\n<p>The complexity of this algorithm is theta(n*m).</p>\n\n<p><em><strong>I implemented it on the two methods. The second one uses a hash table, but after implementing I found that it's much slower compared to the first algorithm. I cant undrestand why.</em></strong></p>\n\n<p>Here is my implementation:  </p>\n\n<p>First algorithm:</p>\n\n<pre><code>import time\ndef lcs(xstr, ystr):\n    if not (xstr and ystr): return # if string is empty\n    lcs = [''] #  longest common subsequence\n    lcslen = 0 # length of longest common subsequence so far\n    for i in xrange(len(xstr)):\n        cs = '' # common subsequence\n        start = 0 # start position in ystr\n        for item in xstr[i:]:\n            index = ystr.find(item, start) # position at the common letter\n            if index != -1: # if common letter has found\n                cs += item # add common letter to the cs\n                start = index + 1\n            if index == len(ystr) - 1: break # if reached end of the ystr\n        # update lcs and lcslen if found better cs\n        if len(cs) &gt; lcslen: lcs, lcslen = [cs], len(cs) \n        elif len(cs) == lcslen: lcs.append(cs)\n    return lcs\n\nfile1 = open('/home/saji/file1')\nfile2 = open('/home/saji/file2')\nxstr = file1.read()\nystr = file2.read()\n\nstart = time.time()\nlcss = lcs(xstr, ystr)\nelapsed = (time.time() - start)\nprint elapsed\n</code></pre>\n\n<p>Second one using hash table:</p>\n\n<pre><code>import time\nfrom collections import defaultdict\ndef lcs(xstr, ystr):\n    if not (xstr and ystr): return # if strings are empty\n    lcs = [''] #  longest common subsequence\n    lcslen = 0 # length of longest common subsequence so far\n    location = defaultdict(list) # keeps track of items in the ystr\n    i = 0\n    for k in ystr:\n        location[k].append(i)\n        i += 1\n    for i in xrange(len(xstr)):\n        cs = '' # common subsequence\n        index = -1\n        reached_index = defaultdict(int)\n        for item in xstr[i:]:\n            for new_index in location[item][reached_index[item]:]:\n                reached_index[item] += 1\n                if index &lt; new_index:\n                    cs += item # add item to the cs\n                    index = new_index\n                    break\n            if index == len(ystr) - 1: break # if reached end of the ystr\n        # update lcs and lcslen if found better cs\n        if len(cs) &gt; lcslen: lcs, lcslen = [cs], len(cs) \n        elif len(cs) == lcslen: lcs.append(cs)\n    return lcs\n\nfile1 = open('/home/saji/file1')\nfile2 = open('/home/saji/file2')\nxstr = file1.read()\nystr = file2.read()\n\nstart = time.time()\nlcss = lcs(xstr, ystr)\nelapsed = (time.time() - start)\nprint elapsed\n</code></pre>\n", 'ViewCount': '314', 'Title': 'Finding the longest common subsequence algorithm using hash table Slow', 'LastActivityDate': '2013-01-09T20:29:18.410', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7853', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5202', 'Tags': '<algorithms><subsequences>', 'CreationDate': '2013-01-09T16:28:54.807', 'Id': '7851'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How is an algorithm with complexity $O(n \\log n)$ also in $O(n^2)$? I'm not sure exactly what its saying here, I feel it may be something to do with the fact that big-oh is saying less than or equal to, but I am not fully sure. Any have any ideas? Thanks.</p>\n", 'ViewCount': '58', 'Title': 'How is this algorithm in these two complexities?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-12T16:37:28.597', 'LastEditDate': '2013-01-12T16:37:28.597', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7856', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3088', 'Tags': '<algorithms><asymptotics><landau-notation>', 'CreationDate': '2013-01-09T21:20:29.653', 'Id': '7855'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a set of strings.  My goal is to find a minimal set of longest prefixes which will match most of that set.</p>\n\n<p>For instance, if my set is:</p>\n\n<pre><code>abracadabra\nabracado\nabramu\nbanana\nbananasplit\nbananaicecream\nxylophone\nzebra\nzeitgeist\nzello\n</code></pre>\n\n<p>I would want the output to be:</p>\n\n<pre><code>banana (len 6, matches 3)\nabra (len 4, matches 3)\nxylophone (len 9, matches 1)\nze (len 2, matches 3)\n</code></pre>\n\n<p>Now, this question isn't yet properly specified.  That's because I'm ranking my results on two dimensions: maximum matches, and maximum length.  My goal is to find prefixes that cover as much as of the set as possible, which are as long as possible and thus less likely to occur in strings that <em>aren't</em> in the set (all other things being equal, of course).</p>\n\n<p>Ideally, I'd like to find a set of very long strings, ranked by how much of the set they cover.</p>\n\n<p>That's my goal.  Now I'll present my work, and where I need help.</p>\n\n<p><strong>FIRST</strong> Let's specify the problem better.  We want a set of prefixes.  For each prefix, we compute its length, and the number of matches it has in the set, and order the prefixes by their product.  I'm then free to pick the top X prefixes.</p>\n\n<p>I think that's a good specification.</p>\n\n<p><strong>NOW</strong> Comes an efficient implementation.  Brute force is to check every possible prefix against every string, which is complexity n * n * m (n being number of strings, m being average length of strings).</p>\n\n<p>An efficient algorithm?<br>\nSomething like this:   </p>\n\n<ol>\n<li>Build a prefix tree of the set</li>\n<li>Each leaf has value 1</li>\n<li>Work up the tree, with each parent equal to sum of its children , plus 1 if it has an entry</li>\n<li>Now we know each prefix and how many matches it has - I believe complexity is n log n</li>\n<li>Walk through the tree, counting length of each string (complexity n * m)</li>\n<li>And collect all the entries, sort them by length * value (complexity n log n)</li>\n</ol>\n\n<p>That algorithm is roughly n log n, which is efficient enough.  Will it work? How should it be improved? What's a simple way to implement it? </p>\n\n<p>Finally: Since all the data is in a Postgres relational database, I believe it would be simplest to do the algorithm using relational algebra with aggregate functions. Comments on this?</p>\n", 'ViewCount': '216', 'Title': 'Algorithm for determining minimal set of covering prefixes', 'LastEditorUserId': '1623', 'LastActivityDate': '2013-01-11T14:38:16.920', 'LastEditDate': '2013-01-10T16:50:17.210', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5368', 'Tags': '<algorithms><trees><data-compression><sets><coding-theory>', 'CreationDate': '2013-01-10T13:34:14.843', 'Id': '7868'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When doing mental calculus one can do:</p>\n\n<ul>\n<li>Given an integer k, sum all the digits (in base 10),\nand if the result is a multiple of 3, then k is a multiple of 3.</li>\n</ul>\n\n<p>Do you know of any algorithm working similarily but operating on binary numbers digits (bits)? </p>\n\n<ul>\n<li><p>At first, I was thinking of using the ready made functions of my\nlanguage converting integer to ascii to perform the convertion from\nbase 2 to base 10, then apply the mental calculus trick. But of\ncourse then I could also encode the base convertion 2 to 10 myself.\nI have not done it yet, but I'll give it a try.</p></li>\n<li><p>Then I have thought of euclidian division in base 2...</p></li>\n</ul>\n\n<p>However I wonder if there are other means, algorithms.</p>\n", 'ViewCount': '344', 'Title': 'Algorithms computing if a number is a multiple of 3', 'LastActivityDate': '2013-01-11T21:54:05.297', 'AnswerCount': '4', 'CommentCount': '0', 'AcceptedAnswerId': '7880', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2100', 'Tags': '<algorithms><numerical-analysis>', 'CreationDate': '2013-01-11T01:52:36.497', 'Id': '7879'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>We have a broken stick. For every part, we know it's length. Our task is to connect all parts (glue them), that we will use as small amount of glue as possible. </p>\n\n<p>The amount of glue need to connect two parts equal the maximum from their sizes. We can only glue two parts at one time.Can we solve this problem in the time complexity smaller than $O(n^3)$? I know only the answer, using dynamic table in this complexity</p>\n", 'ViewCount': '136', 'Title': 'Broken stick problem', 'LastActivityDate': '2013-01-11T04:51:06.967', 'AnswerCount': '2', 'CommentCount': '10', 'AcceptedAnswerId': '7882', 'Score': '0', 'OwnerDisplayName': 'Jonny', 'PostTypeId': '1', 'OwnerUserId': '5381', 'Tags': '<complexity-theory><algorithms><dynamic-programming>', 'CreationDate': '2013-01-09T12:40:44.533', 'Id': '7881'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '632', 'Title': 'Scheduling algorithm to minimize maximum deadline overshoot in pre-emptive scheduler', 'LastEditDate': '2013-01-12T18:59:53.610', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4751', 'FavoriteCount': '1', 'Body': '<p>Suppose there are $n$ tasks, which need to be scheduled by a <em>pre-emptive</em> scheduler. Each task $T_i$ has a deadline $d_i$ and a total processing time $t_i$ associated with it. Now, all $n$ tasks are given a priory to the scheduler. The scheduler can run a task for 1 unit of time in one go. After each unit of time, it can schedule any process (including the current one) for the next 1 unit of time.</p>\n\n<p>The goal of the scheduler is to <em>minimize</em> the <em>maximum</em> overshoot of its deadline by any process. For example, for tasks $T_1: (2, 2)$, $T_2: (1, 1)$ and $T_3: (4, 3)$ are the 3 tasks with their respective $(d_i, t_i)$, then a schedule of $T_2, T_1, T_3, T_1, T_3, T_3$ gives a maximum overshoot of 2. No other schedule can reduce the maximum overshoot.</p>\n\n<p>My solution is to use "Earliest Deadline First Scheduling", with tie-breaks based on most time/work remaining. Further ties are broken arbitrarily. Basically, after each unit of time, the task with the earliest deadline is scheduled first. Any ties are decided on which task has the most work remaining. Further ties are broken arbitrarily. This seems to work on a few small hand-constructed cases. But I could not prove or disprove it.</p>\n\n<p>To make the question more than a yes/no question, I would really appreciate it if someone could prove if this is correct, or provide a correct and efficient (sub-quadratic time) algorithm for this. This is not homework. It was presented to me by someone, and I suspect it might be a popular interview question or a question on a programming forum.</p>\n', 'Tags': '<algorithms><optimization><scheduling>', 'LastEditorUserId': '2100', 'LastActivityDate': '2013-08-14T07:27:28.117', 'CommentCount': '0', 'AcceptedAnswerId': '7906', 'CreationDate': '2013-01-12T06:01:51.120', 'Id': '7900'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Yesterday I've done some research how to optimize genetic algorithm and I've encountered a very interesting theory that we can use Lamarckian theory (adaptive theory) to optimize the neural network. But I still didn't understand how can it be done? What I've read from some source to optimize the genetic algorithm we must do some adaptation to the parents before we do the mating (crossover &amp; mutation) but how is exactly the adaptation done? For example, if I have a genetic algorithm population to train a neural network, how can I alter the chromosome of the population to be adapted to the environment? Is it based on the error of the output or how?</p>\n", 'ViewCount': '62', 'Title': 'Lamarckian and Genetic algorithm', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-12T19:02:18.517', 'LastEditDate': '2013-01-12T19:02:18.517', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4573', 'Tags': '<algorithms><artificial-intelligence><genetic-algorithms>', 'CreationDate': '2013-01-12T09:40:47.417', 'FavoriteCount': '1', 'Id': '7903'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a system in existence which works on parent-child model.</p>\n\n<ol>\n<li>They share common attributes, as they belong to same class.</li>\n<li>A parent can enforces certain data on any user selected attribute, and the data is required to be propagated down the hierarchy for the same attribute in all children and their children.</li>\n<li>For a given attribute, if a child's parent dies (object gets destroyed) or child has not been enforced by parent, child can behave the same way, like in point #2, (on user request) for its children.</li>\n</ol>\n\n<p>My system is pretty big (in database). For example:</p>\n\n<ul>\n<li>Each object of the class has 150 atributes.</li>\n<li>There are in total about 8 million attributes for objects created. Hence, the objects are close to 53,333.</li>\n<li>There is a single parent for the objects. Its a big tree.</li>\n</ul>\n\n<p>If I want to enforce data on the top most parent for a single attribute, it takes about 8 mins for it to complete.</p>\n\n<p>Does anyone know a model which can perform faster than hierarchical model, but similar to behaviour?</p>\n", 'ViewCount': '47', 'Title': 'Optimization of Hierarchical Data Propagation', 'LastEditorUserId': '5402', 'LastActivityDate': '2013-08-05T14:39:58.180', 'LastEditDate': '2013-08-05T11:48:31.827', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5402', 'Tags': '<algorithms><databases><performance>', 'CreationDate': '2013-01-13T11:08:33.473', 'Id': '7922'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was studying binomial heaps and its time analysis. Are there any inputs that cause DELETE-MIN, DECREASE-KEY, and DELETE to run in $\\Omega(\\log n)$ time for a binomial heap rather than $O(\\log n)$?</p>\n', 'ViewCount': '243', 'Title': 'Input that causes an operation on a binomial heap to run in $\\Omega(\\log n)$ time?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-01-16T08:30:16.393', 'LastEditDate': '2013-01-16T08:30:16.393', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6438', 'Tags': '<algorithms><data-structures><lower-bounds><heaps>', 'CreationDate': '2013-01-16T02:01:40.263', 'Id': '8955'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm thinking about the optimal algorithm for the following problem:</p>\n\n<p>Input data:</p>\n\n<ul>\n<li>a <strong>text</strong>, say it's an article about 5-50 pages.</li>\n<li>a set of <strong>ngrams</strong> (ngram strings, n>2), of arbitrary length, could be more than 20k n-grams.</li>\n</ul>\n\n<p>The algorithm should output the following:</p>\n\n<ul>\n<li>a dictionary of all ngrams that were found in the text with the corresponding quantities, it should also take into account that ngrams could partially intersect or consist of each other (like <em>'probability density', 'probability density function', 'probability density distribution'</em>)</li>\n</ul>\n\n<p>So <strong>the question is</strong> what would be the most time-efficient algorithm to compute this?</p>\n\n<p>Both all words in a text and all words in ngrams are reduced to the canonical forms.</p>\n", 'ViewCount': '169', 'Title': 'Optimal algorithm for finding all ngrams from a pre-defined set in a text', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-27T01:11:03.780', 'LastEditDate': '2013-01-16T21:10:34.413', 'AnswerCount': '4', 'CommentCount': '0', 'AcceptedAnswerId': '8976', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2066', 'Tags': '<algorithms><time-complexity><strings><natural-lang-processing>', 'CreationDate': '2013-01-16T16:38:56.730', 'Id': '8972'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is it possible to use Warshall's algorithm (calculating the transitive closure) to determine if a directed graph is acyclic or not? </p>\n\n<p>I'm trying to achieve this but getting stuck on the reflexive property of the transitive closure!</p>\n", 'ViewCount': '248', 'Title': 'Using transitive closure to determine acyclic property on directed graph', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-17T13:39:46.343', 'LastEditDate': '2013-01-17T13:39:46.343', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'Azz', 'PostTypeId': '1', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-01-16T22:06:41.957', 'Id': '8979'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m having difficulties understanding the solution to the following question:</p>\n\n<blockquote>\n  <p>Consider the following description of a LAN:</p>\n  \n  <p>The main Internet connection is connected to our gateway router. This\n  in turn is part of the backbone network on the 65.21.1.0/24 subnet.\n  The backbone also supports two further routers SALES and RESEARCH.\n  There is one Sales subnetwork 65.21.6.0/23, and two research\n  subnetworks 65.21.100.0/24 and 65.21.200.0/26.</p>\n\n<pre><code>If the Interface addresses are:   \n\n\u2022 i1\u201365.21.1.1\n\u2022 i2\u201365.21.1.2\n\u2022 i3\u201365.21.6.1\n\u2022 i4\u201365.21.1.3\n\u2022 i5\u201365.21.100.1\n\u2022 i6\u201365.21.200.1\n</code></pre>\n  \n  <p>What are the routing tables for the SALES and RESEARCH routers?</p>\n  \n  <p><img src="http://i.stack.imgur.com/XS2Vu.gif" alt="enter image description here"></p>\n</blockquote>\n\n<p>How is the <strong>via</strong> and <strong>interface</strong> column derived in the solution?</p>\n\n<p><img src="http://i.stack.imgur.com/PBm8D.png" alt="enter image description here"></p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '159', 'Title': 'Simple routing table question', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-19T12:20:38.017', 'LastEditDate': '2013-01-17T13:35:13.390', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9039', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1165', 'Tags': '<algorithms><computer-networks><routing>', 'CreationDate': '2013-01-17T09:43:21.247', 'Id': '8984'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>This is a snippet from some pseudocode for a sorting algorithm. In it, the symbol \u2190 is used to denote assignment, for example for the variable <code>done</code>. However, in the <code>while</code> loop the statement <code>done:= false</code> is written. I would assume it is also an assignment statement but I suspect it means somethings else, or perhaps extra, since if not, the \u2190 would have simple be used again.</p>\n\n<pre><code>Algorithm MyAlgorithm(A, n) \n    Input: Array of integer containing n elements \n    Output: Possibly modified Array A\n\ndone \u2190 true \nj \u2190 0\nwhile j \u2264 n - 2 do\n     if A[j] &gt; A[j + 1] then\n        swap(A[j], A[j + 1])\n        done:= false\n     j \u2190 j + 1\n</code></pre>\n', 'ViewCount': '123', 'Title': u'Difference between := and \u2190 in pseudocode', 'LastActivityDate': '2013-01-17T17:20:36.793', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '8996', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-01-17T17:08:05.570', 'Id': '8995'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '150', 'Title': 'Where is the mistake in this apparently-O(n lg n) multiplication algorithm?', 'LastEditDate': '2013-01-18T23:31:19.787', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '535', 'FavoriteCount': '1', 'Body': u'<p>A recent <a href="http://pratikpoddarcse.blogspot.ca/2013/01/evenly-spaced-ones-in-binary-string.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3a%20pratikpoddarcse%20%28CSE%20Blog%29">puzzle blog post</a> about finding three evenly spaced ones lead me to a <a href="http://stackoverflow.com/questions/1560523/onlogn-algorithm-find-three-evenly-spaced-ones-within-binary-string">stackoverflow question</a> with a top answer that claims to do it in O(n lg n) time. The interesting part is that the solution involves squaring a polynomial, referencing a <a href="http://www.cs.iastate.edu/~cs577/handouts/polymultiply.pdf">paper that describes how to do it in O(n lg n) time</a>.</p>\n\n<p>Now, multiplying polynomials is practically the same as multiplying numbers. The only real difference is the lack of carries. But... the carries can also be done in O(n lg n) time. For example:</p>\n\n<pre><code>    var value = 100; // = 0b1100100\n\n    var inputBitCount = value.BitCount(); // 7 (because 2^7 &gt; 100 &gt;= 2^6)\n    var n = inputBitCount * 2; // 14\n    var lgn = n.BitCount(); // 4 (because 2^4 &gt; 14 =&gt; 2^3)\n    var c = lgn + 1; //5; enough space for 2n carries without overflowing\n\n    // do apparently O(n log n) polynomial multiplication\n    var p = ToPolynomialWhereBitsAreCoefficients(value); // x^6 + x^5 + x^2\n    var p2 = SquarePolynomialInNLogNUsingFFT(p); // x^12 + 2x^11 + 2x^10 + x^8 + 2x^7 + x^4\n    var s = CoefficientsOfPolynomial(p2); // [0,0,0,0,1,0,0,2,1,0,2,2,1]\n    // note: s takes O(n lg n) space to store (each value requires at most c-1 bits)\n\n    // propagate carries in O(n c) = O(n lg n) time\n    for (var i = 0; i &lt; n; i++)\n        for (var j = 1; j &lt; c; j++)\n            if (s[i].Bit(j))\n                s[i + j].IncrementInPlace();\n\n    // extract bits of result (in little endian order)\n    var r = new bool[n];\n    for (var i = 0; i &lt; n; i++)\n        r[i] = s[i].Bit(0);\n\n    // r encodes 0b10011100010000 = 10000\n</code></pre>\n\n<p>So my question is this: where\'s the mistake, here? Multiplying numbers in O(n lg n) is a gigantic open problem in computer science, and I really really doubt the answer would be this simple.</p>\n\n<ul>\n<li>Is the carrying wrong, or not O(n lg n)? I\'ve worked out that lg n + 1 bits per value is enough to track the carries, and the algorithm is so simple I\'d be surprised if it was wrong. Note that, although an individual increment can take O(lg n) time, the aggregate cost for x increments is O(x).</li>\n<li>Is the polynomial multiplication algorithm from the paper wrong, or have conditions that I\'m violating? The paper uses a fast fourier transform instead of a number theoretic transform, which could be an issue.</li>\n<li>Have a lot of smart people missed an obvious variant of the <a href="http://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm">Sch\xf6nhage\u2013Strassen algorithm</a> for 40 years? This seems by far the least likely.</li>\n</ul>\n\n<p>I\'ve actually written code to implement this, except for the efficient polynomial multiplication (I don\'t understand the number theoretic transform well enough yet). Random testing appears to confirm the algorithm being correct, so the issue is likely in the time complexity analysis.</p>\n', 'Tags': '<algorithms><time-complexity>', 'LastEditorUserId': '535', 'LastActivityDate': '2013-01-19T05:18:55.963', 'CommentCount': '2', 'AcceptedAnswerId': '9035', 'CreationDate': '2013-01-18T23:26:18.080', 'Id': '9034'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to figure out <strong>a redundant power of two-sided error randomized Karp - reduction.</strong></p>\n\n<p>It\'s well known fact and it is relatively hard to show that <a href="http://en.wikipedia.org/wiki/BPP_(complexity)" rel="nofollow">BPP</a> is reducible by a one-sided error randomized Karp-reduction to coRP (in case of promise problem).</p>\n\n<p>Without delving into details it make sense that the combination of the one - sided error probability of the reduction and the one-sided error probability of coRP leads to two-sided error probability of BPP. Of course the proof of that is not so intuitive.</p>\n\n<p>The question it is possible by two-sided error randomized Karp-reduction to reduce BPP to some constant set in P? In the light of the power of one - sided randomized Karp - reduction, it make sense that two-sided randomized Karp - reduction is strong enough to reduce BPP to constant set, but how to show it formally?</p>\n\n<p><strong>Addendum:</strong></p>\n\n<p><strong>BPP</strong> is the set of the problems that is solvable in polynomial time by two-sided error randomized algorithm, so as a result of two - sided error randomized algorithm we will get some output, them the problem in BPP can be reduced to problem P by two-sided error randomized Karp - reduction in sense that reduction is allowed to make error on both sides. Does it mean that two - sided error randomized reduction will justify the two-sided error that was made by the algorithm in solving the problem in BPP?</p>\n', 'ViewCount': '97', 'Title': 'The Power of Randomized Reduction', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-10-05T07:50:18.583', 'LastEditDate': '2013-01-21T09:54:55.740', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<complexity-theory><reductions><randomized-algorithms>', 'CreationDate': '2013-01-19T09:56:05.707', 'Id': '9037'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that $\\sf BPP[2/3,1/3]= BPP[\\alpha,\\beta]$ when $\\alpha\\lt\\beta$, but I read something on Wikipedia which got me confused:</p>\n\n<blockquote>\n  <p>In practice, an error probability of $1/3$ might not be acceptable, however, the choice of $1/3$ in the definition is arbitrary. It can be any constant between $0$ and $1/2$ (exclusive) and the set $\\mathsf{BPP}$ will be unchanged.</p>\n</blockquote>\n\n<p>The reason for my question is the this question that I'm trying to answer:</p>\n\n<p>We define the class $PP_{\\frac{7}{8}}$: $L \\in PP_{\\frac{7}{8}}$. There's a probabilistic Turing machine that  for $x \\in L$ accepts $x$ with probability $&gt;$ than $\\frac{7}{8}$ and for $x \\notin L$ it accepts $x$ with probabilty $\\leq \\frac{7}{8}$.</p>\n\n<p>So by the $\\alpha, \\beta$ first definition I can conclude  that $PP_{\\frac{7}{8}}$ which  equals to $\\sf BPP[7/8,7/8+\\epsilon]$ also equals to $\\sf BPP[2/3,1/3]$ but the I am asked to prove that $\\sf NP \\subseteq  BPP$ which we don't know yet.</p>\n", 'ViewCount': '52', 'Title': 'BPP clarification', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-21T21:48:07.393', 'LastEditDate': '2013-01-21T21:48:07.393', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<complexity-theory><time-complexity><probabilistic-algorithms>', 'CreationDate': '2013-01-21T21:06:27.130', 'Id': '9077'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to solve exercise 6.5 on page 309 from Richard Crandall\'s "Prime numbers - A computational perspective". It basically asks for an algorithm to factor integers in randomized polynomial time given an oracle for taking square roots modulo $n$.</p>\n\n<p>I think, the basic idea is the following: Given a composite number $n$, to take a random element $r$ in $\\left.\\mathbb{Z}\\middle/n\\mathbb{Z}\\right.$ and square it. If $r$ was a square, $r^2$ can have up to $4$ different square roots and the basic idea of the algorithm is that the oracle has some chance not to choose $\\pm r$, but one of the other two roots. It will turn out that we then can determine a factor of $n$ using Euclidean\'s algorithm. </p>\n\n<p>I formalized this to</p>\n\n<p><strong>Input</strong>: $n=pq\\in\\mathbb{Z}$ with primes $p$ and $q$.</p>\n\n<p><strong>Output</strong>: $p$ or $q$</p>\n\n<ol>\n<li>Take a random number $r$ between $1$ and $n-1$</li>\n<li>If $r\\mid n$ then return $r$ (we were lucky)</li>\n<li>$s:= r^2\\pmod{n}$</li>\n<li>$t:=\\sqrt{s}\\pmod{n}$ (using the oracle)</li>\n<li>If $t\\equiv \\pm r\\pmod{n}$ then goto step 1.</li>\n<li>Return $\\gcd(t-r,n)$</li>\n</ol>\n\n<p>One can show that $t \\not\\equiv \\pm r\\pmod{n}$ implies that $\\gcd(t-r,n)\\neq 1,n$ and therefore get that the return value of the algorithm is a non-trivial factor of $n$. </p>\n\n<p>Inspired by my main question "How do I prove that the running time is polynomial in the bit-size of the input?" I have some follow up questions:</p>\n\n<ol>\n<li>Do I have to show that a lot of numbers between $1$ and $n-1$ are squares? There must be a well-known theorem or easy fact that shows this (well... not well-known to me ;-). </li>\n<li>Are there any more details I have consider? </li>\n<li>Has every square of a square exactly $4$ square roots modulo $n$? </li>\n</ol>\n', 'ViewCount': '109', 'Title': 'Solve Integer Factoring in randomized polynomial time with an oracle for square root modulo $n$', 'LastActivityDate': '2013-01-23T15:57:01.553', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9114', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2103', 'Tags': '<randomized-algorithms><integers><factoring>', 'CreationDate': '2013-01-23T09:39:28.187', 'FavoriteCount': '1', 'Id': '9106'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can someone tell me why almost in every book/website/paper authors use the following:</p>\n\n<pre><code>foreach vertex v in Adjacent(u)\n    relax(u,v)\n</code></pre>\n\n<p>when relaxing the edges, instead of:</p>\n\n<pre><code>foreach vertex v in Adjacent(u)\n    if (v is in Q)\n        relax(u,v)\n</code></pre>\n\n<p>This is extremely confusing for someone when learning the algorithm. Is there any reason why the people are omitting the IF ?</p>\n\n<p>Anyway I wrote a semi-Javascript (I changed it here to a readable syntax) implementation of Dijkstra and I wanted to be sure if it is correct because of this IF case. Here is my code excluding the initialising:</p>\n\n<pre><code>while (queue.length != 0)\n    min = queue.getMinAndRemoveItFromQ()\n    foreach v in min.adjacentVertices\n        // inspect edge from "min" to "v"\n        if ( queue.contains(v) AND min.priority + weight(min,v) &lt; v.priority )\n            v.priority = min.priority + weight(min,v)\n            v.pre = min\n</code></pre>\n\n<p>Is this implementation correct or am I missing something ?</p>\n', 'ViewCount': '204', 'Title': "Why not relax only edges in Q in Dijkstra's algorithm?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-25T13:23:47.117', 'LastEditDate': '2013-01-25T11:30:42.873', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '9121', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5222', 'Tags': '<algorithms><graph-theory><graphs><shortest-path>', 'CreationDate': '2013-01-23T22:07:29.570', 'Id': '9120'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a directed acyclic graph $D = (V,A)$, a vertex $v \\in V$ is a <em>source</em> if its <em>indegree</em> is zero, meaning that it has only outgoing arcs.</p>\n\n<p>Does there exist a linear time algorithm to find a source in a given directed acyclic graph?</p>\n\n<p>Follow-up question:  Can one in linear time find all sources?</p>\n', 'ViewCount': '1940', 'Title': 'Finding a source of a directed acyclic graph in linear time', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-29T16:44:27.813', 'LastEditDate': '2013-01-26T18:01:05.863', 'AnswerCount': '2', 'CommentCount': '9', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1108', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-01-24T17:59:04.973', 'FavoriteCount': '1', 'Id': '9133'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the CLRS book (<a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">http://en.wikipedia.org/wiki/Introduction_to_Algorithms</a>) Chapter 26 (Maximum Flow) page 744 (third edition), there is the following equation -</p>\n\n<p>$$\n\\sum_{u \\in U}e(u) \\;=\\;\n\\sum_{u \\in U}\\;\\sum_{v \\in U}f(v, u) \\;+\\;\n\\sum_{u \\in U}\\;\\sum_{v \\in \\bar{U}}f(v, u) \\;-\\;\n\\sum_{u \\in U}\\;\\sum_{v \\in U}f(u, v) \\;-\\;\n\\sum_{u \\in U}\\;\\sum_{v \\in \\bar{U}}f(u, v)\n$$</p>\n\n<p>where $f(u, v)$ is the flow between vertices $u$ and $v$, $e(u)$ is the excess flow at a particular vertex, $U$ is the set of vertices which are reachable from the source, and $\\bar{U}$ is the set of remaining vertices.</p>\n\n<p>In the next line, the first and third terms disappear. I don\'t understand why that holds. I do realize that those are flow values from vertex $u$ to vertex $v$ where both of them are in the same set $U$, but I don\'t understand why they cancel out to zero.</p>\n', 'ViewCount': '102', 'Title': 'Push relabel algorithms in flow networks', 'LastEditorUserId': '2152', 'LastActivityDate': '2013-04-03T16:18:22.383', 'LastEditDate': '2013-04-03T16:18:22.383', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10993', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6558', 'Tags': '<algorithms><graph-theory><graphs><network-flow>', 'CreationDate': '2013-01-25T16:14:49.193', 'Id': '9153'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to implement efficiently "streaming Knapsack" problem in java.</p>\n\n<p>The problem is I have a stream input of integer data coming continuously for example -1, 2, 9, 5, 5, 11, 1 -3,...</p>\n\n<p>The question is to find the first "k" elements in which their sum is "n>0". for example k=3 and n=12, \nthen the solution is: ...,2,...,5, 5.</p>\n\n<p>I found an answer in <a href="http://programmingpraxis.com/2012/05/15/streaming-knapsack/2/" rel="nofollow">http://programmingpraxis.com/2012/05/15/streaming-knapsack/2/</a> \nas: (It is mainly for positive Integer Values)</p>\n\n<p>But looking for simpler one! Any Ideas?</p>\n', 'ViewCount': '200', 'Title': 'Streaming Knapsack Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-07T11:30:04.040', 'LastEditDate': '2013-01-26T17:50:18.787', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><optimization><streaming-algorithm><online-algorithms>', 'CreationDate': '2013-01-25T18:37:58.193', 'FavoriteCount': '1', 'Id': '9155'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is an algorithm to calculate the maximum "score" possible in a directed graph, with the constraint that edges with the same value can only be traversed once? For example, in the graph below, given that you can use each of <em>a</em>, <em>b</em>, and <em>c</em> once, the best path is to use <em>a</em> and <em>b</em> to get to 7, and <em>c</em> to get to 2, for a score of 9.</p>\n\n<p><img src="http://i.stack.imgur.com/YDBLh.png" alt="enter image description here"></p>\n\n<p><strong>Edit:</strong> This is another way to formulate the problem:<br>\n<strong>Given:</strong> A set $S$ of $(key:value)$ pairs, where the key itself is a non-empty set of variables, eg - $S = \\{(\\{a\\}:2), (\\{c\\}:2), (\\{a, b\\}:7), (\\{b\\}:3), (\\{c\\}:1)\\}$. Keys may not be unique, so the word \'key\' may be misleading.<br>\n<strong>Required:</strong> A subset $S\' \\subseteq S$ such that the total sum of values of each member of $S\'$ is maximized.<br>\n<strong>Constraint:</strong> Each variable present in any of the keys of $S$ (eg. $a, b$ etc.) belongs to the list (key) of exactly one member of $S\'$<br>\nFor example, $S\' = \\{(\\{c\\}:2), (\\{a, b\\}:7)\\}$ has a total value of 9, and each of $a, b, c$ occur in exactly one list.</p>\n', 'ViewCount': '88', 'Title': 'Maximal value of directed graph with constraints', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-01-26T18:55:29.827', 'LastEditDate': '2013-01-26T18:55:29.827', 'AnswerCount': '0', 'CommentCount': '12', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6563', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-01-25T23:22:30.377', 'Id': '9158'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Graham_scan" rel="nofollow">Graham scan</a> algorithm computes the convex hull of a finite sets of points. It works only in the plane but is also fast (time $O(n \\log n)$).</p>\n\n<p>An old exam question asks, why does the algorithm not extend for three dimensional space? I just can\'t find an answer; it seems to me as if it should work.</p>\n\n<ul>\n<li>Sorting the points according to a pivot should not be a problem.</li>\n<li>Detecting a Left/Right turn (or measering the inner angle) neither.</li>\n</ul>\n\n<p>Then what is the problem when we try to extend the algorithm to three dimensions?</p>\n', 'ViewCount': '264', 'Title': 'Why does Graham Scan not extend to three dimensions?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-26T17:58:48.267', 'LastEditDate': '2013-01-26T17:58:48.267', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5222', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-01-26T00:14:19.143', 'Id': '9161'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '138', 'Title': 'Example for the analysis of a recursive function', 'LastEditDate': '2013-01-26T18:10:30.180', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5222', 'FavoriteCount': '1', 'Body': "<pre><code>l is a matrix of size [1...n, 1...n]\n\nfunction: rec(i,j)\n  if (i*j == 0)\n    return 1\n  else\n    if (l[i,j] == 0)\n      l[i,j] = 1 * rec(i-1,j) + 2 * rec(i,j-1) + 3 * rec(i-1,j-1)\n    return l[i,j]\nend_function\n\nfor i=1 to n\n  for j=1 to n\n    l[i,j] = 0\n\nrec(n,n)\n</code></pre>\n\n<p>The nested for's are O(n<sup>2</sup>). But i have difficulties to analyse the recursive part. There is another variation of this example with l as 3d. And the essential part of 3drec function is defined as:</p>\n\n<pre><code>if (l[i,j,k] == 0)\n  l[i,j,k] = 2 * rec(i-1,j,k) + 2 * rec(i,j-1,k) + 2 * rec(i,j,k-1)\n</code></pre>\n\n<p>Anyway let's think about the 2d version again. I thought something like that (that's the running time for the whole code including the nested loops):</p>\n\n<p>T(n) = T(n-1, n<sup>2</sup>) + T(n, n-1<sup>2</sup>) + T(n-1<sup>2</sup>, n-1<sup>2</sup>)</p>\n\n<p>And i'm stuck here. Besides i don't know if i did right till this point.</p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-26T22:54:57.413', 'CommentCount': '4', 'AcceptedAnswerId': '9172', 'CreationDate': '2013-01-26T01:11:04.723', 'Id': '9162'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '266', 'Title': 'Proof of the Stable Matching Problem', 'LastEditDate': '2013-02-06T04:32:07.017', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6577', 'FavoriteCount': '1', 'Body': u'<p>Looking at the document <em>Fundamentals of Computing Series</em>, <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15251-f10/Site/Materials/Lectures/Lecture21/lecture21.pdf" rel="nofollow">The Stable Marriage Problem</a>.</p>\n\n<p>Theorem 1.2.3 - page 12:</p>\n\n<blockquote>\n  <p>In a man-optimal version of stable matching, each woman has worst\n  partner that she can have in any stable matching.</p>\n</blockquote>\n\n<p>Proof:</p>\n\n<blockquote>\n  <p>Suppose not. Let $M_0$ be the man-optimal stable matching, and\n  suppose there is a stable matching $M\u2019$ and a woman $w$ such\n  that $w$ prefers $m = p_{M_0}(w)$ to $m\' = p_{M\'}(w)$ . But\n  then $(m,w)$ blocks $M\'$ unless $m$ prefers $p_{M\'}(m)$ to \n  $w = p_{M_0}(m)$, in contradiction of the fact that $m$ has no\n  stable partner better than his partner in $M_0$.</p>\n</blockquote>\n\n<p>I\'m having trouble visualizing the definition of the problem and the proof (what is the contradiction?).</p>\n\n<p>First, what is the question implying?  From what I read and the fact that in most stable matching examples, all the women do not end up with the completely last person on their list ... So I\'m a bit confused.</p>\n\n<p>In the proof, here is what I am getting: in $M\'$ we suppose $w$ prefers $m$ to $m\'$.  But then if there is a stable matching containing $(m,w)$ this would leave $w$ with her worst partner and that is a contradiction.  Is this correct?</p>\n\n<p>In addition, if $m$ did prefer $w\'$ it would contradict that it is not his first pick ?</p>\n\n<p>I\'m new to computer science concepts so any help is appreciated.</p>\n', 'Tags': '<algorithms><proof-techniques><matching>', 'LastEditorUserId': '6577', 'LastActivityDate': '2013-02-06T04:32:07.017', 'CommentCount': '0', 'AcceptedAnswerId': '9197', 'CreationDate': '2013-01-27T06:02:59.647', 'Id': '9196'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Do we need to use BFS or DFS algorithm to find the k shortest loopless paths in a graph between any two nodes? \nIf so where can it be useful?</p>\n', 'ViewCount': '148', 'Title': 'BFS in K shortest paths', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-28T13:31:19.390', 'LastEditDate': '2013-01-28T10:05:21.847', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '5328', 'Tags': '<algorithms><graphs><shortest-path><graph-traversal>', 'CreationDate': '2013-01-27T16:54:04.673', 'FavoriteCount': '1', 'Id': '9208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a graph with $n \\leq 50 $ vertices. Count all $k$-cliques of this graph, where $k = 1, \\ldots , n$.</p>\n\n<p>I need the most efficient algorithm.</p>\n', 'ViewCount': '170', 'Title': 'Finding all cliques of a graph', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T19:26:57.273', 'LastEditDate': '2013-02-02T19:26:57.273', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6583', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-01-27T17:52:18.483', 'FavoriteCount': '0', 'Id': '9209'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>SAT [5] can be solved with resolution definitively, i.e. if the formula has a true assignment, resolution can find it, and if it cant be satisfied, resolution can show that no assignment exists (at least in exponential time/space.). [4] </p>\n\n<blockquote>\n  <p>Is there a good fully-self contained description somewhere of solving SAT with resolution?</p>\n</blockquote>\n\n<p>The descriptions on Wikipedia of resolution are focused on a single logical operation, not how to use it in an algorithm to solve SAT, and the Davis Putnam algorithm is described mostly in terms of 1st order logic. I am looking for a description of solving SAT with resolution that does not refer to 1st order logic, just in terms of boolean input variables. Online description is preferred if possible. The connection with DPLL would be helpful also.</p>\n\n<hr>\n\n<p>[1] <a href="http://en.wikipedia.org/wiki/Davis-Putnam_algorithm" rel="nofollow">Davis Putnam algorithm</a>, Wikipedia</p>\n\n<p>[2] <a href="http://en.wikipedia.org/wiki/Resolution_%28logic%29" rel="nofollow">Resolution in logic</a>, Wikipedia</p>\n\n<p>[3] <a href="http://en.wikipedia.org/wiki/DPLL_algorithm" rel="nofollow">Davis Putnam Logemann Loveland algorithm</a>, Wikipedia</p>\n\n<p>[4] <a href="http://cs.stackexchange.com/questions/9095/is-resolution-complete-or-only-refutation-complete">Is resolution complete or only refutation-complete?</a></p>\n\n<p>[5] <a href="http://en.wikipedia.org/wiki/Propositional_satisfiability" rel="nofollow">The boolean satisfiability problem</a></p>\n', 'ViewCount': '118', 'Title': 'Description of resolution algorithm as it applies to SAT', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-18T22:02:44.297', 'LastEditDate': '2013-02-18T22:02:44.297', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '9235', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><complexity-theory><reference-request><logic><np-complete>', 'CreationDate': '2013-01-28T01:57:42.843', 'Id': '9233'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was asked to design an algorithm that solves the following problem :</p>\n\n<blockquote>\n  <p>Consider a travel from city A to city B, made of several trips by\n  train through other cities in between. With an access to the\n  unordered list of train tickets from which you can read the cities of\n  departure and arrival of each trip, find A and B.</p>\n</blockquote>\n\n<p>Unfortunately, I was unable to give an efficient algorithm when I needed to (in pseudo code), but once I got home, I came up with this answer (in JavaScript).</p>\n\n<pre><code>var tickets = [\n    {from:'Paris', to:'Berlin'},\n    {from:'London', to:'Paris'},\n    {from:'Zurich', to:'Milan'},\n    {from:'Berlin', to:'Zurich'}\n    ];\n\nfunction getTrip(tickets)\n{\n    var ticket = tickets.shift();\n    var trip = {from:ticket.from, to:ticket.to};\n\n    while(tickets.length &gt; 0) \n    {\n        ticket = tickets.shift();\n\n        if(ticket.from == trip.to)\n        {\n            trip.to = ticket.to;\n        }\n        else if(ticket.to == trip.from)\n        {\n            trip.from = ticket.from;\n        }\n        else\n        {\n            tickets.push(ticket);\n        }\n    }\n\n    return trip;\n}\n\nvar trip = getTrip(tickets);\n\nconsole.log('The trip was from %s to %s', trip.from, trip.to);\n</code></pre>\n\n<p>While this might look like a very simple problem, I am still curious to see if there is a more efficient solution (for both time and space) or simply considerations I completely forgot. This may not need to be in JavaScript (especially if the language gives greater control over some aspects of the problem).</p>\n", 'ViewCount': '111', 'Title': 'Find the origin and the destination of a trip from a serie of tickets', 'LastEditorUserId': '6587', 'LastActivityDate': '2013-05-24T03:28:10.100', 'LastEditDate': '2013-01-28T04:14:01.783', 'AnswerCount': '5', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6587', 'Tags': '<algorithms>', 'CreationDate': '2013-01-28T04:08:20.420', 'Id': '9238'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have seen a few years back a nice and simple algorithm that, given a (finite) set of words in some alphabet, builds a context-free grammar for a language including these words and in some sense "natural" (e.g., the grammar doesn\'t produce all words in the alphabet). The algorithm is very simple,  it has something like 3--4 rules for grammar transformation attempted on each new word. Any help in finding it would be appreciated.</p>\n', 'ViewCount': '181', 'Title': 'Construct a context-free grammar for a given set of words', 'LastEditorUserId': '6591', 'LastActivityDate': '2013-12-10T07:31:05.390', 'LastEditDate': '2013-01-28T15:39:42.217', 'AnswerCount': '2', 'CommentCount': '9', 'AcceptedAnswerId': '9268', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6591', 'Tags': '<algorithms><formal-languages><reference-request><formal-grammars><machine-learning>', 'CreationDate': '2013-01-28T10:01:35.720', 'Id': '9246'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that we take an initial search problem and we add $c &gt; 0$ to the costs on all edges. Will <a href="http://en.wikipedia.org/wiki/Uniform-cost_search" rel="nofollow">uniform-cost search</a> return the same answer as in the initial search problem?</p>\n\n<p>Definitions: Uniform-cost search is also known as lowest cost first. Initial search problem can be any graph with a start and a goal state. You just apply the uniform cost search algorithm on the graph. </p>\n', 'ViewCount': '426', 'Title': 'Uniform-cost Search Problem', 'LastEditorUserId': '867', 'LastActivityDate': '2013-11-19T07:11:47.707', 'LastEditDate': '2013-01-29T03:40:50.913', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6601', 'Tags': '<graphs><search-algorithms><search-trees><search-problem>', 'CreationDate': '2013-01-29T02:33:45.660', 'Id': '9265'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for algorithms to prioritize equipment renewals.</p>\n\n<p>Input: (years since last renewal, cost of renewal, importance of renewal).</p>\n\n<p>Output: An ordering of the equipment according to which it will be renewed.</p>\n\n<p>I do not know if there are any algorithms for this particular problem. If you have any idea how to fit this problem into a more general context, that would be useful too.</p>\n\n<p>A way to rephrase the problem:</p>\n\n<p>You have $n$ pieces of equipment $E_1,\\ldots,E_n$. For each piece $E_i$ you have a triple $(\\text{age}_i,\\text{cost}_i,\\text{importance}_i)$. At the beginning of the year you have $X$ amount of money. You want to spend these money in order to <em>minimize</em> the function $\\sum_i \\text{age}_i\\cdot \\text{importance}_i$ at the end of the year. So, during the year you have to select a subset $S$ of $\\{1,\\ldots,n\\}$ such that:\n$$\\sum_{i\\in S} \\text{cost}_i\\le X \\text{ (cost constraint)}$$ and the sum $$\\sum_{i\\in S} \\text{age}_i\\cdot \\text{importance}_i$$ is <em>maximal</em> among all subsets of $\\{1,\\ldots,n\\}$ that satisfy the cost constraint.</p>\n\n<p>Any help?</p>\n', 'ViewCount': '67', 'Title': 'Algorithms to prioritize equipment renewals', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-30T17:19:11.647', 'LastEditDate': '2013-01-30T17:19:11.647', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '9286', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6610', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-01-29T17:36:29.230', 'Id': '9282'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On Facebook HackerCup 2013, they asked the following question:</p>\n\n<blockquote>\n  <p>Your friend John uses a lot of emoticons when you talk to him on\n  Messenger. In addition to being a person who likes to express himself\n  through emoticons, he hates unbalanced parenthesis so much that it\n  makes him go :(</p>\n  \n  <p>Sometimes he puts emoticons within parentheses, and you find it hard\n  to tell if a parenthesis really is a parenthesis or part of an\n  emoticon.</p>\n  \n  <p>A message has balanced parentheses if it consists of one of the\n  following:</p>\n  \n  <ul>\n  <li>An empty string "" </li>\n  <li>One or more of the following characters: \'a\' to\n  \'z\', \' \' (a space) or \':\' (a colon) </li>\n  <li>An open parenthesis \'(\', followed\n  by a message with balanced parentheses, followed by a close\n  parenthesis \')\'. </li>\n  <li>A message with balanced parentheses followed by\n  another message with balanced parentheses. </li>\n  <li>A smiley face ":)" or a\n  frowny face ":(" </li>\n  </ul>\n  \n  <p>Write a program that determines if there is a way to\n  interpret his message while leaving the parentheses balanced.</p>\n</blockquote>\n\n<p><a href="http://stackoverflow.com/questions/6447289/how-to-print-all-possible-balanced-parentheses-for-an-expression">Balancing parentheses</a> is talked about in a lot of places.  E.g. <code>(()) vs ()()</code>.  You can count</p>\n\n<ul>\n<li><code>\'(\' = +1 </code> </li>\n<li><code>\')\' = -1 </code></li>\n</ul>\n\n<p>Then you have to make sure your sum never falls below <strong>0</strong>. </p>\n\n<p>In this question they consider <code>:)</code> and <code>:(</code> as balanced and I wonder how much it changes things. One <a href="https://gist.github.com/4660602" rel="nofollow">solution</a> says that you can replace <code>r\'[^a-z:() ], \'\'</code>, <code>\':)\' -> \'}\'</code> and <code>\':(\' -> \'{\'</code>.  </p>\n\n<p>While I agree with the first sub, why are the last two substitutions valid?</p>\n', 'ViewCount': '177', 'Title': 'Facebook Hackercup 2013: Balanced Smileys', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-30T12:46:36.447', 'LastEditDate': '2013-01-30T12:46:36.447', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><context-free><binary-trees><parsing>', 'CreationDate': '2013-01-29T22:41:19.300', 'Id': '9285'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p><strong>Problem:</strong> Consider a graph $G = (V, E)$ on $n$ vertices and $m &gt; n$ edges, $u$ and $v$ are two vertices of $G$.</p>\n  \n  <p>What is the asymptotic complexity to calculate the shortest path from $u$ to $v$ with Dijkstra\'s algorithm using <a href="http://en.wikipedia.org/wiki/Binary_heap" rel="nofollow">Binary Heap</a> ?</p>\n</blockquote>\n\n<p>To clarify, Dijkstra\'s algorithm is run from the source and allowed to terminate when it reaches the target. Knowing that the target is a neighbor of the source, what is the time complexity of the algorithm?</p>\n\n<p><strong>My idea:</strong></p>\n\n<p>Dijkstra\'s algorithm in this case makes $O(n)$ <strong>inserts</strong> ( $n$ if the graph is complete) and 1 <strong>extract min</strong> in the binary heap, before calculate the shortest path from $u$ to $v$.</p>\n\n<p>In a binary heap insert costs $O(\\log n)$ and extract min $O(\\log n)$ too.</p>\n\n<p>So the cost in my opinion is $O(n \\cdot \\log n + \\log n) = O(n \\log n)$</p>\n\n<p>But the answer is $\\Theta(n)$, so there is something wrong in my thinking.</p>\n\n<p>Where is my mistake?</p>\n', 'ViewCount': '793', 'Title': "What's the complexity of calculating the shortest path from $u$ to $v$ with Dijkstra's algorithm using binary heap?", 'LastEditorUserId': '3011', 'LastActivityDate': '2013-01-30T17:46:35.080', 'LastEditDate': '2013-01-30T17:41:32.873', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '9319', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<algorithms><graphs><algorithm-analysis><runtime-analysis><shortest-path>', 'CreationDate': '2013-01-30T16:17:56.463', 'Id': '9314'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '161', 'Title': 'Convergence of Simulated Annealing Based Algorithms', 'LastEditDate': '2013-01-31T14:01:23.310', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6550', 'FavoriteCount': '2', 'Body': "<p>I designed a simulated annealing-based optimization algorithm. My simulation shows that it converge fast. I am looking for some sort of proof to show that simulation annealing-based algorithm converge fast (based on satisfying some properties) to global/local optimal point and doesn't oscillate in the optimal points (or any related fast about its stability). Are there any useful literature about it?</p>\n", 'Tags': '<algorithms><reference-request><proof-techniques><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T04:03:50.740', 'CommentCount': '7', 'AcceptedAnswerId': '9400', 'CreationDate': '2013-01-31T00:36:58.170', 'Id': '9340'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>An assignment question asks me to find the complexity of a [tail] recursive algorithm, copied below. While I understand all the complexity specifics, for example that the while loop's complexity is $n-1$ and the complexity of setting $j$ to $0$ is 1, I don't understand how I could trace the code recursively, that is within itsel - it's too hard to keep track of. </p>\n\n<p>What I tried doing, is turning the algorithm into an iterative one, by simply putting all the code into a big while loop and thus avoiding the recursive call. But I'm not sure if this affects the complexity of the original algorithm.</p>\n\n<pre><code>Algorithm MyAlgorithm(A, n) \n   Input: Array of integer containing n elements \n   Output: Possibly modified Array A\n     done \u2190 true \n     j \u2190 0\n     while j \u2264 n - 2 do {\n       if A[j] &gt; A[j + 1] then {\n       swap(A[j], A[j + 1])\n       done:= false\n       }\n     j \u2190 j + 1\n     end while\n     j \u2190 n - 1\n     while j \u2265 1 do\n       if A[j] &lt; A[j - 1] then\n       swap(A[j - 1], A[j])\n       done:= false\n    j \u2190 j - 1\n    end while\n    if \xac done\n       MyAlgorithm(A, n)\n    else\n      return A\n</code></pre>\n", 'ViewCount': '278', 'Title': 'Finding the complexity of a recursive method', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-01T10:51:52.300', 'LastEditDate': '2013-02-01T10:51:52.300', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-01-31T16:59:09.883', 'Id': '9360'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am interested in precise time complexity of distributed algorithm for finding MIS (Maximum Independent Set) of a given graph $G$.</p>\n\n<p>I investigate the Slow MIS distributed algorithm (from <a href="http://disco.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">these lecture notes</a>, page 2).</p>\n\n<p>Following is the more detailed version than in lecture notes.</p>\n\n<ul>\n<li>Every node sends its UID to it\'s neighbors.</li>\n<li>Run procedure join</li>\n</ul>\n\n<p><strong>on event</strong>: getting a message <em>decide(1)</em>  from a neighbor $w$ do</p>\n\n<ul>\n<li><p>Set $b = 0$ - flag that node terminates and not participating in further phases.</p></li>\n<li><p>Send <em>decide(0)</em> to all neighbors</p></li>\n</ul>\n\n<p><strong>on event</strong>: getting a message <em>decide(0)</em> from a neighbor $w$ do:</p>\n\n<ul>\n<li>Invoke procedure <strong>join</strong>.</li>\n</ul>\n\n<p>Procedure <strong>Join</strong></p>\n\n<p>if every neighbor $w$ of $v$ with a larger identifier has decided $b(w) = 0$, then do</p>\n\n<ul>\n<li>Set $b = 1$.</li>\n<li>Send <em>decided(1)</em> to all neighbors.</li>\n</ul>\n\n<p>The question is what\'s time complexity of the algorithm $\\Theta(n)$ or $\\Theta(D)$, where $D$ is a diameter of $G$.</p>\n\n<p>In the lecture notes linked above, they say that time complexity is $O(n)$. I  think that in our case it can be expressed as $\\Theta(D)$ (simultaneously $O(D)$ and $\\Omega(D)$) for special cases.</p>\n\n<p>The problem is how to prove that that time complexity in general is $O(D)$ and there are special cases when time complexity is $\\Omega(D)$ if it\'s right at all.</p>\n\n<p>Let\'s take a look at the example I have in mind.</p>\n\n<p><img src="http://i.stack.imgur.com/Q3dGs.png" alt="enter image description here"></p>\n\n<p>$D=1$ and $n=4$, and as I understood the algorithm every vertex will decide to join MIS on the first round.</p>\n\n<p>If you have an idea how to show that, please, share it with us. </p>\n', 'ViewCount': '131', 'Title': 'Slow MIS Distributed Algorithm', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-02-03T17:10:12.623', 'LastEditDate': '2013-02-02T19:53:16.413', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><distributed-systems>', 'CreationDate': '2013-02-01T06:56:49.720', 'FavoriteCount': '1', 'Id': '9379'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1342', 'Title': 'Why is push_back in C++ vectors constant amortized?', 'LastEditDate': '2013-02-01T22:45:15.410', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '2', 'Body': '<p>I am learning C++ and noticed that the running time for the push_back function for vectors is constant "amortized." The documentation further notes that "If a reallocation happens, the reallocation is itself up to linear in the entire size."</p>\n\n<p>Shouldn\'t this mean the push_back function is $O(n)$, where $n$ is the length of the vector? After all, we are interested in worst case analysis, right?</p>\n\n<p>I guess, crucially, I don\'t understand how the adjective "amortized" changes the running time.</p>\n', 'Tags': '<algorithms><time-complexity><amortized-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-02-01T22:45:15.410', 'CommentCount': '1', 'AcceptedAnswerId': '9382', 'CreationDate': '2013-02-01T07:17:05.997', 'Id': '9380'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I don\'t know the right name for this problem, or if there is a name, but it is inspired by my initial interpretation of the title of <a href="http://cs.stackexchange.com/questions/9155/streaming-knapsack-problem">this</a> question (my question is very different, so the link may be misleading). Anyways, my question is this:</p>\n\n<p>We are initially given a list of "items" to be filled in a knapsack of fixed size. Each item has a weight (bounded, integral) and value, and we need to maximize the total value of items in the knapsack. So far, this is identical to the 0/1 Knapsack problem. Now, at each step, we perform one of the following:</p>\n\n<ul>\n<li>Remove the first item in the list (first means encountered earliest)</li>\n<li>Add a new item to the list at the end.</li>\n</ul>\n\n<p>To keep the solution space small, we can assume that the maximum size of the list is fixed, so that it will behave like a fixed size buffer overflow - oldest item is removed before new item is added.</p>\n\n<p>Now, the list is smallish, so the initial instance of the knapsack on the original list can be performed to obtain the first solution. Now, <strong>after <em>every</em> operation</strong> on the list (addition or removal of items), we again want to find out the best way to fill a <strong>new (empty) knapsack with the items in the new list</strong>. And we want to do it <strong>without repeating a full knapsack algorithm</strong> on this slightly modified list (since there will be many such operations).</p>\n\n<p>Is there some way the results of the previous state can be utilized to speed up the process? Is there some information from the previous state that can <em>usually</em> speed up the process? Is there any research on this or some related problem? </p>\n\n<p>The pseudo-polynomial time DP algorithm can be adapted for the case where an item is added (since the table depends on the previous items), but I could not figure out how to deal with it in case the first item is removed from the list. Similarly, a branch-and-bound approach seems pointless. Any ideas or references?</p>\n', 'ViewCount': '189', 'Title': 'Dynamic Knapsack Problem - Algorithms and References', 'LastEditorUserId': '4751', 'LastActivityDate': '2013-02-01T19:15:14.520', 'LastEditDate': '2013-02-01T19:15:14.520', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4751', 'Tags': '<algorithms><reference-request><optimization><combinatorics><knapsack-problems>', 'CreationDate': '2013-02-01T11:46:06.723', 'Id': '9384'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been tasked with building a library of books on algorithms for our small company (about 15 people). The budget is more than 5k, but certainly less than 10k, so I can buy a fair number of books. All people here have at least a Bachelor's degree in CS or a closely related field, so while I will get some basic textbook like Cormen, I'm more interested in good books on advanced topics. (I will get Knuth's 4 volumes, BTW.)</p>\n\n<p>Some list of topics would be:</p>\n\n<ul>\n<li><p>Sorting algorithms</p></li>\n<li><p>Graph algorithms</p></li>\n<li><p>String algorithms</p></li>\n<li><p>Randomized algorithms</p></li>\n<li><p>Distributed algorithms</p></li>\n<li><p>Combinatorial algorithms</p></li>\n<li><p>etc.</p></li>\n</ul>\n\n<p>Essentially I'm looking for good recommendations on books about major topics within CS related to algorithms and data structures. Especially stuff that goes beyond what's typically covered in algorithm and data structure classes as part of a Bachelor's degree at a good school. I know the question is quite fuzzy, since I'm looking for generically useful material. The software we develop is mostly system level stuff handling large amounts of data.</p>\n\n<p>The ideal would also be to find anything that would cover fairly recent cool data structures and algorithms, which most people might not have heard about.</p>\n\n<hr>\n\n<p>EDIT: Here are some preliminary books that I think I should get:</p>\n\n<ul>\n<li><p>Introduction to Algorithms by Cormen et al. </p></li>\n<li><p>Algorithm Design by Kleinberg, Tardos</p></li>\n<li><p>The Art of Computer Programming Vol 1-4 by Knuth</p></li>\n<li><p>Approximation Algorithms by Vazirani</p></li>\n<li><p>The Design of Approximation Algorithms by Williamson, Shmoys</p></li>\n<li><p>Randomized Algorithms by Motwani, Raghavan</p></li>\n<li><p>Introduction to the Theory of Computation by Sipser</p></li>\n<li><p>Computational Complexity by Arora, Barak</p></li>\n<li><p>Computers and Intractability by Garey and Johnson</p></li>\n<li><p>Combinatorial Optimization by Schrijver</p></li>\n</ul>\n\n<p>A few other books my colleagues wanted that deal with techniques and algorithms for language design, compilers and formal methods are:</p>\n\n<ul>\n<li><p>Types and Programming Languages by Pierce</p></li>\n<li><p>Principles of Model Checking by Baier, Katoen</p></li>\n<li><p>Compilers: Principles, Techniques, and Tools by Aho, Lam, Sethi, Ullman</p></li>\n<li><p>The Compiler Design Handbook: Optimizations and Machine Code Generation, Second Edition by Srikant, Shankar</p></li>\n<li><p>The Garbage Collection Handbook: The Art of Automatic Memory Management by Jones, Hosking, Moss</p></li>\n</ul>\n", 'ViewCount': '368', 'Title': 'Algorithm books on a range of topics', 'LastEditorUserId': '6675', 'LastActivityDate': '2013-02-02T21:46:22.347', 'LastEditDate': '2013-02-02T19:14:58.860', 'AnswerCount': '5', 'CommentCount': '10', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6675', 'Tags': '<algorithms><reference-request><data-structures><books>', 'CreationDate': '2013-02-02T05:57:06.183', 'FavoriteCount': '2', 'Id': '9413'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I read a <a href="http://cstheory.stackexchange.com/q/9750/13562">post</a> which talks about pretty much the same problem. But here I simplify the problem hoping that a concrete proof can be offered.</p>\n\n<p>There is a set $A$ which contains some discrete points (one-dimensional), like $\\{1, 3, 37, 59\\}$. I want to pick one point from $A$ which minimizes the sum of distances between this point and others.</p>\n\n<p>There may be lot of posts out there, and my problem is just the one-dimensional version of those. I know how to prove it if $A$ is not discrete, but I fail when $A$ is discrete like above.</p>\n\n<p>Please answer with a concrete proof.</p>\n', 'ViewCount': '91', 'Title': 'Choose a "middle" point from a set', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-03T03:07:38.757', 'LastEditDate': '2013-02-02T21:33:57.517', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'OwnerDisplayName': 'loganecolss', 'PostTypeId': '1', 'OwnerUserId': '6686', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-02-02T12:27:53.440', 'Id': '9435'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '983', 'Title': 'Algorithm for building a suffix array in time $O(n \\log^2 n)$', 'LastEditDate': '2013-02-04T15:51:33.583', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4916', 'FavoriteCount': '1', 'Body': '<p>I\'ve been working with suffix arrays lately, and I can\'t find an efficient algorithm for building a suffix array which is easy to understand.  I have seen in many sites that there is an $O(n \\log^2 n)$ algorithm, but I can\'t understand it, as many important details are omitted.  There\'s an example at <a href="http://apps.topcoder.com/forums/?module=Thread&amp;threadID=627379&amp;start=0&amp;mc=33#1039014" rel="nofollow">Top Coder</a>.</p>\n\n<p>Could someone introduce me an efficient algorithm for suffix array construction, which is easy to comprehend?</p>\n', 'Tags': '<algorithms><data-structures><strings>', 'LastEditorUserId': '4916', 'LastActivityDate': '2013-02-05T22:22:27.363', 'CommentCount': '0', 'AcceptedAnswerId': '9466', 'CreationDate': '2013-02-03T13:11:42.780', 'Id': '9447'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it true or false that for running a dfs on an undirected graph G with a simple cycle than this cycle will have exactly one back edge?</p>\n\n<p>Looks to me likes its true ,is it?</p>\n', 'ViewCount': '122', 'Title': 'Dfs algorithm and cycles question', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-04T12:13:14.260', 'LastEditDate': '2013-02-04T12:13:14.260', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'Nusha', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><search-algorithms><graph-traversal>', 'CreationDate': '2013-02-03T11:26:50.813', 'Id': '9458'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>There is a square table composed of <code>N*N</code> Cells. Initially all cell is filled with a number <code>0</code>.\nTwo types of Operations can be performed on the table.</p>\n\n<p>R i k: All the numbers in the cells on ith row has been changed to k.</p>\n\n<p>C i k: All the numbers in the cells on ith column has been changed to k.</p>\n\n<p>Please Note : <strong>k \u2208 {0, 1} (ie k = 0 or 1).</strong></p>\n\n<p>At any time ,Alice is interested to know  the total number of 0's on some row or total number of 0's on some column.</p>\n\n<p>Our task is to answer Alice Query.</p>\n\n<p><em><strong>Input</em></strong></p>\n\n<p>First line contains a single integer N as described above.Next line contains a single integer Q representing the total operations and queries from Alice.\nThen Followed Q lines , containing operations or queries in following format:</p>\n\n<pre><code>R i k: All the numbers in the cells ith row has been changed to k.\n\nC i k: All the numbers in the cells ith column has been changed to k.\n\nqc i :How many 0's are there in ith row?\n\nqr i:How many cols are there in ith col?\n</code></pre>\n\n<p><strong>Examples</strong>:\nInput:</p>\n\n<pre><code>3\n\n6\n\nqr 1\n\nC 1 1\n\nqr 1\n\nqc 1\n\nR 1 0\n\nqc 1\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>3\n\n2\n\n0\n\n1\n</code></pre>\n\n<p>I tried a Naive solution.But it got time out. The constraint is quite large and the time limit is just 0.55 Seconds.</p>\n\n<p>**1 \u2264 N, Q \u2264 500000 (6 * 10^5)</p>\n\n<p>How can i solve this problem within the given time-limit?</p>\n\n<p>P.S: <em>This is not a homework</em>.</p>\n", 'ViewCount': '62', 'Title': '0 and 1 Queries in tables of N*N cells', 'LastActivityDate': '2013-02-05T12:34:19.653', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-02-04T06:06:42.547', 'Id': '9465'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the following problem:</p>\n\n<blockquote>\n  <p>Given an edge-colored DAG $G = (V,A)$, vertices $s$ and $t$, a set of colors $C$ and $k \\in \\mathbb{N}$,<br>\n   does there exist a path from $s$ to $t$ using exactly $k$ distinct colors?</p>\n</blockquote>\n\n<p>Can anyone provide pointers to the complexity of this problem?  More specific, is it $\\mathsf{NP}$-complete or in $\\mathsf{P}$?</p>\n', 'ViewCount': '84', 'Title': 'Path on an edge-colored DAG using exactly $k$ colors', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-02-05T10:28:30.163', 'LastEditDate': '2013-02-05T10:28:30.163', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '1', 'OwnerDisplayName': 'user547616', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2013-02-01T07:27:24.787', 'Id': '9490'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p><strong>Input:</strong>\nA set of $\\ell$ arrays $A_i$ (of numbers). \n<br>\nThe elements within each array are in sorted order, but the set of arrays is not necessarily sorted. The arrays are not necessarily the same size. The total number of elements is $n$.</p>\n\n<p><strong>Output:</strong>\nThe $k$th smallest element out of all elements in the input.</p>\n\n<p>What's the most efficient algorithm for this problem?</p>\n\n<p>Is it possible, for example to achieve a running time of $O(\\ell + \\log n)$?</p>\n", 'ViewCount': '278', 'Title': 'Find the median of a list of lists', 'LastEditorUserId': '71', 'LastActivityDate': '2013-02-13T00:06:56.700', 'LastEditDate': '2013-02-06T01:21:59.107', 'AnswerCount': '4', 'CommentCount': '8', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-02-04T23:55:34.930', 'FavoriteCount': '1', 'Id': '9497'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I molded my problem as the following game (it is a <a href="http://en.wikipedia.org/wiki/Congestion_game" rel="nofollow">congestion game</a> with varying price):</p>\n\n<p>$N$ players share resources $E$,</p>\n\n<p>$S_i$ is the strategy space of player $i$ which is in $2^E$ (where $2^E$ is the power set of resources).</p>\n\n<p>$P_e^i$ is the price of resource $e \\in E$ considering player $i$. <strong>The price of resource $e$ is different for different users.</strong></p>\n\n<p>The goal of each player is to select a strategy $S_i$ which minimize its price $\\sum_{e\\in S_i}P_e^i$ .</p>\n\n<p>My questions are:</p>\n\n<ol>\n<li>Does this game have any <a href="http://en.wikipedia.org/wiki/Nash_equilibrium" rel="nofollow">Nash Equilibrium</a> (NE)? If so under which conditions? </li>\n<li>If it has any NE, what is a sample algorithm for achieving it?</li>\n</ol>\n\n<p>I searched literature but could not find any appropriate information! Any solution is appreciated!</p>\n', 'ViewCount': '52', 'Title': 'Congestion Game with Varying Price', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-09-13T13:49:18.273', 'LastEditDate': '2013-02-10T14:36:23.537', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><optimization><game-theory>', 'CreationDate': '2013-02-05T16:41:45.657', 'FavoriteCount': '2', 'Id': '9512'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m reading <a href="http://bero.freqvibez.net/public/segs/icassp03_senseg.pdf" rel="nofollow">\u201cSpeech segmentation without speech recognition\u201d by Dong Wang, Lie Lu and Hong-Jiang Zhang</a>.\nThe algorithm I\'m looking at is a V/C/P (Vowel/Consonant/Pause) classification algorithm on a digital speech signal. It is described as such:</p>\n\n<ol>\n<li><p>Audio data is segmented into 20ms-long non-overlapping frames, \nwhere features, including ZCR, Energy and Pitch, are extracted.  </p></li>\n<li><p>Energy and pitch curve is smoothed. </p></li>\n<li><p>The  <code>Mean_En</code> and  <code>Std_En</code> of energy curve are calculated to \ncoarsely estimate the background noise energy level, as: </p>\n\n<pre><code>NoiseLevel = Mean_En - 0.75 Std_En. \n</code></pre>\n\n<p>Similarly the threshold of ZCR (<code>ZCR_dyna</code>) is defined as: </p>\n\n<pre><code>ZCR_dyna = Mean_ZCR + 0.5 Std_ZCR\n</code></pre></li>\n<li><p>Frames are classified as V/C/P coarsely by using the following \nrules, where FrameType is used to denote the type of each frame. </p>\n\n<pre><code>If ZCR &gt; ZCR_dyna then FrameType = Consonant \nElse if Energy &lt; NoiseLevel, then  FrameType = Pause \nElse FrameType = Vowel   \n</code></pre></li>\n<li><p>Update the <code>NoiseLevel</code> as the weighted average energy of the \nframes at each vowel boundary and the background segments. </p></li>\n<li><p>Re-classify the frames using algorithm in step 4 with the updated \n<code>NoiseLevel</code>. Pauses are merged by removing isolated short \nconsonants. Vowel will be split at its energy valley if its duration is \ntoo long  </p></li>\n</ol>\n\n<p>I do not understand step #5. Like I don\'t know how to interpret the wording - is there another way to describe what they are doing? I get that we want to update the <code>NoiseLevel</code> variable and re-run step #4 for every frame, I just don\'t understand how exactly. </p>\n', 'ViewCount': '49', 'Title': 'Help understanding an audio processing algorithm', 'LastEditorUserId': '39', 'LastActivityDate': '2013-02-05T20:16:13.140', 'LastEditDate': '2013-02-05T20:16:13.140', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6720', 'Tags': '<algorithms><machine-learning><natural-lang-processing><signal-processing>', 'CreationDate': '2013-02-05T16:56:55.613', 'Id': '9513'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '621', 'Title': 'Finding the minimum subset of intervals covering the whole set', 'LastEditDate': '2013-02-06T07:53:35.410', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6723', 'FavoriteCount': '2', 'Body': '<p>Suppose we have a set $A$ of pairs $(a,b)$ such that $a$ and $b$ are real numbers and $a &lt; b$. What is the most efficient algorithm to find the smallest subset $B \\subseteq A$ such that, for any value within the range of any pair in $A$, the value is also within the range of any pair in $B$. </p>\n\n<p>I am currently considering setting $B$ equal to $A$ and, for each pair in $B$, if removing the pair still upholds the property of $B$ described above, remove it. However, I feel this algorithm is too inefficient. Is there a more efficient algorithm? </p>\n', 'Tags': '<algorithms><sets>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-06T14:52:43.370', 'CommentCount': '2', 'AcceptedAnswerId': '9536', 'CreationDate': '2013-02-06T00:18:54.913', 'Id': '9531'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some difficulties in understanding distributed algorithm for tree 6 - coloring in $O(\\log^*n)$ time.</p>\n\n<p>The full description can be found in following paper: <a href="http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1613&amp;context=cstech" rel="nofollow">Parallel Symmetry-Breaking in Sparse Graphs. Goldberg, Plotkin, Shannon</a>.</p>\n\n<p>In short, the idea is ...</p>\n\n<p>Starting from the valid coloring given by the processor ID\'s, the procedure iteratively reduces the number of bits in the color descriptions by recoloring each nonroot node $v$ with the color obtained by concatenating the index of a bit in which $C_v$ differs from $C_{parent}(v)$ and the value of this bit. The root $r$  concatenates $0$ and $C_r[0]$ to form its new color.</p>\n\n<p><strong>The algorithm terminates after $O(\\log^*n)$ iterations.</strong></p>\n\n<p>I don\' have the intuitive understanding why it\'s actually terminates in $O(\\log^*n)$ iterations. As it\'s mentioned in the paper on the final iteration there is the smallest index where two bit string differs is at most 3. So 0th bit and 1th bit could be the same and $2^2=4$, so this two bit will give us 4 colors + another 2 colors for different 3th bit, and in total 8 colors and not 6 like in the paper, and why we cannot proceed further with 2 bits, it\'s still possible to find different bits and separate them.</p>\n\n<p>I would appreciate a little bit deeper analysis of the algorithm than in the paper.</p>\n', 'ViewCount': '101', 'Title': '6-coloring of a tree in a distributed manner', 'LastEditorUserId': '31', 'LastActivityDate': '2014-05-03T20:56:08.843', 'LastEditDate': '2013-02-06T13:18:13.150', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><time-complexity><colorings>', 'CreationDate': '2013-02-06T10:58:35.073', 'Id': '9539'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there a linear-time algorithm to check that a sequence of characters is a concatenation of palindromes? The only thing that comes to my mind is the naive solution:</p>\n\n<pre><code>1. k = 1\n2. Split string into k substrings (all possibilities) and check\n3. k++\n4. repeat\n</code></pre>\n\n<p>Note: the answer is trivially yes if length 1-strings are defined to be palindromes. Let's assume that this is not the case.</p>\n", 'ViewCount': '1027', 'Title': 'Is there an algorithm for checking if a string is a catenation of palindromes?', 'LastEditorUserId': '2499', 'LastActivityDate': '2013-04-22T17:01:18.820', 'LastEditDate': '2013-04-22T17:01:18.820', 'AnswerCount': '3', 'CommentCount': '9', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><efficiency><strings>', 'CreationDate': '2013-02-06T11:22:25.257', 'FavoriteCount': '2', 'Id': '9540'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Problem:</p>\n\n<p>Given 11 numbers</p>\n\n<pre><code>{N1,N2,N3,N4,N5,N6,N7,N8,N9,N10,N11}\n</code></pre>\n\n<p>where</p>\n\n<ul>\n<li><p>N1:amount of profit from product A</p>\n\n<p>N2:amount of profit from Product B </p>\n\n<p>N3:amount of time in <strong>hours</strong> required to make A in factory1(F1)</p>\n\n<p>N4:amount of time in <strong>hours</strong> required to make B in factory1(F1)</p>\n\n<p>N5:maximum number of hours available in a week to make products in\nF1</p>\n\n<p>N6:amount of time in <strong>hours</strong> required to make A in factory2(F2)</p>\n\n<p>N7:amount of time in <strong>hours</strong> required to make B in factory2(F2)</p>\n\n<p>N8:maximum number of hours available in a week to make products in\nF2</p>\n\n<p>N9:amount time in <strong>hours</strong> required to make A in factory3(F3)</p>\n\n<p>N10:amount time in <strong>hours</strong> required to make B in factory3(F3)</p>\n\n<p>N11:maximum number of hours available in a week to make products in\nF3</p></li>\n</ul>\n\n<p>Output: </p>\n\n<p>3 numbers <code>{R1,R2,R3}</code> or <code>"-1"</code></p>\n\n<p>where:</p>\n\n<pre><code>R1:total number of A\'s to manufacture in a week\n\nR2:total number of B\'s to manufacture in a week\n\nR3:total profit which is **maximum possible profit** with the right mix of A and B\n\n**result = `-1` if R1 and/or R2 is non integer(contains fraction)**\n</code></pre>\n\n<p>consider the Input <code>{10.5,13,3,7,5,6,5,11,16,11,21.6}</code></p>\n\n<pre><code>                    PRODUCT A               PRODUCT B        total hours/Week\n\nProfit/Piece             10.5                  13\n\nHours required in F1     3                     7                   5\n\nHours required in F2     6                     5                   11\n\nHours required in F3     16                    11                  21.6 \n</code></pre>\n\n<p>Any Idea/Algorithm?</p>\n\n<p>Thanks in Advance</p>\n', 'ViewCount': '208', 'Title': 'Maximizing profit', 'LastEditorUserId': '6709', 'LastActivityDate': '2013-02-06T21:53:01.297', 'LastEditDate': '2013-02-06T17:44:48.183', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '-6', 'PostTypeId': '1', 'OwnerUserId': '6709', 'Tags': '<algorithms><dynamic-programming><knapsack-problems>', 'CreationDate': '2013-02-06T14:37:10.063', 'FavoriteCount': '0', 'Id': '9543'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p><em><strong>Article:</em></strong>  </p>\n\n<h1>CC-Radix: a Cache Conscious Sorting Based on Radix sort</h1>\n\n<p><strong>(IEEE 2003)</strong></p>\n\n<p>I\'m trying to figure out what the author means by this section:</p>\n\n<blockquote>\n  <p>Explanation of CC-Radix For clarity reasons, we explain the recursive\n  version of CC-Radix sort as shown in Figure 3. However, we use the\n  iterative implementation of CC-Radix for the evaluations in this paper\n  because it is more efficient. The parameters of CC-Radix are bucket, which\n  is the data set to be sorted, and b, which is the number of bits of the\n  key that still have to be sorted. Constant b is explained below.</p>\n\n<pre><code>CC-Radix(bucket, b)\n\n\ufffcif fits in cache (bucket) then\n   Radix sort(bucket, b )\n\ufffc\ufffc\ufffc\ufffc\ufffc\ufffcelse\n   sub-buckets = Reverse sorting(bucket, ) \n   for each sub-bucket in sub-buckets\n      CC-Radix(sub-bucket, ) \nendfor\n\ufffc\ufffc\ufffc\ufffc\ufffcendif \nend\n</code></pre>\n  \n  <p>Figure 3. Pseudocode of CC-Radix The algorithm starts by checking\n  whether the data struc- tures to sort bucket fit in cache level Li.\n  Those data struc- tures are vectors S and D and the counter vectors C, one\n  for each of the digits of the key. If so, CC-Radix calls Radix sort.\n  If the data structures do not fit in cache level , the algorithm\n  partitions that bucket into sub-buckets by sorting the bucket by the\n  most significant digit using the counting algorithm (explained above).\n  Here, we call this process of partitioning, Reverse Sorting, and it\n  takes into account the computer architecture of the machine. For each\n  sub-bucket, the CC-Radix sort is called again. Note that the first\n  call to the routine processes the com- plete data set. Further calls\n  to the routine process sub- buckets. Note also that certain subsets of\n  the data may need more Reverse sorting calls than others depending on\n  the data skew.</p>\n  \n  <p>Now, we explain the details of Reverse sorting and the sizing of the\n  parameters of Radix sort. Reverse sorting. After each call of Reverse\n  sorting, the number of digits that remain to be sorted for a specific\n  sub- bucket is decremented by one.</p>\n</blockquote>\n\n<p>I don\'t understand the "Reverse Sorting" part of this - how is that supposed to work?</p>\n\n<p>I know this is a long shot, but I\'m really stuck on what they mean.  Any help is appreciated!</p>\n', 'ViewCount': '130', 'Title': 'Can someone help me understand cache conscience radix sort? (excerpt from journal article attached)', 'LastActivityDate': '2013-02-07T06:59:13.893', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9568', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6733', 'Tags': '<algorithms><sorting>', 'CreationDate': '2013-02-06T15:17:52.903', 'Id': '9547'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3590', 'Title': 'Computing the longest common substring of two strings using suffix arrays', 'LastEditDate': '2013-02-07T13:55:33.637', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '4916', 'FavoriteCount': '2', 'Body': "<p>After I learned how to build a suffix array in $O(N)$ complexity, I am interested in discovering the applications of the suffix arrays. One of these is finding the longest common substring between two strings, in $O(N)$ time. I found on the internet the following algorithm:</p>\n\n<ol>\n<li>merge the two strings $A$ and $B$ into one string $AB$</li>\n<li>compute the suffix array of $AB$</li>\n<li>compute the $LCP$(longest common prefix) array</li>\n<li>the answer is the largest value $LCP[i]$</li>\n</ol>\n\n<p>I tried to implement it, but as many implementation details were not said(i.e. when concatenating the strings, should I put a special character between them($AcB$)?), my code failed on many test cases. Could someone elaborate more on this algorithm?</p>\n\n<p>Thanks in advance.</p>\n\n<p><strong>Note:</strong> I do not guarantee the correctness of this algorithm; I found it on a blog, and I'm not sure it is working. If you think it is incorrect, please suggest another algorithm.</p>\n", 'Tags': '<algorithms><suffix-array>', 'LastEditorUserId': '4916', 'LastActivityDate': '2013-02-10T01:10:19.597', 'CommentCount': '2', 'AcceptedAnswerId': '9619', 'CreationDate': '2013-02-06T20:18:26.923', 'Id': '9555'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I just recently started learning in a CS context (as opposed to a programming context) about simple recursive functions, along with the combinatorial applications, and techniques such as Backtracking and Divide et Impera.</p>\n\n<p>The example problem that I chose for my questions is the n queens problem (given n queens and a n*n chessboard, how do you position the queens such that they can't attack each other?).\nI understood that the basic idea is to generate all possible placements (by generating a cartesian product) and then simply discarding them as they come along if they are invalid (through the validation function).</p>\n\n<p>Here is my sample implementation:</p>\n\n<pre><code>    int valid (int k)\n    {\n    for (int i = 1; i &lt; k; i++)\n    {\n        if (sol[i] == sol[k]) return 0;\n        if (abs(sol[i] - sol[k]) == abs(i - k)) return 0;\n    }\n    return 1;\n}\n\nvoid print ()\n{\n    for (int i = 1; i &lt;= n; i++)\n    {\n        cout &lt;&lt; sol[i];\n    }\n    cout &lt;&lt; endl;\n    how++;\n}\n\nvoid backtrack (int k)\n{\n    if (k == n+1) print();\n    else\n    {\n        sol[k] = 0;\n        while (sol[k] &lt; n)\n        {\n            sol[k]++;\n            if (valid(k)) backtrack(k+1);\n        }\n    }\n}\n</code></pre>\n\n<ol>\n<li>Why is it that in the validation function, the checking of the solution is done progressively by checking 1 with k, 2 with k and so on. I think that the solution can be correct in pairs of (i, k) but wrong overall (for example 1 and 2 relative to 3 are placed correctly, but 1 relative to 2 is placed incorrectly)?</li>\n</ol>\n", 'ViewCount': '232', 'Title': 'Why does backtracking work the way it does?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-08T11:55:37.743', 'LastEditDate': '2013-02-08T08:27:23.527', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9599', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2530', 'Tags': '<algorithms><recursion><backtracking>', 'CreationDate': '2013-02-08T06:03:24.863', 'Id': '9590'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a set $A$ of pairs $(a,b)$ such that $a$ and $b$ are real numbers defined in the interval $[c,d]$. Assume no two values are identical. For $(a,b)$, if $a &gt; b$, the range is $[a,d] \\cup [c,b]$; otherwise, it is $[a,b]$. What is the most efficient algorithm to find the smallest subset $B \\subseteq A$ such that, for any value within the range of any pair in $A$, the value is also within the range of any pair in $B$. </p>\n\n<p>There exists an $O(n \\log n)$ algorithm to find the smallest subset when, for any pair, $a$ is strictly less than $b$. I can run this algorithm for each pair to determine my solution. However, what is a more efficient algorithm to obtain the smallest subset?</p>\n', 'ViewCount': '99', 'Title': 'Finding the minimum subset within a cycle', 'LastActivityDate': '2013-02-09T05:52:39.373', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '9615', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6723', 'Tags': '<algorithms>', 'CreationDate': '2013-02-08T22:49:47.047', 'Id': '9611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '144', 'Title': 'Quick union and heuristic by size', 'LastEditDate': '2013-02-10T16:30:31.787', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4765', 'FavoriteCount': '1', 'Body': "<p>Studying Quick-Find and Quick-Union heuristic I've found clear that:</p>\n\n<ul>\n<li><p>with quick find trees and a union based on the size of the trees we can make a union in $T_{am}(n)=O(\\log(n))$</p></li>\n<li><p>with quick find trees and a union based on the height of the trees we can make a find in $T(n)=O(\\log(n))$</p></li>\n</ul>\n\n<p>But I read that using quick union trees and an union based on the size of the trees we can also have a find in $T(n)=O(\\log(n))$, so my question is how can this be demonstrated? What relationship is there between height and size?</p>\n\n<p>For example knowing that $\\text{size}(A)=4$ I could have both:</p>\n\n<pre><code>        A                A\n      / | \\              |\n     1  2  3             1\n                         |\n                         2\n                         |\n                         3\n</code></pre>\n", 'Tags': '<algorithms><heuristics>', 'LastEditorUserId': '4765', 'LastActivityDate': '2013-02-10T17:37:27.287', 'CommentCount': '2', 'AcceptedAnswerId': '9643', 'CreationDate': '2013-02-09T13:42:27.810', 'Id': '9618'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This <a href="http://www.cs.utexas.edu/~moore/best-ideas/string-searching/index.html">page</a> about Knuth-Moriss-Pratt Algorithm compared to Boyer-Moore describes a possible case where the Boyer-Moore algorithm suffers from small skip distance while KMP could perform better.<br>\nI\'m looking for a good example (text,pattern) that can clearly demonstrate this case.</p>\n', 'ViewCount': '504', 'Title': 'An example where Knuth-Morris-Pratt Algorithm is faster than Boyer-Moore?', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-20T19:23:37.120', 'LastEditDate': '2013-02-10T14:23:12.430', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '736', 'Tags': '<algorithms><substrings><matching>', 'CreationDate': '2013-02-10T10:41:28.470', 'Id': '9635'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>The Floyd-Warshall algorithm is defined as follows:</p>\n\n<pre><code>   for k from 1 to |V|\n      for i from 1 to |V|\n         for j from 1 to |V|\n            if dist[i][k] + dist[k][j] &lt; dist[i][j] then\n               dist[i][j] \u2190 dist[i][k] + dist[k][j]\n</code></pre>\n\n<p>Why doesn't it work if I simply use</p>\n\n<pre><code>for i from 1 to |V|\n  for j from 1 to |V|\n     for k from 1 to |V|\n        if dist[i][k] + dist[k][j] &lt; dist[i][j] then\n           dist[i][j] \u2190 dist[i][k] + dist[k][j]\n</code></pre>\n\n<p>In this case, the intermediate node k is iterated in the innermost loop. I expect it will make the same comparisons, but maybe different order. Why is the result different and incorrect?</p>\n", 'ViewCount': '265', 'Title': "Why doesn't the Floyd-Warshall algorithm work if I put k in the innermost loop", 'LastEditorUserId': '31', 'LastActivityDate': '2013-02-10T20:15:19.823', 'LastEditDate': '2013-02-10T15:11:05.887', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6805', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2013-02-10T12:07:12.883', 'Id': '9636'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Professor Tim Roughgarden from Stanford University while teaching a <a href="https://class.coursera.org/algo2-2012-001/class/index" rel="nofollow">MOOC</a> said that solutions to problems in the class NP must be polynomial in length. But the <a href="https://en.wikipedia.org/wiki/NP_%28complexity%29" rel="nofollow">wikipedia article</a> says that NP problems are decision problems. So what type of problems are basically in the class NP ? And is it unnecessary to say that solutions to such problems have a polynomial length output(as decision problems necessarily output either 0 or 1) ? </p>\n', 'ViewCount': '83', 'Title': 'Is it necessary for NP problems to be decision problems?', 'LastActivityDate': '2013-02-11T06:23:11.520', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9665', 'Score': '3', 'OwnerDisplayName': 'user189535', 'PostTypeId': '1', 'OwnerUserId': '6823', 'Tags': '<algorithms><np><p-vs-np>', 'CreationDate': '2013-02-10T16:30:58.997', 'FavoriteCount': '2', 'Id': '9664'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '641', 'Title': 'The purpose of grey node in graph depth-first search', 'LastEditDate': '2013-02-11T17:33:21.087', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6805', 'FavoriteCount': '1', 'Body': '<p>In many implementations of depth-first search that I saw (for example: <a href="http://ramos.elo.utfsm.cl/~lsb/elo320/aplicaciones/aplicaciones/CS460AlgorithmsandComplexity/lecture9/COMP460%20Algorithms%20and%20Complexity%20Lecture%209.htm" rel="nofollow">here</a>), the code distinguish between a grey vertex (discovered, but not all of its neighbours was visited) and a black vertex (discovered and all its neighbours was visited). What is the purpose of this distinction? It seems that DFS algorithm will never visit a visited vertex regardless of whether it\'s grey or black.</p>\n', 'Tags': '<algorithms><graphs><search-algorithms><graph-traversal>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-12T00:43:33.593', 'CommentCount': '0', 'AcceptedAnswerId': '9681', 'CreationDate': '2013-02-11T13:10:44.110', 'Id': '9676'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If I have a system of N polynomial equations with N unknowns in GF(2):</p>\n\n<ul>\n<li>What are some good methods to solve them?</li>\n<li>What are some software packages or libraries that implement this?</li>\n<li>What's the highest value of N that can be reasonable solved?</li>\n</ul>\n\n<p>Now, my root interest isn't GF, it's crypto.  Here's my reasoning:</p>\n\n<ol>\n<li>Any function from a n-dimensional binary vector to {0,1} can be represented as a GF(2) polynomial function of n variables.</li>\n<li>Thus, for instance, any cipher from (Plaintext, Key) to Ciphertext can be represented as a series of equations (one for each bit in the ciphertext), each a GF(2) polynomial of (p-bits + k-bits) variables.</li>\n<li>Thus, if we know P and C, and we can solve systems of GF(2) equations, we can determine K.</li>\n</ol>\n", 'ViewCount': '115', 'Title': 'Complexity of GF(2) and applications to cryptography', 'LastActivityDate': '2013-09-11T23:27:21.163', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '9682', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '6824', 'Tags': '<algorithms><cryptography><discrete-mathematics><sat-solvers>', 'CreationDate': '2013-02-11T13:38:17.613', 'FavoriteCount': '1', 'Id': '9678'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What are the fastest known algorithms for general convex optimisation under linear inequality constraints?</p>\n', 'ViewCount': '67', 'Title': 'Convex optimisation under linear inequality constraints', 'LastActivityDate': '2013-02-11T13:48:41.673', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '192', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-02-11T13:48:41.673', 'Id': '9679'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What does a pseudo-polynomial algorithm tell us about the problem it solves? I don't see how running time improves if the algorithm is exponential in the input length and polynomial in the input value; so how do we explain this shift from exponential to polynomial?</p>\n", 'ViewCount': '229', 'Title': 'Weak and strong completeness', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-24T03:24:29.867', 'LastEditDate': '2013-02-12T06:27:18.567', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><complexity-theory><np-complete><pseudo-polynomial>', 'CreationDate': '2013-02-11T18:23:44.420', 'FavoriteCount': '1', 'Id': '9686'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm an autodidact who works best with a mix of theory and implementation examples. I'm having a hard time finding resources on implementing algorithms and data structures in Python (2.7-3.X  preferably). I found a text ...mastering basic algorithms in the python language but I find it to be far too introductory and thus cluttered with unnecessary prose, which makes it for me difficult to move efficiently over it. What I have in mind is a text or github gist or whatever really, that has some coded python imps of what I've determined to be foundational data-structures in algorithmic's(not to sure on the usage/conjugation of that). Specifically I'm looking for trees, traversal, dynamic and greedy algorithms and the like. If this question is a repeat, or adequately answered elsewhere, links are adequate replies.  </p>\n", 'ViewCount': '49', 'Title': 'Looking for non-entry level implementation of foundational algorithms and data structures in python. where to look?', 'LastEditorUserId': '6830', 'LastActivityDate': '2013-02-12T00:03:28.820', 'LastEditDate': '2013-02-11T23:23:51.723', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9697', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6830', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-02-11T20:44:52.050', 'Id': '9691'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '88', 'Title': 'Bipartite graph question', 'LastEditDate': '2013-02-11T22:39:58.533', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2003', 'FavoriteCount': '1', 'Body': "<p>Assume you are given a bipartite graph $G = (U, V, E)$ and you are given an integer $n$. Assume also that for each $v \\in V$, you are given two integers $v_{min}$ and $v_{max}$ (where $v_{min} \\le v_{max}$).</p>\n\n<p>The problem is to find a subset $U'$ of $U$ of size $n$ such that for each $v \\in V$, the number of edges coming into $v$ from $U'$ is between $v_{min}$ and $v_{max}$.</p>\n\n<p>Given a problem like this, can we determine efficiently whether there is a solution? And, if there is a solution, can we find one efficiently?</p>\n\n<p>If we can't do so efficiently, is there an approximation algorithm?</p>\n", 'Tags': '<algorithms><graphs><bipartite-matching>', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-02-11T22:57:36.760', 'CommentCount': '2', 'AcceptedAnswerId': '9694', 'CreationDate': '2013-02-11T22:35:47.340', 'Id': '9693'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>That is, can the running time of every algorithm $A$ be written as $O(f_A(n))$ and $\\Omega(f_A(n))$, for the same function $f_A$?</p>\n', 'ViewCount': '484', 'Title': "Can every algorithm's running time be expressed as $\\Theta(f(n))$?", 'LastActivityDate': '2013-02-14T02:39:23.590', 'AnswerCount': '6', 'CommentCount': '3', 'Score': '4', 'OwnerDisplayName': 'user13731', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-02-13T01:21:43.743', 'FavoriteCount': '0', 'Id': '9728'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm really trying to wrap my head around vectorization but I can't seem to understand it. I don't know if I don't understand how to vectorize, or if I don't understand the array notation that's being used. An example of a loop I'm working on with school is below:</p>\n\n<pre><code>for (M=0; M&lt; number_of_iterations/2; M++){\n   for (i=2; i&lt;n-1; i++)  \n      for (j=1; j&lt;n-1; j++)  \n          y[i][j]= (x[i-1][j]+x[i][j-1]+x[i+1][j]+x[i][j+1]+x[i-2][j])/5.;\n</code></pre>\n\n<p>I'm not sure I quite understand the whole thing on dependencies - Is there a way to vectorize this using array notation as is, or do I need to adjust it somehow to account for dependencies throughout?</p>\n", 'ViewCount': '44', 'Title': 'Vectorizing and Array Notation?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-14T06:37:19.947', 'LastEditDate': '2013-02-14T06:37:19.947', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6733', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-02-13T03:16:26.610', 'Id': '9732'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an efficient algorithm to find streaming data median. <a href="http://en.wikipedia.org/wiki/Median" rel="nofollow">Median</a> is described as the numerical value separating the higher half of a sample, a population, or a probability distribution, from the lower half. </p>\n\n<p>We have stream of data in our system like 1, 10, -40, 20, 2, 6,.....Our task is find median of data as they arrive.</p>\n', 'ViewCount': '186', 'Title': 'Streaming Median', 'LastActivityDate': '2013-02-16T02:04:01.783', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><statistics>', 'CreationDate': '2013-02-15T17:42:13.830', 'Id': '9816'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am concerned with the question of <strong>the asymptotic running time of the Ukkonen\'s algorithm</strong>, perhaps the most popular algorithm for constructing <strong>suffix trees</strong> in linear (?) time.</p>\n\n<p>Here is a citation from the book "Algorithms on strings, trees and sequences" by Dan Gusfield (section 6.5.1):</p>\n\n<blockquote>\n  <p>"... the Aho-Corasick, Weiner, <strong>Ukkonen</strong> and McCreight algorithms all either require $\\Theta(m|\\Sigma|)$ space, or the $O(m)$ time bound should be replaced with the minimum of $O(m \\log m)$ and $O(m \\log|\\Sigma|)$".</p>\n  \n  <p><em>[$m$ is the string length and $\\Sigma$ is the size of the alphabet]</em></p>\n</blockquote>\n\n<p>I don\'t understand why that is true.</p>\n\n<ul>\n<li><strong>Space:</strong> well, in case we represent branches out of the nodes using arrays of size $\\Theta(|\\Sigma|)$, then, indeed, we end up with $\\Theta(m|\\Sigma|)$ space usage. However, as far as I can see, it is also possible to store the branches using hash tables (say, dictionaries in Python). We would then have only $\\Theta(m)$ pointers stored in all hash tables altogether (since there are $\\Theta(m)$ edges in the tree), while still being able to access the children nodes in $O(1)$ time, as fast as when using arrays.</li>\n<li><strong>Time</strong>: as mentioned above, using hash tables allows us to access the outgoing branches of any node in $O(1)$ time. Since the Ukkonen\'s algorithm requires $O(m)$ operations (including accessing children nodes), the overall running time then would be also $O(m)$.</li>\n</ul>\n\n<p>I would be very grateful to you for any hints on why I am wrong in my conclusions and why Gusfield is right about the dependence of the Ukkonen\'s algorithm on the alphabet.</p>\n', 'ViewCount': '238', 'Title': "How does the runtime of the Ukkonen's algorithm depend on the alphabet size?", 'LastEditorUserId': '162', 'LastActivityDate': '2013-02-18T14:59:52.107', 'LastEditDate': '2013-02-16T08:05:47.417', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings>', 'CreationDate': '2013-02-15T22:05:13.680', 'FavoriteCount': '1', 'Id': '9820'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Corman, Introduction To Algorithms, 3rd edition, question 2-4 it asks to count the number of inversions in a list of numbers in $\\theta( n \\lg n )$ time.  He uses a modified Merge Sort to accomplish this.  However, there is something in his algorithm which seems redundant / unnecessary to me:</p>\n\n<pre><code>MERGE-INVERSIONS(A, p, q, r)\nn1 = q - p + 1\nn2 = r - q\nlet L[1 ... n1 + 1] and R[1 ... n2 + 1] be new arrays\nfor i = 1 to n1\n    L[i] = A[p + i - 1]\nfor j = 1 to n2\n    R[j] = A[q + j] \nL[n1 + 1] = infinity\nR[n2 + 1] = infinity\ni = 1\nj = 1\ninversions = 0\ncounted = FALSE\nfor k = p to r\n    if counted == FALSE and R[j]  &lt; L[i]\n        inversions = inversions + n1 - i + 1\n        counted = TRUE\n    if L[i] &lt;= R[j] \n        A[k] = L[i]\n        i++\n    else A[k] = R[j] \n        j++\n        counted = FALSE\nreturn inversions\n</code></pre>\n\n<p>The <code>counted</code> variable seems redundant to me and I would have written the last for loop as follows:</p>\n\n<pre><code>inversions = 0\nfor k = p to r\n    if L[i] &lt;= R[j] \n        A[k] = L[i]\n        i++\n    else A[k] = R[j] \n        inversions = inversions + n1 - i + 1\n        j++\nreturn inversions\n</code></pre>\n\n<p>What am I missing, or is <code>counted</code> really unnecessary?</p>\n', 'ViewCount': '959', 'Title': 'Counting Inversions Using Merge Sort', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-17T11:56:28.703', 'LastEditDate': '2013-02-17T11:38:20.870', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9861', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><algorithm-analysis><sorting><program-correctness>', 'CreationDate': '2013-02-17T10:23:24.240', 'Id': '9858'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '251', 'Title': 'Computing inverse matrix when an element changes', 'LastEditDate': '2013-02-18T03:19:37.600', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '867', 'FavoriteCount': '1', 'Body': "<p>Given an $n \\times n$ matrix $\\mathbf{A}$. Let the inverse matrix of $\\mathbf{A}$ be $\\mathbf{A}^{-1}$ (that is, $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$). Assume that one element in $\\mathbf{A}$ is changed (let's say $a _{ij}$ to $a' _{ij}$). The objective is to find $\\mathbf{A}^{-1}$ after this change. Is there a method to find this objective that is more efficient than re-calculating the inverse matrix from scratch. </p>\n", 'Tags': '<algorithms><numerical-analysis><online-algorithms>', 'LastEditorUserId': '683', 'LastActivityDate': '2013-02-18T14:24:30.640', 'CommentCount': '3', 'AcceptedAnswerId': '9881', 'CreationDate': '2013-02-18T01:04:51.930', 'Id': '9875'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1038', 'Title': 'Worst case analysis of bucket sort using insertion sort for the buckets', 'LastEditDate': '2013-02-18T22:03:28.823', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6815', 'FavoriteCount': '1', 'Body': '<p>Suppose I am using the <a href="http://en.wikipedia.org/wiki/Bucket_sort#Pseudocode" rel="nofollow">Bucket-Sort</a> algorithm, and on each bucket/list I sort with insertion sort (replace nextSort with insertion sort in the wikipedia pseudocode).</p>\n\n<p>In the worst case, this would imply that we would have $O(n^2)$ performance, because if every element was in one bucket, then we would have to use insertion sort on $n$ elements which is $O(n^2)$. </p>\n\n<p>So the first thing that comes to mind to fix the worst case running time is to not use insertion-sort, because it is $O(n^2)$. Instead we could use merge-sort or heap-sort m, because the worst case running time for both of those algorithms is $O(n\\log n)$. However, if we use merge-sort and heap-sort, do they preserve the expected linear running-time of bucket-sort?</p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-18T22:03:28.823', 'CommentCount': '1', 'AcceptedAnswerId': '9882', 'CreationDate': '2013-02-18T01:16:55.900', 'Id': '9876'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '933', 'Title': 'Algorithm to find the mode in a unimodal array', 'LastEditDate': '2013-02-18T06:27:00.790', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4492', 'FavoriteCount': '1', 'Body': u'<p>I am given the following problem in an Algorithms class:</p>\n\n<blockquote>\n  <p>Assume that you are given an array A[1 . . . n] of distinct numbers.\n  You are told that the sequence of numbers in the array is unimodal, in\n  other words, there is an index i such that the sequence A[1 . . . i]\n  is increasing (A[j] &lt; A[j + 1] for 1 \u2264 j &lt; i), and the sequence A[i .\n  . . n] is decreasing. The index i is called the mode of A. Give an\n  O(log n) algorithm that find the mode of A</p>\n</blockquote>\n\n<p>I have written this draft solution as my solution but I want to make sure that this is an acceptable CORRECT solution.</p>\n\n<p>My Algorithm:</p>\n\n<pre><code>FIND_MODE(A)\nn = A.length\nif n == 1\n    return 1\n\nmid = floor(n/2)\nif A[mid] &lt; A[mid+ 1]\n    return FIND_MODE(A[1 \u2026 mid])\nelse\n    return mid + FIND_MODE(A[mid+1 \u2026 n])\n</code></pre>\n\n<p>Is it this acceptable and correct pseudocode algorithm?</p>\n\n<p>Is it correct that this is a Big-O(log n) algorithm?</p>\n', 'Tags': '<algorithms><algorithm-analysis><arrays>', 'LastEditorUserId': '4492', 'LastActivityDate': '2013-02-18T21:58:16.793', 'CommentCount': '0', 'AcceptedAnswerId': '9890', 'CreationDate': '2013-02-18T06:14:07.823', 'Id': '9888'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to analyze the running time of a bad implementation of generating the $n$th member of the fibonacci sequence (which requires generating the previous 2 values from the bottom up).</p>\n\n<p>Why does this algorithm have a time complexity of $\\Omega(2^{\\frac{n}{2}})$? Where does the exponent come from?</p>\n', 'ViewCount': '852', 'Title': 'Why does a recurrence of $T(n - 1) + T(n - 2)$ yield something in $\\Omega(2^{\\frac{n}{2}})$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-19T23:48:57.897', 'LastEditDate': '2013-02-19T06:22:05.190', 'AnswerCount': '4', 'CommentCount': '5', 'AcceptedAnswerId': '9908', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2013-02-18T19:02:47.990', 'Id': '9899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m developing software to run variations on a base process flow (see #1, below). A user specifies in a text file what steps in the process to modify. Because each step takes a long time to run, I\'d like to minimize the amount of duplicate processing required. For example, if variations occur at step B, I could run step A one for all results before "branching" at step B (see #2 below). Similarly, I could branch again at step D if additional variations on step D are indicated (see #3 below).</p>\n\n<p><strong>1) Base Process:</strong></p>\n\n<pre><code>input --&gt; A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; result\n</code></pre>\n\n<p><strong>2) Modification of Step B:</strong></p>\n\n<pre><code>input --&gt; A --&gt; B1 --&gt; C --&gt; D --&gt; E --&gt; result1\n                B2 --&gt; C --&gt; D --&gt; E --&gt; result2\n</code></pre>\n\n<p><strong>3) Modification of Steps B and D:</strong></p>\n\n<pre><code>input --&gt; A --&gt; B1 --&gt; C --&gt; D1 --&gt; E --&gt; result1\n                             D2 --&gt; E --&gt; result2\n                B2 --&gt; C --&gt; D1 --&gt; E --&gt; result3\n                             D2 --&gt; E --&gt; result4\n</code></pre>\n\n<p>Is there a simple algorithm to determine the the common process steps and the branch points as in #3 given a base flow as in #1 and a list of steps to change, e.g.</p>\n\n<pre><code>Variation  StepB  StepD\n   1         1      1\n   2         1      2\n   3         2      1\n   4         2      2\n</code></pre>\n\n<p>The above example is simple but there could be hundreds of variations modifying dozens of different steps in actual usage. </p>\n', 'ViewCount': '56', 'Title': 'Algorithm for finding optimal branch points', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T14:08:38.050', 'LastEditDate': '2013-04-21T14:08:38.050', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6936', 'Tags': '<algorithms><optimization><scheduling>', 'CreationDate': '2013-02-18T19:13:35.097', 'Id': '9901'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '249', 'Title': 'What are some problems which are easily solved by human brain but which would take more time computers?', 'LastEditDate': '2013-02-18T22:59:58.213', 'AnswerCount': '5', 'Score': '2', 'OwnerDisplayName': 'avi', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Body': '<p>Are there any problems which can be solved by human brain in a very less time but a computer may take a lot of time or a computer could never solve it ?</p>\n', 'Tags': '<algorithms><artificial-intelligence><computer-vs-human>', 'LastEditorUserId': '157', 'LastActivityDate': '2013-02-20T15:17:13.653', 'CommentCount': '2', 'AcceptedAnswerId': '9937', 'CreationDate': '2013-02-18T16:03:37.297', 'Id': '9911'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an explanation Christofides\'s Heuristic for solving TSP which does not simply state the algorithm and go ahead to prove the bound?</p>\n\n<p>To be specific:\n(Disclaimer : I am an engineer who knows very little about graph theory but need this for a logistics course)</p>\n\n<ul>\n<li>I understand that I first create an MST. So far, so good.</li>\n<li>Now, I need to find a perfect minimum weight matching on all odd degree nodes. I have no clue what this is; googling this tells me this is a set of edges containing maximum $n/2$ edges such that no node is shared by 2 sets. I don\'t see why I am doing this..... I am not even sure I understand what this statement means.</li>\n<li>Now, I need to merge the MST and the matchings to create a "multigraph" and then find an Eulerian tour on this. No clue what I am doing here.</li>\n<li>Run the shortcut algorithm exploiting the triangle inequality. (No clue what happened till now and this obviously then makes no sense either)</li>\n</ul>\n\n<p>Can someone point me to a good resource with possible examples and illustrations for why Christofides works in a language that isn\'t full of graph theory terms (or alternately, provide me an answer here)?</p>\n\n<p>I have already looked at :</p>\n\n<ul>\n<li><a href="http://ieor.berkeley.edu/~kaminsky/ieor251/notes/2-16-05.pdf" rel="nofollow">A Berkeley PDF</a></li>\n<li>Wikipedia</li>\n</ul>\n', 'ViewCount': '507', 'Title': "A Good Resource for Christofides' Heuristic", 'LastEditorUserId': '39', 'LastActivityDate': '2013-02-19T23:46:33.413', 'LastEditDate': '2013-02-19T23:46:33.413', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9924', 'Score': '4', 'OwnerDisplayName': 'user6422', 'PostTypeId': '1', 'Tags': '<algorithms><heuristics><graph-traversal>', 'CreationDate': '2013-02-19T06:37:03.837', 'Id': '9923'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>I am developing a chart and graph library and am having trouble developing an algorithm.</strong> </p>\n\n<p>**This is not a homework assignment for a student. See my open source project: <a href="https://github.com/eddieios/CoreChart" rel="nofollow">https://github.com/eddieios/CoreChart</a></p>\n\n<p>The algorithm is to space the Y axis labels in a given coordinate graphical space. It should space the labels at about 50 pixels apart, but have no less than 5 labels. All data points are assumed to be positive integers. The labels should be multiples of 5 (so as to have nice clean numbers). The last label can be smaller or larger than the maximum possible value, but it should be the closer of the two. The algorithm inputs are 1) the maximum possible value from the chart data, 2) the height of the graphical space. The output is a list of labels and their vertical positions. </p>\n\n<p>For example:</p>\n\n<ul>\n<li>Maximum possible chart value = 79</li>\n<li>Height of graphical space = 200</li>\n</ul>\n\n<p>Output would be:</p>\n\n<ul>\n<li>Label: 0, Vertical Position: 0</li>\n<li>Label: 20, Vertical Position: 50</li>\n<li>Label: 40, Vertical Position: 100</li>\n<li>Label: 60, Vertical Position: 150</li>\n<li>Label: 80, Vertical Position: 200</li>\n</ul>\n\n<p>I have written the following code (Obj-C). But I\'m having trouble handling end-cases. For example, if the maxValue = 39, then the labels are set 5 apart. The optimal case here would be to set labels 10 apart. There\'s something about I\'m deciding how many labels there should be that isn\'t working for all cases. </p>\n\n<pre><code>int maxValue = 39;\nfloat graphHeight = 259.0f;\n\nint numLevels = (int)(graphHeight / 50.0f);\n\nfloat offset = (int)(maxValue / (float)numLevels);\noffset /= 5;\noffset = (float)((int)(offset + 0.5));\noffset *= 5;\n\nCGFloat stepY = graphHeight * ((float)offset/maxValue);\n\nfor (int i = 0; i &lt;= numLevels; i++) {\n    NSLog(@"label %f position %f", i*offset, i*stepY);\n}\n</code></pre>\n', 'ViewCount': '47', 'Title': 'Algorithm for graphically spacing items', 'LastActivityDate': '2013-02-19T22:43:56.073', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9946', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6946', 'Tags': '<algorithms>', 'CreationDate': '2013-02-19T08:47:43.347', 'Id': '9926'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '431', 'Title': 'Is there an efficient test for if an NFA accepts a subset of another NFA?', 'LastEditDate': '2014-04-03T11:58:31.807', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2253', 'FavoriteCount': '3', 'Body': "<p>So, I know that testing if a regular language $R$ is a subset of regular language $S$ is decidable, since we can convert them both to DFAs, compute $R \\cap \\bar{S}$, and then test if this language is empty.</p>\n\n<p>However, since this requires converting to DFAs, it's possible that the DFAs, and thus the testing algorithm, will be exponential in terms of the number of states in the input NFAs.</p>\n\n<p>Is there a known way to do this in polynomial time? Has this problem in general been proved Co-NP complete? </p>\n\n<p>Note that the problem is in Co-NP since a word accepted by $R$ but not by $S$ would be a polynomial certifier that $R \\not \\subseteq S $.</p>\n\n<p>EDIT: this is incorrect, as there is no guarantee that such a word would be polynomial in the number of states.</p>\n", 'Tags': '<algorithms><regular-languages><automata><np-complete><decision-problem>', 'LastEditorUserId': '15050', 'LastActivityDate': '2014-04-03T11:58:31.807', 'CommentCount': '5', 'AcceptedAnswerId': '9955', 'CreationDate': '2013-02-20T06:11:51.547', 'Id': '9954'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Some sorting algorithms like counting sort/insertion sort can work in $O(n)$ time while other algorithms such as quicksort require $O(n \\log n)$ time.</p>\n\n<p>As I understand it, it's not always possible to use the $O(n)$ sorting algorithms. What are those cases when they can not be used?</p>\n", 'ViewCount': '115', 'Title': 'When can one use a $O(n)$ time sorting algorithm?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-20T14:59:51.977', 'LastEditDate': '2013-02-20T14:59:51.977', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '9967', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6821', 'Tags': '<algorithms><sorting>', 'CreationDate': '2013-02-20T12:19:32.960', 'Id': '9965'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given an array of size n that holds ones and zeros I need to find an index of a  <code>1</code> cell that has <code>0</code> to his right (in then next cell) there could be more than one pair in a given array, any one of them is fine. The array is not sorted, but we do know that the first element is <code>1</code> and the last element is <code>0</code>.</p>\n\n<p>The search should be in $O(\\log n)$ time. I'm thinking that a binary search variation is the answer but I'm not sure how. </p>\n", 'ViewCount': '60', 'Title': "Finding a '1' cell with a '0' to its right in a binary array", 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-20T18:15:12.737', 'LastEditDate': '2013-02-20T18:15:12.737', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9973', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6963', 'Tags': '<algorithms><arrays><search-algorithms><binary-search>', 'CreationDate': '2013-02-20T16:04:03.810', 'Id': '9969'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some questions regarding tree search and graph search (Uninformed search) as explained in chapter 3 of the book : <a href="http://aima.cs.berkeley.edu/" rel="nofollow">http://aima.cs.berkeley.edu/</a></p>\n\n<p>As I see, the only difference between the two is that the graph search handles loops (avoids them).</p>\n\n<p>First question: Do both graph search and tree search build dynamic trees of the problem at hand?</p>\n\n<p>Second question: I assume graph search was used to solve the map of Romania problem (getting from Arad to Bucharest) with DFS, BFS, UCS as strategies that only sort the frontier queue. Now is there a standard way to change the graph of map of Romania to a tree, and then use tree search? </p>\n\n<p>Third question: What are some of the Criteria that help us choose between graph and tree search for different problems?</p>\n\n<p>Thanks you in advance</p>\n', 'ViewCount': '85', 'Title': 'Using tree search', 'LastActivityDate': '2013-02-20T20:53:51.773', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9990', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6966', 'Tags': '<artificial-intelligence><search-algorithms>', 'CreationDate': '2013-02-20T18:53:46.650', 'Id': '9979'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a set $S = \\{(a_1,b_1),...,(a_n,b_n)\\}$ where $a_i &lt; m$, $b_i = m-a_i$, $m \\in \\mathbb{Z}^{+}$, $m&gt;2$ and $n$ is an even number greater than $3$. What is the most efficient algorithm to determine if it is possible to partition $S$ into two distinct subsets, $C$ and $D$, of equal size such that</p>\n\n<p>$\\sum_{a \\in C} a &gt; \\sum_{b \\in C} b$ &nbsp;&nbsp;and&nbsp;&nbsp; $\\sum_{a \\in D} a &gt; \\sum_{b \\in D} b$ </p>\n\n<p>or&nbsp; $\\sum_{b \\in C} b &gt; \\sum_{a \\in C} a$ &nbsp;and&nbsp; $\\sum_{b \\in D} b &gt; \\sum_{a \\in D} a$ &nbsp;?</p>\n\n<p>For example, if $S = \\{(56,44),(48,52),(43,57),(60,40)\\}$, $C = \\{(56,44),(48,52)\\}$, and $D = \\{(43,57),(60,40)\\}$. </p>\n\n<p>I am considering iteratively matching a pair with the best value of $a$ with a pair with the worst value of $a$. Is there another algorithm? </p>\n', 'ViewCount': '184', 'Title': 'Optimal partition of a set of pairs', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:10:06.683', 'LastEditDate': '2013-02-24T16:10:06.683', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '6723', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-02-21T08:21:51.740', 'Id': '10004'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We only consider the <a href="http://en.wikipedia.org/wiki/Reverse_Polish_notation" rel="nofollow">reverse Polish notation</a> as an arithmetic expression.</p>\n\n<p>Formally, RNP is a sequence consisted of numbers and arithmetic operators: $+,-,*,/$, and its syntax is:\n$$\\newcommand\\RNF{\\mathrm{RNF}}\\newcommand\\num{\\mathrm{number}}\\newcommand\\op{\\mathrm{operator}}\\RNF=\\num\\,\\big\\vert\\,\\RNF,\\RNF,\\op$$\nand its value\n$$\\newcommand\\eval{\\operatorname{eval}}\\eval\\num=\\num$$\n$$\\eval\\RNF_1,\\RNF_2,\\op=\\eval\\RNF_1\\ \\op\\ \\eval\\RNF_2$$</p>\n\n<p>The following pseudo code to evaluate $\\eval\\RNF$ is quoted from K&amp;R:</p>\n\n<pre><code>while (next operator or operand isn\'t empty)\n  if (it\'s a number)\n    push it\n  else if (it\'s an operator, say +,-,*,/)\n    pop operands\n    do operation\n    push result\n</code></pre>\n\n<p>The algorithm is somewhat straightforward, but it\'s not as evident as considered. I found it difficult to formulate a <a href="http://en.wikipedia.org/wiki/Loop_invariant" rel="nofollow">loop invariant</a> for the outer while-loop, and it\'s quite hard to prove the algorithm through <a href="http://en.wikipedia.org/wiki/Hoare_logic" rel="nofollow">Floyd-Hoare logic</a>.</p>\n\n<p>Through some search work, I found a <a href="http://cs.stackexchange.com/q/3458">related question</a>, about the unambiguity of RPN. Unfortunately, I don\'t think the answer to that question is a rigorous proof.</p>\n', 'ViewCount': '483', 'Title': 'Evaluation of reverse Polish notation', 'LastEditorUserId': '1715', 'LastActivityDate': '2013-02-23T05:29:17.650', 'LastEditDate': '2013-02-23T05:25:43.650', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10035', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1715', 'Tags': '<algorithms><formal-languages><software-verification>', 'CreationDate': '2013-02-23T04:22:45.387', 'Id': '10034'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What data structure should I store my graph in to get the best performance from the Dijkstra algorithm?</p>\n\n<p>Object-pointer? Adjacency list? Something else?</p>\n\n<p>I want the lowest O(). Any other tips are appreciated too!</p>\n', 'ViewCount': '678', 'Title': "What graph data structure works fastest with Dijkstra's algorithm?", 'LastActivityDate': '2013-02-24T08:49:26.973', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10046', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-02-24T08:46:02.133', 'Id': '10044'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1436', 'Title': "Is Dijkstra's algorithm just BFS with a priority queue?", 'LastEditDate': '2013-02-24T16:18:24.030', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Barry Fruitman', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Body': '<p>According to <a href="http://community.topcoder.com/tc?module=Static&amp;d1=tutorials&amp;d2=graphsDataStrucs3" rel="nofollow">this page</a>, Dijkstra\'s algorithm is just BFS with a priority queue. Is it really that simple? I think not.</p>\n', 'Tags': '<algorithms><graphs><shortest-path>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:18:24.030', 'CommentCount': '3', 'AcceptedAnswerId': '10048', 'CreationDate': '2013-02-24T07:46:57.263', 'Id': '10047'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1157', 'Title': 'Is search a binary heap operation?', 'LastEditDate': '2013-02-24T16:24:27.193', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'Barry Fruitman', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Body': '<p>According to the <a href="http://en.wikipedia.org/wiki/Binary_heap" rel="nofollow">Wikipedia page</a>, search is "not an operation" on binary heaps (see complexity box at top-right).</p>\n\n<p>Why not? Binary heaps may not be sorted, but they are ordered, and a full graph traversal can find any object in $O(n)$ time, no?</p>\n\n<p>Is the page wrong or am I?</p>\n', 'Tags': '<data-structures><search-algorithms><heaps>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:24:27.193', 'CommentCount': '1', 'AcceptedAnswerId': '10052', 'CreationDate': '2013-02-24T07:37:18.370', 'Id': '10049'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have some confusion related to a divide and conquer problem. Here is the problem</p>\n\n<p>You\u2019re consulting for a small computation-intensive investment company, and they have the following type of problem that they want to solve over and over. A typical instance of the problem is the following. They\u2019re doing a simulation in which they look at n consecutive days of a given stock, at some point in the past. Let\u2019s number the days i=1,2,...,n; for each day i, they have a price p(i) per share for the stock on that day. (We\u2019ll assume for simplicity that the price was fixed during each day.) Suppose during this time period, they wanted to buy 1,000 shares on some day and sell all these shares on some (later) day. They want to know: When should they have bought and when should they have sold in order to have made as much money as possible? (If\nthere was no way to make money during the n days, you should report this instead.)</p>\n\n<p>For example, suppose n = 3, p(1) = 9, p(2) = 1, p(3) = 5. Then you should return \u201cbuy on 2, sell on 3\u201d (buying on day 2 and selling on day 3 means they would have made $4 per share, the maximum possible for that period).</p>\n\n<p>Clearly, there\u2019s a simple algorithm that takes time $O(n^2)$: try all possible pairs of buy/sell days and see which makes them the most money. Your investment friends were hoping for something a little better.</p>\n\n<p>Show how to find the correct numbers i and j in time O(n log n).</p>\n\n<p>Solution We\u2019ve seen a number of instances in this chapter where a brute- force search over pairs of elements can be reduced to O(n log n) by divide and conquer. Since we\u2019re faced with a similar issue here, let\u2019s think about how we might apply a divide-and-conquer strategy.</p>\n\n<p>A natural approach would be to consider the first n/2 days and the final\nn/2 days separately, solving the problem recursively on each of these two\nsets, and then figure out how to get an overall solution from this in O(n) time.\nThis would give us the usual recurrence T (n) \u2264 2T(n/2) + O(n), and hence \n\ufffcO(n log n).</p>\n\n<p>Also, to make things easier, we\u2019ll make the usual assumption that n is a power of 2. This is no loss of generality: if n\u2032 is the next power of 2 greater than n, we can set p(i) = p(n) for all i between n and n\u2032. In this way, we do not change the answer, and we at most double the size of the input (which will not affect the O() notation).</p>\n\n<p>Now, let S be the set of days 1,...,n/2, and S\u2032 be the set of days n/2+ 1, . . . , n. Our divide-and-conquer algorithm will be based on the following observation: either there is an optimal solution in which the investors are holding the stock at the end of day n/2, or there isn\u2019t. Now, if there isn\u2019t, then the optimal solution is the better of the optimal solutions on the sets S and S\u2032. If there is an optimal solution in which they hold the stock at the end of day n/2, then the value of this solution is p(j) \u2212 p(i) where i \u2208 S and j \u2208 S\u2032. But this value is maximized by simply choosing i \u2208 S which minimizes p(i), and choosing j \u2208 S\u2032 which maximizes p(j).</p>\n\n<p>Thus our algorithm is to take the best of the following three possible solutions.</p>\n\n<pre><code>. The optimal solution on S.\n. The optimal solution on S\u2032.\n. The maximum of p(j)\u2212p(i), over i\u2208S and j\u2208S\u2032.\n</code></pre>\n\n<p>The first two alternatives are computed in time T(n/2), each by recursion, and the third alternative is computed by finding the minimum in S and the\nmaximum in S\u2032, which takes time O(n). Thus the running time T(n) satisfies\nT(n) \u2264 2T(n/2) + O(n),\nas desired.</p>\n\n<p>Can anyone explain whats going on here? I didn't get how this algorithm actually works. I know they are dividing the days into first half and second half. But I didn't get this part specially </p>\n\n<p><strong>Our divide-and-conquer algorithm will be based on the following observation: either there is an optimal solution in which the investors are holding the stock at the end of day n/2, or there isn\u2019t. Now, if there isn\u2019t, then the optimal solution is the better of the optimal solutions on the sets S and S\u2032. If there is an optimal solution in which they hold the stock at the end of day n/2, then the value of this solution is p(j) \u2212 p(i) where i \u2208 S and j \u2208 S\u2032. But this value is maximized by simply choosing i \u2208 S which minimizes p(i), and choosing j \u2208 S\u2032 which maximizes p(j).</strong></p>\n", 'ViewCount': '228', 'Title': 'Confusion related to a divide and conquer problem', 'LastActivityDate': '2013-02-24T11:25:31.807', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6999', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-02-24T09:27:01.003', 'Id': '10050'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a quick question on the bubble sort algorithm. Why does it perform $\\Theta(n^2)$ comparisons on an $n$ element list?</p>\n\n<p>I looked at the Wikipedia page and it does not seem to tell me. I know that because of its magnitude it takes a lot of work with large numbers.</p>\n', 'ViewCount': '541', 'Title': 'Why does bubble sort do $\\Theta(n^2)$ comparisons on an $n$ element list?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-27T12:14:10.343', 'LastEditDate': '2013-02-25T07:26:15.987', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '10061', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2013-02-24T18:04:04.793', 'Id': '10058'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For an academic question, I plan to design a medical diagnosis system. Given a description of symptoms, produce a list of probable diseases (with closeness matching). I'm having some trouble finding papers for getting started. Which algorithms and data structure should I consider?</p>\n", 'ViewCount': '175', 'Title': 'Data structure and algorithms for a medical diagnosis software', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-25T07:12:28.043', 'LastEditDate': '2013-02-25T07:12:28.043', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7014', 'Tags': '<algorithms><machine-learning>', 'CreationDate': '2013-02-24T20:05:02.043', 'Id': '10063'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2134', 'Title': 'What are the characteristics of an $O(n \\log n)$ time complexity algorithm?', 'LastEditDate': '2013-02-26T07:32:50.337', 'AnswerCount': '6', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '7007', 'FavoriteCount': '3', 'Body': '<p>Sometimes it\'s easy to identify the time complexity of an algorithm my examining it carefully. Algorithms with two nested loops of $N$ are obviously $N^2$. Algorithms that explore all the possible combinations of $N$ groups of two values are obviously $2^N$.</p>\n\n<p>However I don\'t know how to "spot" an algorithm with $O(N \\log N)$ complexity. A recursive mergesort implementation, for example, is one. What are the common characteristics of mergesort or other $O(N \\log N)$ algorithms that would give me a clue if I was analyzing one?</p>\n\n<p>I\'m sure there is more than one way an algorithm can be of $O(N \\log N)$ complexity, so any and all answers are appreciated. BTW I\'m seeking general characteristics and tips, not rigorous proofs.</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><intuition>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-26T19:08:10.450', 'CommentCount': '5', 'AcceptedAnswerId': '10102', 'CreationDate': '2013-02-25T21:02:00.667', 'Id': '10091'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>An unhappy number is a number that is not happy, i.e., a number $n$ such that iterating this sum-of-squared-digits map starting with n never reaches the number 1. </p>\n</blockquote>\n\n<p>For example, $23\\rightarrow 2^2+3^2 = 13 \\rightarrow  1^2 + 3^2 = 1$, so $23$ is a happy number. But, number $2$ is not and you can verify it.</p>\n\n<p>The problem around my question (from <a href="http://ser.cs.fit.edu/ser2012/problems/division_1/SER2012%20Problem%20Set%20-%20Division%20I.pdf#page=14" rel="nofollow">2010 acp icpc problem set</a>) is to count unhappy numbers in an interval $[\\textrm{lo}, \\textrm{hi}]$. I\'m looking for an algorithm that is practical for $\\textrm{hi}$ up to $10^{18}$. </p>\n\n<p>How can I  write an algorithm for this problem, efficient and correct?</p>\n\n<p>I know that the solution is with dynamic programming, but I don\'t know how can I get it.</p>\n\n<p>My approach when I read the problem was use a backtracking to mark all numbers in the interval, like a dfs, and see that when you\'re processing a number and got the result, you should mark all numbers with form permutations of digits for the initial number. Then, I thought that a backtracking approach is faster. But this is not enough for contest, because the interval is $[1, 10^{18}]$ so, clearly dynamic programming is the way.</p>\n', 'ViewCount': '236', 'Title': 'Count unhappy numbers in a large interval', 'LastEditorUserId': '1152', 'LastActivityDate': '2013-03-27T12:56:15.447', 'LastEditDate': '2013-02-26T19:28:06.080', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2013-02-26T04:06:10.473', 'FavoriteCount': '1', 'Id': '10112'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is Harrison hashing and what are its applications in web searching? \nCan some one give me some relevant information?    </p>\n\n<p><strong>Update:</strong></p>\n\n<p>I found it <a href="http://www.cs.manchester.ac.uk/ugt/COMP26120/" rel="nofollow">here</a> , and is a part of M.Tech syllabus of a friend of mine. I need to explain him this concept and then see how it can be applied in web applications.\nMany thanks for spending your precious time.    </p>\n', 'ViewCount': '207', 'Title': 'What is Harrison hashing, its applications in web search engines?', 'LastEditorUserId': '6466', 'LastActivityDate': '2013-10-15T01:10:32.837', 'LastEditDate': '2013-02-27T06:34:22.677', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10126', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><strings><search-algorithms><hash>', 'CreationDate': '2013-02-26T15:01:05.643', 'Id': '10121'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve recently came across a <a href="http://www.lsv.ens-cachan.fr/Publis/PAPERS/PDF/FSF-ciaa06.pdf">paper</a> describing the parsing technique\nmentioned in the title. Unfortunately, the terminology used in said paper\nis somewhat beyond my comprehension, so I\'ve been attempting to grasp the\nconstruction algorithm more intuitively. I believe I succeeded (<a href="http://www.docstoc.com/docs/79482088/Approximating-Context-Free-Grammars-for-Parsing-and-Verification">this\npresentation</a> was the source of the ah-ha moment), but a verification of\ncorrectness from someone either familiar with the technique or the terminology\ncontained therein would be greatly appreciated.</p>\n\n<p>I\'m going to describe my take on the solution (if it\'s correct, I believe it\ncould be of help to other people attempting to understand the technique) and\nask additional questions afterwards. To ensure there\'s no misunderstanding,\nI\'m going to use the following standard notation: $a, b, c, ... \\in T$, $A, B,\nC, ... \\in N$, $... X, Y, Z \\in N \\cup T$, $\\alpha, \\beta, \\gamma, ... \\in \\{N\n\\cup T\\}^*$ and, as in the paper, $A \\xrightarrow{i} \\omega$ to denote rule number $i$. However, I\'ll probably use different names for concepts than\nthe original paper.</p>\n\n<p>Also, throughout the description, the equivalence relation $\\kappa_0$ is used.</p>\n\n<h3>Construction</h3>\n\n<p>There are two kinds of items inside the parsing automaton: simple LR(0) items\nof the form $A \\xrightarrow{i} \\alpha \\bullet \\beta$ which I call <em>shift items</em>\nand items of the form $A \\xrightarrow{i} \\alpha \\bullet \\beta, m, n$ which I\ncall <em>resolve items</em>; these tell the parser to push $n$ symbols back the\ninput stream and then reduce by rule number $m$ upon the first symbol of $\\beta$.</p>\n\n<p>The grammar is augmented with the rule $S\' \\xrightarrow{0} S \\$$ and the construction starts with the shift item $S\' \\xrightarrow{0} \\bullet S \\$ $ in the initial state.</p>\n\n<p>Now, to construct the automaton, decide between these alternatives for each item in a state $q$:</p>\n\n<ol>\n<li><p>If the item is a shift item $A \\xrightarrow{i} \\alpha \\bullet \\beta$,\nthere will be a transition $q \\xrightarrow{X} q\'$ in the automaton, where\n$X$ is the first symbol of $\\beta$.</p></li>\n<li><p>If the item is a finished shift item $A \\xrightarrow{i} \\omega \\bullet$,\nadd a resolve item $B \\xrightarrow{j} \\alpha A \\bullet \\beta, i, 0$ for each rule $B \\xrightarrow{j} \\alpha A \\beta$.</p></li>\n<li><p>If the item is a resolve item $A \\xrightarrow{i} \\alpha \\bullet \\beta, m, n$, let $X$ be the first symbol of $\\beta$. If $X \\in N$, add a shift item $X \\xrightarrow{j} \\bullet \\omega$ for each rule $X \\xrightarrow{j} \\omega$. If other items than $A \\xrightarrow{i} \\alpha \\bullet \\beta, m, n$ have $X$ as their dot lookahead, add a transition $q \\xrightarrow{X} q\'$ to the automaton.\nEvery resolve item $C \\xrightarrow{i} \\alpha \\bullet X \\beta, m, n$ in $q$ will result in a resolve item $C \\xrightarrow{i} \\alpha X \\bullet \\beta, m, n + 1$ in $q\'$.</p></li>\n<li><p>If the item is a resolve item $A \\xrightarrow{i} \\omega \\bullet, m, n$ it\nwon\'t contribute any lookahead information and can be discarded, but first add a resolve item $B \\xrightarrow{j} \\alpha A \\bullet \\beta, m, n$ for each rule\n$B \\xrightarrow{j} \\alpha A \\beta$.</p></li>\n</ol>\n\n<p>This is, of course, just a sketch; actually, a closure of the state must be calculated first and only then can we deal with transitions/shifts and resolutions.</p>\n\n<p>Transforming the automaton into a shift-resolve parsing table is trivial afterwards; just, as a minor variation, the authors of the paper interpret a resolution $r_{0,0}$ as the accept action. Given the resulting automaton, I found it handier to simply treat a shift of $\\$$ as the accept action.</p>\n\n<h3>Questions</h3>\n\n<p>The first one is, obviously, whether the process described above is correct.</p>\n\n<p>The second one is about the equivalence relations. I can only guess that the equivalence relation $\\kappa$ is what\'s responsible for deciding which resolve items are brought in when a finished shift item has been seen. $\\kappa_0$ seems to result in lookahead strikingly similar to the $FOLLOW_{LM}$ sets of LSLR parsers. The paper describes a "finer equivalence relation" on page 11; is there a way to interpret this relation in intuitive terms? Are there other relations known?</p>\n\n<p>And the final one is about conflict resolution. The paper describes well what constitutes an inadequacy in a shift-resolve automaton; is there a way of resolving these inadequacies, similar to ways of resolving conflicts in a traditional LR parser? Could something like <em>yacc</em>-style conflict resolution via precedence and associativity be implemented in a ShRe parser generator?</p>\n\n<p>Thanks if you read all this and any answers will be greatly appreciated :)</p>\n', 'ViewCount': '165', 'Title': 'Shift-resolve parsing - questions', 'LastEditorUserId': '4383', 'LastActivityDate': '2013-02-28T19:09:45.330', 'LastEditDate': '2013-02-26T20:28:26.713', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4383', 'Tags': '<algorithms><formal-grammars><context-free><parsing>', 'CreationDate': '2013-02-26T16:10:49.380', 'FavoriteCount': '2', 'Id': '10123'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Let $S$ be a finite set of integers (this set contains about 200000 elements). Let $T \\subset S$ be a particular subset of $S$ called <em>target</em>. $S$ keeps growing. So does $T$. Each new element of $S$ might or might not be in $T$.</p>\n\n<p>No (known, or practical) algorithm can determine if an element $s \\in S$ is in the <em>target</em> set: a human being must give the final word (ie, it is subjective). It is estimated that $T$ has about 30000-35000 elements. I already know $T_1$, a first approximation of $T$, with about 25000 elements. I also already know some thousands of elements of $S$ that are certainly not in $T$.</p>\n\n<p>What I want is a way to approximate $T$ as closely as possible, and present only those elements to a human being. Also, for each new element of $S$, I want to determine if it has high probability of being in $T$ -- and present only those with high probability to a human being.</p>\n\n<hr>\n\n<p>Now, I describe what I can use to try to approximate $T$.</p>\n\n<p>Each integer $s \\in S$ has some <em>labels</em> associated. These can be represented as subsets $L_i \\subset S, \\forall i \\in \\{1, ..., n\\}$ ($n$ is about 250). These subsets are known, determined by algorithms (ie, I have functions $l_i \\to \\{in,out\\}$ such that $l_i(s) = in \\iff s \\in L_i$).</p>\n\n<p>Some label algorithms are very fast, some are slow. Anyways, these labels (ie, the sets $L_i$) have already been determined. Some of these labels contain very few (1-100) elements, some contain a lot (100000-150000). Many labels are independent, some are closely related (ie, I know that some labels are subsets of others, I know that some are disjoint, etc).</p>\n\n<hr>\n\n<p>So, given this framework, what kind of algorithms can I use to approximate $T$? They can be interactive, ie, they could get better after each new approximation of $T$, if this makes the problem easier.</p>\n\n<p>I thought about using a <strong>genetic algorithm</strong> to determine which labels, when intersected, give good approximations of $T$. However, this can get slow, with a na\xefve intersection algorithm (ie, suppose $L_1, L_2, L_3$ are to be intersected; if they are all "big" (50000-150000), it can be quite time consuming to calculate the intersection! -- now, imagine a gene that would require to intersect, say, 50 labels...).</p>\n\n<p>How can I speed this, without sacrificing too much the precision?</p>\n', 'ViewCount': '43', 'Title': 'Approximate target subset by intersecting other subsets', 'LastEditorUserId': '7051', 'LastActivityDate': '2013-02-26T22:51:21.717', 'LastEditDate': '2013-02-26T22:51:21.717', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7051', 'Tags': '<algorithms><probabilistic-algorithms><finite-sets>', 'CreationDate': '2013-02-26T22:42:08.043', 'Id': '10128'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '233', 'Title': 'Why can the state space of the 15 puzzle be divided into two separate parts?', 'LastEditDate': '2013-02-27T02:30:03.507', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2644', 'FavoriteCount': '1', 'Body': '<p>I am trying to understand the <a href="http://www.cut-the-knot.com/pythagoras/fifteen.shtml">proof here</a> of why the state space in 15 puzzle is divided into two separate parts, but the explanation is complicated for me.</p>\n\n<p>Could someone please explain it in simpler terms? I have been struggling with this for days :(</p>\n', 'Tags': '<artificial-intelligence><search-algorithms>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-27T04:32:32.970', 'CommentCount': '3', 'AcceptedAnswerId': '10133', 'CreationDate': '2013-02-26T23:04:18.613', 'Id': '10130'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In a recitation video for <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=P7frcB_-g4w">MIT OCW 6.006</a> at 43:30, </p>\n\n<p>Given an $m \\times n$ matrix $A$ with $m$ columns and $n$ rows, the 2-D peak finding algorithm, where a peak is any value greater than or equal to it\'s adjacent neighbors, was described as:</p>\n\n<p><em>Note: If there is confusion in describing columns via $n$, I apologize, but this is how the recitation video describes it and I tried to be consistent with the video.  It confused me very much.</em></p>\n\n<blockquote>\n  <ol>\n  <li><p>Pick the middle column $n/2$ // <em>Has complexity $\\Theta(1)$</em></p></li>\n  <li><p>Find the max value of column $n/2$ //<em>Has complexity  $\\Theta(m)$ because there are $m$ rows in a column</em></p></li>\n  <li><p>Check horiz. row neighbors of max value, if it is greater then a peak has been found, otherwise recurse with $T(n/2, m)$ //<em>Has complexity $T(n/2,m)$</em></p></li>\n  </ol>\n</blockquote>\n\n<p>Then to evaluate the recursion, the recitation instructor says</p>\n\n<blockquote>\n  <p>$T(1,m) =  \\Theta(m)$ because it finds the max value</p>\n  \n  <p>$$ T(n,m) =  \\Theta(1) +  \\Theta(m) + T(n/2, m) \\tag{E1}$$</p>\n</blockquote>\n\n<p>I understand the next part, at 52:09 in the video, where he says to treat $m$ like a constant, since the number of rows never changes.  But I don\'t understand how that leads to the following product:</p>\n\n<p>$$ T(n,m) = \\Theta(m) \\cdot \\Theta(\\log n) \\tag{E2}$$</p>\n\n<p>I think that, since $m$ is treated like a constant, it is thus treated like $\\Theta(1)$ and eliminated in $(E1)$ above.  But I\'m having a hard time making the jump to $(E2)$.  Is this because we are now considering the case of $T(n/2)$ with a constant $m$?</p>\n\n<p>I think can "see" the overall idea is that a $\\Theta(\\log n)$ operation is performed, at worst, for m number of rows.  What I\'m trying to figure out is how to describe the jump from $(E1)$ to $(E2)$ to someone else, i.e. gain real understanding.</p>\n', 'ViewCount': '168', 'Title': '2-D peak finding complexity (MIT OCW 6.006)', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-28T14:11:09.913', 'LastEditDate': '2013-02-27T22:09:59.223', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '7062', 'Tags': '<algorithms><algorithm-analysis><asymptotics><matrices>', 'CreationDate': '2013-02-27T18:22:56.113', 'FavoriteCount': '0', 'Id': '10141'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have a admissible and consistent heuristic.</p>\n\n<p>Is it true, that when I expand a node, I have guaranteed that the path I found to this node is optimal?</p>\n\n<p>Look at this pseudocode from wikipedia:</p>\n\n<pre><code>function A*(start,goal)\n closedset := the empty set    // The set of nodes already evaluated.\n openset := {start}    // The set of tentative nodes to be evaluated, initially containing the start node\n came_from := the empty map    // The map of navigated nodes.\n\n g_score[start] := 0    // Cost from start along best known path.\n // Estimated total cost from start to goal through y.\n f_score[start] := g_score[start] + heuristic_cost_estimate(start, goal)\n\n while openset is not empty\n     current := the node in openset having the lowest f_score[] value\n     if current = goal\n         return reconstruct_path(came_from, goal)\n\n     remove current from openset\n     add current to closedset\n     for each neighbor in neighbor_nodes(current)\n         tentative_g_score := g_score[current] + dist_between(current,neighbor)\n         if neighbor in closedset\n             if tentative_g_score &gt;= g_score[neighbor]\n                 continue\n\n         if neighbor not in openset or tentative_g_score &lt; g_score[neighbor] \n             came_from[neighbor] := current\n             g_score[neighbor] := tentative_g_score\n             f_score[neighbor] := g_score[neighbor] + heuristic_cost_estimate(neighbor, goal)\n             if neighbor not in openset\n                 add neighbor to openset\n\n return failure\n</code></pre>\n\n<p>I suppose it should be true. Because of this:</p>\n\n<pre><code>if current = goal\n     return reconstruct_path(came_from, goal)\n</code></pre>\n\n<p>If it wasn't true then this test would not guarantee me that the solution is optimal right?</p>\n\n<p>What I don't get and the reason I am asking this question is this:</p>\n\n<pre><code>if neighbor in closedset\n         if tentative_g_score &gt;= g_score[neighbor]\n             continue\n</code></pre>\n\n<p>If the neighbor is in closed list, it means that it has already been expanded. Why are they testing the scores then? Why would not the next condition work?</p>\n\n<pre><code>if neighbor in closedset\n         continue\n</code></pre>\n", 'ViewCount': '97', 'Title': 'A* optimality of the expanded node', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-29T23:28:22.880', 'LastEditDate': '2013-04-02T07:37:43.930', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '16559', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7075', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><correctness-proof>', 'CreationDate': '2013-02-28T15:36:33.660', 'Id': '10152'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading chapter 32 - String Matching from the book <a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">"Introduction to Algorithms" 3rd edition Cormen et al</a>. The Rabin-Karp Algorithm is not clear to me despite heaving read it several times. Specifically, when the authors start proposing about some modulo operations for solving some problem, I am unable to follow. To be precise, the paragraphs from</p>\n\n<blockquote>\n  <p>We have intentionally overlooked one problem: p and ts may be too\n  large too work with conveniently...</p>\n</blockquote>\n\n<p>on till equation 32.2 (pp 991 in 3rd edition) are <a href="http://i.stack.imgur.com/d5ptz.jpg" rel="nofollow">the paragraphs I am having problems with</a>.</p>\n\n<p>What does "convenient" mean here? Why do they use modulo? I am totally lost.\nIs it possible to explain this in simple english?</p>\n\n<p>I will appreciate if someone can help me in understanding this paragraph. And if there is any other reference (with similar mathematical treatment) where this can be read, please tell me.</p>\n', 'ViewCount': '199', 'Title': "Problem with Cormen's treatment of the Rabin-Karp algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:08:04.267', 'LastEditDate': '2013-04-07T12:08:04.267', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><search-algorithms><strings>', 'CreationDate': '2013-03-01T12:51:39.397', 'Id': '10173'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I asked a similar question <a href="http://cs.stackexchange.com/questions/10173/rabin-karp-searching-algorithm">here</a> on Rabin Karp algorithm. My present question is, how do we find the best $q$ (i.e modulus)? What is the criterion? We need to choose a $q$ which will be quick to calculate and also must result in lesser number of spurious hits, right? </p>\n\n<p>Wow do we ensure these things?</p>\n', 'ViewCount': '499', 'Title': 'How do we find the optimal modulus q in Rabin-Karp algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:09:29.210', 'LastEditDate': '2013-04-07T12:09:29.210', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><search-algorithms><strings>', 'CreationDate': '2013-03-01T14:34:49.860', 'Id': '10174'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am looking to generate a heat map from some data. I have a value and a location (longitude and latitude). I understand generating a colour from the value, however I'm not sure how I would go about generating the matrix to use for this. </p>\n\n<p>I figure it's best to place the location points into buckets essentially (so that I have the same number of buckets as pixels in my map) but I'm not sure how to calculate the values for the buckets where I don't have any readings. </p>\n\n<p>Has anyone got any experience with this?</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '675', 'Title': 'Algorithm for generating heat maps', 'LastActivityDate': '2013-03-01T16:49:34.613', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4392', 'Tags': '<algorithms><matrices>', 'CreationDate': '2013-03-01T15:44:36.647', 'Id': '10178'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an undirected tree (with no specific root), how to find the longest path, i.e. 2 vertices that are the farthest apart from each other? There are no lengths associated with the edges (each edge has length 1 by default).<br>\nObviously one idea is to check the path lengths between all pairs of vertices (e.g. by doing a DFS from each vertex), but there should be a more efficient solution. Please include a short proof in your answer.</p>\n', 'ViewCount': '683', 'ClosedDate': '2013-03-04T17:16:27.133', 'Title': 'Longest path in undirected tree', 'LastEditorUserId': '7096', 'LastActivityDate': '2013-03-01T22:46:48.810', 'LastEditDate': '2013-03-01T19:35:40.383', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7096', 'Tags': '<algorithms><graph-theory><trees>', 'CreationDate': '2013-03-01T18:10:03.390', 'Id': '10181'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In many discussions of binary heap, normally only decrease-key is listed as supported operation for a min-heap. For example, CLR chapter 6.1 and <a href="http://en.wikipedia.org/wiki/Heap_%28data_structure%29" rel="nofollow">this wikipedia page</a>. Why isn\'t increase key normally listed for min-heap? I imagine it is possible to do that in O(height) by iteratively swapping the increased element (x) with the minimum of its children, until none of its children is bigger than x.</p>\n\n<p>e.g.</p>\n\n<pre><code>IncreaseKey(int pos, int newValue)\n{\n   heap[pos] = newValue;\n   while(left(pos) &lt; heap.Length)\n   {\n      int smallest = left(pos);\n      if(heap[right(pos)] &lt; heap[left(pos)])\n         smallest = right(pos);\n      if(heap[pos] &lt; heap[smallest])\n      { \n         swap(smallest, pos);\n         pos= smallest;\n      }\n      else return;\n   }   \n}\n</code></pre>\n\n<p>Is the above correct? If not, why? If yes, why isn\'t increase key listed for min-heap?</p>\n', 'ViewCount': '3873', 'Title': 'Increase-key and decrease-key in a binary min-heap', 'LastActivityDate': '2013-03-03T22:31:44.110', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7105', 'Tags': '<algorithms><data-structures><heaps><priority-queues>', 'CreationDate': '2013-03-02T11:08:39.657', 'FavoriteCount': '2', 'Id': '10203'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '143', 'Title': 'Are probabilistic search data structures useful?', 'LastEditDate': '2013-03-03T16:57:40.847', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7022', 'FavoriteCount': '1', 'Body': '<p>A SkipList provides the same $O(\\log n)$ bounds for search as a balanced tree with the advantage that rebalancing isn\'t necessary. Since the SkipList is constructed using random coin flips, these bounds only hold as long as the structure of the SkipList is sufficiently "balanced". In particular, with probability $1/n^c$ for some constant $c&gt;0$, the balanced structure might be lost after inserting an element.</p>\n\n<p>Let\'s say I want to use a skip list as a storage backend in a web application that potentially runs forever. So after some polynomial number of operations, the balanced structure of the SkipList is very likely to be lost. </p>\n\n<p>Is my reasoning correct? Do such probabilistic search/storage data structures have practical applications and if so, how is the above problem avoided? </p>\n\n<p>Edit: I\'m aware that there are deterministic variants of the SkipList, which, are much more complicated to implement in comparison to the (classic) randomized SkipList.</p>\n', 'Tags': '<data-structures><search-trees><probabilistic-algorithms>', 'LastEditorUserId': '7022', 'LastActivityDate': '2013-03-08T13:10:42.580', 'CommentCount': '1', 'AcceptedAnswerId': '10239', 'CreationDate': '2013-03-03T14:09:52.280', 'Id': '10229'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '448', 'Title': 'Finding the size of the smallest subset with GCD = 1', 'LastEditDate': '2013-03-04T17:08:46.660', 'AnswerCount': '2', 'Score': '10', 'OwnerDisplayName': 'user7134', 'PostTypeId': '1', 'OwnerUserId': '7137', 'FavoriteCount': '3', 'Body': '<p>This is a problem from the practice session of the <a href="http://main.edu.pl/en/archive/amppz/2012/dzi">Polish Collegiate Programming Contest 2012</a>. Although I could find the solutions for the main contest, I can\'t seem to find the solution for this problem anywhere.</p>\n\n<p>The problem is: Given a set of $N$ distinct positive integers not greater than $10^9$, find the size $m$ of the smallest subset that has no common divisor other than 1. $N$ is at most 500, and a solution can be assumed to exist.</p>\n\n<p>I managed to show that $m \\le 9$. My reasoning is: Suppose there exists a minimal subset $S$ of size $|S|=10$, with gcd = 1. Then all 9-subsets of $S$ must have gcd > 1. There are exactly 10 such subsets, and their gcds must be pairwise coprime. Let these gcds be $1 &lt; g_1 &lt; g_2 &lt; ... &lt; g_{10}$, where $\\gcd(g_i,g_j)=1$, for $i \\neq j$. Then the maximum number in $S$ is $g_2g_3...g_{10}$. But $g_2g_3...g_{10} \\ge 3\\times5\\times7\\times11\\times...\\times29=3234846615 &gt; 10^9$, a contradiction.</p>\n\n<p>However, even with this, a straightforward brute force is still too slow. Does anyone have any other ideas?</p>\n', 'Tags': '<algorithms><number-theory>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-08T19:29:52.007', 'CommentCount': '2', 'CreationDate': '2013-03-04T06:25:47.053', 'Id': '10249'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a set $S$ of $n$ elements, and a set $\\mathcal{X}$ of $m$ subsets of $S$, decide if there exist $U,V \\in \\mathcal{X}$, s.t. $U \\cup V = S$.</p>\n\n<p>Brute force would take time $O(nm^2)$ but is there any way of solving this more efficiently?</p>\n', 'ViewCount': '113', 'Title': 'Test if there are two subsets which cover a set', 'LastActivityDate': '2013-03-04T19:58:53.887', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '10272', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4259', 'Tags': '<algorithms><data-structures><sets>', 'CreationDate': '2013-03-04T06:43:10.150', 'FavoriteCount': '1', 'Id': '10250'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a undirected graph with no edge costs. A subset of the nodes are labeled $c_1, c_2, ..., c_k$ and one node is labeled $K$. I want to find the minimum cut of the graph with the extra condition that all nodes $c_i$ are in the same half of the cut and the node K is in the other cut.</p>\n\n<p>My idea was to begin by doing a BFS from $K$ to all nodes $c_i$, saving predecessors and then finding all paths from $K$ to a node $c_i$ and finally picking the minimum set of edges from the paths so that at least one edge from each path was chosen. Unfortunately, if I understand this correctly, this is equivalent to the NP-complete <a href="http://en.wikipedia.org/wiki/Set_cover_problem" rel="nofollow">set cover problem</a>.</p>\n\n<p>Is there  anything sane with this approach? Do you have any hints to push me in the right direction?</p>\n\n<p>Note: this is homework so I\'d rather have some hints than a full solution.</p>\n', 'ViewCount': '87', 'Title': 'min-cut with extra condition', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-06T12:10:21.790', 'LastEditDate': '2013-03-05T07:03:20.143', 'AnswerCount': '1', 'CommentCount': '13', 'AcceptedAnswerId': '10316', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7139', 'Tags': '<algorithms><complexity-theory><np-complete>', 'CreationDate': '2013-03-04T10:58:21.523', 'Id': '10255'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I asked a question on Rabin-Karp Searching algorithm <a href="http://cs.stackexchange.com/questions/10173/rabin-karp-searching-algorithm">here</a>, which I am reading from the book "<a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">Introduction to Algorithms" 3rd edition Cormen et al.</a>. </p>\n\n<p>After reading few para of the section on Rabin-Karp, I got some more confusions:</p>\n\n<p>In the third paragraph the authors say that the if we could find <strong>p</strong>  (decimal value of pattern P[1....m] )  in  time O(m) <em>and all the <strong>ts</strong> values</em> (i.e decimal value of length-m sub-string T[s+1....s+m], s=0,1,2,,,,n-m) in time O(n-m+1),  then we could determine all valid shifts s in time O(m) + O(n-m+1) by comparing <strong>p</strong> with each of the <strong>ts</strong> values. </p>\n\n<p>How is this possible? O(m) is for finding p, O(n-m+1) is for finding all ts, so total pre-processing time so far is O(m) + O(n-m+1). This is the total pre-processing time; the comparison has yet to start, I have to spend some extra $ for doing comparison of a decimal p with each of the (n-m+1)-ts values. </p>\n\n<p>1-Then why the authors say in the first para that the pre-processing time is O(m)? Why it is not O(m) + O(n-m+1) which include processing time of p and all ts values? </p>\n\n<p>2- Now if we talk about worst case matching time, what should be that? So in the worst my decimal number p (already calculated ) will be compared with <em>each</em> of the another (m-n+1) decimal numbers, which are the values of ts (already calculated, no extra cash needed for doing this job now ). The worst case is when I am most unlucky and I have to compare every value of ts with p. Right? </p>\n\n<p>Based on my understanding,(if I am right) the worst case matching time should be O(m-n+1) and not O((m-n+1)m) as claimed by the authors in the first para. For example let us say my Pattern is P[1...m]=226 and Text is T[1....n]=224225226. so  my p is decimal 226, and ts is decimal value of T[s+1, s+2, s+3], for s=0,1,2...6 as n=9, and m=3. The ts values will be as follows: </p>\n\n<blockquote>\n  <p>s=0 => T[224]=> ts=224     </p>\n  \n  <p>s=1 => T[242]=> ts=242 </p>\n  \n  <p>s=2 => T[422]=> ts=422 </p>\n  \n  <p>s=3 => T[225]=> ts=225 </p>\n  \n  <p>s=4 => T[252]=> ts=252 </p>\n  \n  <p>s=5 => T[522]=> ts=522 </p>\n  \n  <p>s=6 => T[226]=> ts=226</p>\n</blockquote>\n\n<p>Now you will be comparing p=226 with all these values. So are you not making n-m+1=7 comparisons to achieve search for 226 in T, and not (n-m+1)m =7 x3=21? So the worst case time should be O(n-m+1) and not O((n-m+1)m). </p>\n\n<p>In short I understand that:</p>\n\n<blockquote>\n  <p>Total pre-processing time = O(m) + O(n-m+1) (including for both p and\n  all the ts values)</p>\n  \n  <p>Total matching time in worst case = O(n-m+1)</p>\n</blockquote>\n\n<p>Where I am making mistake? </p>\n', 'ViewCount': '877', 'Title': 'Time Complexity of Rabin-Karp matching algorithm', 'LastEditorUserId': '6466', 'LastActivityDate': '2013-03-07T13:53:20.220', 'LastEditDate': '2013-03-04T12:43:50.803', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><complexity-theory><time-complexity><search-algorithms>', 'CreationDate': '2013-03-04T12:37:40.083', 'FavoriteCount': '0', 'Id': '10258'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Given a tree $T = (V , F)$, find an algorithm which finds $u \\in V$, so in the graph $T = (V \\setminus \\{u\\} , F)$ the size of each connected component is $\\lceil |V| / 2 \\rceil$ at most. What is the complexity?</p>\n</blockquote>\n\n<p>Can I please have a hint?</p>\n', 'ViewCount': '211', 'Title': 'Find node that splits tree in half', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T02:37:14.300', 'LastEditDate': '2013-03-05T07:06:43.247', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'OwnerDisplayName': 'user2102697', 'PostTypeId': '1', 'Tags': '<algorithms><complexity-theory><graph-theory><trees>', 'CreationDate': '2013-03-01T15:40:43.320', 'Id': '10262'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '696', 'Title': 'How to find the maximum independent set of a directed graph?', 'LastEditDate': '2013-03-06T01:05:54.933', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5218', 'FavoriteCount': '2', 'Body': u'<p>I\'m trying to solve <a href="http://www.spoj.com/problems/DIVREL/" rel="nofollow">this problem</a>.  </p>\n\n<blockquote>\n  <p><strong>Problem</strong>: Given $n$ positive integers, your task is to select a maximum number of integers so that there are no two numbers $a, b$ in which $a$ is divisible by $b$.</p>\n</blockquote>\n\n<p>I have to find the Maximum independent set and the size of this set.   The size can be found by <a href="http://en.wikipedia.org/wiki/K%C3%B6nig%27s_theorem_%28graph_theory%29" rel="nofollow">K\xf6nig\'s theorem</a>. But how can I find the Maximum Independent Set (i.e. which vertex are part of the set).  </p>\n\n<p>I did some search also and found something <a href="http://apps.topcoder.com/forums/?module=Thread&amp;threadID=647913&amp;start=0&amp;mc=5#1127789" rel="nofollow">here</a>:</p>\n\n<p><code>If removing a vertex does not change minimum path cover then I can get the desired  result without that vertex.</code></p>\n\n<p>But I don\'t understand the underlying theorem. Any help will be greatly helpful.</p>\n', 'Tags': '<algorithms><graph-theory><bipartite-matching>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-06T01:05:54.933', 'CommentCount': '3', 'AcceptedAnswerId': '10303', 'CreationDate': '2013-03-04T22:16:02.387', 'Id': '10274'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What I only got currently from PCP theorem is that it needs at most $O(\\log n)$ randomness and $O(1)$ query of proof to approximate. So how does this result relate to the fact that solution to NP problems are hard to approximate?</p>\n', 'ViewCount': '84', 'Title': 'Why does PCP theorem imply that NP problems are hard to approximate?', 'LastActivityDate': '2013-03-05T19:11:45.947', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10299', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7154', 'Tags': '<complexity-theory><randomized-algorithms>', 'CreationDate': '2013-03-05T12:56:15.360', 'Id': '10289'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been looking into the math behind converting from any base to any base. This is more about confirming my results than anything. I found what seems to be my answer on mathforum.org  but I'm still not sure if I have it right. I have the converting from a larger base to a smaller base down okay because it is simply take first digit multiply by base you want add next digit repeat. My problem comes when converting from a smaller base to a larger base. When doing this they talk about how you need to convert the larger base you want into the smaller base you have. An example would be going from base 4 to base 6 you need to convert the number 6 into base 4 getting 12. You then just do the same thing as you did when you were converting from large to small. The difficulty I have with this is it seems you need to know what one number is in the other base. So I would of needed to know what 6 is in base 4. This creates a big problem in my mind because then I would need a table. Does anyone know a way of doing this in a better fashion. </p>\n\n<p>I thought a base conversion would help but I can't find any that work. And from the site I found it seems to allow you to convert from base to base without going through base 10 but you first need to know how to convert the first number from base to base. That makes it kinda pointless.</p>\n\n<p>Commenters are saying I need to be able to convert a letter into a number. If so I already know that. That isn't my problem however.\nMy problem is in order to convert a big base to a small base I need to first convert the base number I have into the base number I want. In doing this I defeat the purpose because if I have the ability to convert these bases to other bases I've already solved my problem.</p>\n\n<p>Edit: I have figured out how to convert from bases less than or equal to 10 into other bases less than or equal to 10. I can also go from a base greater than 10 to any base that is 10 or less. The problem starts when converting from a base greater than 10 to another base greater than 10. Or going from a base smaller than 10 to a base greater than 10. I don't need code I just need the basic math behind it that can be applied to code.</p>\n", 'ViewCount': '3899', 'Title': 'The math behind converting from any base to any base without going through base 10?', 'LastEditorUserId': '6912', 'LastActivityDate': '2013-03-12T23:02:49.237', 'LastEditDate': '2013-03-12T21:18:02.180', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6912', 'Tags': '<algorithms><arithmetic><number-formats>', 'CreationDate': '2013-03-06T14:17:37.380', 'FavoriteCount': '2', 'Id': '10318'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an algorithm which, basically given an array of $n$ numbers, checks if there is any repeated numbers in the array, and returns true if there is and false otherwise.</p>\n\n<p>It uses a direct access table (hashing function $h(x)=x$), which makes the running time linear. So it creates a new array, initializes all values to false, then iterates through the given array, and since each value is an index to the hash table, it accesses that array location and changes it to $1$. If it is already a $1$, then you know that it is a repeated number.</p>\n\n<p>But when getting the expected running time, you need to first define a probability space. This is the part I am confused about. How can you define a probability of a number being repeated in A, if you are just given an array with arbitrary numbers? </p>\n', 'ViewCount': '93', 'Title': 'How to get the expected running time of an algorithm', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-06T17:51:22.880', 'LastEditDate': '2013-03-06T17:51:22.880', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-03-06T15:35:43.183', 'FavoriteCount': '1', 'Id': '10320'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If I had a  Dijkstra graph with the number shortest paths from Node A to O being 1,\nwould it be correct to say: <strong>the equal number of shortest paths from A to O is 1 and not 0</strong>, because that node is included as an \'equal shortest path\'? I am really confused.</p>\n\n<p>Here is an image to illustrate my question:</p>\n\n<p><img src="http://i.stack.imgur.com/29PIi.png" alt="automaton"></p>\n', 'ViewCount': '188', 'Title': 'Dijkstra algorithm: equal number of shortest paths', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-20T18:59:30.713', 'LastEditDate': '2013-12-20T18:59:30.713', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '10336', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7173', 'Tags': '<algorithms><shortest-path>', 'CreationDate': '2013-03-06T20:01:48.040', 'Id': '10333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an undirected unweighted  multigraph $G=(V,E)$ and $s,t \\in V$ find a simple $st$-path $P$ s.t. the number of edges leaving $P$ (i.e. the edges with exaclty one endpoint in $P$ ) is minimized.</p>\n\n<p>Does anybody have any idea how to solve this?\nI first thought about replacing edges by two arcs and weight them according to some node degrees but the edges staying in $P$ make life difficult.</p>\n', 'ViewCount': '45', 'Title': '$st$-path with fewest leaving edges', 'LastEditorUserId': '4259', 'LastActivityDate': '2013-03-07T01:43:30.247', 'LastEditDate': '2013-03-07T01:34:15.203', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10351', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4259', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-03-06T23:21:53.257', 'Id': '10339'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a recurrence relation which is like the following:<p></p>\n\n<p>$T(n) = 2T(\\frac{n}{2}) + \\log_{2}n$</p>\n\n<p>I am using recursion tree method to solve this. And at the end, i came up with the following equation:<p></p>\n\n<p>$T(n)=(2\\log_{2}n)(n-1)-(1\\times 2 + 2\\times 2^{2} + \\ldots + k\\times2^{k})$ where $k=\\log_{2}n$</p>\n\n<p>I am trying to find a theta notation for this equation. But i cannot find a closed formula for the sum $(1\\times 2 + 2\\times 2^{2} + \\ldots + k\\times2^{k})$. How can I find a big theta notation for $T(n)$? </p>\n', 'ViewCount': '250', 'Title': 'Need help about solving a recurrence relation', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-03-07T20:27:23.937', 'LastEditDate': '2013-03-07T00:27:04.883', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7175', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation><recursion>', 'CreationDate': '2013-03-07T00:10:55.800', 'FavoriteCount': '2', 'Id': '10346'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a recurrence relation and trying to use master theorem to solve it. The recurrence relation is:</p>\n\n<p>$T(n) = 3T(n/5) + n^{0.5}$</p>\n\n<p>Can I use the master theorem in that relation? If so, can I say that $T(n)$ is $\u0398(n^{0.5})$?</p>\n', 'ViewCount': '24', 'ClosedDate': '2013-03-07T11:23:42.837', 'Title': 'The use of master theorem appriopriately', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-09-28T12:35:50.493', 'LastEditDate': '2013-09-28T12:35:50.493', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7175', 'Tags': '<algorithms><time-complexity><recurrence-relation><recursion><master-theorem>', 'CreationDate': '2013-03-07T09:48:45.607', 'Id': '10356'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a recurrence relation, it is like the following:</p>\n\n<p>$T(e^n) = 2(T(e^{n-1})) + e^n$, where $e$ is the base of the natural logarithm.</p>\n\n<p>To solve this and find a $\\Theta$ bound, I tried the following: I put $k=e^n$, and the equation transforms into:</p>\n\n<p>$$T(k)=2T(k/e)+k$$</p>\n\n<p>Then, I try to use the Master Theorem. According to Master Theorem, $a=2$, $b=e\\gt 2$ and $f(k)=k$. So, we have the case where $f(k)=\\Omega(n^{\\log_b a+\u03b5})$ for some $\u03b5\\gt 0$, thus we have $T(k)=\\Theta(f(k))=\\Theta(k)$. Then put $k=n$, we have $T(n)=\\Theta(n)$. Does my solution have any mistakes?</p>\n', 'ViewCount': '81', 'Title': 'Not sure if my solution to following recurrence is correct', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-09-28T12:36:02.527', 'LastEditDate': '2013-09-28T12:36:02.527', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7175', 'Tags': '<algorithms><time-complexity><recurrence-relation><master-theorem><check-my-answer>', 'CreationDate': '2013-03-07T11:52:11.320', 'FavoriteCount': '1', 'Id': '10359'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am seeking help understanding Floyd\'s cycle detection algorithm. I have gone through the explanation on wikipedia (<a href="http://en.wikipedia.org/wiki/Cycle_detection#Tortoise_and_hare" rel="nofollow">http://en.wikipedia.org/wiki/Cycle_detection#Tortoise_and_hare</a>)</p>\n\n<p>I can see how the algorithm detects cycle in O(n) time. However, I am unable to visualise the fact that once the tortoise and hare pointers meet for the first time, the start of the cycle can be determined by moving tortoise pointer back to start and then moving both tortoise and hare one step at a time. The point where they first meet is the start of the cycle.</p>\n\n<p>Can someone help by providing an explanation, hopefully different from the one on wikipedia, as I am unable to understand/visualise it?</p>\n', 'ViewCount': '3398', 'Title': "Floyd's Cycle detection algorithm | Determining the starting point of cycle", 'LastActivityDate': '2013-03-07T12:24:41.433', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7183', 'Tags': '<algorithms><linked-lists>', 'CreationDate': '2013-03-07T12:24:41.433', 'Id': '10360'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '95', 'Title': 'Using the appropriate machine learning algorithm', 'LastEditDate': '2013-03-09T01:35:50.600', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7196', 'FavoriteCount': '1', 'Body': '<p>I am not sure if this is the right forum to ask this.</p>\n\n<p>I have some data of the houses, like their size(in square meters), if they use aircondition, how many residents live in, I have their electricity consumption as well.\nI want to train any Machine Learning Algorithm to the dataset above, in order to create a model that estimates the houses consumption.</p>\n\n<p>I tried many different algorithms (using weka), but I did not have good results.\nI was said that SVMs could solve this problem, with the right preprocessing. However, i did not have good results either. </p>\n\n<p>Can anyone help me, in the way i should approach this problem?</p>\n\n<p>Thanks in advance</p>\n', 'ClosedDate': '2013-04-24T06:46:56.037', 'Tags': '<algorithms><machine-learning><artificial-intelligence>', 'LastEditorUserId': '7196', 'LastActivityDate': '2013-03-09T21:40:15.973', 'CommentCount': '1', 'CreationDate': '2013-03-08T20:41:55.537', 'Id': '10392'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So I\'ve been doing some reading on community detection in graphs as I\'m planning on working on my thesis for it. I\'ve been reviewing papers regarding the same and came across the <a href="http://www.pnas.org/content/99/12/7821.abstract" rel="nofollow">Girvan-Newman algorithm</a>. I\'ve read the paper and have a doubt which I couldn\'t really figure out.</p>\n\n<p>The algorithm works by removing the edge which has the highest value of "edge betweenness" in every iteration. Suppose I have a graph $G(V, E)$ such that $V = \\{ v_{1}, v_2, \\cdots , v_n\\}$. Now suppose this graph is such that it has 2 distinct communities (something we already know but that\'s what the algorithm should detect). After the first iteration, it would remove the edge which has the highest value of edge betweenness. Now my question is, after this is done (or even after a few iterations), say I have any vertex $v_i$, how do we know which community this vertex belong to? Am I missing something here?</p>\n', 'ViewCount': '65', 'Title': 'How do we know to what community a vertex belongs to in the Girvan-Newman algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-10T17:28:08.773', 'LastEditDate': '2013-03-10T11:58:27.300', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10431', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5020', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-03-09T22:55:03.733', 'Id': '10411'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for good resources which have classified and solved typical large scale data processing in MapReduce framework (like graph algorithms, statistics, numerical algorithms ...). Any help is appreciated!</p>\n', 'ViewCount': '240', 'Title': 'Problem Solving in MapReduce Framework', 'LastEditorUserId': '157', 'LastActivityDate': '2013-03-17T21:28:11.707', 'LastEditDate': '2013-03-17T21:28:11.707', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><reference-request><parallel-computing>', 'CreationDate': '2013-03-11T16:34:31.853', 'FavoriteCount': '2', 'Id': '10453'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the <a href="http://en.wikipedia.org/wiki/Interval_scheduling" rel="nofollow">interval scheduling problem</a>, see also <a href="http://www.phailed.me/2012/08/interval-scheduling-problem/" rel="nofollow">here</a>. </p>\n\n<p>In order to schedule the $n$ job requests over one resource, you sort the requests in order of finish time, choose the request with earliest finish time, choose the next compatible one, and so on. </p>\n\n<p>Now, let us say we have two resources instead of one. How do I schedule my jobs now? As my idea goes, again you start by sorting the requests in order of finish time. My problem is, how do I proceed after that? Do I choose the resources in sequential or in an alternate fashion?</p>\n\n<p>If I go for sequential manner, I schedule all the possible jobs in the first resource and then do the same for second resource with the jobs yet to be scheduled. </p>\n\n<p>If I go for alternate fashion, I choose the first possible job in the first resource, then second possible job in the second resource and so on.</p>\n\n<p>In each case we will have take in to account the chosen jobs being compatible, needless to say.</p>\n\n<p>I can not decide which of these is going to be optimal.</p>\n\n<p>Any input will be appreciated.</p>\n', 'ViewCount': '168', 'Title': 'Interval Scheduling Problem with more than One Resource', 'LastEditorUserId': '7200', 'LastActivityDate': '2013-03-18T00:07:18.963', 'LastEditDate': '2013-03-18T00:07:18.963', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7200', 'Tags': '<algorithms><optimization><scheduling>', 'CreationDate': '2013-03-12T00:49:56.563', 'Id': '10460'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to calculate the amortized cost of a dynamic array, that\'s size becomes 4 times the size when the array is filled. (when you re-size, you create a new one and copy the elements there).</p>\n\n<p><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-13-amortized-algorithms-table-doubling-potential-method/lec13.pdf" rel="nofollow">Here</a> is what I am reading from. (starts at pg. 30)\nThis example has the array doubling when it is filled.</p>\n\n<p>This is my potential function analysis so far:\nBut in the end I am getting 7-2i, I don\'t think it can be like that. I think the i\'s should cancel out.</p>\n\n<p>Does anyone know whats wrong?</p>\n\n<p>Potential of the array after the $i^{th}$ insertion is $\\Phi(D_i) = 4i - 4^{\\left\\lceil \\log_{4}i\\right\\rceil}$.</p>\n\n<p>Assume $4^{\\left\\lceil \\log_{4}0\\right\\rceil} = 0$.</p>\n\n<p>The amortized cost of the $i^{th}$ insertion is:</p>\n\n<p>$\\^c_i = c_i + \\Phi(D_i)-\\Phi(D_{i-1})$</p>\n\n<p>$~~~= \\{ i$ if $i-1$ is an exact power of $4$</p>\n\n<p>$~~~~~~~\\{ 1$ otherwise</p>\n\n<p>$~~~~~~~+ (4i-4^{\\left\\lceil \\log_{4}i\\right\\rceil})-(4(i-1)-4^{\\left\\lceil \\log_{4}\n(i-1)\\right\\rceil})$</p>\n\n<p>$~~~= \\{ i$ if $i-1$ is an exact power of $4$</p>\n\n<p>$~~~~~~~\\{ 1$ otherwise</p>\n\n<p>$~~~~~~~+ 4-4^{\\left\\lceil \\log_{4}i\\right\\rceil}+4^{\\left\\lceil \\log_{4}(i-1)\\right\\rceil}$</p>\n\n<p>Case 1 ($i-1$ is an exact power of $4$):</p>\n\n<p>$\\^c_i = i + 4-4^{\\left\\lceil \\log_{4}i\\right\\rceil}+4^{\\left\\lceil \\log_{4}(i-1)\\right\\rceil}$</p>\n\n<p>$~~~= i + 4-4(i-1)+(i-1)$</p>\n\n<p>$~~~= i + 4-4i+4+i-1$</p>\n\n<p>$~~~= 7-2i$</p>\n', 'ViewCount': '25', 'ClosedDate': '2013-03-12T10:25:30.603', 'Title': 'Potential function in amortized analysis', 'LastActivityDate': '2013-03-12T02:25:07.893', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><amortized-analysis>', 'CreationDate': '2013-03-12T02:25:07.893', 'Id': '10463'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '743', 'Title': 'What is an Efficient Algorithm?', 'LastEditDate': '2013-03-12T14:10:14.620', 'AnswerCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6728', 'FavoriteCount': '1', 'Body': '<p>From the point of view of asymptotic behavior, what is considered an "efficient" algorithm?  What is the standard / reason for drawing the line at that point?  Personally, I would think that anything which is what I might naively call "sub-polynomial", such that $f(n) = o(n^2)$ such as $n^{1+\\epsilon}$ would be efficient and anything which is $\\Omega(n^2)$ would be "inefficient".  However, I\'ve heard anything which is of any polynomial order be called efficient.  What\'s the reasoning?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-12T17:39:15.060', 'CommentCount': '1', 'AcceptedAnswerId': '10477', 'CreationDate': '2013-03-12T10:11:19.533', 'Id': '10472'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some questions regarding local search / optimization as explained in chapter 4 of the book : <a href="http://aima.cs.berkeley.edu/" rel="nofollow">http://aima.cs.berkeley.edu/</a></p>\n\n<p>In classical search (Chap 3), the search starts from an initial node, then the search continues based on strategies of BFS, DFS, etc. What I am unsure of is the process of local search (Chap 4). </p>\n\n<ol>\n<li><p>Does the local search algorithm start from one node in state space? Check if constraints are satisfied? If Yes, this is goal? If not, move to neighbours?</p></li>\n<li><p>Is the entire state space considered goal state? Even nodes that don\'t satisfy the constraints?</p></li>\n<li><p>In optimization, the search is conducted in a part of search space where all constraints are met, but tries to find a better solution. In that case, what if the search algorithm moves to nodes where they don\'t satisfy the constraints?</p></li>\n</ol>\n', 'ViewCount': '178', 'Title': 'Local Search vs Classical Search', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-17T20:37:57.327', 'LastEditDate': '2013-03-17T20:37:57.327', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10499', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6966', 'Tags': '<artificial-intelligence><search-algorithms>', 'CreationDate': '2013-03-12T17:09:53.467', 'Id': '10491'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of observations of real data, and a set of Random Variables I produce my self. The goal is to generated Random Variables with the same Distribution as the real data. To investigate the simliarity the two distributions, I use the Kullback-Leibler Divergence. Now with the genetic algorithm of Matlab, I try to find the best parameters for my generator, the thing only is that genetic algorithm is stochastic and so I have no consistent results for the Kullback Leibler Divergence. One idea would be though to run my generator say a thousand times then chosse the minimum Kullback-Leibler Divergence and reproduce the result with this: <a href="http://www.mathworks.ch/ch/help/gads/reproducing-your-results-1.html" rel="nofollow">http://www.mathworks.ch/ch/help/gads/reproducing-your-results-1.html</a>\nWould that work? The idea would be to get a continuous fitness function...</p>\n', 'ViewCount': '40', 'Title': 'Stochasticity of Genetic Algorithm', 'LastActivityDate': '2013-03-12T21:50:37.423', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7255', 'Tags': '<genetic-algorithms>', 'CreationDate': '2013-03-12T21:50:37.423', 'Id': '10496'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much for an expected running time. Am I making a mistake here?</p>\n", 'ViewCount': '321', 'Title': 'How to get expected running time of hash table?', 'LastActivityDate': '2013-03-12T23:38:43.887', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '10500', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><hash-tables><hash><probabilistic-algorithms>', 'CreationDate': '2013-03-12T23:20:41.050', 'Id': '10498'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an example of the "iterative lengthening search". I have searched and I was only able to find definitions like</p>\n\n<blockquote>\n  <p>iterative lengthening search an iterative analog to uniform cost\n  search. The idea is to use increasing limits on path cost. If a node\n  is generated whose path cost exceeds the current limit, it is\n  immediately discarded. For each new iteration, the limit is set to the\n  lowest path cost of any node discarded in the previous iteration</p>\n</blockquote>\n\n<p>I\'d be deeply grateful for an example, that enables me to understand.</p>\n', 'ViewCount': '318', 'Title': 'iterative lengthening search example', 'LastActivityDate': '2014-02-03T08:04:45.670', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5052', 'Tags': '<artificial-intelligence><search-algorithms>', 'CreationDate': '2013-03-13T02:25:48.830', 'Id': '10504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'd like to start by stating this isn't homework! I'm studying for a job interview and would appreciate a second opinion. (Well, I guess it is homework, but not for school!).</p>\n\n<p>I've written an algorithm (see pseudocode below) to merge two <em>sorted</em> lists into one sorted list. The problem requires that I implement it as a recursive divide-and-conquer algorithm. My algorithm is recursive and works <em>but does it count as divide-and-conquer</em>?</p>\n\n<p>The reason I'm asking is that the other people working on the same problem insist D&amp;C must divide the lists in half every time and have $O(n \\log n)$ complexity, like quicksort and mergesort. My algorithm doesn't divide the lists in the middle and has a complexity of $O(n+m)$ (where $n$ and $m$ are the lengths of the lists).</p>\n\n<p>To summarize my question: <strong>Does a D&amp;C algorithm have to have $O(n \\log n)$ complexity and divide the problem in half every time? Or does this algorithm count as D&amp;C?</strong></p>\n\n<pre><code>merge_sorted_lists(list1, list2)\n    if(list1 and list2 are empty)\n        return empty list\n\n    if(list1 is empty)\n        return list2\n\n    else if(list2 is empty)\n        return list1\n\n    else if(head of list1 &lt; head of list2)\n        smaller = pop head of list1\n\n    else if(head of list2 &lt; head of list1)\n        smaller = pop head of list2\n\n    return smaller + merge_sorted_lists(list1, list2)\n</code></pre>\n\n<p>P.S. I've implemented the algorithm in Java and it works.</p>\n", 'ViewCount': '239', 'Title': 'Is this a divide-and-conquer algorithm?', 'LastEditorUserId': '7007', 'LastActivityDate': '2013-03-17T03:43:34.093', 'LastEditDate': '2013-03-14T15:12:38.100', 'AnswerCount': '3', 'CommentCount': '7', 'AcceptedAnswerId': '10520', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Tags': '<algorithms><terminology><divide-and-conquer>', 'CreationDate': '2013-03-14T06:33:28.713', 'Id': '10518'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a kind of cutting problem. There is an irregular polygon that doesn't have any holes and a list of standard sized of rectangular tiles and their values.</p>\n\n<p>I want an efficient algorithm to find the single best valued tile that fit in this polygon; or an algorithm that just says if a single tile can fit inside the polygon. And it should run in deterministic time for irregular polygons with less than 100 vertices.</p>\n\n<p>Please consider that you can rotate the polygon and tiles. Answers/hints for both convex and non-convex polygons are appreciated.</p>\n", 'ViewCount': '383', 'Title': 'An algorithm for fitting a rectangle inside a polygon', 'LastActivityDate': '2013-03-14T18:32:50.853', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7279', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-03-14T10:26:51.393', 'Id': '10522'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It seems that (in a broad sense) two approaches can be utilized to produce an algorithm for solving various optimization problems: </p>\n\n<ol>\n<li>Start with a feasible solution and expand search until constraints are tight and solution is maximal (or minimal).  </li>\n<li>Begin with violated constraints and search for maximal (or minimal) feasible approach.</li>\n</ol>\n\n<p>For the Max-Flow problem Ford-Fulkerson satisfies condition (1), while Push-Relabel satisfies condition (2). An interesting point is that Push-Relabel is a more efficient algorithm than Ford-Fulkerson. My question is this:</p>\n\n<blockquote>\n  <p>What other examples are there where (2)-based approaches outperform their (1)-based counterparts?</p>\n</blockquote>\n\n<p>A follow up is:</p>\n\n<blockquote>\n  <p>Do there exist meta-theorems regarding approaches based on condition (2)? </p>\n</blockquote>\n', 'ViewCount': '106', 'Title': 'Constraint violation and efficiency in search', 'LastEditorUserId': '19', 'LastActivityDate': '2013-03-19T00:26:53.147', 'LastEditDate': '2013-03-15T19:22:46.253', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10530', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '19', 'Tags': '<algorithms><optimization><lower-bounds>', 'CreationDate': '2013-03-15T02:46:32.423', 'Id': '10528'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2573', 'Title': 'BIT: What is the intuition behind a binary indexed tree and how was it thought about?', 'LastEditDate': '2013-03-16T19:33:32.607', 'AnswerCount': '1', 'Score': '19', 'PostTypeId': '1', 'OwnerUserId': '6823', 'FavoriteCount': '22', 'Body': '<p>A binary indexed tree has very less or relatively no literature as compared to other data structures. The only place where it is taught is <a href="http://community.topcoder.com/tc?module=Static&amp;d1=tutorials&amp;d2=binaryIndexedTrees">the topcoder tutorial</a>. Although the tutorial is complete in all the explanations, I cannot understand the intuition behind such a tree? How was it invented? What is the actual proof of its correctness?</p>\n', 'Tags': '<algorithms><binary-trees><trees>', 'LastEditorUserId': '7304', 'LastActivityDate': '2013-12-17T10:36:55.957', 'CommentCount': '2', 'AcceptedAnswerId': '10541', 'CreationDate': '2013-03-15T17:56:58.113', 'Id': '10538'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '266', 'Title': 'What algorithm would compute the maximum choices from two sets?', 'LastEditDate': '2013-03-17T18:03:31.260', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7284', 'FavoriteCount': '1', 'Body': '<p>Given two vectors of integers of possibly unequal lengths, how can I determine the maximum result possible from accumulating choosing the maximum between corresponding pairs of numbers between the two vectors with extra zeros inserted into the shorter vector to make up for the size difference?</p>\n\n<p>For example, consider the following two vectors as inputs:</p>\n\n<pre><code>[8 1 4 5]\n[7 3 6]\n</code></pre>\n\n<p>The choices for inserting the zero and the resulting sum are:</p>\n\n<pre><code>[0 7 3 6]  =&gt; Maximums: [8 7 4 6]  =&gt;  Sum is: 25\n[7 0 3 6]  =&gt; Maximums: [8 1 4 6]  =&gt;  Sum is: 19\n[7 3 0 6]  =&gt; Maximums: [8 3 4 6]  =&gt;  Sum is: 21\n[7 3 6 0]  =&gt; Maximums: [8 3 6 5]  =&gt;  Sum is: 22\n</code></pre>\n\n<p>Therefore, in this case, the algorithm should return 25.</p>\n\n<p>I could do this by brute force by calculating for all permutations of placing zeros into the smaller vector (as just done above) but this would be computationally expensive, and worst in the case when one vector is exactly half the size of the other.</p>\n\n<p>Is there a way to compute the answer in linear time proportional to the length of the longer vector even when the vectors differ in length?  If not, can we do better than the number of factorial permutations being chosen?</p>\n', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-17T18:03:31.260', 'CommentCount': '3', 'AcceptedAnswerId': '10544', 'CreationDate': '2013-03-15T20:44:46.160', 'Id': '10543'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The goal is to distribute approximately 100 million variable length strings, average length 100 characters, uniformly among 100 million buckets. Perfection not required, just no egregious clumping. The strings are URLs. They tend to begin much the same way and end much the same way (e.g. h t t p:// or w w w. and  ".c o m", ".e d u", "a s p x" and so forth) and show their greatest variation in the latter half of the string, except for the final few characters.</p>\n\n<p>What\'s a good algorithm that would accept the string and the number of slots as inputs, and return a number between 0 and SlotCount-1, and satisfy the uniform-distribution requirement?</p>\n', 'ViewCount': '123', 'Title': 'Hashing algorithm for millions of variable length strings (URLs)', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-03-16T23:38:10.913', 'LastEditDate': '2013-03-16T23:38:10.913', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7302', 'Tags': '<algorithms><hash>', 'CreationDate': '2013-03-16T18:16:42.840', 'Id': '10554'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m trying to figure out what are currently the two most efficent algorithms that permit, starting from a Left/Right pair of stereo images created using a traditional camera (so affected by some epipolar lines misalignment), to produce a pair of adjusted images plus their depth information by looking at their disparity.</p>\n\n<p>Actually I\'ve found lots of papers about these two methods, like:</p>\n\n<ul>\n<li>"Computing Rectifying Homographies for Stereo Vision" (Zhang - seems one of the best for rectification only)</li>\n<li>"Three-step image recti\ufb01cation" (Monasse)</li>\n<li>"Rectification and Disparity" (slideshow by Navab)</li>\n<li>"A fast area-based stereo matching algorithm" (Di Stefano - seems a bit inaccurate)</li>\n<li>"Computing Visual Correspondence with Occlusions via Graph Cuts" (Kolmogorov - this one produces a very good disparity map, with also occlusion informations, but is it efficient?)</li>\n<li>"Dense Disparity Map Estimation Respecting Image Discontinuities" (Alvarez - too long for a first review)</li>\n</ul>\n\n<p>Could someone please give me some advice for diving into this wide topic? </p>\n\n<p>What kind of algorithm/method should I treat first, considering that I\'ll work on a very simple input: a pair of left and right images and nothing else, no more information (some papers are based on additional, pre-obtained, calibration information)?</p>\n\n<p>Speaking about working implementations, the only interesting results I\'ve seen so far belongs to <a href="http://stereo.jpn.org/eng/stphmkr/index.html" rel="nofollow">this</a> piece of software, but only for automatic rectification, not disparity.</p>\n\n<p>I tried the "auto-adjustment" feature and it seems really effective. Too bad there is no source code.</p>\n', 'ViewCount': '120', 'Title': 'Stereo images rectification and disparity: which algorithms?', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-17T18:19:58.393', 'LastEditDate': '2013-03-17T18:19:58.393', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7314', 'Tags': '<algorithms><reference-request><image-processing>', 'CreationDate': '2013-03-17T16:33:52.137', 'FavoriteCount': '1', 'Id': '10584'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m working on a type system and hit upon a problem that seems similar to lowest common ancestor. Given two types, I need to find the smallest sequence of conversions which will result in the same target type. If I had a simple type tree I know how to get the result, but unfortunately I have a slightly more complex graph structure.</p>\n\n<p>That graph has a few key points. It is unidirectional and no loops are ever formed. Due to an unlimited number of types however it cannot be produced statically. The distance of a path is generally quite low. It "feels" more like a tree with a bunch of shortcut edges.</p>\n\n<p>Initially I looked at lowest common ancestor, but it is mainly described as a tree algorithm. I\'ve not yet given up hope that I could adapt it. The other possibility would be a more generic path-finding algorithm.</p>\n\n<p>I\'m hoping somebody has seen this problem before, or a similar one, and can give me some references on how to further approach it. It seems familiar enough that I assume something must exist and I\'m just searching for the wrong terms/names.</p>\n\n<hr>\n\n<p>Here\'s my attempt to describe this more formally. </p>\n\n<p>Let there be a graph $G = \\{ V \\}$ such that each vertex has a set of outgoing edges $V = \\{ E=V_x \\}$. Note, as the graph is dynamic, possibly infinite, there is no way to construct the form $G = \\{V, E=(V_x,V_y)\\}$ for the entire graph.</p>\n\n<p>A path is formed from a vertex by following any of the available edges from that node. $Pnm_x = V_n, ..., V_m$. The length of this path is equal to the number of vertices in the sequence. There is no cycle possible. The set of all paths between two nodes is expressed as $Pnm = \\{ V_n, ..., V_m \\}$. </p>\n\n<p>Note that $Pnm$ can be determined to be empty in a finite number of steps. Enumerating the entire $Pnm$ set is not practically possible.</p>\n\n<p>The problem is finding the shortest path from two vertices to a third vertex. That is, given $V_a, V_b$, find $V_c$ such that paths $Pac_x$ and $Pbc_y$ exist and $length(Pac_x) + length(Pbc_y)$ is minimal.</p>\n', 'ViewCount': '624', 'Title': 'Lowest common ancestor similar algorithm for a graph', 'LastEditorUserId': '1642', 'LastActivityDate': '2013-03-19T04:11:32.030', 'LastEditDate': '2013-03-19T04:11:32.030', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1642', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-03-18T19:03:14.890', 'Id': '10603'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '271', 'Title': 'Weighted sum of last N numbers', 'LastEditDate': '2013-03-21T03:56:56.377', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '4213', 'FavoriteCount': '2', 'Body': u"<p>Suppose we're receiving numbers in a stream. After each number is received, a weighted sum of the last $N$ numbers needs to be calculated, where the weights are always the same, but arbitrary. </p>\n\n<p>How efficiently can this done if we are allowed to keep a data structure to help with the computation? Can we do any better than $\\Theta(N)$, i.e. recomputing the sum each time a number is received?</p>\n\n<p>For example:\nSuppose the weights are $W= \\langle w_1, w_2, w_3, w_4\\rangle$. At one point we have the list of last $N$ numbers $L_1= \\langle a, b, c, d \\rangle&gt;$, and the weighted sum $S_1=w_1*a+w_2*b+w_3*c+w_4*d$. </p>\n\n<p>When another number, $e$, is received, we update the list to get $L_2= \\langle b,c,d,e\\rangle$ and we need to compute $S_2=w_1*b+w_2*c+w_3*d+w_4*e$.</p>\n\n<p><strong>Consideration using FFT</strong>\nA special case of this problem appears to be solvable efficiently by employing the Fast Fourier Transform. Here, we compute the weighed sums $S$ in multiples of $N$. In other words, we receive $N$ numbers and only then can we compute the corresponding $N$ weighed sums. To do this, we need $N-1$ past numbers (for which sums have already been computed), and $N$ new numbers, in total $2N-1$ numbers. </p>\n\n<p>If this vector of input numbers and the weight vector $W$ define the coefficients of the polynomials $P(x)$ and $Q(x)$, with coefficients in $Q$ reversed, we see that the product $P(x)\\times Q(x)$ is a polynomial whose coefficients in front of $x^{N-1}$ up to $x^{2N-2}$ are exactly the weighted sums we seek. These can be computed using FFT in $\\Theta(N*\\log (N))$ time, which gives us an average of $\u0398(\\log (N))$ time per input number.</p>\n\n<p>This is however not a solution the the problem as stated, because it is required that the weighted sum is computed efficiently <em>each</em> time a new number is received - we cannot delay the computation.</p>\n", 'Tags': '<algorithms><data-structures><online-algorithms>', 'LastEditorUserId': '683', 'LastActivityDate': '2013-03-21T18:20:56.223', 'CommentCount': '4', 'AcceptedAnswerId': '10670', 'CreationDate': '2013-03-19T05:24:49.463', 'Id': '10612'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm doing a practice test for a course on routing problems. There's a question asking to infere the time complexity of the 2-opt method.</p>\n\n<p>I can see that the complexity per iteration is in $O(n^2)$ because you need to check all combinations of 2 arcs. I don't know how to calculate the worst case number of iterations though. The paper we got doesn't say anything about it's tc, so i'm kind of stuck here.</p>\n\n<p>How do i calculate this?</p>\n", 'ViewCount': '136', 'Title': 'Time complexity 2-opt method', 'LastEditorUserId': '31', 'LastActivityDate': '2013-03-19T14:31:03.800', 'LastEditDate': '2013-03-19T14:31:03.800', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7332', 'Tags': '<algorithms><heuristics><routing>', 'CreationDate': '2013-03-19T14:09:55.420', 'Id': '10618'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>E.g.</p>\n\n<p><strong>Matching Problem</strong> The DFA of regex <code>good</code> is like a chain.</p>\n\n<pre><code>match:     "good"\nnot match: "people do not give good comments are not good people"\n</code></pre>\n\n<p><strong>Searching Problem</strong> The DFA used in searching regex <code>good</code> could be:</p>\n\n<p><img src="http://i.stack.imgur.com/BcnoQ.jpg" alt="enter image description here"></p>\n\n<pre><code>1 matching:  "good"\n2 matchings: "people do not give good comments are not good people"\n</code></pre>\n\n<p>Here are the questions:</p>\n\n<ol>\n<li>In the above two problems, it seems the searching problem\'s DFA is the matching problem\'s DFA plus some backedges and an additional state (here state <code>0</code>). Is this the difference in general?</li>\n<li>What is the regex of searching problem in this case?</li>\n</ol>\n', 'ViewCount': '139', 'Title': 'Difference between pattern matching and pattern searching in terms of DFA/Regex', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-20T18:51:02.610', 'LastEditDate': '2013-03-20T11:08:46.557', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'JackWM', 'PostTypeId': '1', 'Tags': '<finite-automata><regular-expressions><search-algorithms>', 'CreationDate': '2013-03-19T00:16:58.863', 'Id': '10632'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given [[1,4,7],[2,5,8],[3,6,9]] which is a list of the column vectors of matrix</p>\n\n<pre><code>|1, 2, 3|\n|4, 5, 6|\n|7, 8, 9|\n</code></pre>\n\n<p>is $ \\Omega(n^2) $ a lower bound for transposing? Assume the matrix is not always square. I have to touch each element at least once, because going from 2 x 5 to 5 x 2 matrix for example, will mean going from a list of 5 lists to a list of 2 lists, so I can't really do any tricks with the array indices, right?</p>\n\n<p>Is there a faster way to transpose matrices?</p>\n", 'ViewCount': '88', 'Title': 'Complexity of transposing matrices represented as list of row or column vectors', 'LastEditorUserId': '7362', 'LastActivityDate': '2013-03-22T21:57:06.330', 'LastEditDate': '2013-03-22T21:57:06.330', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '10672', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7341', 'Tags': '<algorithms><lower-bounds><matrices>', 'CreationDate': '2013-03-20T05:13:01.430', 'Id': '10636'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Inspired by <a href="http://cs.stackexchange.com/q/2336/71">this question</a> in which the asker wants to know if the running time changes when the comparator used in a standard search algorithm is replaced by a fair coin-flip, and also <a href="http://www.robweir.com/blog/2010/02/microsoft-random-browser-ballot.html">Microsoft\'s</a> prominent failure to write a uniform permutation generator, my question is thus:</p>\n\n<p>Is there a comparison based sorting algorithm which will, depending on our implementation of the comparator:</p>\n\n<ol>\n<li>return the elements in sorted order when using a <em>true</em> comparator (that is, the comparison does what we expect in a standard sorting algorithm) </li>\n<li>return a uniformly random permutation of the elements when the comparator is replaced by a fair coin flip (that is, return <code>x &lt; y = true</code> with probability 1/2, regardless of the value of x and y)</li>\n</ol>\n\n<p>The code for the sorting algorithm must be the same. It is only the code inside the comparison "black box" which is allowed to change.</p>\n', 'ViewCount': '296', 'Title': 'Is there a "sorting" algorithm which returns a random permutation when using a coin-flip comparator?', 'LastActivityDate': '2013-03-23T11:27:52.923', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '10658', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<sorting><randomized-algorithms><permutations>', 'CreationDate': '2013-03-20T18:14:44.773', 'Id': '10656'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '218', 'Title': 'Is detecting "doubly" arithmetic progressions 3SUM-hard?', 'LastEditDate': '2013-03-21T10:36:09.113', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '7362', 'FavoriteCount': '2', 'Body': '<p>This is inspired by an <a href="http://www.careercup.com/question?id=15877680">interview question</a>.</p>\n\n<p>We are given an array of integers $a_1, \\dots, a_n$ and have to determine if there are distinct $i \\lt j \\lt k$ such that</p>\n\n<ul>\n<li>$a_k - a_j = a_j - a_i$</li>\n<li>$k - j = j - i$</li>\n</ul>\n\n<p>i.e, the sequences $\\{a_i, a_j, a_k\\}$ and $\\{i,j,k\\}$ are both in arithmetic progression.</p>\n\n<p>There is an easy $O(n^2)$ algorithm for this, but finding a sub-quadratic algorithm seems elusive.</p>\n\n<p>Is this a known problem? Can we prove 3SUM-hardness of this? (or maybe provide a sub-quadratic algorithm?)</p>\n\n<p>If you like, you can assume $0 \\lt a_1 \\lt a_2 \\lt ... \\lt a_n$ and that $a_{r+1} - a_{r} \\le K$ for some known constant $K &gt; 2$. (In the interview problem, $K = 9$).</p>\n', 'Tags': '<algorithms><complexity-theory><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-24T03:39:07.830', 'CommentCount': '0', 'AcceptedAnswerId': '10725', 'CreationDate': '2013-03-21T07:56:00.003', 'Id': '10681'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I want to find the n-th prime. Is there an algorithm to directly calculate it or must I do with sieving? I know always calculate the next prime with a sieve principle, but what if I want the n-th prime?</p>\n\n<p>Duplicate:</p>\n\n<p><a href="http://math.stackexchange.com/questions/1257/is-there-a-known-mathematical-equation-to-find-the-nth-prime">http://math.stackexchange.com/questions/1257/is-there-a-known-mathematical-equation-to-find-the-nth-prime</a></p>\n', 'ViewCount': '1212', 'Title': 'What is the time complexity of generating n-th prime number?', 'LastEditorUserId': '41', 'LastActivityDate': '2013-03-22T15:22:39.600', 'LastEditDate': '2013-03-22T05:17:32.637', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2529', 'Tags': '<algorithms><primes>', 'CreationDate': '2013-03-21T14:01:43.727', 'Id': '10683'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need an efficient algorithm that takes input a collection of intervals and outputs the largest subset of non-intersecting intervals. </p>\n\n<p>i.e. Given a set of intervals $I =  \\{I_1, I_2, \\ldots, I_n\\}$ of the real line, we need to output a set of intervals $O = \\{O_1, O_2, \\ldots, O_k\\}$ such that</p>\n\n<ul>\n<li>$O$ is a subset of $I$.</li>\n<li>For any $i \\neq j$, $O_i$ and $O_j$ are non-intersecting.</li>\n<li>$k$ is the maximum possible.</li>\n</ul>\n\n<p>Example: if the intervals are $[1,100], [2,3], [4,5], [6,7], [3,20]$ we should return $\\{[2,3], [4,5], [6,7]\\}$.</p>\n', 'ViewCount': '503', 'Title': 'Algorithm to return largest subset of non-intersecting intervals', 'LastEditorUserId': '7391', 'LastActivityDate': '2013-03-24T21:29:01.710', 'LastEditDate': '2013-03-24T21:29:01.710', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'OwnerDisplayName': 'user2112791', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><integers><intervals><set-cover>', 'CreationDate': '2013-03-23T02:48:15.283', 'FavoriteCount': '0', 'Id': '10713'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Just a quick question,</p>\n\n<p>If i were to alter the general DFS algorithm to do this:</p>\n\n<pre><code>minDFS(Vertex v)\n{\n   if (!v.getVisted())\n   {\n       v.setVisited();\n       Vertex temp = findClosestVertex();\n       graph.addEdge(v, temp);\n       minDFS(temp);\n   }\n}\n</code></pre>\n\n<p>Would I eventually (at the end of DFS) get  minimum spanning tree? I know there are other ways of getting the MST (Kruskal's, Prim's etc..),but I was just wondering if this would work.</p>\n", 'ViewCount': '88', 'Title': 'DFS miniumum spanning tree', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-25T10:04:43.703', 'LastEditDate': '2013-03-25T10:04:43.703', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10721', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7381', 'Tags': '<algorithms><graphs><spanning-trees>', 'CreationDate': '2013-03-23T19:28:16.987', 'Id': '10717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '623', 'Title': 'Destination-based vs source-based routing', 'LastEditDate': '2013-03-23T19:42:08.950', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'user1796218', 'PostTypeId': '1', 'OwnerUserId': '7414', 'FavoriteCount': '0', 'Body': '<p>I understand that destination-based routing builds the "route" from the destination backwards to the source (e.g. if using a spanning tree, then the tree is routed at the destination). With source-based routing the opposite is true: the route is build from the source onwards towards the destination.</p>\n\n<p>However I don\'t understand the practical difference. How does it make a difference if I base my decision on the source or on the destination. Say, a shortest path algorithm such as Dijkstra\'s should give the same result regardless?</p>\n\n<p>Could someone explain?</p>\n', 'Tags': '<algorithms><distributed-systems><shortest-path><routing>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-03-25T16:40:32.177', 'CommentCount': '2', 'AcceptedAnswerId': '10779', 'CreationDate': '2013-03-22T19:34:10.097', 'Id': '10718'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p><strong>Definition:</strong> A preserved invariant of a state machine is a predicate, $P$, on\nstates, such that whenever $P(q)$ is true of a state, $q$, and $q  \\rightarrow r$ for some state, $r$,\nthen $P(r)$ holds.</p>\n\n<p><strong>Definition:</strong> A line graph is a graph whose edges are all on one path.</p>\n\n<p><strong>Definition:</strong> Formally, a state machine is nothing more than a binary relation on a set, except\nthat the elements of the set are called \u201cstates,\u201d the relation is called the transition\nrelation, and an arrow in the graph of the transition relation is called a transition.\nA transition from state $q$ to state $r$ will be written $q \\rightarrow r$.</p>\n\n<p><strong>DAG</strong>: Directed Acylic Graph</p>\n\n<blockquote>\n  <p>The following procedure can be applied to any directed graph, $G$:</p>\n  \n  <ol>\n  <li>Delete an edge that is in a cycle.</li>\n  <li>Delete edge $&lt;u \\rightarrow v&gt;$ if there is a path from vertex $u$ to vertex $v$ that does not\n  include $&lt;u \\rightarrow v&gt;$.</li>\n  <li>Add edge $&lt;u \\rightarrow v&gt;$ if there is no path in either direction between vertex $u$ and\n  vertex $v$.</li>\n  </ol>\n  \n  <p>Repeat these operations until none of them are applicable.</p>\n</blockquote>\n\n<p>This procedure can be modeled as a state machine. The start state is $G$, and the\nstates are all possible digraphs with the same vertices as $G$.</p>\n\n<p><strong>(b)</strong> Prove that if the procedure terminates with a digraph, $H$, then $H$ is a line\ngraph with the same vertices as $G$.</p>\n\n<p>Hint: Show that if $H$ is not a line graph, then some operation must be applicable.</p>\n\n<p><strong>(c)</strong> Prove that being a DAG is a preserved invariant of the procedure.</p>\n\n<p><strong>(d)</strong> Prove that if $G$ is a DAG and the procedure terminates, then the walk relation\nof the final line graph is a topological sort of $G$.</p>\n\n<p>Hint: Verify that the predicate\n$P(u,v)$:: there is a directed path from $u$ to $v$\nis a preserved invariant of the procedure, for any two vertices $u, \\ v$ of a DAG.</p>\n\n<p><strong>(e)</strong> Prove that if $G$ is finite, then the procedure terminates.</p>\n\n<p>Hint: Let $s$ be the number of cycles, $e$ be the number of edges, and $p$ be the number\nof pairs of vertices with a directed path (in either direction) between them. Note\nthat $p \\leq n^2$ where $n$ is the number of vertices of $G$. Find coefficients $a,b,c$ such\nthat as+bp+e+c is nonnegative integer valued and decreases at each transition.</p>\n\n<blockquote>\n  <p><em><strong>My Problems:</em></strong></p>\n  \n  <p>I got stuck with problems $d$ and $e$ but solutions to other problems are welcome too.</p>\n  \n  <p>At problem  $d$, <strong>I could not understand the hint and why it is given, how it helps</strong>. </p>\n</blockquote>\n\n<p>In my way for proving $d$, I am trying to show that given procedure always preserves the order of vertices, which are associated with edges, on the start graph $G$. So a line graph is automatically a topological sort since the "precedence order" of the vertices are preserved. </p>\n\n<blockquote>\n  <p>But procedure number $3$ is problematic, <strong>how to show it preserves precedence ?</strong></p>\n</blockquote>\n', 'ViewCount': '486', 'Title': 'A procedure for Topological sort, proof for its correctness', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-18T19:45:06.263', 'LastEditDate': '2013-09-20T15:21:45.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7337', 'Tags': '<algorithms><algorithm-analysis><sorting><correctness-proof>', 'CreationDate': '2013-03-23T20:42:41.853', 'Id': '10720'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There are 3 processors: $P_1$, $P_2$, $P_3$. $P_1$ can execute only one job while the other two can execute any number of jobs in parallel.</p>\n\n<p>Each job consists of 3 parts, and each part is processed by their respective processor (eg. part 1 by $P_1$). Furthermore part 2 can be executed only after $P_1$ finishes executing part 1, and part 3 can be executed only after $P_2$ finishes executing part 2.</p>\n\n<p>Suppose there are $n$ jobs, and again each job has 3 parts. The processor $P_k$ takes time $T_{i\\,k}$ to complete part $k$ of job $i$.\nCan anyone suggest an algorithm that takes set of $n$ jobs and determines the the jobs should be processed  so that the total time is minimized?</p>\n\n<p>This is a homework problem and I don't know where ti start</p>\n", 'ViewCount': '102', 'Title': 'Minimize the total execution time of jobs executed by 3 processors in sequence', 'LastEditorUserId': '5020', 'LastActivityDate': '2013-03-26T02:22:27.973', 'LastEditDate': '2013-03-24T19:24:09.633', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10793', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7391', 'Tags': '<algorithms><scheduling>', 'CreationDate': '2013-03-24T05:04:14.490', 'Id': '10736'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am facing problem in understanding the KMP algorithm as given in the book  "Introduction to Algorithm" by Cormen et. al. (Chapter 32): </p>\n\n<p><img src="http://i.stack.imgur.com/5LPPL.jpg" alt="enter image description here"></p>\n\n<p><img src="http://i.stack.imgur.com/MX5hv.jpg" alt="enter image description here"></p>\n\n<p>With respect to the above algorithm, I have following question:</p>\n\n<ol>\n<li><p>In both the algorithms, where exactly (which line number?) while loops, if and for loop ends? It is difficult to visualize the functioning of the algorithm without knowing the \'end\' for each loop and conditional statements.</p></li>\n<li><p>On line number 6 in KMP-MATCHER() and COMPUTE-PREFIX-FUNCTION(), what is the need of checking q>0 and k>0, when we know q and k will always be 0 or positive. Or do the authors want to test q!=0, k!=0? </p></li>\n<li><p>If q=0 and i=1 in KMP-MATCHER(), from 6 to 10 which line will be executed (If I get answer to 1- above then I can find this)       </p></li>\n</ol>\n', 'ViewCount': '164', 'Title': 'Unable to understand control flow in KMP algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-19T19:35:20.363', 'LastEditDate': '2013-04-24T21:15:41.077', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><terminology><strings>', 'CreationDate': '2013-03-24T11:49:45.607', 'Id': '10738'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I have a graph $G$ with $M(G)$ the (unknown) set of perfect matchings of $G$. Suppose this set is non-empty, then how difficult is it to sample uniformly at random from $M(G)$? What if I am okay with a distribution that is close to uniform, but not quite uniform, then is there an efficient algorithm?</p>\n', 'ViewCount': '113', 'Title': 'Sampling perfect matching uniformly at random', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-26T10:12:47.543', 'LastEditDate': '2013-03-26T10:12:47.543', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '10758', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><complexity-theory><matching><sampling>', 'CreationDate': '2013-03-24T21:22:33.253', 'Id': '10756'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can anyone suggest an algorithm faster than $\\Theta(n^{2})$ for computing the following function:</p>\n\n<p>$$||n||:=\\frac{1}{\\max\\{k \\in \\mathbb{N}: 1|n, 2|n,\\ldots,k|n\\}}$$</p>\n', 'ViewCount': '49', 'Title': 'Faster Algorithm for Computing Norm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-25T13:33:31.867', 'LastEditDate': '2013-03-25T10:34:36.653', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '10772', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4829', 'Tags': '<algorithms><discrete-mathematics><number-theory>', 'CreationDate': '2013-03-25T05:15:10.013', 'Id': '10762'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The worst case running time of insertion sort is $\\Theta(n^2)$, we don\u2019t write it as $O(n^2)$.</p>\n\n<p>$O$-notation is used to give upper bound on function. If we use it to bound a worst case running time of insertion sort, it implies that $O(n^2)$ is upper bound of algorithm no matter what type of input is, means it doesn\u2019t matter whether input is sorted, unsorted, reverse sorted, have same values, etc the upper bound will be same $O(n^2)$. But this is not the case of insertion sort. Insertion sort running time depends on type of input used. So when the input is already sorted, it runs in linear time and doesn\u2019t take more that $O(n)$ time.</p>\n\n<p>Therefore to write insertion sort running time as $O(n^2)$ is technically not good.</p>\n\n<p><strong>We use $\\Theta$-notation to write worst case running time of insertion sort. But I\u2019m not able to relate properties of $\\Theta$-notation with insertion sort, why $\\Theta$-notation is suitable to insertion sort.If $f(n)$ belong to $\\Theta(g(n))$ we write it as $f(n)= \\Theta(g(n))$, then $f(n)$ must satisfies the properties. And properties state that there exits constants $c_1$, $c_2$ and $n_0$ such that $0$$\\leq$$c_1\\cdot g(n)$$\\leq$$f(n)$$\\leq$$c_2\\cdot g(n)$ For all $n&gt;n_0$. How does the insertion sort function lies between the $c_1\\cdot n^2$ and $c_2\\cdot n^2$ for all $n&gt;n_0$.</strong></p>\n\n<p>Running time of insertion sort as $\\Theta(n^2)$ implies that it has upper bound $O(n^2)$ and lower bound $\\Omega(n^2)$. I\u2019m confused as to whether the lower bound on insertion-sort is $\\Omega(n^2)$ or $\\Omega(n)$.</p>\n', 'ViewCount': '477', 'Title': 'Why is $\\Theta$ notation suitable to insertion sort to describe its worst case running time?', 'LastEditorUserId': '7384', 'LastActivityDate': '2013-04-29T21:19:54.897', 'LastEditDate': '2013-04-29T16:22:00.450', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'siddstuff', 'PostTypeId': '1', 'OwnerUserId': '7384', 'Tags': '<algorithms><time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-03-25T06:59:09.077', 'Id': '10763'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '106', 'Title': 'How to find specificity of a regex match?', 'LastEditDate': '2013-03-26T11:20:44.637', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7419', 'FavoriteCount': '1', 'Body': "<p>I'm thinking about a routing system. Imagine I have the two following regexes</p>\n\n<ul>\n<li>pathpart1/pathpart2 => specific match that routes to controller1</li>\n<li>.* => catch-all that routes to controller2</li>\n</ul>\n\n<p>And I let them match on a URL, e.g. 'pathpart1/pathpart2'.</p>\n\n<p>They both match, but I would want to give prevalence to the most specific regex, i.e. the regex where <strong>the cardinality of all possible matches of that regex</strong> is the lowest.</p>\n\n<blockquote>\n  <p>Is there a good way to calculate that the first regex has a low cardinality on its match-set (so I want to to go with that match) and the second has a very high cardinality on its match-set (i.e. it is completely not specific, so a match is basically a catch all last resort)...?</p>\n</blockquote>\n\n<p>I do not know upfront which routes are registered with the router, so I can't loop over them in order of cardinality by hand (i.e. low cardinality first, and the catch-all last if all others don't match).</p>\n", 'Tags': '<algorithms><regular-expressions><strings>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-27T10:20:57.763', 'CommentCount': '0', 'AcceptedAnswerId': '10807', 'CreationDate': '2013-03-25T21:22:07.827', 'Id': '10786'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need help with the following problem:</p>\n\n<blockquote>\n  <p><strong>Input:</strong>  An undirected, unweighted graph $G = (V,E)$ and a set of vertices $F \\subseteq V$.</p>\n  \n  <p><strong>Question:</strong>\n  Find a vertex $v$ of $V$ such that the distance from each vertex of $F$ to $v$ is the same and all the distances are minimized?  Return <code>None</code> if there is no such $v$.</p>\n</blockquote>\n\n<p>The runtime should be $O(|V| + |E|)$.</p>\n\n<p>My thoughts were to do a breadth-first search for each vertex in $F$, so for each vertex in $F$, you store all vertices with their distances, then find the intersection of all these.</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '234', 'Title': 'Find a vertex that is equidistant to a set of vertices?', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-03-31T10:13:00.143', 'LastEditDate': '2013-03-31T10:13:00.143', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><graph-theory><search-algorithms><shortest-path>', 'CreationDate': '2013-03-26T05:51:11.470', 'Id': '10794'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What does it mean when we say that a run of Prim's algorithm is <em>trivial</em>? What are example graphs for either case, that is with and without trivial runs?</p>\n", 'ViewCount': '134', 'Title': "Non-trivial runs of Prim's algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T02:02:34.337', 'LastEditDate': '2013-03-26T13:42:55.697', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '10809', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7269', 'Tags': '<algorithms><terminology><graphs>', 'CreationDate': '2013-03-26T11:52:10.267', 'Id': '10804'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://en.wikipedia.org/wiki/3-partition_problem" rel="nofollow">3-Partition</a> problem is $\\mathsf{NP}$-Complete in a strong sense meaning there is no pseudo-polynomial time algorithm for it unless $\\mathsf{P}=\\mathsf{NP}$. I\'m looking for the fastest known exact algorithm that solves 3-Partition. Is there a fast (e.g subexponential) algorithm for 3-Partition? Is it possible to solve it faster than using SAT solvers?</p>\n', 'ViewCount': '284', 'LastEditorDisplayName': 'user742', 'Title': 'Fastest known algorithm for 3-Partition problem', 'LastActivityDate': '2013-03-27T15:53:45.660', 'LastEditDate': '2013-03-27T15:53:45.660', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '4', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'Tags': '<algorithms><np-complete><partition-problem>', 'CreationDate': '2013-03-26T12:06:40.963', 'Id': '10805'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the original paper of A* algorithm, <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=a%20formal%20basis%20for%20the%20heuristic%20determination%20of%20minimum%20cost%20paths&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDYQFjAA&amp;url=http://fai.cs.uni-saarland.de/teaching/winter12-13/heuristic-search-material/Astar.pdf&amp;ei=m-NRUeOUMYa0kAXpgIHIDw&amp;usg=AFQjCNFp9yfCwE0_B_epYI4kPmEmZjOGww&amp;bvm=bv.44342787,d.dGI" rel="nofollow"><em>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</em></a>, the author proved the optimality of A* in <em>Theorem 2</em>, page 105.</p>\n\n<p>However, I cannot understand the proof. The assumption is that we have a node in $G_s$ which is not expanded by algorithm A, but in the proof we change the graph to $G_{n,\\theta}$, isn\'t it a problem?</p>\n', 'ViewCount': '96', 'Title': 'While proving optimality of the A* algorithm, why can we change graphs?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-27T13:07:35.247', 'LastEditDate': '2013-03-27T12:03:09.513', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7432', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><correctness-proof>', 'CreationDate': '2013-03-27T00:57:43.747', 'Id': '10817'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an undirected tree whose vertices I want to label. The leaf nodes should be labeled one. Then, assume the leaves were removed. In the tree that remains, the leaves should be labeled two. This process continues in the obvious way until all vertices have a label. The reason I do this is I want to store the vertices in a queue, and go through them "leaves first". Is there an easy way to do this $O(n+m)$ time?</p>\n\n<p>I can solve the problem by doing a BFS on every step. But in the worst case, on every step I go through every vertex, remove exactly two leaves and enqueue them. I believe this takes quadratic time.</p>\n\n<p>Another idea was to first find all the leaves, and then do a BFS from every leaf. This doesn\'t give me the desired solution. For example, consider a kind of "crown graph" as in the figure below. The desired solution is shown, but launching a BFS from each leaf would result in only two labels used.</p>\n\n<p><img src="http://i.stack.imgur.com/nNtzL.png" alt="enter image description here"></p>\n\n<p>Ideally, the linear time algorithm would also be easy to explain and implement.</p>\n', 'ViewCount': '232', 'Title': 'Linear time labeling algorithm for a tree?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-10T19:00:50.640', 'LastEditDate': '2013-03-27T21:33:17.693', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10821', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '7434', 'Tags': '<algorithms><trees>', 'CreationDate': '2013-03-27T02:54:50.987', 'Id': '10819'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we are given $n$ variables $X_i, i=1,\\dots,n$, each taking values from $\\{0,1\\}$, and a constant integer $k$ with $ 0\\leq k \\leq n$.</p>\n\n<p>What are some efficient ways to enumerate all possible combinations of values of $X_i$'s subject to the constraint $\\sum_{i=1}^n X_i = k$?</p>\n\n<p>A naive way is to first enumerate one by one all possible combinations of values of $X_i$'s without the constraint  $\\sum_{i=1}^n X_i = k$, and for each combination, check if it satisfies  $\\sum_{i=1}^n X_i = k$ (if it does, keep it; if it doesn't, discard it).  </p>\n\n<p>That naive way may be inefficient.\nFor example, when $k=1$, a more efficient way will be for each $i$, letting $X_i=1$ and $X_j =0, \\forall j \\neq i$.</p>\n\n<p>So I wonder for general $k$, what are some efficient ways to do the above task? </p>\n", 'ViewCount': '190', 'Title': 'How to enumerate all combinations of $n$ binary variables s.t. their sum is $k$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T11:49:26.130', 'LastEditDate': '2013-03-28T10:42:56.347', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><combinatorics><enumeration>', 'CreationDate': '2013-03-28T08:21:59.237', 'Id': '10861'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given $n$ arrays of size $k$ each, we want to show that at least $\\Omega(nk \\log k)$ comparisons are needed to sort all arrays (indepentent of each other). </p>\n\n<p>My proof is a simple modification of the decision tree argument used to obtain the lower bound for comparison-based sorting of one array. More specifically, I argue that there are in total $k!^n$ possible permutations for the entries in all given arrays, and that a binary tree with that number of leaves is of height $h \\in \\Omega(nk \\log k)$. Is that argument correct? </p>\n\n<p>Furthermore, I was told that merely observing that one needs $\\Omega(k \\log k)$ comparisons for each of the arrays and we need to sort $n$ times in total (for $n$ arrays) is <em>not</em> a sufficient argument. Why is that? My answer would be that this is just <em>one</em> possible approach to this problem, and not a general argument excluding each and every other potential comparison-based algorithm for solving the given task with less than $\\Omega(nk \\log k)$ comparisons. \nHowever, this is not particularly concise and I would consider a rather technical argument (which I don't see) as more appropriate. What would that be?</p>\n", 'ViewCount': '132', 'Title': 'Lower bound for sorting n arrays of size k each', 'LastEditorUserId': '7486', 'LastActivityDate': '2013-03-29T13:24:00.140', 'LastEditDate': '2013-03-29T13:24:00.140', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10893', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7486', 'Tags': '<algorithms><sorting><arrays><lower-bounds><check-my-proof>', 'CreationDate': '2013-03-29T12:07:19.733', 'Id': '10890'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have $n\\times k$ matrix with $k&lt;n$ and I would like to find all its $n\\choose k$ submatrices which are $k\\times k$ matrices that are the concatenations of all possible $k$ rows. Actually I tried to do it with Matlab but it takes too long time specially when $n&gt;20$ and I couldn\'t find a way how to generate in parallel the $n\\choose k$ indices. </p>\n\n<p>I found online a paper titled <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1240626" rel="nofollow">A Parallel Algorithm for Enumerating Combinations</a> but they didn\'t provide their code in that paper.</p>\n', 'ViewCount': '116', 'Title': 'How to enumerate combinations in parallel', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T07:52:48.890', 'LastEditDate': '2013-04-02T07:52:48.890', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10902', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7487', 'Tags': '<algorithms><combinatorics><parallel-computing><matrices>', 'CreationDate': '2013-03-29T17:29:21.997', 'Id': '10899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here is the problem: we are given vectors $v_1, \\ldots, v_k$ lying in $\\mathbb{R}^n$ which are orthogonal. We assume that the entries of $v_i$ are rational, with numerator and denominator taking $K$ bits to describe. We would like to find vectors $v_{k+1}, \\ldots, v_n$ such that $v_1, \\ldots, v_n$ is an orthogonal basis for $\\mathbb{R}^n$. </p>\n\n<p>I would like to say this can be done in polynomial time in $n$ and $K$. However, I'm not sure this is the case. <strong>My question</strong> is to provide a proof that this can indeed be done in polynomial time. </p>\n\n<p>Here is where I am stuck.  Gram-Schmidt suggests to use the following iterative process. Suppose we currently have the collection $v_1, \\ldots, v_l$. Take the basis vectors $e_1, \\ldots, e_n$, go through them through them one by one, and if some $e_i$ is not in the span of the $v_1, \\ldots, v_l$, then set $v_{l+1} = P_{{\\rm span}(v_1, \\ldots, v_l)^\\perp} e_i$ (here $P$ is the projection operator). Repeat. </p>\n\n<p>This works in the sense that the number of additions and multiplications is polynomial in $n$. But what happens to the bit-sizes of the entries? The issue is that the projection of $e_i$ onto, say, $v_1$ may have denominators which need $2K$ bits or more to describe - because $P_{v_1}(e_i)$ is $v_1$ times its $i$'th entry, divided by $||v_1||$. Just $v_1$ times its $i$'th entry may already need $2K$ bits to describe. </p>\n\n<p>By a similar argument, it seems that each time I do this, the number of bits doubles. By the end, I may need $2^{\\Omega(n)}$ bits to describe the entries of the vector. How do I prove this does not happen? Or perhaps should I be doing things differently to avoid this?</p>\n", 'ViewCount': '90', 'Title': 'Can you complete a basis in polynomial time?', 'LastActivityDate': '2013-03-30T20:26:18.397', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10921', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7498', 'Tags': '<algorithms><linear-algebra>', 'CreationDate': '2013-03-30T04:09:45.353', 'Id': '10906'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '521', 'Title': 'Understanding DPLL algorithm', 'LastEditDate': '2013-03-31T14:44:46.170', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7509', 'FavoriteCount': '1', 'Body': u'<p>I\'m trying to understand <a href="http://en.wikipedia.org/wiki/DPLL_algorithm" rel="nofollow">DPLL algorithm</a> for solving SAT problem. And here it is:</p>\n\n<pre><code>Algorithm DPLL\n  Input: A set of clauses \u03a6.\n  Output: A Truth Value.\nfunction DPLL(\u03a6)\n   if \u03a6 is a consistent set of literals\n       then return true;\n   if \u03a6 contains an empty clause\n       then return false;\n   for every unit clause l in \u03a6\n      \u03a6 \u2190 unit-propagate(l, \u03a6);\n   for every literal l that occurs pure in \u03a6\n      \u03a6 \u2190 pure-literal-assign(l, \u03a6);\n   l \u2190 choose-literal(\u03a6);\n   return DPLL(\u03a6 \u2227 l) or DPLL(\u03a6 \u2227 not(l));\n</code></pre>\n\n<p>At first, I don\'t clearly understand how <code>unit-propagate(l, \u03a6)</code>, <code>pure-literal-assign(l, \u03a6)</code> and <code>choose-literal(\u03a6)</code> work. I\'ll try to guess on particular examples. Correct me please if I do something wrong. </p>\n\n<ul>\n<li><p>For the first one </p>\n\n<p><code>unit-propagate(a, (0 v -a) \u2227 (a v b) \u2227 (b v d) \u2227 (f v g) v ...)</code> </p>\n\n<p>we will have </p>\n\n<p><code>((0 v -0) \u2227 (0 or 1) \u2227 (1 v d) \u2227 (f v g) \u2227 ... = (f v g) v ...</code>,</p>\n\n<p>having <code>a = 0</code>, <code>b = 1</code>.</p></li>\n<li><p>For second procedure</p>\n\n<p><code>pure-literal-assign(a, (a v b v c) \u2227 (d v -b v a) \u2227 (-d v b))</code></p>\n\n<p>result is </p>\n\n<p><code>(b v c) \u2227 (d v -b) \u2227 (-d v b)</code>,</p>\n\n<p>assigning <code>a = 1</code>.</p></li>\n<li><p>And finally <code>choose-literal(\u03a6)</code> just returns some random (in common case) unassigned literal for further computations.</p></li>\n</ul>\n\n<p>Now, I don\'t understand why algorithm has such strange conditions for finishing? Why does it work?</p>\n\n<p>Thanks!</p>\n', 'Tags': '<algorithms><logic><satisfiability><sat-solvers>', 'LastEditorUserId': '7509', 'LastActivityDate': '2013-03-31T17:36:48.023', 'CommentCount': '0', 'AcceptedAnswerId': '10933', 'CreationDate': '2013-03-31T08:26:51.050', 'Id': '10932'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Counting all possible paths, or all possible paths with a given length, between a couple of nodes in a directed or undirected graph is a classical problem. Attention should be given to what <em>all</em> means, due to the possibles cycles.</p>\n\n<p>This question is slightly different, or at least I think.</p>\n\n<p><strong>INPUT:</strong>  Be <strong>G</strong> a directed graph. <strong>G</strong> can have cycles and also selfconnected nodes. Let <strong>A(G)</strong> be the adjacency matrix of <strong>G</strong> (with a 1 in <strong>G</strong><sub>i,j</sub> if there's a link going from i to j and a 0 otherwise). Define <strong>T</strong> and <strong>B</strong> two subset of nodes of <strong>G</strong>, possibly with void intersection.</p>\n\n<p><strong>OUTCOME:</strong> A list of all paths of length <em>at most</em> k going from one node in <strong>T</strong> to one node in <strong>B</strong>. Paths can contain multiple time the same edges, as long as they go from the source node to the target node in strictly less than k+1 steps.</p>\n\n<p><strong>QUESTION:</strong> I would like to know which algorithm perform best in this task. I'm trying to develop a possible answer based on the fact that the n-th power of the adjacency matrix, if computed symbolically (with a different variable for each entry instead of a 1), keep traks of all this paths (and it reduces to the counting of paths if computed numerically with 1 in the entries). But I really don't know if this is the fastest way of doing the task (probably not).</p>\n\n<p><strong>CAVEAT:</strong> I'm not asking for the counting problem, nor for the shortest paths, the length of a path is defined as the number of edges used (counting the repetition). I'm using R, but if you prefer think about it in any other language! I'm really sorry if the question was already posed and solved. Thank you for the kind help!</p>\n\n<p><strong>additional info:</strong> I tried a matrix power series approach (A^3 gives all the 3 long path, ...) and dfs / bfs. I think the latter two are far from optimality as they don' take into account that I'm working on sets of sources and targets, and hence do a lot of redundant work...</p>\n", 'ViewCount': '793', 'Title': 'All paths of less than a given length in a directed graph between couple of nodes', 'LastActivityDate': '2013-04-01T16:54:58.877', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '10954', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7521', 'Tags': '<algorithms><graphs><search-algorithms><shortest-path>', 'CreationDate': '2013-03-31T21:50:11.637', 'FavoriteCount': '1', 'Id': '10949'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '409', 'Title': 'Sort array of 5 integers with a max of 7 compares', 'LastEditDate': '2013-04-02T08:10:20.363', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '6728', 'FavoriteCount': '2', 'Body': "<p>How can I sort a list of 5 integers such that in the worst case it takes 7 compares?  I don't care about how many other operations are performed.  I don't know anything particular about the integers.</p>\n\n<p>I've tried a few different divide and conquer approaches which get me down to 8 compares, such as following a mergesort approach, or combining mergesort with using binary search to find the insertion position, but every time I end up with 8 compares worst case.</p>\n\n<p>Right now I'm just looking for a hint, not a solution.</p>\n", 'Tags': '<algorithms><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-29T17:52:49.513', 'CommentCount': '9', 'AcceptedAnswerId': '10968', 'CreationDate': '2013-04-01T18:18:44.710', 'Id': '10960'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem similar to the interval scheduling algorithm. The differences are:</p>\n\n<ul>\n<li>The jobs have the same length.</li>\n<li>There are several categories of jobs and only one job from each category can be chosen.</li>\n<li>There can be no overlap in time between different categories.</li>\n</ul>\n\n<p>I have illustrated the problem with a picture:</p>\n\n<p>Each line is a job. Different color means that they belong to a different category. So I need to choose one of each colored line to try to cover as large an area as possible without any lines overlapping. Remember that lines with the same color should not be chosen twice. And yes, for the illustration the problem is trivial.</p>\n\n<p><img src="http://i.stack.imgur.com/fLk1Z.png" alt="enter image description here"></p>\n\n<p>Does my problem have a name? Does it require a DP algorithm?</p>\n', 'ViewCount': '98', 'Title': 'Variation of interval scheduling algorithm with several job categories, only one from each can be used', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T15:13:15.947', 'LastEditDate': '2013-04-02T15:13:15.947', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><optimization><scheduling>', 'CreationDate': '2013-04-02T13:16:11.903', 'FavoriteCount': '2', 'Id': '10970'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was wondering since randomness is embedded in genetic algorithms at almost every level, is there a really fine line between genetic algorithms and pure random search?</p>\n\n<p>Ever since I finished my implementation of a GA , since randomness is present in the mutation function,the initialization part (as well as the reinitialization part) and crossbreeding part as well... other than a encoder which tries to sense of the chromsomes (encoder tailored to make sense of the chromosome in context of the problem) and a fitness function , it feels like genetic algorithms are just random search functions in disguise .</p>\n\n<p>So my question is : are GA implementations just plain old random searches with a shot of memory to make it look like there is some sort of meaningful feedback? </p>\n', 'ViewCount': '108', 'ClosedDate': '2013-04-02T22:15:50.887', 'Title': 'Are genetic algorithms special instances of random search done in an unexpectedly short run-time?', 'LastEditorUserId': '7545', 'LastActivityDate': '2013-04-02T21:07:18.593', 'LastEditDate': '2013-04-02T20:35:30.713', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7545', 'Tags': '<search-algorithms><efficiency><randomness><evolutionary-computing><genetic-algorithms>', 'CreationDate': '2013-04-02T15:57:17.913', 'Id': '10975'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A graph $G$ is chordal if it doesn't have induced cycles of length $4$ or more. A clique tree $T$ of $G$ is a tree in which the vertices of the tree are the maximal cliques of $G$. An edge in $T$ corresponds to a minimal separator. The number of distinct clique trees can be exponential in the number of vertices in a chordal graph. </p>\n\n<p>The <em>reduced clique graph</em> $C_r(G)$ is the union of all clique trees of $G$. That is, it has all the same vertices, and all possible edges. What is the complexity of computing $C_r(G)$ for a given $G$?</p>\n\n<p>I think I once saw a presentation claiming $C_r(G)$ can be computed in $O(m+n)$ time without proof. This would mean it is as easy as computing a clique tree of $G$. Is there a reference that confirms this, or gives a slower algorithm for computing it?</p>\n", 'ViewCount': '118', 'Title': 'Given a chordal graph $G$, what is the complexity of computing the reduced clique graph $C_r(G)$?', 'LastActivityDate': '2013-07-20T21:55:57.460', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13369', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><graph-theory><reference-request>', 'CreationDate': '2013-04-02T21:23:52.710', 'FavoriteCount': '1', 'Id': '10979'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1020', 'Title': 'Colour a binary tree to be a red-black tree', 'LastEditDate': '2013-04-03T16:56:13.100', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '2', 'Body': '<p>A common interview question is to give an algorithm to determine if a given binary tree is height balanced (AVL tree definition).</p>\n\n<p>I was wondering if we can do something similar with Red-Black trees.</p>\n\n<blockquote>\n  <p>Given an arbitrary uncoloured binary tree (with NULL nodes), is there a "fast" algorithm which can determine if we can colour (and find a colouring) the\n  nodes Red/Black so that they satisfy all the properties of a Red-Black\n  tree (definition as in this <a href="http://cs.stackexchange.com/questions/342/not-all-red-black-trees-are-balanced">question</a>)?</p>\n</blockquote>\n\n<p>An initial thought was that we can just remove the NULL nodes and try to recursively verify if the resulting tree can be a red-black tree, but that didn\'t seem to go anywhere.</p>\n\n<p>I did (a brief) web search for papers, but could not seem to find any which seem to deal with this problem.</p>\n\n<p>It is possible that I am missing something simple.</p>\n', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'LastEditorUserId': '139', 'LastActivityDate': '2013-04-20T17:57:19.710', 'CommentCount': '1', 'AcceptedAnswerId': '10999', 'CreationDate': '2013-04-03T08:02:32.960', 'Id': '10990'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was learning about algorithms with polynomial time complexity. I found the following algorithms interesting.</p>\n\n<ul>\n<li><p>Linear Search - with time complexity $O(n)$</p></li>\n<li><p>Matrix Addition - with time complexity $O(n^2)$</p></li>\n<li><p>Matrix Multiplication - with time complexity  $O(n^3)$</p></li>\n</ul>\n\n<p>Is there any algorithm with a higher complexity like $n^4$, $n^5$ etc? I would like to know about practical algorithms with polynomial time complexity only.</p>\n\n<p>(I am familiar with algorithms having exponential complexity and class NP algorithms. My doubt is not about them.)</p>\n', 'ViewCount': '568', 'Title': 'Algorithms with polynomial time complexity of higher order', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-03T21:05:45.970', 'LastEditDate': '2013-04-03T21:05:45.970', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '11002', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7363', 'Tags': '<algorithms><complexity-theory><asymptotics>', 'CreationDate': '2013-04-03T18:10:35.997', 'Id': '10997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When clustering a set of data points, what exactly are the differences between <a href="http://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_c-means_clustering" rel="nofollow">Fuzzy C-Means</a> (aka Soft K-Means) and <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="nofollow">Expectation Maximization</a>?</p>\n\n<p>In slide 30 and 32 of <a href="http://eniac.cs.qc.cuny.edu/andrew/gcml-11/lecture10c.pptx" rel="nofollow">this lecture</a> I found, it says that Soft K-Means is a special case of EM in Soft K-Means only the means are re-estimated and not the covariance matrix, why\'s that and what are the advantages / disadvantages? How does covariance matrix affect the outcomes of EM?</p>\n\n<p>Another question about these two algorithms: When they converge, all the data points will be hard-assigned to a particular cluster if the probability of it being in the said cluster is highest, right?</p>\n', 'ViewCount': '444', 'Title': 'Differences between Fuzzy C-Means and EM', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-29T14:41:12.010', 'LastEditDate': '2013-08-29T14:41:12.010', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14017', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7329', 'Tags': '<algorithms><terminology><algorithm-analysis><machine-learning><statistics>', 'CreationDate': '2013-04-04T17:14:03.000', 'Id': '11022'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I mean Dijkstra's algorithm for the shortest path.</p>\n\n<p>In all descriptions that I saw (including wikipedia),\non every step,\nit always selects the nearest neighbor based on examining their weights.</p>\n\n<p>Imagine that we have following paths from source A to destination B\n(I will list weights of different paths, not full graph - for brevity):</p>\n\n<h1>1: $$A {19\\atop\\longrightarrow} A_1 {2\\atop \\longrightarrow} A_2{2\\atop \\longrightarrow} A_3{10\\atop \\longrightarrow} B$$</h1>\n\n<h1>2: $$A {5\\atop\\longrightarrow} A_4 {10\\atop \\longrightarrow} A_5{10\\atop \\longrightarrow} A_6{5\\atop \\longrightarrow} B$$</h1>\n\n<h1>3: $$A {2\\atop\\longrightarrow} A_7 {15\\atop \\longrightarrow} A_8{15\\atop \\longrightarrow} A_9{10\\atop \\longrightarrow} A_{10} {10\\atop \\longrightarrow} B$$</h1>\n\n<p>If Dijkstra always select the neighbor with smallest weight,\nit will always go for #3 - although it is the heaviest path!</p>\n\n<p>Where am I wrong?\nDoes anybody has a 'working' pseudo-code for Dijkstra algorithm?</p>\n", 'ViewCount': '117', 'Title': 'WP pseudocode for Dijkstra does not work', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-05T14:30:34.853', 'LastEditDate': '2013-04-05T13:56:24.997', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '7598', 'Tags': '<algorithms><shortest-path>', 'CreationDate': '2013-04-05T12:58:27.407', 'Id': '11055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would you algorithmically detect for any given photo whether the sun was shining when the picture was taken?</p>\n\n<p><strong>Examples</strong></p>\n\n<p>A sample from <a href="http://www.saentisbahn.ch/wetter-am-saentis.html" rel="nofollow">this webcam</a> at a mountain top: </p>\n\n<p><img src="http://frightanic.com/misc/saentis-20130406-110012-links.jpg" alt="sunshine example"></p>\n\n<p>Clearly the sun is shining.</p>\n\n<p>In this other sample it\'s far less obvious:</p>\n\n<p><img src="http://frightanic.com/misc/bettmeralp-20130407-135203-cam7.jpg" alt="cloudy example"></p>\n\n<p>One could probably detect fairly easy whether it\'s foggy by trying to identify the tiny church spire on the chapel in the center. However, knowing very little about image processing I\'d be surprised if there was a (combination of) algorithm that could reliably tell if there\'s sunshine or not. </p>\n', 'ViewCount': '137', 'Title': 'How to detect sunshine on a photo', 'LastEditorUserId': '7599', 'LastActivityDate': '2013-04-07T12:23:05.503', 'LastEditDate': '2013-04-07T12:23:05.503', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7599', 'Tags': '<algorithms><image-processing>', 'CreationDate': '2013-04-05T14:58:06.533', 'FavoriteCount': '1', 'Id': '11060'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am running some experiments with a maze, and trying different variations of A*. Based on my experiments, I have been able to form some opinion (that at least in those cases, graph checking is better than IDA). </p>\n\n<p>I am looking for online articles that have done similar experiments, comparing variations of A* with respect to expanded nodes, but have not come across anything concrete.</p>\n', 'ViewCount': '81', 'Title': 'Comparing variations of A*', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:52:54.573', 'LastEditDate': '2013-04-08T14:52:54.573', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '11136', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6966', 'Tags': '<algorithms><reference-request><artificial-intelligence><search-algorithms><empirical-research>', 'CreationDate': '2013-04-05T22:21:35.823', 'Id': '11067'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The usual statement of the <a href="http://en.wikipedia.org/wiki/Fair_division" rel="nofollow">fair cake-cutting problem</a> assumes that all $n$ players get their share at the same time. However, in many cases the players arrive incrementally. For example, we may divide a cake over $n$ players, but then a new player arrives and wants a share.</p>\n\n<p>Usually, fair cake division requires a lot of effort (for example, requires the players to answer many queries), especially when the number of players is large.</p>\n\n<p>Is it possible to use the existing division of the cake over $n$ players, in order to create a new division of the cake to $n+1$ players, with minimal additional effort (i.e. substantially less effort than re-distributing the cake from scratch)?</p>\n', 'ViewCount': '204', 'Title': 'Fair cake-cutting when players join late', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-01-19T02:03:22.010', 'LastEditDate': '2013-04-12T11:28:27.013', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '11204', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><game-theory><online-algorithms>', 'CreationDate': '2013-04-06T18:40:39.860', 'Id': '11077'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '210', 'Title': 'Proving correctness of the algorithm for convex polygon minimum cost triangulation', 'LastEditDate': '2013-04-07T12:38:35.643', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7336', 'FavoriteCount': '1', 'Body': '<p>I have read many solutions for the minimum cost of triangulation problem and intuitively get the idea , however I am struggling to figure out how to prove it formally. I kind of feel that it has to be proven by induction but I struggle at choosing the right quantity to look at and also at the inductive step portion of the proof.</p>\n\n<p>For example, can you provide a formal proof for the algorithm described <a href="http://users.eecs.northwestern.edu/~dda902/336/hw6-sol.pdf" rel="nofollow">here</a> (page 5 problem 6.12).</p>\n', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming><recursion><correctness-proof>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:53:48.043', 'CommentCount': '2', 'AcceptedAnswerId': '11098', 'CreationDate': '2013-04-07T01:15:34.200', 'Id': '11085'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Input:</p>\n\n<ul>\n<li>A list $A=a_1,\\ldots,a_n$, with $\\oplus$ a associative operation on $M$, and $A\\subset M$. </li>\n<li>pairs $(s_i,t_i)$ for all $1\\leq i\\leq m$.</li>\n</ul>\n\n<p>Output: \nThe list $b_1,\\ldots,b_m$, where $b_i = \\bigoplus_{j=s_i}^{t_i} a_j$.</p>\n\n<p>We want an efficient algorithm to compute the output, such that it uses minimum number of $\\oplus$ operations.</p>\n\n<p>For example, if we have $m=2$ and want to find the sum on interval $(1,n-1)$ and $(2,n)$, then the best way is to first compute $t = a_2\\oplus \\ldots \\oplus a_{n-1}$, then compute $b_1 = a_1\\oplus t$ and $b_2 = t \\oplus a_n$, using $n-1$ operations.</p>\n\n<p>Note it is possible that we have to use $\\omega(n)$ operations, consider we have the input that contain all pairs $(i,j)$, where $1\\leq i\\leq j\\leq n$. This would require at least ${n \\choose 2}$ operations, because there could be ${n \\choose 2}$ different values.</p>\n\n<p>Assume all elements in $M$ take $O(1)$ space. We also want to use as little additional memory as possible. </p>\n', 'ViewCount': '156', 'Title': 'Find interval sums with minimum number of operation', 'LastEditorUserId': '139', 'LastActivityDate': '2013-04-07T21:52:49.757', 'LastEditDate': '2013-04-07T21:52:49.757', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms>', 'CreationDate': '2013-04-07T06:14:35.300', 'Id': '11091'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My task is to compare templates of two websites. I am ready with my algorithm. But it takes too much time to give a final answer. \nHere, "template" means the way any page presents its contents.</p>\n\n<p><strong>Example:</strong></p>\n\n<p>Any shopping website have page of any Shoes, that contains,</p>\n\n<pre><code>Images in the left.\nPrice and Size in the right.\nReviews in the bottom.\n</code></pre>\n\n<p>If two websites are of any specific product, then it returns "Both are from same templates". Example, <a href="http://www.jabong.com/Lara-Karen-Full-Sleeve-Black-Polyester-Top-With-Cotton-Lace-196636.html" rel="nofollow">this link</a> and <a href="http://www.jabong.com/Puma-Flash-Ind-Black-Running-Shoes-187831.html" rel="nofollow">this link</a> have the same template.</p>\n\n<p>If one website shows any product and another website shows any category, then it shows "No match".\nExample, <a href="http://www.jabong.com/Lara-Karen-Full-Sleeve-Black-Polyester-Top-With-Cotton-Lace-196636.html" rel="nofollow">this link</a> and <a href="http://www.jabong.com/women/clothing/womens-tops/" rel="nofollow">this link</a> are from different template. </p>\n\n<p>I think that this algorithm requires some optimization, that\'s why I am posting this question in this forum.</p>\n\n<p><strong>My algorithm</strong></p>\n\n<ol>\n<li>Fetch, parse two input URLS and make their <a href="http://en.wikipedia.org/wiki/Document_Object_Model" rel="nofollow">DOM trees</a>.</li>\n<li>Then if any page contains , UL and TABLE , then remove that tag. I done this because, may be two pages contains different number of items.</li>\n<li>Then, I count number of tags in both URLS. say, initial_tag1, initial_tag2.</li>\n<li>Then, I start removing tags that have same position on corresponding pages and same Id and their below subtree, if that tree has number of nodes less than 10.</li>\n<li>Then, I start removing tags that have same position on coresponding pages and same Class name and their below subtree, if that tree has number of nodes less than 10..</li>\n<li>Then, I start removing tags that have no Id ,and No Class name and their below subtree, if that tree has number of nodes less than 10.</li>\n<li>Steps 4, 5, 6 have (N*N) complexity. Here, N, is number of tags. [In this way, in every step DOM tree going to shrink]</li>\n<li>When it comes out from this recursion, then I check final_tag1 and final_tag2.</li>\n<li>If final_tag1 and final_tag2 is less than initial_tag1*(0.2) and initial_tag2*(0.2) then I <strong>can</strong> say that <code>Two URL matched</code>, otherwise <code>not</code>.</li>\n</ol>\n\n<p>I wrote my code in <em>Java</em> using <em>Jsoup</em> and <em>Selenium</em>. I <a href="http://stackoverflow.com/questions/15718235/optimized-algorithm-to-compare-templates-of-two-urls">asked before on Stack Overflow</a>, but the answers did not help me.</p>\n\n<p>I think a lot about this algorithm, and I found that removing node from DOM tree is pretty slow process. This may be the culprit for slowing this algorithm.</p>\n\n<p>I discussed with some geeks, and </p>\n\n<blockquote>\n  <p>they said that use a score for every tag instead of removing them, and add them , and \n  at the end return (score I Got)/(accumulatedPoints) or something similar, and on the \n  basis of that you decide two websites are either similar or not.</p>\n</blockquote>\n\n<p>But I didn\'t understand this. So can you explain this statement, or can you give any other algorithm that solves this problem efficiently?</p>\n', 'ViewCount': '102', 'Title': 'Optimized algorithm to compare templates of two websites', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T13:36:52.260', 'LastEditDate': '2013-04-07T13:36:52.260', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2715', 'Tags': '<algorithms><efficiency>', 'CreationDate': '2013-04-07T10:07:13.130', 'Id': '11094'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for an algorithm which would find a minimal spanning tree given certain constraints (CSP) about importance of some nodes, e.g. consider a graph with next distance matrix:\n$$\n\\left[ \\begin{array}{c}\n- &amp; A &amp; B &amp; C &amp; D &amp; E &amp; F \\\\\nA &amp; 0 &amp; 120 &amp; 100 &amp; inf &amp; inf &amp; 30 \\\\\nB &amp; 120 &amp; 0 &amp; 70 &amp; inf &amp; 150 &amp; inf \\\\\nC &amp; 100 &amp; 70 &amp; 0 &amp; 60 &amp; 60 &amp; inf \\\\\nD &amp; inf &amp; inf &amp; 60 &amp; 0 &amp; inf &amp; 50 \\\\\nE &amp; inf &amp; 150 &amp; 60 &amp; inf &amp; 0 &amp; inf \\\\\nF &amp; 30 &amp; inf &amp; inf &amp; 50 &amp; inf &amp; 0 \\\\\n\\end{array} \\right]\n$$\nPrim's algorithm will result in something like this:\n$$\n\\left[ \\begin{array}{c}\n- &amp; A &amp; B &amp; C &amp; D &amp; E &amp; F \\\\\nA &amp; 0 &amp; inf &amp; inf &amp; inf &amp; inf &amp; 30 \\\\\nB &amp; inf &amp; 0 &amp; 70 &amp; inf &amp; inf &amp; inf \\\\\nC &amp; inf &amp; 70 &amp; 0 &amp; 60 &amp; 60 &amp; inf \\\\\nD &amp; inf &amp; inf &amp; 60 &amp; 0 &amp; inf &amp; 50 \\\\\nE &amp; inf &amp; inf &amp; 60 &amp; inf &amp; 0 &amp; inf \\\\\nF &amp; 30 &amp; inf &amp; inf &amp; 50 &amp; inf &amp; 0 \\\\\n\\end{array} \\right]\n$$</p>\n\n<p>However, node A is now zoned and it will take at least 3 transitions from $A$ to get to $C$ and for my specific CSP I need at most 2 transitions. It is fairly easy to incorporate such CSP into Prim's algorithm. <strong>The question is: are there any generic algorithms which deal with finding a minimal spanning tree given a set of constraints ?</strong></p>\n", 'ViewCount': '62', 'Title': "Node-weighted CSP in Prim's algorithm?", 'LastEditorUserId': '6793', 'LastActivityDate': '2013-04-07T14:44:13.350', 'LastEditDate': '2013-04-07T14:44:13.350', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6793', 'Tags': '<algorithms><graphs><graph-traversal><constraint-programming>', 'CreationDate': '2013-04-07T13:37:16.603', 'Id': '11100'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In a depth first tree, there are the edges define the tree (i.e the edges that were used in the traversal).</p>\n\n<p>There are some leftover edges connecting some of the other nodes. What is the difference between a cross edge and a forward edge?</p>\n\n<p>From wikipedia:</p>\n\n<blockquote>\n  <p>Based on this spanning tree, the edges of the original graph can be divided into three classes: forward edges, which point from a node of the tree to one of its descendants, back edges, which point from a node to one of its ancestors, and cross edges, which do neither. Sometimes tree edges, edges which belong to the spanning tree itself, are classified separately from forward edges. If the original graph is undirected then all of its edges are tree edges or back edges.</p>\n</blockquote>\n\n<p>Doesn't an edge that is not used in the traversal that points from one node to another establish a parent-child relationship? </p>\n", 'ViewCount': '1017', 'Title': 'Difference between cross edges and forward edges in a DFT', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:37:35.677', 'LastEditDate': '2013-04-08T14:37:35.677', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11125', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '863', 'Tags': '<algorithms><terminology><graphs><graph-traversal>', 'CreationDate': '2013-04-07T22:57:20.517', 'Id': '11116'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having hard times learning Dynamic Programming.</p>\n\n<p>I looked around the web and found many tutorials with examples.</p>\n\n<p>Each time I tried to figure out how to solve a new problem before looking at the solution my mind locked up and I could not continue trying...</p>\n\n<p>I do not where to start!! For example, in divide and conquer I know how to start but In dynamic programming there is no a fixed starting point or at least (technique).</p>\n\n<p>Last try I moved forward but I found that the solution was in a completely different direction.</p>\n', 'ViewCount': '353', 'ClosedDate': '2013-04-08T14:44:49.460', 'Title': 'How to master Dynamic Programming?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:42:15.260', 'LastEditDate': '2013-04-08T14:42:15.260', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4492', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2013-04-08T02:27:40.133', 'Id': '11128'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '59', 'Title': 'Independent set where two vertices need to have distance >= c', 'LastEditDate': '2013-04-08T03:03:23.500', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4259', 'FavoriteCount': '1', 'Body': "<p>An independent set (IS) in a graph is a set $V' \\subseteq V(G)$ of pairwise non-adjacent vertices. </p>\n\n<p>I am interested in the generalization $c$-IS where two nodes in  $V' \\subseteq V(G)$ need to have distance at least $c$ to any other vertex in $V'$.</p>\n\n<p>Has this problem been studied before? </p>\n", 'Tags': '<algorithms><reference-request><graphs>', 'LastEditorUserId': '4259', 'LastActivityDate': '2013-04-08T03:23:27.007', 'CommentCount': '1', 'AcceptedAnswerId': '11131', 'CreationDate': '2013-04-08T02:56:45.800', 'Id': '11129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The problem I have is like this bin packing problem, but instead I have $n$ bins and a collection of items with discrete masses. I need to put at least $m$ kg of stuff in each bin.</p>\n\n<p>Is there an efficient way of doing this? Is there a way that will assure there is approximately the same amount in each bin? Does having a good guess at the probability distribution of the masses help?</p>\n\n<p><strong>More explicitly:</strong></p>\n\n<p>I have $q$ objects $\\{o_1...o_q\\}$, each has a size $w(o_i) \\in \\mathbb{N}$.</p>\n\n<p>I need to find a collection of $n$ disjoint bins $B = \\{b_1...b_n\\}$ containing the objects such that</p>\n\n<p>$$\\forall b_i \\in B: \\sum_{o \\in b_i}w(o) &gt; m$$</p>\n\n<p>for some $m$. When it is possible that is.</p>\n', 'ViewCount': '53', 'Title': 'Relaxed Bin Packing Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:55:04.787', 'LastEditDate': '2013-04-08T14:55:04.787', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7641', 'Tags': '<algorithms><efficiency><packing>', 'CreationDate': '2013-04-08T11:11:38.520', 'Id': '11139'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '216', 'Title': 'Average length of s-t (simple) paths in a directed graph', 'LastEditDate': '2013-04-08T19:10:43.587', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '7644', 'FavoriteCount': '3', 'Body': '<p>Given the fact that $s$-$t$ path enumeration is a #P-complete problem, could there be efficient methods that compute (or at least approximate) the average length of $s$-$t$ path without enumerating them? <strike>What if paths are allowed to revisit vertices?</strike> </p>\n\n<p>Relevant results on special graphs could also be helpful.</p>\n', 'Tags': '<algorithms><complexity-theory><graphs><approximation><enumeration>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T04:41:12.817', 'CommentCount': '5', 'AcceptedAnswerId': '11184', 'CreationDate': '2013-04-08T18:28:42.923', 'Id': '11146'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem that can be viewed in two different ways:</p>\n\n<ol>\n<li><p>Compute an $n$-dimensional integral, numerical context. The domain of integration is an $n$-dimensional hyper-cube of side $L$.</p></li>\n<li><p>Count (just count) the roots of an $n$-dimensional function (not a polynomial).</p></li>\n</ol>\n\n<p>Solving just one of them is sufficient for solving the original problem.\nI know that simple algorithms for numerical integration would take $O(L^n)$, taking linear time per dimension. But I am not sure if there an asymptotically faster algorithms for (1). </p>\n\n<p>For (2), I am aware of algorithms that can find roots (Newton and Bisection), but I am not sure about the best algorithms just for counting how many roots are in a non-polynomial $n$-dimensional function.</p>\n\n<p>What are the best algorithms for (2)? Are they better than the fastest of (1)?</p>\n', 'ViewCount': '79', 'Title': 'numerical integral vs counting roots', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-14T19:29:09.583', 'LastEditDate': '2013-04-08T21:53:00.623', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2588', 'Tags': '<algorithms><numerical-analysis><counting>', 'CreationDate': '2013-04-08T21:02:35.247', 'Id': '11147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '731', 'Title': "Tarjan's Strongly Connected Component algorithm", 'LastEditDate': '2013-04-08T21:29:36.877', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'wrick', 'PostTypeId': '1', 'OwnerUserId': '7647', 'FavoriteCount': '0', 'Body': u'<p>I am trying to understand Tarjan\'s strongly connected component algorithm and I have a few questions (the line numbers I am referring to are from <a href="http://en.algoritmy.net/article/44220/Tarjans-algorithm" rel="nofollow">Algoritmy.net</a>):</p>\n\n<ol>\n<li><p>On line 33 why is <code>node.lowlink = min(node.lowlink, n.index)</code> \u2014 shouldn\'t it be same as line 31: <code>node.lowlink = min(node.lowlink, **n.lowlink**)</code>?</p></li>\n<li><p>Do we have to generate the components on line 40 in the while loop as we pop? Isn\'t it true that when the algorithm finishes, all the vertices grouped by <code>lowLink</code> should be the SCC?</p></li>\n<li><p>Is it ever true that after we recurse in line 30, <code>n</code> may not be in stack? If not and (1) is true we can simplify line 29-33 as follows:</p>\n\n<pre><code>if n.index == -1 \n   tarjanAlgorithm(n, scc, s, index)\nif stack.contains(n)\n   node.lowlink = min(node.lowlink, n.lowlink)\n</code></pre></li>\n<li><p>I went ahead and <a href="https://github.com/pathikrit/scalgos/blob/master/src/main/scala/scalgos/Graph.scala#L129" rel="nofollow">implemented</a> the algorithm in Scala. However, I dislike the code - it is very imperative/procedural with lots of mutating states and book-keeping indices. Is there a more "functional" version of the algorithm? I believe imperative versions of algorithms hide the core ideas behind the algorithm unlike the functional versions. I found <a href="http://clj-me.cgrand.net/2013/03/18/tarjans-strongly-connected-components-algorithm/" rel="nofollow">someone else encountering the same problem</a> with this particular algorithm but I have not been able to translate his Clojure code into idomatic Scala.</p></li>\n</ol>\n\n<p>Note: If anyone wants to experiment, I have a good setup that generates random graphs and <a href="https://github.com/pathikrit/scalgos/blob/master/src/test/scala/scalgos/GraphSpec.scala#L50" rel="nofollow">tests your SCC algorithm vs running Floyd-Warshall</a></p>\n\n<p>Here is the full pseudocode (\xa9 Algoritmy.net, <a href="http://en.algoritmy.net/article/39416/About" rel="nofollow">MIT licensed</a>).</p>\n\n<pre><code>index = 0\n\n/*\n* Runs Tarjan\'s algorithm\n* @param g graph, in which the SCC search will be performed\n* @return list of components\n*/\nList executeTarjan(Graph g)\nStack s = {}\nList scc = {} //list of strongly connected components\nfor Node node in g\nif (v.index is undefined)\ntarjanAlgorithm(node, scc, s)\n\nreturn scc\n\n/*\n* Tarjan\'s algorithm\n* @param node processed node\n* @param SCC list of strongly connected components\n* @param s stack\n*/\nprocedure tarjanAlgorithm(Node node, List scc, Stack s)\nv.index = index\nv.lowlink = index\nindex++\ns.push(node) //add to the stack\nfor each Node n in Adj(node) do //for all descendants\nif n.index == -1 //if the node was not discovered yet                  // &lt;--- line 29\ntarjanAlgorithm(n, scc, s, index) //search\nnode.lowlink = min(node.lowlink, n.lowlink) //modify parent\'s lowlink  // &lt;--- line 31\nelse if stack.contains(n) //if the component was not closed yet\nnode.lowlink = min(node.lowlink, n.index) //modify parents lowlink     // &lt;--- line 33\n\nif node.lowlink == node.index //if we are in the root of the component\nNode n = null\nList component //list of nodes contained in the component\ndo\nn = stack.pop() //pop a node from the stack\ncomponent.add(n) //and add it to the component                         // &lt;--- line 40\nwhile(n != v) //while we are not in the root\nscc.add(component) //add the compoennt to the SCC list\n</code></pre>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-09T08:20:07.607', 'CommentCount': '1', 'CreationDate': '2013-04-08T18:30:45.350', 'Id': '11148'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This may be basic to some of you, but excuse my inexperience with comp. geometry: </p>\n\n<p>Given a set of $n$ circles with centers $(x_i, y_i)$ for $1 \\leq i \\leq n$  and each having radii $r$. Also given a rectangle. All objects are on a plane. How to verify that every point inside the rectangle (including its edges) is fully covered by the circles. That is, each point in the rectangle lay on at least one of the circles. </p>\n\n<p>Anyone have hints ? I am currently trying with voronoi diagrams. </p>\n', 'ViewCount': '179', 'Title': 'Circles covering a rectangular, how to verify it?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T08:59:01.673', 'LastEditDate': '2013-04-10T08:59:01.673', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11164', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '867', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-04-09T14:01:11.527', 'Id': '11163'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We are given an (unsorted) list $L=(a_1,\\dots,a_n)$ of numbers of size $n$, where $a_i\\in \\{ 1,\\dots,B\\}$. </p>\n\n<p>We want to find the minimum number $x$ from $\\{ 1,\\dots,B\\} \\backslash L$. </p>\n\n<blockquote>\n  <p>What is the space complexity of this problem ? (The space to store the input, $L$, does not count.) What if the input $L$ is in a stream which you can only read from left to right for at most constant number of passes ?</p>\n</blockquote>\n\n<p>The obvious way to solve this is just to copy $L$ into the working memory and then (in-place) sort $L$, and find $x$ in the obvious way. This algorithm uses space of size $n$.</p>\n\n<p>Can we do better ?</p>\n', 'ViewCount': '283', 'Title': 'Space complexity for finding the minimum number outside the list of numbers', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-15T17:29:27.610', 'LastEditDate': '2013-04-10T08:52:21.597', 'AnswerCount': '4', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4706', 'Tags': '<algorithms><space-complexity><streaming-algorithm>', 'CreationDate': '2013-04-09T22:56:11.750', 'FavoriteCount': '1', 'Id': '11174'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have to solve the following problem:</p>\n\n<blockquote>\n  <p>Consider the problem Connected:</p>\n  \n  <p><strong>Input:</strong> An unweighted, undirected graph $G$.</p>\n  \n  <p><strong>Output:</strong> True if and only if $G$ is connected.</p>\n  \n  <p>Show that Connected can be decided in polynomial time.</p>\n</blockquote>\n\n<p>I have been at this for hours, and I can't seem to find a way to prove this.\nAny hints?</p>\n", 'ViewCount': '384', 'Title': 'How to check whether a graph is connected in polynomial time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-20T10:18:03.417', 'LastEditDate': '2013-04-10T08:49:17.583', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7643', 'Tags': '<algorithms><complexity-theory><graph-theory><polynomial-time>', 'CreationDate': '2013-04-10T00:08:01.310', 'Id': '11177'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'ve been reading about hypercube connection template for parallel algorithms. The general scheme is explained in <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node124.html#SECTION04310000000000000000" rel="nofollow"><em>Designing and Building Parallel Programs</em> by Ian Foster</a> and it\'s pretty clear.</p>\n\n<p>What I don\'t understand is how it\'s applied on the merge sort <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node127.html" rel="nofollow">in \xa711.4</a> The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<p>The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<pre><code>procedure parallel_mergesort(myid, d, data, newdata)\nbegin\n  data = sequential_mergesort(data)\n  for dim = 1 to d\n    data = parallel_merge(myid, dim, data)\n  endfor\n  newdata = data\nend\n</code></pre>\n\n<p>Please, explain to me step by step, assuming we have an array of twelve elements $(3,1,5,7,4,2,8,9,4,2,7,5)$ and we\'ve broken this data to four processors like this: </p>\n\n<p>$\\qquad ((3,1,5),(7,4,2),(8,9,4),(2,7,5))$. </p>\n\n<p>What data will have each process after each iteration? I understand why we use the hybercube template in this algorithm, but why do we have exactly $i$ compare-exchanges at the $i$-th level? I mean, when $i=1$, we compare-exchange data from processes $1-2, 3-4, .. P-1, P$. That\'s not $1$, that\'s $P/2$? Do I misunderstand something?</p>\n', 'ViewCount': '282', 'Title': 'Parallel merge sort using hypercube connection template', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T20:43:03.117', 'LastEditDate': '2013-04-10T20:43:03.117', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'OwnerDisplayName': u'\u0418\u0433\u043e\u0440\u044c \u041c\u043e\u0440\u043e\u0437\u043e\u0432', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><parallel-computing><machine-models>', 'CreationDate': '2013-04-10T18:14:08.597', 'Id': '11205'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If I am given a graph which forms a tree, I am interested in finding a vertex which maximizes the minimum distance to any leaf.</p>\n\n<p>I am sure this problem has been studied before.\nDoes anybody know the name of this problem or an algorithm for solving it?</p>\n', 'ViewCount': '250', 'Title': 'Given a tree, find a vertex which maximizes the minimum distance to any leaf', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T22:49:24.883', 'LastEditDate': '2013-04-10T21:35:43.710', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '11212', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7675', 'Tags': '<algorithms><graph-theory><reference-request><trees>', 'CreationDate': '2013-04-10T21:05:36.970', 'Id': '11208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From <a href="http://programmers.stackexchange.com/questions/194552/what-is-the-difference-between-quantum-annealing-and-simulated-annealing">that question</a> about differences between Quantum annealing and simulated annealing, we found (in commets to answer) that physical implementation of quantum annealing is exists (D-Wave quantum computers).</p>\n\n<p>Can anyone explain that algorithm in terms of quantum gates and quantum algorithms, or in physical terms (a part of algorithm that depends on quantum hardware)?</p>\n\n<p>Does anyone have any ideas about that?\nPlease tell me, if you know some links related this question.</p>\n', 'ViewCount': '419', 'Title': 'The physical implementation of quantum annealing algorithm', 'LastActivityDate': '2013-04-19T20:20:45.790', 'AnswerCount': '4', 'CommentCount': '3', 'AcceptedAnswerId': '11362', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7672', 'Tags': '<algorithms><randomized-algorithms><quantum-computing>', 'CreationDate': '2013-04-11T07:05:16.517', 'FavoriteCount': '2', 'Id': '11218'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In one of the text book its mentioned that 'running time of this algorithm is 200 computer years'. Can somebody please explain what is the meaning of a computer year?</p>\n", 'ViewCount': '488', 'Title': 'What is a computer year?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-11T12:51:40.690', 'LastEditDate': '2013-04-11T12:51:40.690', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<algorithms><terminology><runtime-analysis>', 'CreationDate': '2013-04-11T10:17:15.833', 'FavoriteCount': '1', 'Id': '11220'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A bridge (critical edge) in an undirected graph is an edge whose removal increases the number of connected components.</p>\n\n<p>I need to determine all critical edges in an undirected graph, in $O(V+E)$ time. From what I found out, I need to use a modified DF search, but all pseudo-code algorithms I found have <code>low[v]</code> and <code>d[v]</code> which I don't understand.</p>\n\n<p>Can someone please explain to me the $O(V+E)$ bridge determination algorithm?</p>\n", 'ViewCount': '257', 'Title': 'Bridge determination in undirected graphs', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-04-11T14:22:16.190', 'LastEditDate': '2013-04-11T14:22:16.190', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'OwnerDisplayName': 'user7681', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><graphs><graph-traversal>', 'CreationDate': '2013-04-11T13:18:37.097', 'Id': '11229'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have been reading up on algorithm for finding the strongly connected components in a directed graph $G=(V,E)$. It considers two DFS search and the second step is transposing the original graph $G^T$. </p>\n\n<p>The algorithm is the following :</p>\n\n<ol>\n<li>Execute DFS on $G$ (starting at an arbitrary starting vertex), keeping track of the \ufb01nishing times of all vertices.</li>\n<li>Compute the transpose, </li>\n<li>Execute DFS on $G^T$, starting at the vertex with the latest \ufb01nishing time, forming a tree rooted at that vertex. Once a tree is completed, move on to the unvisited vertex with the next latest \ufb01nishing time and form another tree using DFS and repeat until all the vertices in $G^T$ are visited.</li>\n<li>Output the vertices in each tree formed by the second DFS as a separate strongly connected component.</li>\n</ol>\n\n<p>My question is :</p>\n\n<ol>\n<li>What is the intuition behind this middle step of computing a transpose?</li>\n</ol>\n', 'ViewCount': '460', 'Title': 'Correctness of Strongly Connected Components algorithm for a directed graph', 'LastEditorUserId': '2223', 'LastActivityDate': '2013-04-12T11:29:20.083', 'LastEditDate': '2013-04-11T15:32:00.840', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11257', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><graphs><graph-traversal>', 'CreationDate': '2013-04-11T15:08:51.047', 'Id': '11232'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A program takes as input a balanced binary search tree with $n$ leaf nodes and computes the value of a function $g(x)$ for each node $x$. If the cost of computing $g(x)$ is </p>\n\n<p>$\\qquad \\min(\\#\\text{leaves in } L(x), \\#\\text{leaves in } R(x))$</p>\n\n<p>for $L(x), R(x)$ the left resp. right subtree of $x$, then the worst-case time complexity of the program is</p>\n\n<ol>\n<li>$\\Theta(n)$</li>\n<li>$\\Theta(n \\log n)$</li>\n<li>$\\Theta(n^2)$ </li>\n<li>$\\Theta(n^2 \\log n)$</li>\n</ol>\n\n<p>I am actually looking for a subtle hint. </p>\n', 'ViewCount': '492', 'Title': 'Finding no. of leaf nodes for each node in a BST', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-22T14:55:46.203', 'LastEditDate': '2013-04-12T10:03:59.117', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '11254', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><binary-trees><search-trees>', 'CreationDate': '2013-04-12T08:03:52.183', 'Id': '11252'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2886', 'Title': 'Longest path in an undirected tree with only one traversal', 'LastEditDate': '2013-04-12T14:33:50.713', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '4980', 'FavoriteCount': '4', 'Body': "<p>There is this standard algorithm for finding longest path in undirected trees using two depth-first searches:</p>\n\n<ul>\n<li>Start DFS from a random vertex $v$ and find the farthest vertex from it; say it is $v'$. </li>\n<li>Now start a DFS from $v'$ to find the vertex farthest from it. This path is the longest path in the graph.</li>\n</ul>\n\n<p>The question is, can this be done more efficiently? Can we do it with a single DFS or BFS?</p>\n", 'Tags': '<algorithms><trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-24T17:42:01.533', 'CommentCount': '4', 'AcceptedAnswerId': '11264', 'CreationDate': '2013-04-12T14:00:52.687', 'Id': '11263'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<blockquote>\n  <p>Given a set of n jobs with [start time, end time, cost] find a subset so that no 2 jobs overlap and the cost is maximum.</p>\n</blockquote>\n\n<p>Now I'm not sure if a greedy algorithm will do the trick. That is, sort by cost and always take the next job that doesn't intersect and with max cost between the two.</p>\n\n<p>Is this equivalent to a knapsack problem? How could I approach it?</p>\n", 'ViewCount': '407', 'Title': 'Find non-overlapping scheduled jobs with maximum cost', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-16T17:54:04.300', 'LastEditDate': '2013-11-09T15:21:08.183', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7705', 'Tags': '<algorithms><scheduling><greedy-algorithms><knapsack-problems>', 'CreationDate': '2013-04-12T15:24:23.293', 'Id': '11265'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I want an algorithm that calculates which element, among two, appears more often than the other in a sorted array. The array will have only two types of elements. </p>\n\n<p>Example : $aaaaaabbb$ </p>\n\n<p>Here $a&gt;b$. </p>\n\n<p>I have to find an constant time algorithm. Is it possible? The only thing I could come up with was using stack. Push all $a$'s and pop them with $b$. But it takes $O(n)$ operations. Any better approaches? Need a hint (no solution).</p>\n", 'ViewCount': '44', 'Title': 'Finding the element that occurs more often than the other', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-12T23:40:12.290', 'LastEditDate': '2013-04-12T23:40:12.290', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '11268', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithms><data-structures><search-algorithms><arrays>', 'CreationDate': '2013-04-12T15:30:57.237', 'Id': '11266'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a partial ordered set (poset) $S$, is there a known procedure or algorithm to find the set of chains (i.e. subsets of $S$ where every two elements are comparable)? </p>\n\n<p>Note: I am asking here instead of math.SE because i'm looking for <em>an algorithm</em> for the problem.  </p>\n", 'ViewCount': '65', 'Title': 'Extracting the set of chains from a partial order', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-14T13:59:34.690', 'LastEditDate': '2013-04-14T13:59:34.690', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11279', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<algorithms><partial-order>', 'CreationDate': '2013-04-13T04:13:26.907', 'Id': '11276'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '773', 'Title': 'How to find the minimum number of vertices whose removal make the graph disjoint', 'LastEditDate': '2013-04-14T10:36:16.130', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7711', 'FavoriteCount': '2', 'Body': '<p>Given a graph $G = (V,E)$.</p>\n\n<p>Is there any algorithm which finds the minimum number of vertices to be removed from $G$ so that every vertex in the graph becomes disjoint, <em>i.e.</em>, every vertex is disconnected from every other vertex?</p>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-14T10:36:16.130', 'CommentCount': '2', 'AcceptedAnswerId': '11283', 'CreationDate': '2013-04-13T11:32:28.130', 'Id': '11281'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Note that I had asked this question in <a href="http://gis.stackexchange.com/questions/41724/space-filling-between-random-2d-lines">GIS forum</a>, although it\n  has gotten many up-votes, still has not received any answer. Hope you can\n  break the silence, some collaboration :)</p>\n</blockquote>\n\n<p>Consider a region (2D) filled with lines randomly (following Figure). We are interested in filling the empty spaces between lines including four boundary edges in a way:</p>\n\n<p><strong>0-</strong> maximizing the size of parcels;<br>\n<strong>1-</strong> shape of filling parcels is square aligned horizontally or vertically;<br>\n<strong>2-</strong> shape of filling parcels is square, <em>i.e., relaxed alignment</em>;<br>\n<strong>3-</strong>  <strong>shape of filling parcels is any quadrangle.</strong> <em>our original question</em>  </p>\n\n<p>So for now there are three different scenarios.<br>\n<strong><em>Note</strong> that the lines are of the form <code>[x1,y1,x2,y2]</code> point set, real numbers.</em></p>\n\n<p>[* * *] <strong><em>Ideas of possible solutions/algorithms/code snippets/etc are more than welcome.</em></strong></p>\n\n<p><img src="http://i.stack.imgur.com/K0gTM.png" alt="enter image description here"></p>\n\n<hr>\n\n<p>For the first case i.e., horizontally/vertically aligned squares, our proposal as a solution is:<br>\n<strong>1-</strong> <em>rasterising input lines into bitmap (matrix) using e.g., <a href="http://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm" rel="nofollow">Bresenham\'s algorithm</a></em><br>\n<strong>2-</strong> <em>searching nearby cells for each cell of desired colour (i.e., the same colour) with an objective function to maximize the associated area i.e., the number of pixels.</em>  </p>\n\n<p><img src="http://i.stack.imgur.com/Pb8pZ.png" alt="enter image description here">  </p>\n\n<p>We assumed that the reader is familiar with the concept of space-filling-tiling. You may follow <a href="http://paulbourke.net/texture_colour/randomtile/" rel="nofollow">this link</a> for inspiration. However, note that our problem is different. As we do not fill the empty space randomly and we do not choose the size of filler shapes randomly. The solution should be iterative, starting from largest to smallest permitted. The size boundary can be defined, for example, according to our explanation below for a particular study (our interest). For all the cases, there is no limit on the number of parcel being fitted. Indeed, it is up to user to limit the iteration number, by choosing a minimum area for parcels, for example. This is obvious in the example given above in which we discretised lines into pixels with specified size. That is, the procedure should run until entire empty area is filled respecting the criterion e.g., the maximum area of parcels.</p>\n\n<p><strong>A mathematical view for the problem can be stated as follows:</strong><br>\n<strong>2D:</strong> Find all rectangles that could be extracted from a given 2D region with some lines optimized for larger rectangle size as possible.<br>\n<strong>3D:</strong> Find all rectangular cubes that could be extracted from a given 3D region with some sub-planes (better: polygons) optimized for larger block size as possible. </p>\n\n<p><strong>- Application:</strong><br>\nOne application is to find out the distribution of extractable intact \'rock\' blocks in a heavily fractured \'mine\'. This could be very helpful for many aspects including drilling design, financial evaluation and so on.  </p>\n\n<p><em>-- Details:</em><br>\nFor a mine of decorative rock (stone) the products which are the blocks of intact rocks cut as rectangular cubes the price is closely dependent to the size of the block.  Extraction of a block from a suitable area i.e., with no major fracture will be desired if the amount of remaining parts is small as possible. Usually, the small pieces of rocks have no economic value relatively and are considered so as waste.<br>\nThe question in this post investigates solutions for this kind of problem.  </p>\n\n<p><strong>Size Constrains:</strong><br>\nYou may put some restrictions on the solution for the ultimate question, although, we believe it is always possible to add more later.  For example, follow these:\n{2D case}<br>\nThe best size of a block (economically optimum rectangle) to be extracted under the conditions mentioned above, is <code>1x1 m</code> given <code>10x10 m</code> for the region in the example. This is one constraint defined based on economical value. The minimum workable size for cutting etc, let be <code>0.15x0.15 m</code>; so this the second size limit.<br>\n<img src="http://i.stack.imgur.com/akj12.jpg" alt="enter image description here"><br>\nThe figure above shows the economic value function depending to the block size. So for this particular case every rock piece smaller than <code>0.15x0.15 m</code> is just waste. There will be no block size larger than <code>1.7x1.7 m</code> due to operation limits.</p>\n', 'ViewCount': '121', 'Title': 'Space filling between random 2D lines', 'LastEditorUserId': '7712', 'LastActivityDate': '2013-04-14T10:49:45.397', 'LastEditDate': '2013-04-14T10:49:45.397', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7712', 'Tags': '<algorithms><computational-geometry><image-processing>', 'CreationDate': '2013-04-13T12:03:42.897', 'FavoriteCount': '2', 'Id': '11282'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3059', 'Title': 'Finding shortest and longest paths between two vertices in a DAG', 'LastEditDate': '2013-04-14T16:59:17.863', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7715', 'FavoriteCount': '3', 'Body': '<p>Given an unweighted DAG (directed acyclic graph) $D = (V,A)$ and two vertices $s$ and $t$, is it possible to find the shortest and longest path from $s$ to $t$ in polynomial time?  Path lengths are measured by the number of edges.</p>\n\n<p>I am interested in finding the range of possible path lengths in polynomial time.</p>\n\n<p>Ps., this question is a duplicate of the StackOverflow question <a href="http://stackoverflow.com/questions/10712495/longest-path-in-a-dag">Longest path in a DAG</a>.</p>\n', 'Tags': '<algorithms><graphs><shortest-path><polynomial-time>', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-04-15T00:36:10.273', 'CommentCount': '0', 'AcceptedAnswerId': '11296', 'CreationDate': '2013-04-14T00:02:56.393', 'Id': '11295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '300', 'Title': "What's harder: Shuffling a sorted deck or sorting a shuffled one?", 'LastEditDate': '2013-04-14T15:00:30.950', 'AnswerCount': '2', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '4197', 'FavoriteCount': '1', 'Body': '<p>You have an array of $n$ distinct elements. You have access to a comparator (a black box function taking two elements $a$ and $b$ and returning true iff $a &lt; b$) and a truly random source of bits (a black box function taking no arguments and returning an independently uniformly random bit). Consider the following two tasks:</p>\n\n<ol>\n<li>The array is currently sorted. Produce a uniformly (or approximately uniformly) randomly selected permutation.</li>\n<li>The array consists of some permutation selected uniformly at random by nature. Produce a sorted array.</li>\n</ol>\n\n<p>My question is</p>\n\n<blockquote>\n  <p>Which task requires more energy asymptotically?</p>\n</blockquote>\n\n<p>I am unable to define the question more precisely because I don\'t know enough about the connection between information theory, thermodynamics, or whatever else is needed to answer this question. However, I think the question can be made well-defined (and am hoping someone helps me with this in an answer!).</p>\n\n<p>Now, algorithmically, my intuition is that they are equal. Notice that every sort is a shuffle in reverse, and vice versa. Sorting requires $\\log n! \\approx n \\log n$ comparisons, while shuffling, since it picks a random permutation from $n!$ choices, requires $\\log n! \\approx n \\log n$ random bits. Both shuffling and sorting require about $n$ swaps.</p>\n\n<p>However, I feel like there should be an answer applying <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer\'s principle</a>, which says that it requires energy to "erase" a bit. Intuitively, I think this means that sorting the array is more difficult, because it requires "erasing" $n \\log n$ bits of information, going from a low-energy, high-entropy ground state of disorder to a highly ordered one. But on the other hand, for any given computation, sorting just transforms one permutation to another one. Since I\'m a complete non-expert here, I was hoping someone with a knowledge of the connection to physics could help "sort" this out!</p>\n\n<p>(The question didn\'t get any answers on <a href="http://math.stackexchange.com/questions/359911/which-takes-more-energy-shuffling-a-sorted-deck-or-sorting-a-shuffled-one">math.se</a>, so I\'m reposting it here. Hope that is ok.)</p>\n', 'Tags': '<algorithms><algorithm-analysis><information-theory><entropy>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-21T14:19:51.400', 'CommentCount': '15', 'AcceptedAnswerId': '11452', 'CreationDate': '2013-04-14T03:49:03.497', 'Id': '11299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In database query processing, the approximate time for selection operation using primary index when equality is on key is $2(b_s + b_t)$ where $b_s$ is disk seek time and $b_t$ is disk transfer time (assuming one level of indexing), because one seek and transfer time will be needed for finding the index and another one will be for the actual data.  </p>\n\n<p>But what will happen if the equality is on a no- key value? Since now we cannot search in the index, don't we have to do a linear search?</p>\n", 'ViewCount': '52', 'Title': 'Approximate time for selection operation using index when equality is on nonkey', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-15T15:11:13.230', 'LastEditDate': '2013-04-14T11:22:36.033', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11326', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '778', 'Tags': '<runtime-analysis><search-algorithms><databases>', 'CreationDate': '2013-04-14T06:08:19.807', 'Id': '11301'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a positively weighted DAG (directed acyclic graph) $D = (V,E)$, can you create a new non-weighted DAG $D'$ by converting each edge with weight $w(e) = x$ into x non-weighted edges and vertices? I believe this would take $O(|E|+W)$ time where $|E|$ is the number of edges and $W$ is the total weight of all edges. My concern is whether I can include this weight variable and still consider this algorithm to be in polynomial time.</p>\n\n<p>(NOTE: This algorithm may apply to all positively weighted graphs, not just DAGs.)</p>\n", 'ViewCount': '37', 'Title': 'Can you convert a positively weighted DAG into a non-weighted DAG in polynomial time?', 'LastActivityDate': '2013-04-15T22:05:19.823', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11344', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7715', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><polynomial-time>', 'CreationDate': '2013-04-15T20:19:10.183', 'Id': '11343'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '175', 'Title': 'Transforming an arbitrary cover into a vertex cover', 'LastEditDate': '2013-04-19T19:42:54.777', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '4259', 'FavoriteCount': '0', 'Body': "<p>Given is a planar graph $G=(V,E)$ and let $\\mathcal{G}$ denote its embedding in the plane s.t. each edge has length $1$. \nI have furthermore a set $C$ of points where each point $c \\in C$ is contained in $\\mathcal{G}$. Furthermore, it holds  for any point $p$ in $\\mathcal{G}$ that there exists a $c \\in C$ with  geodesic distance to $p$ at most one. (The distance is measured as the shortest distance within $\\mathcal{G}$.)</p>\n\n<p>I want to argue that given a $C$ for which the above condition holds, I can easily transform it into a vertex cover, or put differently, transform it into a $C'$ of same cardinality s.t any $c \\in C'$ is placed in $\\mathcal{G}$  at a vertex of $G$, and $C'$ still covers $G$.</p>\n\n<p>My approach was to orient the edges and move the points in $C$ at the end vertex of the arc. But so far I did not find a correct orientation which yields $C'$ from $C$.</p>\n\n<p>Does anybody have an idea?</p>\n", 'Tags': '<algorithms><graph-theory><set-cover>', 'LastEditorUserId': '4259', 'LastActivityDate': '2013-04-22T16:50:16.823', 'CommentCount': '2', 'AcceptedAnswerId': '11416', 'CreationDate': '2013-04-16T00:49:10.817', 'Id': '11347'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an undirected graph $G = (V,E)$, what is the clique number $\\omega(G)$ given $|E|$, <em>i.e.</em>, the size of the largest clique in a graph with $|E|$ edges.</p>\n\n<p>I think this is doable after realizing that the number of edges in a clique is equal to the <em><a href="http://en.wikipedia.org/wiki/Triangular_number" rel="nofollow">triangular number</a></em>:\n$$|E(K_k)| = \\frac{1}{2}k(k-1).$$</p>\n\n<p>I am looking for a closed formula.</p>\n', 'ViewCount': '219', 'Title': 'Size of maximum clique given a fixed amount of edges?', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-04-17T14:16:52.153', 'LastEditDate': '2013-04-17T14:16:52.153', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7715', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2013-04-16T19:12:03.283', 'Id': '11360'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I get values $x_t$ in an online fashion and want to buy "good" ones, where "good" means that some measure $P(x_t) &gt;T$. Consider the following simple algorithm.</p>\n\n<pre><code>T = 0.7\nN = 100 // or any value N &gt; B\nB = 20 // or any value 1 &lt; B &lt; N\n\nl = 0\n\nfor t from 1 to N:\n    input a new observation x_t\n    let P(x_t) the probability associated to x_t\n\n    if P(x_t) &gt; T:\n        l = l + 1\n        pay 1 dollar to buy y_t the label of x_t\n        output immediately the label y_t\n</code></pre>\n\n<p>If the condition $P(x_t) &gt; T$ is used then we get about $l = 100-70 = 30$, this is ok since the value of $T$ is set to $0.7$.</p>\n\n<p>Now if I want to add a constraint which is: additionally to the fact that elements $x_t$ for which the label $y_t$ is purchased are those for which $P(x_t) &gt; T$, I want also that we do not buy more than $B=20$ labels (for example because we only have 20 dollars as budget).</p>\n\n<p>But the problem is that, if I replace the the condition ($P(x_t) &gt; T$) by ($P(x_t) &gt; T \\wedge l &lt; B$), then the elements $x_t$ for which we buy a label are more likely to be among the first elements $t$ that we browse (that is, for an element $x_{95}$ for $t = 95$ for example we will never have a chance to buy its label even if its probability was $P(x_{95}) \\gg T$). But I want that all the elements from $t = 1$ to $N$ will have equal chance to buy their label (not advantaging only the first elements).</p>\n\n<p>Note: the condition that $P(x_t) &gt; T$ for buying the label of a new observation $x_t$, should not be removed from my code. This is important for me: only labels of observations for which $P(x_t)$ was higher than $T$ at time $t$, are possibly purchased; and we should not purchase more than our budget $B$. Note also that a purchased label should immediately be output after we buy it, we should not wait until the end to decide if we buy it or not.</p>\n\n<p>Also, note that we do not have the N elements beforehand; at each time $t$ we see just one new observation $x_t$. And note that you pay 1 dollar when you select a given $x_t$ to ask for its label and that you should output answer (label of selected $x_t$) immediately; so you can not select some $B$ elements then replace them with new selected other elements, because your budget $B$ will already be finished.</p>\n', 'ViewCount': '64', 'Title': 'How to sample uniformly from a stream of elements, some of which are unsuited?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-04-18T19:57:32.787', 'LastEditDate': '2013-04-18T17:53:54.317', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2895', 'Tags': '<probabilistic-algorithms><online-algorithms><sampling>', 'CreationDate': '2013-04-17T22:07:38.767', 'Id': '11370'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would you explain why the Fast Fourier Transform is faster than the Discrete Fourier Transform, if you had to give a presentation about it for the general (non-mathematical) public?</p>\n', 'ViewCount': '978', 'Title': 'Explaining why FFT is faster than DFT for the general public?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-18T11:54:12.870', 'LastEditDate': '2013-04-18T08:25:48.223', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<algorithms><efficiency><education><didactics><fourier-transform>', 'CreationDate': '2013-04-17T22:22:57.513', 'Id': '11371'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to understand FFT algorithm explained <a href="http://www.drdobbs.com/cpp/a-simple-and-efficient-fft-implementatio/199500857" rel="nofollow">here</a></p>\n\n<pre><code>FFT(x) {   n=length(x);   \nif (n==1) return x;   \nm = n/2;   \nX = (x_{2j})_{j=0}^{m-1};   \nY = (x_{2j+1})_{j=0}^{m-1};  \nX = FFT(X);   \nY = FFT(Y);  \nU = (X_{k mod m})_{k=0}^{n-1};   \nV = (g^{-k}Y_{k mod m})_{k=0}^{n-1};   \nreturn U+V; \n} \n</code></pre>\n\n<p>The author says that the above comes from <a href="http://www.tcm.phy.cam.ac.uk/~pdh1001/Talks/parallel_fft/node5.html" rel="nofollow">Danielson-Lanczos Lemma</a>. </p>\n\n<p>I am unable to understand what is the meaning of the lines:</p>\n\n<p>X = (x_{2j})_{j=0}^{m-1}; </p>\n\n<p>Y = (x_{2j+1})_{j=0}^{m-1};</p>\n', 'ViewCount': '327', 'Title': 'FFT implementation using Danielson-Lanczos Lemma', 'LastEditorUserId': '683', 'LastActivityDate': '2013-06-18T02:38:07.777', 'LastEditDate': '2013-04-19T01:45:15.373', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms>', 'CreationDate': '2013-04-18T10:04:50.063', 'Id': '11377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Cormen talks briefly about the advantages of picking a random pivot in quicksort.  However as pointed out <a href="http://algoviz.org/OpenDSA/Books/OpenDSA/html/Quicksort.html" rel="nofollow">here</a>(4th to the last paragraph):</p>\n\n<blockquote>\n  <p>Using a random number generator to choose the positions is relatively\n  expensive</p>\n</blockquote>\n\n<p>So how is picking a random pivot actually implemented in practice, and how random is it?  It can\'t be too expensive, since from what I understand one of quicksort\'s main advantages over other $\\cal{O}(n \\lg n)$ sorts is the small constant factors, and spending allot of cycles picking pivots would undermine that advantage.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>As an example, the <code>C</code> code from "<a href="http://www.catonmat.net/blog/three-beautiful-quicksorts/" rel="nofollow">Three Beautiful Quicksorts</a>" actually calls the <code>C</code> library <code>rand</code> function:</p>\n\n<pre><code>int randint(int l, int u) {\n    return rand()%(u-l+1)+l;\n}\n\nvoid quicksort(int l, int u) {\n    int i, m;\n    if (l &gt;= u) return;\n    swap(l, randint(l, u));\n    m = l;\n    for (i = l+1; i &lt;= u; i++)\n        if (x[i] &lt; x[l])\n            swap(++m, i);\n    swap(l, m);\n    quicksort(l, m-1);\n    quicksort(m+1, u);\n}\n</code></pre>\n\n<p>While the pivot picking code here is clearly $\\cal{O}(1)$, it would seem that the hidden $c$ here is relatively high.</p>\n', 'ViewCount': '207', 'Title': 'From Whence the Randomization in Randomized Quicksort', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-23T09:45:57.250', 'LastEditDate': '2013-04-21T14:02:28.190', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11387', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><randomized-algorithms><quicksort>', 'CreationDate': '2013-04-18T18:04:50.873', 'Id': '11385'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Studying parallel algorithms for CLRS, old edition Chapter 30.  Can some one explain with a simple example what is pointer jumping and how exactly it works ?   </p>\n', 'ViewCount': '185', 'Title': 'What is Pointer Jumping ?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T14:05:55.223', 'LastEditDate': '2013-04-21T14:05:55.223', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><terminology><parallel-computing>', 'CreationDate': '2013-04-19T10:51:17.793', 'Id': '11407'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '361', 'Title': 'FFT-less $O(n\\log n)$ algorithm for pairwise sums', 'LastEditDate': '2013-04-20T15:02:37.317', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '5', 'Body': '<p>Suppose we are given $n$ distinct integers $a_1, a_2, \\dots, a_n$, such that $0 \\le a_i \\le kn$ for some constant $k \\gt 0$, and for all $i$.</p>\n\n<p>We are interested in finding the counts of all the possible pairwise sums $S_{ij} = a_i + a_j$. ($i = j$ is allowed).</p>\n\n<p>One algorithm is to construct the polynomial $P(x) = \\sum_{j=1}^{n} x^{a_j}$ of degree $\\le kn$, and compute its square using the Fourier transform method and read off the powers with their coefficients in the resulting polynomial. This is an $O(n \\log n)$ time algorithm.</p>\n\n<p>I have two questions:</p>\n\n<ul>\n<li><p>Is there an $O(n \\log n)$ algorithm which does not use FFT? </p></li>\n<li><p>Are better algorithms known (i.e $o(n \\log n)$)? (FFT allowed).</p></li>\n</ul>\n', 'Tags': '<algorithms><time-complexity><fourier-transform>', 'LastEditorUserId': '139', 'LastActivityDate': '2013-11-04T01:13:21.093', 'CommentCount': '4', 'AcceptedAnswerId': '16667', 'CreationDate': '2013-04-20T00:51:54.663', 'Id': '11418'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This problem is about finding a route on a square grid.\nThe starting point is $(1,1)$ and the target point $(n,m)$.\nI can move each step from my current point $(x,y)$ either to $(x+y,y)$ or $(x,y+x)$.\nNow I need to determine if there is a path from $(1,1)$ to $(n,m)$, and if so to return the shortest one.</p>\n\n<p>Now I believe that if I trace back my steps from the input point $(n,m)$ I can always know which move I made out of the two possible ones since if $n=m$ then there is no route, this means I'm always take the smaller coordinate and subtract it from the bigger one.\nBut that means I have at most only one possible route to $(n,m)$ so why was  I asked to return the shortest one?</p>\n\n<p>Am I missing anything ?</p>\n", 'ViewCount': '188', 'Title': u'Route on a square grid with only (x,y) \u2192 (x,x+y) and (x,y) \u2192 (x+y,y) moves', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-20T23:28:40.130', 'LastEditDate': '2013-04-20T23:28:40.130', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6632', 'Tags': '<algorithms><graph-theory><shortest-path><square-grid>', 'CreationDate': '2013-04-20T13:56:24.683', 'Id': '11427'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>First of all, let me preface by saying that this question is not completly new but the original question hasn\'t been answered. More important, this is only basic question on understanding the proof itself.</p>\n\n<p>So after some search in the site I found the following question : <a href="http://cs.stackexchange.com/questions/7565/bellman-ford-algorthm-and-negative-cycle-proof">Bellman-Ford algorthm and negative cycle proof</a>.</p>\n\n<p>The guy is trying to understand how to prove <em>that a cycle in parent pointer is necessarily a negative cycle</em>.</p>\n\n<p>My question is very basic and isn\'t duplicate, I just couldn\'t find anything in the document on the related question which answers my question.</p>\n\n<p>What does it mean a cycle in the parent pointer? I mean I could a graph whereas the cycle in the parent pointer isn\'t a negative cycle... I don\'t understand why It must be a negative cycle.</p>\n\n<p>See an example of a graph I have in mind :</p>\n\n<p><img src="http://i.stack.imgur.com/DSlY3.jpg" alt="enter image description here"></p>\n\n<p>Now suppose the first node we start with is (a) and suppose the we travling the edges in the following order: ab,bc,cd.</p>\n\n<p>and here we goes, we have a cycle in the parent pointer, and as far as I understand that is (c) becuase it is the parent of (d) and yet the cycle isn\'t negative cycle.</p>\n', 'ViewCount': '154', 'Title': 'Bellman-Ford parent pointer (?) negative cycle', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-23T14:01:36.147', 'LastEditDate': '2013-04-21T14:16:44.637', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11482', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4514', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2013-04-20T14:15:41.437', 'Id': '11428'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Prove that if G is an undirected connected graph, then each of its edges is either in the depth-first search tree or is a back edge.</p>\n\n<p>Now, from intuition and in class lectures by Steven Skiena, I know that the above holds true, since it dives all the way down, and then throw a rope back to a previous vertex. I also know that DFS is great in finding cycles.</p>\n\n<p>However, my problem here is that I don't know how to <em>prove</em> that the edge is either a tree edge or a back edge.</p>\n", 'ViewCount': '1467', 'Title': 'Why does DFS only yield tree and back edges on undirected, connected graphs?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T09:39:45.153', 'LastEditDate': '2013-04-21T14:32:57.923', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><graphs><algorithm-analysis><graph-traversal>', 'CreationDate': '2013-04-20T19:03:21.160', 'FavoriteCount': '1', 'Id': '11438'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Pebbling is a solitaire game played on an undirected graph $G$ , where\n  each vertex has zero or more pebbles. A single pebbling move consists\n  of removing two pebbles from a vertex $v$ and adding one pebble to an\n  arbitrary neighbor of $v$ . (Obviously, the vertex v must have at\n  least two pebbles before the move.) The PebbleDestruction problem\n  asks, given a graph $G = ( V; E )$ and a pebble count $p ( v )$ for\n  each vertex $v$ , whether there is a sequence of pebbling moves that\n  removes all but one pebble. Prove that PebbleDestruction is\n  NP-complete.</p>\n</blockquote>\n\n<p>First, I show that it is in NP since I can verify the solution in polynomial time, tracing back the pebble count from just one pebble.</p>\n\n<p>Next, what are some ideas on which problems to use as the basis for a polynomial-time reduction?</p>\n\n<p>Would something like vertex cover work? Or a vertex cover of different sizes? </p>\n\n<p>If so, how can it handle the varying number of pebbles on each move?</p>\n\n<p>Thank You.</p>\n\n<p>From: <a href="http://courses.engr.illinois.edu/cs473/sp2011/hw/disc/disc_14.pdf" rel="nofollow">http://courses.engr.illinois.edu/cs473/sp2011/hw/disc/disc_14.pdf</a></p>\n', 'ViewCount': '199', 'Title': 'Pebbling Problem', 'LastEditorUserId': '903', 'LastActivityDate': '2013-07-09T16:23:27.433', 'LastEditDate': '2013-05-01T01:25:01.613', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7812', 'Tags': '<algorithms><graph-theory><np-complete>', 'CreationDate': '2013-04-20T21:30:03.620', 'FavoriteCount': '2', 'Id': '11443'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to solve a system of inequalities in the following form:\n$\\ x_i - x_j \\leq w $</p>\n\n<p>I know these inequalities can be solved using <code>Bellman-Ford</code> algorithm. But there is also another condition. I want to find the solution that maximizes $\\ x_n - x_1$</p>\n\n<p>As far as I know the default <code>Bellman-Ford</code> algorithm minimizes it. How do I do that?</p>\n', 'ViewCount': '405', 'Title': 'Solving system of linear inequalities', 'LastActivityDate': '2013-08-26T17:37:29.993', 'AnswerCount': '4', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'Kia.celever', 'PostTypeId': '1', 'Tags': '<algorithms><linear-algebra><linear-programming><shortest-path>', 'CreationDate': '2013-04-19T09:14:20.977', 'Id': '11445'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '5058', 'Title': 'Quicksort Partitioning: Hoare vs. Lomuto', 'LastEditDate': '2013-04-21T15:09:19.573', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '6728', 'FavoriteCount': '4', 'Body': '<p>There are two quicksort partition methods mentioned in Cormen:</p>\n\n<pre><code>Hoare-Partition(A, p, r)\nx = A[p]\ni = p - 1\nj = r + 1\nwhile true\n    repeat\n        j = j - 1\n    until A[j] &lt;= x\n    repeat\n        i = i + 1\n    until A[i] &gt;= x\n    if i &lt; j\n        swap( A[i], A[j] )\n    else\n        return j\n</code></pre>\n\n<p>and:</p>\n\n<pre><code>Lomuto-Partition(A, p, r)\nx = A[r]\ni = p - 1\nfor j = p to r - 1\n    if A[j] &lt;= x\n        i = i + 1\n        swap( A[i], A[j] )\nswap( A[i +1], A[r] )\nreturn i + 1\n</code></pre>\n\n<p>Disregarding the method of choosing the pivot, in what situations is one preferable to the other?  I know for instance that Lomuto preforms relatively poorly when there is a high percentage of duplicate values ( i.e. where say more than 2/3rds the array is the same value ), where as Hoare performs just fine in that situation.</p>\n\n<p>What other special cases make one partition method significant better than the other?</p>\n', 'Tags': '<algorithms><sorting><quicksort>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T07:17:36.723', 'CommentCount': '2', 'AcceptedAnswerId': '11550', 'CreationDate': '2013-04-21T08:02:32.013', 'Id': '11458'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is NOT HW, this is from Skienas book, and I just couldn't solve it at all.</p>\n\n<p>Please give me a hand here, in understanding and solving it, thanks.</p>\n\n<p>Let G = (V, E) be a binary tree. The distance between two vertices in G is the length of the path connecting these two vertices, and the diameter of G is the maximal distance over all pairs of vertices. Give a linear-time algorithm to find the diameter of a given tree. (*)</p>\n\n<p>I figured I'd do a DFS, and increment on each node in terms of the depth of the tree</p>\n", 'ViewCount': '51', 'ClosedDate': '2013-04-21T23:06:21.807', 'Title': 'LInear time algorithm to find the diameter of a tree', 'LastEditorUserId': '139', 'LastActivityDate': '2013-04-21T23:43:53.940', 'LastEditDate': '2013-04-21T19:20:21.703', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><search-problem>', 'CreationDate': '2013-04-21T17:47:36.077', 'Id': '11470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been trying for a while now to find a solution for the problem in the title: determining if a number is perfect using a Turing Machine. I only had one class on the TM and while I did "get" how it works, this particular algorithm is being really hard for me to develop.</p>\n\n<p>The algorithm I\'m trying to implement on the TM is basically this (on C, returns <code>true</code> iff <code>n</code> is a perfect number):</p>\n\n<pre><code>int main(int n) {\n  int i=1, sum=0;\n\n  while ( n &gt; i ) {\n    if ( n % i == 0 ) {\n      sum = sum + i;\n    }\n    i++;\n  }\n\n  return sum == n\n}\n</code></pre>\n\n<p>The tough part for me right now is the <code>while(n&gt;i)</code> loop and the <code>n%i</code> inside it.</p>\n\n<p>Since I already have a program that does <code>a%b</code>, I was trying to build the TM graph around it, but I\'m not sure it\'s the best idea, specially since the <code>b</code> on this case changes on every iteration. The software I\'m using to simulate the TM is called JFlap.</p>\n\n<p>The algorithm on table or graph form would be perfect.</p>\n', 'ViewCount': '366', 'Title': 'Algorithm to determine if a number is perfect on a Turing Machine', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-22T16:39:50.503', 'LastEditDate': '2013-04-22T11:20:04.560', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7833', 'Tags': '<algorithms><turing-machines><decision-problem><integers>', 'CreationDate': '2013-04-22T00:23:08.297', 'Id': '11481'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p><em>First off, I am not sure if this is the correct stackexchange site to ask this question on, so moderators can feel free to move it.</em></p>\n\n<p>I am working on an application that identifies an object in an image.  For example, let's say that the object is an apple, and the apple can be green, red, or both.</p>\n\n<p>My plan is to create an array the same size as the image, then scan through the image, pixel by pixel, and if the current pixel is within my range of colors (green-ish or red-ish), add a 1 to the corresponding location in the array, else add a 0.</p>\n\n<p>After this scan through the array, I will have an array of 1s and 0s, which will hopefully contain a concentration of 1s, representing the apple.</p>\n\n<hr>\n\n<p>I am a computer science student, but have yet to take an AI class.  So my question is:  is this a good way to go about doing this?  Or are there more established AI methods (or algorithms) for identifying objects in an image based on color?</p>\n\n<p>EDIT:  I should clarify that the problem isn't identifying if the image contains an apple, the problem is identifying where the apple is in the image.</p>\n", 'ViewCount': '84', 'Title': 'Identifying an object in an image based on color (AI ?)', 'LastActivityDate': '2013-04-22T20:24:16.660', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '11499', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7838', 'Tags': '<algorithms><artificial-intelligence><image-processing>', 'CreationDate': '2013-04-22T15:04:35.920', 'Id': '11489'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>From Skiena's book:</p>\n\n<p>Let $G = (V,E,w)$ be a directed weighted graph such that all the weights are positive. Let $v$ and $u$ be two vertices in $G$ and $k \\leq |V|$ be an integer. Design an algorithm to find the shortest path from $v$ to $u$ that contains exactly $k$ edges. Note that the path need not be simple.</p>\n\n<p>This is <strong>not homework</strong>, its me preparing for an interview. I have no clue how to approach this.</p>\n", 'ViewCount': '614', 'Title': 'Shortest path with exactly $k$ edges', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-05-23T14:44:05.013', 'LastEditDate': '2013-05-23T14:44:05.013', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2013-04-22T23:22:16.367', 'FavoriteCount': '1', 'Id': '11503'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Can someone give me a hand here, I am new to backtracking, and preparing for an interview. I couldn't even attempt this question, please help.</p>\n\n<p>Describe a back tracking algorithm for efficiently listing all k-element subsets of <code>n</code> items.</p>\n\n<p>For <code>n = 5</code> the 3 element subsets are <code>(1,2,3), (1,2,4), (1,2,5), (1,3,4), (1,3,5), (1,4,5), (2,3,4), (2,3,5), (2,4,5), (3,4,5)</code></p>\n\n<p>In particular, I am interesting in first describing the solution vector representation to use, and then how I would partition the work among construct-candidates, is-a-solution and process-solution functions.</p>\n", 'ViewCount': '386', 'Title': 'Backtracking for listing k elements', 'LastActivityDate': '2013-04-23T03:54:03.443', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><algorithm-analysis><backtracking>', 'CreationDate': '2013-04-23T02:18:34.563', 'Id': '11507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider this problem: </p>\n\n<blockquote>\n  <p>Given an undirected graph $G = (V, E)$, find $G' = (V', E')$ such that:</p>\n  \n  <ol>\n  <li>$G'$ is an induced subgraph of $G$</li>\n  <li>$G'$ has no 3-cliques</li>\n  <li>$|V'|$ is maximal</li>\n  </ol>\n</blockquote>\n\n<p>So the least number of vertices must be eliminated from $G$ so that 3-cliques are eliminated.</p>\n\n<p>An equivalent problem would be to find a 2-coloring for $G$ such that if $(v_1, v_2, v_3) \\in V$ and $((v_1, v_2), (v_2, v_3), (v_3, v_1)) \\in V$, </p>\n\n<ol>\n<li><p>$(v_1.color == v_2.color \\wedge v_2.color == v_3.color \\wedge v_3.color == v_1.color) = False$</p></li>\n<li><p>The (absolute) difference between the number of nodes with color 1 and the number of nodes with color 2 is maximal.</p></li>\n</ol>\n\n<p>Can anyone think of a polynomial-time algorithm to solve one of these problems?</p>\n", 'ViewCount': '155', 'Title': 'Finding the largest 3-clique-free induced subgraph', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-24T10:36:57.313', 'LastEditDate': '2013-04-24T06:13:48.183', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '11534', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '3101', 'Tags': '<algorithms><graph-theory><graphs><optimization>', 'CreationDate': '2013-04-23T15:30:56.217', 'Id': '11518'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the dataset which you can find <a href="https://dl.dropboxusercontent.com/u/8546316/Dataset.csv" rel="nofollow">here</a>, containing many different characteristics of different houses, including their types of heating, or the number of adults and children living in the house. In total there are about 500 records. I want to use an algorithm, that can be trained using the dataset above, in order to be able to predict the electricity consumption of a house that is not in the set.</p>\n\n<p>I have tried every possible machine learning algorithm (using weka) (linear regression, SVM etc) . However I had about 350 mean absolute error, which is not good. I tried to make my data to take values from 0 to 1, or to delete some characteristics. I did not managed to find some good results.</p>\n\n<p>I also tried to use R tool, and I did not have good results either...</p>\n\n<p>I would be very grateful, if someone could give me some advice, or if you could examine a little the dataset and run some algorithms on it. What type of preprocessing should I use, and what type of algorithm?</p>\n\n<p>I have posted a <a href="http://cs.stackexchange.com/questions/10392/using-the-appropriate-machine-learning-algorithm">similar question</a> last month, but I did not get any useful answers.</p>\n', 'ViewCount': '132', 'Title': 'Predicting energy consumption of households', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-24T06:43:43.053', 'LastEditDate': '2013-04-24T06:43:43.053', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7196', 'Tags': '<algorithms><machine-learning><statistics>', 'CreationDate': '2013-04-23T22:37:26.683', 'Id': '11527'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Been stuck on this for a while, would really appreciate some help:</p>\n\n<blockquote>\n  <p>Suppose you are given an array A[1...n] of sorted integers that has been circularly shifted k positions to the right. For example, [35,42,5,15,27,29] is a sorted array that has been circularly shifted k = 2 positions, while  [27,29,35,42,5,15] has been shifted k = 4 positions. Give an algorithm for finding the maximum element in A that runs in O(log n) time.</p>\n</blockquote>\n\n<p>The elements in A are distinct.</p>\n\n<hr>\n\n<p>I understand that to achieve O(log n) time I'll probably have to search through the list by starting at the middle, and then going left or right, then splitting the list in half over and over, but I'm not sure how to attack it beyond that.</p>\n", 'ViewCount': '1137', 'Title': 'Find maximum element in sorted arrays in logarithmic time', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T19:54:26.320', 'LastEditDate': '2013-04-25T07:38:48.193', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '11547', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7881', 'Tags': '<algorithms><search-algorithms><arrays>', 'CreationDate': '2013-04-25T00:01:42.933', 'Id': '11545'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From graph isomorphism, we know that two graphs A and B are isomorphic if there is a permutation matrix P such that \n$A = P \\times B \\times P^{-1}$</p>\n\n<p>So, to solve the problem, if two graphs are isomorphic, we need to find such a permutation matrix P. The problem is believed to be NP (and NP complete for the case of subgraph isomorphism). However, I found an example to solve for P which seemed promising to me and can be found in \n<a href="http://en.wikipedia.org/wiki/Permutation_matrix" rel="nofollow">http://en.wikipedia.org/wiki/Permutation_matrix</a>\nin section: solving for P. </p>\n\n<p>The confusion I have now is, does that work for larger matrices? very large? am I right the above equation is hard to solve and can be candidate for a cryptographic system?</p>\n', 'ViewCount': '112', 'Title': 'How hard is it to solve for $P$ in $A = PBP^{-1}$?', 'LastEditorUserId': '755', 'LastActivityDate': '2013-04-26T23:40:13.320', 'LastEditDate': '2013-04-25T22:09:10.967', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '7', 'OwnerDisplayName': 'boFatom', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><cryptography><graph-isomorphism>', 'CreationDate': '2013-04-16T21:10:39.030', 'Id': '11553'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Recently, I came across <a href="http://www.spoj.com/problems/HANOI/">this problem</a>, a variation of <a href="https://en.wikipedia.org/wiki/Towers_of_hanoi">towers of hanoi</a>.</p>\n\n<p>Problem statement:</p>\n\n<blockquote>\n  <p>Consider the folowing variation of the well know problem Towers of\n  Hanoi:</p>\n  \n  <p>We are given $n$ towers and m disks of sizes $1,2,3,\\dots,m$ stacked on some\n  towers. Your objective is to transfer all the disks to the $k^{\\text{th}}$ tower\n  in as few moves as you can manage, but taking into account the\n  following rules:</p>\n  \n  <ul>\n  <li>moving only one disk at a time, </li>\n  <li>never moving a larger disk one onto a\n  smaller one, </li>\n  <li>moving only between towers at distance at most $d$. </li>\n  </ul>\n  \n  <p>(Limits in the original problem:\n  $3 \\le n \\le 1000$ and $m \\le 100$. Number of test cases $\\le 1000$.\n  You can assume that all the problems can be solved in not more than\n  $20000$ moves.)</p>\n</blockquote>\n\n<p>It\'s an interesting one. The classic towers of hanoi problem has one source, destination and temporary tower that is used to move the disks from source to destination. The problem pitched on that site basically has an initial and final configuration. </p>\n\n<p>How does one approach this problem? </p>\n', 'ViewCount': '496', 'Title': 'Towers of Hanoi but with arbitrary initial and final configuration', 'LastEditorUserId': '139', 'LastActivityDate': '2013-10-26T23:48:26.990', 'LastEditDate': '2013-04-26T15:39:15.363', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '7905', 'Tags': '<algorithms><combinatorics><recursion>', 'CreationDate': '2013-04-26T06:26:18.847', 'FavoriteCount': '1', 'Id': '11562'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am interested in Distributed Algorithms especially in communication in network with failures.</p>\n\n<p>I look for the proof of the following randomized algorithm of communication in network with failures. For me it seems like very general result in the communication, nevertheless I haven\u2019t found the proof yet. </p>\n\n<p><strong>Algorithm</strong>: Initially only vertex $v_0$ has the message, at the end of the algorithm every vertex of the network should have the message. </p>\n\n<p>On every round every vertex that has the message choice the neighbour randomly and sends it the message.</p>\n\n<p><strong>Assumptions</strong>: only $f$ failures might happen on the edges between the vertices.\n$T = O(\\log n)$ - time complexity and the entire network will know the message with high probability, when $f&lt;n/3$, where n - number of vertices.</p>\n\n<p>I will appreciate for link or reference to the paper.</p>\n', 'ViewCount': '79', 'Title': 'Algorithm of Communication with Failures', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T06:47:58.020', 'LastEditDate': '2013-05-28T06:47:58.020', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11593', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '4778', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2013-04-26T22:08:48.070', 'Id': '11590'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an undirected graph $G=(V,E)$ could we build a tree $T$ that approximates the distances from given vertex $r$ and the total weight, i.e. $\\forall x \\in V, d_G(r,x) \\le d_T(r,x) \\le 3 \\cdot d_G(r,x)$ and $w(T) \\le 3\\cdot w(\\text{MST}(G))$, where $\\text{MST}$ is the minimum spanning tree and $w(\\cdot)$ is the weight function i.e. $w:\\Bbb E \\to \\Bbb R^+$. $d_G(v,u)$ denotes the shortest path distance between $v$ and $u$ in $G$, and $d_T(v,u)$ is the shortest path distance between $v$ and $u$ in $T$.</p>\n\n<p>Could any one help me to understand how to build this tree and if there is any material that would help?</p>\n', 'ViewCount': '85', 'Title': 'Finding a tree that approximates the distances and total weights', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-28T14:06:38.913', 'LastEditDate': '2013-04-28T14:06:38.913', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '11617', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><approximation>', 'CreationDate': '2013-04-27T18:36:55.457', 'Id': '11607'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was looking over <a href="http://stackoverflow.com/q/352203/2327689">this question</a> requesting an algorithm to generate all permutations of a given string. A comment in the answer caught my eye:</p>\n\n<p><em>It might seem that it can take O(n) time per permutation, but if you think about it more carefully, you can prove that it takes only O(n log n) time for all permutations in total, so only O(1) -- constant time -- per permutation.</em></p>\n\n<p>This seemed strange to me because the best method I was aware of to generate all permutations of a string is in O(2^n) time. Looking through the other results, I came across a <a href="http://stackoverflow.com/a/7140205/2327689">response to a similar question</a> which states: <em>While it technically produces the desired output, you\'re solving something that could be O(n lg n) in O(n^n)</em></p>\n\n<p>I am aware of an algorithm to unrank permutations in O(n log n) time, but these responses seem to imply that all permutations in total can be generated in time O(n log n). Am I misunderstanding these responses?</p>\n', 'ViewCount': '1009', 'Title': 'Can all permutations of a set or string be generated in O(n log n) time?', 'LastActivityDate': '2013-04-28T05:47:47.137', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '11626', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7938', 'Tags': '<algorithms><complexity-theory><time-complexity>', 'CreationDate': '2013-04-27T20:47:12.143', 'Id': '11611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to solve fractional knapsack in linear time? I found this on <a href="https://www.google.com.sg/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CC0QFjAA&amp;url=http://algo2.iti.kit.edu/sanders/courses/algdat03/sol12.pdf&amp;ei=UJt8UZ_tGMHVrQfk1ICQDg&amp;usg=AFQjCNFKeWMLX_Gr2Pu-wS4zzjCT-ESCkg&amp;bvm=bv.45645796,d.bmk&amp;cad=rja" rel="nofollow">Google</a> but don\'t really understand it. </p>\n\n<ol>\n<li>Choose element $r$ at random from $R$ (set of profit/weight ratios)</li>\n<li>Determine\n<ul>\n<li>$R_1 = \\{ p_i / w_i | p_i / w_i &gt; r, for 1 \\leq i \\leq n \\}, W_1 = \\sum_{i \\in R_1} w_i$</li>\n<li>$R_2 = \\{ p_i / w_i | p_i / w_i = r, for 1 \\leq i \\leq n \\}, W_2 = \\sum_{i \\in R_3} w_i$</li>\n<li>$R_3 = \\{ p_i / w_i | p_i / w_i &lt; r, for 1 \\leq i \\leq n \\}, W_3 = \\sum_{i \\in R_3} w_i$</li>\n</ul></li>\n<li>if $W_1 &gt; W$\n<ul>\n<li>recurse $R_1$ and return computed solution</li>\n</ul></li>\n<li>else\n<ul>\n<li>while (there\'s space in knapsack and $R_2$ is not empty)\n<ul>\n<li>add items from $R_2$</li>\n</ul></li>\n<li>if (knapsack gets full)\n<ul>\n<li>return items in $R_1$ and items just added from $R_2$</li>\n</ul></li>\n<li>else \n<ul>\n<li>reduce knapsack capacity by $W_1 + W_2$</li>\n<li>recurse on $R_3$ and return items in $R_1 \\cup R_2$</li>\n<li>add items returned from recursive call </li>\n</ul></li>\n</ul></li>\n</ol>\n\n<p>I don\'t get how it works, what $R$ and $W$ are supposed to represent ... can someone explain? Or maybe if you have another algorithm to propose? </p>\n', 'ViewCount': '605', 'Title': 'Fractional Knapsack in linear time', 'LastEditorUserId': '683', 'LastActivityDate': '2013-04-28T04:43:06.850', 'LastEditDate': '2013-04-28T04:43:06.850', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3099', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms>', 'CreationDate': '2013-04-28T04:03:32.447', 'Id': '11620'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There is an infinite wall with a hole somewhere, you are placed on that wall at an unknown position. Let the distance between your initial position &amp; the hole be $x$. Find the average distance traveled in terms of $x$ until you find the hole. What's the complexity of this problem in terms of $x$ and how does an algorithm look like that solves it?</p>\n", 'ViewCount': '164', 'Title': 'Find a hole while travelling along an infinite wall', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-12-22T00:29:47.893', 'LastEditDate': '2013-06-03T22:05:07.963', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'OwnerDisplayName': 'bludger', 'PostTypeId': '1', 'OwnerUserId': '7946', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-04-26T15:39:13.763', 'Id': '11624'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have usually been using the Cormen algorithm format to teach some introductory courses in Programming. I mean something like this:</p>\n\n<pre><code>TreeSearch(k,n)\n1. if x==NIL or k==x.key\n2.     return x\n3. if k&lt;x.key\n4.     return TreeSearch(k.left,n)\n5. else return TreeSearch(k.right,n)\n</code></pre>\n\n<p>Actually I have not agree with a couple of lecturers in my institution that they insist to put the type of the variable that they are using in the algorithm. I mean, to do that, will it not be to make a bias toward the programming language and not to focus on the algorithm? For example what would happen if the student grab other programming language, like R or Python, that really do not care about the type of variable.</p>\n\n<p>The other issue that I have is how to represent OOP algorithms in a nice algorithmic way. For example when I make a constructor should I put something like:</p>\n\n<pre><code>Class: car\nAttributes: wheels\nConstructor car()\n</code></pre>\n\n<p>or something like</p>\n\n<pre><code>Class: car\nFunction car()\n</code></pre>\n\n<p>also when I come to the part of inheritance, one of my colleages put the word super() to define inheritance in an algorithmic way, but again I think that is too Java-way to do this part. Usually they teach in that way because the practical part is made in Java, but again I think that the algorithm should be more freely, directly towards the logic, and not to an specific programming language.</p>\n\n<p>Does anybody knows some standard to represent algorithms for OOP?</p>\n', 'ViewCount': '213', 'Title': 'How to represent OOP concepts in algorithms in a standard way?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-29T18:27:24.440', 'LastEditDate': '2013-04-29T18:27:24.440', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6430', 'Tags': '<algorithms><terminology><education><object-oriented><didactics>', 'CreationDate': '2013-04-29T13:30:18.693', 'Id': '11658'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of binary vectors where each vector represents one day of occupancy in a house and consists of 48 elements (each element for 30 minutes of the day). Each element can be 1 meaning that house was occupied and 0 for non occupied house.</p>\n\n<p>My task is to predict the next day based on the history of the same days (Monday from history of Mondays etc.). So far I am using hamming distance to find 5 most similar days in the history and from them I calculate the probabilities of the occupancy as a mean of those 5 numbers. When the probability is higher than some X, in my case 0.4, I predict it to be occupied.</p>\n\n<p>But there is definitely some more efficient way to do this, any algorithms that would capture the trend in the history?</p>\n', 'ViewCount': '24', 'Title': 'Predict binary occupancy vector from history of vectors', 'LastActivityDate': '2013-04-30T19:15:59.970', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7982', 'Tags': '<algorithms><modelling>', 'CreationDate': '2013-04-30T19:15:59.970', 'Id': '11681'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '156', 'Title': 'What type of formal notation is being used here to represent functional algorithms?', 'LastEditDate': '2013-05-03T02:16:49.393', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7129', 'FavoriteCount': '1', 'Body': '<p>Interested in learning more about algorithm design in functional programming, I picked up Andrew Bird\'s <a href="http://rads.stackoverflow.com/amzn/click/0521513383" rel="nofollow">Pearls of Functional Algorithm Design</a>.  I have experience with a number of programming languages, but my only experience with functional programming is in Scala.  I understood that I would have to pick-up Standard ML and Haskell from the description of the book, but when I started reading the first section, I wasn\'t familiar with some of the operators being used.</p>\n\n<p>Here are some examples of function definitions from the first chapter of the book (free to preview on Amazon):</p>\n\n<hr>\n\n<p><img src="http://i.stack.imgur.com/mVx5z.png" alt="weird syntax"></p>\n\n<p>I have seen "^" and "v" used to represent "and" and "or," but some of the other syntax (like <code>False (0,n)</code>) still throws me off.</p>\n\n<p><img src="http://i.stack.imgur.com/aSG9Z.png" alt="more weird syntax"></p>\n\n<p>In this one, I\'m not sure what the <code>accumArray(+)...</code> is referring to.  I\'m thinking it\'s like a fold method using addition, but I don\'t understand the rest of the line.</p>\n\n<p><img src="http://i.stack.imgur.com/JPWEZ.png" alt="kinda weird"></p>\n\n<p>Here, the author has done a good job of describing that \\\\ is <a href="http://en.wikipedia.org/wiki/Set_difference#Relative_complement" rel="nofollow">set difference</a> and the two vertical lines crossed with a horizontal one is <a href="http://en.wikipedia.org/wiki/Union_%28set_theory%29" rel="nofollow">union</a>.  However, I\'ve never seen anything like that union symbol before.</p>\n\n<hr>\n\n<p>I don\'t want to know what each of these examples means as much as I want to know <strong>what library of formal representation is Bird using to represent these algorithms</strong>, and also, if a specific programming language (Haskell/SML?) syntax is being used as well in conjunction with these special symbols.</p>\n', 'Tags': '<algorithms><formal-languages><functional-programming><notation>', 'LastEditorUserId': '7129', 'LastActivityDate': '2013-05-03T02:16:49.393', 'CommentCount': '5', 'AcceptedAnswerId': '11710', 'CreationDate': '2013-05-01T17:48:19.410', 'Id': '11707'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is well known that the efficiency of randomized algorithms (at least those in BPP and RP) depends on the quality of the random generator used. Perfect random sources are unavailable in practice. Although it is proved that for all $0 &lt; \\delta \\leq \\frac{1}{2}$ the identities BPP = $\\delta$-BPP and RP = $\\delta$-RP hold, it is not true that the original algorithm used for a prefect random source can be directly used also for a $\\delta$-random source. Instead, some simulation has to be done. This simulation is polynomial, but the resulting algorithm is not so efficient as the original one.</p>\n\n<p>Moreover, as to my knowledge, the random generators used in practice are usually not even $\\delta$-sources, but pseudo-random sources that can behave extremely badly in the worst case.</p>\n\n<p>According to <a href="http://en.wikipedia.org/wiki/Randomized_algorithm" rel="nofollow" title="Wikipedia">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.</p>\n</blockquote>\n\n<p>In fact, the implementations of randomized algorithms that I have seen up to now were mere implementations of the algorithms for perfect random sources run with the use of pseudorandom sources.</p>\n\n<p>My question is, if there is any justification of this common practice. Is there any reason to expect that in most cases the algorithm will return a correct result (with the probabilities as in BPP resp. RP)? How can the "approximation" mentioned in the quotation from Wikipedia be formalized? Can the deviation mentioned be somehow estimated, at least in the expected case? Is it possible to argue that a Monte-Carlo randomized algorithm run on a perfect random source will turn into a well-behaved stochastic algorithm when run on a pseudorandom source? Or are there any other similar considerations?</p>\n', 'ViewCount': '92', 'Title': 'Random generator considerations in the design of randomized algorithms', 'LastActivityDate': '2013-05-02T22:23:28.033', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11744', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2091', 'Tags': '<randomized-algorithms><randomness><pseudo-random-generators>', 'CreationDate': '2013-05-02T10:41:01.760', 'Id': '11726'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to reduce $MAX3SAT$ to $MAX2SAT$ ...<br>\nMAX-n-SAT : given $\\phi $ n-CNF formula and number k does $\\phi$ has an assignment that satisfy k clauses? </p>\n', 'ViewCount': '62', 'Title': 'reducing Max3SAT to Max2sat', 'LastActivityDate': '2013-05-02T21:51:27.307', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11742', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><np-complete><reductions><approximation>', 'CreationDate': '2013-05-02T20:59:20.690', 'Id': '11739'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The exercises in a textbook I studied asks about the best case for shell sort. I have scribbled a derivation for the same along the margins almost two years ago. Basically I don't know if this was my own derivation or one copied from an authoritative source. </p>\n\n<p>I have elaborated upon the same below. Could you let me know if the reasoning is right here?</p>\n\n<ul>\n<li>The least number of comparisons occur when the data is completely sorted.</li>\n<li>For a particular value of the increment, say, $h_i$, each of the $h_i$ sub-sequences require at most one less comparison than the number of elements in the sub-sequence(as insertion sort is used) which is,${N \\over h_i} - 1$ ,where N is the total number of data items.</li>\n<li>For the given data in this situation $h_i \\times \\left (N \\over h_i - 1 \\right ) = N - h_i$ number of comparisons are needed as there are $h_i$ sub-sequences.</li>\n<li>If the increment sequence selected is has $k$ increments(such that $h_k = 1$), the total number of comparisons required would be $C(N) \\ge (N - h_i) + (N - h_2) + ... + (N - h_k) = kN - \\sum h_i = O(N)$</li>\n</ul>\n", 'ViewCount': '369', 'Title': 'Best case analysis for shell sort', 'LastActivityDate': '2013-05-03T02:56:18.547', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2980', 'Tags': '<algorithms><data-structures><algorithm-analysis><sorting>', 'CreationDate': '2013-05-03T02:56:18.547', 'FavoriteCount': '1', 'Id': '11749'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to solve job assignment problem using <em>Hungarian algorithm of Kuhn and Munkres</em> in case when matrix is not square. Namely we have more jobs than workers.\nIn this case adding additional row is recommended to make matrix square.\nFor example in the following \n<a href="http://books.google.kz/books?id=yXYzLm1e_IUC&amp;pg=PA94&amp;lpg=PA94&amp;dq=job%20assignment%20problem%20matrix%20is%20non%20square&amp;source=bl&amp;ots=1LU3_P-Ss5&amp;sig=k4hlPV0GLjUDT9PxoLEsomt0K6Q&amp;hl=en&amp;sa=X&amp;ei=lFODUcW_FcKQOJfUgPgJ&amp;ved=0CFIQ6AEwBQ#v=onepage&amp;q=job%20assignment%20problem%20matrix%20is%20non%20square&amp;f=false" rel="nofollow">link</a>.</p>\n\n<p><img src="http://i.stack.imgur.com/tWzYi.png" alt="enter image description here">\nAnd here task IV is assumed to be done.\nBut in real we do not have man D. Who will actually do task IV?\nCan someone explain this phenomena?</p>\n\n<p>In general I want to complete all tasks by loading workers uniformly and get maximum cost.\nSo how to implement this task by using job assignment algorithm above?</p>\n', 'ViewCount': '122', 'Title': 'Job assignment problem', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-29T20:11:29.027', 'LastEditDate': '2013-11-29T20:11:29.027', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5207', 'Tags': '<algorithms><linear-programming><assignment-problem>', 'CreationDate': '2013-05-03T06:52:36.190', 'FavoriteCount': '1', 'Id': '11751'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If we have polynomial algorithm that $c$-approximation, $c&lt;\\frac{4}{3}$ for graphs that their chromatic number $\\geq k$ then $NP=P$, how to prove such statements?</p>\n\n<p>I also have some sort of explanation of this statement: It's NP-hard to separate between graphs that have chromatic number $k$ and chromatic number $c \\cdot k$ when $c&lt;\\frac{4}{3} \\quad \\forall k\\geq 3$ </p>\n", 'ViewCount': '80', 'Title': 'Hardness of approximation of the 3 colorability problem', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-09T01:58:07.277', 'LastEditDate': '2013-05-09T01:58:07.277', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11769', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><np-hard><approximation>', 'CreationDate': '2013-05-03T22:19:10.063', 'Id': '11766'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '133', 'Title': 'Sublinear query time for the $i$th element of an array after some additions?', 'LastEditDate': '2013-05-04T18:07:11.023', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8044', 'FavoriteCount': '1', 'Body': "<p>We are given an array $A[1..n]$ of integers, and an array of 3-tuples known as queries. The query tuples $(i,j,v)$ denote additions of an integer $v$ to the subarray of $A[i..j]$. I'm interested in the query time of $A[k]$ for $1 \\leq k \\leq n$ between the queries.</p>\n\n<p>For example, let $A = [1,2,3,4]$, and let the queries be $Q = [[1,2,5],[2,3,6],[1,3,10]]$. After the second query has been processed, say I want to find the value of $A[3]$, which would be $3+5+6=14$. Does an algorithm exists to do this in less than linear time?</p>\n", 'Tags': '<algorithms><arrays>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-06T03:41:39.833', 'CommentCount': '2', 'AcceptedAnswerId': '11816', 'CreationDate': '2013-05-04T16:54:17.153', 'Id': '11777'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would I solve the following.</p>\n\n<p>An algorithm that is $O(n^2)$ takes 10 seconds to execute on a particular computer when n=100, how long would you expect to take it when n=500?</p>\n\n<p>Can anyone help me answer dis. </p>\n', 'ViewCount': '140', 'Title': 'Algorithm analysis question in growth of functions', 'LastEditorUserId': '6980', 'LastActivityDate': '2013-05-05T03:09:56.113', 'LastEditDate': '2013-05-05T03:09:56.113', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11782', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithms><time-complexity><asymptotics>', 'CreationDate': '2013-05-04T21:35:39.893', 'Id': '11781'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am looking for the paper : "B. Awerbuch, A. Baratz, and D. Peleg, E\ufb03cient broadcast and light-weight spanners, Manuscript, (1991)."<br>\nIt claims that we can build $(\\alpha ,1+\\frac{4}{\\alpha -1})-LAST$ where LAST hold for "Light Approximation Shortest path Tree"<br>\nif not available could any one explain the algorithms used there ?  </p>\n', 'ViewCount': '50', 'Title': 'Light approximation for shortest path tree', 'LastEditorUserId': '139', 'LastActivityDate': '2013-05-07T07:25:47.910', 'LastEditDate': '2013-05-07T07:25:47.910', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><reference-request><approximation>', 'CreationDate': '2013-05-04T22:40:46.600', 'Id': '11786'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can alpha-beta pruning be made safe for a chess program? Pruning away a certain set of nodes is safe when the tree is complete and it is known that no other new nodes can be found deeper in the tree, but obviously this is not the case in chess. If my program prunes away a move that looks bad because it sacrifices valuable material that may just be the sacrificial move that would have checkmated the opponent in another few additional ply (or just gain a material and/or tactical advantage)?</p>\n', 'ViewCount': '287', 'Title': 'Alpha-Beta pruning in chess?', 'LastEditorUserId': '114', 'LastActivityDate': '2013-05-06T05:01:20.767', 'LastEditDate': '2013-05-06T04:17:56.753', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'ioctlvoid', 'PostTypeId': '1', 'Tags': '<algorithms>', 'CreationDate': '2013-05-05T01:11:29.200', 'Id': '11800'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>We're asked to prove the above mentioned lemma but I having a hard time proving this rigorously.</p>\n\n<p>We did prove that given $n$ values AVL's height is $\\Theta\\left (\\log \\left ( n \\right ) \\right )$ So I thought that after inserting a $\\frac{n}{2}$ values the height of the tree will be at least $\\Theta\\left (\\log \\left ( \\frac{n}{2} \\right ) \\right )$ which and because each isertion we make is now on a tree with at least $\\frac{n}{2}$ and insertion is $\\log \\left (h \\right ) $ where $h$ is the height of the tree.</p>\n\n<p>So for a function $F$ using the previous logic:</p>\n\n<p>$\\begin{align}  F &amp;= \\frac{n}{2} \\times \\log \\left (h \\right ) \n\\\\&amp; \\geq \\frac{n}{2} \\times \\log \\left (\\frac{n}{2} \\right ) \n\\\\&amp;=\\Omega\\left( n\\log \\left (n \\right )  \\right)\n\\end{align}$</p>\n\n<p>But I have a few issues with this </p>\n\n<ul>\n<li>This does feel fishy to me don't know why but it doesn't feel like a good well defined calculus proof :)</li>\n<li>I'm not sure which way to take it in order to prove the upper boud i.e $\\mathcal{O}$</li>\n</ul>\n\n<p>If I haven't given all the required information I'd be glad to.</p>\n", 'ViewCount': '78', 'Title': 'Prove that inserting $n$ sorted values in to an AVL using AVL insertion is $\\Theta\\left (n \\log \\left ( n \\right ) \\right )$', 'LastEditorUserId': '31', 'LastActivityDate': '2013-05-05T16:35:06.797', 'LastEditDate': '2013-05-05T16:35:06.797', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11808', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8004', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'CreationDate': '2013-05-05T15:30:17.330', 'Id': '11807'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an algorithm (or hint where to start), for Toads and Frogs Game. What I am interested in is not how to solve the problem (it\'s NP-hard), but <strong>how to plan one player\'s moves</strong>. I.e. how to design a computer player (AI), which could win against another player (another program or human player). I was looking for some clues but with no luck so far, there\'s not much about it on the Web.</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Toads_and_Frogs_%28game%29" rel="nofollow">Link</a> to game description on Wikipedia.</p>\n\n<p>And <a href="http://nrich.maths.org/1246" rel="nofollow">here</a> you can play the game. Please bear in mind, that starting positions may not be that straightforward. They may be mixed up from the very start.</p>\n', 'ViewCount': '323', 'Title': 'Toads and frogs game algorithm', 'LastEditorUserId': '8067', 'LastActivityDate': '2013-05-06T16:44:20.893', 'LastEditDate': '2013-05-06T16:44:20.893', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8067', 'Tags': '<algorithms><game-theory>', 'CreationDate': '2013-05-06T09:27:38.977', 'FavoriteCount': '1', 'Id': '11824'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given we change the rule to: </p>\n\n<blockquote>\n  <p>$-s \\ \\ \\leq$ height(left-subtree) - height(right-subtree) $\\leq \\ \\ s$</p>\n</blockquote>\n\n<p>I was wandering whether it's possible and how would it affect the trees' height, would it still be logarithmic? </p>\n\n<p>Would the exact same balancing techniques work? (if we took those methods from a normal AVL and try to convert our modified AVL to a normal AVL running from down to top or to down).</p>\n\n<p>I've tired drawing some schematics in order to find out what would be the minimal number of nodes $m$ for some tree $T$ with height $h$ like we did with a regular AVL but I had a real hard time formalizing it.</p>\n", 'ViewCount': '54', 'Title': "Changing AVL's balance factor to some other $s>2 \\in \\mathbb{N}$", 'LastEditorUserId': '8004', 'LastActivityDate': '2013-05-06T17:15:16.980', 'LastEditDate': '2013-05-06T17:15:16.980', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11834', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8004', 'Tags': '<algorithms><data-structures><binary-trees><trees>', 'CreationDate': '2013-05-06T14:22:34.710', 'Id': '11832'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What's the time complexity of the clique problem for input graphs where each connected component has size at most $3\\log|V|$? Is it in P?</p>\n", 'ViewCount': '69', 'Title': "Clique in P when the input graph's connected components have at most $3\\log|V|$ vertices?", 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-06T21:59:39.687', 'LastEditDate': '2013-05-06T21:59:39.687', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11839', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8072', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-05-06T19:08:54.887', 'Id': '11838'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to understand why quicksort using Lomuto partition and a fixed pivot is performing erratically, but overall poorly, on randomly generated inputs.  I\'m thinking that even though the inputs are randomly generated, there may be allot of order to the sequences, but I\'m not sure how to measure the level of disorder in the sequences.  I thought about using the number of inversions, but I saw from <a href="http://cs.stackexchange.com/q/11836/6728">this other question I asked</a> that that\'s not really a good measure in this case.</p>\n\n<p>The reason I suspect that my random sequences have allot of "order" to them is that randomizing the pivot fixes the performance problem.  But theoretically there shouldn\'t be an performance problem on these supposedly "random" input sequences.</p>\n', 'ViewCount': '107', 'Title': 'What Measure of Disorder to use when Analysing Quicksort', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-07T11:07:28.523', 'LastEditDate': '2013-05-07T11:07:28.523', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><sorting>', 'CreationDate': '2013-05-07T06:08:11.727', 'Id': '11846'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We wish to manufacture n distinct hardware items. Each item needs to go through 3 stages of processing. The first stage called design can only be performed by our master designer who works by starting work on an item, designing it through to the end, and only then starting on another item. The remaining two phases, called assembly and testing, are outsourced and for each of them there is an infinite supply of people who can perform the corresponding task as soon as it is assigned to them. Naturally, each item first needs to be designed, then assembled, and then tested.</p>\n\n<p>Each item a requires d\u2090 hours of design, a\u2090 hours of assembly, and t\u2090 hours of testing. We are interested in determining the order in which we should design the items so that we minimize the time by which all items will be ready. For example, if we only had two pieces and we first designed item 1 and then designed item 2, the time by which both items would be finished is</p>\n\n<p>max{d\u2081 + a\u2081 + t\u2081, d\u2081 + d\u2082 + a\u2082 + t\u2082}.</p>\n\n<p>If, alternatively, we first design item 2 and then item 1, the time by which both items would</p>\n\n<p>be finished is</p>\n\n<p>max{d\u2082 + a\u2082 + t\u2082, d\u2082 + d\u2081 + a\u2081 + t\u2081}.</p>\n\n<p>Give a O(n log n) algorithm which takes as input n triples (d, a, t) and determines the optimal design order.</p>\n\n<p>Thanks for your help guys!</p>\n', 'ViewCount': '80', 'Title': 'Interval Scheduling Optimization type of Problem, optimal order of manufacture', 'LastActivityDate': '2013-05-07T06:43:02.403', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8075', 'Tags': '<algorithms><asymptotics><optimization><scheduling>', 'CreationDate': '2013-05-07T06:43:02.403', 'Id': '11847'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Lets say we need a function to add two numbers and another function to multiply two numbers.</p>\n\n<p>To take a trivial example, consider the function $F(a, b, c, d) = a \\cdot (c+d) + b \\cdot (c\\cdot d)$\nIf $a=1$ and $b=0$, then $F$ adds $c$ and $d$.  If $a=0$ and $b=1$, then $F$ multiplies $c$ and $d$.</p>\n\n<p>So to evaluate both add and mult I can have only one function $F$ with variable inputs.</p>\n\n<p>Can we have a general function that can evaluate any arbitrary function ? of course with an upper bound on the complexity ? If yes how do we prove in theory that such a general function is possible ? What is this called in theory ? </p>\n', 'ViewCount': '44', 'Title': 'Can we have a general function of any function this way?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-23T22:57:50.783', 'LastEditDate': '2013-05-23T22:57:50.783', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12230', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4421', 'Tags': '<algorithms><terminology>', 'CreationDate': '2013-05-07T17:54:12.650', 'Id': '11858'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $N(G)$ be the null graph. What's the number of vertex cover for this graph? I wanted to modify the reduction from SAT to vertex cover by adding vertices that are not connect to any vertices.</p>\n", 'ViewCount': '118', 'Title': "What's the vertex cover of the null graph?", 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-08T19:06:28.147', 'LastEditDate': '2013-05-08T17:50:41.690', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11873', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<algorithms><approximation>', 'CreationDate': '2013-05-07T21:28:32.327', 'Id': '11865'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am struggling to calculate the lower bounds of an algorithm. What is the right way to proceed.</p>\n\n<p>For eg, I have the following algorithm</p>\n\n<pre><code>For i=1, 2,...,n\n    For j=i+1, i+2,...,n\n        Add up array entries A[i] through A[j]\n    Store the result in B[i, j] Endfor\nEndfor\n</code></pre>\n\n<p>How do I calculate the lower bounds of this algorithm</p>\n', 'ViewCount': '56', 'Title': 'Finding the lower bounds of an algorithm', 'LastActivityDate': '2013-05-08T00:11:49.063', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8086', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-05-07T22:57:21.287', 'Id': '11868'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have this confusion related to solving this problem</p>\n\n<p>You\u2019vebeenworkingwithsomephysicistswhoneedtostudy,aspartof their experimental design, the interactions among large numbers of very small charged particles. Basically, their setup works as follows. They have an inert lattice structure, and they use this for placing charged particles at regular spacing along a straight line. Thus we can model their structure as consisting of the points $ \\{1, 2, 3, \\cdots , n\\}$ on the real line; and at each of these points $j$, they have a particle with charge $q_j$. (Each charge can be either positive or negative.)</p>\n\n<p>They want to study the total force on each particle, by measuring it and then comparing it to a computational prediction. This computational part is where they need your help. The total net force on particle $j$, by Coulomb\u2019s Law, is equal to</p>\n\n<p>$F_j = \\sum_{i&lt;j}\\frac{Cq_iq_j}{(j-i)^2} - \\sum_{i&gt;j}\\frac{Cq_iq_j}{(j-i)^2}$</p>\n\n<p>They\u2019ve written the following simple program to compute $F_j$ for all $j$:</p>\n\n<pre><code>For j = 1, 2,...,n \n   Initialize Fj to 0 \n   For i = 1, 2, ..., n\n\ufffc     If i &lt; jthen\n       Add Cqiqj/(j-i)^2 to Fj\n     Elseif i &gt; j then\n       \ufffcAdd \u2212Cqiqj/(j\u2212i)^2 to Fj\n     Endif \n   Endfor\n   Output Fj \nEndfor\n</code></pre>\n\n<p>It\u2019s not hard to analyze the running time of this program: each invocation of the inner loop, over i, takes $O(n)$ time, and this inner loop is invoked $O(n)$ times total, so the overall running time is $O(n^2)$.</p>\n\n<p>The trouble is, for the large values of n they\u2019re working with, the pro- gram takes several minutes to run. On the other hand, their experimental setup is optimized so that they can throw down n particles, perform the measurements, and be ready to handle n more particles within a few sec- onds. So they\u2019d really like it if there were a way to compute all the forces $F_j$ much more quickly, so as to keep up with the rate of the experiment.</p>\n\n<p>Help them out by designing an algorithm that computes all the forces $Fj$ in $O(n \\log n)$ time.</p>\n\n<p>I am sure that this problem is solved by convolution which takes time $O(n \\log n)$. However, I am not being able to proceed and see how it's converted to a problem related to convolution. Any suggestions?</p>\n", 'ViewCount': '429', 'Title': 'Solving a problem related to convolution', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-06-07T04:33:38.667', 'LastEditDate': '2013-05-08T02:45:27.943', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8086', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-05-08T02:20:07.617', 'FavoriteCount': '1', 'Id': '11875'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given $n$ points in $\\mathbf{R}^2$, define the optimal Euclidean Steiner tree to be a minimum (Euclidean) length tree containing all $n$ points and any other subset of points from $\\mathbf{R}^2$.\nProve that each of the additional points must have degree 3, with all three angles being $120^\\circ$.</p>\n', 'ViewCount': '113', 'Title': 'Euclidean Steiner Tree Question in Approximation Algorithms', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-05-08T07:18:28.980', 'LastEditDate': '2013-05-08T07:18:28.980', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11881', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7300', 'Tags': '<algorithms><algorithm-analysis><computational-geometry><approximation><trees>', 'CreationDate': '2013-05-08T06:34:08.107', 'Id': '11880'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>During a programming contest I was asked to find just smallest prime number to given number N.\nAs Sieve cannot be used and brute force also doesn't work.</p>\n\n<p>So, I was wondering is there any other faster implementation.</p>\n\n<p>Here N -> (2, 10^18).</p>\n", 'ViewCount': '162', 'Title': 'What is the fastest to find just smallest prime number to a given number N where N can be as large as 10^18?', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-24T14:07:02.130', 'LastEditDate': '2014-03-24T14:07:02.130', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '11884', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7711', 'Tags': '<algorithms><primes>', 'CreationDate': '2013-05-08T08:41:37.337', 'Id': '11883'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I need to find largest value for $\\frac{\\phi(i)}{i}$ for $i \\in (2, N)$ where $N$ can be as large as $10^{18}$.</p>\n\n<p>I tried this approach , but is too slow.\nFinding the just smallest prime number to $N$, as its $\\frac{\\phi(i)}{i}$ value is $\\frac{i-1}{i}$, which is maximum in the range. (See <a href="http://math.stackexchange.com/questions/381053/maximum-of-frac-phiii">Maximum of \u03d5(i)i\\frac{\\phi(i)}i</a>)</p>\n\n<p>So, I was wondering if there is any other faster way to find the maximum value. More precisely I need the value of i where $\\frac{\\phi(i)}{i}$ is maximum.</p>\n', 'ViewCount': '76', 'Title': 'Finding largest value for $\\frac{\\phi(i)}{i}$ for $i \\in (2, N)$', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-08T15:05:00.257', 'LastEditDate': '2013-05-08T14:20:51.633', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7711', 'Tags': '<algorithms><arithmetic><primes>', 'CreationDate': '2013-05-08T09:29:46.807', 'Id': '11885'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A few years ago I participated in  German highschool computer science competition. One of the <a href="http://www.bundeswettbewerb-informatik.de/fileadmin/templates/bwinf/aufgaben/bwinf28/Aufgaben282.pdf" rel="nofollow">problems</a> was this (slightly abbreviated):</p>\n\n<blockquote>\n  <p>A somewhat unusual electronic lock consists of $n$ switches than can be turned on ($1$) or off ($0$). For economic reasons only $k$ of these switches are actually connected to the lock, the rest are dummies. It\'s impossible to tell these apart. The lock opens when all $k$ connected switches are set to the right combination, regardless of the setting of the dummy switches.</p>\n  \n  <p>If you want to open the lock without knowing the right combination, you could repeatedly setup the switches into different positions and then press open until the lock actually opens. Construct an algorithm that for a given pair $(n,k)$ finds a set of setups for all keys such that regardless of which keys are connected to the lock and of regardless what combination of those actually opens the lock, one of the setups will open the lock. Try to construct an algorithm that computes a minimal set.</p>\n</blockquote>\n\n<p>The solution that I coded (which is more or less equal to the <a href="http://www.bundeswettbewerb-informatik.de/fileadmin/templates/bwinf/aufgaben/bwinf28/Loesungshinweise282_2up.pdf" rel="nofollow">reference solution</a>) works like this:</p>\n\n<ol>\n<li>Let $S$ be the set of all setups in the output.</li>\n<li>Generate $M$, a set of $m$ setups randomly where $m$ is a customizeable parameter</li>\n<li>For each element of $M$, compute how many combinations it covers that were not already covered</li>\n<li>Add the element of $M$ that covers the most new solutions to $S$</li>\n<li>For each element of $S$, check if if you remove it from $S$, the number of solutions covered by $S$ would fall. If that is not the case, remove that setup from $S$.</li>\n<li>If $S$ covers all possible combinations, terminate.</li>\n</ol>\n\n<hr>\n\n<p>Can somebody explain me better ways to solve this problem? Is it possible to find a minimal set of setups in reasonable aka polynomial time (the reference solution hints that this is not possible)?</p>\n', 'ViewCount': '125', 'Title': 'Efficiently finding key sets', 'LastEditorUserId': '139', 'LastActivityDate': '2013-05-09T04:02:57.443', 'LastEditDate': '2013-05-09T04:02:57.443', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2280', 'Tags': '<algorithms>', 'CreationDate': '2013-05-08T18:29:50.187', 'FavoriteCount': '1', 'Id': '11897'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was asked this question at an interview, and couldn't answer it, and would like to know how it is 'shown' that two Turing machines which accept the same language is undecidable. This is not a homework question!</p>\n", 'ViewCount': '482', 'Title': 'Show that it is undecidable if two Turing Machines accept the same language', 'LastEditorUserId': '6890', 'LastActivityDate': '2013-05-14T03:12:53.943', 'LastEditDate': '2013-05-09T20:06:15.133', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8104', 'Tags': '<algorithms><computability><finite-automata><undecidability>', 'CreationDate': '2013-05-09T17:18:46.500', 'Id': '11916'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to know which is the best way to find the longest common subsequence of two strings</p>\n', 'ViewCount': '147', 'Title': 'Find the longest subsequence of two strings', 'LastEditorUserId': '8105', 'LastActivityDate': '2013-05-10T12:19:08.163', 'LastEditDate': '2013-05-10T12:19:08.163', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8105', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming><subsequences>', 'CreationDate': '2013-05-10T01:58:55.797', 'FavoriteCount': '2', 'Id': '11924'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a directed weighted graph $D = (V, A, w)$ with weight function $w \\colon A \\to [0,\\infty)$ and a source vertex $s$.  How can I find, for each vertex $v \\in V$ a shortest (with respect to $w$) cycle that contains both $v$ and $s$ (it is allowed to repeat the same arc in a cycle).</p>\n', 'ViewCount': '131', 'Title': 'Finding cycles in a graph', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-05-12T14:14:47.620', 'LastEditDate': '2013-05-10T16:04:22.047', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'OwnerDisplayName': 'user8113', 'PostTypeId': '1', 'Tags': '<algorithms>', 'CreationDate': '2013-05-10T12:43:29.140', 'Id': '11932'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an expression $$Ax+By+Cz.$$ where $A$, $B$ and $C$ are positive constants $\\ge1$. The variables $x$, $y$ and $z$ are non-negative integers. I am also given a number $T$. </p>\n\n<p>I want to find the largest integer value such that it is less than $T$ and not satisfied by $Ax+By+Cz$, how can I do it without using brute force. </p>\n', 'ViewCount': '82', 'Title': 'Finding the required value of an algebric expression', 'LastEditorUserId': '8110', 'LastActivityDate': '2013-05-14T02:02:03.263', 'LastEditDate': '2013-05-12T21:43:10.770', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><optimization><linear-programming><linear-algebra>', 'CreationDate': '2013-05-10T17:11:35.057', 'Id': '11940'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is meant by relinquishing criteria for a scheduling algorithm such as First in fist out? I know this algorithm choses the first process to be executed by the scheduler.  Is it non pre-emptive  relinquishing criteria? As in a context switch only occurs when the proccess is complete or when that process gives up CPU time. </p>\n', 'ViewCount': '51', 'Title': 'Relinquishing criteria for operating systems', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-12T14:35:35.663', 'LastEditDate': '2013-05-12T14:35:35.663', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8054', 'Tags': '<algorithms><operating-systems>', 'CreationDate': '2013-05-11T16:31:30.280', 'Id': '11952'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have an expression $$Ax+By+Cz.$$ where $A$, $B$ and $C$ are positive constants $\\ge1$. The variables $x$, $y$ and $z$ are non-negative integers. I am also given a number $T$. </p>\n\n<p>I want to find the largest integer value such that it is less than $T$ and not satisfied by $Ax+By+Cz$, how can I do it without using brute force. </p>\n\n<p>I can use LinearProgramming but that will give me the value that is satisfied by $Ax+By+Cz$ such that it is less than T. But I want to find the largest value which is less than T, but doesn't belong to $Ax+By+Cz$ for any value of x, y and z</p>\n", 'ViewCount': '67', 'Title': 'Issues with an optimization problem', 'LastEditorUserId': '8110', 'LastActivityDate': '2013-05-12T22:24:06.363', 'LastEditDate': '2013-05-12T22:24:06.363', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><optimization><linear-programming>', 'CreationDate': '2013-05-12T21:48:08.947', 'FavoriteCount': '1', 'Id': '11977'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>You\u2019re doing some stress-testing on various models of glass jars to determine the height from which they can be dropped and still not break. The setup for this experiment, on a particular type of jar, is as follows. You have a ladder with $n$ rungs, and you want to find the highest rung from which you can drop a copy of the jar and not have it break. We call this the highest safe rung.\nIt might be natural to try binary search: drop a jar from the middle rung, see if it breaks, and then recursively try from rung $n/4$ or $3n/4$ depending on the outcome. But this has the drawback that you could break a lot of jars in finding the answer.</p>\n\n<p>If your primary goal were to conserve jars, on the other hand, you could try the following strategy. Start by dropping a jar from the first rung, then the second rung, and so forth, climbing one higher each time until the jar breaks. In this way, you only need a single jar\u2014at the moment it breaks, you have the correct answer\u2014but you may have to drop it $n$ times (rather than $\\log n$ as in the binary search solution).</p>\n\n<p>So here is the trade-off: it seems you can perform fewer drops if you\u2019re willing to break more jars. To understand better how this trade- off works at a quantitative level, let\u2019s consider how to run this experiment given a fixed \u201cbudget\u201d of $k \\ge 1$ jars. In other words, you have to determine the correct answer\u2014the highest safe rung\u2014and can use at most $k$ jars in doing so.</p>\n\n<p>Suppose you are given a budget of $k = 2$ jars. Describe a strategy for finding the highest safe rung that requires you to drop a jar at most $f (n)$ times, for some function $f (n)$ that grows slower than linearly. (In other words, it should be the case that $\\lim_{n\\to\\infty} f (n)/n = 0$.)</p>\n\n<p>Anyone any insights how to solve this problem?</p>\n\n<p>I know that the answer is $h = \\sqrt n$. I know  that it solves the problem. But I want to know how you got that answer, there must be a general way to do it?</p>\n', 'ViewCount': '672', 'Title': 'Balancing subproblems in resilience testing', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-11T01:48:19.290', 'LastEditDate': '2013-08-01T07:17:26.863', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'user75300', 'PostTypeId': '1', 'OwnerUserId': '8086', 'Tags': '<algorithms><binary-trees><decision-problem><divide-and-conquer>', 'CreationDate': '2013-05-07T21:28:25.053', 'Id': '11981'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to solve a problem of finding incompatible jobs set using greedy algorithm. However, I am not sure if greedy algorithm can solve this problem or I need to perform another approach.</p>\n\n<p>I have a set of jobs with start and finish time and I want to find the smallest subset of this jobs such that all the jobs are incompatible with at least one job of this subset.</p>\n\n<p>Suppose</p>\n\n<pre><code>job  start   end\n1    1       3\n2    2       11\n3    4       6\n4    7       8\n</code></pre>\n\n<p>My required job set J is {2} since  all the jobs are incompatible with at least one job of the job set J. I tried to use greedy algorithm like sorting jobs by start time, end time ( adding one  and removing all the ones incompatible and so on) But it is not optimal. As you can see in this example. If I add job 1 and then remove all the job incompatible with it, I will remove job 2, Then I will have to add 3 and 4 in the jobset J.</p>\n\n<p>Am I going the right way?</p>\n', 'ViewCount': '200', 'Title': 'Issues with using greedy algorithm (Interval scheduling variant)', 'LastEditorUserId': '8153', 'LastActivityDate': '2013-05-14T22:45:50.280', 'LastEditDate': '2013-05-14T14:31:20.733', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8153', 'Tags': '<algorithms><combinatorics><dynamic-programming><scheduling><greedy-algorithms>', 'CreationDate': '2013-05-14T04:16:16.893', 'Id': '12001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I saw a proof by Saeed Amiri,\nWe will add one extra vertex v to the graph G and we make new graph G\u2032, such that v is connected to the all other vertices of G. G has a Hamiltonian cycle if and only if G\u2032 has a Wn+1. It is easy to check that if G has a Hamiltonian cycle then G\u2032 has a Wn+1 wheel (just set v as a center). On the other hand, if G\u2032 has a Wn+1 then there are two possibility:</p>\n\n<p>v is the center of Wn+1\u2192G has a Hamiltonian cycle.\nAnother vertex u is the center of Wn+1 in G\u2032. So both deg(u)=deg(v)=n. Then we can change the labeling of this two vertices (actually they are equivalence under isomorphic), now we have again first possibility.\nP.S1: By Wn I mean the wheel with n vertex.</p>\n\n<p>P.S2: In this proof we say if we fix k=n+1 (size of the artificial graph), then the problem is NP-Complete in this restricted version, So it's also NP-Complete in the case k is as input parameter.</p>\n\n<p>The proof is valid one way. If a graph has a hamiltonian cycle adding a node to the graph converts it a wheel. If the graph of k+1 nodes has a wheel with k nodes on ring. It has a hamiltonian cycle. BUT IF THE GRAPH OF N nodes has a wheel of size k. Then identifying which k nodes cannot be done in polynomial time. Thus the reduction cannot be done in polynomial time.</p>\n", 'ViewCount': '106', 'ClosedDate': '2013-05-18T03:40:01.500', 'Title': 'Reducing from Hamiltonian Cycle problem to the Graph Wheel problem cannot be proved vice versa', 'LastActivityDate': '2013-05-15T07:51:32.010', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12034', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8160', 'Tags': '<algorithms><graphs><np-complete><polynomial-time>', 'CreationDate': '2013-05-14T08:13:46.597', 'Id': '12009'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to solve a problem of finding compatible jobs set using greedy algorithm. However, I am not sure if greedy algorithm can solve this problem or I need to perform another approach.</p>\n\n<p>I have a set of jobs with start and finish time and I want to find the smallest subset of this jobs such that all the jobs are incompatible with at least one job of this subset. And all the jobs in this subset are compatible</p>\n\n<p>Suppose</p>\n\n<pre><code>job  start   end\n1    1       3\n2    2       11\n3    4       6\n4    12       14\n</code></pre>\n\n<p>My required job set J is {2,4} since  all the jobs are incompatible with at least one job of the job set J. And all the jobs in the job set J are compatible. I tried using earliest deadline first and schedule but it doesn't work. Any suggestions?</p>\n\n<p>Am I going the right way?</p>\n", 'ViewCount': '171', 'Title': 'Solving a variant of interval scheduling problem', 'LastActivityDate': '2013-05-14T12:34:59.047', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms>', 'CreationDate': '2013-05-14T12:34:59.047', 'Id': '12018'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Hi I've read this lemma in my book:</p>\n\n<blockquote>\n  <p><strong>Lemma 2.1.</strong> Let $p(n) = \\sum_{i=0}^{k} a_in^i$ denote any polynomial and assume $a_k &gt; 0$. Then $p(n) \\in \\Theta(n^k)$</p>\n  \n  <p><strong>Proof.</strong> It suffices to show that $p(n) \\in O(n^k)$  and $p(n) \\in \\Omega(n^k)$. First observe that for $n &gt; 0$,\n  $$p(n) \\leq \\sum_{i=0}^{k} |a_i|n^i \\leq n^k \\sum_{i=0}^{k}|a_i|,$$\n  and hence $p(n) \\leq (\\sum_{i=0}^{k}|a_i|)n^k$ for all positive $n$. Thus $p(n) \\in O(n^k)$.</p>\n  \n  <p>Let $A = \\sum_{i=0}^{k-1}|a_i|$. For positive $n$ we have\n  $$p(n) \\geq a_kn^k -An^{k-1} = \\frac{a_k}{2}n^k + n^{k-1}(\\frac{a_k}{2}n - A)$$\n  and hence $p(n) \\geq (a_k/2)n^k$ for $n &gt; \\frac{2A}{a^k}$. We choose $c=a_k/2$ and $n_0 = 2A/a^k$ in the definition of $\\Omega(n^k)$, and obtain $p(n) \\in \\Omega(n^k)$.</p>\n</blockquote>\n\n<p>Can anyone explain me the part $p(n)\\in \\Omega(n^k)$ of the proof? Why should we divide $a_k\\cdot n^k$ by 2? Why can't we take $a_kn^k$ as coefficient of $n^k$? And how do we obtain that $n&gt;2A/a_k$?</p>\n", 'ViewCount': '118', 'Title': 'Understanding why the polynomial $p(n) = \\sum_{i=0}^{k} a_in^i$ is in $\\Theta(n^k)$', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-14T23:41:17.610', 'LastEditDate': '2013-05-14T21:26:38.743', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8173', 'Tags': '<algorithms><algorithm-analysis><proof-techniques>', 'CreationDate': '2013-05-14T17:34:14.640', 'Id': '12022'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In the Hopcroft-Karp algorithm for bipartite matching, I don't understand the purpose of the breadth first search.  I think it's used to find a set of vertex disjoint augmenting paths, but I'm not sure what the significance of that is or even what that means.  Why do the augmenting paths have to be the shortest?  And why do they have to be vertex disjoint?</p>\n", 'ViewCount': '169', 'Title': 'In the Hopcroft-Karp algorithm, what is the purpose of the breadth first search?', 'LastActivityDate': '2013-08-17T13:24:49.037', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8184', 'Tags': '<algorithms><graphs><bipartite-matching>', 'CreationDate': '2013-05-15T02:46:31.397', 'Id': '12030'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>The MAX k-SAT problem is:</p>\n\n<blockquote>\n  <p>\u201cGiven a set of clauses C1,\u2026,Ck, each of length k, over a set of\n  variables x1,\u2026,xn, find a truth assignment that satisfies as many of\n  the clauses as possible.\u201d</p>\n</blockquote>\n\n<p>I'm trying for find a randomized 0.999-approximation algorithm for the MAX 10-SAT problem. Help :(</p>\n", 'ViewCount': '87', 'Title': 'MAX 10-SAT Algorithm', 'LastActivityDate': '2013-05-15T16:16:03.370', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '12032', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '3083', 'Tags': '<algorithms><proof-techniques><dynamic-programming>', 'CreationDate': '2013-05-15T02:48:00.273', 'Id': '12031'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a directed graph with the following attributes: - a chain from node $a$ to node $z$ passing nodes $b$ to $y$ exists and is unidirectional. - additionally a set of nodes having bidirectional vertices to at least two of the nodes $a \\ldots z$ exists. These nodes are connected in a second unidirectional chain.</p>\n\n<p><img src="http://i.stack.imgur.com/W1NjY.png" alt="enter image description here"></p>\n\n<p>(the red route is the requested result, the squares are the first chain (unidirectional) and the circles are the second chain (unidirectional). $1$ is the start node and $5$ is the destination node.)</p>\n\n<p>Is it possible to find the shortest path from $a$ to $z$ that includes nodes $b$ to $y$ and the additional nodes once without probing all possibilities?</p>\n\n<p>I think the problem is roughly the same as the minimal traveling salesman problem since adding a vertex from $z$ to $a$ will result in the min-TSP - but this problem is slightly easier since a path from $a$ to $z$ is already known.</p>\n', 'ViewCount': '145', 'Title': u'Is \u201cFind the shortest tour from a to z passing each node once in a directed graph\u201d NP-complete?', 'LastEditorUserId': '8188', 'LastActivityDate': '2013-05-16T07:26:57.557', 'LastEditDate': '2013-05-16T06:17:36.050', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12061', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8188', 'Tags': '<algorithms><graph-theory><graphs><np-complete>', 'CreationDate': '2013-05-15T11:03:35.493', 'Id': '12036'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '242', 'Title': "Shortest sub-sequence of one string, that's not a sub-sequence of another string", 'LastEditDate': '2013-05-18T07:32:13.657', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3083', 'FavoriteCount': '1', 'Body': "<p>Given two strings $x$ and $y$ over the alphabet $\\{A,C,G,T\\}$, I'm trying to determine a shortest string $z$ such that $z$ is a subsequence of $x$ and not a subsequence of $y$.</p>\n\n<blockquote>\n  <p><strong>Example:</strong> a shortest string that is a subsequence of AATGAAC but not a subsequence of ATCGATC is AAG.</p>\n</blockquote>\n", 'Tags': '<algorithms><strings>', 'LastEditorUserId': '755', 'LastActivityDate': '2013-05-18T07:32:13.657', 'CommentCount': '2', 'AcceptedAnswerId': '12038', 'CreationDate': '2013-05-15T11:38:38.767', 'Id': '12037'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is a famous <a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29" rel="nofollow">Consensus Problem</a> in Distributed Computing.</p>\n\n<p>Let\'s consider and try to find the best possible algorithm for a simplified version of the consensus problem. </p>\n\n<p><strong>Assumptions:</strong> a process may undergo only crash failure (process abruptly stops and does not resume), process represent a complete graph.</p>\n\n<p><strong>Simplification</strong>: a crash failure may occur only between round, so there no a case when a process succeeds in sending some message and fails to send other message during the same round.</p>\n\n<p>The algorithm for the general case <strong><em>FloodSet</em></strong> when a process may crash during the round is described in <a href="http://books.google.co.il/books?id=2wsrLg-xBGgC&amp;lpg=PP1&amp;pg=PA103#v=onepage&amp;q&amp;f=false" rel="nofollow">Distributed algorithms - Nancy Ann Lynch</a>. By the analysis it was shown that $f+1$ rounds is enough to reach a consensus.</p>\n\n<p>Intuitively, it looks like that the simplification completely changes the approach to the solution. It is may be enough just one round, every processes send an input message to every other processes and agree on the minimal value. </p>\n\n<p>What is the simplest possible algorithms for simplified problem ? </p>\n\n<p>What can we do in the case of general graph?</p>\n', 'ViewCount': '83', 'Title': 'Is this simplified consensus problem easier than the original?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-16T06:17:45.843', 'LastEditDate': '2013-05-16T06:17:45.843', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12059', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2013-05-15T16:55:04.127', 'Id': '12043'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given three strings $x$, $y$, and $z$ over an arbitrary finite alphabet, I need to determine their longest common subsequence (LCS).</p>\n\n<p><strong>Example</strong>: A longest common subsequence of <code>bandana</code>, <code>cabana</code>, and <code>magazine</code> is <code>aan</code>.</p>\n\n<p>I'm trying to find an algorithm which uses $O(|x|\\cdot |y| \\cdot |z|)$ space where $|s|$ denotes the length of the string $s$.</p>\n", 'ViewCount': '163', 'Title': 'Find longest common subsequence in limited space', 'LastEditorUserId': '2205', 'LastActivityDate': '2014-03-25T19:29:35.657', 'LastEditDate': '2013-05-15T18:05:44.963', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12046', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3083', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming><strings>', 'CreationDate': '2013-05-15T17:37:25.197', 'Id': '12045'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1087', 'Title': 'Graph Has Two / Three Different Minimal Spanning Trees?', 'LastEditDate': '2013-05-15T20:00:02.037', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '8202', 'FavoriteCount': '0', 'Body': "<p>I'm trying to find an efficient method of detecting whether a given graph G has two different minimal spanning trees. I'm also trying to find a method to check whether it has 3 different minimal spanning trees. The naive solution that I've though about is running Kruskal's algorithm once and finding the total weight of the minimal spanning tree. Later , removing an edge from the graph and running Kruskal's algorithm again and checking if the weight of the new tree is the weight of the original minimal spanning tree , and so for each edge in the graph. The runtime is O(|V||E|log|V|) which is not good at all, and I think there's a better way to do it.</p>\n\n<p>Any suggestion would be helpful, thanks in advance</p>\n", 'Tags': '<algorithms><graph-theory><graphs><spanning-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-17T21:55:27.857', 'CommentCount': '5', 'AcceptedAnswerId': '12058', 'CreationDate': '2013-05-15T18:45:13.007', 'Id': '12048'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is more of an open-ended information question, but to make it concrete, here's an example problem I have thought up:</p>\n\n<p>Consider an $N\\times N$ grid, $N$ odd, and consider that a single chunk of rare, valuable metal, lies buried under one of the unit squares with uniform probability.</p>\n\n<p>An agent, $X$, starts at the center square of this grid, and this agent can do three things:</p>\n\n<ul>\n<li>Move to any adjacent square, (no diagonals), with cost $M &gt; 0$</li>\n<li>Dig at the current square, with cost $D &gt; 0$, in search of the treasure.  If the treasure is buried at said square, the agent is guaranteed to find it.</li>\n<li><p>With cost $S, 0 &lt; S &lt; D$, the agent may attempt to 'detect' the presence of the treasure with a radar device.  Given the actual location of the treasure, set $L$ to be the Euclidean distance between the center of $X$'s square and the square containing the treasure.</p>\n\n<p>The agent then receives, after conducting the test, a random real-valued number $r$, from the uniform distribution on the interval $[0, 1/(L^2 +1) ] $.  Note that the agent can, and will, with 100% probability, receive different signals from the same square over multiple tests.</p></li>\n</ul>\n\n<p>Given that the agent is a perfect logician, what is the expected total cost $C$ he will incur on a search for the treasure?</p>\n\n<hr>\n\n<p>My suspicion is that this problem is, at the very least, NP-hard in terms of $N^2$, the size of the grid.  And though I suspect a perfect agent would, given a proper adversarial input, incur infinite cost over time, there exists a comprehensive naive solution involving no signal tests, with worst-case cost $M*(N^2-1)+D*N^2$, and so the agent must always only consider choices with expected total-cost of finding the treasure less than this bound - i.e., stick to a finite number of decision paths.</p>\n\n<p><strong>Does anyone have any experience or familiarity with a problem like this?</strong> Is there a name for this variety of problem?  Is there, perhaps, some other choice of signal function that has been studied more in-depth?  I'm not interested in a deterministic function, which would be rather trivial to analyze - one key property of this function that makes the problem interesting is that, for any finite number of tests, there is non-zero probability that all the signals come back in the range $0 &lt; r &lt; \\epsilon$, where $\\epsilon$ is the minimum upper bound on a signal distribution given the grid size $N$ - such an adversarial set of signals would convey 0 information asymptotically over time, which means that a deterministic, signal-based solution algorithm (much less an optimal one) is impossible.</p>\n\n<hr>\n\n<p>Also, I realize that optimal strategies will likely vary wildly for different values of $M$, $S$, and $D$, so for the purpose of discussion, let's suppose they are $3$, $1$, and $10$ respectively.</p>\n", 'ViewCount': '48', 'Title': 'Signal-based Search', 'LastActivityDate': '2013-05-17T01:58:39.543', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7614', 'Tags': '<probabilistic-algorithms>', 'CreationDate': '2013-05-17T01:58:39.543', 'FavoriteCount': '1', 'Id': '12081'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '406', 'Title': 'Converting (math) problems to SAT instances', 'LastEditDate': '2013-09-17T08:05:55.883', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '7443', 'FavoriteCount': '1', 'Body': '<p>What I want to do is turn a math problem I have into a boolean satisfiability problem (SAT) and then solve it using a SAT Solver.  I wonder if someone knows a manual, guide or anything that will help me convert my problem to a SAT instance.</p>\n\n<p>Also, I want to solve this in a better than exponential time. I hope a SAT Solver will help me.</p>\n', 'Tags': '<algorithms><satisfiability>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-17T08:05:55.883', 'CommentCount': '13', 'AcceptedAnswerId': '12123', 'CreationDate': '2013-05-17T16:39:55.210', 'Id': '12087'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '290', 'LastEditorDisplayName': 'user742', 'Title': 'The sum of all integers less than n with a zero', 'LastEditDate': '2013-05-17T22:25:45.143', 'AnswerCount': '2', 'Score': '2', 'OwnerDisplayName': 'Alex Su', 'PostTypeId': '1', 'OwnerUserId': '8232', 'Body': '<p>For example, if n=14, the output should be 10; n=22, the output should be 30=10+20; n=102, output=(10+...+100)+101+102=5703</p>\n\n<p>In this problem, n is smaller than $10^{18}$ , and the algorithm should finish within 1 second.</p>\n', 'Tags': '<algorithms>', 'LastActivityDate': '2013-05-18T17:58:42.390', 'CommentCount': '5', 'AcceptedAnswerId': '12099', 'CreationDate': '2013-05-17T05:47:13.367', 'Id': '12096'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have recently been asked in an interview to devise an algorithm that divides a set of points in a coordinate system so that half of the points lie on one side of the line, and the rest on the other side.</p>\n\n<p>The points are unevenly placed and the line must not pass through any of the points.</p>\n\n<p>Can any one give any approach to solve the problem? Analysis of the algorithm is appreciated.</p>\n\n<p>Hints: Count the points, use medians.</p>\n\n<p>The number of points is assumed to be even.</p>\n', 'ViewCount': '168', 'Title': 'Algorithm to find a line that divides the number of points equally', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:54:52.617', 'LastEditDate': '2013-05-19T14:54:52.617', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12114', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6699', 'Tags': '<algorithms><machine-learning><computational-geometry><classification>', 'CreationDate': '2013-05-18T13:51:24.557', 'Id': '12111'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm taking a grad level randomized algorithms course in the fall. The professor is known for being very detail oriented and mathematically rigorous, so I will be required to have an in-depth understanding of probability. What would be a good probability book to learn from that would be intuitive, but also have some mathematical rigor to it?</p>\n", 'ViewCount': '79', 'Title': 'Randomized Algorithms Probability', 'LastActivityDate': '2013-05-19T13:03:22.573', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12133', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8242', 'Tags': '<probability-theory><randomized-algorithms>', 'CreationDate': '2013-05-18T14:20:52.533', 'Id': '12113'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '44', 'Title': 'how to prove this unsolvable problem about halting problem (turing machine)', 'LastEditDate': '2013-05-19T06:11:29.767', 'AnswerCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8244', 'FavoriteCount': '1', 'Body': u'<p>Show that the problem of deciding, for a given TM M, whether M halts for all inputs within n^2(namely n square ) steps(n is the length of the input) is unsolvable. You can use the fact without proof that the \u03b5-halting problem(that asks if a given TM halts for input \u03b5,namely, for the empty string) is unsolvable. (Hint: Use the input just to count the number of steps and basically simulate the behavior against the \u03b5 input for m steps. m may be any (easy to realize) number if it is at most n^2 and grows unlimitedly as n.).                             My answer is: Assume TM M decides whether halt or not within n^2 steps. TM S decides that when \u03b5 is a input halting or not.        use as a input of S, M1 is a TM. make a TM S:1.Use   on TM M.        2. If reject,then S rejects.        3.if accept, execute \u03b5 on M until halting. Who can tell me is this right or wrong? If it is wrong, then how to correct?</p>\n', 'ClosedDate': '2013-05-18T21:58:12.343', 'Tags': '<algorithms><complexity-classes><correctness-proof>', 'LastEditorUserId': '8244', 'LastActivityDate': '2013-05-19T06:11:29.767', 'CommentCount': '3', 'CreationDate': '2013-05-18T15:19:56.223', 'Id': '12116'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $G = (V,E)$ be a directed graph  with a weight function $w$ such that there are no negative-weight cycles, and let $v \\in V$ be a vertex such that there is a path from $v$ to every other vertex. Let $f : V \\to \\mathbb R$ be a given function. Describe an algorithm that runs in $O(|V| + |E|)$ time that answers yes/no to the question: is it true that for all $u \\in V, f(u) = \\delta(v,u)$, where $\\delta(v,u)$ is the weight of the shortest path from $v$ to $u$?</p>\n\n<p>Obviously what comes to mind is Bellman-Ford algorithm, but it doesn't satisfy the time requirement. I don't really see how having the candidate $f$ function helps us in this regard.</p>\n", 'ViewCount': '194', 'Title': 'Shortest paths candidate', 'LastActivityDate': '2013-06-05T15:32:06.230', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12126', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8247', 'Tags': '<algorithms><graph-theory><graphs><shortest-path>', 'CreationDate': '2013-05-18T19:19:11.787', 'Id': '12120'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can someone suggest an algorithm to solve job assignment problem with condition?</p>\n\n<p>With condition means that some jobs cannot be done by some workers. For example table as shown below:</p>\n\n<p><img src="http://i.stack.imgur.com/48Tqv.png" alt="enter image description here"></p>\n\n<p>In this table x - means that it is impossible to do. For example, worker 1 cannot do jobs 1,3 and 5.</p>\n\n<p>I encountered such situation and there may be cases as shown above when usual Hungarian algorithm seems cannot solve such task because there is no way to complete all tasks by distributing one task per worker. </p>\n\n<p>However, my main case it is allowed that one worker wil do several tasks (tasks, which worker can do). Main task is to complete all jobs using existing workers, but it is desirable that, all workers do roughly same number of tasks.</p>\n\n<p>So is there some solution of such problem? May be any algorithms do exist?</p>\n', 'ViewCount': '137', 'Title': 'Algorithm to solve job assignment problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:51:50.040', 'LastEditDate': '2013-05-19T14:51:50.040', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5207', 'Tags': '<algorithms><optimization><linear-programming><scheduling><assignment-problem>', 'CreationDate': '2013-05-18T19:39:04.773', 'Id': '12122'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've ran some tests and found that Shellsort runs much faster on ordered and reversed lists compared to random lists and almost ordered lists.</p>\n\n<pre><code>Results:\n        Random Reverse Order AlmostOrder\n  time    24      5      4        29\n</code></pre>\n\n<p>The problem that is confusing me is that Shellsort performs insertion sorts on lists separated by gaps, and insertion sort only runs very fast on ordered lists, not reversed lists.</p>\n\n<p>So my question is why does Shellsort work well on ordered and reversed lists when it uses insertion sort and insertion sort doesn't work well on reversed lists?</p>\n", 'ViewCount': '160', 'Title': 'Why does Shellsort work well on Sorted and Reverse ordered lists?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:52:46.560', 'LastEditDate': '2013-05-19T14:52:46.560', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'clay', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-05-18T02:16:41.193', 'FavoriteCount': '0', 'Id': '12124'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1483', 'Title': 'Finding the path of a negative weight cycle using Bellman-Ford', 'CommunityOwnedDate': '2013-05-21T19:50:47.733', 'LastEditDate': '2013-05-22T15:08:01.357', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8248', 'Body': '<p>I wrote a program which implements Bellman-Ford, and identifies when negative weight cycles are present in a graph. However what I\'m actually interested in, is given some starting vertex and a graph, which path do I actually trace to get to the original vertex having traveled a negative amount. </p>\n\n<p>So to be clear say I have a graph with vertexes, a, b, c, and d and there is a negative cycle between a, b, and d, then when I check for negative weight cycles </p>\n\n<pre><code>// Step 1: initialize graph\n   for each vertex v in vertices:\n       if v is source then distance[v] := 0\n       else distance[v] := infinity\n       predecessor[v] := null\n\n   // Step 2: relax edges repeatedly\n   for i from 1 to size(vertices)-1:\n       for each edge (u, v) with weight w in edges:\n           if distance[u] + w &lt; distance[v]:\n               distance[v] := distance[u] + w\n               predecessor[v] := u\n\n   // Step 3: check for negative-weight cycles\n   for each edge (u, v) with weight w in edges:\n       if distance[u] + w &lt; distance[v]:\n           "Graph contains a negative-weight cycle"\n</code></pre>\n\n<p>Instead of it just telling me that a negative cycle is there, I would like it to tell me, go from <code>a -&gt; b -&gt; d -&gt; a</code>.  After the relaxing step what do I have to change in my check for negative weight cycles to get it to output this information? </p>\n\n<ul>\n<li><p><a href="http://blog.alirabiee.com/?p=576" rel="nofollow">Here</a> is the best information I\'ve been able to find, but I\'m still having trouble making sense of it.  </p></li>\n<li><p>Also <a href="http://www.cs.ucdavis.edu/~amenta/f05/hw5.pdf" rel="nofollow">this</a> which suggests that I need to run breadth first search on the predecessor array to find the information, but I\'m not exactly sure where to start (what do I queue first?) </p></li>\n<li><p><a href="http://stackoverflow.com/questions/2282427/interesting-problem-currency-arbitrage">Here</a> is a stack overflow question which shows how to find one of the nodes in the path.</p></li>\n</ul>\n', 'Tags': '<algorithms><graphs><shortest-path>', 'LastEditorUserId': '8248', 'LastActivityDate': '2013-05-22T15:08:01.357', 'CommentCount': '3', 'AcceptedAnswerId': '12206', 'CreationDate': '2013-05-19T04:41:17.347', 'Id': '12129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '161', 'Title': 'Hanoi tower with forbidden direct move from source to destination', 'LastEditDate': '2013-05-19T07:30:47.883', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8251', 'FavoriteCount': '0', 'Body': "<p>I want to know what is algorithm and time complexity of Hanoi tower with forbidden direct move from source to destination (<em>it means you cannot move disk from source to destination directly and you instead of that should first move disk from source to middle and then from middle to destination</em>) and other rules as normal problem?</p>\n\n<p>I didn't find any article about it.</p>\n", 'Tags': '<algorithms><recurrence-relation><board-games>', 'LastEditorUserId': '8251', 'LastActivityDate': '2013-11-21T05:09:37.510', 'CommentCount': '6', 'AcceptedAnswerId': '12399', 'CreationDate': '2013-05-19T06:20:09.157', 'Id': '12130'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was wondering. May the source and sink have in-out going edges in a flow-network, and if so - does Ford-Fulkerson and the max-flow min-cut theorem apply ?</p>\n\n<p>Flow-networks are always pictures with no edges entering source, and no edges leaving sink.</p>\n\n<p>I've tried to search the web for an answer, but did not come across an answer i fully understand. Also, I've yet to see a flow network pictured with these edges from source/sink.</p>\n\n<p>Could I transform an undirected graph into a flow-network ? This network will have edges going into source and edges leaving sink ?? This hypothesis is the reason for my question.</p>\n", 'ViewCount': '206', 'Title': 'In flow networks, may source/sink have incoming/outgoing edges?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T18:57:23.833', 'LastEditDate': '2013-05-19T15:03:37.647', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8254', 'Tags': '<algorithms><graphs><network-flow>', 'CreationDate': '2013-05-19T14:20:17.700', 'Id': '12134'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '189', 'Title': 'Modeling the problem of finding all stable sets of an argumentation framework as SAT', 'LastEditDate': '2013-05-20T14:48:10.623', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7443', 'FavoriteCount': '2', 'Body': '<p>As a continuation of my previous <a href="http://cs.stackexchange.com/questions/12087/converting-math-problems-to-sat-instances?noredirect=1#comment25370_12087">question</a> i will try to explain my problem and how i am trying to convert my algorithm to a problem that can be expressed in a CNF form.</p>\n\n<p>Problem: Find all stable sets of an <a href="http://en.wikipedia.org/wiki/Argumentation_framework" rel="nofollow">argumentation framework</a> according to <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.4129" rel="nofollow">Dung\'s proposed framework</a>.</p>\n\n<p>Brief theory: Having an argumentation framework AF, with A the set of all arguments and R the set of the relations, a stable set is a set which attacks all arguments not in their set and there is no attack relation between arguments in the stable set.\nExample:</p>\n\n<p>Let\'s say we have an argumentation framework AF ,A={1,2,3,4}(arguments of AF) and attack relations R{1,3} and R{2,4}.\nIt\'s obvious that the set {1,2} is a stable extension of the framework because:</p>\n\n<p>a)it attacks all arguments not in their set (3 and 4)</p>\n\n<p>b)it\'s conflict free(no attacks between arguments in the set) because argument 1 does not attack argument 2 and the opposite </p>\n\n<p>My exhaustive abstract algorithm:</p>\n\n<pre><code>argnum=number of arguments;\n\nAi[argnum-1]=relation "attacks" ,where 1&lt;=i&lt;=argnum\n\nP[2^argnum-1]=all possible relations that can be generated from all the arguments\n\nS[2^argnum-1]=empty; where S are all the stable sets\n\nj=0; //counter for while\nk=1; //counter for counting stable sets\nwhile j&lt;2^argnum-1\n    if P[j] attacks all arguments not in P[j](check using Ai[])\n        if all arguments in P[j] are conlfict-free\n            S[k++]=P[j];\n        end if\n    end if \n    j++;\nend while\n</code></pre>\n\n<p>I want to solve the above problem either by transforming the above algorithm to CNF or by using a different algorithm and finally use a SAT Solver(or anything similar if exists) give CNF as input and get stable sets as output.</p>\n\n<p>I wonder if someone can give me any feedback of how i can transform any algorithm like the above to CNF in order to be used into a SAT Solver.</p>\n\n<p>I decided to use <a href="http://fmv.jku.at/precosat/" rel="nofollow">precosat</a>.</p>\n', 'Tags': '<algorithms><complexity-theory><time-complexity><np-complete><satisfiability>', 'LastEditorUserId': '7443', 'LastActivityDate': '2013-05-26T00:33:46.807', 'CommentCount': '7', 'AcceptedAnswerId': '12176', 'CreationDate': '2013-05-19T15:50:58.177', 'Id': '12135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let G be a directed graph with non-negative weights. We call a path between two vertices an "odd path" if its weight is odd.</p>\n\n<p>We are looking for an algorithm for finding the weight of the shortest odd path between any two vertices in the graph.</p>\n\n<p>If possible, describe one algorithm that is reduction-based (that is, make some modification to the graph so that application of Floyd-Warshall, or any other "known" algorithm, and then deciphering the answer will give the result, see <a href="http://en.wikipedia.org/wiki/Reduction_(complexity)" rel="nofollow">http://en.wikipedia.org/wiki/Reduction_(complexity)</a>) and one that is "direct" (that is, make some modification to Floyd-Warshall in order for it to solve this problem).</p>\n', 'ViewCount': '301', 'Title': 'Shortest path with odd weight', 'LastActivityDate': '2013-05-22T07:57:41.470', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12155', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8247', 'Tags': '<algorithms><graph-theory><graphs><shortest-path>', 'CreationDate': '2013-05-20T11:52:45.343', 'Id': '12154'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have recently learned about various randomized algorithms for load balancing. The model is always that there are $m$ balls and $n$ bins and the balls arrive one at a time. The task is to minimize the maximum load of any bin.  However there is something I don't understand.</p>\n\n<p>Why not just keep a priority queue of the loads of the bins and allocate any new ball to the bin with the lowest current load?  This seems to give you the optimal load without any complications.</p>\n", 'ViewCount': '227', 'Title': 'Load balancing. Why not use priority queues?', 'LastActivityDate': '2013-05-20T20:16:53.273', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '12169', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8271', 'Tags': '<priority-queues><online-algorithms>', 'CreationDate': '2013-05-20T19:39:14.903', 'Id': '12165'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is  a greedy algorithm for finding minimum vertex cover of a tree which uses DFS traversal.</p>\n\n<ol>\n<li>For each leaf of the tree, select its parent (i.e. its parent is in minimum vertex cover).</li>\n<li>For each internal node:<br>\nif any of its children is not selected, then select this node.</li>\n</ol>\n\n<p>How do I prove that this greedy strategy gives an optimal answer? That there is no vertex cover smaller in size than the one that the above algorithm produces?</p>\n', 'ViewCount': '1379', 'Title': 'Correctness-Proof of a greedy-algorithm for minimum vertex cover of a tree', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-05-21T18:43:17.647', 'LastEditDate': '2013-05-21T16:09:33.600', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4980', 'Tags': '<algorithms><trees><greedy-algorithms>', 'CreationDate': '2013-05-21T03:52:48.837', 'Id': '12177'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>[Input]: the begin and end points of an arbitrary line (small red points) and the line width (green line)\n<br/>[Example]: begin=(20,20), end=(100,50), width=5</p>\n\n<p><img src="http://i.stack.imgur.com/WKabw.png" alt="enter image description here"></p>\n\n<p>[Output]: The set of pixels (not the total area) that are in the yellow rectangle\n<br/>[Example]: {(20,20), (20,21), (20,22),...etc.}</p>\n\n<p>How can i calculate the output set?</p>\n', 'ViewCount': '64', 'Title': 'Bounding rectangle of a line', 'LastEditorUserId': '5222', 'LastActivityDate': '2013-05-21T15:45:01.207', 'LastEditDate': '2013-05-21T14:23:51.863', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5222', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-05-21T10:54:37.237', 'Id': '12181'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've seen two definitions of a universal hash family, and my questions is if those are equivalent, i think they are and will explain why but i'm not sure if it is.</p>\n\n<p>Definition 1:</p>\n\n<p>$H$ is a universal hash family if and only if $$\\mathbb{Pr_{h \\in H}}[h(k) = h(l)] = 1/m$$</p>\n\n<p>where $k \\neq l$ and $k,l\\in Key$ and $m$ is the Size of the Hashtable</p>\n\n<p>Definition 2:</p>\n\n<p>The same as above just substitute the equals sign with a less or equal sign.</p>\n\n<p>Well, i think those definitions are equivalent, because i think the probability for a collision can't possibly be strictly less than 1/m or am i missing something?</p>\n\n<p>P.S.\nI'm assuming that the cardinality of Key is bigger than the size of the hashtable m</p>\n", 'ViewCount': '82', 'Title': 'Are those definitions of universal hash family equivalent?', 'LastEditorUserId': '8282', 'LastActivityDate': '2013-05-22T00:16:10.807', 'LastEditDate': '2013-05-21T17:02:54.330', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8282', 'Tags': '<algorithms><algorithm-analysis><hash><hash-tables>', 'CreationDate': '2013-05-21T16:00:59.023', 'Id': '12193'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been recently studying Monte-Carlo and other randomized methods for a lot of applications, and one that popped into my mind was making an (approximate) convex hull by examining random points, and try to get them inside the convex hull. I would like to know if there are algorithms for convex hulls that can improve the $O(n \\log n)$ bound of comparison based algorithms, and the $O(n\\cdot h)$ bound for Jarvis march and related to $O(n)$, either by building an approximate convex hull in $O(n)$ (with or without some approximation criteria) or by building an exact convex hull in expected linear time.</p>\n", 'ViewCount': '43', 'Title': 'Randomized convex hull', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-21T23:02:42.530', 'LastEditDate': '2013-05-21T19:34:43.807', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12208', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2138', 'Tags': '<algorithms><asymptotics><computational-geometry><randomized-algorithms>', 'CreationDate': '2013-05-21T19:30:47.060', 'Id': '12199'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here's the problem:</p>\n\n<p>I have a collection of collections, $C$, where each $c\\in C$ is a collection of sets $X\\subset U$.  Denote $c_i$ as the i-th $X$ in $c$.  Informally, I want to map all the sets in each collection to bins, where no two sets in a single collection can occupy the same bin, such that the sum of the sizes of the unions of all sets in each bin is minimized.  More formally:</p>\n\n<p>Let $N = \\max_{c \\in C} |c|$, and let $P_N$ be the set of all permutations of all non-empty subsets of the set $\\{1,2,...,N\\}$.  I wish to define a mapping:</p>\n\n<p>$$F : C \\rightarrow P_{N},\\ s.t.\\ \\forall c \\in C\\ (|F(c)| = |c|)$$</p>\n\n<p>with bin sets\n$$B(k) = \\{X \\subset U : \\exists c \\in C\\ (\\exists i \\in \\{1,2,...,|c|\\}\\ s.t.\\ c_i = X \\wedge (F(c))_i = k)\\}$$</p>\n\n<p>Such that the quantity</p>\n\n<p>$$\\sum_{k=1}^{N} { \\Biggl|\\bigcup_{X \\in B(k)}\\Biggr| } $$</p>\n\n<p>is minimized.</p>\n\n<hr>\n\n<p>Off the bat, I'd guess that this is an NP-hard problem - a reduction from Set Cover seems to be just within reach.  </p>\n\n<p>Even a greedy algorithm that iteratively processes each collection $c \\in C$, producing minimal results each time, requires $O(2^N \\cdot |C|)$ time using dynamic programming, where $|U|$ is assumed to be a constant factor.</p>\n\n<p>I'm having trouble proving whether or not the Greedy algorithm is even optimal - or if a more efficient solution exists.  Anyone have any thoughts?</p>\n\n<hr>\n\n<p>Alternatively, minimizing the quantity:</p>\n\n<p>$$\\max_{1 \\leq k \\leq N} {\\Biggl| \\bigcup_{X \\in B(k)} \\Biggr| }$$</p>\n\n<p>Is also of interest.  It's definitely a different problem, as demonstrated by a simple case where $C$ has 2 collections, one of the form $\\{\\{1\\}, \\{3, 4\\}\\}$, and the other $\\{\\{2\\}, \\{3, 4\\}\\}$.  I am not sure this problem is any easier though</p>\n", 'ViewCount': '74', 'Title': 'Overlap Maximization problem', 'LastEditorUserId': '7614', 'LastActivityDate': '2013-05-22T20:36:21.743', 'LastEditDate': '2013-05-22T20:36:21.743', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7614', 'Tags': '<algorithms><time-complexity><optimization><sets>', 'CreationDate': '2013-05-22T17:58:46.787', 'FavoriteCount': '1', 'Id': '12219'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1018', 'Title': 'Practical Applications of Radix Sort', 'LastEditDate': '2013-05-23T22:58:26.053', 'AnswerCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '6728', 'FavoriteCount': '1', 'Body': '<p>Radix sort is theoretically very fast when you know that the keys are in a certain limited range, say $n$ values in the range $[0\\dots n^k -1]$ for example.  If $k&lt;\\lg n$ you just convert the values to base $n$ which takes $\\Theta(n)$ time, do a base $n$ radix sort and then convert back to your original base for an overall $\\Theta(nk)$ algorithm. </p>\n\n<p>However, I\'ve read that <a href="http://www.lamarca.org/anthony/caches.html">in practice radix sort is typically much slower than doing for example a randomized quicksort</a>:</p>\n\n<blockquote>\n  <p>For large arrays, radix sort has the lowest instruction count, but\n  because of its relatively poor cache performance, its overall\n  performance is worse than the memory optimized versions of mergesort\n  and quicksort.</p>\n</blockquote>\n\n<p>Is radix sort just a nice theoretical algorithm, or does it have common practical uses?</p>\n', 'Tags': '<algorithms><sorting><applied-theory><radix-sort>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-15T00:11:26.887', 'CommentCount': '0', 'AcceptedAnswerId': '12228', 'CreationDate': '2013-05-23T07:29:45.403', 'Id': '12223'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This was a question at SO, and I think it\'s very interesting, I thought about it, but I could not provide any efficient algorithm neither showing the NP-Hardness:</p>\n\n<blockquote>\n  <p>Find the length of the longest non-decreasing sequence through\n  adjacent, non-repeating cells (including diagonals). For example, in the\n  following grid, one legal path (though not the longest) that could be\n  traced is 0->3->7->9 and its length would be 4.</p>\n  \n  <p>8 2 4 </p>\n  \n  <p>0 7 1 </p>\n  \n  <p>3 7 9 </p>\n  \n  <p>The path can only connect adjacent locations (you could not connect 8 ->\n  9). The longest possible sequence for this example would be of length\n  6 by tracing the path 0->2->4->7->7->9 or 1->2->4->7->7->8.</p>\n</blockquote>\n\n<p>For first attempts and possible misinterpretations is not bad to see <a href="http://stackoverflow.com/questions/15553218/technical-interview-longest-non-decreasing-subsequence-in-mxn-matrix/15554014#15554014">this</a> answer at SO.</p>\n\n<p>My question: above problem is in $P$?</p>\n', 'ViewCount': '365', 'LastEditorDisplayName': 'user742', 'Title': 'Longest path in grid like graph', 'LastActivityDate': '2013-05-24T02:46:31.383', 'LastEditDate': '2013-05-23T23:29:03.860', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'Tags': '<algorithms><complexity-theory><graph-theory>', 'CreationDate': '2013-05-23T23:20:39.770', 'FavoriteCount': '1', 'Id': '12239'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a weighted digraph, I can check whether a given vertex belongs to a negative cycle in $O(|V|\\cdot|E|)$ using Bellman-Ford. But what if I need to find all vertices on negative cycles? Is there a way to do it faster than Floyd-Warshall's $O(|V|^3)$?</p>\n", 'ViewCount': '102', 'Title': 'Finding all vertices on negative cycles', 'LastActivityDate': '2013-12-12T18:42:02.590', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18933', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8329', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2013-05-24T03:25:43.543', 'Id': '12243'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given two arbitrary regular expressions, is there an "efficient" algorithm to determine whether they match the same set of strings?</p>\n\n<p>More generally, can we compute the size of the intersection of the two match sets?</p>\n\n<p>What algorithms are there to do this, and what complexity class do they live in?</p>\n\n<p>If we disallow the Kleene star, does that alter the picture at all?</p>\n', 'ViewCount': '209', 'Title': 'Algorithm to determine whether two regexes are equivalent', 'LastEditorUserId': '472', 'LastActivityDate': '2013-06-17T16:08:55.307', 'LastEditDate': '2013-06-17T16:08:55.307', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1951', 'Tags': '<algorithms><regular-expressions>', 'CreationDate': '2013-05-25T11:29:02.060', 'Id': '12267'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let,</p>\n\n<ul>\n<li><code>game</code> be the state of a 2-player, turn based game </li>\n<li><code>actions game player -&gt; [move]</code> be a function that gets the state of a game and returns all valid moves, where a move if a function <code>move game -&gt; game</code> that updates the state of the game</li>\n<li><code>state game -&gt; state</code> returns wether a game is <code>incomplete</code>, <code>won by player1</code> or <code>won by player 2</code></li>\n</ul>\n\n<p>Countless games such as chess and tic-tac toe can be described this ways. Is there an algorithm that, given such description, returns the optimal strategy, where <code>strategy</code> takes the game state as input and returns the optimal move for that turn?</p>\n', 'ViewCount': '191', 'Title': 'Is there an algorithm to find the best strategy for a game?', 'LastActivityDate': '2013-05-29T14:17:32.720', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8359', 'Tags': '<algorithms><artificial-intelligence>', 'CreationDate': '2013-05-26T05:26:53.363', 'FavoriteCount': '1', 'Id': '12284'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Find the weight of the lightest path from u to v the goes through node a or/and b.</p>\n\n<p>Do you have a suggestion on how it can be done?</p>\n', 'ViewCount': '94', 'LastEditorDisplayName': 'user742', 'Title': 'Find the weight of the lightest path from u to v', 'LastActivityDate': '2013-09-20T09:34:09.967', 'LastEditDate': '2013-09-20T09:34:09.967', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '8361', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2013-05-26T07:55:17.363', 'FavoriteCount': '0', 'Id': '12291'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '142', 'Title': 'Can a Boolean circuit be considered an algorithm?', 'LastEditDate': '2013-05-28T14:50:54.623', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8367', 'FavoriteCount': '1', 'Body': '<p>Can a Boolean circuit by itself be considered an algorithm (a single step algorithm if you like)? For instance say you have a simple tree circuit with two AND gates as the input gates feeding a single OR gate for a depth two circuit. Now change the AND gates to XOR gates, is it correct to say that I now have a different algorithm for any given input? </p>\n', 'Tags': '<algorithms><terminology><circuits>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-28T14:50:54.623', 'CommentCount': '0', 'AcceptedAnswerId': '12298', 'CreationDate': '2013-05-26T13:26:47.687', 'Id': '12294'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '102', 'Title': 'Preventing oversell, allocation of limited resources with overlapping properties', 'LastEditDate': '2013-05-29T12:21:39.890', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8380', 'FavoriteCount': '1', 'Body': '<p>I am trying to solve problem of preventing oversell of limited resources.</p>\n\n<p>Consider resources (people) who are described by set of properties where each property belongs to different category (example properties from four categories:  male, age 25-30, 2 children, interested in games).</p>\n\n<p>Buyers want to allocate access to resources. Buyers can specify subset of categories and one property from each category (example: allocate 1000 males, age 25-30 or allocate 100 females, age 25-30, interested in music).</p>\n\n<p>In my real life example I have 6m+ possible set of properties (profiles) where for each set of properties I know how many profiles exists.</p>\n\n<p>My initial approach was to build a graph like one below:</p>\n\n<p><img src="http://i.imgur.com/gQsXfWB.png" alt="alt text"></p>\n\n<p>and then traverse using edge weights, for instance validating if demand for 100 females, age2 can be satisfied:</p>\n\n<ol>\n<li>check if size(female, age2) &lt; 100</li>\n<li>for each parent:\n<ol>\n<li>check if size(parent) &lt; 100 and go to 2.</li>\n</ol></li>\n<li>for each child:\n<ol>\n<li>check if size(child) &lt; 100 * weight(edge(node, child)) go to 1.</li>\n</ol></li>\n</ol>\n\n<p>(above algorithm is simplified as does not prevent visiting same node multiple times)</p>\n\n<p>It all works fine when graph is small, however when number of nodes and edges (dependencies) between nodes (profile universe groups) grows it does not scale very well.</p>\n\n<p>Consider example:</p>\n\n<ul>\n<li>large graph, 6m nodes, 20m+ edges</li>\n<li>buyer wants to allocate 1000 males (and there are only males and females in gender category)</li>\n</ul>\n\n<p>algorithm would start with top-level \'male\' node which probably has 10m+ outgoing edges and 10m+ checks would be required (and probably each of those 10m outgoing edges has incoming edges which need to be checked as well).</p>\n\n<p>I was trying to find different approach but failed. I was trying to google out existing solutions but seems like I am unable to even name problem properly. Any reference to what is this problem similar to would be good for me as a starting point.</p>\n\n<p>Thanks for comments/help.</p>\n\n<p>Two more graphs to present exponential growth of the graph:\n3 categories\n<img src="http://i.imgur.com/gLg6Y6i.png" alt="alt text"></p>\n\n<p>4 categories\n<img src="http://i.imgur.com/0TM4mSy.png" alt="alt text"></p>\n\n<p><strong>Update</strong></p>\n\n<p>Regarding size, assuming 8 categories of properties where each category has: 2, 6, 6, 6, 6, 8, 1140, 150 values respectively then estimated number of profiles: 2*6^4*8*1140*150 ~= 3.5 * 10^9. Number of nodes in graph: at least 7 * 10^9, number of edges in graph: at least 140 * 10^9.</p>\n\n<p><strong>Update #2</strong></p>\n\n<p>Formula for number of nodes is:</p>\n\n<p>$\\sum_{i&lt;n}\\prod_{k&lt;i \\atop j_1, j_2, ..., j_k &lt; n} s_{j_{1}} ... s_{j_{n}}$</p>\n\n<p>where $n$ is number of categories and $s_x$ is size of category $x$.</p>\n\n<p>So in my example there would be 11\'169\'108\'657 nodes.</p>\n\n<p><strong>Update #3</strong></p>\n\n<p>As per @Raphael advice - I have reduced number of nodes and now formula is:</p>\n\n<p>$\\sum_{i&lt;n-M}\\prod_{k&lt;i \\atop j_1, j_2, ..., j_k &lt; n} s_{j_{1}} ... s_{j_{n}}$</p>\n\n<p>where $M&lt;n$ and assumed that distribution of resources across smallest slices of universe is equal.\nAt the same time removed lot of edges from graph.</p>\n\n<p>Example of sub-graph size reduction:\n<img src="http://i.stack.imgur.com/SSSU8.png" alt="Example of sub-graph size reduction"></p>\n', 'Tags': '<algorithms><databases><counting>', 'LastEditorUserId': '8380', 'LastActivityDate': '2013-05-29T12:21:39.890', 'CommentCount': '6', 'AcceptedAnswerId': '12327', 'CreationDate': '2013-05-27T10:45:05.100', 'Id': '12304'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>First I apologize if the title is unclear, but I didn\'t find anything better.</p>\n\n<p>I\'m solving a differential equation that has two parameters , here denoted by points of a plane.These parameters are real numbers. For some points of the plane (or, equivalently, for some parameters of the differential equation) , the solution of the equation satisfies some condition and we denote such points of the plane with 1. This points make a simply-connected region in the plane, and I know that this region lies somewhere in $0&lt;x&lt;1.6, -.7&lt;y&lt;.3$ rectangle. My goal is to find this region.Remember that this region is on the real plane.($\\mathbf R^2$)</p>\n\n<p><strong>My main question is this:</strong> I already know these two things about the wanted region:</p>\n\n<ol>\n<li><p>The set of points on the plane that satisfy the conditions (the wanted region) , form a simply-connected region</p></li>\n<li><p>This region lies somewhere in $0&lt;x&lt;1.6, -.7&lt;y&lt;.3$ rectangle</p></li>\n</ol>\n\n<p>for example it may be something like this:</p>\n\n<p><img src="http://i.stack.imgur.com/Q8lvC.png" alt="enter image description here"></p>\n\n<p>I want to use this two facts to find the region with less computations; i.e. instead of checking the condition on all points on the rectangle, actually , on a very high-resolution grid (this the first approach below), use an algorithm (below : Variant 2) that more quickly converges to <strong>the boundary</strong> (and so determines the region without inspecting all points).  </p>\n\n<p>(If you know a better approach ,I\'ll be happy to hear) </p>\n\n<h3>Variant 1 (the naive approach, noted above)</h3>\n\n<p>Divide each axis to identical steps (of length $\\Delta$ ) and check the condition on each node to find the region.$\\Delta$ must be as small as possible to find the region with an acceptable accuracy ($\\Delta0.001$ suffices for my purpose) . (in the picture : nodes = intersections). This method needs a huge number of check operations , but can be used to find all kinds of regions; I mean if I didn\'t know that the wanted region is connected or it had sharp edges, etc. ,this method was the only way.</p>\n\n<p><img src="http://i.stack.imgur.com/P9RoC.png" alt="http://i.stack.imgur.com/MvhRH.png"></p>\n\n<h3>Variant 2</h3>\n\n<p>(It may be a famous method, but I haven\'t seen it before) </p>\n\n<p>Because the region will be simply connected, it suffices to find its boundary .We use a <strong>recursive</strong> approach. We start from a grid (like the first step, but with much larger distance between nodes, say, $100\\Delta$) and check the condition on this grid.For the next step, I assume the interval between two adjacent 1s is 1 everywhere and between two adjacent 0s is 0 everywhere. <em>If two adjacent nodes gave different results (1 on one them and 0 on the other) , I put a point between them and check the condition on that point (red points in the picture) check this point. If it was 1, I put a new point between this point and the adjacent 0 and check that point; and if it was 0, I put a new point between this zero and adjacent 1 and check that point</em>. I continue till I arrive at a distance of $\\Delta$ between points.So I\'ve found the boundary. (This method is like bisection method  for finding the roots of a function)</p>\n\n<p>In the picture, the first iteration is shown.Black and yellow points are the points of the initial grid (that are distributed on the whole rectangle) and red points are those that are added after checking the initial grid nodes. Black points are points that are determined to satisfy the condition (are 1) and so are certainly inside the region. Yellow points are those that did not satisfy the condition and so are outside, and red points are those added in the 2nd iteration , between adjacent nodes with different results (between a 1 and a 0) ,according to the  above paragraph.</p>\n\n<p><img src="http://i.stack.imgur.com/CMgqT.png" alt="http://i.stack.imgur.com/ZqtFN.png"></p>\n\n<p>So , using this method I\'ve found the region with the same accuracy as in the first method, and saved a lot of time too.</p>\n\n<p>I want to know <em>how much this method is faster.</em> A qualitative answer that shows if it is better to implement this method , suffices. My problem is so computational intensive that I can\'t use the first approach.</p>\n', 'ViewCount': '124', 'Title': 'Complexity of an algorithm for bounding a region in 2D', 'LastEditorUserId': '8381', 'LastActivityDate': '2013-05-28T18:01:59.797', 'LastEditDate': '2013-05-28T18:01:59.797', 'AnswerCount': '2', 'CommentCount': '13', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8381', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><computational-geometry>', 'CreationDate': '2013-05-27T10:46:57.837', 'Id': '12305'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to build an online web-based machine learning system, where users can continuously add classified samples, and have the model updated online. I would like to use a perceptron or a similar online-learning algorithm. </p>\n\n<p>But, users may make mistakes and insert irrelevant examples. In that case, I would like to have the option to delete a specific example, without re-training the perceptron on the entire set of examples (which may be very large).</p>\n\n<p>Is this possible?</p>\n', 'ViewCount': '143', 'Title': 'Can a perceptron forget?', 'LastActivityDate': '2013-05-31T03:11:27.920', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '12389', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<machine-learning><online-algorithms>', 'CreationDate': '2013-05-27T11:12:29.320', 'FavoriteCount': '2', 'Id': '12306'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Starting to use <a href="https://code.google.com/p/nanoflann/" rel="nofollow">nanoflann</a> to do some point cloud nearest neighbor searching and it got me thinking about just how "approximate" ANN methods are.</p>\n\n<p>If I have a (more or less) randomly distributed point cloud what is the likelihood that I get the exact nearest neighbor given a target point within the clouds bounding box?  I know that it is dataset dependent... but does anyone have a good numerical study somewhere that shows trends?</p>\n', 'ViewCount': '94', 'Title': 'How approximate are "approximate" nearest neighbor (ANN) search algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T17:44:30.770', 'LastEditDate': '2013-05-28T06:46:45.047', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8395', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><approximation><heuristics>', 'CreationDate': '2013-05-27T20:13:40.783', 'FavoriteCount': '1', 'Id': '12310'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am looking for the English name of the following algorithm:</p>\n\n<p>We are given an array <code>a</code> with numbers and we need to be able to efficiently retrieve the sum of a continuous interval <code>[f,t]</code> of numbers in that array. In order to do that we precompute an array <code>sums</code>(of size <code>size(a) + 1</code>) that stores the sums of the prefixes of the initial array. More formally <code>sums[i] = a[0] + a[1] + ... a[i-1]</code>. This array can be constructed with linear complexity and now in order to compute the sum of the numbers in the interval <code>[f,t]</code>, we simply compute <code>sums[t]-sums[f-1]</code>.</p>\n\n<p>Direct translation of the name of the algorithm(or more precisely the datastructure) that I've seen used in Bulgaria is <code>prefix array</code>, but in my experience direct translation often turns out to be wrong when it comes to algorithms and data structures. </p>\n\n<p>How is this algorithm(or datastructure) called in English?</p>\n", 'ViewCount': '96', 'Title': 'Looking for the English name of algorithm using a precomputed array for interval sum computation', 'LastActivityDate': '2013-05-28T13:37:42.383', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12332', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8403', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-05-28T10:00:24.787', 'Id': '12330'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question is an extension of <a href="http://cs.stackexchange.com/questions/12305/complexity-of-an-algorithm-for-bounding-a-region-in-2d">a previous question</a> I\'ve asked.</p>\n\n<p>Consider the rectangle $a&lt;x&lt;b , c&lt;y&lt;d$ in the $\\mathbf R^2$ plane. Each point in this rectangle can be of kind #1 or #2 (We have to check each point to know its kind).</p>\n\n<p>Assume that somehow we know that the points of kind 1 (and so the points of kind 2) form a connected region (i.e. , 1s and 2s are not scattered in the plane arbitrarily). Given the condition of being of kind 1 or 2, The goal is to find the region occupied by 1s (a <em>search</em> problem). Consider somehow we know the following attributes of the region occupied by 1s: (one at a time)</p>\n\n<ol>\n<li>The region occupied by 1s forms a convex set (so it is <a href="http://en.wikipedia.org/wiki/1-connected" rel="nofollow">1-connected</a> too).\n<img src="http://i.stack.imgur.com/xKMBb.png" alt="enter image description here"></li>\n</ol>\n\n<p>2.The region occupied by 1s forms a simply connected region , but not necessarily convex.</p>\n\n<p><img src="http://i.stack.imgur.com/7fId9.png" alt="enter image description here"></p>\n\n<p>The simplest algorithm for finding the region of 1s is to simply start from bottom of the rectangle and sweep it and check all of the points in the rectangle to determine their kind and this way find the region.This is not an efficient algorithm, because we can use the known fact of convexity (or simply-connectivity) of the region of 1s to find it more easily without inspecting all of the points.</p>\n\n<p>What more efficient algorithms are there to find the region , as fast as possible? (with an acceptable accuracy, which is about 0.001 in my work). The regions may have sharp edges. But their detection is limited to the mentioned accuracy too. (It is clear that finding the boundary of the region suffices)</p>\n\n<p>Please don\'t forget that the problem is <em>to find an unknown set of points</em>, not <em>bound a known set of points</em>. i.e., <strong>it\'s a search problem , not a convex hull finding problem.</strong></p>\n\n<p><strong>(also, speed is very important for me)</strong></p>\n\n<p><strong>EDIT1:</strong> </p>\n\n<p>After some suggestions (in the comments) I should say that I think we can take advantage of simply-connectivity of the region to write an algorithm that tries to find the boundary of the region instead of checking more points to find the region directly.</p>\n', 'ViewCount': '105', 'Title': 'Efficient algorithms for finding a region in $\\mathbf R^2$', 'LastEditorUserId': '8381', 'LastActivityDate': '2013-05-29T00:24:16.510', 'LastEditDate': '2013-05-28T23:26:13.947', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8381', 'Tags': '<algorithms><computational-geometry><search-problem>', 'CreationDate': '2013-05-28T20:28:05.040', 'Id': '12343'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m studying <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">CLRS</a>, and solving problems listed after each chapter. I\'m curious about problem 1-1. That is right after Chapter 1.</p>\n\n<p>The question is, what is the best way to find the largest integer $n$ such that $n \\lg(n) \\leq 10^6$.</p>\n\n<p>The simplest but the longest one is substitution. Are there some elegant ways to solve this?</p>\n\n<p>Some explanations: $n$ is what I should calculate - total quantity of input elements, $10^6$ - time in microseconds - total algorithm running time. I should figure out $n_{\\text{max}}$.</p>\n', 'ViewCount': '207', 'Title': 'Elegant approach to finding largest $n$ such that $n\\lg(n) \\leq 10^6$', 'LastEditorUserId': '472', 'LastActivityDate': '2013-06-10T23:50:01.823', 'LastEditDate': '2013-06-10T23:50:01.823', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '3', 'OwnerDisplayName': 'DaddyM', 'PostTypeId': '1', 'Tags': '<algorithms>', 'CreationDate': '2013-05-30T10:46:44.513', 'Id': '12380'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>This is my first question, so please, be soft on me.</p>\n\n<p>I have a following problem:</p>\n\n<ul>\n<li>I'm a programmer not a mathematician, I don't often understand pure mathematical language and marks or symbols, I need to write a program using Java/C++</li>\n<li>I have a histogram (table[vector] of size 255 filled with integers)</li>\n<li>I\u2019ve got to write an algorithm that will approximate it</li>\n<li><p>I just need to find a proper polynomial : let's assume that degree is given (for example 4), my job is to find best fitted adverbials for every degree</p>\n\n<p>Can anyone help me to write it or send links to a proper step-by-step algorithm?</p>\n\n<p>I am aware it's not an easy task, any help would be appreciated.</p></li>\n</ul>\n", 'ViewCount': '69', 'Title': 'Aproximation algorithm for histogram', 'LastActivityDate': '2013-05-31T11:17:08.347', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8461', 'Tags': '<algorithms><approximation><image-processing>', 'CreationDate': '2013-05-31T11:17:08.347', 'Id': '12395'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there any convex hull algorithm that can be extended to non-euclidean metric, such as the geodesic distance on the surface of a sphere?</p>\n', 'ViewCount': '54', 'Title': 'Convex Hull on a Spherical Surface', 'LastActivityDate': '2013-05-31T21:24:10.647', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7524', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-05-31T21:24:10.647', 'Id': '12405'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The problem is defined as:</p>\n\n<blockquote>\n  <p>Given $N$ points on an infinite line, find the least number of line segments of length $L$ that cover all points (including endpoints) after changing one point.</p>\n</blockquote>\n\n<p>That means that if I have the points $P = 1,10,15,28$ , $L = 10$ , 3 line segments are needed , changes will occur that the point $i$ will move to position $X$ , after each change , get the number of line segments needed.</p>\n\n<p>Example:</p>\n\n<p>$P = 1,5,11,21,25$ , $L = 6$\n, Changes are</p>\n\n<p>$$\n1 , 29\n$$</p>\n\n<p>$$\n2,36\n$$</p>\n\n<p>after change #1 : $P = 5,11,21,25,29$ , number of line segments needed is 3</p>\n\n<p>after change #2 : $P = 11,21,25,29,36$ , number of line segments needed is 4</p>\n\n<p>What would be the best approach to do this? , Given that number of points can be $50,000$ and the number of changes can be $50,000$.</p>\n', 'ViewCount': '183', 'Title': 'Finding least number of line segments with length $L$ that cover $N$ points', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-03T23:19:59.423', 'LastEditDate': '2013-06-02T22:48:05.320', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8044', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-06-01T17:08:19.347', 'Id': '12410'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am reading Algorithm design manual by Skiena. It gives proof of Insertion sort by Induction. I am giving the proof described in the below.</p>\n\n<blockquote>\n  <p>Consider the correctness of insertion sort, which we introduced at the beginning of this chapter. The reason it is correct can be shown inductively:</p>\n  \n  <ol>\n  <li>The basis case consists of a single element, and by definition a\n  one-element array is completely sorted.</li>\n  <li>In general, we can assume that the first n \u2212 1 elements of array A\n  are completely sorted after n \u2212 1 iterations of insertion sort.</li>\n  <li>To insert one last element x to A, we find where it goes, namely the\n  unique spot between the biggest element less than or equal to x and\n  the smallest element greater than x. This is done by moving all the\n  greater elements back by one position, creating room for x in the\n  desired location.</li>\n  </ol>\n</blockquote>\n\n<p>I do not understand paragraph #3. Could someone please explain it to me with an example?</p>\n', 'ViewCount': '368', 'Title': 'Insertion sort Proof by Induction', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-02T22:48:42.980', 'LastEditDate': '2013-06-02T22:48:42.980', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8496', 'Tags': '<algorithms><algorithm-analysis><sorting><correctness-proof><induction>', 'CreationDate': '2013-06-02T16:03:58.043', 'Id': '12434'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to understand what this text means in my textbook about distributed leader election algorithms, but I can\'t make any sense of it. Either they didn\'t explain what was meant, or I missed it somewhere.</p>\n\n<blockquote>\n  <p>In this section, we show that the leader election algorithm of Section 3.3.2 is asymptotically optimal. That is, we show that any algorithm for electing a leader in and asynchronous ring sends at least $\\Omega(n\\log n)$ messages. The lower bound we prove is for uniform algorithms, namely, algorithms that do not know the size of the ring.</p>\n  \n  <p>We prove the lower bound for a special variant of the leader election problem, where the elected leader must be the processor with the maximum identifier in the ring; in addition, all the processors must know the identifier of the elected leader. The proof of the lower bound for the more general definition of the leader election problem follows by reduction.</p>\n  \n  <p>Assume we are given a uniform algorithm $A$ that solves the above variant of the leader election problem. We will show that there exists an admissible execution of $A$ in which $\\Omega(n\\log n)$ messages are sent. <strong>Intuitively, this is done by building a "wasteful" execution of the algorithm for rings of size $n/2$, in which many messages are sent.</strong> Then we "paste together" two different rings of size $n/2$ to form a ring of size $n$, in such a way that we can combine the wasteful executions of the smaller rings and force $\\Theta(n)$ additional messages to be received.</p>\n  \n  <p>Although the preceding discussion referred to pasting together exections, we will actually work with <strong>schedules</strong>. The reason is that executions include configurations, which pin down the number of processors in the ring. We will want to apply the same sequence of events to different rings, with different numbers of processors. Before presenting the details of the lower bound proof, we first define schedules that can be "pasted together".</p>\n  \n  <p><em>A schedule $\\sigma$ of $A$ for a particular ring is open if there exists an edge $e$ of the ring such that in $\\sigma$ no message is delivered over the edge $e$ in either direction; $e$ is an open edge of $\\sigma$.</em></p>\n</blockquote>\n\n<p>There is lots more, but I don\'t know if I should type it all out if only for copyright reasons. I hope this is enough to help clarify my question and get an explanation.</p>\n', 'ViewCount': '40', 'Title': 'What is a "wasteful" execution? And a "schedule"?', 'LastActivityDate': '2013-06-03T07:03:00.550', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6569', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2013-06-02T18:42:41.907', 'Id': '12435'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Bloom filter use a hash function to test membership for S by checking if an item is present of not at the specified position. To mitigate the effect of hash collision, multiple functions are used, yielding probabilistic bound if using universal hash.\nWe can use 10 bits per elements to have 'reasonable' error rate.</p>\n\n<p>If we could build directly a perfect hashing function for the set  S + $\\infty$, where last the element is one not present in S, we could use only 1 bit per element and have perfect recovery.</p>\n\n<p>What are the fundamental reasons why this reasonning is wrong ?</p>\n", 'ViewCount': '182', 'Title': 'Bloom filter and perfect hashing', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:07.190', 'LastEditDate': '2014-04-29T11:35:07.190', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4469', 'Tags': '<data-structures><hash><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2013-06-03T15:20:55.557', 'Id': '12444'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here is the question:</p>\n\n<p>I have a given tree with n nodes. The task is to find the number of subtrees of the given tree with outgoing edges to its complement less than or equal to a given number K.</p>\n\n<p>for example: If <code>n=3</code> and <code>k=1</code></p>\n\n<p>and the given tree is    <code>1---2---3</code></p>\n\n<p>Then the total valid subtrees would be 6</p>\n\n<pre><code>{}, {1}, {3}, {1,2}, {2,3}, {1,2,3}\n</code></pre>\n\n<p>I know I can enumerate all <code>2^n</code> trees and chack the valid ones, but is there some approach that is faster? Can I achieve polynomial time in <code>n</code>? Something close to <code>O(n^3)</code> or even <code>O(n^4)</code> would be nice.</p>\n\n<p>for k=1 this value turns out to be <code>2*n</code></p>\n\n<p><strong>There was a solution provided for this one as:</strong></p>\n\n<p>This is a fairly typical instance of the DP-on-a-tree paradigm. Let's generalize the problem slightly by allowing the specification of a root vertex v and stratifying the counts of the small-boundary trees in two ways: whether v is included, and how many edges comprise the boundary.</p>\n\n<p>The base case is easy. There are no edges and thus two subtrees: one includes v, the other excludes v, and both have no boundary edges. Otherwise, let e = {v, w} be an edge incident to v. The instance looks like this.</p>\n\n<pre><code>|\\         /|\n| \\   e   / |\n|L v-----w R|\n| /       \\ |\n|/         \\|\n</code></pre>\n\n<p>Compute recursively the stratified counts for L rooted at v and R rooted at w.</p>\n\n<p>Subtrees that include v consist of a subtree in L that includes v, plus optionally e and a subtree in R that includes w. Subtrees that don't include v consist of either a subtree in L that doesn't include v, or a subtree in R (double counting the empty tree). This means we can obtain the stratified counts by convolving the stratified counts for L with the stratified counts for R.</p>\n\n<p>Here's how this works on your example. Let's choose root 1.</p>\n\n<pre><code>  e\n1---2---3\n</code></pre>\n\n<p>We choose e as shown and recurse.</p>\n\n<pre><code>1\n</code></pre>\n\n<p>The vector for includes-1 is [1], since the one subtree is {1}, with no boundary. The vector for excludes-1 is [1], since the one subtree is {}, also with no boundary.</p>\n\n<pre><code>2---3\n</code></pre>\n\n<p>We compute 2 and 3 as we did for 1. The vector for includes-2 is [1, 1], since {2, 3} has no boundary edges, and {2} has one. We obtained this vector by adding the includes-2 vector for 2, shifted by one because of the new boundary edge to make [0, 1], to the convolution of the includes-2 vector for 2 with the includes-3 vector for 3, which is [1, 0]. The vector for excludes-2 is [1] + [1, 1] - [1] = [1, 1], where [1, 1] is the sum of the shifted includes-3 vector and the excludes-3 vector, and the subtraction is to compensate for double-counting {}.</p>\n\n<p>Now, for the original invocation, to get the includes-1 vector, we add [0, 1], the includes-1 vector for 1 shifted by one, to the convolution of [1] with [1, 1], obtaining [1, 2]. To check: {1, 2, 3} has no boundary, and {1} and {1, 2} have one boundary edge. The excludes-1 vector is [1] + [1, 2, 1] - [1] = [1, 2, 1]. To check: {} has no boundary, {2, 3} and {3} have one boundary edge, and {2} has two boundary edges.</p>\n\n<p><strong>I am unable to understand this fully. Can anyone help?</strong></p>\n", 'ViewCount': '242', 'LastEditorDisplayName': 'user742', 'Title': 'Trouble understanding this dynamic programming solution', 'LastActivityDate': '2013-09-20T09:33:50.137', 'LastEditDate': '2013-09-20T09:33:50.137', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12449', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8505', 'Tags': '<algorithms><graph-theory><dynamic-programming><trees>', 'CreationDate': '2013-06-03T16:03:41.183', 'Id': '12445'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Recently I got interested in a game called <a href="http://en.wikipedia.org/wiki/Toads_and_Frogs_%28game%29" rel="nofollow">Toads and Frogs</a> and I\'m trying my best to come up with some software which would be able to beat an average (i.e. not knowing the strategy) human though I\'m struggling with the strategy.</p>\n\n<p>I read on it everything I could google out but turns out there\'s not that much to read about it as one may think - the most helpful thing I\'ve found was probably an <a href="http://compgeom.cs.uiuc.edu/~jeffe/pubs/pdf/toads.pdf" rel="nofollow">analysis by Erickson</a> though it concentrates more on evaluating each position rather than some algorithm which would have to decide how to move a toad or a frog given a particular board as input. Same goes for "Winning Ways", where they evaluate a bunch of positions but don\'t give you too much insight on how to optimally play your game</p>\n\n<p>The best playthrough strategy I could come up with is evaluating the value of the current  position (using "Winning Ways" and Erickson\'s tricks), evaluating the values of all the positions we can go to with  our toads/frogs in a given moment and then performing a move which leads to the lowest  value of the board so that our opponent has the worst moves to choose  from. </p>\n\n<p>Is there anything better? Or if all the "good" strategies are very hard (as I said, I\'m just starting out with my game theory interest), what\'s the best one that even someone with my experience could utilize? :)</p>\n', 'ViewCount': '218', 'Title': 'Finding a winning strategy for toads and frogs', 'LastEditorUserId': '5281', 'LastActivityDate': '2013-06-05T16:04:19.097', 'LastEditDate': '2013-06-05T13:20:39.710', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '12473', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5281', 'Tags': '<algorithms><combinatorics><game-theory>', 'CreationDate': '2013-06-05T12:37:00.333', 'Id': '12471'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I want to solve the Physical Travelling Salesman Problem using an evolutionary algorithm.</p>\n\n<p>The objective of the PTSP is to visit the maximum number of waypoints of the map in the minimum number of time steps. \nThe map takes the form of a two-dimensional board, where ten waypoints are scattered around and multiple obstacles are present.</p>\n\n<p>Now, I have a way to run random games and create "population" \u2014 a list of waypoints in the order we visit them.\nI need some idea how to do the crossover. I mean, how to create the next generation.\nI saw ideas, but they take in account that each gene contains <em>all</em> the waypoints on the map. (Take a random part from parent1 and take the waypoints that do not appear in parent1 from parent2 in the order they appear in parent2).\nWhat to do in case a gene contains just part of the waypoints?</p>\n', 'ViewCount': '120', 'Title': 'Evolutionary algorithm for the Physical Travelling Salesman Problem', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-06T15:03:33.913', 'LastEditDate': '2013-06-06T15:03:33.913', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'user2459338', 'PostTypeId': '1', 'OwnerUserId': '8546', 'Tags': '<genetic-algorithms><traveling-salesman>', 'CreationDate': '2013-06-06T11:02:17.433', 'Id': '12485'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a network $G=(V,E)$ , a max flow f and an edge $e \\in E$ , I need to find an efficient algorithm in order to detect whether there is some min cut which contains $e$.\nAnother question is, how do I decide whether if $e$ is the lightest edge of at least one minimal cut?</p>\n\n<p>I've thought about running Ford-Fulkerson algorithm, and then increasing / decreasing the capacity of the given edge and see what happens, but I haven't came up with something that might help me solve the problem.</p>\n\n<p>I'd be grateful if anyone could point me to the solution, thanks in advance.</p>\n", 'ViewCount': '178', 'Title': 'Max-Flow: Detect if a given edge is found in some Min-Cut', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-08T15:56:02.467', 'LastEditDate': '2013-06-07T20:49:03.987', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8202', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2013-06-07T13:10:00.247', 'Id': '12507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let  consider a general version of <a href="http://en.wikipedia.org/wiki/Two_Generals%27_Problem" rel="nofollow">Two Generals\' Problem</a>, when there are $n$ generals located on the arbitrary graph and they should agree on exactly the same value whether to attack or not to attack. </p>\n\n<p>It\'s well known  that Two Generals\' Problem represents a version of the <a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29" rel="nofollow">Consensus Problem</a> with unlimited number of the stopping failures, I think this is the only reason why Two Generals\' Problem and Generals\' Problem (with $n$ generals) don\'t have solution.</p>\n\n<p>There is a proof of lacking solution for Two Generals\' Problem (can be found in the textbook of Lynch).</p>\n\n<p>The following is the exercise from the textbook of Lynch, that I have not solved so far.</p>\n\n<p>Show that a solution to the (deterministic) coordinated attack problem (Generals\' Problem) for any nontrivial connected graph implies a solution for the simple graph consisting of two processes connected by one edge. (Therefore, this problem is unsolvable in any nontrivial graph.)</p>\n\n<p>Apparently, there is a reduction from an edge case to graph. But how to show it mathematically rigorous?</p>\n\n<p><strong>Addendum:</strong></p>\n\n<p>Can I say something like this?\nWhen we are given the primary problem of Two Generals and they initial values $a_i$ (inclination whether to attack or not), we in arbitrarily add more dummy generals with the only requirement if $a_1=a_0=a$ for primary problem set all dummy\'s general input to $a$, otherwise set arbitrary input value $b \\in \\{0,1\\}$. Find the solution on the graph, the solution on the graph is the solution for the primary problem. </p>\n', 'ViewCount': '157', 'LastEditorDisplayName': 'user742', 'Title': 'Coordinated Attack Problem On The Arbitrary Graph', 'LastActivityDate': '2013-09-20T17:48:32.680', 'LastEditDate': '2013-09-20T09:33:24.167', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4778', 'Tags': '<algorithms><complexity-theory><graph-theory><reductions><distributed-systems>', 'CreationDate': '2013-06-07T13:49:01.323', 'Id': '12508'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '854', 'Title': 'Convex polygon formulation', 'LastEditDate': '2013-06-08T08:41:20.797', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8505', 'FavoriteCount': '1', 'Body': '<p>We have a sorted list of side lengths that can be used to form a polygon. There are $n$ such values ($n \\le 1000$).</p>\n\n<p>Now we need to find if we can use any 10 of these values to form a non-degenerate convex polygon.</p>\n\n<p>How do we approach this? Anything up to the order of $O(n^2 \\log n)$ is acceptable. Better if possible. I need the general idea on how to proceed, the properties of convex polygons which can be exploited here, etc.</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '220', 'LastActivityDate': '2013-06-13T16:35:04.607', 'CommentCount': '5', 'AcceptedAnswerId': '12519', 'CreationDate': '2013-06-07T19:16:14.993', 'Id': '12516'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What are degenerate polygons? How to check whether a given pair of polygons is degenerate or not?</p>\n', 'ViewCount': '829', 'Title': 'What are degenerate polygons?', 'LastActivityDate': '2013-06-09T11:25:39.123', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '12547', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8505', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-06-07T23:24:57.893', 'FavoriteCount': '1', 'Id': '12521'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We have two integers $z, k$ </p>\n\n<p>We form a sequence now of first z natural numbers. i.e. $1, 2, 3, 4, \\ldots z$.</p>\n\n<p>Now we have to find total number of permutations of this sequence such that the sum of any two adjacent numbers is $ \\le k$</p>\n\n<p>( $z \\leq 10^6$, $\\;\\;z &lt; k &lt; 2*z$ )</p>\n\n<p>Here is what I have been able to think untill now. If <code>k=2*z-1,</code> the answer is <code>z!</code></p>\n\n<p>Now if we reduce the value of <code>k to 2*z-2</code>, then we take the highest pair as a group and permute with rest of the elements, we subtract this value from the previous case of <code>k=2*z-1</code></p>\n\n<p>i.e. <code>dp(z,k)= z!</code> for <code>k=2*z-1</code>. and <code>dp(z,k-1)=dp(z,k)-(z-1)!*2</code> for <code>k=2*z-2</code>.</p>\n\n<p>I want to know if I am going in the right direction. Any help on the closed form would be good.</p>\n', 'ViewCount': '278', 'Title': 'Permuting natural numbers', 'LastEditorUserId': '8505', 'LastActivityDate': '2013-06-09T02:10:39.487', 'LastEditDate': '2013-06-08T21:07:24.343', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '12548', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8505', 'Tags': '<algorithms><permutations><mathematical-programming>', 'CreationDate': '2013-06-08T00:10:29.857', 'Id': '12522'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $a$ and $b$ be integers, and let $\\text{RANDOM}(a,b)$ be a method returning an integer from the range $[a,b]$ uniformly at random. Now consider the following program, that takes as input an array $A$ of integers.</p>\n\n<pre><code>PERMUTE-BY-SORTING(A)\n    1. n = A.length\n    2. let P[1..n] be a new array\n    3. for i = 1 to n \n    4.     P[i] = RANDOM(1, n^3)\n    5. sort A, using P as sort keys\n</code></pre>\n\n<p>I\'m solving the problem 5.3-6 in <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">CLRS</a>, which is asking me to explain how to implement the algorithm PERMUTE-BY-SORTING to handle the case in which two or more priorities are identical. In other words, the algorithm should produce a uniform random permutation, even if two or more priorities are identical. </p>\n\n<p>Because priorities are repeated in $P$, we will not get a uniform random permutation. I thought of adding <code>i</code> to step 4 but that doesn\'t produce the uniform random permutation. More specifically, the problem is that if two or more priorities are identical we will not get a uniform random permutation since the probability is not same for all the numbers. Ex 1,2,2,3 the probability of 2 in the example is 1/2 and the probability of 1 is 1/4 and 3 is 3/4. </p>\n', 'ViewCount': '319', 'Title': 'PERMUTE-BY-SORTING with similar priorities', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-02T12:14:26.043', 'LastEditDate': '2013-06-09T21:52:16.823', 'AnswerCount': '2', 'CommentCount': '11', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8263', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-06-09T00:49:55.697', 'FavoriteCount': '2', 'Id': '12551'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Apparently, if we use Djikstra's algorithm to find the shortest path between the root node and all other nodes in a weighted graph with no negative cycles, we are done after updating the distance of each node $|V| - 1$ times.</p>\n\n<p>This puzzles me because I think that a single round of breadth first search is enough. Why must we do $|V| - 1$ of these searches?</p>\n", 'ViewCount': '59', 'Title': "For Djikstra's algorithm, why are we surely done if we update all edges $|V|-1$ times?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-10T09:44:37.167', 'LastEditDate': '2013-06-10T09:44:37.167', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12572', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><graphs><shortest-path><correctness-proof>', 'CreationDate': '2013-06-09T09:20:18.353', 'Id': '12558'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '151', 'Title': 'Use minimum number of swaps so each bin contains balls of the same color', 'LastEditDate': '2013-06-12T02:25:54.510', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '220', 'FavoriteCount': '3', 'Body': '<p>There are $n$ bins, the $i$th bin contain $a_i$ balls. The balls has $n$ colors, there are $a_i$ balls of color $i$. Let $m=\\sum_{i=1}^n a_i$.</p>\n\n<p>A swap is take a ball from one bin and swap with a ball from another bin. We want minimum number of swaps such that each bin only contain balls with the same color. </p>\n\n<p>I know a easy special cases $a_i\\leq 2$ for all $i$. (If $a_i=2$ for all $i$, then you can even do it by swapping each ball at most once.)</p>\n\n<p><strong>Edit</strong>: This is wrong because finding $c(D)$ is NP-hard.</p>\n\n<p><del>If we know which color goes to which bin, the problem is easy.</del></p>\n\n<p><del>Consider a multi-digraph $D=(V,A)$, $V=\\{v_1,\\ldots,v_n\\}$. If we know color $i$ goes to bin $b(i)$, then there are $k$ parallel arcs $(j,b(i))$ in $A$ iff bin $j$ contains $k$ balls of color $i$. Each component of the graph is Eulerian. \nThe minimum number of swaps required is $m-c(D)$, where $c(D)$ is the number of arc disjoint cycles that covers $A$. \nWe can swap by "following" a Eulerian circuit. (a swap using an arc of a minimal cycle can change it to a smaller minimal cycle and a self loop). Once the entire graph is set of self loops, we have made all the necessary swaps. </del></p>\n\n<p>How hard is this problem in general?  </p>\n', 'Tags': '<algorithms><complexity-theory>', 'LastEditorUserId': '220', 'LastActivityDate': '2013-06-12T02:25:54.510', 'CommentCount': '0', 'AcceptedAnswerId': '12630', 'CreationDate': '2013-06-09T10:31:21.867', 'Id': '12560'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>By default, a hash table is unordered.  I have a simple question on the retrieval of elements in a hash table</p>\n\n<blockquote>\n  <p>Can we retrieve elements from a hash table in the same order as they are put inside?</p>\n</blockquote>\n', 'ViewCount': '786', 'Title': 'Retrieving data from hash table ordered by insertion time', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-07-30T08:02:11.913', 'LastEditDate': '2013-07-30T08:02:11.913', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12565', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8596', 'Tags': '<algorithms><data-structures><hash><hash-tables>', 'CreationDate': '2013-06-09T10:50:57.380', 'Id': '12562'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was thinking about the following problem the other day:</p>\n\n<blockquote>\n  <p>You are stuck on a desert island with a battery-powered emergency radio.  You know that if you feed enough power into the radio that you should be able to alert someone to your position.  In fact, as soon as you provide the radio enough power to reach someone else, they'll respond back to you and send help.</p>\n  \n  <p>The problem is that you don't know where anyone else is, so you have no idea how much power to feed into the radio.  You can assume that the radio can transmit at power levels that are natural numbers.</p>\n  \n  <p>What is the most power-efficient way to signal for help?</p>\n</blockquote>\n\n<p>One inefficient solution to this would be to transmit at power $1, 2, 3, ...$ until you reach the necessary power $n$.  This works, but it requires power usage $1 + 2 + 3 + ... + n = \\frac{n(n+1)}{2}$.</p>\n\n<p>The algorithm I currently think is optimal is to instead transmit at power $1, 2, 4, 8, 16, ... $ until the necessary power $n$ is reached or exceeded.  This requires power \n$$1 + 2 + 4 + 8 + ... + 2^{\\lceil \\log_2 n \\rceil} = 2^{\\lceil \\log_2 n \\rceil + 1} - 1$$</p>\n\n<p>which is somewhere between $2^{\\log_2 n + 1} - 1 = 2n - 1$ and $2^{\\log_2 n + 2} - 1 = 4n - 1$</p>\n\n<p>My question is whether or not this is the optimal algorithm for solving this problem, given that $n$ is completely unknown.  I can't seem to find a more efficient algorithm, nor can I find a proof that this is optimal.</p>\n\n<p>Is there a better way to solve this problem?  Or is this the optimal solution?</p>\n", 'ViewCount': '56', 'Title': 'Optimal algorithm for signaling for help?', 'LastActivityDate': '2013-06-10T03:36:02.400', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms>', 'CreationDate': '2013-06-10T02:48:58.237', 'Id': '12578'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am reading <a href="http://www.cs.sysu.edu.cn/~lxm/DSA/textbook/Skiena.-.TheAlgorithmDesignManual.pdf" rel="nofollow">The Algorithm Design Manual - 2nd Edition</a>. Can somebody explain me the war story given in chapter 1 named <code>War Story: Psychic Modeling</code>.</p>\n\n<p>The story:</p>\n\n<blockquote>\n  <p>At Lotto Systems Group, we market a program designed to improve our customers\u2019 psychic ability to predict winning lottery numbers. In a standard lottery, each ticket consists of six numbers selected from, say, 1 to 44. Thus, any given ticket has only a very small chance of winning. However, after proper training, our clients can visualize, say, 15 numbers out of the 44 and be certain that at least four of them will be on the winning ticket.</p>\n</blockquote>\n', 'ViewCount': '58', 'Title': 'Can any one describe Set Cover modeling?', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-06-10T11:57:50.067', 'LastEditDate': '2013-06-10T11:57:50.067', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8496', 'Tags': '<algorithms>', 'CreationDate': '2013-06-10T08:36:16.210', 'Id': '12590'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have given an undirected graph $G$ with vertex $\\{1, ... n\\}$ and two star subgraphs $S_1$ and $S_2$, always consisting of ALL neighbors of a given vertex,  and the goal is to check wether the two star graphs have a vertex in common. This will be will be executed $M$ times for $M$ a large integer.</p>\n\n<p>My approach would be to store the graph in adjacency list format and for each vertex store its adjacency list in sorted order.</p>\n\n<p>We can then check in $O(M n \\log n)$ time in total if two star graphs of the sequence of $M$ star graph pairs have a vertex in common. </p>\n\n<p>But maybe this can be done more efficiently?</p>\n', 'ViewCount': '124', 'LastEditorDisplayName': 'user742', 'Title': 'Efficiently checking if two star graphs are disjoint', 'LastActivityDate': '2013-09-20T09:32:52.133', 'LastEditDate': '2013-09-20T09:32:52.133', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8613', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-06-10T19:58:39.027', 'FavoriteCount': '1', 'Id': '12603'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Don Knuth\'s famous series of books, <em>The Art of Computer Programming</em>, section 2.3.1, he describes an algorithm to traverse binary tree in inorder, making use of an auxiliary stack:</p>\n\n<blockquote>\n  <p><strong>T1</strong> [Initialize.] Set stack $\\rm A$ empty and set the link variable $\\rm P\\gets T$</p>\n  \n  <p><strong>T2</strong> [$\\rm P=\\Lambda$?] If $\\rm P=\\Lambda$, go to step T4.</p>\n  \n  <p><strong>T3</strong> [Stack$\\rm \\;\\Leftarrow P$] (Now $\\rm P$ points to a nonempty binary tree that is to be traversed.) push the value of $\\rm P$ onto stack $\\rm A$, then set $\\rm P\\gets LLINK(P)$</p>\n  \n  <p><strong>T4</strong> [$\\rm P\\Leftarrow Stack$] If stack $\\rm A$ is empty, the algorithm terminates; otherwise pop the top of $\\rm A$ to $\\rm P$.</p>\n  \n  <p><strong>T5</strong> [Visit $\\rm P$] Visit $\\rm NODE(P)$. Then set $\\rm P\\gets RLINK(P)$ and return to step T2.</p>\n</blockquote>\n\n<p>We can plot a flow chart of the algorithm. In the succeeding paragraph, he gives a <em>formal proof</em> of the algorithm:</p>\n\n<blockquote>\n  <p>Starting at step T2 with $\\rm P$ a pointer to a binary tree of $n$ nodes and with the stack $\\rm A$ containing $\\rm A[1]\\dotsc A[m]$ for some $m\\ge 0$, the procedure of steps T2-T5 will traverse the binary tree in question, in inorder, and will then arrive at step T4 with stack $\\rm A$ returned to its original value $\\rm A[1]\\dotsc A[m]$.</p>\n</blockquote>\n\n<p>However, as far as I know, such a formal proof is quite different from the general method described in section 1.2.1:</p>\n\n<blockquote>\n  <p>for each box in the flow chart, that if an assertion attached to any arrow leading into the box is true before the operation in that box is performed, then all of the assertions on relevant arrows leading away from the box are true after the operation.</p>\n</blockquote>\n\n<p>In fact, such a method is somewhat equivalent to <a href="http://en.wikipedia.org/wiki/Hoare_logic">Hoare logic</a>, which is used to formally check the validity of algorithms.</p>\n\n<p>Can we turn the statement mentioned to prove the traversing algorithm into a schema of Hoare logic, or the assertion-attachment of a flow chart?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '122', 'Title': 'Question about the formal proof of the inorder traversing', 'LastActivityDate': '2013-06-11T20:17:23.337', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1715', 'Tags': '<algorithms><correctness-proof><graph-traversal><hoare-logic>', 'CreationDate': '2013-06-11T08:47:50.447', 'Id': '12610'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Recently I\'ve seen this very good apps</p>\n\n<p><a href="https://play.google.com/store/apps/details?id=com.kingjim.shotnote&amp;hl=en" rel="nofollow">https://play.google.com/store/apps/details?id=com.kingjim.shotnote&amp;hl=en</a>\n<a href="https://itunes.apple.com/JP/app/id411332997?mt=8" rel="nofollow">https://itunes.apple.com/JP/app/id411332997?mt=8</a></p>\n\n<p>this apps reconstruct the image from the camera based on the marker (this is what I think :) ) it also have a special paper that have marker. What I\'ve still haven\'t got is what is the algorithm to do this? what computer vision theory used in this one? I understand artificial intellegence and basic computer vision, is there anyone have the ide how can this be done? at first I thought this based on stereo image but based on what I\'ve studied before stereo vision need two camera. I hope my question is clear enough but if is there any information needed please ask in the comment :)</p>\n\n<p>Thank you!</p>\n', 'ViewCount': '112', 'Title': 'Computer Vision - Algorithm for reconstructing tilted image', 'LastActivityDate': '2014-03-07T09:52:25.013', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4573', 'Tags': '<algorithms><artificial-intelligence><computer-vision>', 'CreationDate': '2013-06-11T12:45:29.420', 'Id': '12618'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to understand the algorithms by Peterson and Dekker which are very similar and display a lot of symmetries.</p>\n\n<p>I tried to formulate the algorithms in informal language like follows:</p>\n\n<pre><code>Peterson\'s: "I want to enter."                 flag[0]=true;\n            "You can enter next."              turn=1;\n            "If you want to enter and          while(flag[1]==true&amp;&amp;turn==1){\n            it\'s your turn I\'ll wait."         }\n            Else: Enter CS!                    // CS\n            "I don\'t want to enter any more."  flag[0]=false;\n\nDekker\'s:   "I want to enter."                 flag[0]=true;\n            "If you want to enter              while(flag[1]==true){\n             and if it\'s your turn               if(turn!=0){\n             I don\'t want to enter any more."      flag[0]=false;\n            "If it\'s your turn                     while(turn!=0){\n             I\'ll wait."                           }\n            "I want to enter."                     flag[0]=true;\n                                                 }\n                                               }\n            Enter CS!                          // CS\n            "You can enter next."              turn=1;\n            "I don\'t want to enter any more."  flag[0]=false;\n</code></pre>\n\n<p>The difference seems to be the point where <code>"You can enter next."</code> occurs and the fact that <code>"if it\'s your turn I don\'t want to enter any more."</code> occurs in Dekker\'s.</p>\n\n<p>In Peterson\'s algorithm, the two processes seem to be dominant. A process seems to force his way in into the critical section unless it\'s the other one\'s turn.</p>\n\n<p>Conversely, in Dekker\'s algorithm, the two processes seem to be submissive and polite. If both processes want to enter the critical section, and it\'s the other one\'s turn, the process decides to no longer want to enter. (Is this needed for starvation-freedom? Why?)</p>\n\n<p>How exactly do these algorithms differ? I imagine that when both processes try to enter the critical section, in Peterson\'s, the process says "I enter", while in Dekker\'s the process says "You may enter". Can someone clear up the way the processes behave in each algorithm? Is my way of putting it in informal terms correct?</p>\n', 'ViewCount': '2071', 'Title': u'Understanding Peterson\u2019s and Dekker\u2019s algorithms', 'LastActivityDate': '2013-12-01T18:19:12.083', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '8623', 'Tags': '<algorithms><concurrency>', 'CreationDate': '2013-06-11T14:47:31.950', 'FavoriteCount': '3', 'Id': '12621'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Or at least generate a set of strings that one NFA accepts, so I can feed it into the other NFA.  If I do a search through every path of the NFA, will that work?  Although that will take a long time.</p>\n', 'ViewCount': '149', 'Title': 'Is there a way to test if two NFAs accept the same language?', 'LastActivityDate': '2013-06-11T18:47:24.080', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '12625', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8184', 'Tags': '<algorithms><regular-languages><finite-automata>', 'CreationDate': '2013-06-11T17:23:26.430', 'FavoriteCount': '1', 'Id': '12624'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to solve Physical Travelling Salesman Problem using evolutionary algorithm and I have diffucult to detemine how to choose the parent , on which we do the crossover.</p>\n\n<p>Assume I have popultion of 100. \nThe parent should be selected randomally? or according to their fitness? \nIf I choose them randomally I don\'t think the result be enough good but in the other hand, if I will choose the 2 with the best fitness, I will always choose the same! (Maybe child  of them , but most of my population will never be choosed!).</p>\n\n<p>What I miss?</p>\n\n<p>I also need some advise on how to calculate fitness. The parameters are the amount of waypoints each van collect and the total time.</p>\n\n<p>My problem here is becuase there are unit that don\'t manage to collect any points and thus thier fitness is 0 and step by step my array full with 0....(I am working with the code here : <a href="http://www.ptsp-game.net/" rel="nofollow">http://www.ptsp-game.net/</a> ant there is problem of time exceed..)</p>\n', 'ViewCount': '56', 'Title': 'Evolutionary algorithm - how to select the parents', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-05T14:23:17.103', 'LastEditDate': '2014-02-05T14:23:17.103', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8546', 'Tags': '<traveling-salesman><genetic-algorithms>', 'CreationDate': '2013-06-13T05:07:01.893', 'Id': '12648'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I need your help with an exercise on <a href="http://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm" rel="nofollow">Ford-Fulkerson</a>.</p>\n\n<blockquote>\n  <p>Suppose you are given a flow network with capacities $(G,s,t)$ and you are also given the max flow $|f|$ in advance.</p>\n  \n  <p>Now suppose you are given an arc $e$ in $G$ and suppose this arc\'s capacity is increased by one.</p>\n  \n  <p>Give an efficent algorithm which returns true iff the increase of the capacity of the arc $e$ will allow an increase in the max flow.</p>\n</blockquote>\n\n<p>I suppose we shouldn\'t run Ford-Fulkerson again but somehow use the given $|f|$\u2026 Any ideas how?</p>\n', 'ViewCount': '237', 'LastEditorDisplayName': 'user742', 'Title': 'Effect of increasing the capacity of an edge in a flow network with known max flow', 'LastActivityDate': '2013-06-17T23:03:08.163', 'LastEditDate': '2013-06-17T23:03:08.163', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'SyndicatorBBB', 'PostTypeId': '1', 'OwnerUserId': '4514', 'Tags': '<algorithms><graph-theory><network-flow><weighted-graphs>', 'CreationDate': '2013-06-16T16:41:05.080', 'Id': '12703'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Consider 3-player game. </p>\n\n<p>Players $x,y,z$, each player has two strategies. $x$: $x_1$ and $x_2$, $y$: $y_1$ and $y_2$, $z:z_1$ and $z_2$.</p>\n\n<p>The outcome of the game are represented by the labels of the player who has playoff $1$, otherwise outcome is $0$.</p>\n\n<p>The best way to represent the game is to depict two tables, the first one for the case when $c$ plays $c_1$ and the second one when $c$ plays $c_1$</p>\n\n<p>$c$ plays $c_1$</p>\n\n<pre><code>\u2554\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 c1 \u2551 b1 \u2551 b2  \u2551\n\u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 a1 \u2551    \u2551 B   \u2551\n\u2551 a2 \u2551 A  \u2551 A;C \u2551\n\u255a\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>\n\n<p>$c$ plays $c_2$</p>\n\n<pre><code>\u2554\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 c2 \u2551 b1  \u2551 b2  \u2551\n\u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 a1 \u2551 C   \u2551 A;B \u2551\n\u2551 a2 \u2551 B;C \u2551     \u2551\n\u255a\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>\n\n<p>The problem is to find all Nash equilibria (pure and mixed) and to show that there is no other Nash equilibria.</p>\n\n<p><strong>pure NE</strong>: There are three pure Nash equilibria $(x_2,y_2,z_1), (x_1,y_2,z_2), (x_2,y_1,z_2)$ (this is all I found, may be there is more).</p>\n\n<p><strong>mixed NE</strong>:Let's place equalities for mixed strategies. Assume that $p_x$ - the probability that $x$ will play $x_1$, therefore $(1-p_x)$- the probability that $x$ will play $x_2$ in the same manner define $p_y$ and $p_z$.</p>\n\n<p>For $x$ to be indifferent between $x_1$ and $x_2$ - $(1-p_y)(1-p_z) = p_y \\cdot p_z + (1-p_y)p_z$,</p>\n\n<p>For $y$ to be indifferent between $y_1$ and $y_2$ - $(1-p_x)(1-p_z) = p_x \\cdot p_z + p_x(1-p_z)$,</p>\n\n<p>For $z$ to be indifferent between $z_1$ and $z_2$ - $(1-p_x)(1-p_y) = p_x \\cdot p_y + (1-p_x)p_y$.</p>\n\n<p>Apparently there is a symmetry, however it's not a symmetric game.</p>\n\n<p>The main problem is how to proceed from this point. There are might be few cases either given one of the equalities we should consider only pure strategies of the rest two players or consider more complicated way when the rest two players play mixed strategies. May be because of the pattern we have symmetric solution? One more problem how to show that there are no other Nash equilibria.</p>\n\n<p><strong>Another approach:</strong> Let represent outcome in terms of vectors.</p>\n\n<pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 number \u2551 outcome \u2551 winners \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551      0 \u2551     000 \u2551 -       \u2551\n\u2551      1 \u2551     001 \u2551 C       \u2551\n\u2551      2 \u2551     010 \u2551 B       \u2551\n\u2551      3 \u2551     011 \u2551 A,B     \u2551\n\u2551      4 \u2551     100 \u2551 A       \u2551\n\u2551      5 \u2551     101 \u2551 B,C     \u2551\n\u2551      6 \u2551     110 \u2551 A,C     \u2551\n\u2551      7 \u2551     111 \u2551 -       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>\n\n<p>It looks like every player have exactly the same chances to win, every player has three permutations  that leads to payoff 1. $A=\\{3,4,6\\}, B=\\{2,3,5\\}, C=\\{1,5,6\\}$, among them the following outcomes appear twice $(3) 011, (5), 101 (6) 110$.</p>\n\n<p>The problem is I don't know how to approach the solution, I would appreciate for ideas.</p>\n", 'ViewCount': '106', 'Title': 'Nash equilibria in 3-player game with symmetry', 'LastActivityDate': '2013-06-17T10:45:38.010', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8200', 'Tags': '<algorithms><game-theory>', 'CreationDate': '2013-06-17T10:45:38.010', 'Id': '12710'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been working on this graph and just completely botching it. I mean to say that my solution may be the worst possible other than if a monkey had thrown darts at the graph to decide the next path.  Anyhow, I\'m lost and really trying to get a grasp of where my conclusion and the proper conclusion diverged dramatically.</p>\n\n<p>I wanted to perform a DFS, show discovery/finish times, the DF forest, and edge classifications. I assumed that: 1) The vertices are listed in alphabetical order in each adjacency list. 2) The vertices are taken in the alphabetical order in the main loop of the DFS algorithm.</p>\n\n<p><img src="http://i.stack.imgur.com/fgGDu.jpg" alt=""></p>\n\n<p>Should I be treating this as a directed acyclic graph?</p>\n', 'ViewCount': '103', 'Title': 'How to perform alphabetically ordered DFS?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-18T00:02:57.537', 'LastEditDate': '2013-06-17T21:22:38.820', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12729', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8396', 'Tags': '<graph-theory><graphs><search-algorithms>', 'CreationDate': '2013-06-17T20:47:29.400', 'Id': '12728'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '213', 'Title': 'Get nodes that are participating in any cycle in a graph', 'LastEditDate': '2013-06-20T12:30:34.290', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8044', 'FavoriteCount': '1', 'Body': '<p>I have a problem that states the following :</p>\n\n<blockquote>\n  <p>Given a cyclic graph , output for each node if the node removes all cycles in the graph.</p>\n</blockquote>\n\n<p>The most trivial way to do this is using a Union-find disjoint set , and for each node , try <strong>not</strong> putting it in the Union-find disjoint set , if there are no cycles , then this node should output "Yes" , otherwise "No".</p>\n\n<p>This approach would take about $\\Theta(N^2)$ time and $\\Theta(N)$ memory.</p>\n\n<p>The problem also stated that $N \\leq 1,000,000$ which would definitely get a TLE (Time Limit Exceeded) Answer on any problem.</p>\n\n<p>So, my question is, What\'s the algorithm that would take $\\Theta(N \\lg N) $ or $\\Theta(N)$ time and $\\Theta(N)$ memory?</p>\n', 'Tags': '<algorithms><graph-theory><data-structures>', 'LastEditorUserId': '8044', 'LastActivityDate': '2013-06-21T23:01:41.097', 'CommentCount': '0', 'AcceptedAnswerId': '12763', 'CreationDate': '2013-06-19T14:55:23.630', 'Id': '12762'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have an issue for which I am looking for an algorithm (if it exists)</p>\n\n<p>What I have:\nAn array of items which have certain properties, e.g. item $A$ has properties $x$ and $y$.</p>\n\n<p>Example: $[ A(x,y), B(x,y), C(x,y), D(x,y), E(x,y) ]$</p>\n\n<p>What I want:\nA result list consisting of elements of the original list, such as $[ A(x,y), C(x,y), E(x,y) ]$, for which the following properties are true:</p>\n\n<ul>\n<li>No reordering of elements, they are in the same order as the original list</li>\n<li>The result has the maximum number of elements, i.e. the longest 'path' possible</li>\n<li>For each pair of consecutive items $(A(x,y), B(x,y))$ in the result, $A.x \\lt B.y$. In other words, an item's $x$ must be less than the next item's $y$.</li>\n</ul>\n\n<p>Complexity: The list in the case I have is about 35 items long, so an algorithm which is $O(n!)$ might not work.</p>\n\n<p>Does such an algorithm exist?</p>\n", 'ViewCount': '102', 'Title': 'Longest subsequence such that A[i].x < A[i+1].y', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-27T11:03:43.220', 'LastEditDate': '2013-06-20T09:33:57.090', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8755', 'Tags': '<algorithms><sorting><subsequences><lists>', 'CreationDate': '2013-06-19T15:27:23.673', 'FavoriteCount': '1', 'Id': '12764'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been trying to solve this LCA problem for many queries on a tree of size ~ 10^5</p>\n\n<p>There are about 10^5 queries that have to be handled. What is the best way to do this? I am aware of the naive solution that takes linear time in worst case. I have read a few articles where they claimed it was analogous to Range Minimum Query. Like <a href="http://www14.in.tum.de/konferenzen/Jass03/presentations/kiefer.pdf" rel="nofollow">this one</a>. But after they find the L{} array things get hard to understand. I would be extremely greatful if someone could give a link to alternate solution or provide one. Something with a worst case complexity of O(log n) or lower would be fine. Thanks.</p>\n', 'ViewCount': '77', 'Title': 'Lowest Common Ancestor Problem', 'LastEditorUserId': '8757', 'LastActivityDate': '2013-06-19T21:24:34.693', 'LastEditDate': '2013-06-19T21:24:34.693', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8757', 'Tags': '<algorithms><trees>', 'CreationDate': '2013-06-19T21:18:42.883', 'Id': '12768'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem that I think should have been studied. I am looking for algorithms for it.</p>\n\n<p>Each item is a set of key-value pairs.\nLet $x$ be an item and $F$ be a set of items.</p>\n\n<p>Each key and each value can appear multiple times.\nThe number of possible keys and possible values can be arbitrary large.</p>\n\n<p>We are given $x$ and $F$. We want to find all those items $y$ in $F$ such that $y.val \\subseteq x$.</p>\n\n<p>For example,</p>\n\n<p>$x = \\{(a,1), (b,2), (c,3), (d,4)\\}$</p>\n\n<p>$F= \\{$<br>\n$(A, \\{(a,1)\\}), $<br>\n$(B, \\{(a,1), (b,2)\\}),$<br>\n$(C, \\{(a,1), (b,3)\\}),$<br>\n$(D, \\{(b,2), (c,3), (d,4)\\}),$<br>\n$(E, \\{(a,1), (b,2), (c,3), (d,4)\\}),$<br>\n$(F, \\{(a,1), (b,2), (c,3), (d,4), (e,5)\\}),$<br>\n$(G, \\{(a,1), (b,2), (c,3), (e,5)\\})$<br>\n$\\}$</p>\n\n<p>The answer is:\n$A$ yes, $B$ yes, $C$ no (right keys, wrong values), $D$ yes, $E$ yes (exact match),\n$F$ no, $G$ no.</p>\n\n<p>Has this problem been studied?</p>\n\n<p>The problem seems similar to finding features from a DNA sequence or detecting plagiarism in a document.</p>\n\n<p>I asked this problem in theoretical CS stack exchange and did not get very helpful answers. <a href="http://cstheory.stackexchange.com/questions/18052/find-all-items-which-are-subsets-of-an-item">http://cstheory.stackexchange.com/questions/18052/find-all-items-which-are-subsets-of-an-item</a></p>\n', 'ViewCount': '54', 'Title': 'Find all items which are subsets of an item', 'LastActivityDate': '2013-06-21T05:46:04.807', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8762', 'Tags': '<algorithms><search-algorithms><sets>', 'CreationDate': '2013-06-20T03:34:14.500', 'Id': '12777'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '217', 'LastEditorDisplayName': 'user742', 'Title': 'Find which vertices to delete from graph to get smallest largest component', 'LastEditDate': '2013-09-20T09:28:39.900', 'AnswerCount': '1', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8770', 'FavoriteCount': '1', 'Body': '<p>Given a graph $G = (V, E)$, find $k$ vertices $\\{v^*_1,\\dots,v^*_k\\}$, which removal would result in a graph with smallest largest component.  </p>\n\n<p>I assume for large $n = |V|$ and large $k$ the problem is difficult (NP-hard), but I am interested in small values of $k$ ($k \\in \\{1, 2, 3, 4\\}$).</p>\n\n<p>For $k = 1$, I think it is possible to find best vertex $\\{v^*_1\\}$ to remove by performing single depth-first-search of the graph (i.e., checking articulation points).</p>\n\n<p>For $k = 2$, it would be possible to find best vertices $\\{v^*_1, v^*_2\\}$ by performing $n$ depth-first searches (each of them for graph $G_i = G / \\{v_i\\}$). A similar approach could be applied in the case $k &gt; 2$.</p>\n\n<p>I wonder if there is any better solution than that.</p>\n\n<p>(Related: <a href="http://cs.stackexchange.com/questions/12783/find-min-no-of-vertices-to-remove-to-make-graph-max-component-k">counting the minimum number of vertices without necessarily enumerating them</a>)</p>\n', 'Tags': '<algorithms><complexity-theory><graph-theory><parametrized-complexity>', 'LastActivityDate': '2014-03-30T17:24:55.313', 'CommentCount': '4', 'AcceptedAnswerId': '12809', 'CreationDate': '2013-06-20T14:40:37.167', 'Id': '12789'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to understand how the Mersenne Twister random number generator works (in particular, the 32-bit TinyMT). I am still relatively new to the concept of RNG. As I read the source code, I noticed there were two ways to seed the MT: with a single integer seed or an array of integer seeds. What is the point in seeding with an array? Does it produce a better distribution or a longer period? </p>\n\n<p>Also, I would appreciate it if somebody could explain to me what is meant by the "state" of the RNG, as I am seeing that word all over the source code. Is it like a finite state machine in a way? </p>\n\n<p>Thanks for your time!</p>\n', 'ViewCount': '152', 'Title': 'Seeding the Mersenne Twister Random Number Generator', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-24T14:19:28.083', 'LastEditDate': '2014-03-24T14:19:28.083', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<algorithms><randomized-algorithms><pseudo-random-generators><random-number-generator>', 'CreationDate': '2013-06-20T16:09:56.607', 'Id': '12792'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let <code>S</code> be a system whose state can be altered by performing actions. Each action has two possible outcomes, and each outcome brings to a specific system state. A state is never visited two times, i.e., the state graph is a DAG (a tree, more specifically, where the root corresponds to the initial system state and each edge corresponds to an action). The probability of obtaining either one of the outcomes is known.</p>\n\n<p>Notice that just <code>B</code> actions can be performed, and the pool of actions contains <code>N</code> different actions.</p>\n\n<p>My objective is to devise an <strong>online optimal</strong> algorithm which identifies the best path to be taken, i.e., the path which guarantees the minimum cost. With the term "online" I am referencing to the following behavior:</p>\n\n<ol>\n<li>An action is chosen</li>\n<li>The system state is consequently modified</li>\n<li>A new action is chosen (taking into account the system state modification performed ad 2.)</li>\n<li>The system state is again modified according to the selected action</li>\n<li>...</li>\n</ol>\n\n<p>My first idea was the one of using A* in the following way:</p>\n\n<ol>\n<li>I ask A* to find the best sequence <code>S</code> of <code>B</code> actions to be performed</li>\n<li>I perform just the first action contained in <code>S</code>, and I modify the system state consequently (according to the outcome of the action)</li>\n<li>I ask A* to find the best sequence <code>S\'</code> of <code>(B-1)</code> actions to be performed</li>\n<li>I perform just the first action contained in <code>S\'</code> and I modify the system state consequently</li>\n</ol>\n\n<p>The problem is that I don\'t know whether this solution is optimal (I didn\'t succeed in finding an optimality proof), and an optimal solution would be required in my case.</p>\n\n<p>May you suggest another online algorithm (or, alternatively, a way of proving the optimality of the method I propose) to find an optimal solution for the problem?</p>\n\n<p><strong>EDIT</strong>: maybe something from game theory can be used instead (<a href="http://en.wikipedia.org/wiki/Extensive-form_game#Imperfect_information" rel="nofollow">extensive form games with imperfect information</a>). I am the player I that chooses the actions, while the player II is a dummy player that chooses the action "first-outcome" or "second-outcome", where "first-outcome" and "second-outcome" are my actions\' possible outcomes.</p>\n\n<p><strong>NOTE:</strong> I posted a similar question <a href="http://stackoverflow.com/questions/17149113/gain-maximization-on-trees">here</a>, although in my previous question an <em>offline</em> version was required, i.e., the modifications of the actions were not taken into account at each iteration.</p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '51', 'Title': 'Online algorithm for planning', 'LastEditorUserId': '8798', 'LastActivityDate': '2013-06-24T07:28:52.660', 'LastEditDate': '2013-06-24T07:28:52.660', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8798', 'Tags': '<decision-problem><game-theory><online-algorithms>', 'CreationDate': '2013-06-21T09:52:04.120', 'FavoriteCount': '2', 'Id': '12810'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '132', 'Title': 'Why don\'t we emphasize "length of input string" when considering time complexity of sorting algorithms?', 'LastEditDate': '2013-06-26T13:42:50.850', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8809', 'FavoriteCount': '1', 'Body': "<p>The knapsack problem is $O(c\\,n)$ where $c$ is the capacity of knapsack and $n$ is the number of items. Yet it's exponential because the size of the input is $\\log(c)$.</p>\n\n<p>However, why don't we emphasize length of input in other algorithms? To name one example, what would be the <strong>input size $n$</strong> and <strong>worst case time complexity $T$</strong> of the following input when using insertion sort:</p>\n\n<p><code>111111111,101,11,10,1,0</code></p>\n\n<p>Answer A: $n=6$, $T=O(n^2)$<br>\nAnswer B: n = space(all_digits)+space(delimiters_between_numerics)</p>\n\n<p>If B is correct, what is the time complexity $T$?</p>\n", 'Tags': '<algorithms><terminology><time-complexity><runtime-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:42:50.850', 'CommentCount': '1', 'AcceptedAnswerId': '12827', 'CreationDate': '2013-06-22T07:43:42.833', 'Id': '12825'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When analysing treaps (or, equivalently, BSTs or Quicksort), it is not too hard to show that</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[d(k)] \\in O(\\log n)$</p>\n\n<p>where $d(k)$ is the depth of the element with rank $k$ in the set of $n$ keys.\nIntuitively, this seems to imply that also</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\in O(\\log n)$</p>\n\n<p>where $h(T)$ is the height of treap $T$, since</p>\n\n<p>$\\qquad\\displaystyle h(T) = \\max_{k \\in [1..n]} d(k)$.</p>\n\n<p>Formally, however, there does not seem to be an (immediate) relationship. We even have</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\geq \\max_{k \\in [1..n]} \\mathbb{E}[d(k)]$</p>\n\n<p>by Jensen\'s inequality. Now, one can show expected logarithmic height via tail bounds, using more insight into the distribution of $d(k)$.</p>\n\n<p>It is easy to construct examples of distributions that skrew with above intuition, namely extremely asymmetric, heavy-tailed distributions. The question is, can/do such occur in the analysis of algorithms and data structures?</p>\n\n<p>Are there example for data structures $D$ (or algorithms) for which</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(D)] \\in \\omega(\\max_{e \\in D} \\mathbb{E}[d(e)])$?</p>\n\n<p>Nota bene:</p>\n\n<ul>\n<li><p>Of course, we have to interpret "depth" and "height" liberally if we consider structures that are not trees. Based on the posts Wandering Logic links to, "Expected average search time" (for $1/n \\cdot \\sum_{e \\in D} \\mathbb{E}[d(e)]$) and "expected maximum search time" (for $\\mathbb{E}[h(D)]$) seem to be used.</p></li>\n<li><p>A <a href="http://math.stackexchange.com/q/426998/3330">related question</a> on math.SE has yielded an interesting answer that may allow deriving useful bounds on $\\mathbb{E}[h(D)]$ given suitable bounds on $\\mathbb{E}[d(e)]$ and $\\mathbb{V}[d(e)]$.</p></li>\n</ul>\n', 'ViewCount': '93', 'Title': 'Can expected "depth" of an element and expected "height" differ significantly?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-23T20:06:53.820', 'LastEditDate': '2013-06-23T16:04:31.497', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12833', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><data-structures><algorithm-analysis><probability-theory><average-case>', 'CreationDate': '2013-06-22T16:23:33.990', 'Id': '12830'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '254', 'Title': 'Why do Bloom filters work?', 'LastEditDate': '2014-04-29T11:35:41.260', 'AnswerCount': '3', 'Score': '4', 'OwnerDisplayName': 'user220201', 'PostTypeId': '1', 'OwnerUserId': '8842', 'Body': "<p>Let's say I am using Bloom filters to create a function to check if a word exists in a document or not.  If I pick a hash function to fill out a bit bucket for all words in my document. Then if for a given number of words, wouldn't the whole bit bucket be all 1s? If so then checking for any word will return true? What am I missing here? </p>\n", 'Tags': '<data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:41.260', 'CommentCount': '0', 'AcceptedAnswerId': '12838', 'CreationDate': '2013-06-22T13:12:57.470', 'Id': '12834'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '3687', 'Title': 'Heap - Give an $O(n \\lg k)$ time algorithm to merge $k$ sorted lists into one sorted list', 'LastEditDate': '2013-06-24T19:13:31.113', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7678', 'FavoriteCount': '1', 'Body': "<p>Most probably, this question is asked before. It's from CLRS (2nd Ed) problem 6.5-8 -- </p>\n\n<blockquote>\n  <p>Give an $O(n \\lg k)$ time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. (Hint: Use a min-heap for $k$-way merging.)</p>\n</blockquote>\n\n<p>As there are $k$ sorted lists and total of $n$ values, let us assume each list contains $\\frac{n}{k}$ numbers, moreover each of the lists are sorted in strictly ascending order, and the results will also be stored in the ascending order. </p>\n\n<p>My pseudo-code looks like this --</p>\n\n<pre><code>    list[k]   ; k sorted lists\n    heap[k]   ; an auxiliary array to hold the min-heap\n    result[n] ; array to store the sorted list\n    for i := 1 to k                 ; O(k)\n    do\n        heap[i] := GET-MIN(list[i]) ; pick the first element \n                                    ; and keeps track of the current index - O(1)\n    done\n    BUILD-MIN-HEAP(heap) ; build the min-heap - O(k)\n    for i := 1 to n\n    do\n        array[i] := EXTRACT-MIN(heap)   ; store the min - O(logk)\n        nextMin := GET-MIN(list[1])     ; get the next element from the list 1 - O(1)\n        ; find the minimum value from the top of k lists - O(k)\n        for j := 2 to k                 \n        do\n            if GET-MIN(list[j]) &lt; nextMin\n                nextMin := GET-MIN(list[j]) \n        done\n        ; insert the next minimum into the heap - O(logk)\n        MIN-HEAP-INSERT(heap, nextMin)\n    done\n</code></pre>\n\n<p>My overall complexity becomes $O(k) + O(k) + O(n(k + 2 \\lg k)) \\approx O(nk+n \\lg k) \\approx O(nk)$. I could not find any way to avoid the $O(k)$ loop inside the $O(n)$ loop to find the next minimum element from k lists. Is there any other way around? How to get an $O(n \\lg k)$ algorithm?</p>\n", 'Tags': '<algorithms><algorithm-analysis><heaps><priority-queues>', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-06-24T20:38:10.317', 'CommentCount': '0', 'AcceptedAnswerId': '12854', 'CreationDate': '2013-06-24T06:20:18.200', 'Id': '12853'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been searching for long but unable to find a solution for this. My question is "Suppose you have n street lights(cannot be moved) and if you get any m from them then it should have atleast k working.Now in how many ways can this be done"</p>\n\n<p>This seems to be a combination problem, but the problem here is "m" must be sequential.</p>\n\n<p>Eg: 1 2 3 4 5 6 7 (Street lamps) \nLet m=3 \nThen the valid sets are,<br>\n1 2 3<br>\n2 3 4<br>\n3 4 5<br>\n4 5 6<br>\n5 6 7<br></p>\n\n<p>Whereas, 1 2 4 and so are invalid selections.<br><br></p>\n\n<p>So every set must have atleast 2 working lights. I have figured how to find the minimum lamps required to satisfy the condition but how can I find the number of ways in it can be done ?<br></p>\n\n<p>There should certainly some formula to do this but I am unable to find it.. :(<br></p>\n\n<p>Eg: Let n=7,m=4,k=3. The minimum number of lights that must be working to satisfy the condition that "atleast 3 of any 4 lights must be working" is 5. It is if 23467 are working. But there are more ways in which it can be satisfied like if 23467,13457... We have 4 such combinations in all for the taken values of n,m,k. I want to know how can we generalize this?</p>\n\n<p>Can be represented like this :\nn => 1111111<br>\n1 Indicates light working and 0 indicated not working.<br>\n0111110<br>\n0111011<br>\n1011101<br>\n1011110<br></p>\n\n<p>Hope the question is clear.</p>\n', 'ViewCount': '45', 'Title': 'Combination with a minimum number of elements in a fixed length subset', 'LastActivityDate': '2013-06-25T17:01:09.710', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8831', 'Tags': '<algorithms><logic><combinatorics>', 'CreationDate': '2013-06-24T07:59:15.317', 'Id': '12855'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to solve a problem on Sphere Online Judge (SPOJ) link to which is: <a href="http://www.spoj.com/problems/TRANSP/" rel="nofollow">http://www.spoj.com/problems/TRANSP/</a></p>\n\n<p>The matrix can be thought of as a permutation and its transposition as another permutation. I need to convert the first one into another. I have found a relation between the Cycles in the Permutation and the number of swaps required as:</p>\n\n<p>Minimum Swaps = Total Elements in Permutation - Number of Cycles</p>\n\n<p>However, I don\'t know how to calculate the matrix of size $2^a 2^b$ where $a+b \\leq 500000$.</p>\n', 'ViewCount': '101', 'Title': 'Number of permutation cycles in matrix transposition', 'LastEditorUserId': '2253', 'LastActivityDate': '2013-06-26T23:36:34.183', 'LastEditDate': '2013-06-24T23:32:17.213', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8619', 'Tags': '<algorithms><matrices><permutations>', 'CreationDate': '2013-06-24T10:25:29.587', 'Id': '12860'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am a bit confused about calculating complexities.</p>\n\n<p>Above is a C++ program converting a char array into an int, incrementing the value, parsing it back to char array.</p>\n\n<pre><code>#include &lt;iostream&gt;\n\nint main() {\n    char number[] = {'4', '3', '1'};    \n    int num = 0;\n    //char to int conversion\n    for (int i = 0; i &lt; (int)sizeof(number); i++) {\n        num += number[i] - '0';\n        num*=10;\n    }\n    num/=10;\n\n    //incrementation\n    num++;\n\n    //int to char conversion\n    for (int i = (int)sizeof(number) -1; i &gt;= 0; i--) {\n        number[i] = '0' + num % 10;\n        num/=10;\n    }\n\n    //printing the result\n    std::cout &lt;&lt; number &lt;&lt; endl;\n    return 0;\n}\n</code></pre>\n\n<p>Now let's say array size(3) is n. In that case I would say that the complexity is O(n+n) which is O(2n). However I've heard that O(2n) is actually O(n) for some reason but I could not find any actual source about it. What is the time complexity of this program?</p>\n", 'ViewCount': '72', 'Title': 'Complexity of a particular algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-25T08:26:42.210', 'LastEditDate': '2013-06-25T08:26:42.210', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-06-24T18:53:48.697', 'Id': '12873'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The main question is, how exactly is the big O analysis calculated on routines? Is there a specific formula that relates what each function in a program does to a big O calculation?</p>\n\n<p>Also, what about more complex iterations, such as colour conversions etc?</p>\n\n<p>I would like to point out that this is not a homework question, rather, it is a question from my own research/programming learning curve. I have code that I am working on, but would like to know how this analysis is carried out.</p>\n', 'ViewCount': '251', 'Title': "Analysis of algorithms, 'big O' question", 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-17T15:40:51.770', 'LastEditDate': '2013-07-17T10:26:48.837', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '12901', 'Score': '3', 'OwnerDisplayName': 'user8872', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><runtime-analysis>', 'CreationDate': '2013-06-25T19:08:19.800', 'Id': '12899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to implement and optimize the <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/TINYMT/" rel="nofollow">Tiny Mersenne Twister (TinyMT)</a> algorithm as required by an API I am developing with my team at work. The algorithm utilizes a C structure with 32-bit unsigned integers "mat1", "mat2", "tmat", and an array called "status" which is four 32-bit unsigned integers wide.</p>\n\n<p>I am relatively new to the subject of random number generation; however, I have been able to teach myself a lot about the subject over the past couple of weeks. I know what the purpose of a seed is, different methods such as linear congruent, LSFR, GFSR, etc. So I\'ve been doing my "homework" and researching the topic to the best of my ability (contrary to what the guys at Stack Overflow think). Unfortunately, the Mersenne Twister in general is extremely poorly documented and very few documents exist to explain the code and math side-by-side. The TinyMT\'s documentation is even worse, it\'s virtually non-existent! So developing accurate Doxygen comments for this part of the API is going to be tricky.</p>\n\n<p>With that said, hopefully somebody more qualified than I can help me out here. What is the significance of the aforementioned parameters? What do they do, what do they mean, what do they stand for, etc? My guess would be as follows:</p>\n\n<ul>\n<li>mat1 - Matrix 1</li>\n<li>mat2 - Matrix 2</li>\n<li>tmat - Tempering Matrix</li>\n<li>status - 127 bit wide "seed" (where the last bit goes, I\'m not sure)</li>\n</ul>\n\n<p>Given that the user provides values for "mat1", "mat2", and "tmat," are there any precautions they need to take before supplying values for them? Again, this is for an API and its documentation, so I would like to be able to give the customers a good idea of what they need to use the RNG and hopefully make the lives of other fellow programmers easier. Thanks!</p>\n', 'ViewCount': '76', 'Title': 'Significance of parameters in Tiny Mersenne Twister algorithm', 'LastActivityDate': '2013-06-26T02:46:19.197', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12904', 'Score': '1', 'OwnerDisplayName': 'audiFanatic', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<linear-algebra><randomized-algorithms><matrices>', 'CreationDate': '2013-06-25T18:06:30.620', 'Id': '12902'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m writing deduplication program that implements content-defined chunking. Just now i know 3 algorithms to do it (<a href="http://mattmahoney.net/dc/dce.html#Section_527" rel="nofollow">hashing a fixed-size window and selecting hashes &lt; maxuint/N</a>, <a href="http://encode.ru/threads/456-zpaq-updates?p=29800&amp;viewfull=1#post29800" rel="nofollow">hashing an order-1 bounded window</a>, and <a href="http://research.microsoft.com/~gurevich/Opera/183.pdf" rel="nofollow">local maxima chunking</a>). Are you know other algorithms or ideas to improve these ones? My goal is to get best compression for a given average chunk size.</p>\n\n<p>A few deduplication-related links:<br>\n<a href="http://en.wikipedia.org/wiki/Data_deduplication" rel="nofollow">http://en.wikipedia.org/wiki/Data_deduplication</a><br>\n<a href="http://encode.ru/threads/1726-Deduplication-X-Files" rel="nofollow">http://encode.ru/threads/1726-Deduplication-X-Files</a>  </p>\n', 'ViewCount': '200', 'Title': 'Deduplication: how to implement content-defined chunking?', 'LastEditorUserId': '8825', 'LastActivityDate': '2013-06-26T20:16:24.343', 'LastEditDate': '2013-06-26T20:16:24.343', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8825', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-06-26T19:40:17.520', 'FavoriteCount': '1', 'Id': '12919'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was admiring this rendition of the Mona Lisa from <a href="http://www.flickr.com/photos/quasimondo/4652406419/" rel="nofollow">quasimondo</a>\'s Flickr account.  He says:</p>\n\n<blockquote>\n  <p>Combining circle packing with data visualization. The pie charts show\n  the distribution of the dominant colors under the circle area.</p>\n  \n  <p>The circle packing technique used here is a combination of an image\n  segmentation with a distance transform and the first one who came up\n  with it is John Balestrieri: www.flickr.com/photos/tinrocket/</p>\n</blockquote>\n\n<p>I have traced it to an app called <a href="http://www.percolatorapp.com/blog/" rel="nofollow">Percolator</a></p>\n\n<p>How are such circle packings calculated? How are the pie charts calculated from the image?</p>\n\n<p><img src="http://farm5.staticflickr.com/4004/4652406419_f071593885.jpg"></p>\n', 'ViewCount': '350', 'Title': 'circle packing algorithm used by Percolator', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-27T15:34:34.887', 'LastEditDate': '2013-06-27T14:19:40.753', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12930', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><computational-geometry><discrete-mathematics><image-processing>', 'CreationDate': '2013-06-27T12:06:27.987', 'Id': '12925'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This seems like a question that should have an easy answer, but I don\'t have a definitive one: </p>\n\n<blockquote>\n  <p>If I have two $n$ bit numbers $a, p$, what is the complexity of\n  computing $a\\bmod p$ ?</p>\n</blockquote>\n\n<p>Merely dividing $a$ by $p$ <a href="http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">would take time</a> $O(M(n))$ where $M(n)$ is the complexity of multiplication. But can $\\bmod$ be performed slightly faster ? </p>\n', 'ViewCount': '134', 'Title': 'Complexity of taking mod', 'LastActivityDate': '2013-06-28T01:35:18.507', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12934', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '45', 'Tags': '<algorithms><number-theory>', 'CreationDate': '2013-06-27T19:34:48.577', 'FavoriteCount': '2', 'Id': '12931'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '236', 'Title': 'What is a good algorithm for generating random DFAs?', 'LastEditDate': '2013-06-29T08:40:23.377', 'AnswerCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8184', 'FavoriteCount': '1', 'Body': "<p>I am generating random DFAs to test a DFA reduction algorithm on them.</p>\n\n<p>The algorithm that I'm using right now is as follows: for each state $q$, for each symbol in the alphabet $c$, add $\\delta (q, c)$ to some random state.  Each state has the same probability of becoming a final state.</p>\n\n<p>Is this a good method of generating unbiased DFAs?  Also, this algorithm doesn't generate a trim DFA (a DFA with no obsolete states) so I'm wondering if there is a better way of generating random DFAs that can somehow ensure that it is trim?</p>\n", 'Tags': '<algorithms><finite-automata><random><pseudo-random-generators><random-graphs>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-18T10:13:05.943', 'CommentCount': '9', 'AcceptedAnswerId': '12949', 'CreationDate': '2013-06-28T05:14:15.410', 'Id': '12943'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A bit of background, the work that I currently and will be doing involves sorting very large amounts of data (in this case, grayscale pixels in descending order), sometimes up to 4 million.  </p>\n\n<p>Which are the most effective and efficient sorting algorithms that could handle multiple large datasets (such as descried in the 1st paragraph)?  Is there an algorithm that could simultaneously sort through 2 or more sets of pixels?</p>\n', 'ViewCount': '243', 'LastEditorDisplayName': 'user8872', 'Title': 'Which are the most effective sorting algorithms for a large dataset?', 'LastActivityDate': '2013-06-30T23:45:55.733', 'LastEditDate': '2013-06-30T23:45:55.733', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '12991', 'Score': '0', 'OwnerDisplayName': 'user8872', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2013-06-30T08:50:22.210', 'Id': '12983'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Given an <strong>undirected</strong> graph with <strong>positive weights</strong>, there are 2 kinds of edges: locked edges and unlocked edges. \nDetermination if a given edge is either locked or unlocked edge takes O(1).</p>\n\n<p>For given two vertices s , t and a positive number k = O(1), How can I find the shortest path between s and t which contains <strong>at most</strong> k locked edges?\nHow to find that path if it must contains <strong>exactly</strong> k locked edges?  note that the second path might be contains the same locked edge <strong>more than 1 time</strong>.</p>\n\n<p>So my first solution was to define a counter that is initialized to k, and decrease its value each time I found locked edge during dijkstra algo.  but it doesn\u2019t meet the requirement, so @tmyklebu suggest me to copy the graph k times, and modify each graph to <strong>directed one</strong> so that a locked edge vw in G_i turns into an edge from u in G_i to v in G_{i+1} and from v in G_i to u in G_{i+1}.  I didn\u2019t undertand this solution, so I hope you can help (or suggest another one to solve this problem) :-)</p>\n\n<p>Thanks a lot!\n<br>\nTomer.</p>\n', 'ViewCount': '89', 'Title': 'Minimum path in an undirected graph with 2 kinds of edges', 'LastActivityDate': '2013-06-30T11:06:56.353', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12988', 'Score': '-1', 'OwnerDisplayName': 'TomerGod', 'PostTypeId': '1', 'OwnerUserId': '8930', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-06-29T22:53:22.833', 'Id': '12987'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A <em>binary sequence</em> of length $n$ is just an ordered sequence $x_1,\\ldots,x_n$ so that each $x_j$ is either $0$ or $1$. In order to generate all such binary sequences, one can use the obvious binary tree structure in the following way: the root is "empty", but each left child corresponds to the addition of $0$ to the existing string and each right child to a $1$. Now, each binary sequence is simply a path of length $n+1$ starting at the root and terminating at a leaf. </p>\n\n<p>Here\'s my question:</p>\n\n<blockquote>\n  <p>Can we do better if we only want to generate all binary strings of length $2n$ which have precisely $n$ zeros and $n$ ones?</p>\n</blockquote>\n\n<p>By "can we do better", I mean we should have lower complexity than the silly algorithm which first builds the entire tree above and then tries to find those paths with an equal number of "left" and "right" edges.</p>\n', 'ViewCount': '257', 'Title': "How does one efficiently produce all binary sequences with an equal number of 0's and 1's?", 'LastActivityDate': '2013-07-01T08:25:02.287', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '12994', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '8940', 'Tags': '<algorithms><graphs><binary-trees>', 'CreationDate': '2013-06-30T14:02:49.333', 'Id': '12992'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have a rather interesting exercise in Game Theory.</p>\n\n<p><em>Assume there is a 2-players game, and player $i$ has $n_i$ pure strategies. The game is given by listing the payoffs for each player for each $n_1 \xd7 n_2$ possible plays.</em></p>\n\n<p><em>Give a polynomial time algorithm to check if there is a Nash equilibrium for the game in which each player mixes between at most two strategies.\nGive a \ufb01nite algorithm for finding a Nash equilibrium for general games with two players. Your algorithm may run in exponential time.</em></p>\n\n<p>The answer to the first question hopefully can be solved by convex optimization.</p>\n\n<p>In the second case some kind of exhauivet search can be used.</p>\n\n<p>Unfortunately I don't know how to proceed.</p>\n", 'ViewCount': '130', 'Title': 'Nash Equilibrium of 2-players game', 'LastActivityDate': '2013-10-29T07:01:57.073', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4778', 'Tags': '<algorithms><linear-programming><game-theory>', 'CreationDate': '2013-06-30T18:16:54.620', 'FavoriteCount': '1', 'Id': '12996'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying write a program to solve equations from the following form:</p>\n\n<p>$$ \\begin{align}\na \\bmod x &amp;= t_1 \\\\\nb \\bmod x &amp;= t_2 \\\\\n\\end{align} $$</p>\n\n<p>where $a$, $b$, $t_1$ and $t_2$ are known values.</p>\n\n<p>I have multiple equations of the same form and I'd like to solve them for $x$.\nAssuming I have constraints on $t_1$ and on $t_2$ for some constant $C$:</p>\n\n<p>$$ \\begin{align}\n0 \\le t_1 \\lt C \\\\\n0 \\le t_2 \\lt C \\\\\n\\end{align} $$</p>\n\n<p>e.g :\nfor $a=150$, $b=50$, $t1=2$, $t_2=1$:\n$$ \\begin{align}\n150 \\bmod x &amp;= 2 \\\\\n50 \\bmod x &amp;= 1 \\\\\n1, 2 \\lt 5 \\\\\n\\end{align} $$</p>\n\n<p>What would be the most efficient way to program such a thing?</p>\n", 'ViewCount': '74', 'Title': 'Solve modulus with constraints for multiple equations', 'LastEditorUserId': '39', 'LastActivityDate': '2013-07-02T09:23:14.223', 'LastEditDate': '2013-07-02T09:22:39.470', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '13033', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '736', 'Tags': '<algorithms><computer-algebra>', 'CreationDate': '2013-07-01T12:22:48.720', 'Id': '13011'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From a very strictly adhering sense to the hardware and circuit-level operations of any standard (non-specialized, DSPs, or supercomputing systems, etc.) microprocessor follow very similar, almost exact in some ways, operations.</p>\n\n<p>The typical role of the (main) processor in a computer, integrated with other hardware circuits or not, and excluding DMA is to have a memory subsystem fetch byte(s) for it to "process" in whatever way. To have a processor "randomly" selective something can be abstracted and seen from a data algorithm <a href="http://en.wikipedia.org/wiki/High-level_programming_language" rel="nofollow">HLL-type</a> point of view, but on the circuit-level the operations can only get so complex. I know some Assembly of x86, so I can demonstrate further on the details of what I\'m asking.</p>\n\n<p>If you fetch a byte, or series of bytes, and then use some schematic to cycle through potential jump offsets, that is the only way to do randomness? Are their other ways?</p>\n', 'ViewCount': '39', 'Title': 'Speaking of "randomness" in computing terms, to what sense can any extant digital processor make "random" results?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-03T07:33:53.753', 'LastEditDate': '2013-07-03T07:33:53.753', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8963', 'Tags': '<randomized-algorithms><randomness>', 'CreationDate': '2013-07-01T21:46:50.027', 'Id': '13021'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<pre><code>"The designer of an algorithm needs to balance between space complexity and time\ncomplexity." - Comment on the validity of the statement in the context of recursive\nalgorithms.\n</code></pre>\n\n<p>This is a question from my university\'s previous paper. But i couldn\'t find a decent answer. Actually i am confused about how can a developer minimize the time-complexity for any recursive function. I get that if there is a <strong>tail-recursion</strong> then space complexity can be minimized. But can\'t get the idea of time-complexity.  </p>\n', 'ViewCount': '2006', 'Title': 'Time complexity and space complexity in recursive algorithm', 'LastActivityDate': '2013-07-05T16:00:28.187', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13058', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8987', 'Tags': '<algorithms><time-complexity><space-complexity>', 'CreationDate': '2013-07-03T10:49:10.123', 'Id': '13055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '57', 'Title': 'maximal subset such as $A[i].X \\lt A[j].Y$ for each $j \\lt i$?', 'LastEditDate': '2013-07-16T22:59:38.273', 'AnswerCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8044', 'FavoriteCount': '1', 'Body': '<p>I need to find the maximal subset $S$ of an array $A$ of length $N$ ,where each element of $A$ contains $X$ and $Y$, such that for each $0 \\leq i \\lt N$ , $j \\lt i$ the rule is $S[i].X \\lt S[j].Y$ , What would be the best approach for finding such subset of $A$ (in terms of time, knowing that $N \\leq 100,000$) ?</p>\n\n<p><strong>EDIT:</strong>\nAn example to that is $A = \\{(2, 15), (6, 3), (6, 11), (4, 7), (8, 5)\\}$</p>\n\n<p>$S$ can be $\\{(6,11),(2,15),(4,7),(6,3)\\}$\nwhere each $X$ of $S$ is smaller than each $Y$ before it</p>\n', 'ClosedDate': '2013-08-02T15:22:22.033', 'Tags': '<algorithms>', 'LastEditorUserId': '8044', 'LastActivityDate': '2013-07-16T22:59:38.273', 'CommentCount': '10', 'CreationDate': '2013-07-03T13:01:28.293', 'Id': '13060'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m currently writing my thesis, which uses genetic algorithms at some point.\nNow I need to define some parameters for the genetic algorithm </p>\n\n<p>I know that, because of the <a href="http://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="nofollow">No Free Lunch Theorem</a> there is no optimal parameter configuration for all problems.</p>\n\n<p>However I do need some default parameters, because finding the optimal parameters is out of scope of my paper.\nAlso I don\'t want to use random values as these parameters.</p>\n\n<p>I would prefer to reuse parameters used in some well trusted sources in the field of genetic algorithms.\nThen I could rely on these cited sources and reuse these parameters.</p>\n\n<p>I need default values for these parameters:</p>\n\n<ul>\n<li>population size</li>\n<li>mutation rate</li>\n<li>number of generations</li>\n</ul>\n\n<p>I searched for papers, but am not sure which paper is well-known.\nI\'m looking for papers that are well-known and have these parameters used.\nA online available pdf is preferable.</p>\n', 'ViewCount': '273', 'Title': 'Standard Parameters for Genetic Algorithms', 'LastActivityDate': '2013-07-03T22:41:59.493', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8992', 'Tags': '<genetic-algorithms>', 'CreationDate': '2013-07-03T13:32:01.630', 'Id': '13062'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am having problems understanding a basic concept in CLRS, it is about worst case partitioning during quicksort -- </p>\n\n<blockquote>\n  <p>The worst-case behavior for quicksort occurs when the partitioning routine produces one subproblem with $n \u2212 1$ elements and one with $0$ elements.</p>\n</blockquote>\n\n<p>What does it mean by subproblem of size $0$? But what I see is there will be two subproblems, one with size $n-1$ and another with size $1$. I fail to grasp the idea of $0$ size. </p>\n\n<p>Can someone explain?</p>\n', 'ViewCount': '68', 'Title': 'what does it mean by problem of size $0$?', 'LastActivityDate': '2013-07-03T22:24:22.237', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13073', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><quicksort>', 'CreationDate': '2013-07-03T22:19:08.847', 'Id': '13072'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider we have a finite set $S$ with $n$ distinct elements. We want to find a subset $\\{a_1, a_2, \\dotsc, a_k\\}\\subseteq S$ ($k\\ll n$) such that a function $f(a_1,a_2,\\dotsc,a_k)$ is maximized. Consider $f$ to be a <a href="http://en.wikipedia.org/wiki/Symmetric_function" rel="nofollow">symmetric function</a> that takes $k$ arguments.</p>\n\n<hr>\n\n<p>More specifically, we are given $n = 120$ items, each item being associated with three positive numbers $(A_i, B_i,C_i)$, and we want to choose $k=12$ items within this set such that</p>\n\n<p>$$ \\frac{\\sum_{k=1}^{12} A_{i_k} \\times \\left\\lceil\\frac{\\sum_{k=1}^{12} B_{i_k}}{10000}\\right\\rceil}{\\sum_{k=1}^{12} C_{i_k}} $$</p>\n\n<p>is maximal.</p>\n\n<hr>\n\n<p>If we solve it by exhaustive search it requires $\\binom{120}{12} \\approx 10^{16}$ operations. Is there faster method to this problem? Approximate solution is also fine.</p>\n', 'ViewCount': '107', 'Title': 'Subset optimization problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-07T07:40:27.017', 'LastEditDate': '2013-08-07T07:40:27.017', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9021', 'Tags': '<algorithms><optimization><approximation>', 'CreationDate': '2013-07-04T15:56:53.990', 'Id': '13088'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to find out an algorithm to find out the largest least common multiple (LCM) of the partitions of an integer $n$.</p>\n\n<p>Example:  $5 = 1 + 4$, $5 = 2 + 3$, since $\\mathrm{LCM}(1,4) &lt; \\mathrm{LCM}(2,3) = 6$, the largest LCM of the partitions of $5$ is 6.</p>\n\n<p>The definition of "partition" is the standard definition of it. In order to make the problem more clearly, I will take n = 10 as an example. The largest LCM if the partitions of 10 is 30, since 10 = 2 + 3 + 5.\nMore examples(Let g(n) be the answer that I want to get):\ng(11) = 30\ng(12) = 60\ng(13) = 60</p>\n\n<p>I want to find out an algorithm that can get the largest LCM in 1 second with the n less or equal to 250.</p>\n', 'ViewCount': '408', 'Title': 'Find out the largest LCM of the partitions of n', 'LastEditorUserId': '9042', 'LastActivityDate': '2013-08-15T10:53:20.320', 'LastEditDate': '2013-07-07T07:24:40.940', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9042', 'Tags': '<algorithms><number-theory>', 'CreationDate': '2013-07-06T02:43:45.263', 'Id': '13101'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The running time of knapsack is $O(n*W)$, but we always specify that this is only pseudo-polynomial. I was wondering if somebody could tell me if I understand the notion of pseudo-polynomial time correctly. </p>\n\n<p>My current understanding is that pseudo polynomial time means polynomial in the magnitude of the input, and polynomial time is polynomial in the number of bits it takes to represent the input. Thus, looking through each element of an array is $O(n)$ in the magnitude of its length (pseudo-polynomial), but it is exponential in the number of bits in the length of the array. In the same way, binary search is $O(log_2 n)$ in the magnitude of the length of $n$, but is linear in the number of bits in $n$ making it "pseudo-logarithmic". </p>\n\n<p>If I am correct, why do we never specify that binary search is linear in the number of bits, but we always specify that knapsack is exponential in the number of bits?</p>\n', 'ViewCount': '596', 'Title': 'Do I understand pseudo polynomial time correctly?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-06T13:36:01.240', 'LastEditDate': '2013-07-06T13:36:01.240', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'OwnerDisplayName': 'Maksim', 'PostTypeId': '1', 'Tags': '<algorithms><complexity-theory><terminology><pseudo-polynomial>', 'CreationDate': '2013-07-05T19:56:54.220', 'Id': '13104'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://en.wikipedia.org/wiki/Selection_sort#Comparison_to_other_sorting_algorithms">It is written on Wikipedia</a> that "... selection sort almost always outperforms bubble sort and gnome sort." Can anybody please explain to me why is selection sort considered faster than bubble sort even though both of them have:  </p>\n\n<ol>\n<li><p><strong>Worst case time complexity</strong>: $\\mathcal O(n^2)$  </p></li>\n<li><p><strong>Number of comparisons</strong>:      $\\mathcal O(n^2)$</p></li>\n<li><p><strong>Best case time complexity</strong> :  </p>\n\n<ul>\n<li>Bubble sort: $\\mathcal O(n)$</li>\n<li>Selection sort: $\\mathcal O(n^2)$</li>\n</ul></li>\n<li><p><strong>Average case time complexity</strong> :  </p>\n\n<ul>\n<li>Bubble sort: $\\mathcal O(n^2)$</li>\n<li>Selection sort: $\\mathcal O(n^2)$ </li>\n</ul></li>\n</ol>\n', 'ViewCount': '5657', 'Title': 'Why is selection sort faster than bubble sort?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-09T19:48:10.230', 'LastEditDate': '2013-07-06T14:23:05.397', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8951', 'Tags': '<algorithms><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-07-06T09:33:35.463', 'Id': '13106'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an  algorithmic problem.</p>\n\n<p>I have a set of different polygons in the 2D space. Each polygon is represented according to its vertex representation (<code>x</code> and <code>y</code> coordinates) and may contain up to <code>N</code> different vertices. </p>\n\n<p>Assuming I have a set of <code>N</code> polygons, then for a new polygon <code>k</code>, how do I determine whether or not <code>k</code> overlaps with each one of the <code>N</code> polynomials in the set AND what is the overlapping reign percentage?</p>\n\n<p>This is a purely algorithmic question but I plan on writing the algorithm in parallel using the CUDA platform, therefore any highly parallel solution would be preferred.   </p>\n', 'ViewCount': '57', 'Title': 'How to find polygons overlap reign', 'LastActivityDate': '2013-07-06T12:56:54.447', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9046', 'Tags': '<algorithms><graphs><computational-geometry>', 'CreationDate': '2013-07-06T09:43:23.897', 'Id': '13107'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I have a problem which depends on two variables, $m$ and $n$.  I also have two algorithms for solving the problem.  How do I decide which algorithm to use?</p>\n\n<p>For example, say I have an array of unique numbers $A$, not necessarily sorted, and a second sorted array $B$.  I want to create a third array $C$ which contains how many times each number in $A$ appears in $B$.  </p>\n\n<p>Algorithm 1 runs in time $O(m \\lg n)$ and algorithm 2 in $O(m \\lg m + n)$.</p>\n\n<p>It seems to me I would need to divide into three cases:</p>\n\n<ol>\n<li>$n=\\Theta(m)$</li>\n<li>$n\\gg m$</li>\n<li>$m \\gg n$</li>\n</ol>\n\n<p>How would I generally proceed from here?</p>\n", 'ViewCount': '81', 'Title': 'Deciding between two algorithms with similar runtime in two parameters', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-07T17:51:42.857', 'LastEditDate': '2013-07-07T17:51:42.857', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '13134', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><time-complexity><asymptotics>', 'CreationDate': '2013-07-07T13:25:17.193', 'Id': '13129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I recently learned how to implement merge-sort, using a standard recursive algorithm. Can the algorithm be implemented in a way that allows for a tail-recursive implementation? Can it be implemented in an iterative style?</p>\n\n<p>In general how can a recursive algorithm converted into an iterative and tail-recursive algorithm? What are the possible pros and cons of this conversion?</p>\n', 'ViewCount': '889', 'Title': 'Iterative and/or tail-recursive implementations of merge sort?', 'LastEditorUserId': '755', 'LastActivityDate': '2013-07-08T18:34:56.770', 'LastEditDate': '2013-07-08T01:12:53.063', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '13145', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9051', 'Tags': '<algorithms><sorting><recursion>', 'CreationDate': '2013-07-07T17:20:43.893', 'Id': '13139'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>From what I have come to understand, the best way to implement it is to use the suffix  array $S$ of the string $w$ and its LCP-array (Longest Common Prefix) $L$.</p>\n\n<p>The answer can be obtained by </p>\n\n<p>$$ \\sum_{i=1}^{|w|} \\left( |S[i]| -L[i-1] \\right).$$</p>\n\n<p>What I don't get is how and why is this working?</p>\n\n<p>I would be very grateful if someone explained this.</p>\n", 'ViewCount': '914', 'Title': 'Number of distinct substrings in a string', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-19T07:11:22.433', 'LastEditDate': '2013-07-19T07:11:22.433', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13241', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9073', 'Tags': '<algorithms><data-structures><strings><substrings><suffix-array>', 'CreationDate': '2013-07-07T19:30:34.867', 'Id': '13140'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a set $S$ of integers.  I want to remove all elements of $S$ that are divisors of another element of $S$.  In other words, I want to compute $T = \\{y \\in S : \\forall d \\in S . d \\nmid y \\}$.</p>\n\n<p>How do I do this efficiently?</p>\n\n<p>I can see how to do it in $\\Theta(|S|^2)$ time, by examining all pairs of elements of $S$ and keeping only the ones that don't have any divisor in $S$.  Can it be done substantially faster?  (For simplicity, I'm willing to assume that all standard integer operations---addition, multiplication, division, etc.---can be done in $O(1)$ time.  Yes, I know this is an imperfect approximation, but if it makes your answer cleaner, I'm fine with it.)</p>\n", 'ViewCount': '64', 'Title': 'Remove divisors from a set of integers', 'LastActivityDate': '2013-07-08T06:31:54.970', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><integers><number-theory>', 'CreationDate': '2013-07-08T03:30:37.670', 'Id': '13153'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We need to convert wireframe representations of 3D objects into a representation consisting of 6 orthogonal views each containing nested (non intersecting) closed contours, each having a Z (depth/height) value. We are only concerned with 3D shapes compatible with the target representation. Can someone at least point us to any related existing algorithms?</p>\n', 'ViewCount': '81', 'Title': '3D wireframe algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-08T14:17:48.260', 'LastEditDate': '2013-07-08T14:17:48.260', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9084', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-07-08T13:23:13.637', 'Id': '13162'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question is from an exam preparation that I have to demonstrate to my teacher to show him that I understood the topic thoroughly .</p>\n\n<p>Given a set $S$ of integers with $n$ elements, an integer $z$ and an integer $k \\leq 2$, sketch an algorithm that checks whether there are $k$ elements in $S$ of which the product equals $z$.</p>\n\n<p>I need to find two algorithms: one that solves the problem in $O(n^k)$ and the other one that solves the problem in $O(n^{k-1}\\log n)$.</p>\n\n<p>How can I do this? Could you help me with some hints? Thank you!</p>\n', 'ViewCount': '126', 'Title': 'Subset product problems (one "easy" one "difficult")', 'LastEditorUserId': '9087', 'LastActivityDate': '2013-07-09T10:03:06.003', 'LastEditDate': '2013-07-09T10:03:06.003', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '9087', 'Tags': '<algorithms><sets>', 'CreationDate': '2013-07-08T19:10:44.463', 'Id': '13164'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I often interact with people who want to ask for an algorithm for a computational problem (or its complexity), but they don\'t express it in a rigorous way for us (computer scientists) to understand.</p>\n\n<p>Referring them to books like CLRS is not helpful because the examples there usually have a quite straightforward way of stating rigorously, e.g. given the adjacency list of a graph and two vertices in it compute the shortest path between those vertices.</p>\n\n<blockquote>\n  <p>Is there any good book (or some other resource) where a person with minimal knowledge of CS can learn how one should formulate and state computational problems in a rigorous way that is understandable to computer scientists? </p>\n</blockquote>\n\n<p>Preferably the book should have many examples of how to formulate computational problems rigorously from various domain and real world examples.</p>\n\n<hr>\n\n<h3>Clarification</h3>\n\n<p>To make the question more specific, let\'s assume that they know basic math/CS terminology like sets, functions, graphs, lists, etc. at the level of 1st/2nd year undergraduate CS student (which is the case with people who I have in mind). For example, they have read some introductory textbook like Aho and Ullman (although they might not have understood it completely).</p>\n\n<ul>\n<li>Al Aho and Jeff Ullman, <a href="http://infolab.stanford.edu/~ullman/focs.html" rel="nofollow">Foundations of Computer Science</a>, 1992.</li>\n</ul>\n', 'ViewCount': '244', 'Title': 'How to formulate a computational problem rigorously?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T18:58:51.437', 'LastEditDate': '2013-09-02T10:36:14.787', 'AnswerCount': '2', 'CommentCount': '13', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<algorithms><reference-request><books>', 'CreationDate': '2013-07-08T20:06:02.727', 'FavoriteCount': '2', 'Id': '13165'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given two graphs $G_{1}(E_{G1},V_{G1})$ and $G_{2}(E_{G2},V_{G2})$, with scalar weights on the vertices, I would like to find a subgraph $H_{1}$ of $G_{1}$ that best matches some subgraph $H_{2}$ of $G_{2}$.  However, in most papers I've seen the weights are usually on the edges, $H_{1}$ is pre-specified and the matching being attempted was exact. I would be grateful if somebody could point me in the right direction.</p>\n\n<p>Clarification edit: I'm looking for $H_1(E_{H1},V_{H1})\\subset G1(E_{G1},V_{G1})$ and an injective function $f:V_{H1}\\rightarrow V_{G2}$ so that $|\\{(f(u),f(v))$ $\\in$ $E_{G2}$ | $(u,v) \\in E_{H1}\\}| / |E_{H1}|$ is maximized while the sum of the node weight differences - $\\sum_{v \\in V_{H1}} |w(f(v))-w(v)|$ is minimized.     </p>\n", 'ViewCount': '65', 'Title': 'What are the popular approaches to inexact attributed-subgraph matching?', 'LastEditorUserId': '9095', 'LastActivityDate': '2013-07-11T14:15:08.310', 'LastEditDate': '2013-07-11T05:48:25.567', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9095', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-07-09T14:08:46.273', 'Id': '13175'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I've developed the following backtrack algorithm, and I'm trying to find out it time complexity.</p>\n\n<p>A set of $K$ integers defines a set of modular distances between all pairs of them. In this\nalgorithm, I considered the inverse problem of reconstructing all integer sets which realize a given distance multiset. i.e. :</p>\n\n<p><br>\nInputs: $D=\\{p_i\u2212p_j \\mod N, i\u2260j \\},K $\n<br>\nOutput : $P=\\{p_1,p_2,...,p_K\\},\\qquad p_i \\in \\{0,1,2,...,N-1\\},\\qquad p_i &gt; p_j $ for $i&gt;j$\n<br></p>\n\n<p>Simply saying, the algorithm puts $K$ blanks to be filled. Initially, puts 1 in the first blank. For the second blank it looks for the first integer that if we add to P, it doesn't produce any difference exceeding the existent differences in $D$. Then, it does so, for next blanks. While filling a blank if it checked all possible integers and found no suitable integer for that blank, it turns back to the previous blank and looks for next suitable integer for it. If all blanks are filled, it has finished his job, otherwise it means that there weren't any possible $P$'s for this $D$.</p>\n\n<p>Here's my analysis so far.\nSince the algorithm checks at most all members of $\\{2,...,N\\}$ for each blank (upper bound) there is $N-1$ search for each blank. If each visited blank was filled in visiting time, the complexity would be $O((K-1)(N-1))$ since we have $K-1$ blank (assuming first one is filled with 1). But the algorithm is more complex since for some blanks it goes backward and some blanks may be visited more that once. I'm looking for the worst case complexity i.e. the case that all blanks are visited and no solution is found.</p>\n", 'ViewCount': '1179', 'Title': 'Time complexity of a backtrack algorithm', 'LastEditorUserId': '9098', 'LastActivityDate': '2013-07-13T23:34:21.333', 'LastEditDate': '2013-07-12T07:56:41.700', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9098', 'Tags': '<algorithms><algorithm-analysis><combinatorics><search-algorithms><greedy-algorithms>', 'CreationDate': '2013-07-09T18:22:51.307', 'FavoriteCount': '2', 'Id': '13181'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>This is a cross-post of <a href="http://stackoverflow.com/questions/17538933/route-planning-in-public-transport-application">this StackOverflow question</a>, (I\'m not aware of linking questions between StackExchange sites). You can ignore the part about programming.</p>\n\n<p>I\'m making a journey planner (or a general timetable application) for all the public transport in my country (bus/train/air).</p>\n\n<p>The state of the project is at midpoint, now I\'m having a bit of a hard time getting the more difficult part of the application done.</p>\n\n<p>Currently, I have all the stops, routes and departure/arrival times.</p>\n\n<p>When there are direct connections between two points, all is fine, I can get the trips for a chosen day. The hard part is getting a complete journey when there are no direct lines.</p>\n\n<p>Say the user wants to travel from <code>city A</code> to <code>city D</code>, but because there are no direct lines between those cities, he needs to pass through <code>city B</code> and <code>city C</code>.</p>\n\n<p>How can I get the optimized routes and <em>transfers</em> for this situation?</p>\n\n<p>My ideas so far a gravitating towards using a graph, but in that case I need a <strong>Time-Dependant Directed Weighted Multigraph</strong>, and I really have no idea at the moment how to implement the <strong>Time-Dependant</strong> part.</p>\n\n<p>Getting just the route can be done by using <code>Dijkstra</code>, <code>A*</code> or <code>Floyd\u2013Warshall</code> algorithms , but because there are departures at different times, I\'m not sure how will this be implemented, to get the optimal solution. I need to take into consideration the duration of a segment (A to B, B to C), waiting time for the transfer, maybe the distance too.</p>\n\n<p>Just to clarify, I don\'t need a single result. I want to get a daily list of all departures from <code>city A</code> that can get the user to <code>city D</code>, with transfers if needed.</p>\n\n<p>Basically, what I\'m trying to get is something like this (taken from Bulgarian Railways, or for that matter, whichever railway site), a list of all departures for a chosen day going from <code>Sofia</code> to <code>Kystendil</code> making transfer in <code>Radomir</code> if needed:</p>\n\n<p><img src="http://i.stack.imgur.com/Yrron.jpg" alt="Sample Result"></p>\n\n<p>If I\'m not clear enough, please ask.</p>\n\n<p>I know that this is done so many times (almost any train website has the solution), but I don\'t know by which terms to even search.</p>\n\n<p><strong>So, my question is: can someone give me guidance how this type of problem is solved?</strong></p>\n\n<p><strong>Or at least by which terms should I search for ideas and how should it be done.</strong></p>\n\n<p>Maybe some suggestions for other sites in the StackExchange network.</p>\n', 'ViewCount': '210', 'Title': 'Route planning in public transport application', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-10T07:46:09.137', 'LastEditDate': '2013-07-10T07:46:09.137', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9100', 'Tags': '<algorithms><graphs><dynamic-programming><shortest-path>', 'CreationDate': '2013-07-09T21:26:13.667', 'FavoriteCount': '1', 'Id': '13189'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We have an array of integers $a[]$, with each $|a[i]| \\leq 10^{6}$, $size(a)\\leq10^{5}$, and $a[i]-a[i-1]\\leq100$.</p>\n\n<p>Then we define the term range as a subarray $[x,y]$ of the array $a[]$,  $x \\lt y$.</p>\n\n<p>We need to find total number of ranges that satisfy the property that for two given ranges $([x_{1},y_{1}]$ and $[x_{2},y_{2}])$ either one or more of the below conditions are true:</p>\n\n<p>1) $(y_{1} \u2013 x_{1} \\neq y_{2} \u2013 x_{2})$</p>\n\n<p>2) $a[x_{1}+k] \u2013 a[x_{1}+k-1] \\neq a[x_{2}+k] \u2013 a[x_{2}+k\u20131]$ for some $k \\in \\{1, 2, ... y_{1}-x_{1}\\}$.</p>\n\n<p>How can we visualize this problem? Is this related to some classical problem?</p>\n', 'ViewCount': '102', 'Title': 'Counting subarrays that satisfy either of two conditions', 'LastEditorUserId': '8305', 'LastActivityDate': '2013-07-10T05:19:47.167', 'LastEditDate': '2013-07-10T02:26:52.047', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '13191', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8622', 'Tags': '<algorithms><arrays>', 'CreationDate': '2013-07-10T01:42:01.590', 'Id': '13190'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an undirected simple graph $G$ and two nodes $s$ and $t$, the question asks for an algorithm to find the shortest simple cycle (no edge or vertex reuse) that contains the two. As far as I know, the problem is NP-complete if the two constraint were changed to arbitrary, but what if the number is given?</p>\n', 'ViewCount': '272', 'Title': 'Can the shortest simple cycle between two given nodes be found in polynomial time?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-22T09:45:28.230', 'LastEditDate': '2013-07-22T09:45:28.230', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9106', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2013-07-10T06:11:43.530', 'Id': '13194'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '180', 'Title': 'Computer science problems related to music?', 'LastEditDate': '2013-07-10T08:56:09.893', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2529', 'FavoriteCount': '1', 'Body': '<p>Are there any CS problems, preferably open, that are related to music or musical theory somehow? I would think of problem with musical notation but also probabilities when randomizing according to a scale or a tonality or general what is considered harmony in frequencies and physics, electromagnetism and waveforms. </p>\n\n<p>Can you give examples of the area I want to know of?</p>\n\n<p>For instance, given an algorithm that guesses a melody, how successful will the melody be in resembling an artist or likewise decision problem that could be feasible or what do you think?</p>\n', 'Tags': '<probability-theory><decision-problem><probabilistic-algorithms>', 'LastEditorUserId': '31', 'LastActivityDate': '2013-07-16T07:52:42.197', 'CommentCount': '2', 'AcceptedAnswerId': '13200', 'CreationDate': '2013-07-10T08:06:34.607', 'Id': '13199'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '576', 'Title': 'Decision problems in $\\mathsf{P}$ without fast algorithms', 'LastEditDate': '2013-07-12T05:17:37.383', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '4', 'Body': u'<p>What are some examples of difficult decision problems that can be solved in polynomial time? I\'m looking for problems for which the optimal algorithm is "slow", or problems for which the fastest known algorithm is "slow".</p>\n\n<p>Here are two examples:</p>\n\n<ul>\n<li><p><strong>Recognition of perfect graphs.</strong> In their FOCS\'03 paper [1] Cornu\xe9jols, Liu and Vuskovic gave an $O(n^{10})$ time algorithm for the problem, where $n$ is the number of vertices. I\'m not sure if this bound has been improved upon, but as I understand it, more or less a breakthrough is needed to obtain a faster algorithm.</p></li>\n<li><p><strong>Recognition of map graphs.</strong> Thorup [2] gave a rather complex algorithm with the exponent being (about?) $120$. Perhaps this has been even dramatically improved upon, but I don\'t have a good reference.</p></li>\n</ul>\n\n<p>I\'m especially interested in problems that have practical importance, and obtaining a "fast" (or even a practical) algorithm has been open for several years.</p>\n\n<hr>\n\n<p>[1] Cornu\xe9jols, G\xe9rard, Xinming Liu, and Kristina Vuskovic. "A polynomial algorithm for recognizing perfect graphs." Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on. IEEE, 2003.</p>\n\n<p>[2] Thorup, Mikkel. "Map graphs in polynomial time." Foundations of Computer Science, 1998. Proceedings. 39th Annual Symposium on. IEEE, 1998.</p>\n', 'Tags': '<algorithms><complexity-theory><reference-request>', 'LastEditorUserId': '41', 'LastActivityDate': '2014-04-04T20:09:25.113', 'CommentCount': '6', 'AcceptedAnswerId': '13227', 'CreationDate': '2013-07-10T15:22:02.280', 'Id': '13202'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm doing some research with NFAs, and I'm wondering there are algorithms which quasi-efficiently minimize them. I realize that this problem is $PSPACE$ hard, so I'm not looking for a polynomial time algorithm.</p>\n\n<p>What I mean by this is an algorithm which may run in exponential time in the worst cases, but which uses some sort of heuristic to speed up the process, albiet not enough to make it exponential.</p>\n\n<p>I'm only using this to try to get a better idea of what the minimal NFAs of certain languages look like. I'm not using it in any production code, so it doesn't need to be blazingly fast.</p>\n\n<p>For example, the Antichains algorithm for NFAs does equivalence testing which is usually fast but sometimes has exponential explosion. I'm looking for something similar, but for minimization.</p>\n\n<p>Note that I'm NOT looking for things like equivalences, etc. which run efficiently but don't produce a minimal NFA.</p>\n\n<p>Bonus points to anyone who find one with an implementation, and quadruple bonus points if it's in Prolog or Python.\nIf the tool I'm looking for doesn't exist, I'd be happy if anyone gave any old implementation of NFA minimization.</p>\n", 'ViewCount': '152', 'Title': 'Are there algorithms to exactly minimize NFAs which are sometimes efficient?', 'LastActivityDate': '2014-02-05T14:19:54.790', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2253', 'Tags': '<algorithms><formal-languages><automata><finite-automata><nondeterminism>', 'CreationDate': '2013-07-10T17:56:35.363', 'FavoriteCount': '1', 'Id': '13206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm writing a program (using genetic algorithms) that finds sort-of-optimal scheduling plan for a factory.</p>\n\n<ul>\n<li>The factory has several types of machines (say, <code>locksmith, miller, welding</code>)</li>\n<li>There are few machines of each type. (say, <code>3 locksmiths, 2 millers, 3 welders</code>)</li>\n<li>There are several types of operations (some machines do more than one operation on the job, say, <code>locksmith does soldering and assembling</code>).</li>\n<li>The jobs on the machines have different times, all known beforehand.</li>\n<li>The jobs have dependencies on jobs done before (say, <code>a product's made of 10 screws and 4 subparts, each of which needs 4 screws</code>).</li>\n</ul>\n\n<p>From what I searched, this looks sort of like a Flow Shop problem. The difference is in the dependencies and in the same machine doing different operations with different times on a job.</p>\n\n<hr>\n\n<h2>My main question is:</h2>\n\n<p><strong>Is there some kind of a classification of these problems?</strong> A summary telling the differences?</p>\n\n<p>For example, I don't understand much of how do these differ: Open Shop, Job Shop, Flow Shop, Permutation Flow Shop. And whether or not I missed something similar that could fit better to my problem.</p>\n\n<hr>\n\n<p>As a side question, what approach do you think could help me best with the unusual requirements I've posted above? I'm writing my current approach below.</p>\n\n<p>So far I've been able to work with the tree of dependencies without regard to the makespan times: just making a plan - a list of IDs, really - of what comes after what, from looking at the tree of what's been done so far and what are the leaves (nodes having done all their dependencies).</p>\n\n<p>This allows for fast creation of meaningful individuals in the Genetic Algorithm population, but there seems to be no computationally cheap way to learn the individual's makespan time (which I have as the fitness function).</p>\n\n<p>For that I have to create a calendar, or Gantt chart, if you will, to which I put the operations on the jobs in the earliest place possible, in the machine queue that's free at that moment, etc. The whole plan has to materialize and that seems the most costly computation of the whole problem.</p>\n", 'ViewCount': '234', 'Title': 'Classification of job shop scheduling problems', 'LastActivityDate': '2013-07-23T01:50:49.417', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13366', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9122', 'Tags': '<optimization><scheduling><heuristics><genetic-algorithms>', 'CreationDate': '2013-07-11T03:03:06.187', 'Id': '13219'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Below is a well-known equation for generalized recurrence relation in a divide and conquer paradigm (as described in CLRS) --</p>\n\n<p>$$T(n) = aT(n/b) + f(n), \\quad \\text{where} \\quad a \\gt 1 \\text{ , } b \\geq 1$$</p>\n\n<p>If we consider a case for merge-sort, the relation will look like this -- </p>\n\n<p>$$T(n) = 2T(n/2) + \\Theta(n) \\qquad \\qquad (i)$$</p>\n\n<p>which is quite straight-forward, i.e. we have $2$ sub-problems of size $n/2$ each, ($1/2$ of the original sub-problem), and $\\Theta(n)$ operations to merge them. </p>\n\n<p>Now if we have a relation like --</p>\n\n<p>$$T(n) = 3T(n/2) + \\Theta(n) \\qquad \\qquad (ii)$$</p>\n\n<p>then we can assume that there are $3$ "overlapping" sub-problems, as each of them with size $n/2$. Let\'s consider again -- </p>\n\n<p>$$T(n) = 2T(n/4) + \\Theta(n) \\qquad \\qquad (iii)$$</p>\n\n<p>now, what does it mean? Are there $2$ sub-problems with size $n/4$? How is it possible? If we divide the whole problem into $4$ equal sizes then we should need something like $4T(n/4)$ instead of $2T(n/4)$ to balance the recurrence tree (each node with 4 leaves), right? Is this relation realistic?</p>\n\n<p>If this is the case, then why there is no constraint like $b &lt; a$ ? Moreover, is there any algorithm that follows the recurrence as in $(ii)$ and $(iii)$?</p>\n', 'ViewCount': '92', 'Title': 'Relation between the number of sub-problems ($a$) and the size of sub-problems ($b$) in a recurrence', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-08-04T05:39:58.010', 'LastEditDate': '2013-08-04T05:39:58.010', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13238', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation>', 'CreationDate': '2013-07-12T04:38:44.167', 'Id': '13234'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array of integers. Find two disjoint contiguous sub-arrays such that the absolute difference between the sum of two sub-array is maximum.</p>\n\n<p>The sub-arrays should not overlap and it can not be empty.</p>\n\n<p>eg [2 -1 -2 1 -4 2 8]</p>\n\n<p>ans (-1 -2 1 -4) (2 8)</p>\n\n<p>diff = 16</p>\n\n<p>Better than $\\mathcal O(n^2)$ solution was expected.</p>\n', 'ViewCount': '445', 'Title': 'Find disjoint contiguous sub-arrays in better than $\\mathcal O(n^2)$', 'LastEditorUserId': '713', 'LastActivityDate': '2013-07-12T14:19:09.267', 'LastEditDate': '2013-07-12T07:55:55.977', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8596', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-07-12T06:42:51.217', 'Id': '13239'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Searching for a number in an array is said to have a runtime of O(n) because there may be cases where the number doesn't exist in the array. In such cases, you'd have to have gone through the entire array, which is O(n).</p>\n\n<p>But how about in the case where we know the number definately exists in the array? Does the runtime change then?</p>\n\n<p>Also is there a way to find out the average number of searches it would have to do before a number is found in an array based on its size?</p>\n", 'ViewCount': '290', 'Title': 'Runtime of searching for a number in an array?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-18T07:19:24.290', 'LastEditDate': '2013-07-12T14:49:49.223', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'garbage collection', 'PostTypeId': '1', 'OwnerUserId': '9155', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><arrays>', 'CreationDate': '2013-07-12T04:44:49.213', 'Id': '13244'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>During my involvement in a course on dealing with NP-hard problems I have encountered the PCP theorem, stating</p>\n\n<p>$\\qquad\\displaystyle \\mathsf{NP} = \\mathsf{PCP}(\\log n, 1)$. </p>\n\n<p>I understand the technical definition of a PCP verifier, so I know in principle what kind of algorithm has to exist for every NP problem: a randomised algorithm that checks $O(1)$ bits of the given certificate for the given input using $O(\\log n)$ random bits, so that this algorithm is essentially a one-sided error Monte-Carlo verifier.</p>\n\n<p>However, I have trouble imagining how such an algorithm can deal with an NP-complete problem. Short of reading the proof of the PCP theorem, are there concrete examples for such algorithms?</p>\n\n<p>I skimmed the relevant sections of <a href="http://www.cs.princeton.edu/theory/complexity/" rel="nofollow">Computational Complexity: A Modern Approach</a> by Arora and Barak (2009) but did not find any.</p>\n\n<p>An example using a $\\mathsf{PCP}(\\_,\\ll n)$ algorithm would be fine.</p>\n', 'ViewCount': '220', 'Title': 'Example for a non-trivial PCP verifier for an NP-complete problem', 'LastActivityDate': '2013-07-19T09:24:04.750', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><complexity-theory><np-complete><approximation><randomized-algorithms>', 'CreationDate': '2013-07-12T11:10:36.380', 'Id': '13246'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have been recently teaching myself algorithm by reading this classical textbook  <strong>Introduction to Algorithm 3rd</strong> by <strong>Thomas H. Cormen</strong> and others. I was encountered with this following problem and was not sure how to solve it, especially when it comes to the optimal substructure of it and its aciton space in each recursion. Could anyone kindly enlighten me on this </p>\n\n<blockquote>\n  <p>15-11: Inventory Planning, p.411</p>\n  \n  <p>The Rinky Dink Company makes machines that resurface ice rinks. The\n  demand for such products varies from month to month, and so the\n  company needs to develop a strategy to plan its manufacturing given\n  the fluctuating, but predictable, demand. The company wishes to design\n  a plan for the next n months. For each month i, the company knows the\n  demand di, that is, the number of machines that it will sell. Let\n  $D = \\sum^{n}_{i=1}d_i$ be the total demand over the next $n$ months. The company\n  keeps a full-time staff who provide labor to manufacture up to $m$\n  machines in a given month, it can hire additional, part-time labor, at\n  a cost that works out to $c$ dollars per machine. Furthermore, if, at\n  the end of a month, the company is holding any unsold machines, it\n  must pay inventory costs. The cost for holding j machines is given as\n  a function h(j) for $u=1,2,...,D$, where $h(j)\\geq 0$ for $1\\leq j \\leq D$ and\n  $h(j)\\leq h(j+1)$ for $j\\leq 1 \\leq D\u22121$. Give an algorithm that calculates a plan for\n  the company that minimizes its costs while fulfilling all the demand.\n  The running time should be polynomical in $n$ and $D$.</p>\n</blockquote>\n', 'ViewCount': '30', 'ClosedDate': '2013-07-13T09:03:56.323', 'Title': 'What is the optimal substructure for the following DP question', 'LastEditorUserId': '3094', 'LastActivityDate': '2013-07-12T16:38:20.857', 'LastEditDate': '2013-07-12T16:38:20.857', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9153', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2013-07-12T16:29:44.597', 'Id': '13250'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an efficient algorithm which gives the minimum cost closed walk in an undirected graph, which visits all vertices?</p>\n\n<p>Does this problem have a name? I tried to reduce this to similar problems (in particular the traveling salesman problem) to see if it was NP-hard, but was unsuccessful.</p>\n\n<p>Here\'s an example:</p>\n\n<p><img src="http://i.stack.imgur.com/3dxzt.png" alt="enter image description here"></p>\n\n<p>Then a possible closed walk is: A,B,C,D,C,B,A, with a cost of 6.</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '203', 'Title': 'Minimum cost closed walk in a graph', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-15T11:10:55.033', 'LastEditDate': '2013-07-14T09:51:41.813', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '13283', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9173', 'Tags': '<algorithms><graphs><np-hard><graph-traversal><traveling-salesman>', 'CreationDate': '2013-07-14T05:04:52.683', 'Id': '13267'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a set $S$ of $k$ numbers in $[0, N)$.\nThe task is to randomly generate numbers in the range $[0, N)$ such that none belongs to $S$.</p>\n\n<p><strong>Edit</strong> - Also given an API to generate random numbers between $[0, N)$. We have to use it  to randomly generate numbers in the range $[0, N)$ such that none belongs to $S$.</p>\n\n<p>I would also like a generic strategy for such questions. Another one I came across was to generate random numbers between [0,7] given a random number generator that generates numbers in range [0, 5].</p>\n', 'ViewCount': '225', 'Title': 'Generate random numbers from an interval with holes', 'LastEditorUserId': '9166', 'LastActivityDate': '2013-07-17T16:03:32.970', 'LastEditDate': '2013-07-15T07:05:30.703', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '13272', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9166', 'Tags': '<algorithms><probability-theory><sampling><random-number-generator>', 'CreationDate': '2013-07-14T13:12:52.417', 'Id': '13271'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There is a river which can be considered as an infinitely long straight line with width W.</p>\n\n<p>There are A piles on the river, and B types of wooden disks are available.\nThe location of the $i$-th pile is $(X_i, Y_i)$.</p>\n\n<p>The $i$-th type of wooden disks has radius $R_i$, and its price is $C_i$ per disk.</p>\n\n<p>Disks can be placed on the river such that for each wooden disk, its center must be one of the locations $(X_i, Y_i)$ of piles. We can only move on the wooden disks.</p>\n\n<p>How to find the minimum cost such that we can cross the river.I am unable to approach questions like these. What would be the best method to approach this one?</p>\n\n<p>I have come to realize that we can use Dijkstra's algorithm here. We treat this as a graph, with piles as the nodes. Start point can be $y=0$. and $y=W$ as the end point. But i am having implementation problems. There are multiple disks which can be used in going from on state to another. How to handle this?</p>\n", 'ViewCount': '156', 'LastEditorDisplayName': 'user742', 'Title': 'Finding path with minimum weight', 'LastActivityDate': '2013-09-20T09:27:54.150', 'LastEditDate': '2013-09-20T09:27:54.150', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9184', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2013-07-14T23:16:01.920', 'Id': '13273'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have about 500,000,000 64-bit integers, so these numbers could be very large.\nI want to sort them as quickly as possible. I have a couple of questions:</p>\n\n<ol>\n<li><p>What data structure do you suggest for storing this data?</p></li>\n<li><p>What algorithm do you suggest for sorting these numbers?</p></li>\n</ol>\n\n<p>My main restriction is speed.</p>\n', 'ViewCount': '224', 'Title': 'How should I store and sort a large number of 64-bit integers?', 'LastEditorUserId': '1055', 'LastActivityDate': '2013-07-17T20:07:11.840', 'LastEditDate': '2013-07-17T05:13:23.943', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '13291', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1055', 'Tags': '<algorithms><data-structures><sorting><integers><performance>', 'CreationDate': '2013-07-15T15:49:46.370', 'Id': '13290'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>does $BPP\\subseteq P^{NP}$ ? it seems reasonable but I don't know if there is a proof of this!could any one post a proof or any material that discusses the statement or something that look like this .  </p>\n", 'ViewCount': '54', 'Title': 'BPP upper bound', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-17T05:48:37.233', 'LastEditDate': '2013-07-17T05:48:37.233', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13301', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8570', 'Tags': '<complexity-theory><complexity-classes><randomized-algorithms><oracle-machines>', 'CreationDate': '2013-07-16T12:50:19.600', 'Id': '13300'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Recently I have had this question in one of my interviews.</p>\n\n<p>You have 1 Million sorted integer, you have a value of $x$, compare each pair in this array and if the addition of two pair is less or equal to $x$ then increment the paircounter.</p>\n\n<p>I have implemented this solution in C++ however I will write pseudocode here(I am sorry I am not very good at pseudo-code)</p>\n\n<pre><code>Initialise ARR_SIZE to 1000000\n\nInitialise index_i to 0\n\nInitialise pairCount to 0\n\nInitialise x to 54321\n\nwhile index_i is less than ARR_SIZE\n\n  Initialise index_j to index_i+1\n\n    while index_j is less than ARR_SIZE\n\n      if array[index_i]+array[index_j] is less or equal to x\n\n        Increment pairCount\n\n      increment index_j\n\n    endof while\n\n    increment index_x\n\nendof while\n</code></pre>\n\n<p>At first I said it is $O(n \\log n)$ but then with the hint the second loop itself average complexity is O(n/2) so overall I said it would be $O(n\\cdot n/2)$ but in Big $O$ notation it would be $O(n)$ because $n/2$ is a constant(although I was not too sure). So what is the average complexity of this overall algorithm?</p>\n\n<p>PS: I know that I could have decreased the complexity by adding an extra else, index_j = ARR_SIZE, which would be $O(N)$ complexity, but I couldn't think of it during the interview.</p>\n", 'ViewCount': '818', 'Title': 'Average Time Complexity of Two Nested Loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-20T18:26:01.993', 'LastEditDate': '2013-07-16T16:38:41.020', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '13303', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-07-16T14:06:15.760', 'Id': '13302'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can I solve $\\mathcal{O}$-notations without using Java or any other programming language?</p>\n\n<p>I only want to use pen and paper.</p>\n', 'ViewCount': '325', 'ClosedDate': '2013-07-17T09:17:35.527', 'Title': "How to do Big 'O' notations", 'LastEditorUserId': '16189', 'LastActivityDate': '2014-03-27T17:04:35.730', 'LastEditDate': '2014-03-27T17:04:35.730', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '9210', 'Tags': '<algorithms><formal-languages><terminology><regular-languages><asymptotics>', 'CreationDate': '2013-07-17T05:51:43.213', 'Id': '13305'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '139', 'Title': 'Reachability queries on a tree in $O(1)$ time with $O(n+m)$ time preprocessing', 'LastEditDate': '2013-07-18T07:18:05.990', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9214', 'FavoriteCount': '3', 'Body': '<p>I am given an undirected tree $T$ in the usual graph theoretic sense. Given a vertex $v$ and an edge $(v,u)$ incident to $v$, I need to answer queries of the form <em>return any leaf of $T$ that is reachable from $v$ with a path including $(u,v)$, and no other edges incident to $v$?</em> More informally, the restriction is that when the edge is given, we can only proceed in to that direction. </p>\n\n<p>I can simply perform a DFS and return a leaf found. I think this would take $O(d)$ time, where $d$ is the diameter of $T$. However, I\'d like to answer a query in $O(1)$ time. Moreover, I\'d only like to allow linear preprocessing time. My idea for achieving this was to use a DFS, label leaves, and then label edges when the search backtracks. This idea might work with some additional effort, but I\'m really unsure about the details.</p>\n\n<p>"Graph reachability" turned up some results, but maybe they are dealing with more complex problems. I\'m happy with any method that uses $O(n+m)$ preprocessing time and answers the queries in $O(1)$ time.</p>\n', 'Tags': '<algorithms><data-structures><trees>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-18T07:18:05.990', 'CommentCount': '4', 'AcceptedAnswerId': '13320', 'CreationDate': '2013-07-17T15:31:23.960', 'Id': '13316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I want to write a classical software simulator of a quantum circuit with $N$ qubits.  When it comes time to simulate the quantum Fourier transform I can evaluate all $2^N$ states to determine the probability amplitudes, and then perform a Fast-Fourier Transform on the probability amplitudes in time $o(N 2^N)$.  Finally in $o(2^N)$ time I can generate a scan of the partial sums of the probabilities of all the result states.  Then I can choose a random number in the range $[0,1]$ and use it to do a binary search of the partial sums.</p>\n\n<p>This results in a simulator that, each time is run outputs a single $N$ bit binary number with the probability distribution predicted by theory.</p>\n\n<p>Can I do better?  Of course I can\'t do exponentially better in general, but perhaps I could reduce the time to simulate a single experiment to $o(2^N)$?</p>\n\n<p>I can do significantly better under some circumstances.  For example, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.7607">Gilbert, Guha, Indyk, Muthukrishnan, and Strauss, "Near-Optimal Sparse Fourier Representations via Sampling", <em>ACM Symp Theory Comp</em>, STOC-44:152-161, 2002,</a> seems to indicate that if there are only $B$ frequencies (or if the $B$ frequencies make up "most" of the power of the signal) then there is a randomized algorithm that will recover all of them (and their amplitudes) in time, space and number of samples polynomial in $B$ and $N$.</p>\n\n<p>I guess I\'m hoping for something like that, but only to get one frequency, and to have some guarantee that the probability of getting a particular frequency will be proportional to the amplitude of its coefficient.</p>\n', 'ViewCount': '61', 'Title': '(Slightly) faster simulation of quantum Fourier transform', 'LastActivityDate': '2013-07-18T03:50:14.887', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7459', 'Tags': '<randomized-algorithms><quantum-computing><fourier-transform>', 'CreationDate': '2013-07-18T03:50:14.887', 'FavoriteCount': '1', 'Id': '13325'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to find parameters $\\omega, \\alpha,$ and $\\beta$ which maximize </p>\n\n<p>$$\n\\sum_{i=1}^m{\\left\\{ -\\ln(v_i) - \\frac{r_i^2}{v_i} \\right\\}}\n$$</p>\n\n<p>where</p>\n\n<p>$$\nv_i = \\omega + \\alpha r_{i-1}^2 + \\beta v_{i-1}^2\n$$</p>\n\n<p>Since numerical values for $r_i$ and $v_1$ are given my first thoughts were to use the normal equation </p>\n\n<p>$$\n\\Theta = (X^TX)^{-1}X^Ty\n$$</p>\n\n<p>This I realized quickly wouldn\'t work because there are no observed values of $v$ to fill $y$ with. I similarly considered something along the lines of gradient descent but then again I ran into the same problem. I only have $v_1$ as a starting point but no observed $v_2, v_3$ etc. I\'ve also found my way on the <a href="http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm" rel="nofollow">Levenberg-Marquardat</a> wikipedia page but then again I came up empty because I can\'t figure out how to produce (independent, dependent) bundles under these conditions.</p>\n\n<p>The problem I\'m basically trying to solve is that of estimating the three parameters of a GARCH(1,1) model ($v_i$ above).</p>\n\n<p>MathLab offers tools which can estimate these parameters for me but what I\'m trying to do is understand how the under-the-hood algorithm works in coming up with these numbers. Even excel\'s solver can figure out these parameters given an initial guess but I have not been able to find how it actually works. I understand that starting from an initial guess it uses an iterative method to arrive at a solution but I can\'t figure out how it updates the parameters at every run.</p>\n', 'ViewCount': '77', 'Title': 'Algorithm for estimating function maximizing parameters', 'LastActivityDate': '2013-07-19T06:02:57.217', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13343', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9235', 'Tags': '<algorithms>', 'CreationDate': '2013-07-18T11:18:58.603', 'Id': '13331'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>My problem.</strong> Given $n$, I want to count the number of valid multisets $S$.  A multiset $S$ is valid if</p>\n\n<ul>\n<li>The sum of the elements of $S$ is $n$, and</li>\n<li>Every number from $1$ to $n$ can be expressed uniquely as a sum of some of the elements of $S$.</li>\n</ul>\n\n<p><strong>Example.</strong>\nFor example if $n=5$ then $\\{1,1,1,1,1\\}, \\{1,2,2\\}, \\{1,1,3\\}$ are valid.</p>\n\n<p>However, $S=\\{1,1,1,2\\}$ is invalid because 2 can be formed by both $\\{1,1\\}$ and $\\{2\\}$ (i.e., 2 can be expressed as both $2=1+1$ and $2=2$), so the second condition doesn\'t hold. Similarly 3 can be formed by $\\{2,1\\}$ and $\\{1,1,1\\}$. </p>\n\n<p>$S=\\{1,2,4$} is also invalid because all numbers from $1$ to $5$ can be uniquely made, but the sum of the elements of $S$ is not $5$.</p>\n\n<hr>\n\n<p>I\'ve tried to find a good algorithm for this problem for quite some time but cannot solve it. It is from <a href="http://www.codechef.com/problems/MONEY" rel="nofollow">codechef</a>.  I\'ve seen some of the submitted solutions but I still couldn\'t get the logic for solving the problem. <strong>NOTE:</strong> The time limit for the question is 10 seconds and $n&lt;10^9$  </p>\n\n<p>For a multiset I will use the notation $S = \\{(a_1, c_1), (a_2, c_2) ... \\}$ $a_i&lt;a_j$ if $i&lt;j$, which means $a_i$ occurs $c_i$ times in multiset S.</p>\n\n<p>Till now I have drawn some conclusions</p>\n\n<ul>\n<li>First element of the required sorted multiset should be $1$ </li>\n<li>Let $S=\\{1,a_2 \\cdots a_k\\} | a_1 \\leq a_2\\cdots \\leq a_k $ be a set following the two properties then $\\forall r&lt;k \\ \\ a_{r+1} = a_r \\text{ or } (\\sum_{i=0}^ra_i) + 1$  </li>\n<li>Let $S=\\{(1,c_1),(a_2,c_2) \\cdots (a_k,c_k)\\} | a_1 \\leq a_2\\cdots \\leq a_k$, where $a_i$ is occurring $c_i$ times, follows the required properties then from the above conclusion we can say that $\\forall i \\ a_i|n+1$ and $a_i | a_j$ if $j &gt; i$ .<br>\nProof: $a_{i+1} = (a_ic_i + a_i -1 )  + 1 \\Rightarrow a_i | a_{i+1}$</li>\n<li>Now consider $S=\\{ \\underbrace{1,1 \\cdots 1}_{d-1},d,d \\cdots d,dm_1, dm_1 \\cdots dm_1,dm_2, dm_2 \\cdots dm_2, \\cdots \\}$ i.e. all the subsequent numbers after 1 will be a multiple of $d$. So let $f(n)$ be the count of such multiset possible then $f(n) = \\sum_{d|n+1, d\\neq 1} f(\\frac{n-(d-1)}{d})$ where I am summing over all possible number of $1\'s$($=d-1$). In other terms $f(n-1)=g(n)=\\sum_{d|n,d \\neq n}g(d)$  </li>\n</ul>\n\n<p>Finally my problem is reduced to this - find $g(n)$ in an efficient way so that it doesnt exceed the time limit.</p>\n', 'ViewCount': '124', 'Title': 'Number of multisets such that each number from 1 to $n$ can be uniquely expressed as a sum of some of the elements of the multiset', 'LastEditorUserId': '2589', 'LastActivityDate': '2013-07-20T17:55:05.087', 'LastEditDate': '2013-07-20T12:13:37.120', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '13362', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2589', 'Tags': '<algorithms><integers>', 'CreationDate': '2013-07-18T20:23:20.293', 'Id': '13336'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to proove $NP$-membership for a problem from the following certificate.\nI have $n$ sets of integers :\n$$(S_i)_{i \\in \\{1,\\dots,n\\}}$$\nEach set has a number $m_i$ of integers.\nI make "combination" from those sets by taking at most one element in each set.\nA "combination" has between $0$ and $n$ elements (assuming sets are not empty).\nA "combination" has a value : the sum of its elements.\nI have to compute the sum of every possible "combination" values,\nand look if it is greater than a given value.\nAn analytical formula would be nice, but I\'m not sure it exists.\nOtherwise, do you think this sum is easily computable ?</p>\n', 'ViewCount': '80', 'Title': 'Computing every possible sum of integers taken from different sets', 'LastEditorUserId': '8326', 'LastActivityDate': '2013-07-20T08:00:09.180', 'LastEditDate': '2013-07-20T08:00:09.180', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8326', 'Tags': '<algorithms><sets>', 'CreationDate': '2013-07-19T09:55:35.433', 'Id': '13346'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="http://rosalind.info/glossary/alignment/" rel="nofollow">global alignment</a> problem can be generalized by setting the cost of some boundary gaps to 0.</p>\n\n<ul>\n<li>Gaps at both end in both strings have 0 cost, then we get the <a href="http://rosalind.info/glossary/semiglobal-alignment/" rel="nofollow">semiglobal alignment</a>.</li>\n<li>Gaps are at the left end of the first string, and right end of second string have 0 cost, then we get <a href="http://rosalind.info/glossary/overlap-alignment/" rel="nofollow">overlap alignment</a>.</li>\n<li>If both boundary gaps for the second string has no cost, we get the <a href="http://rosalind.info/glossary/fitting-alignment/" rel="nofollow">fitting alignment</a>.</li>\n</ul>\n\n<p>But this can\'t capture <a href="http://rosalind.info/glossary/local-alignment/" rel="nofollow">local alignment</a>. Seems strange to let the local alignment to be the odd man out, are there a general (but not too general) framework that captures all the above alignment problems? </p>\n', 'ViewCount': '56', 'Title': 'A framework to capture common variation of sequence alignments', 'LastEditorUserId': '4287', 'LastActivityDate': '2013-07-27T23:29:16.013', 'LastEditDate': '2013-07-27T23:29:16.013', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms><strings><bioinformatics>', 'CreationDate': '2013-07-20T07:18:14.790', 'Id': '13354'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a real function $f:\\mathbb{{R}}^{d}\\mapsto\\mathbb{R}$, where\n$d&gt;1$. The question is how to compute the level set $A=\\left\\{ \\theta:f\\left(\\theta\\right)\\geq a\\right\\} $.\nI am a statistician knowing very few things about computer science.\nI would appreciate very much if anyone could suggest some reference, methods\nand/or alogirthms.</p>\n\n<p>In my problem, $f\\left(\\theta\\right)=\\sum_{i=1}^{N}\\log g\\left(\\mathbf{z}_{i};\\theta\\right)$,\nwhere $g\\left(\\mathbf{z}_{i};\\theta\\right)$ is a probability density\nfunction of $\\mathbf{z}_{i}$ parameterized by a vector of parameters\n$\\theta$. $\\mathbf{z}_1,\\ldots, \\mathbf{z}_N$ are observed samples. For example, $g\\left(\\mathbf{z}_{i};\\theta\\right)$ is the density function of\na normal distribution with variance $1$\n$$\ng\\left(\\mathbf{z}_{i};\\theta\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\left(y_{i}-\\mathbf{x}_{i}'\\theta\\right)^{2}}{2}\\right).\n$$\nHere $\\mathbf{z}_{i}=\\left(y_{i},\\mathbf{x}_{i}\\right)'$.</p>\n", 'ViewCount': '60', 'Title': 'How to compute a level set $A=\\left\\{ \\theta:f\\left(\\theta\\right)\\geq a\\right\\} $', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-20T21:29:32.707', 'LastEditDate': '2013-07-20T20:50:44.000', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9264', 'Tags': '<algorithms><probability-theory><mathematical-analysis><real-numbers>', 'CreationDate': '2013-07-20T19:24:23.690', 'Id': '13365'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've constructed an algorithm that solves the 3SUM problem in $O(n\\lg n) + \\frac{n^2}{4}$ time. I'm new to algorithms and was wondering how good is my running time? Googling didn't help.\nthanks..</p>\n", 'ViewCount': '158', 'Title': 'Computing 3SUM problem in $O(n\\lg n) + \\frac{n^2}{4}$ time', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T05:17:15.700', 'LastEditDate': '2013-07-22T05:17:15.700', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9273', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-07-21T02:47:31.517', 'Id': '13372'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The paragraph on V-opt heuristic TSP algorithms at <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem#Heuristic_and_approximation_algorithms" rel="nofollow">this site</a> mentions a very effective algorithm due to Lin-Kernigham-Johnson. That page says: </p>\n\n<blockquote>\n  <p>For many years Lin\u2013Kernighan\u2013Johnson had identified optimal solutions for all TSPs where an optimal solution was known and had identified the best known solutions for all other TSPs on which the method had been tried.</p>\n</blockquote>\n\n<p>Impressive, so what is the complexity of that algorithm? Does the algorithm often work faster than predicted  based on theoretical complexity (if yes, how much)? Is that algorithm used most often in software that solves the TSP?</p>\n', 'ViewCount': '138', 'Title': 'Complexity of the V-opt heuristic Traveling Salesman algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-23T22:21:29.497', 'LastEditDate': '2013-07-22T14:15:56.187', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13404', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6641', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><traveling-salesman>', 'CreationDate': '2013-07-22T01:01:32.150', 'Id': '13380'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '231', 'Title': 'Selection in expected linear time: Why am I getting $O(n)$ bound instead of $\\Omega(n \\lg n)$?', 'LastEditDate': '2013-07-22T04:53:45.530', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Body': '<p>The problem is from CLRS 9.3-1:</p>\n\n<blockquote>\n  <p>In the algorithm <code>SELECT</code>, the input elements are divided into groups of\n  $5$. Argue that <code>SELECT</code> does not run in linear time if groups of $3$ are used.</p>\n</blockquote>\n\n<p>If we do the "divide by $3$" technique, we will come up with this recurrence -- </p>\n\n<p>$$T(n) = \\begin{cases}\n\\Theta(1) &amp; \\text{if $n \\le K$} \\\\\nT(\\lceil n/3 \\rceil)+T(2n/3+4) + O(n) &amp; \\text{if $n \\ge K$} \n\\end{cases}$$</p>\n\n<p>I have solved by substituting $T(n) \\le cn$ and  $O(n) = an$ --</p>\n\n<p>$$\\begin{aligned}\nT(n) &amp; \\le \\lceil n/3 \\rceil + c(2n/3 + 4) + an \\\\\n     &amp; \\le cn/3 + c + 2cn/3 + 4c + an \\\\\n     &amp; = cn + 5c + an \\\\\n     &amp; = (c+a)n + 5c \\\\\n     &amp; = c_1n + c_2 \\le c_1n \\approx O(n)\n\\end{aligned}$$</p>\n\n<p>But the solution says it should be $\\Omega(n \\lg n)$. I understand that substitution like $cn \\lg n$ could give $\\Omega(n \\lg n)$ bound, but what is wrong with $O(n)$ formulation above? </p>\n', 'ClosedDate': '2013-07-22T14:13:11.047', 'Tags': '<algorithm-analysis><recurrence-relation><search-algorithms>', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T04:53:45.530', 'CommentCount': '2', 'AcceptedAnswerId': '13382', 'CreationDate': '2013-07-22T04:40:51.103', 'Id': '13381'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that I have run maxflow algorithm on a graph G and, as a result, I have a set of edges with flow on them.\nI would like to enumerate all possible sets of paths that comprise the maxflow.\nThat is, each set of paths will make the max flow.\nBut I suspect there could be multiple set of such paths.</p>\n', 'ViewCount': '102', 'ClosedDate': '2013-11-28T21:50:32.407', 'Title': 'Finding paths from the result of max flow', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-24T09:17:42.807', 'LastEditDate': '2013-08-26T08:28:54.027', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9311', 'Tags': '<algorithms><graphs><network-flow>', 'CreationDate': '2013-07-23T22:13:52.240', 'Id': '13403'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A team has decided that every morning someone should bring croissants for everybody. It shouldn\'t be the same person every time, so there should be a system to determine whose turn it is next. The purpose of this question is to determine an algorithm for deciding whose turn it will be to bring croissants tomorrow.</p>\n\n<p>Constraints, assumptions and objectives:</p>\n\n<ul>\n<li>Whose turn it is to bring croissants will be determined the previous afternoon.</li>\n<li>On any given day, some people are absent. The algorithm must pick someone who will be present on that day. Assume that all absences are known a day in advance, so the croissant buyer can be determined on the previous afternoon.</li>\n<li>Overall, most people are present on most days.</li>\n<li>In the interest of fairness, everyone should buy croissants as many times as the others. (Basically, assume that every team member has the same amount of money to spend on croissants.)</li>\n<li>It would be nice to have some element of randomness, or at least perceived randomness, in order to alleviate the boredom of a roster. This is not a hard constraint: it is more of an aesthetic judgement. However, the same person should not be picked twice in a row.</li>\n<li>The person who brings the croissants should know in advance. So if person P is to bring croissants on day D, then this fact should be determined on some previous day where person P is present. For example, if the croissant bringer is always determined the day before, then it should be one of the persons who are present the day before.</li>\n<li>The number of team members is small enough that storage and computing resources are effectively unlimited. For example the algorithm can rely on a complete history of who brought croissants when in the past. Up to a few minutes of computation on a fast PC every day would be ok.</li>\n</ul>\n\n<p>This is a model of a real world problem, so you are free to challenge or refine the assumptions if you think that they model the scenario better.</p>\n\n<p>Origin: <a href="http://stackoverflow.com/questions/17807531/find-out-whos-going-to-buy-the-croissants">Find out who\'s going to buy the croissants</a> by <a href="http://stackoverflow.com/users/851498/florian-margaine">Florian Margaine</a>. My reformulation here has slightly different requirements.</p>\n', 'ViewCount': '425', 'Title': 'Find out whose turn it is to buy the croissants', 'LastActivityDate': '2013-07-25T18:05:50.707', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<algorithms><scheduling>', 'CreationDate': '2013-07-24T20:23:27.230', 'FavoriteCount': '1', 'Id': '13420'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am confusing with the following example.</p>\n\n<ol>\n<li><p>$n^{1.001} + n\\log n = \\Theta ( n^{1.001} )$, why not $n\\log n$?</p>\n\n<p>$c_1 \\le \\frac{\\log n}{n^{0.001}} \\le c_2 $</p>\n\n<p>OR</p>\n\n<p>$c_1\\le \\frac{n^{0.001}}{\\log n} \\le c_2$</p>\n\n<p>For me both are same. Means both are giving some constant range for $c_1$ and $c_2$.   </p></li>\n<li><p>$10 n^3 + 15 n^4 + 100 n^2 2^n = \\mathcal O (100n^2 2^n) $</p></li>\n<li><p>$\\frac{6 n^3}{ \\log n + 1} = \\mathcal O(n^3)$</p></li>\n</ol>\n', 'ViewCount': '204', 'Title': 'Few Big O example', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-07-27T18:10:49.237', 'LastEditDate': '2013-07-26T16:41:59.173', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<algorithms>', 'CreationDate': '2013-07-26T12:10:23.593', 'Id': '13451'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have made a connect four game in JS and currently have a functioning minimax algorithm. The problem I\'m having is that it is very, very easy to beat, even with a large depth. This is leading me to believe that I need a better heuristic function, but I cannot come up with much better than what I already have. So, I thought I would ask if anyone has any experience, or just a good idea, for a very good heuristic to use. I would also accept improvement ideas on my current heuristic below. Thank you in advance!</p>\n\n<p>EDIT: I\'m going to go ahead and post my total minimax in here (3 functions total), because I have added disjoints and made my 4-in-a-row higher, but my AI is still terrible. I know my 2,3 and 4-in-a-rows work because I tested them, but I can\'t pin-point why I\'m still having trouble even at high depths.</p>\n\n<pre><code>    function getBestMove(currBoard,depth,who) {\n        var opp;\n        //Get opponent for next piece\n        if(who == \'a\') {\n            opp = \'p\';\n        } else {\n            opp = \'a\';\n        }\n\n        var tBoard = new Array(rows);\n        for(var i=0; i&lt;tBoard.length; i++) {\n            tBoard[i] = new Array(cols);\n        }\n\n        var moves = new Array(aiOpenCols.length);\n        //Drop each piece and use minimax function until depth == 0\n        for(var i=0; i&lt;aiOpenCols.length; i++) {\n            for(var j=0; j&lt;rows; j++) {\n                for(var k=0; k&lt;cols; k++) {\n                    tBoard[j][k] = currBoard[j][k];\n                }\n            }\n            tBoard = dropPiece(aiOpenCols[i],who,tBoard);\n            moves[i] = minimax(tBoard,(+depth - 1),opp,aiOpenCols[i]);\n        }\n\n        var bestAlpha = -100000;    //Large negative\n        //Use random column if no moves are "good"\n        var bestMove;// = Math.floor(Math.random() * aiOpenCols.length);\n        //bestMove = +aiOpenCols[bestMove];\n        //Get largest value from moves for best move\n        for(var i=0; i&lt;aiOpenCols.length; i++) {\n            if(+moves[i] &gt; bestAlpha) {\n                bestAlpha = moves[i];\n                bestMove = aiOpenCols[i];\n            }\n        }\n\n        bestMove++; //Offset by 1 due to actual drop function\n        return bestMove;\n    }\n    function minimax(currBoard,depth,who,col) {\n        //Drop current piece, called from getBestMove function\n        currBoard = dropPiece(col,who,currBoard);\n\n        //When depth == 0 return heuristic/eval of board\n        if(+depth == 0) {\n            var ev = evalMove(currBoard);\n            return ev;\n        }\n        var alpha = -100000;    //Large negative\n        var opp;\n        //Get opponent for next piece\n        if(who == \'a\') {\n            opp = \'p\';\n        } else {\n            opp = \'a\';\n        }\n\n        //Loop through all available moves\n        for(var i=0; i&lt;aiOpenCols.length; i++) {\n            var tBoard = new Array(rows);\n            for(var i=0; i&lt;tBoard.length; i++) {\n                tBoard[i] = new Array(cols);\n            }\n            for(var j=0; j&lt;rows; j++) {\n                for(var k=0; k&lt;cols; k++) {\n                    tBoard[j][k] = currBoard[j][k];\n                }\n            }\n            //Continue recursive minimax until depth == 0\n            var next = minimax(tBoard,(+depth - 1),opp,aiOpenCols[i]);\n            //Alpha = max(alpha, -minimax()) for negamax\n            alpha = Math.max(alpha, (0 - +next));\n        }\n        return alpha;\n    }\n    function evalMove(currBoard) {\n        //heuristic function\n        //AI = # of 4 streaks + # of 3 streaks + # of 2 streaks - # of 3 streaks opp - # of 2 streaks opp           \n        var fours = checkFours(currBoard,\'b\');\n        //If win return large positive\n        if(fours &gt; 0) return 100000;\n        var threes = checkThrees(currBoard,\'b\') * 1000;\n        var twos = checkTwos(currBoard,\'b\') * 10;\n        var oppThrees = checkThrees(currBoard,\'r\') * 1000;\n        var oppTwos = checkTwos(currBoard,\'r\') * 10;\n\n        var scores = threes + twos - oppThrees - oppTwos;\n\n        //If opponent wins, return large negative\n        var oppFours = checkFours(currBoard,\'r\');\n        if(+oppFours &gt; 0) {\n            return -100000;\n        } else {\n            return scores;\n        }\n    }\n</code></pre>\n', 'ViewCount': '2190', 'Title': 'Trying to improve minimax heuristic function for connect four game in JS', 'LastEditorUserId': '9365', 'LastActivityDate': '2013-07-27T07:38:13.270', 'LastEditDate': '2013-07-26T19:24:17.710', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9365', 'Tags': '<algorithms><heuristics><board-games>', 'CreationDate': '2013-07-26T14:27:09.390', 'FavoriteCount': '1', 'Id': '13453'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $A$ be an array of totally ordered elements. A subarray $A[i..j]$ is decreasing if $A[k]\\geq A[k+1]$ for all $i \\leq k &lt; j$. It is increasing if $A[k]\\leq A[k+1]$ for all $i \\leq k &lt; j$. It is monotonic if it\'s either decreasing or increasing. </p>\n\n<p>We call an array is $k$ monotonic if the array can be partitioned into $k$ monotonic subarrays. </p>\n\n<blockquote>\n  <p>Given $A$ a $k$ monotonic array of length $n$, and the value $k$, find the\n  minimum value in the array.</p>\n</blockquote>\n\n<p>How many comparisons do we need?</p>\n\n<p>If we let $m$ be the maximum number of times a value occurs in $A$, here are some known cases:</p>\n\n<p>$k=1$, then the array is sorted, the minimum is at either end, a $O(1)$ solution.</p>\n\n<p>$k=2$, then we could solve it in $O(m+ \\log \\frac{n}{m})$, by take the min of two cases. </p>\n\n<ul>\n<li>First decrease, then increase, use a $O(m+ \\log \\frac{n}{m})$ <a href="http://www.chaoxuprime.com/posts/2013-07-27-find-the-minimum-of-an-array.html" rel="nofollow">solution</a>. Basically do ternary search unless one find a large segment of equal values, which turns into linear search.  </li>\n<li>First increase, then decrease, just check the boundary values.</li>\n</ul>\n\n<p>$k=n$, then a $O(n)$ time linear search is the best possible.</p>\n\n<p>I believe a $\\Omega(m+k \\log \\frac{n}{k})$ lower bound exists, since even if we are given position for $k/2$ partitions of first decreasing then increasing subarrays, $A_1,\\ldots,A_{k/2}$, we still need to spend $O(m_i+\\log \\frac{|A_i|}{m_i})$ time to search for the minima in each one of them, where $m_i$ is the max number of times a value can appear in $A_i$.</p>\n', 'ViewCount': '288', 'Title': 'Search for minimum in an array of $k$ monotonic subarrays', 'LastEditorUserId': '220', 'LastActivityDate': '2013-07-29T18:55:09.897', 'LastEditDate': '2013-07-29T08:17:14.577', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '13496', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms><arrays>', 'CreationDate': '2013-07-27T19:52:21.887', 'Id': '13468'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m studying randomized algorithms and I sometimes come across results like</p>\n\n<blockquote>\n  <p>(1) The algorithm has an expected $O(f(n))$ cost.</p>\n</blockquote>\n\n<p>and</p>\n\n<blockquote>\n  <p>(2) With constant probability, the cost is bounded by $O(f(n))$.</p>\n</blockquote>\n\n<p>I\'m perfectly fine with statements like (2), but I\'m puzzled to what extent a statement like (1) is useful: \nFor certain probability distributions of a random variable, the expected value itself occurs with less than constant probability; for other distributions, it occurs with $1-1/n$ probability. Of course, in many cases, (1) is extended via concentration bounds to show high probability, but in cases where this isn\'t done, it doesn\'t seem that a statement on the "expected cost" lets us derive any implications on the actual performance of the algorithm, right?</p>\n', 'ViewCount': '88', 'Title': 'Interpretation of "expected cost" of an algorithm', 'LastEditorUserId': '9398', 'LastActivityDate': '2013-07-29T15:43:02.297', 'LastEditDate': '2013-07-29T03:06:35.717', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9398', 'Tags': '<probability-theory><randomized-algorithms>', 'CreationDate': '2013-07-29T01:58:55.503', 'Id': '13484'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '93', 'Title': "Finding all marked elements using Grover's algorithm", 'LastEditDate': '2013-07-30T13:30:03.557', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9423', 'FavoriteCount': '0', 'Body': "<p>Grover's algorithm uses an oracle function $f(x) \\to \\{0,1\\}$ to find the location of a single marked element from an unordered database of $2^n$ elements with high probability. As part of an assignment I am supposed to design a variant of the algorithm that finds <em>all</em> of the $t$ marked elements ($t$ is known). I want to do this by repeating the following procedure $t$-times:</p>\n\n<ol>\n<li>Use Grover's algorithm to find any marked element $x$.</li>\n<li>Delete $x$ from the data base.</li>\n</ol>\n\n<p>I think my solution is correct, but I am still not feeling comfortable with quantum computing. In particular, I am unsure of how one could implement the removal step since it would mean to manipulate the function $f$. </p>\n\n<p>Can anyone clarify?</p>\n", 'Tags': '<algorithms><quantum-computing>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-08-05T15:00:31.243', 'CommentCount': '3', 'AcceptedAnswerId': '13615', 'CreationDate': '2013-07-30T11:06:23.053', 'Id': '13513'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose, we have an array of numbers $x_j$ and their corresponding weights $w_j$ where $\\sum_j w_j \\gt 1$. Now we need to find $x_m$ such that </p>\n\n<p>$$\\sum_{j=1}^{m-1} w_j \\lt 1/2 \\quad \\text{and} \\quad \\sum_{j=m+1}^{n} w_j \\ge 1/2$$</p>\n\n<p>Moreover, $x_m &gt; x_j$, $x_m &lt; x_k$ where $j \\ne k$. i.e. a solution should be like this -- </p>\n\n<p>$$\\underbrace{x_1, x_2, \\ldots, x_{m-1}}_{\\lt \\, x_m}, x_m, \\underbrace{x_{m+1}, \\ldots, x_{n-1}, x_n}_{\\ge \\, x_m} \\\\\n\\underbrace{w_1, w_2, \\ldots, w_{m-1}}_{\\lt \\, 1/2}, w_m, \\underbrace{w_{m+1}, \\ldots, w_{n-1}, w_n}_{\\ge \\, 1/2}$$</p>\n\n<p>Moreover, it was also mentioned that I may use Dynamic Programming that could be bounded by $O(n\\lg n)$.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>$\\{x_j, w_j\\}: \\quad x_j \\text{ is the value and } w_j \\text{ is the weight.}$</p>\n\n<p>Example Input: $\\{10, 0.4\\}, \\, \\{5, 0.1\\}, \\,  \\{6, 0.9\\}, \\, \\{2, 0.3\\}, \\, \\{3, 0.1\\}$</p>\n\n<p>Example Output: $\\{2, 0.3\\}, \\, \\{3, 0.1\\}, \\,  \\underbrace{\\{5, 0.1\\}}_{x_m}, \\, \\{6, 0.9\\}, \\, \\{10, 0.4\\}$ </p>\n\n<p><strong>How I tried</strong></p>\n\n<p>Step 1: First sort the list according to $w_j$. -- $O(n \\lg n)$</p>\n\n<p>Step 2: Start from the first element from the left, add the weights $w_j$ until \n$\\sum_j w_j \\ge \\, 1/2$. The current $x_j$ is the $x_m$. -- $O(n)$</p>\n\n<p>Step 3: Stop, now we have two lists. One is on the left $L=\\{x_1, x_2, \\ldots, x_{m-1}\\}$ and the other is on the right $R = \\{x_m, x_{m+1}, \\ldots, x_n\\}$.</p>\n\n<p>Step 4: Go through the list $L$, if there is any value $x_k &gt; x_m$, move $x_k$ into $R$ at an appropriate position. Do this until all elements in $L$ is smaller than $x_m$. -- $O(n^2)$</p>\n\n<p>Step 5: if $L \\ne \\emptyset$, $x_m$ is the answer, otherwise $x_1$ is the answer.</p>\n\n<p>The overall complexity will be $O(n \\lg n) + O(n) + O(n^2) \\approx O(n^2)$. I got confused about the DP stuff at the end of the question, so I was wondering if there is really any way to do it in $O(n \\lg n)$ (or better), how do I build the optimal substructure in the case of DP?</p>\n', 'ViewCount': '174', 'Title': 'A complicated variant of Weighted Median problem', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-08-03T00:16:40.533', 'LastEditDate': '2013-08-01T23:17:14.153', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming>', 'CreationDate': '2013-07-31T05:35:21.857', 'Id': '13535'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '628', 'Title': 'What is the most power/energy efficient sorting algorithm?', 'LastEditDate': '2013-08-01T12:57:53.000', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7858', 'FavoriteCount': '1', 'Body': '<p>I am writing a Android phone application that needs to be very power efficient, and I would like to use the most power efficient sorting algorithm. I will implement it in C for extra power efficiency. What algorithm is the most power efficient algorithm for sorting arbitrary text strings?</p>\n', 'Tags': '<algorithms><sorting><efficiency><power-consumption>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-01T13:10:14.323', 'CommentCount': '6', 'AcceptedAnswerId': '13549', 'CreationDate': '2013-07-31T22:27:29.337', 'Id': '13548'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a $D$-dimensional grid with the size $(N_1, \\ldots, N_D)$, where $N_i$ are natural numbers, and a "flat block size" $M$, also a natural number. I want to find a decomposition $(m_1, \\ldots, m_D)$ such that:</p>\n\n<ol>\n<li><p>$\\prod_{i=1}^D m_i = M$,</p></li>\n<li><p>$R = \\prod_{i=1}^D f(N_i, m_i) m_i - \\prod_{i=1}^D N_i$ is as low as possible. Here $f(N,m)$ is the minimal number of blocks of length $m$ necessary to cover a 1D grid with size $N$ (or, formally, $f(N, m) = N / m$ if $N$ is a multiple of $m$, and $f(N,m) = N\\,\\mathrm{div}\\,m + 1$ otherwise).</p></li>\n<li><p>The number of $m_i$ equal to 1 is as high as possible (but this is low priority, the condition 2 is more important).</p></li>\n</ol>\n\n<p>How should I approach this? Is there some standard algorithm this can be reduced to?</p>\n\n<p>In my case $M$ is not very big (of the order of 1000). Also, an absolute minimum in all cases is not strictly required; if there is an approximate algorithm, it will do to.</p>\n\n<p>(In case anyone is interested in the application, I want to use it to find work group dimensions for an OpenCL kernel with a known global size and total number of work items).</p>\n', 'ViewCount': '23', 'Title': 'Optimal coverage of a $D$-dimensional grid with small blocks', 'LastEditorUserId': '9455', 'LastActivityDate': '2013-08-01T06:13:22.680', 'LastEditDate': '2013-08-01T05:44:12.093', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13556', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9455', 'Tags': '<algorithms><optimization><dynamic-programming>', 'CreationDate': '2013-08-01T02:03:46.657', 'Id': '13555'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm seeking to write an algorithm which, given a value of N, will fill a matrix consisting of (N+1)<em>(N+2)</em>(N+3)/6 rows and 4 columns with the integers from 0, ... , N, subject to the conditions that:</p>\n\n<ul>\n<li>The sum of values in any row is N</li>\n<li>No rows are repeated (this should ensure that every such possibility is listed)</li>\n</ul>\n\n<p>For example, with N=2, we have 10 (=3*4*5/6) rows:</p>\n\n<pre><code>2 0  0 0\n1 1  0 0\n0 2  0 0\n1 0  1 0\n1 0  0 1\n0 1  1 0\n0 1  0 1\n0 0  2 0\n0 0  1 1 \n0 0  0 2\n</code></pre>\n\n<p>I've been trying for a while to program this (in R, for what it's worth), but I'm struggling to get anywhere. Any advice?</p>\n", 'ViewCount': '172', 'Title': 'Filling Rows of a Matrix Subject to Conditions', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-12T08:25:28.050', 'LastEditDate': '2013-08-11T13:03:29.910', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '13577', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9322', 'Tags': '<algorithms><integers><linear-programming>', 'CreationDate': '2013-08-01T10:31:49.617', 'Id': '13562'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for geometry algorithm.</p>\n\n<p>I have an axis-aligned box $B$ and a triangle $T$ in 3D space. I want to compute an axis-aligned bounding box of their intersection.</p>\n\n<p>Both $B$ and $T$ are convex polytopes and $B\\cap T$ is also convex polytope. I don\'t need general algorithm, I need something fast and simple.</p>\n\n<p>Have you heard about such an algorithm? Or is it trivial and I can\'t see it?</p>\n\n<p>I got an idea to "crop" the triangle with each of 6 box\'s faces. Cropping triangle with each plane can add 1 new vertex, so the resulting object may have up to 9 vertices. Then I compute the bounding box of that object. Am I right? Can it be simplified to get only bounding box output?</p>\n', 'ViewCount': '136', 'Title': 'Box and triangle intersection', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-03T18:56:25.267', 'LastEditDate': '2013-08-03T18:56:25.267', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'OwnerDisplayName': 'Ivan Kuckir', 'PostTypeId': '1', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-07-16T14:12:03.177', 'Id': '13573'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>A typical way of proving the greedy choice property of the fractional knapsack problem is as follows:</p>\n\n<hr>\n\n<p>From Slide 5 of <a href="http://www.cs.kzoo.edu/cs215/lectures/f4-knapsack.pdf" rel="nofollow">this link</a>:</p>\n\n<p>Given: A set of items $I = \\{I_1,I_2..I_n\\}$ with weights $\\{w_1,w_2 ... w_n\\}$ and values $\\{v_1,v_2 ...v_n\\}$. Let $P$ be the problem of selecting items from $I$, with the weight limit $K$ such that the resulting value is maximum.</p>\n\n<p>Let $O = \\{o_1,o_2 ... o_j\\} \\subseteq I$ be the <strong>optimum</strong> solution of problem $P$. </p>\n\n<p>Let\xa0$G = \\{g_1,g_2 ... g_k\\} \\subseteq I$ be\xa0the\xa0greedy\xa0solution,\xa0where\xa0the\xa0 items\xa0are\xa0ordered\xa0according\xa0to the\xa0greedy\xa0choices.\xa0</p>\n\n<p>We\xa0need to\xa0show\xa0that\xa0there\xa0exists\xa0some\xa0optimal\xa0solution\xa0$O\'$\xa0that\xa0includes\xa0the\xa0choice $g_1$\n.</p>\n\n<p>CASE\xa01:\xa0$g_1$\xa0is\xa0non-\xadfractional.</p>\n\n<ol>\n<li>If\xa0$g_1$\xa0is\xa0included\xa0in $O$,\xa0then\xa0we\xa0are\xa0done.</li>\n<li>If\xa0$g_1$\xa0is\xa0not\xa0included\xa0in\xa0$O$,\xa0then\xa0we\xa0arbitrarily\xa0remove\xa0$w_{g_1}$\xa0worth\xa0of\xa0stuff from\xa0$O$\xa0and\xa0replace\xa0it\xa0with\xa0$g_1$\xa0to\xa0produce\xa0$O\'$.</li>\n<li>$O\'$ is\xa0a\xa0solution, and\xa0it\xa0is at\xa0least\xa0as\xa0good as\xa0$O$.</li>\n</ol>\n\n<hr>\n\n<p>In the above proof, step $3$ for CASE 1 merely shows that weight criteria is satisfied. How does it show that $O\'$ is also an optimal solution(i.e. in terms of value achieved), more so when we are "arbitrarily removing $w_{g_1}$ worth of stuff" without paying attention to corresponding change in value ?</p>\n\n<p><strong>UPDATE</strong>: I found the answer in terms of change in value <a href="http://oucsace.cs.ohiou.edu/~razvan/courses/cs4040/lecture15.pdf" rel="nofollow">here</a>. I am not sure if this should go into the answer part. Mods, please suggest.</p>\n', 'ViewCount': '1236', 'Title': 'Proving greedy choice property of fractional knapsack', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-05T20:37:19.433', 'LastEditDate': '2013-08-05T20:37:19.433', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'curryage', 'PostTypeId': '1', 'OwnerUserId': '8660', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><greedy-algorithms>', 'CreationDate': '2013-07-03T00:09:45.020', 'Id': '13575'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an sorted array of integers, I want to find the number of pairs that sum to $0$. For example, given $\\{-3,-2,0,2,3,4\\}$, the number of pairs sum to zero is $2$.</p>\n\n<p>Let $N$ be the number of elements in the input array. If I use binary search to find the additive inverse for an element in the array, the order is $O(\\log N)$. If I traverse all the elements in the set, then the order is $O(N\\log N)$. </p>\n\n<blockquote>\n  <p>How to find an algorithm which is of order $O(N)$?</p>\n</blockquote>\n', 'ViewCount': '403', 'Title': 'How to develop an $O(N)$ algorithm solve the 2-sum problem?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-08-07T05:31:51.260', 'LastEditDate': '2013-08-06T11:19:05.043', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13586', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8036', 'Tags': '<algorithms>', 'CreationDate': '2013-08-03T11:06:29.543', 'Id': '13585'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I found the paper "On the State Minimization of Nondeterministic Finite Automata" which, I assume, contains the Kameda-Weiner algorithm that I\'ve been searching for. It\'s behind a paywall though. I\'m just a hobbyist. Can someone explain it, or point me to another source?</p>\n', 'ViewCount': '135', 'Title': 'Free description of the Kameda-Weiner algorithm?', 'LastActivityDate': '2013-09-26T02:01:37.907', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8233', 'Tags': '<algorithms><automata><finite-automata>', 'CreationDate': '2013-08-03T18:57:58.530', 'FavoriteCount': '1', 'Id': '13591'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've had a bit of experience programming Neural networks but I am fairly new with genetic algorithms (I'm only 17). I have a major issue that I can't understand. If a child get's one chromatid from one parent and another from another parent, then the genes are crossed over, how do you determine which alleles are active in the child? How do other people go about mating with genetic algorithms. No need to go deep into it because I also want to work somethings out for myself but a general idea would be good enough. Thanks in advance.</p>\n", 'ViewCount': '106', 'Title': "Something I don't understand about Genetic Algorithms", 'LastActivityDate': '2013-08-04T12:46:31.363', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13595', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9506', 'Tags': '<artificial-intelligence><neural-networks><genetic-algorithms>', 'CreationDate': '2013-08-04T12:05:08.643', 'Id': '13594'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>According to CLRS, the Prim's algorithms is implemented as below -- </p>\n\n<blockquote>\n  <p>$\\mathtt{\\text{MST-PRIM}}(G,w,r)$  </p>\n  \n  <ul>\n  <li>for each $u \\in V[G]$ do<br>\n  <ul>\n  <li>$\\mathtt{\\text{key}}[u] \\leftarrow \\infty$  </li>\n  <li>$\\pi[u] \\leftarrow \\mathtt{\\text{NIL}}$ </li>\n  </ul></li>\n  <li>$\\mathtt{\\text{key}}[r] \\leftarrow 0$  </li>\n  <li>$Q \\leftarrow V[G]$  </li>\n  <li>while $Q \\ne \\emptyset$ do // ... $O(V)$\n  <ul>\n  <li>$u$ $\\leftarrow$ $\\mathtt{\\text{EXTRACT-MIN}}(u)$ // ... $O(\\lg V)$<br>\n  <ul>\n  <li>for each $v \\in \\mathtt{\\text{adj}}[u]$ do // ... $O(E)$<br>\n  <ul>\n  <li>if $v \\in Q$ and $w(u,v) \\gt \\mathtt{\\text{key}}[v]$\n  <ul>\n  <li>then $\\pi[v] \\leftarrow u$\n  <ul>\n  <li>$\\mathtt{\\text{key}} \\leftarrow w(u,v)$ // $\\mathtt{\\text{DECREASE-KEY}}$ ... $O(\\lg V)$</li>\n  </ul></li>\n  </ul></li>\n  </ul></li>\n  </ul></li>\n  </ul></li>\n  </ul>\n</blockquote>\n\n<p>The book says the total complexity is $O(V \\lg V + E \\lg V) \\approx O(E \\lg V)$. However, what I understood is that the inner <code>for</code> loop with the <code>DECREASE-KEY</code> operation will cost $O(E \\lg V)$, and the outer <code>while</code> loop encloses both the <code>EXTRACT-MIN</code> and the inner <code>for</code> loop, so the total complexity should be $O(V (\\lg V + E \\lg V)) = O(V \\lg V + EV \\lg V) \\approx O(EV \\lg V)$. </p>\n\n<p>Why the complexity analysis is not performed as such? and What is wrong with my formulation?</p>\n", 'ViewCount': '534', 'Title': "MST: Prim's algorithm complexity, why not $O(EV \\lg V)$?", 'LastActivityDate': '2013-08-05T06:45:45.717', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13609', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><algorithm-analysis><spanning-trees>', 'CreationDate': '2013-08-05T05:59:33.073', 'FavoriteCount': '1', 'Id': '13608'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '218', 'Title': 'Finding maximal factorization of regular languages', 'LastEditDate': '2013-08-10T18:28:15.740', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1382', 'FavoriteCount': '3', 'Body': u"<p>Let language $\\mathcal{L} \\subseteq \\Sigma^*$ be regular.</p>\n\n<p>A factorization of $\\mathcal{L}$ is a maximal pair $(X,Y)$ of sets of words with</p>\n\n<ul>\n<li>$X \\cdot Y \\subseteq \\mathcal{L}$</li>\n<li>$X  \\neq \\emptyset \\neq Y$,</li>\n</ul>\n\n<p>where $X \\cdot Y = \\{xy$ | $x \\in X, y \\in Y\\}$.</p>\n\n<p>$(X,Y)$ is maximal if for each pair $(X',Y') \\neq (X,Y)$ with $X'\\cdot Y' \\subseteq \\mathcal{L} $ either $X \\not \\subseteq X'$ or $Y \\not \\subseteq Y'$.</p>\n\n<p>Is there a simple procedure to find out which pairs are maximal?</p>\n\n<p>Example:</p>\n\n<p>Let $\\mathcal{L} = \\Sigma^\u2217ab \\Sigma^\u2217$. The set $F = \\{u, v, w\\}$ is computed: </p>\n\n<ul>\n<li><p>$u =(\\Sigma^\u2217, \\Sigma^\u2217ab\\Sigma^\u2217)$</p></li>\n<li><p>$v = (\\Sigma^\u2217a\\Sigma^\u2217, \\Sigma^\u2217b\\Sigma^\u2217)$</p></li>\n<li><p>$w = (\\Sigma^\u2217ab\\Sigma^\u2217, \\Sigma^\u2217) $</p></li>\n</ul>\n\n<p>where $\\Sigma = \\{a,b\\}$.</p>\n\n<p>Another example:</p>\n\n<p>$\\Sigma = \\{a, b\\}$ and $\\mathcal{L} = \\Sigma^*a\\Sigma$\nFactorization set $F = \\{q, r, s, t\\}$ with</p>\n\n<ul>\n<li><p>$q = (\\Sigma^*, \\mathcal{L})$</p></li>\n<li><p>$r = (\\Sigma^*a, \\Sigma + \\mathcal{L})$</p></li>\n<li><p>$s = (\\Sigma^*aa, \\epsilon + \\Sigma + \\mathcal{L})$</p></li>\n<li><p>$t = (\\mathcal{L}, \\epsilon + \\mathcal{L}) $</p></li>\n</ul>\n", 'Tags': '<algorithms><regular-languages><optimization>', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-12T07:32:56.617', 'CommentCount': '9', 'AcceptedAnswerId': '13713', 'CreationDate': '2013-08-05T16:12:02.770', 'Id': '13617'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to understand algorithm complexity, and a lot of algorithms are classified as polynomial. I couldn't find an exact definition anywhere. I assume it is the complexity that is not exponential. </p>\n\n<p>Do linear/constant/quadratic complexities count as polynomial? An answer in simple English will be appreciated :)</p>\n", 'ViewCount': '226', 'Title': 'What exactly is polynomial time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-07T16:32:18.580', 'LastEditDate': '2013-08-07T07:59:04.743', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '13626', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9538', 'Tags': '<algorithms><terminology><time-complexity><runtime-analysis><polynomial-time>', 'CreationDate': '2013-08-06T01:28:41.500', 'Id': '13625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I am trying to figure out difference between fully- and quasi-reduced BDDs. I have read a lot of material but still it is not very clear. As I am trying to figure out the quasi reduced version for union between two BDDs.\nThe algorithm for union between two fully-reduced BDDs is </p>\n\n<pre><code>bdd Union(bdd p, bdd q) \n   //fully-reduced version\n   local bdd r;\n1  if p=0 or q=1 then return q;\n2  if q=0 or p=1 then return p;\n3  if p=q then return p;\n4  if Cachecontainsentry\u27e8UnionCODE,{p,q}:r\u27e9 then return r;\n5  if p.lvl = q.lvl then\n6    r \u2190 UniqueTableInsert(p.lvl, Union(p[0], q[0]), Union(p[1], q[1]));\n7  else if p.lvl &gt; q.lvl then\n8    r \u2190 UniqueTableInsert(p.lvl, Union(p[0], q), Union(p[1], q));\n9  else since p.lvl &lt; q.lvl then\n10   r \u2190 UniqueTableInsert(q.lvl, Union(p, q[0]), Union(p, q[1]));\n11 enter\u27e8UnionCODE,{p,q}:r\u27e9inCache;\n12 return r;\n</code></pre>\n\n<p>I have read the paper <em>Binary decision diagrams in theory and practice</em> by Rolf Drechsler, Detlef Sieling for basics of BDD, and <em>Data Representation and Efficient Solution: A Decision Diagram Approach</em> by Gianfranco Ciardo for quasi-reduced and fully reduced definitions. Then I read more papers with more or less same description of quasi- and fully-reduced BDDs. In the former paper I mentioned the authors talk about reduced BDDs, I am not clear whether these BDDs are fully reduced. Quasi-reduced BDDs has no variable skipping so how come they are reduced when they have redundant nodes. I am pretty confused between BDD, quasi-reduced BDD and fully-reduced BDD. Yes, I am trying to find the difference between union algorithm for quasi-reduced and fully-reduced, for this I need to look at the quasi-reduced version of union algorithm.</p>\n\n<p>I figured out an algorithm for union of two quasi reduced BDDs <code>p</code> and <code>q</code> resulting in <code>r</code>.</p>\n\n<pre><code>bdd Union(bdd p, bdd q) \n  local bdd r;\n1 if p=0 or q=1 then return q;\n2 if q=0 or p=1 then return p;\n3 if p=q then return p;\n4 if Cachecontainsentry\u27e8UnionCODE,{p,q}:r\u27e9 then return r;\n  //p.lvl = q.lvl in case of quasi reduced BDDs\n5 r \u2190 UniqueTableInsert(p.lvl, Union(p[0], q[0]), Union(p[1], q[1]));\n6 enter\u27e8UnionCODE,{p,q}:r\u27e9 in Cache;\n7 return r;\n</code></pre>\n\n<p>Since there is no variable skipping, <code>p.lvl</code> is always equal to <code>q.lvl</code>. I have a question about this algorithm.</p>\n\n<p>If I want to implement Xor or Xnor for quasi-reduced BDDs, can it be done the same way as union or should I implement the expression <code>pq' + p'q</code> where <code>q' = !q</code>.</p>\n", 'ViewCount': '86', 'Title': 'Difference between fully-reduced BDD and quasi-reduced BDD', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-13T08:56:46.307', 'LastEditDate': '2013-08-13T08:56:46.307', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9556', 'Tags': '<algorithms><terminology><logic><binary-trees><bdd>', 'CreationDate': '2013-08-06T23:35:14.983', 'Id': '13637'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is a question from the book <code>Data structures using C and C++ by Tenenbaum</code>. Not a homework problem but self-study.</p>\n\n<blockquote>\n  <p>Recursive definition of a+b, where a and b are non-negative integers, in terms of successor function <code>succ</code> defined as</p>\n  \n  <p>succ(int x)\n      {\n          return(x++);\n      }</p>\n</blockquote>\n\n<p>I have been thinking how is it possible? Can I change the function? I am not sure from the problem definition in the book. So how can this be done?</p>\n', 'ViewCount': '282', 'Title': 'Recursive definition of sum of two numbers in terms of the successor function', 'LastEditorUserId': '683', 'LastActivityDate': '2013-09-02T11:13:25.650', 'LastEditDate': '2013-08-07T15:23:59.207', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13660', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9051', 'Tags': '<algorithms><recursion><number-theory>', 'CreationDate': '2013-08-07T14:26:48.393', 'Id': '13659'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '169', 'Title': "Is the memory-runtime tradeoff an equivalent of Heisenberg's uncertainty principle?", 'LastEditDate': '2013-08-08T10:40:15.420', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9571', 'FavoriteCount': '3', 'Body': '<p>When I work on an algorithm to solve a computing problem, I often experience that speed can be increased by using more memory, and memory usage can be decreased at the price of increased running time, but I can never force the product of running time and consumed memory below a clearly palpable limit. This is formally similar to Heisenberg\'s uncertainty principle: the product of the uncertainty in position and the uncertainty in momentum of a particle cannot be less than a given threshold.</p>\n\n<p>Is there a theorem of computer science, which asserts the same thing? I guess it should be possible to derive something similar from the theory of Turing Machines.</p>\n\n<p>(I asked this question originally on <a href="http://stackoverflow.com/questions/18108578/the-uncertainty-principle-of-computer-science">StackOverflow</a>.)</p>\n', 'Tags': '<algorithms><time-complexity><space-complexity><performance>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-08T10:40:15.420', 'CommentCount': '3', 'AcceptedAnswerId': '13664', 'CreationDate': '2013-08-07T16:45:48.973', 'Id': '13661'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I get used to think exhaustive search and combinatorial search are same, but I got confused by reading a paper!</p>\n\n<p>What is the difference between  exhaustive search &amp; combinatorial search ?</p>\n', 'ViewCount': '1059', 'Title': 'What is the difference between exhaustive search & combinatorial search?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-08T11:03:06.587', 'LastEditDate': '2013-08-08T10:49:28.617', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9098', 'Tags': '<algorithms><terminology><search-algorithms>', 'CreationDate': '2013-08-07T18:25:43.057', 'Id': '13663'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m trying to find an algorithm for a motion planning problem. I have $N$ points, $P_1$ to $P_N$, in $k$-dimensional cartesian space, defining $N-1$ segments. The problem is about constructing the fastest motion plan, that is, a function $p(t)$, <em>approximating</em> the path along the segments, bound by some constraints:</p>\n\n<ul>\n<li><p>For each segment defined by the points $P_i$ and $P_{i+1}$, let $M_i$ be some point on this segment ($M_i$ is part of the solution, not the input). For now, let\'s assume it\'s strictly within the segment. Now, the constraint is that our sought plan $p(t)$ starts with an initial segment from $P_1$ to $M_1$, is followed by $N-2$ second-order B\xe9zier curves, each defined by the control points $(M_i, P_{i+1}, M_{i+1})$, and finally followed by a segment from $M_{n-1}$ to $P_n$.</p></li>\n<li><p>Velocity is preserved on boundaries between the curves and the start/end segments. Note that the <em>direction</em> of velocity is implicitly preserved by the above constraint. (because the vector from $P_i$ to $M_i$ is parallel to the one from $M_i$ to $P_{i+1}$)</p></li>\n<li><p>Along each of the B\xe9zier curves, as well as on the initial and final segment, the acceleration of the plan $p(t)$ is constant. This means that, if the plan specifies that we arrive to $M_i$ at time $T_i$, and it takes $t_i$ time to traverse the curve that begins there, this part of the motion plan is equal to $p(t)=B_i((t-T_i)/t_i)$, where $B_i$ is standard B\xe9zier formula for this curve. Informally, we cannot manually accelerate along the B\xe9zier curves, all we can do is scale the speed/acceleration across an entire curve.</p></li>\n<li><p>The absolute value of acceleration along each dimension $d$ must be no more than $A_d$. The maximum velocity limit may also be defined for each dimension, and I\'m not sure whether or not that would make the problem significantly harder.</p></li>\n<li><p>The initial and final velocity is zero (or possibly a constant).</p></li>\n</ul>\n\n<p>Here\'s a picture of such a path composed of B\xe9zier curves.<img src="http://i.stack.imgur.com/uSjZr.jpg" alt="enter image description here"></p>\n\n<p>Notice that the shape of the path is defined by $N-1$ real numbers $m_i \\in(0, 1)$, which define the position of the points $M_i$ as a convex combination of $P_i$ and $P_{i+1}$. The difficulty of this problem is in determining these numbers. The constraints imply that, given $m_i$, the velocities on every point of the plan are determined up to a common factor. Therefore, knowing $m_i$ of the optimal solution, it is not hard to finish the plan by finding the smallest total plan time which does not violate the acceleration constraints on any of the individual components of the plan.</p>\n\n<p>Ideally, the algorithm would work incrementally - given an optimal solution for the first $n$ points, and the next point, it would fix this solution into an optimal plan for the first $n+1$ points.</p>\n\n<p><strong>Some initial work</strong></p>\n\n<p>Let\'s define $D_i=P_{i+1}-P_i$. Also define $V_i$ to be the velocity of the plan at point $M_i$. But since $V_i$ is parallel to $D_i$, we write $V_i=v_i D_i$, for some $v_i \\in \\mathbb{R}$. Let\'s call $v_i$ the <em>relative speeds</em>.</p>\n\n<p>We will now use some knowledge about B\xe9zier curves to express a relationship between the relative velocities on the ends of a single curve, resulting in an equation involving $v_i$ and $v_{i+1}$. Let\'s forget about our points for a moment and assume we have a quadratic B\xe9zier curve with the control points $(A, B, C)$. If we start with the standard formula for B\xe9zier curves, and factor by $t$, we arrive at:</p>\n\n<p>$$ B(t) = A + 2(B-A)t + (A-2B+C)t^2 $$</p>\n\n<p>The parameter in this curve is $t \\in [0, 1]$, and it is easy to see that $B(0)=A$ and $B(1)=C$. We can also take the derivative of this function and compute it at the ends of the curve:</p>\n\n<p>$$ B\'(0) = 2(B-A) $$\n$$ B\'(1) = 2(C-B) $$</p>\n\n<p>Now return to the problem of finding a relationship between $v_i$ and $v_{i+1}$. The above formula tells us how to compute the velocities at the start and end of a quadratic B\xe9zier curve, however we have to take into account that the B\xe9zier curves in our plan are scaled proportionally in time. So, assume the plan takes $t_i$ time to traverse the i-th B\xe9zier curve. Then, the initial and final velocities along this curve are given by:</p>\n\n<p>$$ V_i = \\frac{2(1-m_i)D_i}{t_i} $$\n$$ V_{i+1} = \\frac{2m_{i+1}D_{i+1}}{t_i} $$</p>\n\n<p>since $(1-m_i)D_i$ corresponds to $B-A$ above, and $m_{i+1}D_{i+1}$ corresponds to $C-B$. Then we substitute $V_i=v_i D_i$ and $V_{i+1}=v_{i+1} D_{i+1}$, which turns the vector equations into real equations. Eradicating $t_i$ gives the following:</p>\n\n<p>$$ v_{i+1} = \\frac{m_{i+1}}{1-m_i}v_i $$</p>\n\n<p>This equation captures many of the constraints of the problem, in terms of real numbers $m_i$ (the relative positions of the splice points) and $v_i$ (the relative velocities at the splice points).\nIn fact, given all $m_i$, this effectively defines the ratios between all $v_i$. Therefore, if after choosing $m_i$ we also choose a value for $v_1$, the plan is completely defined.</p>\n\n<p>Though I don\'t know yet how this helps with finding the optimal $m_i$, taking into account the other constraints (maximum acceleration, starting and ending velocity). While the last equation looks nice, its non-linearity in terms of $m_i$ is not encouraging.</p>\n', 'ViewCount': '156', 'Title': u'Motion planning using second order B\xe9zier curves', 'LastEditorUserId': '4213', 'LastActivityDate': '2013-08-08T01:04:59.303', 'LastEditDate': '2013-08-08T01:04:59.303', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4213', 'Tags': '<optimization><computational-geometry><online-algorithms>', 'CreationDate': '2013-08-07T19:41:56.430', 'Id': '13667'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It seems that on this site, people will often correct others for confusing "algorithms" and "problems." What are the difference between these? How do I know when I should be considering algorithms and considering problems? And how do these relate to the concept of a language in formal language theory?</p>\n', 'ViewCount': '453', 'Title': 'What is the difference between an algorithm, a language and a problem?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-15T20:11:23.283', 'LastEditDate': '2013-08-08T10:47:58.127', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '2253', 'Tags': '<algorithms><complexity-theory><formal-languages><terminology><reference-question>', 'CreationDate': '2013-08-08T06:10:27.963', 'FavoriteCount': '6', 'Id': '13669'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>While reading a cryptography textbook, i find the definition of a function that is hard on the average.(More precisely, it is 'hard on the average but easy with auxiliary input', but i omit latter for simplicity.)</p>\n\n<blockquote>\n  <p><strong>Definition : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>there exists</strong> a probabilistic polynomial-time algorithm $G$ such that<br>\n  for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>My question is why the statement of the existence of qualified algorithm G is sufficient? </p>\n\n<p>In other words, why the above definition gives a formal definition of 'hardness on the average' instead of following definition, which is more intuitive(?) to understand and more strict.\nWhy is the above definition sufficient? </p>\n\n<p>( Now I'm thinking that problem might occur when $G$ has only polynomial number of possible outputs, but if so, let's replace 'for any $G$' with 'for any $G$ which have exponentially many possible outputs' in following definition.)</p>\n\n<blockquote>\n  <p><strong>(strong?) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>for any</strong> probabilistic polynomial-time algorithm $G$ and for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>Another question is that whether a following simpler definition is equivalent to original definition or not?</p>\n\n<blockquote>\n  <p><strong>(simple) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(U_n)=h(U_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $U_n$ is a random variable uniformly distributed over $\\{0,1\\}^n$.</p>\n</blockquote>\n", 'ViewCount': '64', 'Title': "Completeness of formal definition of 'hardness on the average'", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-09T07:02:13.240', 'LastEditDate': '2013-08-09T07:02:13.240', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13678', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9565', 'Tags': '<complexity-theory><terminology><cryptography><randomized-algorithms><average-case>', 'CreationDate': '2013-08-08T12:43:21.820', 'Id': '13674'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>While reading the paper <a href="http://dl.acm.org/citation.cfm?id=564727" rel="nofollow">Holistic twig joins: optimal XML pattern matching</a> I came across the pseudo code for liststack algorithm. (available through google scholar)</p>\n\n<p>A function in the algorithm confused me, since I can understand what it is supposed to do, but can\'t deconstruct the notation:</p>\n\n<p>$\\qquad \\mathtt{Function}\\ \\mathtt{end}(q\\mathtt) \\\\\n \\qquad\\qquad \\mathtt{return}\\ \\forall q_i \\in \\mathtt{subtreeNodes}(q) : \\mathtt{isLeaf}(q_i) \\implies \\mathtt{eof}(T_{q_i})$</p>\n\n<p>This function is supposed to return a single boolean result. It is supposed to be true when all lists associated to leaf nodes of a query pattern node are at their end. So true means there are no more nodes in the query pattern to process.</p>\n\n<p>But what is the meaning of the set builder(ish?) notation here? Is it </p>\n\n<p>$\\qquad$ "for all subtree nodes of $q$ for which $\\mathtt{isLeaf}(q_i)$ is true $\\mathtt{eof}(T_{q_i})$ is also true"</p>\n\n<p>(which means that the list is at the end position)? </p>\n\n<p>Or is it </p>\n\n<p>$\\qquad$ "for all subtree nodes of $q$, $\\mathtt{isLeaf}(q_i)$ implies $\\mathtt{eof}(T_{q_i})$ is true"? </p>\n\n<p>Is double arrow representing implies with its truth table? </p>\n\n<p>As you can see, I\'m having a bit difficulty in associating the colon and its precedence.</p>\n', 'ViewCount': '208', 'Title': 'What is the meaning of this pseudo-code function?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-09T09:36:55.053', 'LastEditDate': '2013-08-09T07:21:37.877', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '13684', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9580', 'Tags': '<algorithms><terminology>', 'CreationDate': '2013-08-08T12:46:15.867', 'Id': '13675'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a list of pairs $(a_1,b_1),\\ldots,(a_n,b_n)$, where all $a_i \\geq 0$ and all $b_i &gt; 0$, my general problem is when we can use linear subset scan (described below) to solve the optimization problem of finding the optimal combination of pairs,</p>\n\n<p>$$I^* = \\hbox{argmax}_I F(\\sum_{i \\in I} a_i) / G(\\sum_{i \\in I} b_i)$$</p>\n\n<p>where $F,G$ are given increasing positive functions for positive inputs.</p>\n\n<p>I have found a class of functions where there is a fast solution, namely $F(x) = x + A$ and $G(x) = (x + B)^\\beta$ where $A,B \\geq 0$ and $0 \\leq \\beta \\leq 1$.  In this case, the optimal solution can be found by sorting all pairs $(a_i,b_i)$ in decreasing order according to $a_i/b_i$, and then trying the first $k$ pairs in sorted order for all $k$ and choosing the best solution, and this gives the optimal solution.  (This is an example of linear subset scan optimization.)  </p>\n\n<p>Now I want to know, if there a general class of functions $F,G$, ideally defined by abstract properties, where this linear subset scan approach works, where pairs are sorted either according to $a_i/b_i$ or perhaps sorted according to $H(a_i,b_i)$ where $H$ depends on $F,G$?  This could be a question where someone has a new insight and proof, or simply someone has seen something like this in the literature.  At any rate, I feel like the class of $F,G$ I stated where I have a proof is perhaps not the most general possible, and I'm missing some key abstract property that makes the linear subset scan work.   </p>\n", 'ViewCount': '98', 'Title': 'Generalizing the linear subset scan algorithm to a wider class of objective functions, maybe by finding a paper', 'LastActivityDate': '2014-04-06T23:59:48.870', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><optimization><greedy-algorithms>', 'CreationDate': '2013-08-08T23:01:49.873', 'Id': '13680'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Question:</strong></p>\n\n<p>Suppose I have a </p>\n\n<ul>\n<li>set of male people, and </li>\n<li>a function isAncestor(person1,person2) that checks whether person1 is an ancestor of person2 in O(1) time. Eg, isAncestor(grandfather, grandson) would return true.</li>\n</ul>\n\n<p>What are some fast algorithms for constructing a complete "family tree"  by using only this information? (or "family forest" as not everyone may be related to a single common ancestor).</p>\n\n<p><strong>Comment:</strong></p>\n\n<p>This is not homework or from a book. It is derived from some issues I\'m having coupling an adaptive finite element code to some blackbox mesh generation software, boiled down to the core problem and expressed in an analogy that shares the same mathematical structure. Right now I have a brute-force $O(n^3)$ method that seems horribly inefficient.</p>\n\n<p>Thank you for any help.</p>\n\n<p><strong>Results:</strong></p>\n\n<p>I\'ve implemented Gilles suggestion in Matlab and tested it. Here is the pseudocode, code, and unit test script. I release it all to the public domain.</p>\n\n<p><em>Pseudo-code</em>:</p>\n\n<pre>\nfunction buildAncestorForest\n    roots = empty list\n    for each person p\n        roots = graft(p, roots)\n\n    subfunction new_roots = graft(q, old_roots)\n        descendents = empty list\n        nondescendents = empty list\n        for each person t in old_roots\n            if q is an ancestor of t\n                append t to descendents\n            else\n                append t to nondescendents\n\n        if descendents is still empty\n            q_is_incomparable = true; \n            for each person t in old_roots\n                if t is an ancestor of q\n                    new_roots = old_roots;\n                    new children of t = graft(q, old children of t)\n                    q_is_incomparable = false;\n                    break for loop\n            if (q_is_incomparable)\n                new_roots = append q to old_roots;\n        else\n            new_roots = append q to nondescendents;\n            for each person t in descendents\n                children of q = graft(t, children of q);\n</pre>\n\n<p><em>Matlab code:</em></p>\n\n<p><a href="https://github.com/NickAlger/MeshADMM/blob/master/buildAncestorForest.m" rel="nofollow">https://github.com/NickAlger/MeshADMM/blob/master/buildAncestorForest.m</a></p>\n\n<p><em>Unit test script:</em></p>\n\n<p><a href="https://github.com/NickAlger/MeshADMM/blob/master/test_buildAncestorForest.m" rel="nofollow">https://github.com/NickAlger/MeshADMM/blob/master/test_buildAncestorForest.m</a></p>\n', 'ViewCount': '163', 'Title': 'Constructing Tree (forest) from Ancestor function', 'LastEditorUserId': '9589', 'LastActivityDate': '2013-08-25T01:24:03.963', 'LastEditDate': '2013-08-25T01:24:03.963', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '13692', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9589', 'Tags': '<algorithms><trees>', 'CreationDate': '2013-08-09T09:50:43.147', 'Id': '13685'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Currently, I'm delving into Analysis of Algorithms and I've discovered that I would need to improve my knowledge of Probability Theory. Any recommendation? Where do I start?\nThanks in advance!</p>\n", 'ViewCount': '195', 'Title': 'Recommended readings for Probability theory applied to algorithms', 'LastActivityDate': '2013-08-09T19:23:00.830', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '13690', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9085', 'Tags': '<algorithms><algorithm-analysis><probability-theory>', 'CreationDate': '2013-08-09T11:23:03.367', 'FavoriteCount': '4', 'Id': '13687'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have </p>\n\n<ol>\n<li>two red-black trees $T_1$ of black height $H_1$ and $T_2$ of black height $H_2$</li>\n<li>such that all the nodes $N$ belonging to $T_1$ are less than (in value) all the nodes $N$ of $T_2$</li>\n<li>and a key $K$ such that $K$ is greater than all the nodes of $T_1$ and less than all the nodes of $T_2$.</li>\n</ol>\n\n<p>I wanted to devise an algorithm to combine $T_1$, $K$ and $T_2$ into a single red-black tree $T$.</p>\n\n<p>I could delete each element from either $T_1$ or $T_2$ and put it in other tree. But that will give me an algorithm of time-complexity $2^{H_1}$ or $2^{H_2}$ (depending on the tree from which I have deleted the elements from). I would like to have an algorithm which is $O(\\max(H_1,H_2))$.</p>\n\n<blockquote>\n  <p>Definitions : <br><br> Black-height is the number of black-colored nodes in\n  its path to the root.</p>\n  \n  <p>Red-Black tree : A binary search tree, where each node is coloured\n  either red or black and</p>\n  \n  <ul>\n  <li>The root is black All NULL nodes are black</li>\n  <li>If a node is red, then both its children are black </li>\n  <li>For each node, all paths from that node to descendant NULL nodes have the same number of black nodes.</li>\n  </ul>\n</blockquote>\n', 'ViewCount': '561', 'Title': 'Joining two red-black trees', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-12T23:42:30.383', 'LastEditDate': '2013-08-11T13:20:15.663', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6699', 'Tags': '<algorithms><data-structures><search-trees>', 'CreationDate': '2013-08-11T07:06:44.943', 'Id': '13703'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have here a directed graph that I used to perform <a href="http://en.wikipedia.org/wiki/Dinic%27s_algorithm" rel="nofollow">Dinic\'s algorithm</a> to find maximum flow. I need to adjust this graph and this algorithm to work with dynamic trees (i.e. the Sleator-Tarjan algorithm).</p>\n\n<p>I just wish I could find an image source or video that can help me visualize the steps. Something <a href="http://www.arl.wustl.edu/~jst//cse/542/text/sec19.pdf" rel="nofollow">like this</a> is really close. But I just really want to find thing to lead me step by step in a visual way or a way that doesn\'t involve pseudocode.</p>\n', 'ViewCount': '106', 'Title': "Understanding Dinic's algorithm using dynamic trees", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-13T08:09:51.823', 'LastEditDate': '2013-08-13T08:09:51.823', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9626', 'Tags': '<algorithms><reference-request><network-flow>', 'CreationDate': '2013-08-12T20:48:37.890', 'Id': '13720'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Below is an algorithm for union of two quasi reduced BDDs <code>p</code> and <code>q</code> resulting in <code>r</code>.</p>\n\n<pre><code>bdd Union(bdd p, bdd q) \n  local bdd r;\n1 if p=0 or q=1 then return q;\n2 if q=0 or p=1 then return p;\n3 if p=q then return p;\n4 if Cachecontainsentry\u27e8UnionCODE,{p,q}:r\u27e9 then return r;\n  //p.lvl = q.lvl in case of quasi reduced BDDs\n5 r \u2190 UniqueTableInsert(p.lvl, Union(p[0], q[0]), Union(p[1], q[1]));\n6 enter\u27e8UnionCODE,{p,q}:r\u27e9 in Cache;\n7 return r;\n</code></pre>\n\n<p>Since there is no variable skipping, <code>p.lvl</code> is always equal to <code>q.lvl</code>. I have a question about this algorithm.</p>\n\n<p>If I want to implement Xor or Xnor for quasi-reduced BDDs, can it be done the same way as union or should I implement the expression <code>pq' + p'q</code> where <code>q' = !q</code>.</p>\n", 'ViewCount': '21', 'ClosedDate': '2013-08-13T08:54:25.437', 'Title': 'Can xor and xnor for quasi-reduced BDDs be implemented just like union?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-13T08:19:40.103', 'LastEditDate': '2013-08-13T08:19:40.103', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9598', 'Tags': '<algorithms><logic><binary-trees>', 'CreationDate': '2013-08-13T02:32:32.443', 'Id': '13722'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>How can I write an algorithm which inverts a 2-level BDD? It should take as input a 2L-level quasi-reduced BDD rooted at $r$ encoding a relation $R : B^L \u2192 2^{B^L}$ and returns the 2L-level quasi-reduced BDD rooted at $s$ encoding the relation $R^{\u22121} : B^L \u21922^{B^L}$, that is, $j \\in R(i) \\iff i \\in R^{\u22121}(j)$.</p>\n', 'ViewCount': '39', 'Title': 'Inversion of BDD', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-13T08:20:09.527', 'LastEditDate': '2013-08-13T08:20:09.527', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9598', 'Tags': '<algorithms><logic><binary-trees>', 'CreationDate': '2013-08-13T03:47:07.090', 'Id': '13723'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $k\\in \\mathbb{N}$ be fixed. The naive way to compute a sequence of values $a_1^k,\\ldots,a_n^k$ where $a_i\\in \\mathbb{N}$ for all $1\\leq i\\leq n$ is compute $a_i^k$ individually with the exponentiation by squaring. This takes a total of $O(n\\log k)$ multiplications. </p>\n\n<p>Is this optimal? What if for each $a_i$, we have $\\frac{1}{c} a_i \\leq a_{i+1}\\leq c a_i$.</p>\n\n<p>I come up with this question because I was doing a binary search over $f(n)=n^k$. </p>\n', 'ViewCount': '98', 'Title': 'Find a sequence of integer powers faster than the naive algorithm', 'LastEditorUserId': '220', 'LastActivityDate': '2013-08-13T17:33:19.327', 'LastEditDate': '2013-08-13T17:33:19.327', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms>', 'CreationDate': '2013-08-13T07:00:16.357', 'Id': '13725'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '196', 'Title': 'Data structure for maintaining large space-efficient filtered array', 'LastEditDate': '2013-08-14T06:17:49.983', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9639', 'FavoriteCount': '1', 'Body': "<p>How does one implement a space efficient data structure that satisfies the requirements below?</p>\n\n<ol>\n<li>You have a large array</li>\n<li>You have a filter which tells you which elements in that large array are to be deleted</li>\n<li>Lookup of i'th element in the array should be constant time</li>\n<li>The space formerly occupied by deleted elements in the array should be available for further use.</li>\n</ol>\n\n<p>I've explored approaches ranging from bloom filters to trees, but they violate one requirement or the other.</p>\n\n<p>EDIT: Further clarification.</p>\n\n<p>Space-efficient: Any space not used by elements of the array that have not been deleted by the filter, which we can term extra space or space for book-keeping, should be $O(1)$. We can probably relax this to allow any solution that gets close to $O(1)$ extra space. $O(n)$ extra space is undesirable.</p>\n\n<p>Array in the context of this problem is an array-like collection where you can get the $i$th element in constant time. You lookup by index here. </p>\n\n<p>I suspect amortized performance bounds should be fine.</p>\n\n<p>The filter takes each element of collection and returns 0/1 corresponding to delete/no delete.</p>\n", 'Tags': '<algorithms><data-structures>', 'LastEditorUserId': '9639', 'LastActivityDate': '2013-08-14T06:17:49.983', 'CommentCount': '7', 'AcceptedAnswerId': '13738', 'CreationDate': '2013-08-13T11:44:30.220', 'Id': '13732'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can you provide an example of NN algorithm failure on the Euclidean traveling salesman problem?</p>\n\n<p>I was trying to construct a specific example of this for my friends and was failing.</p>\n', 'ViewCount': '229', 'Title': 'When does the nearest neighbor heuristic fail for the Traveling Salesperson?', 'LastEditorUserId': '2253', 'LastActivityDate': '2013-08-30T18:00:29.553', 'LastEditDate': '2013-08-14T19:07:46.930', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '13744', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9662', 'Tags': '<algorithms><heuristics><traveling-salesman>', 'CreationDate': '2013-08-14T18:37:46.277', 'Id': '13742'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the following function, let $n \\geq m$.  </p>\n\n<pre><code>int gcd(n,m)  \n{  \n if (n % m == 0) return m;  \n n = n % m;  \n return gcd(m, n);  \n}\n</code></pre>\n\n<p>How many recursive calls are made by this function?</p>\n\n<ul>\n<li>$\\Theta (\\log_2 n)$</li>\n<li>$\\Omega (n)$</li>\n<li>$\\Theta (\\log_2(\\log_2 n))$</li>\n<li>$\\Theta ( \\sqrt{n} )$  </li>\n</ul>\n\n<p>I think the answer is $\\Theta (\\log_2(\\log_2n))$, but my book is saying $\\Theta (\\log_2 n)$.    </p>\n\n<p>My reasoning is as follows. Here we are not dividing the number. If there was a division then it would be $\\log n$. But here operation is $\\bmod$. So we will get a very small number after the first call. So it must be $\\log \\log n$. Am I thinking correctly?</p>\n', 'ViewCount': '259', 'Title': 'How many recursive calls are made by this gcd function?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-19T12:42:07.087', 'LastEditDate': '2013-08-17T18:18:39.033', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<algorithms><algorithm-analysis><arithmetic>', 'CreationDate': '2013-08-15T17:26:36.207', 'Id': '13761'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My question is related to <a href="http://math.stackexchange.com/questions/465240/constant-time-algorithm-for-finding-greatest-factor-of-odd-composite-number">this question</a> posted on <a href="http://math.stackexchange.com">math.SE</a>:</p>\n\n<blockquote>\n  <p>Given an odd number, what is the quickest (constant-time) algorithm for finding its largest factor and suppose you can call a helper function $B$ which takes as its input $(N, k)$ and outputs True iff $N$ has a factor greater than or equal to $k$? Obviously, the factor cannot be itself. </p>\n</blockquote>\n\n<p>My slightly altered problem statement goes like this.</p>\n\n<blockquote>\n  <p>Given an odd integer $n$, find its largest factor (that is not itself). You can call a function $B(m,k)$ that returns $1$ iff $m$ has a factor smaller than $k$. The function runs in constant time.</p>\n</blockquote>\n\n<p>Can this be done faster than $O(\\log n)$ in the average case (assuming the input is chosen uniformly at random)? Is my altered problem statement any better than the original? Specifically, I know that the probability some large number will have no factor smaller than $M$ is asymptotic to $\\frac{1}{\\log M}$ (see <a href="http://math.stackexchange.com/questions/94645/expected-smallest-prime-factor">A</a>, and <a href="http://math.stackexchange.com/questions/284500/probability-of-a-number-not-having-factors-below-n">B</a>). Can you use this to your advantage?</p>\n\n<p>You can also assume that division is constant time.</p>\n\n<h1>Edit and Attempt Solution:</h1>\n\n<p>$\n\\newcommand{\\ha}[2]{\\left[#1 \\dots #2\\right)} \n\\newcommand{\\expa}[1]{2^{#1}}\n\\newcommand{\\expb}[1]{2^{2^{#1}}}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\expv}[1]{\\mathrm{E}\\left(#1\\right)}\n\\newcommand{\\sch}{\\mathbb{S}}\n\\newcommand{\\floor}[1]{\\left\\lfloor#1 \\right\\rfloor}\n\\newcommand{\\logb}[1]{\\log \\log #1}\n$\nThe difference between my problem statement and the original, is the definition of the function $B$. In the original, $B(N,k)$ returns $1$ if $N$ has a factor greater than or equal to $k$; in my version, this happens if $N$ has a factor <em>less than</em> $k$. </p>\n\n<p>By making this change, I aim to capitalize on the fact that the probability of a large $N$ having no factors smaller than $M$ is asymptotic to $(\\log M)^{-1}$. While this fact does not change the worst-case performance of an algorithm, it can change average-case performance. Here, by average case, I mean probabilistic analysis of the algorithm over random, uniformly distributed inputs.</p>\n\n<p>(Note that my question involves only exact algorithms)</p>\n\n<p>I conjectured in my comment to the related question that you can benefit from the probability by changing the way in which you partition your search space when performing a binary search (since it is much more likely for the solution to be in $\\ha{1}{1000}$ instead of $\\ha{1000}{N}$). I also conjectured the algorithm could run in $O(\\logb N)$ average-case.</p>\n\n<h2>Attempted Solution</h2>\n\n<p>I decided to do some work on the problem myself, and I think I have a solution. I haven\'t really done this sort of analysis previously, so I may have some error, and there is definitely a lot missing in terms of details. I think the idea is correct, though. Note that here I find the <em>smallest</em> factor of $N$. We can easily find the largest factor by division (which I assume to be contstant time).</p>\n\n<p>I\'m posting it as part of the question because I\'m not sure if it\'s correct, and I still want to know if there\'s a better way.</p>\n\n<p>Let us partition the search space $\\sch = \\ha{1}{N}$ into disjoint integer intervals, \n$$ r_k = \\ha{\\expb k}{\\expb {k+1}} \\qquad 0 \\leq k \\leq \\floor{\\logb N}$$</p>\n\n<p>Note that it can be that,\n$$ \\expb{\\floor{\\logb N+1}} &gt; N$$\nThat doesn\'t really matter; all we want from the partitioning $r$ is to contain the entire search space.</p>\n\n<p>Now, the probability that the smallest factor of $N$, which we will call $A$, is greater than $m$ is asymptotic to $(\\log m)^{-1}$. \nThen let, $P(A &gt; \\expb{k}) = 2^{-k}$, where $A$ is taken to be a random variable. If we let $P(r_k)$ denote the probability that $A \\in r_i$, we can calculate this as:\n$$P(r_k) = P(a &gt; \\expb{k}) - P(a &gt; \\expb{k+1}) = 2^{-(k+1)}$$</p>\n\n<p>We can identify which partition $r_k$ contains $A$ by calling the function $B(N,\\expb{k})$ up to $\\floor{\\logb N} + 1$ times. After finding the $r_k$, we then perform a binary search for $A$ in the partition, which involves $\\log \\abs{r_k}$ operations. Here we note that:\n$$|r_k|=\\expb{k+1}-\\expb{k} = \\expb{k}\\left(\\expb{k} - 1\\right)\\leq \\expb{k+1}$$</p>\n\n<p>Let $X$ be a random variable representing the number of operations taken by the binary search. The value of $X$ for the case when $A \\in r_k$ is given $X_k = \\log \\expb{k+1} = 2^{k+1}$. The expected value of $X$ is then,\n$$\\expv{X} = \\sum_{k=0}^{\\floor{\\logb N} + 1} X_i P(r_i) = \\sum_{k=0}^{\\floor{\\logb N} + 1} 2^{k+1}\\cdot 2^{-(k+1)} = \\floor{\\logb N} +1$$</p>\n\n<h2>Notes</h2>\n\n<p>I\'ve considered partitioning the search space using triple-exponentiation (e.g. $\\expa{\\expb{k}}$), but that provides no benefit. There might be a way to make the search algorithm inside the partitions faster though, but I\'m not sure how. </p>\n\n<p>You can also reduce the search space drastically (such as to something like $\\sqrt{N}$), but I think this will have a constant speedup at most.</p>\n', 'ViewCount': '195', 'Title': 'Time complexity of finding the largest factor of a number (using a specific oracle)', 'LastEditorUserId': '4543', 'LastActivityDate': '2013-08-17T16:51:14.237', 'LastEditDate': '2013-08-17T16:51:14.237', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4543', 'Tags': '<algorithms><time-complexity><number-theory><factoring>', 'CreationDate': '2013-08-16T09:49:37.510', 'FavoriteCount': '2', 'Id': '13773'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '189', 'Title': 'When testing n items, how to cover all t-subsets by as few s-subsets as possible?', 'LastEditDate': '2013-08-17T13:23:45.250', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '6850', 'FavoriteCount': '1', 'Body': '<p>This problem arose from software testing. The problem is a bit difficult to explain. I will first give an example, then try to generalize the problem.</p>\n\n<p>There are 10 items to be tested, say A to J, and a testing tool that can test 3 items at the same time. Order of items in the testing tool does not matter. Of course, for exhaustive testing, we need $^{10}C_{3}$ combinations of items.</p>\n\n<p>The problem is more complex. There is an additional condition that once a pair of items has been tested together, than the same pair does not need to be tested again.</p>\n\n<p>For example, once we executed the following three tests:</p>\n\n<p>A B C</p>\n\n<p>A D E</p>\n\n<p>B D F</p>\n\n<p>we do not have to execute:</p>\n\n<p>A B D</p>\n\n<p>because the pair A,B was covered by the first test case, A,D was covered by the second, and B,D was covered by the third.</p>\n\n<p>So the problem is, what is the minimum number of test cases that we need to ensure that all pairs are tested?</p>\n\n<p>To generalize, if we have n items, s can be tested at the same time, and we need to ensure that all possible t tuples are tested (such that s > t), what is the minimum number of test cases that we need in terms of n, s and t?</p>\n\n<p>And finally, what would be a good algorithm to generate the required test cases?</p>\n', 'Tags': '<algorithms><combinatorics><software-testing>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-11T09:46:09.560', 'CommentCount': '4', 'AcceptedAnswerId': '13797', 'CreationDate': '2013-08-17T09:47:04.113', 'Id': '13788'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If we have an array $A$ of length $N$, which is partitioned into $\\sqrt{N}$ adjacent subarrays $A(i)$, each of which is monotonically ordered from $\\min(i)$ to $\\max(i)$ (it is known what places have those bounds in A for each subarray), with the following conditions:</p>\n\n<pre><code>min(i) &lt; min(i+1),\nmax(i) &lt; max(i+1),\nmin(i+1) &lt; max(i),\nIf subarrays A(i) and A(i+1) both have the j-th element, then A(i,j) &lt; A(i+1,j),\nThe number of elements in subarray A(i) is strictly increasing with i\n</code></pre>\n\n<p>Would it be possible to search an element in the array $A$, in time $O((\\log N)^k)$, for a positive $k$?  If yes how?</p>\n\n<p>If not, what kind of minimal additional conditions would allow that?</p>\n\n<p>Edit. The subarrays are not necessarily of equal length.\nThey are strictly increasing. The above stated conditions are always guaranteed.</p>\n\n<p>What I have tried so far is to binary search them separately, but that is obviously going to be very inefficient. </p>\n\n<p>[Note that this question has obviously nothing to do with the search of the min of max value in the array. And I am not interested in sorting the array. Just the mere search.]</p>\n', 'ViewCount': '145', 'Title': 'Searching a value in a "piecewise" ordered array', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-23T10:37:11.463', 'LastEditDate': '2013-08-23T10:37:11.463', 'AnswerCount': '1', 'CommentCount': '15', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5147', 'Tags': '<algorithms><search-algorithms><arrays><binary-search>', 'CreationDate': '2013-08-18T12:19:35.963', 'FavoriteCount': '2', 'Id': '13801'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Imagine that an algorithm A runs in worst-case time $f(n)$ and that algorithm B runs in worst-case time $g(n)$. Answer either yes, no, or can\u2019t tell and could you explain me why?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(h)=\u03a9(f(n)\\log n)$?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(n)=O(f(n)\\log n)$?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(n)=\u0398(f(n)\\log n)$?</p>\n', 'ViewCount': '133', 'Title': 'Worst-case time algorithm?...which one is faster?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-08-20T17:22:07.640', 'LastEditDate': '2013-08-20T17:22:07.640', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-08-20T03:11:35.790', 'FavoriteCount': '2', 'Id': '13827'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $f(n)$ and $g(n)$ be complexity functions. Are the following statements true?</p>\n\n<p>$$\n\\begin{equation}\n\\frac {f(n)}{g(n)}= O\\left(\\frac{f(n)}{ g(n)}\\right) \\\\\nf(n) \\times g(n) = O(\\max(f^2(n),g^2(n)))\n\\end{equation}\n$$</p>\n', 'ViewCount': '50', 'Title': 'Maximum complexity of a product and of a quotient', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-21T07:41:40.623', 'LastEditDate': '2013-08-21T07:41:40.623', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'CreationDate': '2013-08-21T03:24:32.137', 'Id': '13850'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For example, I've a chromosome with 10 genes, the first 5 genes represent a specific property of phenotype and the last 5 genes represent another property of phenotype.</p>\n\n<p>So, I need a crossover operator that performs the cross considering this configuration, where the parent1's first 5 genes are cross with parent2's first 5 genes and the same for the last 5 genes.</p>\n\n<p>Is there any paper or description of a crossover operator to perform cross in this kind of chromosome?</p>\n", 'ViewCount': '26', 'Title': 'Is there a crossover operator to cross sections of specific phenotype genes?', 'LastEditorUserId': '9766', 'LastActivityDate': '2013-08-21T13:49:28.587', 'LastEditDate': '2013-08-21T13:49:28.587', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9766', 'Tags': '<genetic-algorithms>', 'CreationDate': '2013-08-21T12:43:40.503', 'Id': '13852'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a whole number N..</p>\n\n<p>Arrange 1 to N in a sequence such that no two numbers have their average sitting between them... </p>\n\n<p>Note - \nIf N=20.. average of 19 and 2 = 10.5 is not a whole number .. hence we do not need to worry about such numbers and their averages..\nIn other words, if average of two numbers is not a whole number.. we assume that their average does not exist in the list 1 to N..</p>\n', 'ViewCount': '222', 'Title': 'Order a list of whole numbers so that no two numbers have the average of them sitting between them', 'LastActivityDate': '2013-08-23T10:56:47.850', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '13858', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9166', 'Tags': '<algorithms>', 'CreationDate': '2013-08-21T15:24:05.407', 'FavoriteCount': '2', 'Id': '13857'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to find an algorithm to solve the seating chart problem. The goal is to place pepole at one (or multiple) tables such that the overall happiness is maximized.</p>\n\n<p>Each seat has neighbors. A person on a seat can talk to persons on neighboring seats (note that these neighbors normally sit next to each other or on the opposite side of the table).</p>\n\n<p>Example of a table with 4 seats on each side:</p>\n\n<pre><code>1 2 3 4\n8 7 6 5\n</code></pre>\n\n<p>6 has neighbors 2, 3, 4, 5 and 7, 8 has neighbors 1, 2, 7</p>\n\n<p>There's a matrix describing how well two persons get along with each other. This is a value between 0 and 9 (the relation value). Relations are symmetrical.</p>\n\n<p>Example of a relation matrix with four people:</p>\n\n<pre><code>  A B C D\nA 0 2 0 5\nB 2 0 3 6\nC 0 3 0 1\nD 5 6 1 0\n</code></pre>\n\n<p>The happiness of a single person is the sum of the relation values of all neighboring persons.</p>\n\n<p>How would you find a solution that maximizes the overall happiness?</p>\n", 'ViewCount': '233', 'Title': 'Seating Chart Optimization', 'LastActivityDate': '2013-08-23T09:00:23.500', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13884', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9785', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-08-22T20:00:13.087', 'Id': '13873'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What is the term for an algorithm that always requests the same sequence of pages?  I recall seeing this concept before buti haven't been able to find anything on Google without more specific vocabulary. </p>\n\n<p>One algorithm in this category is a linear scan to (say) find the minimum of a set of data. Each page is accessed in order, regardless of the value of previous accesses. </p>\n\n<p>Most algorithms do not fall into this category. For example, a binary search chooses what page to access based on the values in the page before it. </p>\n", 'ViewCount': '34', 'Title': 'static paging vocabulary request', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-26T07:24:37.880', 'LastEditDate': '2013-08-26T07:24:37.880', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2118', 'Tags': '<algorithms><terminology><memory-access>', 'CreationDate': '2013-08-23T16:04:56.540', 'Id': '13891'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '838', 'Title': "What's Big O of $\\log(n+7)^{\\log(n)}$?", 'LastEditDate': '2013-08-25T11:38:43.547', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9815', 'FavoriteCount': '1', 'Body': "<p>As part of my continual improvement plans, I'm reviewing algorithm analysis and I've hit a roadblock. I cannot figure out how to determine the Big O complexity of $\\log(n+7)^{\\log(n)}$. I've spent the last few days rolling this over in my head and I still haven't made any progress in solving it. Could someone tell me what the Big O of this algorithm is, and explain how you solved it?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-03T17:16:56.050', 'CommentCount': '2', 'AcceptedAnswerId': '13913', 'CreationDate': '2013-08-24T17:35:13.647', 'Id': '13912'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If $u,v \\in \\mathbb{R}^d$ are two $d$-dimensional vectors, say that $u\\le v$ if $u_i \\le v_i$ for all $i=1,\\dots,d$.  In other words, comparisons on vectors will be pointwise.</p>\n\n<p>Let $S,T$ be two subsets of $\\mathbb{N}^d$ of size $m$.  Is there an efficient way to test whether there exists $s\\in S, t \\in T$ such that $s\\le t$?  The naive algorithm does $m^2$ comparisons between vectors; is there a more efficient algorithm?</p>\n\n<p>If $d=1$, this is very easy: we simply find the smallest element in $S$ and the largest element in $T$, which can be done with $O(m)$ comparisons.  But already when $d=2$, it seems much harder.  Any ideas?</p>\n', 'ViewCount': '132', 'Title': 'Comparing sets of vectors', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-29T22:53:35.173', 'LastEditDate': '2013-08-26T02:14:02.180', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-structures><computational-geometry><linear-algebra>', 'CreationDate': '2013-08-26T02:08:59.753', 'FavoriteCount': '1', 'Id': '13927'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '591', 'Title': 'Rearrange an array using swap with 0', 'LastEditDate': '2013-08-26T11:11:43.570', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4752', 'FavoriteCount': '2', 'Body': u'<p>This is a Google interview question. I got it from a website.</p>\n\n<blockquote>\n  <p>You have two arrays <strong>source</strong> and <strong>target</strong>, containing two permutations of the numbers <code>[0..n-1]</code>. You would like to rearrange source so that it equals to target. The only allowed operations is <strong>\u201cswap a number with 0\u201d</strong>. Find the minimum number of swaps?</p>\n  \n  <p>e.g. {1,0,2,3} -> {1,3,2,0}  swap 0 with 3. one swap is enough.</p>\n</blockquote>\n\n<p>My attempt on problem: If we consider arrays as strings we could use edit distance to convert source to target if other edit distance operations like insert,delete,replace etc are allowed but here only allowed operation is swapping.</p>\n', 'Tags': '<algorithms><sorting><permutations>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-30T07:40:21.463', 'CommentCount': '1', 'AcceptedAnswerId': '13938', 'CreationDate': '2013-08-26T05:21:43.070', 'Id': '13930'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A shuffling algorithm is supposed to generate a random permutation of a given finite set. So, for a set of size $n$, a shuffling algorithm should return any of the $n!$ permutations of the set uniformly at random. </p>\n\n<p>Also, conceptually, a randomized algorithm can be viewed as a deterministic algorithm of the input and some random seed. Let $S$ be any shuffling algorithm. On input $X$ of size $n$, its output is a function of the randomness it has read. To produce $n!$ different outputs, $S$ must have read at least $\\log(n!) = \\Omega(n \\log n)$ bits of randomness. Hence, any shuffling algorithm must take $\\Omega(n \\log n)$ time.</p>\n\n<p>On the other hand, the <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle" rel="nofollow">Fisher-Yates shuffle</a> is <a href="http://www.codinghorror.com/blog/2007/12/the-danger-of-naivete.html" rel="nofollow">widely</a> <a href="http://c2.com/cgi/wiki?LinearShuffle" rel="nofollow">believed</a> to run in $O(n)$ time. Is there something wrong with my argument? If not, why is this belief so widespread?</p>\n', 'ViewCount': '161', 'Title': 'How can you shuffle in $O(n)$ time if you need $\\Omega(n \\log n)$ random bits?', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-08-29T08:52:52.413', 'LastEditDate': '2013-08-28T18:42:00.367', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9847', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><randomized-algorithms>', 'CreationDate': '2013-08-28T07:42:37.830', 'Id': '13990'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have two polynomials $f(x)$ and $g(x)$ and I somehow represent their coefficients.\nI have a couple of ways to hold a polynomial depending on how many significant coefficients the polynomial has.\nI want to determine the amount of significant coefficients in the results of\n$f(x) + g(x)$ ,$f(x) \\cdot g(x)$ , $f(x) - g(x)$ etc. .</p>\n\n<p>But I'd like to do it before I create the object that holds them, is there some efficient way of doing this without calculating the result twice?</p>\n\n<p>I can assume that I know the current rank and number of elements in $f(x)$ and $g(x)$</p>\n\n<p>If this is not possible knowing that the new polynomial's non-trivial coefficients will be at least half of the rank will suffice, but I'm unsure how to do it as well.</p>\n\n<p>I did try to apply various heuristics but didn't come up with something consistent and fast. </p>\n", 'ViewCount': '67', 'Title': 'Calculate the number of elements after multiplying/adding two polynomials', 'LastActivityDate': '2013-08-28T21:13:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14006', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8709', 'Tags': '<algorithms><time-complexity><optimization><efficiency><arithmetic>', 'CreationDate': '2013-08-28T19:13:48.453', 'Id': '13997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given two integers $n$ and $m$, how many numbers exist such that all integers have all digits from $0$ to $n-1$, the difference between two adjacent digits is exactly $1$, and the number of digits in the integer is at most $m$?</p>\n\n<p>The integer cannot start with a $0$. All digits from $0$ to $n-1$ must be present.</p>\n\n<p>Example: for $n = 3$ and $m = 6$ there are $18$ such numbers ($210, 2101, 21012, 210121 \\ldots$)</p>\n\n<p>I know there is a dynamic programming method to solve this. After looking the solution, I am not able to understand it. Can anybody please give any good solution to me?</p>\n', 'ViewCount': '78', 'Title': 'Count the number of integers satisfying two conditions using DP', 'LastEditorUserId': '472', 'LastActivityDate': '2013-08-29T10:35:38.797', 'LastEditDate': '2013-08-29T10:35:38.797', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '14005', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8819', 'Tags': '<algorithms><combinatorics><dynamic-programming>', 'CreationDate': '2013-08-28T20:32:31.977', 'Id': '14002'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to sample a uniformly random point in a polygon...</p>\n\n<p>If sample a large number they\'d be equally likely to fall into two regions if they have the same area.</p>\n\n<p>This would be quite simple if it were a square since I would take two random numbers in [0,1] as my coordinates.</p>\n\n<p>The shape I have is a regular polygon, but I\'d like it to work for any polygon.</p>\n\n<p><a href="http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle">http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle</a></p>\n\n<p><img src="http://mathworld.wolfram.com/images/eps-gif/TrianglePointPicking_700.gif" width="400"></p>\n', 'ViewCount': '192', 'Title': 'Random sampling in a polygon', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-29T14:38:32.687', 'LastEditDate': '2013-08-29T14:38:32.687', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><randomness><sampling><random-number-generator>', 'CreationDate': '2013-08-29T00:36:47.990', 'Id': '14007'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to construct a complete graph where each node is connected to every other node. The link between the nodes give a distance function (does not follow triangle inequality) between them. What I require is to merge the closest nodes, (bounded by a threshold) into a single node and recompute the graph each time, recursively. This is because if two nodes are merged, then all the links connected to the new node has to be updated with the newly computed distance for the new edge. Since its a complete graph this would be an expensive operation. </p>\n\n<p>I would have a large number of nodes with $N*(N-1)/2$ links for $N$ nodes , and $N=60$ or more , \nWhat I want to do is to reduce this complete graph into clusters of  sub graphs with low intra node distance, yet with a relatively high distance between the subgraphs. So,How can I go about solving this problem?</p>\n\n<p>This is like applying the graph partitioning techniques to complete graphs.</p>\n\n<p>One of the solutions is <a href="http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf" rel="nofollow">this paper</a> which proposes the normalized cut criterion for partitioning the graph  minimizing normalized cuts on graphs which is clearly NP-complete,Is there a practically efficient way and what are the recent developments to achieve this?</p>\n\n<p>So what I want is an efficient solution to the above problem which is not NP complete ? Since this problem is NP Complete , answers using randomization or approximation methods would be welcome</p>\n', 'ViewCount': '173', 'Title': 'Algorithm for Graph merge and recompute', 'LastEditorUserId': '9881', 'LastActivityDate': '2013-09-01T18:21:45.387', 'LastEditDate': '2013-09-01T18:21:45.387', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9881', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2013-08-29T07:32:27.390', 'Id': '14014'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When it comes to the design of algorithms, one often employs the following techniques:</p>\n\n<ul>\n<li>Dynamic Programming</li>\n<li>The Greedy-Strategy</li>\n<li>Divide-and-Conquer</li>\n</ul>\n\n<p>While for the first two methods, there are well-known theoretical foundations, namely the Bellman Optimality Principle and matroid (resp. greedoid) theory, I could not find such a general framework for algorithms based on D&amp;C. </p>\n\n<p>Firstly, I am aware of something that we (or rather, the prof) introduced  in a functional programming class,  called an "algorithmic skeleton", that arose in the context of combinators. As an example hereof, we gave such a skeleton for D&amp;C algorithms as follows: </p>\n\n<p><strong>Definition</strong>: Let $A,S$ be non-empty sets. We call the elements of $S$ <em>solutions</em>, and the elements of $P:=\\mathfrak{P}(A)$ (that is, the subsets of $A$) are referred to as <em>problems</em>.\nThen, a <em>D&amp;C-skeleton</em> is a 4-tuple $(P_\\beta, \\beta, \\mathcal{D}, \\mathcal{C})$, where: </p>\n\n<ul>\n<li>$P_\\beta$ is a predicate over the set of problems and we say that a problem $p$ is <em>basic</em> iff $P_\\beta(p)$ holds.</li>\n<li>$\\beta$ is a mapping $P_\\beta \\rightarrow S$ that assigns a solution to each basic problem. </li>\n<li>$\\mathcal{D}$ is a mapping  $P \\rightarrow \\mathfrak{P}(P)$ that divides each problem into a set of subproblems.</li>\n<li>$\\mathcal{C}$ is a mapping $P\\times \\mathfrak{P}(S) \\rightarrow S$ that joins the solutions (depending on kind of a "pivot problem") of the subproblems to produce a solution. </li>\n</ul>\n\n<p>Then, for a given skeleton $s=(P_\\beta, \\beta, \\mathcal{D}, \\mathcal{C})$ and a problem $p$, the following generic function $f_s: P\\rightarrow S$ computes a solution (in the formal sense) for $p$:</p>\n\n<p>$f_s(p)= \\left\\{\n  \\begin{array}{l l}\n    \\beta(p) &amp; \\quad \\text{if $p$ is basic}\\\\\n    \\mathcal{C}(p,f(\\mathcal{D}(p))) &amp; \\quad \\text{otherwise}\n  \\end{array} \\right.$</p>\n\n<p>where in the second line we use the notation $f(X) := \\{f(x)  : x\\in X\\}$ for subsets $X$ of the codomain of a mapping $f$.</p>\n\n<p>However, we did not further examine the underlying, "structural" properties of problems that can be formulated this way (as I said, it was a functional programming class and this was only a small example). Unfortunately, I could not find further reference on this very approach. Hence I don\'t think the above definitions are quite standard. If someone recognizes what I have stated above, I would be glad about related articles.</p>\n\n<p>Secondly, for the greedy strategy we have the famous result that a problem is correctly solved by the general greedy algorithm if and only if its solutions constitute a weighted matroid. Are there similar results for D&amp;C-algorithms (not necessarily based on the method outlined above)?</p>\n', 'ViewCount': '355', 'Title': 'Theoretical foundations of Divide and Conquer', 'LastActivityDate': '2014-01-27T13:03:11.173', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '20011', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '7486', 'Tags': '<algorithms><reference-request><divide-and-conquer>', 'CreationDate': '2013-08-29T16:27:18.153', 'FavoriteCount': '4', 'Id': '14022'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the <em>edge-disjoint paths problem</em> (EDP), we are given a (possibly directed) graph $G=(V,E)$, and a set of distinct source-sink pairs $\\{ (s_i,t_i) \\mid 1 \\leq i \\leq k \\}$, and we want to maximize the number of pairs that can be simultaneously connected in an edge-disjoint manner. When we add the constraint that the paths need to be <em>shortest paths</em>, we get the <em>edge-disjoint shortest paths</em> (EDSP) problem.</p>\n\n<p>According to [1], the EDSP problem is hard for a graph with unit edge lengths, even when the graph is planar. Furthermore, it claims this is so for both directed and undirected graphs.</p>\n\n<blockquote>\n  <p>What is known about the approximability of the EDSP problem?</p>\n</blockquote>\n\n<p>I\'m especially interested in results for undirected graphs. In [2], the authors seem to only consider variants of the EDP problem, but not the EDSP problem. Further following the references, it seems like the EDP problem has been studied extensively.</p>\n\n<hr>\n\n<p>[1] <a href="http://www.sciencedirect.com/science/article/pii/S0166218X97001212" rel="nofollow">Eilam-Tzoreff, Tali. "The disjoint shortest paths problem." Discrete applied mathematics 85.2 (1998): 113-138.</a></p>\n\n<p>[2] <a href="http://www.sciencedirect.com/science/article/pii/S0022000003000667" rel="nofollow">Guruswami, Venkatesan, et al. "Near-optimal hardness results and approximation algorithms for edge-disjoint paths and related problems." Journal of Computer and System Sciences 67.3 (2003): 473-496.</a></p>\n', 'ViewCount': '131', 'Title': 'Approximability of the edge-disjoint shortest paths problem', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-01T11:44:40.417', 'LastEditDate': '2013-09-01T11:44:40.417', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><reference-request><shortest-path><approximation>', 'CreationDate': '2013-08-29T16:50:41.277', 'Id': '14023'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm studying for the computer science GRE, and as an exercise I need to provide a recursive algorithm to compute Fibonacci numbers and show its correctness by mathematical induction.</p>\n\n<p>Here is my recursive version of an algorithm to compute Fibonacci numbers:</p>\n\n<pre><code>Fibonacci(n):\n    if n = 0 then     // base case\n        return 0\n    elseif n = 1 then // base case\n        return 1\n    else\n        return Fibonacci(n - 1) + Fibonacci(n - 2)\n    endif\n</code></pre>\n\n<p>How can I prove the correctness of this algorithm by induction?</p>\n", 'ViewCount': '1036', 'Title': 'Prove correctness of recursive Fibonacci algorithm, using proof by induction', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-31T10:28:41.523', 'LastEditDate': '2013-08-31T05:55:13.280', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2013-08-29T18:27:55.383', 'FavoriteCount': '1', 'Id': '14025'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am new to Algorithms and am currently confused about the running time of the ThreeSum program. If we look at ThreeSum\'s count function:</p>\n\n<pre><code>public static int count(int[] a) {\n  int N     = a.length;\n  int count = 0;\n\n  for (int i = 0; i &lt; N; i++) &lt;-- A\n    for (int j = i + 1; j &lt; N; j++) &lt;-- B\n      for (int k = j + 1; k &lt; N; k++) &lt;-- C\n        // check each triple\n        if (a[i] + a[j] + a[k] == 0) \n          count++;\n  // loop i = 0; j = i+1; k = j+1 so that we get each triple just once\n\n  return count;\n  }\n</code></pre>\n\n<p>I understand that <code>int N = a.length</code> means that its frequency is 1 as the statement only runs once and that <code>count++</code>\'s frequency is x as it depends on input however with nested for loops. I understand that say <code>A</code>\'s frequency is $N$ (as it is repeated <code>N</code> times) but then I get a bit confused. </p>\n\n<p>It seems to be that for each nested loop, the other loop has to run which makes sense so <code>B</code>\'s frequency is $N^2$ but then in the book it says that B\'s frequency is $N^2/2$ and C is $N^3/6$ (where I would have assumed $N^2$ and $N^3$) where is the "$/2$" and "$/6$" coming from?</p>\n\n<p>Any help is appreciated, thanks. Sorry for the lengthy question!</p>\n', 'ViewCount': '687', 'ClosedDate': '2013-09-02T10:29:49.600', 'Title': 'Time complexity of a triple nested for loop for ThreeSum problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T10:28:53.567', 'LastEditDate': '2013-09-02T10:28:53.567', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9925', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-01T11:53:16.337', 'Id': '14065'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '192', 'Title': 'Difficulty in understanding an approach to SPOJ $d$-query problem', 'LastEditDate': '2013-09-02T16:24:59.653', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8819', 'FavoriteCount': '1', 'Body': '<p>I was trying to solve <a href="http://www.spoj.com/problems/DQUERY/" rel="nofollow">this problem on SPOJ</a>:</p>\n\n<blockquote>\n  <p>Given a sequence of $n$ numbers $a_1, a_2, \\ldots, a_n$ and a number of $d$-queries. A $d$-query is a pair $(i, j)$, $(1 \\leq i \\leq j \\leq n)$. For each $d$-query $(i, j)$, you have to return the number of distinct elements in the subsequence $a_i, a_{i+1}, \\ldots, a_j$.</p>\n</blockquote>\n\n<p>Specifically, the input to the problem is as follows.</p>\n\n<ul>\n<li>Line 1: $n$, where $1 \\leq n \\leq 30000$</li>\n<li>Line 2: $n$ numbers $a_1, a_2, \\ldots, a_n$, where $1 \\leq a_i \\leq 10^6$</li>\n<li>Line 3: $q$, the number of $d$-queries, where $1 \\leq q \\leq 200000$</li>\n<li>Finally, $q$ lines, where each line contains 2 numbers $i, j$ representing a $d$-query, where $1 \\leq i \\leq j \\leq n$.</li>\n</ul>\n\n<p>But after a few hours of trying to solve it, I googled for a possible hint. I found one approach described <a href="http://apps.topcoder.com/forums/;jsessionid=5A69961AF7DF7FBB00FDFE13B80B5D2E?module=Thread&amp;threadID=627423&amp;start=0&amp;mc=13" rel="nofollow">here by user irancoldfusion:</a></p>\n\n<blockquote>\n  <p>First I thought a good O(n ^ 2 + q log q) would pass, but it didn\'t run in time. Then, I thought of the idea below which I haven\'t implemented yet:</p>\n  \n  <p>The result of a query [a, b] is number of integers whose last occurrence in [1, b] is >= a.</p>\n  \n  <p>Let\'s have two kinds of events: QueryEvent and NumberEvent. First we read whole input and sort all events, the key for QueryEvents are their end (i.e. b for query [a, b]), and for NumberEvents the key is their position in array.</p>\n  \n  <p>We also have a segment tree which answers the queries of kind: how many elements are at position range [x, y]. Initially the tree is empty.</p>\n  \n  <p>Then we process events in order:\n    + When we meet a NumberEvent:\n      1. If number has occurred before at position p, we remove p from segment tree.\n      2. We add position of number to the segment tree.\n    + When we meet a QueryEvent:\n      - Query the segment tree, and find the answer.</p>\n  \n  <p>The overall time complexity of algorithm is O( (n + q) log n + (n + q) log (n + q) )</p>\n</blockquote>\n\n<p>But I am not able to understand it properly. Can anybody help me in understanding the approach. I want to understand how are the positions inserted and deleted from the tree. That is, what does the segment tree hold, say for an interval.</p>\n', 'Tags': '<algorithms>', 'LastEditorUserId': '755', 'LastActivityDate': '2013-09-02T22:42:12.707', 'CommentCount': '0', 'AcceptedAnswerId': '14074', 'CreationDate': '2013-09-01T12:49:10.613', 'Id': '14068'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There are n men and n women. I am unable to understand how stable matching done by simply searching through all the matchings takes n! steps i.e., I cannot figure out why there are n! matchings possible.</p>\n', 'ViewCount': '50', 'Title': 'Why does stable matching without Gale-Shapely takes n! steps', 'LastActivityDate': '2013-09-04T04:40:13.137', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9965', 'Tags': '<algorithms>', 'CreationDate': '2013-09-03T19:03:12.750', 'Id': '14111'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have encountered a surprisingly challenging problem arranging a matrix-like (List of Lists) of values subject to the following constraints (or deciding it is not possible):</p>\n\n<p>A matrix of m randomly generated rows with up to n distinct values (no repeats within the row) arrange the matrix such that the following holds (if possible):</p>\n\n<p>1) The matrix must be "lower triangular"; the rows must be ordered in ascending lengths so the only "gaps" are in the top right corner</p>\n\n<p>2) If a value appears in more than one row it must be in the same column (i.e. rearranging the order of values in a row is allowed).</p>\n\n<p>Example 1 - which has a solution</p>\n\n<p>A B<br>\nC E D<br>\nC A B</p>\n\n<p>becomes (as one solution)</p>\n\n<p>A B<br>\nE D C<br>\nA B C  </p>\n\n<p>since A, B and C all appear in columns 1, 2 and 3, respectively.</p>\n\n<p>Example 2 - which has no solution</p>\n\n<p>A B C<br>\nA B D<br>\nC B D  </p>\n\n<p>has no solution since the constraints require the third row to have the C and D in the third column which is not possible.</p>\n\n<p>In my attempts to solve this naively (e.g. by sorting shortest rows to longest and then trying to order the rows from "most in common" to least and then simple reordering within the row) there are always scenarios that it thinks aren\'t solvable but are.  In other words backtracking/exhaustive search appears to be required, which is OK but I haven\'t yet struck onto a nice concise (ideally functional) algorithm for this.</p>\n', 'ViewCount': '84', 'Title': 'Ordering a list of lists subject to constraints', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-29T00:53:53.787', 'LastEditDate': '2013-09-04T04:50:23.237', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9967', 'Tags': '<algorithms><np-complete><recursion>', 'CreationDate': '2013-09-03T21:03:11.640', 'Id': '14113'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to check time complexity of two procedures for which I am not totally convinced that I got it right. Now the first procedure is this:</p>\n\n<pre><code>public static int c(int n) {\n    int i, j, s = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i*=2) {\n            for(j = n; j &gt; 0; j/=2) {\n                s++;\n            }\n            s+=c(n-2);\n        }\n    }\n}\n</code></pre>\n\n<p>Now I have set the following recurrence equation:\n$T(n)=\\log_2n*T(n-2)+\\Theta(\\log_2n)$</p>\n\n<p>Now the height of the recurrence tree is $n/2$ so the $T(n) = n/2 * (\\log_2n+\\log_2n)=\\Theta(n*\\log_2n)$</p>\n\n<p>The next procedure is this:</p>\n\n<pre><code>public static int d(int n, int m) {\n    int i, j, k, ss = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i++) {\n            for(j = i; j &gt; n; j+=2) {\n                ss+=(i+1)-2*j+m;\n            }\n        }\n        for(k=0; k&lt;8; k++) {\n            ss+=d(n/2, m/(k+1))\n        }\n    }\n    return ss;\n}\n</code></pre>\n\n<p>Again I have set this equation:\n$T(n, m)=8*T(n/2, m/(k+1))+\\Theta(n^2)$</p>\n\n<p>Now I think the $m$ parameter is not important because it is not used in for loop as a counter. Where $n/2$ gives us a recurrence tree of height $\\log_2n$. So we get this:\n$T(n, m) = 8 * \\log_2n*n^2=\\Theta(n^2)$</p>\n\n<p>I know that recurrence equations that I have set up are probably right, while the I am not sure about next steps.</p>\n', 'ViewCount': '109', 'Title': 'Finding asymptotically tight bounds $\\Theta$ of two procedures', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-04T09:50:41.463', 'LastEditDate': '2013-09-04T09:22:55.420', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14124', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9974', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-04T08:14:21.707', 'Id': '14121'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was going through a past paper question but I don't have any answers to know if I'm working out the problems correctly or not.</p>\n\n<p>I need to find the time complexity for:</p>\n\n<pre><code>i) repeat\n      n:=n div 2;\n   until n=1;\n\nii) for i:=1 to n do\n       begin\n          for j:=1 to n do\n             begin\n                for k:=1 to n do;\n             end;\n       end;\n\niii) repeat\n        for i:=1 to n do\n           begin\n              for j:=1 to n do;\n           end;\n     n:=n div 2\n     until n=1\n</code></pre>\n\n<p>In my opinion, the answer for (ii) is $O(\\log n)$ and the answer for (ii) is $O(n^3)$ but I'm not sure about my answers. Regarding question (iii) I have no idea how to come up with a solution.</p>\n", 'ViewCount': '83', 'Title': 'Measuring time complexity of algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T10:19:16.727', 'LastEditDate': '2013-09-09T10:19:16.727', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9979', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-04T12:44:31.107', 'Id': '14127'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the difference between approximation schemes and approximation algorithms?<br>\nWhy do we study approximation schemes?</p>\n', 'ViewCount': '121', 'Title': 'Difference between approximation scheme and approximation algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-06T16:07:46.940', 'LastEditDate': '2013-09-06T16:07:46.940', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '10000', 'Tags': '<algorithms><terminology><approximation>', 'CreationDate': '2013-09-06T02:45:46.770', 'Id': '14162'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m thinking of an application for diabetics, that, given previous values of blood glucose and insulin dosage, predicts the glucose level for the next few hours.</p>\n\n<p>I know a few things about neural networks and perceptrons, but not much. And there are probably whole other worlds of other machine learning methods. So I\'d like to ask about what way should I try to go.</p>\n\n<p>My problem is below:</p>\n\n<hr>\n\n<p>The app would probably have a (very) simplified model over what is the reality (what food, how much of it, did some sport lately?, what kind of insulin, etc.) - and there are also some "expert knowledge" rules that I know but that wouldn\'t get programmed into it (eg. if you have too low glucose level, your body compensates and makes it into a too high glucose level). I want to try how far can I get just with the glucose measurements (when and how much) and insulin dosage (when and how much).</p>\n\n<p>I guess the main question it would be nice if the program solved is <strong>"Given my glucose history, what glucose will I have in a few hours if I take X units of insulin?"</strong> (That could indirectly solve question "How many units of insulin should I take if I want to have a good glucose level in a few hours?" but that\'s more complicated matter.)</p>\n\n<p>The food is an important part of the question, but I think it\'s closely linked to the insulin - the two balance out: if you eat food, you take insulin. You don\'t take insulin without eating (it\'s more complicated than that, but I\'m trying to make it simple), so I guess it could work without putting in the data about food.</p>\n\n<p>Now the data (at least timewise) isn\'t uniformly distributed. Ideally it should be (regular measurements at the morning, noon, evening, etc.) but more often than not the measurements get skipped. So the training dataset would have to deal with having "holes" in it. I guess that\'s ruling out some methods.</p>\n\n<p>What do you recommend to try? (Again, this is not anything medically professional and "to be used in real life" - it\'s just a proof of concept for me and a toy project to try to do some machine learning.)</p>\n', 'ViewCount': '72', 'Title': 'What machine learning method for diabetes prediction SW?', 'LastActivityDate': '2013-09-07T21:06:16.847', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9122', 'Tags': '<algorithms><machine-learning><classification>', 'CreationDate': '2013-09-07T21:06:16.847', 'Id': '14201'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="http://rjlipton.wordpress.com/2009/06/04/the-junta-problem/">junta problem</a> is the following: we have a boolean function $f:\\{0,1\\}^n \\to \\{0,1\\}$ that actually happens to depend on only $k$ of its input variables.  Given the value of $f(x)$ for many random values of $x$, we want to identify the $k$ input variables that $f$ actually depends upon.  In other words, secretly $f(x_1,\\dots,x_n) = g(x_{i_1},\\dots,x_{i_k})$ for some indices $i_1,\\dots,i_k$; given many pairs $(x,f(x))$ where each $x$ is random, we want to find the $i_1,\\dots,i_k$.</p>\n\n<p>I am interested in a variant of the junta problem, where we are allowed membership queries.  In particular, at any point, we can choose a value $x$ and receive the value of $f(x)$.  The goal remains the same: We want to learn the junta, i.e., learn the indices $i_1,\\dots,i_k$.</p>\n\n<p>How many membership queries are needed, and what is the running time to learn the junta?</p>\n\n<p>There is a simple, straightforward algorithm that uses something like $O(n)$ membership queries (first find $x,y$ such that $f(x)\\ne f(y)$, then move from $x$ to $y$ by changing one bit of the input at a time, to identify $x\',y\'$ such that $f(x\')\\ne f(y\')$ and $x\',y\'$ differ in a single bit position).  But can it be done with many fewer membership queries?  For instance, can we learn the junta with, say, $O(\\lg n)$ membership queries?</p>\n', 'ViewCount': '94', 'Title': 'Learning juntas, with membership queries', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-12T15:23:10.917', 'LastEditDate': '2013-09-09T10:27:33.200', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><machine-learning><learning-theory>', 'CreationDate': '2013-09-08T03:20:12.577', 'Id': '14206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Assume that I have a file that consists of pairs of numbers separated by a space. These numbers are the labels for vertices in my graph. For example:</p>\n\n<pre><code>0 5\n0 7\n2 3\n4 5\n1 5\n.\n.\n.\n</code></pre>\n\n<p>I want to create a graph (adjacency list) by reading this file line-by-line. For each line, I will create an edge between the two vertices. Of course, if the vertex doesn\'t exist yet, then I will add it before creating the edge.</p>\n\n<p>I read <a href="https://vismor.com/documents/network_analysis/graph_algorithms/S4.php" rel="nofollow">here</a> of an algorithm that builds a graph with a time complexity of $O(|V| + |E|)$ where $V$ = set of vertices and $E$ = set of edges. That makes sense to me. However my algorithm doesn\'t insert the vertices in a loop first and then insert all of the edges in another loop second. My algorithm just adds the vertices as it\'s looping through the edges.</p>\n\n<p>My question is if my algorithm is $O(|E|)$? It seems like that can\'t be right, but I read <a href="http://stackoverflow.com/questions/12231499/do-if-statements-affect-in-the-time-complexity-analysis">here</a> that when calculating the time complexity you don\'t take into account if statements. That\'s exactly what my vertex creation would be -- an if statement that checks if the node exists in the middle of my looping through all the edges.</p>\n', 'ViewCount': '261', 'Title': 'Time Complexity for Creating a Graph from a File', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T12:29:22.547', 'LastEditDate': '2013-09-09T12:29:22.547', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14224', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10045', 'Tags': '<algorithms><graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-09T04:28:52.703', 'Id': '14223'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>Let $\\Sigma  = {1,2,3,4}$  . We say that symbol $\\sigma $ bonds with\n  symbol $\\tau$  if $\\sigma = \\tau$ of $\\sigma = \\tau +1$ for even\n  $\\sigma$ and $\\sigma = \\tau -1$ for odd $\\sigma$ . In other words  , 1\n  and 2 bond  , and 3 and 4 bond (in addition to every number bonding\n  with itself ) . </p>\n  \n  <p>The  Pattern bonding problem is defined as follows:</p>\n  \n  <p>INPUT :  Text $ T = t_{1}  , ..., t_{n}$  and pattern $ P = p_{1}  ,\n ..., p_{m}$ over alphabet  $\\Sigma  $ . </p>\n  \n  <p>OUTPUT :  All locations $i$ in the text that bond with the pattern , \n  i.e  , where $t_{i+j-1}$ bonds with $p_{j}$ $j=1,...,m$ .</p>\n</blockquote>\n\n<p>Can someone supply an algorithm for the pattern bonding problem (the faster the better   complexity can)  ?  , I thought about "Less then matching" .  </p>\n\n<p><strong>Edit: (Answer)</strong></p>\n\n<p>Map $ (1,2) \\rightarrow \\alpha , (3,4)\\rightarrow  \\beta$ then the text and the pattern is on $\\Sigma = {\\alpha , \\beta}$ and then we know to solve it in $O(n)$ using witness or KMP . </p>\n', 'ViewCount': '35', 'Title': 'Pattern bonding problem', 'LastEditorUserId': '4409', 'LastActivityDate': '2013-09-09T15:58:39.527', 'LastEditDate': '2013-09-09T15:58:39.527', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14235', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4409', 'Tags': '<algorithms><pattern-recognition>', 'CreationDate': '2013-09-09T14:23:55.640', 'Id': '14234'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1799', 'LastEditorDisplayName': 'user742', 'Title': 'What is the significance of negative weight edges in a graph?', 'LastEditDate': '2013-09-20T09:26:20.050', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9987', 'FavoriteCount': '2', 'Body': "<p>I was doing dynamic programming exercises and found the Floyd-Warshall algorithm. Apparently it finds all-pairs shortest paths for a graph which can have negative weight edges, but no negative cycles.</p>\n\n<p>So, I wonder what's the real world significance of negative weight edges? A plain English explanation would be helpful.</p>\n", 'Tags': '<algorithms><graph-theory>', 'LastActivityDate': '2013-09-20T09:26:20.050', 'CommentCount': '5', 'AcceptedAnswerId': '14262', 'CreationDate': '2013-09-10T15:18:19.163', 'Id': '14248'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose you are given an input set $S$ of $n$ numbers, and a black box that if given any sequence of real numbers and an integer $k$ instantly and correctly answers whether there is a subset of input sequence whose sum is exactly $k$. I want to show how to use the black box $O(n)$ times to find a subset of S that adds up to $k$.</p>\n\n<p>This is what I've done: the first time we enter our set $S$. If it returns yes we can continue, otherwise it isn't possible to form the sequence which sums up to $k$. The next step is to test our set without the first element. If the black box returns yes we can delete it from our set otherwise we know that it is needed. We do this for each element and our $S$ shrinks to a set which sums up to $k$. Can I use induction to prove this?</p>\n", 'ViewCount': '282', 'Title': 'Finding the subset of $S$ that sums up to $k$ using a black box in $O(n)$ time', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-14T22:38:12.713', 'LastEditDate': '2013-09-11T19:01:55.813', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><subsequences>', 'CreationDate': '2013-09-11T18:05:11.860', 'Id': '14270'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>One of my courses introduced the following question:</p>\n\n<blockquote>\n  <p>Given the recurrence relation for mergesort:</p>\n  \n  <p>$T(n) = 2T(n/2) + n$</p>\n  \n  <p>How would the following parameter passing strategies influence the relation and running time of the algorithm:</p>\n  \n  <ol>\n  <li><p>A pointer to the array is passed to the sorting function,</p></li>\n  <li><p>The entire array is copied to a separate location before applying the sorting function,</p></li>\n  <li><p>A subsection of the array is copied to a separate location before applying the sorting function on that subsection.</p></li>\n  </ol>\n</blockquote>\n\n<p>For 1., since this is how mergesort is used most of the time, we can solve it easily using the master theorem.</p>\n\n<p>$f(n) = \\Theta(n^{\\log_b a}) = \\Theta(n) \\implies T(n) = \\Theta(n \\log n)$</p>\n\n<p>For 2. however, I am a bit baffled. Although we do an additional work of $O(n)$, we are only doing so <em>once</em> at the beginning of the sort. Hence, doing one additional instance of $O(n)$ work should not influence the order of the running time (because it already <em>is</em> of a larger order). Hence, for both 2. and 3. the running time would remain at $T(n) = \\Theta(n \\log n)$. </p>\n\n<p>Is this reasoning valid? Something tells me that the $O(n)$ copying should have more of an impact, but I can't seem to give myself a good enough reason that it should be responsible to slow it down enough so that it would worse (i.e. $O(n^2)$). </p>\n\n<p>Any thoughts or hints would be quite appreciated!</p>\n", 'ViewCount': '134', 'Title': 'Do different variants of Mergesort have different runtime?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-16T07:58:23.627', 'LastEditDate': '2013-09-16T06:58:55.213', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14349', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10093', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation><sorting>', 'CreationDate': '2013-09-12T03:25:30.877', 'Id': '14275'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '222', 'Title': 'Which of the common sorting algorithms can be parallelized?', 'LastEditDate': '2013-09-16T07:54:36.567', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10127', 'FavoriteCount': '2', 'Body': "<p>I want to know that whether which of the following algorithm can be parallelized?</p>\n\n<p>Bubble Sort,\nInsertion Sort,\nSelection Sort,\nShell Sort,\nQuick Sort,\nMerge Sort,\nRadix Sort.</p>\n\n<p>Those which can't be, please explain me briefly that how? Or please try to tell me in simple words that what is parallelism in sorting algorithms.</p>\n", 'ClosedDate': '2013-09-16T07:57:07.533', 'Tags': '<algorithms><sorting><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:54:36.567', 'CommentCount': '11', 'CreationDate': '2013-09-13T18:08:10.710', 'Id': '14294'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In the last section of chapter 3 (page 54) in <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em> by Mitzenmacher and Upfal, a randomized algorithm is discussed for finding the median of a set $S$ of distinct elements in $O(n)$ time. As a substep the algorithm requires sorting a multiset $R$ of values sampled from $S$, where $|R| = \\lceil n^{3/4} \\rceil$. I'm sure this is a fairly simple, and straight forward question compared to the others on this site, but how is sorting a multiset of this size bounded by $O(n)$? </p>\n\n<p>I assume the algorithm is using a typical comparison-based, deterministic sorting algorithm that runs in $O(n\\log$ $n)$. So, If I have the expression $O(n^{3/4}\\log(n^{3/4}))$, isn't this just equivalent to $O(\\frac{3}{4}n^{3/4}\\log(n)) = O(n^{3/4}\\log(n))$? How does the reduction down to $O(n)$ proceed?</p>\n", 'ViewCount': '107', 'Title': 'Randomized Median Element Algorithm in Mitzenmacher and Upfal: O(n) sorting step?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:46:22.310', 'LastEditDate': '2013-09-16T07:46:22.310', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10128', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><randomized-algorithms>', 'CreationDate': '2013-09-13T20:07:11.977', 'Id': '14296'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '82', 'Title': 'Time Complexity of Algorithm', 'LastEditDate': '2013-09-16T07:43:21.313', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4658', 'FavoriteCount': '0', 'Body': "<p>I need help with finding out the time complexity of the following algorithm:</p>\n\n<pre><code>procedure VeryOdd(integer n):\nfor i from 1 to n do\n  if i is odd then\n    for j from i to n do\n      x = x + 1\n    for j from 1 to i do\n      y = y + 1\n</code></pre>\n\n<p>This is my attempt:</p>\n\n<p>$$ Loop1 = \\Theta(n)$$\n$$ Loop2 = \\Theta(n)$$\n$$ Loop2 = O(n)$$</p>\n\n<p>And we also know that loop2 and loop3 will get executed every second time of the execution of the outer loop. So we know that:</p>\n\n<p>$$T(n) = \\Theta(n) * 1/2(\\Theta(n) + O(n)) = \\Theta(n^2)$$</p>\n\n<p>Now to the thing I'm not so sure about, nameley, is Loop3 really $$O(N)$$ and\nif yes, then is $$\\Theta(n) + O(n) = \\Theta(n)$$</p>\n\n<p>Thanks in advance</p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:43:21.313', 'CommentCount': '2', 'AcceptedAnswerId': '14303', 'CreationDate': '2013-09-13T22:40:48.113', 'Id': '14298'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am new to Advanced Algorithms and I have studied various samples on Google and StackExchange. What I understand is:</p>\n\n<ol>\n<li><p>We use $O(\\log n)$ complexity when  there is division of any $n$ number on each recursion (especially in divide and conquer).</p></li>\n<li><p>I know that for binary search, we have time complexity $O(n \\log n)$, I understood $\\log n$ is because each time it halves the full $n$ size number list in a recursive manner until it finds the required element. But why is it multiplied with $n$ even we just traverse half of the $n$ size element for each execution so why we multiply $\\log n$ with $n$?</p></li>\n<li><p>Please give me any example explaining the complexity $O(n^2 \\log n)$. I hope this will help me in understanding much better the above two questions.</p></li>\n</ol>\n', 'ViewCount': '218', 'Title': 'Confusion regarding several time complexities including the logarithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:56:44.833', 'LastEditDate': '2013-09-16T08:56:44.833', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10129', 'Tags': '<algorithms><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2013-09-13T23:31:05.913', 'Id': '14299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am working on multiple patterns search matching algorithms and I found that two algorithms are the strongest candidates, namely <a href="http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm" rel="nofollow">Aho-Corasick</a> and <a href="http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm" rel="nofollow">Rabin-Karp</a> in terms of running time. However, there are few things still foggy. I could not find any comprehensive comparison between the two algorithms. Besides, what I need to know is which one of them is more suitable for parallel computing and multiple patterns search. Which one require less hardware resources.  </p>\n\n<p>For AC algorithm searching phase time complexity is O(n+m), while it is O(nm) for RK. However, running time for RK is O(n+m) which make it similar to AC. My conclusion is RK practically better as it does not need memory as AC. I need a confirmation of that.  </p>\n', 'ViewCount': '234', 'Title': 'A comparison between Aho-Corasick algorithm and Rabin-Karp algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-09T00:14:12.477', 'LastEditDate': '2013-09-16T07:47:49.863', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9855', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><strings>', 'CreationDate': '2013-09-14T16:34:08.873', 'Id': '14309'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Note: I moved this question from <a href="http://stackoverflow.com/questions/18804045/find-all-intervals-s-e-where-s-e-in-n-0-and-s-e-that-overlap-with-another-gi">stackoverflow.com</a></p>\n\n<p>I have an algorithmic problem where I would like to see if it can be solved in better than $O(n)$:</p>\n\n<p>I have given a table $T$ of $n$ elements where each element is a tuple $(s_i, e_i)$ with $s_i, e_i \\in \\mathbb{N}$ and $s_i &lt; e_i$, i.e. each tuple is some kind of an interval. I have to find all intervals that overlap with a given interval $[t_0, t_1]$ with $t_0, t_1 \\in \\mathbb{N}$ and $t_0 &lt; t_1$. Further, I have available two sorted lists $S$ and $E$, containing the $s$ values, or $e$ values respectively, together with the index $i$ pointing to the respective entry in $T$. The lists are sorted by $s$ values, or $e$ values respectively. (Let\'s assume both, $s$ and $e$ values, are unique.)</p>\n\n<p><strong>Problem:</strong></p>\n\n<p>We have to find each interval/tuple $(s_i, e_i) \\in T$ where $s_i \\leqslant t_1$ and $e_i \\geqslant t_0$.</p>\n\n<p><strong>My thoughts so far:</strong></p>\n\n<p>We can exclude some elements by either applying one of the interval boundaries, i.e. searching $t_1$ in $S$ or $t_0$ in $E$. This gives us a list $L$ of remaining elements:\n$$\n    L \\leftarrow \\{e \\in E \\mid e \\geqslant t_0\\} \\text{ or } L \\leftarrow \\{s \\in S \\mid s \\leqslant t_1\\}\n$$\nHowever, there is no lower bound on the number of elements in $L$, no matter which search we perform. Further, we have to check every element in $L$ if $s \\leqslant t_1$, or $e \\geqslant t_0$ respectively depending on which search we performed before.</p>\n\n<p>The complexity for this solution is $O(n)$.</p>\n\n<p>However, let\'s say that $k$ is the maximum number of elements overlapping with interval $[t_0, t_1]$. <strike>If we assume $k \\ll n$, then the complexity is $O(n/2)$ since we can exclude at least $n/2$ elements by choosing the appropriate search for $L$. Still $O(n/2)$ is in $O(n)$.</strike></p>\n\n<p>Can you think of a better approach to solve this problem?</p>\n\n<p><strong>For record:</strong> </p>\n\n<p>The complexity for finding all intervals overlapping with a certain given interval using an interval tree is $O(\\log n + k)$ where $k$ is the number of overlapping intervals. However, in my practical case I am using a MySQL database which provides index trees for each value, i.e. $s$ and $e$, separately. This way I can not find overlapping intervals in less than $O(n)$. I would need to create an interval tree which is a search tree that stores both interval boundaries, i.e. $s$ and $e$, in a single data structure. The complexity for constructing the interval tree is $O(n \\log n)$. [http://www.dgp.utoronto.ca/people/JamesStewart/378notes/22intervals/]</p>\n', 'ViewCount': '1199', 'Title': 'Find all intervals that overlap with a given interval', 'LastEditorUserId': '10138', 'LastActivityDate': '2013-09-16T12:49:46.127', 'LastEditDate': '2013-09-16T12:49:46.127', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14320', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10138', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-09-14T16:52:27.203', 'Id': '14311'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been studying about <a href="http://en.wikipedia.org/wiki/Activity_selection_problem" rel="nofollow">activity-selection-problem</a> and the solution of greedy choice I came across is to select the activity that finishes in the earliest among the present activities.</p>\n\n<p>But surely there are other greedy choices to solve the problem. The one I have been able to figure out is to select the activity that starts last.</p>\n\n<p>My question is: are there any other greedy choices that can lead to solve activity selection problem? Any formal proof is also appreciated.</p>\n', 'ViewCount': '118', 'Title': 'Other greedy choices to solve activity selection problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:59:27.903', 'LastEditDate': '2013-09-16T07:59:27.903', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6699', 'Tags': '<algorithms><optimization><greedy-algorithms>', 'CreationDate': '2013-09-15T03:55:05.647', 'Id': '14316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am trying to solve the Problem <a href="http://www.spoj.com/problems/CTRICK/" rel="nofollow">CTRICK</a> on SPOJ. I have an $O(n^2)$ solution that is giving \u201cTime Limit Exceeded\u201d. After browsing for some hints, I found that it can be solved using Interval Trees. But I have no idea, how to think it that way. Here is the problem:</p>\n\n<p>The magician shuffles a small pack of cards, holds it face down and performs the\nfollowing procedure:</p>\n\n<blockquote>\n  <ol>\n  <li><p>The top card is moved to the bottom of the pack. The new top card is dealt\n  face up onto the table. It is the Ace of Spades.</p></li>\n  <li><p>Two cards are moved one at a time from the top to the bottom. The next card is\n  dealt face up onto the table. It is the Two of Spades.</p></li>\n  <li><p>Three cards are moved one at a time\u2026</p></li>\n  <li><p>This goes on until the $n$th and last card turns out to be the $n$ of Spades.</p></li>\n  </ol>\n  \n  <p>This impressive trick works if the magician knows how to arrange the cards\n  beforehand (and knows how to give a false shuffle). Your program has to determine\n  the initial order of the cards for a given number of cards, $1 \\le n \\le 20000$.</p>\n  \n  <p>Input 1:</p>\n\n<pre><code>4\n</code></pre>\n  \n  <p>Output 1:</p>\n\n<pre><code>2 1 4 3\n</code></pre>\n  \n  <p>Input 2:</p>\n\n<pre><code>5\n</code></pre>\n  \n  <p>Output 2:</p>\n\n<pre><code>3 1 4 5 2\n</code></pre>\n</blockquote>\n\n<p>Please suggest me some ideas.</p>\n', 'ViewCount': '208', 'Title': 'Approach to solve SPOJ Cardtrick', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-15T18:48:10.033', 'LastEditDate': '2013-09-15T18:48:10.033', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8819', 'Tags': '<algorithms>', 'CreationDate': '2013-09-15T16:33:03.183', 'FavoriteCount': '1', 'Id': '14334'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The energy function in a Hopfield network to determine whether it has converged seems to be the major sink of computational time and makes the Hopfield network run very slowly. Is there a fast substitute for it? </p>\n', 'ViewCount': '20', 'Title': 'Fast energy function substitute for Hopfield network?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:35:57.190', 'LastEditDate': '2013-09-16T08:35:57.190', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10158', 'Tags': '<algorithms><artificial-intelligence><efficiency><neural-networks><heuristics>', 'CreationDate': '2013-09-15T18:54:05.840', 'Id': '14335'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to modify the Floyd-Warshall algorithm to find all-pairs minimax paths in a graph. (That is, the shortest length paths such that the maximum edge weight along a path is minimized.)</p>\n\n<p>Floyd-Warshall algorithm contains the following loop to enhance the distance (<code>ds</code>) and the next vertex (<code>ns</code>) matrices at each iteration.</p>\n\n<pre><code>for (int k = 0; k &lt; n; k++)\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            if (ds[i][k] != inf &amp;&amp; ds[k][j] != inf) {\n                final int d = ds[i][k] + ds[k][j];\n                if (d &lt; ds[i][j]) {\n                    ds[i][j] = d;\n                    ns[i][j] = k;\n                }\n            }\n</code></pre>\n\n<p>I replaced <code>ds</code> with two new matrices: <code>ws</code> (weights) and <code>ls</code> (lengths). Further, updated the iteration step as follows:</p>\n\n<pre><code>for (int k = 0; k &lt; n; k++)\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            if (ws[i][k] != inf &amp;&amp; ws[k][j] != inf) {\n                final int w = Math.max(ws[i][k], ws[k][j]);\n                final int l = ls[i][k] + ls[k][j];\n                if (w &lt; ws[i][j] || (w == ws[i][j] &amp;&amp; l &lt; ls[i][j])) {\n                    ws[i][j] = w;\n                    ls[i][j] = l;\n                    ns[i][j] = k;\n                }\n            }\n</code></pre>\n\n<p>However, the modified algorithm finds paths with loops, that is, paths such as 1-<strong>3-2-3</strong>-4. While the maximum edge weight of the paths 1-3-2-3-4 and 1-3-4 are identical, the latter has a shorter path length and supposed to be returned by the enhanced Floyd-Warshall. Any ideas?</p>\n\n<p>A working Java version of both algorithms and a test case which produces a path with loop can be found <a href="https://gist.github.com/vy/6580214" rel="nofollow">here</a>.</p>\n\n<p><strong>Edit:</strong> Since no solutions were presented yet, I implemented my own shortest minimax path algorithm using incremental link removal method. Java sources to the solutions can be accessed from the <a href="https://gist.github.com/vy/6580214" rel="nofollow">above link</a>. </p>\n', 'ViewCount': '683', 'LastEditorDisplayName': 'user742', 'Title': 'Shortest Minimax Path via Floyd-Warshall', 'LastActivityDate': '2013-10-20T10:59:54.067', 'LastEditDate': '2013-09-20T09:26:04.407', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10167', 'Tags': '<algorithms><graph-theory><optimization><shortest-path>', 'CreationDate': '2013-09-16T12:50:56.260', 'Id': '14353'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m looking for an algorithm that provides a <a href="http://en.wikipedia.org/wiki/Graph_canonization" rel="nofollow">canonical string</a> for a given colored graph. Ie. an algorithm that returns a string for a graph, such that two graphs get the same string if and only if they are isomorphic. </p>\n\n<p>In particular, I\'m looking for a simple algorithm that is easy to implement with a reasonable performance on most graphs (worst case super-polynomial, of course). I\'m expecting small graphs, so performance doesn\'t have to be stellar, just good enough.</p>\n\n<p>Unfortunately, most things I\'ve found are highly complex and more interested in expressing deep mathematical connections than simply describing the algorithm. I\'m afraid I don\'t have the time to dive that deep. Can anyone give me a shortcut?</p>\n\n<p>I\'m hoping for something like the Floyd-Warshall algorithm. Not optimal, but good enough, and easy to implement.</p>\n', 'ViewCount': '174', 'LastEditorDisplayName': 'user742', 'Title': 'Simple graph canonization algorithm', 'LastActivityDate': '2013-10-20T09:59:54.030', 'LastEditDate': '2013-09-20T09:25:40.343', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1428', 'Tags': '<algorithms><graph-theory><combinatorics>', 'CreationDate': '2013-09-16T14:11:06.023', 'Id': '14354'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a $n\\times n$ matrix called $M$, and two integers $k_\\min$ and $k_\\max$. \nEach row and each column of M is sorted in the increasing order. </p>\n\n<p>I would like to know if there is way I can count the number of its elements which are inside $[k_\\min, k_\\max]$, using a $O(n)$ algorithm.</p>\n', 'ViewCount': '95', 'Title': 'Count elements of a sorted matrix that fall into a given interval', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-17T17:14:17.943', 'LastEditDate': '2013-09-17T12:07:52.697', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'user10182', 'PostTypeId': '1', 'Tags': '<algorithms><search-algorithms><matrices><counting>', 'CreationDate': '2013-09-17T09:55:03.567', 'FavoriteCount': '1', 'Id': '14376'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $G = (V, E)$ be a connected graph and let $M\\subseteq V$. We say that a vertex $v$ is <em>marked</em> if $v\\in M$.  The problem is to find a simple path in $G$ that visits the maximum possible number of marked vertices. The associated decision problem is: is there a simple path that visits every $v\\in M$?  </p>\n\n<p>The problem is obviously more general than the problem of finding a Hamiltonian path in an arbitrary graph, so it is NP-hard.  </p>\n\n<p>I see no obvious strategy; one can't simply disregard the unmarked vertices, since they (and their incident edges) may be part of the optimal path.  Indeed, omitting them may disconnect the graph completely.</p>\n\n<p>My questions:</p>\n\n<ol>\n<li>Does this problem have a well-known name?</li>\n<li>Are there any good approximation algorithms, heuristics, or simple reductions to problems I might know more about?</li>\n<li>Where can I find this problem discussed in the literature?</li>\n</ol>\n", 'ViewCount': '201', 'LastEditorDisplayName': 'user742', 'Title': 'Find a simple path visiting all marked vertices', 'LastActivityDate': '2013-09-20T09:24:48.873', 'LastEditDate': '2013-09-20T09:24:48.873', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1786', 'Tags': '<algorithms><graph-theory><reference-request><np-hard><hamiltonian-path>', 'CreationDate': '2013-09-17T21:18:50.210', 'Id': '14390'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have this problem:</p>\n\n<blockquote>\n  <p>Consider the problem of calculating the integral</p>\n  \n  <p>$$y_n =\\int_{0}^{1} \\dfrac{x^n}{x+10} \\mathrm{d}x $$\n  for a positive integer $n$.</p>\n  \n  <p>Observe that $$y_n + 10y_{n-1} = \\int_{0}^{1} \\dfrac{x^n +10x^{n-1}}{x+10} \\mathrm{d}x  = \\int_{0}^{1} x^{n-1}\\mathrm{d}x = \\dfrac{1}{n}$$</p>\n  \n  <p>and that using this relationship in a forward recursion leads to a numerically unstable procedure.</p>\n  \n  <ol>\n  <li><p>Derive a formula for approximately computing these integrals based on evaluating $y_{n-1}$ given $y_n$.</p></li>\n  <li><p>Show that for any given value $\\epsilon &gt; 0$ and positive integer $n_0$, there exists an integer $n_1 \\geq n_0$ such that taking $y_{n_1} = 0$ as a starting value will produce integral evaluations $y_n$ with an absolute error smaller than $\\epsilon$ for all  $0 &lt; n \\leqslant n_0$. </p></li>\n  <li><p>Explain why your algorithm is stable.</p></li>\n  </ol>\n</blockquote>\n\n<p>Here is what I have so far,</p>\n\n<p>for part 1.</p>\n\n<p>$$y_{n-1} = \\dfrac{1}{10} \\left(\\dfrac{1}{n} - y_n\\right)$$</p>\n\n<p>and for part 3.</p>\n\n<p>The algorithm is stable because the magnitude of roundoff errors gets divided by 10 each time the recursion is applied.</p>\n\n<p>I really don't know how to start on the proof for part 2., any hints and help would be appreciated.</p>\n", 'ViewCount': '59', 'Title': 'Error accumulation in a numerical integration', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:12:24.380', 'LastEditDate': '2013-09-18T09:12:24.380', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10189', 'Tags': '<algorithms><numerical-analysis><error-estimation><numerical-algorithms>', 'CreationDate': '2013-09-18T00:43:25.283', 'Id': '14394'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '114', 'LastEditorDisplayName': 'user742', 'Title': 'Approximation algorithm for Feedback Arc Set', 'LastEditDate': '2013-11-17T16:03:32.293', 'AnswerCount': '1', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '10198', 'FavoriteCount': '1', 'Body': '<p>Given a directed graph $G = (V,A)$, a feedback arc set is a set of arcs whose removal leaves an acyclic graph.  The problem is to find the minimum cardinality such set.</p>\n\n<p>I want to find out about is there some approximation algorithm around this problem.</p>\n', 'Tags': '<algorithms><graph-theory><approximation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-17T16:03:32.293', 'CommentCount': '6', 'AcceptedAnswerId': '14432', 'CreationDate': '2013-09-18T09:21:04.343', 'Id': '14410'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the <a href="http://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* search algorithm</a>, we use a priority queue with heuristic function to find optimum result with minimum cost. But, how do we get the path after reaching goal?</p>\n', 'ViewCount': '33', 'Title': 'Recover the path to a goal state in A* search algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-20T15:26:46.447', 'LastEditDate': '2013-09-20T15:26:46.447', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14434', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10193', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-09-19T04:15:02.077', 'Id': '14433'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is well known now that <a href="http://en.wikipedia.org/wiki/Grover%27s_algorithm" rel="nofollow">Grover\'s quantum algorithm</a> can <strong>SORT</strong> a database of $N$ entries in $O(\\sqrt{N})$ time. How can an algorithm work without reading through the list of entries which needs $O(N)$ operation. How does Grover\'s algorithm avoid reading the list? Is there a special representation that can be used classically as well? I understand any such representation will not help the classical case. However I am curious to understand how Grover avoids reading the list?</p>\n', 'ViewCount': '146', 'Title': "How does Grover's Quantum Sorting avoid reading the list?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-22T06:29:18.927', 'LastEditDate': '2013-09-20T16:27:01.477', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '14445', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9753', 'Tags': '<algorithms><sorting><quantum-computing>', 'CreationDate': '2013-09-19T14:55:35.013', 'Id': '14442'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '2654', 'Title': 'Factorial algorithm more efficient than naive multiplication', 'LastEditDate': '2013-09-20T08:18:53.507', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7307', 'FavoriteCount': '2', 'Body': "<p>I know how to code for factorials using both iterative and recursive (e.g. <code>n * factorial(n-1)</code> for e.g.). I read in a textbook (without been given any further explanations) that there is an even more efficient way of coding for factorials by dividing them in half recursively. </p>\n\n<p>I understand why that may be the case. However I wanted to try coding it on my own, and I don't think I know where to start though. A friend suggested I write base cases first. and I was thinking of using arrays so that I can keep track of the numbers... but I really can't see any way out to designing such a code.</p>\n\n<p>What kind of techniques should I be researching?</p>\n", 'Tags': '<algorithms><efficiency><arithmetic>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-16T08:26:18.543', 'CommentCount': '0', 'AcceptedAnswerId': '14476', 'CreationDate': '2013-09-20T02:00:56.260', 'Id': '14456'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following pseudo code for mapreduce to find the frequency of words in a collection of documents:</p>\n\n<pre><code>map(String key, String value)\n// key: document name\n// value: document contents\n for each word w in value\n   EmitIntermediate(w, "1")\n\nreduce(String key, Iterator values):\n// key: word\n// values: a list of counts\n  for each v in values:\n    result += ParseInt(v);\n    Emit(AsString(result));\n</code></pre>\n\n<p>So the map step gets each word as a key and output its frequency in a document. Does the reduce step sum the counts of each word? </p>\n', 'ViewCount': '515', 'ClosedDate': '2013-11-11T13:53:15.630', 'Title': 'MapReduce Pseudocode', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-20T15:59:54.100', 'LastEditDate': '2013-09-20T15:14:13.803', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10246', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-09-20T14:25:59.083', 'Id': '14470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '199', 'Title': 'Fundamental algorithms in formal language-automata theory', 'LastEditDate': '2013-09-20T18:45:15.127', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10249', 'Body': "<p>I'm willing to take a course in formal languages and automata theory , where we will explore side by side a functional programming language to implement the different algorithms we will encounter ,despite i am new to the language , i am assuming i've learned what's nessecary to move to the theoretical part , so i'm wondering what are the must-knows of FL &amp; AT to a programmmer , that is what are the most fundamentals algorithms one necesserily have to know while studying formal languages and automata theory ?</p>\n", 'ClosedDate': '2013-10-01T07:02:02.177', 'Tags': '<algorithms><formal-languages><automata>', 'LastEditorUserId': '10249', 'LastActivityDate': '2013-09-22T04:53:51.763', 'CommentCount': '0', 'AcceptedAnswerId': '14489', 'CreationDate': '2013-09-20T18:38:20.970', 'Id': '14480'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know how to use the FFT for multiplying two equations in $O(n\\,log\\,n)$ time, but is there a way to use FFT to compute the expanded equation before simplifying?</p>\n\n<p>For example, if you are multiplying $A(x) = 1 + 2x + 6x^2$ and $B(x) = 4 + 5x + 3x^2$ such that you get $C(x) = A(x) \\cdot B(x) = 4 + 5x + 3x^2 + 8x + 10x^2 + 6x^3 + 24x^2 + 30x^3 + 18x^4$ without going directly to the simplified answer?</p>\n\n<p>Furthermore, is it possible to use FFT to do this expanded form multiplication in $O(n\\,log\\,n)$ time? If so, can you show me how to apply FFT to this scenario?</p>\n', 'ViewCount': '76', 'Title': 'FFT for expanded form of equation multiplication', 'LastEditorUserId': '10052', 'LastActivityDate': '2014-03-31T23:46:56.920', 'LastEditDate': '2013-09-22T04:25:36.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14510', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10052', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><fourier-transform>', 'CreationDate': '2013-09-22T04:17:41.320', 'Id': '14509'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>C++11 has a convenient Bernoulli RNG, illustrated at \n<a href="http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution" rel="nofollow">http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution</a> .\nHowever, distilling an entire random integer into a single random bit seems inefficient when the expectation parameter $p$ is rational with a small or power-of-two denominator.\nIs there a reasonably fast way to generate 32 random Bernoulli bits at once in such cases? My application uses long streams of bits, so I can keep track of statistics if needed (but this would consume runtime).</p>\n', 'ViewCount': '78', 'Title': "Isn't std::bernoulli_distribution inefficient? Designing a bit-parallel Bernoulli generator", 'LastEditorUserId': '5189', 'LastActivityDate': '2013-09-25T04:26:33.097', 'LastEditDate': '2013-09-25T04:26:33.097', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5189', 'Tags': '<randomized-algorithms><integers><randomness><binary-arithmetic>', 'CreationDate': '2013-09-22T21:10:38.360', 'Id': '14525'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '237', 'Title': 'Minimal size of contracting a DAG into a new DAG', 'LastEditDate': '2013-11-03T18:16:54.507', 'AnswerCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '10273', 'FavoriteCount': '3', 'Body': "<p>We have a DAG. We have a function on the nodes $F\\colon V\\to \\mathbb N$ (loosely speaking, we number the nodes). We would like to create a new directed graph with these rules: </p>\n\n<ol>\n<li>Only nodes with the same number can be contracted into the same new node. $F(x) \\neq F(y) \\Rightarrow x' \\neq y'$. (However, $x' \\neq y'\\nRightarrow F(x) \\neq F(y)$.)</li>\n<li>We add all the old edges between new nodes: $(x,y) \\in E \\land x' \\neq y' \\iff (x',y')\\in E'$.</li>\n<li>This new graph is still a DAG.</li>\n</ol>\n\n<p>What is the minimal $|V'|$? What is an algorithm creating a minimal new graph?</p>\n", 'Tags': '<algorithms><graphs><np-complete><reductions>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-12-10T04:56:53.490', 'CommentCount': '9', 'AcceptedAnswerId': '16277', 'CreationDate': '2013-09-23T08:24:13.643', 'Id': '14552'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Im studying for the computer science subject GRE test, as an exercise i need to implement the followin algorithm in java, any idea on how to aproach it?.</p>\n\n<p>Given a set $X$ and $z$ not in $X$, indicate between which would be the immediate positions $z$ in $X$</p>\n', 'ViewCount': '18', 'Title': 'Immediate positions algorithm?', 'LastActivityDate': '2013-09-25T04:13:11.850', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2013-09-25T03:45:39.477', 'Id': '14588'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am new to StackExchange, and I already made the mistake of posting a new question as a response to a previous question. Here, I rewrote my question more clearly and separately.</p>\n\n<p>I am trying to store an existing 2D triangulation in a DCEL data structure, and I have all of the vertices and edges.\nI was able to store all of the information correctly except the half_edge representative for each triangle. Here is the algorithm I used:\n(taken from <a href="http://cs.stackexchange.com/questions/2450/constructing-of-double-connected-edge-list-dcel">Constructing of Double Connected Edge List (DCEL)</a>)</p>\n\n<blockquote>\n  <p>Algorithm:</p>\n  \n  <p>For each endpoint, create a vertex. For each input segment, create two\n  half-edges and assign their tail vertices and twins. For each\n  endpoint, sort the half-edges whose tail vertex is that endpoint in\n  clockwise order. For every pair of half-edges e1, e2 in clockwise\n  order, assign e1->twin->next = e2 and e2->prev = e1->twin. Pick one of\n  the half-edges and assign it as the representative for the endpoint.\n  (Degenerate case: if there\'s only one half-edge e in the sorted list,\n  set e->twin->next = e and e->prev = e->twin.) The next pointers are a\n  permutation on half-edges. For every cycle, allocate and assign a face\n  structure.</p>\n</blockquote>\n\n<p>The last sentence seems to be easier said than done. How can I ensure that every triangle will have a representative, and that a representative will be assigned only once for each triangle? Furthermore, which cycle is it referring to? If you have any other ideas, please share.</p>\n\n<p>Thank you very much for your help. I\'ve been struggling with this for a while.</p>\n\n<p>PS- I am working in C++. Also, I am using the same structure as provided in the link above.</p>\n', 'ViewCount': '118', 'Title': 'Problem with storing an existing triangulation in a DCEL', 'LastActivityDate': '2013-11-20T14:54:13.830', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18167', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10269', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2013-09-25T07:24:18.060', 'Id': '14591'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I guess at the heart of this is that I don\'t really understand hash functions.\n<img src="http://upload.wikimedia.org/wikipedia/commons/5/58/Hash_table_4_1_1_0_0_1_0_LL.svg" alt=""></p>\n\n<p>One article says any function mapping objects to an object of fixed size:</p>\n\n<blockquote>\n  <p>A hash function usually means a function that compresses, meaning the\n  output is shorter than the input. Often, such a function takes an\n  input of arbitrary or almost arbitrary length to one whose length is a\n  \ufb01xed number, like 160 bits.</p>\n</blockquote>\n\n<p>Why not just use a random number generator to generate the hash keys?  Wikipedia  suggests <a href="http://en.wikipedia.org/wiki/SHA-1" rel="nofollow">SHA1</a> which maps to $2^{160}\\approx 10^{48}$ possible outputs has never experienced a "collision".</p>\n\n<p>I was trying to understand why the hash is necessary in a <a href="http://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/" rel="nofollow">probabilistic counting algorithm</a>:</p>\n\n<pre><code>class LinearCounter():\n    def __init__(self, m, h):\n        self.array =  [False for x in range(m)]\n        self.hash = h\n\n    def add(value):\n        self.array(self.hash[ value ]) = True\n</code></pre>\n\n<p>Is the hash necessary? Why can\'t we use a random number generator right away?</p>\n\n<pre><code>def add(value):\n    self.array(int(m*random.random()))\n</code></pre>\n', 'ViewCount': '218', 'Title': 'why not just use a random number generator as a hash function?', 'LastActivityDate': '2013-09-25T19:24:41.857', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<hash><probabilistic-algorithms>', 'CreationDate': '2013-09-25T17:24:39.487', 'Id': '14601'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can anybody give me a way that i can use to calculate Complexity Time problems, i mean a way that will work on any complex code whatever it was.</p>\n\n<p>I can solve basic problems, but whenever the problem gets a little complex i freeze which means that my way of thinking and solving is wrong.</p>\n\n<p>for example like this:</p>\n\n<p><img src="http://i.stack.imgur.com/7xKdY.png" alt="enter image description here"></p>\n', 'ViewCount': '51', 'ClosedDate': '2013-09-27T19:39:28.710', 'Title': 'How to calculate Complexity Time O()?', 'LastActivityDate': '2013-09-27T13:43:08.540', 'AnswerCount': '0', 'CommentCount': '8', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10361', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-09-27T13:43:08.540', 'Id': '14642'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '234', 'Title': 'Complexity of keeping track of $K$ smallest integers in a stream', 'LastEditDate': '2014-01-30T21:46:49.597', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10384', 'FavoriteCount': '2', 'Body': '<p>I need to analyze the time complexity of an online algorithm to keep track of minimum $K$ numbers from a stream of $R$ numbers. The algorithm is </p>\n\n<ul>\n<li>Suppose the $i$th number in the stream is $S_i$.</li>\n<li>Keep a max heap of size $K$.</li>\n<li>If the heap contains fewer than $K$ elements, add $S_i$ to the heap.</li>\n<li>Otherwise: if $S_i$ is smaller than the maximum element in the heap (i.e., the root of the heap), replace the root of the heap with $S_i$ and apply Max-Heapify to the heap; if $S_i$ is greater than or equal to the max element, do nothing.</li>\n</ul>\n\n<p>The problem now is to find the expected number of times the Max Heapify operation will be called, when the stream of integers is of length $R$ and each element of the stream is (iid) uniformly distributed on $[1,N]$.</p>\n\n<p>If the stream were guaranteed to contain only distinct elements, then the answer is easy: </p>\n\n<p>$$E[X] = E[X_1] + E[X_2] + \\dots + E[X_R],$$</p>\n\n<p>where $X_i$ is the random indicator variable for occurrence of the Max Heapify operation at the $i$th number in the stream.  Now  </p>\n\n<p>$$E[X_i] = \\Pr[\\text{$S_i$ is ranked $\\le K$ among first $i$ elements}] = \\min(K/i, 1).$$</p>\n\n<p>Hence,</p>\n\n<p>$$E[X] = K + K/(K+1) + \\cdots + K/R.$$</p>\n\n<p>That case is relatively easy.  But how do we handle the case where the elements are iid uniformly distributed?</p>\n\n<p>[This was actually a Google interview question.] </p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-07T12:35:40.240', 'CommentCount': '0', 'AcceptedAnswerId': '20261', 'CreationDate': '2013-09-28T19:59:47.083', 'Id': '14661'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have to create an algorithm for homework, but cant figure out where to start. </p>\n\n<p>The divide and conquer algorithm muse be O(nlgn) and returns a pair of numbers <em>p</em> and <em>q</em> in an array <em>A</em>. <em>p</em> must appear before <em>q</em> in the array, and <em>q-p</em> must be the highest possible value</p>\n\n<p>heres and example I made up:</p>\n\n<p>A=[1,4,12,5,9,1,3,8]</p>\n\n<p>the return values would be <em>p=1</em> &amp; <em>q=12</em></p>\n', 'ViewCount': '100', 'Title': 'create divide and conquer algorithm', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-09-29T15:10:53.533', 'LastEditDate': '2013-09-29T15:10:53.533', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '14668', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10387', 'Tags': '<algorithms><divide-and-conquer>', 'CreationDate': '2013-09-28T23:21:10.377', 'Id': '14665'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>We have started learning about analysis of recursive algorithms and I got the gist of it. However there are some questions, like the one I'm going to post, that confuse me a little.</p>\n\n<h3>The exercise</h3>\n\n<blockquote>\n  <p>Consider the problem of multiplying two big integers, i.e. integers represented by a large number of bits that cannot be handled directly by the ALU of a single CPU. This type of multiplication has applications in data security where big integers are used in encryption schemes. The elementary-school algorithm for multiplying two n-bit integers has a complexity of . To improve this complexity, let x and y be the two n-bit integers, and use the following algorithm</p>\n\n<pre><code>Recursive-Multiply(x,y)\n  Write  x = x1 * 2^(n/2)+x0  //x1 and x0 are high order and low order n/2 bits\n       y = y1 * 2^(n/2)+y0//y1 and y0  are high order and low order n/2 bits\n  Compute x1+x0  and y1+y0\n  p = Recursive-Multiply (x1+x0,y1+y0)\n  x1y1 = Recursive-Multiply (x1,y1)\n  x0y0 = Recursive-Multiply (x0,y0)\n  Return  x1y1*2^n + (p-x1y1-x0y0)*2^(n/2)+x0y0\n</code></pre>\n  \n  <p>(a) Explain how the above algorithm works and provides the correct answer.</p>\n  \n  <p>(b) Write a recurrence relation for the number of basic operations for the above algorithm.</p>\n  \n  <p>(c) Solve the recurrence relation and show that its complexity is $O(n^{\\lg 3})$</p>\n</blockquote>\n\n<h3>My conjecture</h3>\n\n<ol>\n<li>Since the method is being called three times, the complexity is going to be $3C(n/2) + n/2$.</li>\n</ol>\n\n<h3>My questions</h3>\n\n<ol>\n<li><p>What do they mean by hi-lo order bits?</p></li>\n<li><p>How can I use a recurrence relation on this if I don't know how each recursion works?</p></li>\n</ol>\n", 'ViewCount': '228', 'Title': 'Complexity of a recursive bignum multiplication algorithm', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-30T00:24:45.967', 'LastEditDate': '2013-09-29T23:25:48.427', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14690', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10398', 'Tags': '<algorithms><time-complexity><recursion>', 'CreationDate': '2013-09-29T23:02:57.907', 'Id': '14685'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was asked to perform merge sort by iterative method. I solved it as follows: I first stored each of the elements of the array given in another array say $A$. Then I sent entries of $A$ two by two for merging(was given that the function for merging is known and it merged two arrays at a time). This function returned the merged array which I stored in another array $B$. After I was done with all the elements of $A$ I deleted $A$ and copied each of the entries of $B$ in $A$.I allocated memories for $A$ and $B$ dynamically. I repeated this for $log(n)$ times and each time I halved the size of $A$ and $B$. Is my algorithm correct??</p>\n', 'ViewCount': '614', 'ClosedDate': '2013-10-17T20:40:54.163', 'Title': 'Iterative merge sort', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-01T01:36:15.517', 'LastEditDate': '2013-09-30T17:33:10.407', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10411', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-09-30T15:51:39.713', 'Id': '14706'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have an array of positive integers, $A = (a_1, a_2, ..., a_n)$.\nLet $s(A)$ denote the sum of elements of array $A$.\nI also have an integer $t$, such that $1 &lt; t \\le s(A)$.</p>\n\n<p>I want to split the array $A$ into $m$ contiguous subarrays $(A_1, ..., A_m)$, for which I'll get a minimum of function $f$, defined as</p>\n\n<p>$$\nf(A_1, ..., A_m) = \\sum_{1 \\le i \\le m}{(s(A_i) - t)^2}.\n$$</p>\n\n<p>Please note that I'm talking specifically about arrays, so the order of elements does matter.</p>\n\n<p>Here is a simple example.</p>\n\n<p>Let $t = 13$ and\n$$\nA = (1, 6, 7, 10, 3, 2, 10).\n$$\nWith the following subarrays\n$$\nA_1 = (1, 6, 7)\\\\\nA_2 = (10, 3) \\\\\nA_3 = (2, 10) \\\\\n$$\nthe value of $f(A_1, A_2, A_3) = (14-13)^2 + (13 - 13)^2 + (12 - 13)^2 = 2$.</p>\n\n<p>I don't need an exact solution. Good heuristic would be sufficient.</p>\n", 'ViewCount': '346', 'Title': 'Algorithm for splitting array into subarrays with sums close to the target value', 'LastEditorUserId': '10417', 'LastActivityDate': '2013-10-01T09:17:45.607', 'LastEditDate': '2013-10-01T08:27:49.883', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '14721', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10417', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2013-09-30T19:29:47.780', 'Id': '14713'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know Gentle Boost, an adaption of the AdaBoost algorithm, can be used for <em>binary</em> classification. However, I need to do $n$-ary classification. How do I modify or extend the GentleBoost algorithm so it can be used for $n$-ary classification?</p>\n', 'ViewCount': '93', 'Title': 'N-ary (NOT binary) Gentle Boost algorithm?', 'LastEditorUserId': '755', 'LastActivityDate': '2013-10-01T19:06:03.560', 'LastEditDate': '2013-10-01T18:47:15.770', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<algorithms><machine-learning><classification>', 'CreationDate': '2013-09-30T19:54:09.537', 'Id': '14714'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm interested in building an AI algorithm to play a board game.  This is a sequential two-player game, where they alternate taking turns, so minimax or alpha-beta algorithms sound natural.</p>\n\n<p>However, the twist here is that the game is randomized.  There is a deck of cards that is shuffled randomly; each turn involves drawing the next card from the deck, then making a move.</p>\n\n<p>What algorithms are appropriate for selecting a move, in a game with randomness?</p>\n\n<p>I understand that one approach is to think of this as a three-player game: in addition to the two players, there is a third player (the dealer) who makes a random move.  For instance, if the two players are Minnie and Maxwell, we might alternate moves in the following order: Dealer (chooses a random card for Minnie), Minnie (selects a move), Dealer (chooses a random card for Maxwell), Maxwell (selects a move), and so on.  I think I've read that alpha-beta search can be adapted to this setting.  However, this doesn't sound too promising to me.  There are perhaps 52 possible outcomes of the moves by Dealer, so the tree will be very bushy (with a large branching factor), causing exponential blowup.  Also, this sort of modified alpha-beta search feels a bit inefficient to me.  Each move likely affects a player's score by only a little bit at a time, so we'd expect that the effects of the randomness to average out over many moves, and there is no need to explore all combinations of possible random outcomes.  To put it another way: the game is more about positional elements and random luck than about tactical/combinatorial elements.</p>\n\n<p>Is there a better algorithm framework for randomized games, that takes into account the stochastic element?  (Bonus points if you can suggest something that takes advantage of the fact the strength of a position changes only slowly over many moves.)</p>\n", 'ViewCount': '130', 'Title': 'Algorithms to play randomized game', 'LastActivityDate': '2013-10-09T11:57:28.203', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><artificial-intelligence><board-games><computer-games>', 'CreationDate': '2013-10-01T06:56:49.967', 'Id': '14729'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Using the following recursive fibonacci algorithm:</p>\n\n<pre><code>def fib(n):\n   if n==0:\n      return 0\n   elif n==1\n      return 1\n   return (fib(n-1)+fib(n-2))\n</code></pre>\n\n<p>if i inputed the number 5 to find fib of 5, i know this will equal 5 but how do i examine the complexity of this algorithm? How do i calculate the steps involved? </p>\n', 'ViewCount': '2849', 'Title': 'Complexity of recursive fibonacci algorithm', 'LastActivityDate': '2013-10-07T05:40:42.840', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '14734', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7173', 'Tags': '<algorithms>', 'CreationDate': '2013-10-01T13:21:20.130', 'Id': '14733'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $S$ be a convex polygon on $n$ points. Given two points $A$ and $B$, where $A$ is left of $S$, and $B$ is right of $S$, what's an algorithm to find the shortest path from $A$ to $B$, that avoids the interior of $S$? What about the longest path?</p>\n", 'ViewCount': '114', 'Title': 'Finding both the longest and shortest path in a convex polygon', 'LastEditorUserId': '9665', 'LastActivityDate': '2013-10-31T17:09:25.687', 'LastEditDate': '2013-10-01T16:38:51.820', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9665', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><computational-geometry>', 'CreationDate': '2013-10-01T16:03:11.983', 'FavoriteCount': '1', 'Id': '14736'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>One naive approach in solving <em>multiple pattern matching</em> problem is to call <em>single pattern matching</em> procedure on each of the pattern.</p>\n\n<p>There <strong>must</strong> be some drawbacks in this approach, given the variety of multiple pattern matching algorithms such as Aho Cornsick algorithm, which prove to be more efficient.</p>\n\n<p>So what are the drawbacks on this straightforward yet naive approach? In what scenario is this algorithm doing unnecessary works?</p>\n', 'ViewCount': '40', 'Title': 'Drawbacks of repeating a single pattern matching procedure for many patterns', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-03T07:09:50.597', 'LastEditDate': '2013-10-04T06:33:51.647', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><search-algorithms><strings><substrings>', 'CreationDate': '2013-10-01T18:24:30.603', 'Id': '14739'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a doubly link-list like this</p>\n\n<pre><code>typedef struct Record\n{\n   int i;\n   Record* next;\n   Record* prev;\n}Record;\n</code></pre>\n\n<p>I have over 5 trillions of records that I need to handle, now that I need to retrieve all of them and sort them out.\nIf its size was small, I could borrow stl's vector or list to do the job but now that it is too huge, I have no idea how to save the object data before sorting is performed</p>\n\n<p>my function prototype</p>\n\n<pre><code>void sortRec(Record**recToSort,bool bASC){}\n</code></pre>\n", 'ViewCount': '71', 'ClosedDate': '2013-10-15T10:05:09.320', 'Title': 'linklist and memory issues', 'LastActivityDate': '2013-10-03T19:11:33.200', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10450', 'Tags': '<algorithms><data-structures><data-mining><linked-lists>', 'CreationDate': '2013-10-02T03:53:31.060', 'Id': '14744'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider the following random process.  We have a $10\\times 10$ grid.  At each time step, we pick a random empty grid cell (selected uniformly at random from among all empty cells) and place a marker in that grid cell.  As soon as we have five contiguous markers in a line (in a row, column, or diagonal), we stop.</p>\n\n<p>I'm given a grid containing some markers in some positions, and I'd like to estimate how long until the process stops if we start from that configuration (i.e., the number of additional time steps until five-in-a-line occurs).  I would be happy with any reasonable metric for that: e.g., the expected time until it stops, or the value $t$ such that there's a probability $0.5$ that the process will stop in $\\le t$ time steps.  I'd be happy with an estimate of any such metric.</p>\n\n<p>Is there any efficient algorithm to estimate this metric, given a configuration where some markers have already been placed?  I'm hoping for something faster than random simulation (repeatedly simulating the process and computing an estimate based upon the resulting empirical distribution).</p>\n", 'ViewCount': '129', 'Title': 'Estimating the time until we obtain five-in-a-row?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-25T20:05:32.690', 'LastEditDate': '2013-10-04T06:45:50.277', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><combinatorics><probability-theory><approximation>', 'CreationDate': '2013-10-02T04:18:07.603', 'FavoriteCount': '1', 'Id': '14745'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <em>MAXDICUT</em> problem on a directed graph $G$ is where one is given integer $t$ and a graph $G$ and one has to decide if there is a subset $S \\subset \\mathcal{V}(G)$ such that $$|\\{(i\\rightarrow j)\\in\\mathcal{E}(G)\\mid i\\in S \\text{ and } j\\cap S=\\emptyset\\}|\\geq t$$</p>\n\n<p>Consider a $(c^a,d^a)$-regular directed hypergraph $H$ (meaning every hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(H)$ has $c^a$ vertices with  $head(i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a})=i_1$ and every vertex is on $d^a$ hyperedges with each vertex being a head on only one hyperedge) on $n^a$ vertices with fixed $n\\geq 4$ and fixed $c,d\\geq 2$ and variable parameter $a\\geq 1$. Consider the problem of given an integer $t$ and a hypergraph $H$ with above parameters, one has to decide if there is a subset $S \\subset \\mathcal{V}(H)$ such that $$|\\{\\mathcal{I}=\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(H)\\mid i_1\\in S \\text{ and } \\mathcal{I}\\setminus i_1\\cap S=\\emptyset\\}|\\geq t$$</p>\n\n<p>Call this problem <em>REGULAR-HYPERGRAPH-MAXDICUT</em>. Clearly this problem is NP-complete. Note that a directed $d$-regular graph on $n$ vertices with one outgoing edge per vertex is a $(2,d)$-regular hypergraph on $n$ vertices. So <em>REGULAR-HYPERGRAPH-MAXDICUT</em> $\\leq$ <em>MAXDICUT</em>.</p>\n\n<p>Now consider a $(c^a,d^a)$-regular directed hypergraph $H$ on $n^a$ vertices with fixed $n\\geq 4$ and fixed $c,d\\geq 2$ and variable parameter $a\\geq 1$. Given this graph, form a sequence of $d^a$-regular directed graphs $G_j$ on $n^a$ vertices for $j=1\\rightarrow c^a-1$ by projecting the hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(H)$ to the edge $\\{i_1 \\rightarrow i_{j+1}\\}$ and dropping the remaining possibles projections. Conversely given such $G_i$s, we can glue each set of edges from $i_1$ to get back the hypergraph $H$. Note each vertex of $G_i$ lies on $d^a$ edges.</p>\n\n<p>Given this graph, form a sequence of $r^m$-regular directed graphs $\\mathcal{G}_j$ on $N^m$ vertices for $j=1\\rightarrow r^m-1$ by projecting the hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(\\mathcal{H}([G,m]))$ to the edge $\\{i_1 \\rightarrow i_{j+1}\\}$ and dropping the remaining possibles projections. Conversely given such $\\mathcal{G}_j$ graphs, we can glue each set of edges from $i_1$ to get back the hypergraph $\\mathcal{H}[G,m]$. Note each vertex of $G_i$ lies on $r^m$ edges. </p>\n\n<p>Now from the $G_j$ construct the graph sum $\\mathcal{G}$ which is the directed graph with the hyperedge $\\{i_1\\rightarrow i_2,i_3,i_4,\\dots,i_{c^a}\\}\\in\\mathcal{E}(\\mathcal{H}([G,m]))$ replaced by a sequence of edges $\\{i_1\\rightarrow i_2,i_1\\rightarrow i_3,i_1\\rightarrow i_4,\\dots,i_1\\rightarrow i_{c^a}\\}\\in\\mathcal{E}(\\mathcal{G}$. It is clear \\emph{MAXDICUT} problem on $\\mathcal{G}$ solves the <em>HYPERGRAPH-MAXDICUT</em> problem in $\\mathcal{H}([G,m])$.</p>\n\n<p>Can the above give <em>MAXDICUT</em> $\\leq$ <em>REGULAR-HYPERGRAPH-MAXDICUT</em>? (THE ISSUE IS WHEN WE CHOOSE AN HYPEREDGE WE CHOOSE ALL ITS VERTICES. THIS IS NOT THE CASE WHEN WE REPLACE THE HYPERGRAPH WITH A DIRECTED GRAPH).</p>\n\n<p>Is <em>REGULAR-HYPERGRAPH-MAXDICUT</em> $\\leq_{APX}$ <em>MAXDICUT</em> and <em>MAXDICUT</em> $\\leq_{APX}$ <em>REGULAR-HYPERGRAPH-MAXDICUT</em>? This will give a constant factor approximation to <em>REGULAR-HYPERGRAPH-MAXDICUT</em> where the constant factor is independent of $a$.</p>\n\n<p>Can we generalize to a possible version of <em>IRREGULAR-HYPERGRAPH-MAXDICUT</em>? Would this yield constant factor approximations to <em>IRREGULAR-HYPERGRAPH-MAXDICUT</em> as well?</p>\n', 'ViewCount': '39', 'Title': 'Constant factor approximation of hypergraph maxdicut', 'LastEditorUserId': '9753', 'LastActivityDate': '2013-10-02T19:15:11.187', 'LastEditDate': '2013-10-02T19:15:11.187', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9753', 'Tags': '<reductions><approximation-algorithms><max-cut>', 'CreationDate': '2013-10-02T08:29:17.627', 'Id': '14751'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>(I've been stuck on this homework assignment for far too long)</p>\n\n<p>I need to find the number of independent sets in a tree. </p>\n\n<p>For example, say the set of nodes in a tree is {A, B, C, D, E}. B and C are children of A and D, E are children of B. This tree has 14 independent sets. </p>\n\n<p>I assume that the algorithm will be recursive and I think that I should make each level of a tree into a linked list, so B->C and D->E, but more than that I'm stumped. </p>\n\n<p>Would grealy appreciate help.</p>\n", 'ViewCount': '131', 'Title': 'Number of Independent Sets in a tree', 'LastActivityDate': '2013-10-03T05:46:54.597', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14770', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9743', 'Tags': '<algorithms><trees>', 'CreationDate': '2013-10-02T23:59:05.887', 'Id': '14768'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<hr>\n\n<p>Is there an incremental directed graph data structure that has the following properties:</p>\n\n<ul>\n<li>Keeps an internal graph structure as a DAG, and the graph is accessible (notwithstanding other helper data-structures)</li>\n<li>The accessible DAG is kept as a transitive reduction (notwithstanding other helper data-structures)</li>\n<li>It should be optimized for a sparse graph (adjacency lists)</li>\n<li>It should condense cycles as they are introduced, keeps a mapping between equivalent vertices that are all replaced with one "representative" vertex</li>\n<li>Ability to quickly answer quickly ancestor/descendant/transitive/relationship queries (in $\\mathcal{O}(1)$ or $\\sim\\mathcal{O}\\left(\\log \\left|V\\right|\\right)$ time)</li>\n<li>Should support vertex, edge insertion, deletion would be nice too</li>\n<li>Mutable operations  (such as insertion) should be as output-sensitive as possible; in other words, the complexity should depend as much as possible on how much the operation must change the graph</li>\n<li>Ability to record changes over an operation, if requested. Obviously this might necessarily increase the complexity, but the increase should be output-sensitive. Examples:\n<ul>\n<li>set of deleted vertices (due to condensation)</li>\n<li>set of deleted edges (due to reduction)</li>\n<li>set of new decendent relationships from $u$ ( example: $insert(G,u,v) \\rightarrow \\left\\{t ~|~ path(u,t)\\in G\'\\wedge path(u,t)\\not\\in G\\right\\}$ )</li>\n<li>set of new ancestor relationships from $v$ ( example: $insert(G,u,v) \\rightarrow \\left\\{t ~|~ path(t,v)\\in G\'\\wedge path(t,v)\\not\\in G\\right\\}$ )</li>\n</ul></li>\n</ul>\n\n<p>The closest I can find is <a href="http://code-o-matic.blogspot.com/2010/07/graph-reachability-transitive-closures.html" rel="nofollow">here</a>, <a href="http://code.google.com/p/transitivity-utils" rel="nofollow">implementation</a>. I think you can build on this to do have most of the properties I list, but I am wondering if there is anything better/well known, or perhaps if there is a name for this problem.</p>\n\n<hr>\n\n<p><strong>EDIT:</strong></p>\n\n<h3>Related:</h3>\n\n<ul>\n<li><a href="http://cstheory.stackexchange.com/q/14343/3377">What is the fastest deterministic algorithm for dynamic digraph reachability with no edge deletion?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/18787/3377">What is the fastest deterministic algorithm for incremental DAG reachability?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/5176/3377">Does an algorithm exist to efficiently maintain connectedness information for a DAG in presence of inserts/deletes?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/2548/3377">Is there an online-algorithm to keep track of components in a changing undirected graph?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/17135/3377">Dynamic shortest path data structure for DAG</a></li>\n</ul>\n', 'ViewCount': '150', 'Title': 'An incrementally-condensed transitive-reduction of a DAG, with efficient reachability queries', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-04T05:16:35.877', 'LastEditDate': '2013-10-04T04:51:04.030', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><data-structures><graphs><shortest-path><online-algorithms>', 'CreationDate': '2013-10-03T21:12:35.287', 'FavoriteCount': '1', 'Id': '14798'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Big-O notation hides constant factors, so some $O(n)$ algorithms exist that are infeasible for any reasonable input size because the coefficient on the $n$ term is so huge.</p>\n\n<p>Are there any known algorithms whose runtime is $O(f(n))$ but with some low-order $o(f(n))$ term that is so huge that for reasonable input sizes it completely dominates the runtime?  I'd like to use an algorithm like this an an example in an algorithms course, as it gives a good reason why big-O notation isn't everthing.</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '180', 'Title': 'Example of an algorithm where a low-order term dominates the runtime for any practical input?', 'LastActivityDate': '2013-12-04T17:31:23.557', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><asymptotics>', 'CreationDate': '2013-10-05T07:50:06.317', 'FavoriteCount': '2', 'Id': '14823'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Suppose we are given an M\xd7N matrix, with some elements are zero, some non-zero. We know the co-ordinates of non-zero elements. Now, if I am allowed to multiply a whole row or a whole column by zero one at a time what will be minimum number of operations (i.e multiplications) I will need. For example, for the matrix</p>\n\n<p>$\\begin{pmatrix}\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 0 \\\\\n    1 &amp; 0 &amp; 1\n  \\end{pmatrix}$</p>\n\n<p>the answer is two. For this example</p>\n\n<p>$\\begin{pmatrix}\n    1 &amp; 1 &amp; 1 \\\\\n    0 &amp; 0 &amp; 1 \\\\\n    0 &amp; 0 &amp; 1\n  \\end{pmatrix}$</p>\n\n<p>the answer is two not three.</p>\n\n<p>Any help to go for head start is appreciated.</p>\n', 'ViewCount': '65', 'Title': 'What will be minimum no of operation to make whole matrix zero if one is allowed to multiply a row or column by zero?', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-06T16:53:54.620', 'LastEditDate': '2013-10-06T16:53:54.620', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10516', 'Tags': '<algorithms><complexity-theory><graph-theory><data-structures><discrete-mathematics>', 'CreationDate': '2013-10-05T18:23:42.310', 'FavoriteCount': '1', 'Id': '14831'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '621', 'Title': 'Running Floyd-Warshall algorithm on graph with negative cost cycle', 'LastEditDate': '2013-10-06T13:05:25.863', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10519', 'FavoriteCount': '1', 'Body': u"<p>I am trying to find the answer to the following question for the Floyd-Warshall algorithm. Suppose Floyd-Warshall algorithm is run on a directed graph G in which every edge's length is either -1, 0, or 1. Suppose that G is strongly connected, with at least one u-v path for every pair u,v of vertices, and that G may have a negative-cost cycle. How large can the final entries A[i,j,n] be, in absolute value (n denotes number of vertices). Choose the smallest number that is guaranteed to be a valid upper bound?\nThere is the following answers:</p>\n\n<ol>\n<li>+\u221e</li>\n<li>n^2</li>\n<li>n - 1</li>\n<li>2^n</li>\n</ol>\n\n<p>I have ruled out 3. (n-1) and 1. (+\u221e) since if a graph has a negative cost cycle, the absolute final value of a path including a negative cycle can be increased further than n-1. The answer also cannot be +\u221e since the algorithm stops after a finite number of steps. But I am having trouble between answers 2. and 4. I am more inclined to 4. since I have run some test cases, and final values seemed to comply to an exponential growth. But I cannot find a proof for it. </p>\n", 'Tags': '<algorithms><graph-theory><shortest-path>', 'LastEditorUserId': '10519', 'LastActivityDate': '2013-10-06T20:41:36.453', 'CommentCount': '7', 'AcceptedAnswerId': '14861', 'CreationDate': '2013-10-05T22:22:47.837', 'Id': '14839'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m taking the MIT Open Courseware for Introduction to Algorithms and I\'m having trouble understanding the first homework problem/solution.</p>\n\n<p>We are supposed to compare the asymptotic complexity (big-O) for different functions:</p>\n\n<p>$f_1(n) = n^{0.999999}\\log(n)$<br>\n$f_2(n) = 10000000n$</p>\n\n<p>$f_2(n)$ is obviously O(n), but the big-O given for $f_1(n)$ confuses me and I don\'t follow the given solution.</p>\n\n<p>The solution says $f_1(n)$ has less Big-O complexity than $f_2(n)$:</p>\n\n<p>"recall that for any $c &gt; 0$, $log(n)$ is $O(n^c)$.</p>\n\n<p>Therefore we have: $f(n) = n^{0.999999}log(n) = O(n^{0.999999}n^{0.000001}) = O(n) = O(f_2(n))$"</p>\n\n<p>I do not understand the logic underlying the solution. I may be forgetting something simple? Can someone break it down for me? Thanks!</p>\n', 'ViewCount': '131', 'Title': 'Big O Notation of $n^{0.999999}\\log(n)$', 'LastActivityDate': '2013-10-06T23:38:39.473', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10536', 'Tags': '<algorithms><asymptotics>', 'CreationDate': '2013-10-06T23:12:56.490', 'FavoriteCount': '1', 'Id': '14864'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to work out the recurrence relation for Ternary Search. This is what I came up with:</p>\n\n<pre><code>C(n) = C(n/3) + 2\n</code></pre>\n\n<p>However, I talked to my professor and he said it's not correct. He says that we need to take all of their cases into account. This part confuses me a little bit, can you please clear it for me?</p>\n\n<p>Thank you.</p>\n", 'ViewCount': '570', 'Title': 'Ternary Search Recurrence Relation', 'LastActivityDate': '2013-10-07T01:30:55.647', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14872', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10398', 'Tags': '<algorithms><recurrence-relation>', 'CreationDate': '2013-10-06T23:59:28.933', 'Id': '14869'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Reading an article on <a href="http://www.ams.org/notices/199612/pomerance.pdf" rel="nofollow">integer factorization</a> I implemented the following -  rather inefficient - factorization method:  </p>\n\n<blockquote>\n  <p>Every odd\n  composite can be factored as a difference of\n  squares: $$ ab = \\left[\\tfrac{1}{2}(a+b)\\right]^2 -\n \\left[\\tfrac{1}{2}(a-b)\\right]^2$$ \n  We can look at values of $f(x) = x^2 - n$ until we find a perfect square and factor.</p>\n</blockquote>\n\n<p>Here\'s my implementation in Python.</p>\n\n\n\n<pre><code>def fermat(n):\n    x = int(np.sqrt(n))+1\n    y = int(np.sqrt(abs(y*y - n)))\n\n    while( n - x*x + y*y != 0):\n        x += 1\n        y = int(np.sqrt(abs(x*x - n)))\n\n    return x, y\n</code></pre>\n\n<p>How expensive are the square root calculations here?  Are they necessary?\nIn order to check I have a perfect square, I compute $\\lfloor \\sqrt{x^2-n}\\rfloor$ many times.  </p>\n', 'ViewCount': '105', 'Title': "integer factoring using Fermat's method", 'LastActivityDate': '2013-10-07T16:54:29.277', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><number-theory><factoring>', 'CreationDate': '2013-10-07T15:49:11.143', 'Id': '14888'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is my approach </p>\n\n<p>First I compute the longest non decreasing sub-sequence in $N \\log N$ time. Algorithm to do this (that only uses arrays and binary search) can be found here:\n<a href="http://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms" rel="nofollow">http://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms</a></p>\n\n<p>Let\'s suppose the longest subsequence has $L$ elements. Then if $L &lt; N - M$, there isn\'t any way to solve the problem since there\'s no subsequence of length $N - M$ that\'s still sorted.</p>\n\n<p>Otherwise, just remove the $N - L$ elements that aren\'t in the subsequence, and then remove more at random until exactly M total have been removed. In all this is an $N \\log N$ algorithm.</p>\n\n<p>I want to know, is there any more efficient algorithm (i.e. $O( N)$ ) to solve this problem ?</p>\n', 'ViewCount': '193', 'Title': 'Given an array of N integers, how can you find M elements to remove so that the array will end up in sorted order?', 'LastEditorUserId': '6890', 'LastActivityDate': '2013-10-08T23:01:04.927', 'LastEditDate': '2013-10-08T08:47:24.010', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8690', 'Tags': '<algorithms><time-complexity><runtime-analysis><subsequences>', 'CreationDate': '2013-10-08T08:40:39.700', 'FavoriteCount': '1', 'Id': '14899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am working on a diagram editor.\nDiagrams display 2D shapes (<em>nodes</em>) connected with connectors (<em>edges</em>).</p>\n\n<p>I\'d like to add an operation that, given a selection of nodes, <em>"disentangles"</em> them: it repositions them to reduce the number of crossing edges, if possible\n(and it\'s OK if the edges will have to be drawn with bend points).</p>\n\n<p>So I want a graph algorithm that, given a (<em>topological</em>) graph embedding and a subset of its nodes, modifies the embedding (its <em>topology</em>) on only those nodes so as to minimize the number of crossing edges.</p>\n\n<p>From reading <a href="https://en.wikipedia.org/wiki/Apex_graph#Characterization_and_recognition" rel="nofollow">about apex graphs</a> and browsing <a href="http://epubs.siam.org/doi/abs/10.1137/120872310" rel="nofollow">Cabello and Mohar (2013)</a>, I suppose this problem is NP-hard.\nSo I\'ll be happy with a parametrized algorithm (e.g. on the number of crossing edges) that has a known, polynomial, time complexity for any given parameter value. This seems feasible, but I don\'t find it easy to come up with such an algorithm on my own.</p>\n\n<p>Questions:</p>\n\n<ul>\n<li>Where do I look for such an algorithm?</li>\n<li>Does it exist?</li>\n<li>In existing software?</li>\n<li>Is there any significant practical experience with such an operation? (What looks good in theory may not be so good in practice, or vice versa.)</li>\n</ul>\n\n<p><em>(I am not sure where best to ask this question: here, on StackOverflow, or MathOverflow?)</em></p>\n', 'ViewCount': '236', 'Title': 'How to reduce the number of crossing edges in a diagram?', 'LastEditorUserId': '917', 'LastActivityDate': '2013-10-08T18:57:07.263', 'LastEditDate': '2013-10-08T10:00:56.180', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '917', 'Tags': '<graph-theory><graph-algorithms><crossing-number>', 'CreationDate': '2013-10-08T09:45:18.533', 'FavoriteCount': '1', 'Id': '14901'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been reading about the Strassen Algorithm for matrix multiplication.</p>\n\n<p>As mentioned in Introduction to Algorithms by Cormen , the algorithm is not intuitive. However I am curious to know if there exists any rigorous mathematical proof of the algorithm and what actually went into the design of the algorithm.</p>\n\n<p>I tried searching on Google and stackoverflow, but all links are only on comparing Strassen\'s approach to standard matrix multiplication approach or they elaborate on the procedure presented by the algorithm.</p>\n\n<p>Note: The same question has been asked on <a href="http://stackoverflow.com/questions/19229454/strassens-algorithm-proof">http://stackoverflow.com/questions/19229454/strassens-algorithm-proof</a> , but without any definite answers</p>\n', 'ViewCount': '237', 'Title': "Strassen's Algorithm proof", 'LastActivityDate': '2013-10-08T17:37:31.217', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9548', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-10-08T11:51:57.770', 'FavoriteCount': '1', 'Id': '14907'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We have two sets of vectors of positive numbers, $X$ and $Y$ where for\n$x\\in X$ we write $x=(x_1,x_2,\\ldots,x_k)$ and similarly for $y\\in Y$\nwe write $y=(y_1,y_2,\\ldots,y_k$).</p>\n\n<p>We are given two vectors $l=(l_1,l_2,\\ldots,l_k)$, and\n$u=(u_1,u_2,\\ldots,u_k)$ such that $l_i\\le u_i$ for all $i$.</p>\n\n<p>We want to find all pairs $(x,y)$, $x\\in X$, and $y\\in Y$ such that \n$l_i\\le x_i+y_i\\le u_i$</p>\n\n<p>Handwaving a bit, we can do this in a divide and conquer sort of way,\nseparating each set into pieces that are larger and smaller than\n$l_i/2$ and throwing away the pairs where both are in the smaller\nhalf.\nThis gives a recurrence\n$$T(m,n) = T(m,n/2) + T(m/2, n/2) + c(m+n)$$\nwhere $m=|X|$ and $n=|Y|$.  For equal size sets, this gives\n$T(n,n) = O(n^{1.7})$, which is better than quadratic, but less than I\nwould hope for.</p>\n\n<p>A similar question could be asked for three or more sets.  </p>\n', 'ViewCount': '83', 'Title': 'Range query for sum of vectors', 'LastEditorUserId': '10576', 'LastActivityDate': '2013-10-09T05:26:48.523', 'LastEditDate': '2013-10-08T15:53:44.703', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10576', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-10-08T14:33:14.263', 'FavoriteCount': '1', 'Id': '14910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a sequence $(a)$ of lenght $n$, such $a_1 = a_n = 1$ and $a_i \\in \\left \\{ -1, 1 \\right\\}$.  </p>\n\n<p>We want to find a subsequence $(b) \\subset (a)$ which statisfies the following conditions:  </p>\n\n<ul>\n<li>The length of $(b)$, (let's call it $k$) is the largest possible</li>\n<li>$b_k = b_1 = 1$</li>\n<li>Each prefix sum $b_1$, $b_1+b_2$,  $b_1+b_2 + b_3,\\ \\ldots\\ , b_1 + b_2 + \\ldots + b_k$ is $\\geq 0$.</li>\n<li>Each suffix sum $b_k$, $b_{k} + b_{k-1}, b_{k} + b_{k-1} + b_{k-2}, \\ \\ldots\\ , b_k + b_{k-1} + \\ldots + b_1$ is $\\geq 0$. </li>\n<li>Sum of $(b)$ is the biggest possible.</li>\n</ul>\n\n<p>Is it possible to construct linear algorithm to solve it? </p>\n", 'ViewCount': '67', 'Title': 'Maximizing length of subsequence', 'LastEditorUserId': '10555', 'LastActivityDate': '2013-10-09T00:08:15.623', 'LastEditDate': '2013-10-08T21:30:50.137', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14922', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10555', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-10-08T18:09:51.267', 'Id': '14918'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>What is the time complexity of the recurrence relation \nT(xn) + T((1 \u2212 x)n) + cn</p>\n', 'ViewCount': '37', 'ClosedDate': '2013-10-09T11:55:46.847', 'Title': u'Time Complexity of T(xn) + T((1 \u2212 x)n) + cn', 'LastActivityDate': '2013-10-09T03:41:12.370', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '10598', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-10-09T00:30:50.530', 'Id': '14936'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Those terminologies confuse me. As I understand </p>\n\n<ul>\n<li>SAT solver: decide the satisfiability of propositional logic (using DPLL or Local Search).</li>\n<li>Decision procedure is a procedure to decide the satisfiability of a certain decidable first-order theory.</li>\n<li>SMT solver is a SAT solver + decision procedure.</li>\n<li>Theorem prover indicates something like Dynamic Logic, e.g. the KeY tool</li>\n<li>Constraint solver: I don't know.</li>\n</ul>\n\n<p>But I see people calling Z3 a theorem prover. So I don't know how to dishtinguish those terms. And what is the most general term for all of them? Thank you.</p>\n", 'ViewCount': '308', 'Title': 'Distinguish Decision Procedure vs SMT solver vs Theorem prover vs Constraint solver', 'LastEditorUserId': '8607', 'LastActivityDate': '2013-10-16T13:03:06.590', 'LastEditDate': '2013-10-16T13:03:06.590', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '8607', 'Tags': '<algorithms><terminology><reference-request><decision-problem>', 'CreationDate': '2013-10-09T12:25:45.727', 'FavoriteCount': '2', 'Id': '14946'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>After skimming <a href="http://www.lirmm.fr/~imbert/pdfs/constmult_arith18.pdf">Multiplication by a Constant is Sublinear (PDF)</a>, (<a href="http://ge.tt/6343C5u/v/0">slides (PDF)</a>, <a href="http://ge.tt/6343C5u/v/1">slides with notes (PDF)</a>) I was wondering if this could be extended to division by a constant in sublinear time?</p>\n\n<p>Additionally, what about division with a constant numerator, ie. "division of a constant"?</p>\n', 'ViewCount': '105', 'Title': 'Division by a constant', 'LastActivityDate': '2013-10-09T17:02:46.717', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14959', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><reference-request><integers>', 'CreationDate': '2013-10-09T15:46:09.137', 'Id': '14954'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In class we saw the followin problem but i didnt undestand the solution. Do anybody could explain me with more detail the procedure to solve this problem or give me a better solution?:</p>\n\n<blockquote>\n  <p>Assume that $n$ points in the plane are given. Find a polygonal arc with $n-1$ sides whose vertices are given points, and whose sides do not intersect. (Adjacent sides may form a $180$ angle). The number of operations shold be of order $n$ $log$ $n$.</p>\n</blockquote>\n\n<p>The teacher solution was:</p>\n\n<blockquote>\n  <p>Sort all the points with respect to the x-coordinate; when x-coordinates are equal, take the y-coordinate into account, then connect all the vertices by line segments(in that order).</p>\n</blockquote>\n', 'ViewCount': '45', 'ClosedDate': '2013-11-28T21:51:16.080', 'Title': 'Finding a polygonal arc algorithm?', 'LastActivityDate': '2013-10-09T15:57:50.583', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><algorithm-analysis><graphs><sorting>', 'CreationDate': '2013-10-09T15:57:50.583', 'Id': '14955'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I am given a number $n$ (less than $10^8$) and $m$ (less than $10^7$) and $p$ (less than $10^4$), I have to write a program to find number of numbers that divide $n^m$ exactly $p$ times.</p>\n\n<p>Mathematically, I have to find number of distinct $x$ such that\n$$ n^m \\equiv 0 \\mod x^p \\qquad\\text{and}\\qquad n^m \\ne 0 \\mod x^{p+1} $$</p>\n\n<p>(From <a href="http://uva.onlinejudge.org/external/122/12216.html" rel="nofollow">UVa Online Judge</a>)</p>\n\n<p>What approach could be better than brute force?</p>\n', 'ViewCount': '55', 'Title': 'Finding number of numbers dividing n^m exactly p times', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-10T09:25:41.220', 'LastEditDate': '2013-10-10T09:25:41.220', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7890', 'Tags': '<algorithms><number-theory><arithmetic><mathematical-programming>', 'CreationDate': '2013-10-10T08:27:13.950', 'Id': '14981'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have some difficulty trying to tell which equation to use when I'm given an explanation on how an algorithm operates. Especially divide and conquer.</p>\n\n<p>Normally I see these kind of equations:</p>\n\n<pre><code>C(n) = aC(n/a) + b where a and b are constants\n</code></pre>\n\n<p>Other times I don't see the a in front of C(n/a) as an answer. That really confuses me.</p>\n\n<p>Can you tell me when I will need to use which?</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '66', 'Title': 'Recurrence Equation Question', 'LastActivityDate': '2013-10-10T19:15:54.833', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '14990', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10398', 'Tags': '<algorithms><recurrence-relation>', 'CreationDate': '2013-10-10T16:58:06.577', 'Id': '14989'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to solve the following problem related to distribution-</p>\n\n<p>I have a list L of items (I1, I2,....In) sorted in order of importance, I1 being the most important. Each item has multiple tags assigned to it and the combination of these tags can be different on each item, so I1 may have tags T1 and T2, I2 can have tags t2, t3 and t4, and I3 can have tag T1, and so on.</p>\n\n<p>Now, I have to make batches from this list L, with a distribution of items (according to the tags) subject to the following constraints-</p>\n\n<p>Each batch has a fixed size B\nEach tag has a range of items in the batch distribution, ranging from a minimum to maximum. So, B should contain minimum x1 items with tag t1, x2 items with tag t2, and maximum y1 items of t1, y2 items of t2 and so on.\nWe start picking items from the top of L and keep filling the batch until we reach the final distribution that satisfies the constraints. If, say, L has 300 items, and we have to make a batch size of 50, we can go until any number of items in the list and pick the items to make the desired distribution.\nRemember that if an item is picked from the list, count of all the tags assigned to it goes up by 1.\nI was thinking of a solution where in first, I make lists of items corresponding to each specific tag. I pick the minimum desired items for a particular tag from its list. So, I'd pick x1 items with tag t1 from the list of items with tags t1, irrespective of whether the items contain any other tag. This way I'd ensure that the 'minimum' criteria of all the tags are satisfied. But for the max part, each tag will most definitely go overboard. How do I recursively keep replacing items from the batch with the remaining items in L to make the final desired distribution?</p>\n\n<p>Any other solution would be great. Or any existing algorithm that can get me in the right direction to approach this problem.</p>\n\n<p>I know the question is a bit too wordy, and probably a bit confusing, but I'v tried to explain it as well as I can, and of course, the problem might be a lot interesting I suppose.</p>\n", 'ViewCount': '92', 'Title': 'Distribution algorithm according to weighted parameters (with a min-max constraint)', 'LastActivityDate': '2013-10-13T05:35:12.270', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '15030', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10652', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2013-10-11T06:21:39.850', 'Id': '15001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So I have some graph, and I know what it's max flow is based of the Ford-Fulkerson Algorithm.</p>\n\n<p>With this information, I need to know how to find a new max flow when I remove an edge of this graph with flow capacity 1. The algorithm must run in O(v+e) time where v is the number of vertices in the graph, and e the number of edges.</p>\n\n<p>I have an idea, but I don't think it will run in O(v+e). \nI might just be confused about big O notation, but my algorithm would run in O(v+e+v)=O(2v+e) time. Would that just be considered O(v+e) time? If not, what part of my proposed algorithm should be reconsidered? Should I make a completely different approach (maybe considering if the deleted edge is in the min-cut or not?).</p>\n\n<p>Thanks in advance!</p>\n", 'ViewCount': '85', 'ClosedDate': '2013-10-14T07:48:03.477', 'Title': 'Recalculate max-flow after removing edge with 1 capacity', 'LastEditorUserId': '10680', 'LastActivityDate': '2013-10-13T21:45:05.603', 'LastEditDate': '2013-10-13T21:44:47.073', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10680', 'Tags': '<algorithms><graph-theory><algorithm-analysis><dynamic-programming>', 'CreationDate': '2013-10-12T20:18:55.893', 'Id': '15023'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a tree $T$ with $n$ vertices, we want to find the number of triplets of vertices $(a,b,c)$ such $d(a,b) = d(b,c) = d(c,a)$ where $d$ is the distance function (length of the shortest path between two nodes).</p>\n\n<p>It's pretty easy to do it in $O(n^3)$ time. Is it possible to do it faster?<br>\nI think that on-line algorithm and pre-processing should help.</p>\n", 'ViewCount': '86', 'Title': 'Find equidistant triplets in a tree', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-14T02:33:40.063', 'LastEditDate': '2013-10-13T18:35:08.433', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10555', 'Tags': '<algorithms><graphs><dynamic-programming><trees>', 'CreationDate': '2013-10-13T18:28:25.567', 'FavoriteCount': '0', 'Id': '16047'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '152', 'Title': 'What do I need to know about algorithms?', 'LastEditDate': '2013-10-14T07:49:48.907', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '2', 'Body': '<p>Drawing from <a href="http://cs.stackexchange.com/questions/15017/are-algorithms-and-efficiency-in-general-getting-less-important">Are algorithms (and efficiency in general) getting less important?</a>, I get that algorithms are more important than ever, but I need to know anything about them other than their complexities?</p>\n\n<p>I would expect to use algorithms out of a library of some sort that has been written by people that are much much more knowledgeable than me, and have been thoroughly tested (most of the STL comes to mind here, but also boost, and probably every other library that someone would care to name).</p>\n\n<p>Thus, unless I am a library maintainer/implementer of some sort, do I need to know anything about these algorithms?</p>\n', 'Tags': '<algorithms><efficiency>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-14T07:49:48.907', 'CommentCount': '2', 'AcceptedAnswerId': '16052', 'CreationDate': '2013-10-13T18:59:13.733', 'Id': '16051'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When using A* or any other best path finding algorithm we say that the heuristic used should be <strong>admissible</strong> i.e. it should never overestimate the actual solution path (moves).</p>\n\n<p>Can someone tell me how this is true? Please avoid equations and all. I need a logical proof.\nIf you  want you can explain using the <strong>Manhattan</strong> distance heuristic of the <strong>8-puzzle.</strong></p>\n', 'ViewCount': '804', 'Title': 'How does an admissible heuristic ensure an optimal solution?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-14T22:14:51.397', 'LastEditDate': '2013-10-14T12:16:36.493', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '16089', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4341', 'Tags': '<algorithms><artificial-intelligence><search-algorithms><heuristics>', 'CreationDate': '2013-10-14T06:34:42.517', 'Id': '16065'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In real word problems, the influence of multiple not perfectly known factors results in using heuristics instead of mathemacial solutions that calculates a perfect value from only precisly defined input data. Consequently, any method that does not supply the mathematical maximum or minimum is not an optimisation but an improvement.</p>\n\n<p>Somehow my opinion on this topic differs from the use of the term <code>optimisation</code> in many papers. Are the people just not precise in their language or is my understanding of the term wrong?</p>\n\n<p><code>Improvement</code> doesn't sound as facy as <code>optimisation</code>, but is there maybe some facy word that allows people to still be precise?</p>\n", 'ViewCount': '40', 'Title': 'Is a non-perfect improvement and optimisation?', 'LastActivityDate': '2013-10-14T13:53:07.487', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10634', 'Tags': '<terminology><optimization><approximation><applied-theory><approximation-algorithms>', 'CreationDate': '2013-10-14T13:28:29.267', 'Id': '16071'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been struggling with this problem for days now, making no progress:</p>\n\n<blockquote>\n  <p>There are N points on an XY plane. In one turn, you can select a set of collinear points on the plane and remove them. Your goal is to remove all the points in the least number of turns. Given the coordinates of the points, calculate two things:</p>\n  \n  <ul>\n  <li>The minimum number of turns (T) needed to remove all the points.</li>\n  <li>The number of ways to to remove them in T turns. Two ways are considered different if any point is removed in a different turn.</li>\n  </ul>\n</blockquote>\n\n<p>-- <a href="https://www.hackerrank.com/challenges/points-in-a-plane" rel="nofollow">https://www.hackerrank.com/challenges/points-in-a-plane</a></p>\n\n<p>I\'ve tried a greedy exhaustive solution, where I draw a line between each pair of points, then start to eliminate the lines in descending order based on the number of points they cross. Unfortunately, there is at least one case where this approach produces suboptimal results:</p>\n\n<p>For ease of discussion, I will call lines "longer" or "shorter", based on how many points they include. The greedy algorithm is simply to eliminate lines in order of descending length.</p>\n\n<p>Suppose we have a set of N longer lines, and another set of M shorter lines. Our greedy algorithm will eliminate the long lines first. But what if every single point of the long lines is also included in a short line?  In that case,our initial elimination of the N longer line was a waste, since we would have gotten those lines "for free" had we just eliminated the shorter lines. Specifically, our greedy approach will require N + M eliminations, where we could have cleared all points in just M steps.</p>\n\n<p>The simplest example input demonstrating this is:</p>\n\n<pre><code>(0, 1), (1, 2), (2, 4), (3, 3)\n(0, 0), (1, 0), (2, 0), (3, 0)\n(0,-1), (1,-2), (2,-4), (3,-3)\n</code></pre>\n\n<p>As you can see, we have a line of length 4 running along the X axis, and 4 shorter vertial lines of length 3 perpendicular to it. Our greedy algorithm will first eliminate the longest line, after which there will be 8 sets of points remaining, with no more than 2 of them collinear. Eliminating those will thus take 4 steps, for a total 5, where we could have eliminated all points in just 4 steps, had we simply eliminated the 4 vertical lines first.</p>\n\n<p>Could someone provide at least a hint at the general body of knowledge required to approach this? I solved many other HackerRank questions, but can\'t make any headway with this one.</p>\n', 'ViewCount': '293', 'Title': 'Points-in-a-plane from HackerRank', 'LastEditorUserId': '10768', 'LastActivityDate': '2013-10-18T20:31:51.190', 'LastEditDate': '2013-10-18T20:31:51.190', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '16150', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10768', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2013-10-15T20:20:51.557', 'Id': '16111'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There are $n$ pairs of socks, all different. They all went out of the dryer, so there are now $2n$ socks scattered around. Given two socks, the only operation I can do is to decide whether they are identical (- belong to the same pair) or different (- belong to different pairs). What is the best way to sort the socks in matching pairs?</p>\n\n<p>The obvious algorithm is: grab a sock, scan the pile for its matching sock, and put that pair aside. This algorithm requires time $O(n^2)$.</p>\n\n<p>I Found a related question, <a href="http://www.mail-archive.com/kragen-tol@canonical.org/msg00084.html" rel="nofollow">On the complexity of sock-matching</a> from about 10 years ago, which also describes an $O(n^2)$ algorithm.</p>\n\n<p>If each sock has a numeric ID, or another ordinal property, such as: wavelength of color, then the entire set of socks can be sorted by that ordinal property in time $O(n log n)$, and then the pairs can just be collected with an $O(n)$ scan. But, in this question I assume that there is no order on the socks - we can only tell whether two socks are identical or different.</p>\n\n<p>In a similar question in StackOverflow (<a href="http://stackoverflow.com/questions/14415881/how-to-pair-socks-from-a-pile-efficiently">How to pair socks from a pile efficiently?</a>), the accepted answer suggests to use a hash, but again, this assumes that each sock can be represented by a set of properties such as color, pattern, etc., so that each property can be handled separately. In this question I assume that the properties of the socks are only accessible via a function that tells whether two socks are identical or different.</p>\n\n<p>There is a paper named <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.4654&amp;rep=rep1&amp;type=pdf" rel="nofollow">Sock sorting</a>, which initially seems related, but actually it deals with another problem, where two socks might seem identical although they are different (this probably makes the problem more difficult).</p>\n\n<p>So, my question is: assuming we only have a binary "identical/different" relation on the socks, can we match the socks faster than $O(n^2)$?</p>\n', 'ViewCount': '140', 'Title': 'sock matching algorithm', 'LastActivityDate': '2013-10-16T13:17:21.433', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16135', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><time-complexity><sorting>', 'CreationDate': '2013-10-16T12:42:54.633', 'FavoriteCount': '1', 'Id': '16133'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is an exercise in the book Introduction to Algorithm, 3rd Edition.</p>\n\n<p>The original question is:</p>\n\n<p>Show how to implement GREEDY-SET-COVER in such a way that it runs in time $O(\\sum_{S\\in\\mathcal{F}}|S|)$.</p>\n\n<p>The GREEDY-SET-COVER in the text book is as follows:</p>\n\n<p><img src="http://i.stack.imgur.com/v55Gn.png" alt="greedy-set-cover-in-text-book"></p>\n\n<p>Definition for $(X,\\mathcal{F})$ is given as:</p>\n\n<p><img src="http://i.stack.imgur.com/3aVbz.png" alt="enter image description here"></p>\n', 'ViewCount': '397', 'ClosedDate': '2013-10-30T10:05:07.743', 'Title': 'How to implement GREEDY-SET-COVER in a way that it runs in linear time', 'LastActivityDate': '2013-10-16T18:49:41.563', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><algorithm-analysis><time-complexity><greedy-algorithms>', 'CreationDate': '2013-10-16T17:15:40.530', 'Id': '16142'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the text book, Introduction to Algorithm, 3rd Edition.</p>\n\n<p>In the chapter, <strong>Approximation Algorithms</strong> and for the problem <strong>Travelling Salesman Problem</strong>, the author says: </p>\n\n<p><img src="http://i.stack.imgur.com/99jYL.png" alt="enter image description here"></p>\n\n<p>I am wondering how triangle inequality gives rise to this assertion? It seems that this property is not that important, as I searched through the rest of this section, it does not appear.</p>\n', 'ViewCount': '143', 'Title': 'without triangle inequality, finding good approximate tours for TSP in polynomial time is impossible unless P=NP?', 'LastActivityDate': '2013-10-18T00:46:42.290', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><complexity-theory><traveling-salesman><approximation-algorithms>', 'CreationDate': '2013-10-16T17:31:49.740', 'Id': '16143'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the chapter, <em>Approximation Algorithms</em> of <em>Introduction to Algorithm, 3rd Edition</em>, for the approximation problem <strong>Travelling Salesman Problem</strong>, the author proposes a approximation method that first constructs a minimum spanning tree.</p>\n\n<p><img src="http://i.stack.imgur.com/G9Bvx.png" alt="enter image description here"></p>\n\n<p>In order to prove this algorithm is a 2-approximation algorithm, the author claims that:</p>\n\n<p>The weight of the minimum spanning tree $T$ is less than the cost of the optimal tour.</p>\n\n<p>I am wondering if the minimum spanning tree(which is <strong>acyclic</strong>) of $G$ ensures that its weight is <strong>necessarily</strong> smaller than any tour(which is <strong>cyclic</strong>) of the same graph $G$</p>\n\n<p>PS: </p>\n\n<p>The original claim is:</p>\n\n<p><img src="http://i.stack.imgur.com/MGgcu.png" alt="enter image description here"></p>\n', 'ViewCount': '134', 'Title': 'Approximated TSP: weight of minimum spanning tree less than cost of the optimal tour?', 'LastActivityDate': '2013-10-16T17:49:55.960', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><graph-theory><traveling-salesman><approximation-algorithms>', 'CreationDate': '2013-10-16T17:45:18.160', 'FavoriteCount': '1', 'Id': '16144'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I use Mathematica to solve problems. I have a question about matrix inverse.</p>\n\n<p>if I want only one element of the inverse matrix, does there exist a faster algorithm than using <code>Inverse</code> to calculate the whole inverse matrix and extract the element which I want?</p>\n', 'ViewCount': '109', 'Title': 'if I want only one element of the inverse matrix, does there exist a fast algorithm?', 'LastActivityDate': '2013-10-17T08:43:47.597', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '10808', 'Tags': '<algorithms>', 'CreationDate': '2013-10-17T08:43:47.597', 'Id': '16153'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is one point I don\'t understand in the DFA construction for mismatch cases.</p>\n\n<p>Here is the lecture note I watched, which describes how to handle mismatched characters during the DFA construction process.</p>\n\n<p><img src="http://i.stack.imgur.com/r4Q1Z.png" alt="enter image description here"></p>\n\n<p>Some information about the presentation screen shot:</p>\n\n<p><code>pat</code> is the pattern, string index starts at 0, <code>dfa</code> is the state transition table, in which the row is indexed by the character(eg, "A","B" or "C"), column by state(1,2,3,...).</p>\n\n<hr>\n\n<p><strong>My question starts</strong>:</p>\n\n<p>In the place in the presentation, where starts with "<em>To compute dfa[c][j]</em>", it says run the simulation using the last j-1 chars.</p>\n\n<p>I am confused:</p>\n\n<p>Why run the simulation using the last <code>j-1</code> chars, rather the last <code>j</code> chars. </p>\n\n<p>What\'s the intuition of this design?</p>\n', 'ViewCount': '277', 'Title': u'Confusion about finite automata construction in Knuth\u2013Morris\u2013Pratt algorithm', 'LastEditorUserId': '4662', 'LastActivityDate': '2013-12-01T06:48:14.043', 'LastEditDate': '2013-10-18T19:04:46.790', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><finite-automata><strings><matching>', 'CreationDate': '2013-10-18T09:15:06.677', 'FavoriteCount': '1', 'Id': '16195'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>To find the maximum flow in a graph, why doesn't it suffice to only saturate all augmenting paths with the minimum edge capacity in that path without considering the back-edges? I mean, what is the point calling it a back-edge if we assume flow from it?</p>\n", 'ViewCount': '52', 'Title': 'Saturating all augmenting paths with the minimum edge capacity in max flow', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-18T21:07:48.467', 'LastEditDate': '2013-10-18T21:07:48.467', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10525', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs><network-flow>', 'CreationDate': '2013-10-18T17:15:17.717', 'Id': '16202'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Problem Statement:\nSuppose we a thousands of words and we need to maintain these words in a data structure in such a way that we should be able to find all anagrams for a given string.\nI tried to achieve this with O(1) complexity.</p>\n\n<p>I am looking for a algorithm to implement above scenario. I implemented this problem with below algo, but i feel that we can improve its complexity. Any suggestion will be helpful.</p>\n\n<p>Algorithms:</p>\n\n<p>Here is trick to utilize hash code, we can also use character histogram.</p>\n\n<p>Step 1:Create an array of prime numbers.</p>\n\n<pre><code>   int primes[] = {2, 3, 5, 7, ...};\n\n   We are using prime number to avoid false collisions.\n</code></pre>\n\n<p>Step 2:Create a method to calculate hash code of a word\\string.</p>\n\n<pre><code>   int getHashCode(String str){\n     int hash = 31;\n     for(i =0 to length of str){\n        hash = hash*primes['a' - str.charAt[i]];\n     }\n     return hash;\n   }\n</code></pre>\n\n<p>Step 3: Now store all words in a HashMap.</p>\n\n<p>void loadDictionary(String[] words){</p>\n\n<pre><code>  for( word from words for i = 0 to length of words)   {\n     int hash  = getHashCode(word);\n     List&lt;String&gt; anagrams = dictionary.get(hash);\n     if(anagrams ! = null){\n         anagrams.add(word);\n     } else\n        List&lt;String&gt; newAnagrams = new ArrayList&lt;String&gt;();\n        newAnagrams.add(word);\n        dictionary.put(hash, newAnagrams);\n     }\n }\n}\n</code></pre>\n\n<p>Step 4: Now here is the approach to find anagrams:</p>\n\n<pre><code>   int findNumberOfAnagrams(String str){\n\n   List&lt;String&gt; anagrams = dictionary.get(getHashCode(str));\n      return anagrams.size();\n\n   }\n</code></pre>\n", 'ViewCount': '635', 'Title': 'Algorithm to write a dictionary using thousands of words to find all anagrams for a given string with O(1) complexity', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-19T14:15:13.607', 'LastEditDate': '2013-10-19T11:43:38.863', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10848', 'Tags': '<algorithms><complexity-theory><time-complexity><strings>', 'CreationDate': '2013-10-19T07:27:25.860', 'FavoriteCount': '1', 'Id': '16221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to minimize the sum of difference of element in sub-sequence of array of length k from given sequence of length n ?</p>\n\n<p>for example : for n=10\n1\n2\n3\n4\n10\n20\n30\n40\n100\n200</p>\n\n<p>the sub-sequence of length will with minimized sum of difference will be\n1 2 3 4\nas  |1-2| + |1-3| + |1-4| + |2-3| + |2-4| + |3-4| = 10 i.e minimum in any sequence.</p>\n', 'ViewCount': '181', 'Title': 'How to minimize the sum of difference of element in sub-sequence of array of length k from given sequence of length n', 'LastActivityDate': '2013-10-20T19:17:17.783', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10851', 'Tags': '<algorithms><dynamic-programming><linear-programming><greedy-algorithms><constraint-programming>', 'CreationDate': '2013-10-19T08:38:14.163', 'Id': '16224'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to know which algorithm is fastest for multiplication of two n-digit numbers?\nSpace complexity can be relaxed here! </p>\n', 'ViewCount': '1423', 'Title': 'What is the fastest algorithm for multiplication of two n-digit numbers?', 'LastActivityDate': '2013-11-03T08:50:13.817', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '16228', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '10700', 'Tags': '<algorithms><mathematical-programming>', 'CreationDate': '2013-10-19T12:49:57.087', 'FavoriteCount': '1', 'Id': '16226'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://courses.csail.mit.edu/6.046/spring04/handouts/prac-quiz2-sol.pdf" rel="nofollow">http://courses.csail.mit.edu/6.046/spring04/handouts/prac-quiz2-sol.pdf</a></p>\n\n<p>I\'m confused as to the solution for the snowball question. To start with, I have two specific questions:</p>\n\n<p>(1) Each pair $a_i,b_j$ will account for one term (and why ONE term)? What is meant by term here? The coefficient, $c_k$ of the polynomial C? Or maybe the $x$ value at the $kth$ spot?</p>\n\n<p>(2) Why is $c_k$ the number of such pairs?</p>\n', 'ViewCount': '50', 'Title': 'Snowball Question FFT', 'LastEditorUserId': '10599', 'LastActivityDate': '2013-10-19T23:11:09.083', 'LastEditDate': '2013-10-19T22:37:55.200', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16242', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10599', 'Tags': '<algorithms><fourier-transform><polynomials>', 'CreationDate': '2013-10-19T22:30:58.810', 'Id': '16240'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In my world there are nodes and lines, I want to see if there is any path between node A, and node B, that do not cross any line(including the lines of the path itself) and do not go thru the same node twice. What would be the efficiency of such algorithm and where can I find more information about it.</p>\n\n<p>So far I been reading about <a href="https://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* algorithm</a>, but I do not need the shortest path, just to see if there is any path like this exist at all.</p>\n', 'ViewCount': '86', 'Title': 'Any path detecting between A to B', 'LastActivityDate': '2013-10-20T18:39:42.557', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '16246', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<search-algorithms>', 'CreationDate': '2013-10-20T05:36:47.547', 'FavoriteCount': '1', 'Id': '16245'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is the problem, given a string with characters from: <code>a-z</code>, <code>.</code>, <code>*</code>, and another string with characters from <code>a-z</code>. where <code>*</code> can delete the character before it, otherwise <code>*</code> is skipped and <code>.</code> can match any single character. the question is whether the first string can match the second one.</p>\n\n<p><strong>Note:</strong> That is the statement of the problem as I found, but in this case the character <code>*</code> performs the same function that <code>?</code> in a regular expression.</p>\n\n<p>Example:</p>\n\n<pre><code>isMatch("a*", "") = true; //"a*" could be "a" or an empty string ""\nisMatch(".", "") = false; \nisMatch("ab*", "a") = true; \nisMatch("a.", "ab") = true; \nisMatch("a", "a") = true;\n</code></pre>\n\n<p>I\'ve already solved this problem using a slightly modified edit distance, which I only know a 2D dynamic programming approach. I wonder whether exists a linear solution for this problem, maybe it is solvable without a dp approach?</p>\n', 'ViewCount': '75', 'Title': 'exact matching between two strings - linear edit distance?', 'LastActivityDate': '2013-10-21T22:57:59.747', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16263', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10254', 'Tags': '<algorithms><dynamic-programming><regular-expressions><strings>', 'CreationDate': '2013-10-20T17:52:06.507', 'FavoriteCount': '1', 'Id': '16260'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Say you have two polynomials: $3 + x$ and $2x^2 + 2$.</p>\n\n<p>I'm trying to understand how FFT helps us multiply these two polynomials. However, I can't find any worked out examples. Can someone show me how FFT algorithm would multiply these two polynomials. (Note: there is nothing special about these polynomials, but I wanted to keep it simple to make it easier to follow.) </p>\n\n<p>I've looked at the algorithms in pseudocode, but all of them seem to be have problems (don't specify what the input should be, undefined variables). And surprisingly, I can't find where anyone has actually walked through (by hand) an example of multiplying polynomials using FFT.</p>\n", 'ViewCount': '464', 'Title': 'Show how to do FFT by hand', 'LastActivityDate': '2013-10-21T01:34:40.843', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16272', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '10599', 'Tags': '<algorithms><fourier-transform><divide-and-conquer>', 'CreationDate': '2013-10-20T19:59:33.633', 'Id': '16266'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '708', 'Title': 'Shortest non intersecting path for a graph embedded in a euclidean plane (2D)', 'LastEditDate': '2013-12-10T03:44:41.487', 'AnswerCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '2755', 'FavoriteCount': '7', 'Body': '<p>What algorithm would you use to find the shortest path of a graph, which is embedded in an euclidean plane, such that the path should not contain any self-intersections (in the embedding)?</p>\n\n<p>For example, in the graph below, you want to go from $(0,0) \\rightarrow (-3,2)$. Normally, an algorithm like Dijkstra\'s algorithm would produce a sequence like:</p>\n\n<p>$$\\left[ (0,0) \\stackrel {3}{\\rightarrow} (0,3) \\stackrel{\\sqrt{2}}{\\rightarrow} (1,2) \\stackrel{4}{\\rightarrow} (-3,2) \\right] = 7+\\sqrt{2}.$$</p>\n\n<p>Full graph:</p>\n\n<p><img src="http://i.stack.imgur.com/l3OkDm.jpg" alt="enter image description here"></p>\n\n<p>Shortest path:</p>\n\n<p><img src="http://i.stack.imgur.com/HwDQtm.jpg" alt="enter image description here"></p>\n\n<p>Shortest non-intersecting path:</p>\n\n<p><img src="http://i.stack.imgur.com/sIj1wm.jpg" alt="enter image description here"></p>\n\n<p>However, this path intersects itself on the euclidean plane, therefore I <em>want</em> an algorithm that would give me the shortest non-intersecting sequence, in this case:</p>\n\n<p>$$\\left[(0,0) \\stackrel{3}{\\rightarrow} (0,3) \\stackrel{3}{\\rightarrow} (0,6) \\stackrel{5}{\\rightarrow} (-3,2) \\right] = 11.$$</p>\n\n<p>This path is longer than the shortest path, but it is the shortest non-intersecting path.</p>\n\n<p><strong>Is there an (efficient) algorithm that can do this?</strong></p>\n\n<h2>TikZ sources</h2>\n\n<ul>\n<li><a href="https://www.writelatex.com/read/jzkhmqgmqcnj" rel="nofollow">Full graph</a>.</li>\n<li><a href="https://www.writelatex.com/read/kwpwzzcfyjcy" rel="nofollow">Shortest path</a>.</li>\n<li><a href="https://www.writelatex.com/read/qzrbvwxgpbtp" rel="nofollow">Shortest non-intersecting path</a>.</li>\n</ul>\n', 'Tags': '<algorithms><graphs><shortest-path><graph-traversal><weighted-graphs>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-12-10T03:44:41.487', 'CommentCount': '20', 'AcceptedAnswerId': '16281', 'CreationDate': '2013-10-20T23:27:17.370', 'Id': '16269'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a string $(a_0,a_1,\\ldots a_n)$. I want to find the length of the longest common prefix of the substrings $(a_0,a_1,\\ldots a_{n-1})$ and $(a_1,a_2,\\ldots a_n)$. I know this has atleast $O(n)$ complexity. Lets call this operation as <em>prefix-suffix match</em>.</p>\n\n<p>I want to calculate <em>prefix-suffix match</em> length for all suffixes of a string. Now the naive algorithm which doesn't take into account that all the strings for which I am doing this operation are related has $O(n^2)$ complexity. Now my question is can we do this in better complexity. </p>\n\n<p>Note that what I want is similar to LCP array of a slightly modified suffix array. Where the suffixes are sorted based on length instead of lexicographic ordering. </p>\n", 'ViewCount': '98', 'Title': 'String matching for all suffixes', 'LastActivityDate': '2013-10-24T16:07:22.660', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10915', 'Tags': '<algorithms><strings><suffix-array>', 'CreationDate': '2013-10-22T10:41:46.603', 'Id': '16328'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I'm interested in finite state automata which have the capacity to require repetition.  That is, the machine may be in a state in which the next character may be any character from set $S$, but, whatever character there is, it must be repeated 3 times.  Or it may be in a different state, which requires a character from set $T$ repeated twice.</p>\n\n<p>I could in theory make a different state for each character in set S, and for each character in set T.  But that overcomplicates things and obscures the pattern - it can be any character in set S, but the same character must be repeated.</p>\n\n<p>Is there any standard approach to this? I'm interested both in terminology and in practical code.</p>\n\n<hr>\n\n<p>Motivation: I do not know the full makeup of the set $S$ or $T$.  I'd like to be able to communicate effectively about the FSM, draw state diagrams, and do implementation, without having to define $S$ or $T$.  In fact, $S$ and $T$ change based on the scenario.  (Now, given a character, it's trivial to tell if it's part of $S$ or $T$ \u2014 but, a priori, it's impossible to enumerate them.)</p>\n\n<p>At the least, I'd like a good way to draw a state diagram for these types of machines.  Perhaps I should use a standard FSM with some type of annotation? Could state charts help with this?</p>\n", 'ViewCount': '85', 'Title': 'Recognizing finite state machines with repetition', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-24T19:06:19.023', 'LastEditDate': '2013-10-23T04:04:41.797', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10923', 'Tags': '<algorithms><regular-languages><formal-grammars><finite-automata>', 'CreationDate': '2013-10-22T23:13:36.837', 'Id': '16343'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been stuck with this problem for 2 weeks. Any idea of how to aproach it?.</p>\n\n<blockquote>\n  <p>Let $L$ be a list of $n$ different integer numbers, assume that the elements of $L$ are in the range $[1,750]$. Design a linear ordering algorithm to order the elements of $L$.</p>\n</blockquote>\n\n<p>I already tried with insertion sort. But I'm not sure if my approach is right:</p>\n\n<ul>\n<li>Construct an array of bits. Initialize them to zero.</li>\n<li>Read the input, for each value you see set the respective bit in the array to 1.</li>\n<li>Scan the array, for each bit set, output the respective value.</li>\n</ul>\n\n<p>Complexity: $O(2n) = O(n)$</p>\n\n<p>I also wanted to use radix sort but I can't understand how to apply it, any idea?</p>\n", 'ViewCount': '146', 'Title': 'Sorting in O(n) time in a finite domain', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-23T10:11:12.493', 'LastEditDate': '2013-10-23T10:11:12.493', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><algorithm-analysis><time-complexity><sorting><radix-sort>', 'CreationDate': '2013-10-23T06:11:54.673', 'Id': '16350'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an Euclidean, undirected graph: each vertex is a point on the 2D plane, so the weight of each edge is the Euclidean distance between the vertices.</p>\n\n<ul>\n<li>The number of vertices with no edges is small in comparison to the total number of vertices. </li>\n<li>No vertex has more than two edges connected to it.</li>\n</ul>\n\n<p>How can I convert this graph to a closed shape, so each vertex will have exactly two edges connected to it and there will be a path between any two vertices? I want to do it with minimum change in the total weight of all the edges.</p>\n', 'ViewCount': '126', 'Title': 'Converting graphs to sets of paths', 'LastEditorUserId': '917', 'LastActivityDate': '2013-12-10T03:14:40.417', 'LastEditDate': '2013-12-10T03:14:40.417', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><graphs><graph-traversal><weighted-graphs>', 'CreationDate': '2013-10-23T07:11:44.910', 'Id': '16351'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have an Euclidean graph: each vertex is a point on the 2D plane, so the weight of each edge is the Euclidean distance between the vertices, also all the vertices are connected with edges.</p>\n\n<p>I want to change the vertices values, so that the average ratio between the length of two edges (taken over all pairs of edges), will be as small as possible, I want to keep just one rule. For any vertex on the graph, it's neighbors sorted by Euclidean distance from that vertex, must remind in the same order.</p>\n\n<p>The idea behind this, if solution could be provided, that it will be possible to treat groups of vertices as one. It will reduce the difficulty of solving TSP.</p>\n", 'ViewCount': '64', 'Title': 'Trim graph to minimum', 'LastEditorUserId': '6890', 'LastActivityDate': '2013-10-23T13:50:52.117', 'LastEditDate': '2013-10-23T13:50:52.117', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><graphs><graph-traversal><traveling-salesman><weighted-graphs>', 'CreationDate': '2013-10-23T12:39:35.833', 'Id': '16361'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I understood that we can apply <a href="http://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme" rel="nofollow" title="fully polynomial-time approximation scheme">FPTAS</a> to the weak NP problems like 0-1 knapsack.</p>\n\n<p>But why we cant apply the same principle to the strong NP problems like bin packing? I also checked wiki page about the same but understood very less.</p>\n', 'ViewCount': '225', 'Title': "Why we can't have FPTAS for strong NP complete problems", 'LastActivityDate': '2013-10-23T21:56:48.887', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'OwnerDisplayName': 'user2159588', 'PostTypeId': '1', 'OwnerUserId': '10944', 'Tags': '<algorithms><np-complete>', 'CreationDate': '2013-10-22T06:25:49.193', 'Id': '16365'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let:</p>\n\n<ul>\n<li>$\\left\\{m_1, ~...~, m_k\\right\\}$ be a set of coprime natural numbers,</li>\n<li>$M=\\prod_{i=1}^{k} m_i$</li>\n<li>$X$ be a natural integer, such that $X &lt; M$</li>\n</ul>\n\n<p>Then $X$ can be expressed in the <a href="http://en.wikipedia.org/wiki/Residue_number_system" rel="nofollow">Residue Number System</a> as:</p>\n\n<p>$$X={\\left(x_1, ~...~, x_k\\right)}_{RNS\\left(m_1, ~...~, m_k\\right)}~.$$</p>\n\n<p>Where $\\forall_{m_i} \\left[\\left(x_i \\equiv X \\mod m_i\\right) ~~~ \\wedge ~~~0 \\le x_i &lt; m_i \\right]$.</p>\n\n<p>There are a plethora of papers attacking the problem of parity/magnitude comparison in <em>Residue Number Systems</em>; however many of these papers are focused on chip-depth or shaving large constants off of chip-area. I am finding it difficult to decipher if there are any exact algorithms that run faster than full binary reconstruction, which takes $\\sim \\mathcal{O}(k^2)$  time. (For simplicty/brevity, I am assuming small $m_i$, and constant-time modulo-multiplication/addition of each RNS "digit").</p>\n\n<p>Most of the papers\' novelties lie in some seeming "gimmick", but no real complexity decrease; examples of results:</p>\n\n<ul>\n<li>Fast inexact parity algorithms (usually work terribly when $X &lt; \\sqrt{M}$, or $|X| \\ll |M|$, where $\\left|n\\right|=\\text{size of }n=\\left\\lceil\\log_2n\\right\\rceil$)</li>\n<li>Algorithms that work quickly "most of the time"; ie. they use an inexact algorithm, and then the full CRT reconstruction or equivalent in the worst case</li>\n<li>The circuit they present competes with some other paper\'s circuit by some constant, or area/depth tradeoff, but makes no complexity advance</li>\n<li>Full CRT reconstruction of $X$, perhaps using some trick to save some constants</li>\n<li>Reconstruct/convert to another number system (including binary) where parity/comparison is easy, but:\n<ul>\n<li>this conversion/reconstruction takes $\\sim \\mathcal{O}(k^2)$ time,</li>\n<li>or it runs in $\\sim \\mathcal{O}(k)$, time but with $k$ processors,</li>\n<li>or it runs in $\\sim \\mathcal{O}(k)$ time because that is the depth of the circuit, but this is not algorithmic complexity,</li>\n<li>or it reuses previous components that must be there for RNS multiplication (saving circuit space), but still runs in $\\sim \\mathcal{O}(k^2)$ sequential-time, or $\\sim \\mathcal{O}(k)$ parallel-time</li>\n</ul></li>\n<li>Using a "core" function which basically boils down to a constant-trimmed-CRT, or an approximate CRT</li>\n<li>Using special moduli, makes individual operations simpler, but parity complexity stays the same</li>\n<li>Using special moduli, but limited number of moduli or can\'t have small $m_i$</li>\n<li>Base extension, saves some constant or allows parallel-ness, but complexity is again $\\sim \\mathcal{O}(k^2)$ sequential-time, or $\\sim \\mathcal{O}(k)$ parallel-time (for multiplication)</li>\n<li>Redundant moduli, but maintaining the redundant moduli takes  $\\sim \\mathcal{O}(k^2)$ sequential-time, or $\\sim \\mathcal{O}(k)$ parallel-time</li>\n<li>Using lookup tables to reduce depth of some parity, no complexity improvement</li>\n</ul>\n\n<p>Many of the papers do not address complexity at all, or do not address sequential complexity, or even more confusingly, some state the depth/parallel complexity without being precise that it is not sequential; until you read and decipher the entire paper, and discover it yourself.</p>\n\n<h3>Bottom line</h3>\n\n<p><strong>What are the best <em>sequential</em>, <em>worst-case</em>, complexity results in RNS<sup>*</sup> for <em>exact</em> parity checking or magnitude comparison?</strong></p>\n\n<p><sup><strong>*Results for RNS-<em>like</em> system would also be interesting, including special moduli sets</strong></sup></p>\n\n<hr>\n\n<p><sup><sup>\n<strong>More background info</strong>:\n</sup></sup></p>\n\n<p><sup><sup> Multiplication of two numbers in the same RNS base is simply pointwise modulo multiplication of the two numbers (this can be approximately linear time). However, overflow detection is difficult (it is difficult with addition as well). Multiplication seems much simpler, but parity and magnitude comparison of two numbers seems much more difficult. Magnitude comparison is simply determining which of two numbers is greater, $X \\stackrel{?}{&lt;} Y$, given <em>only</em> their RNS form with the same RNS bases. Parity is simply deciding if a number, $X={\\left(x_1, ~...~, x_k\\right)}_{RNS\\left(m_1, ~...~, m_k\\right)}$ is even or odd (obviously, $X$ is not given, only its RNS form). An interesting thing is that magnitude comparison and parity are related: If you were able to compute parity, then you can do comparison. To do comparison with parity, you do $(X - Y)$ (in RNS), and if it underflows, the parity will be unexpected. That is, normally, assuming $p(X) = X \\mod 2, ~~~ p(X) \\in \\{0,1\\}$ is the parity function, $p(X-Y) \\equiv p(X) + p(Y) \\mod 2$. However, if it underflows, it will wrap around to $M-1$. Therefore if the parity is off after $X-Y$, you know that $Y &gt; X$.\n</sup></sup></p>\n', 'ViewCount': '208', 'Title': 'Best complexity of parity/comparison in the Residue Number System', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-23T23:32:04.833', 'LastEditDate': '2013-10-23T23:32:04.833', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><reference-request><integers><number-theory>', 'CreationDate': '2013-10-23T21:28:12.190', 'Id': '16374'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Ok, I must admit this a task from my studies but I\'m stuck.</p>\n\n<p>Rules of the game are on <a href="https://en.wikipedia.org/wiki/Hey!_That%27s_My_Fish!" rel="nofollow">Wikipedia.</a> However, we have a modified version: on input we have a board with x fields, there are y penguins on some fields, there may be gaps between fields. All penguins are the same, they can move only to the farthest possible location (in a straight line, can\'t jump over gaps and other penguins). There is one player and he needs to move the penguins in such manner so as to get the maximal number of fish. I have to come up with an algorithm that finds the sequence of moves that leads to the best possible result.</p>\n\n<p>I tried something like guessing the final arrengment of the fields (f.e. all the penguins left on fields with just one fish each) and then reverse-engineer the steps but I didn\'t go any further with my idea.</p>\n\n<p>I also thought it would be probably good to represent this as a graph, possibly with weighted edges and then do some graph search but again, I don\'t know what next.</p>\n\n<p>Finally, there was a thought about using Monte Carlo method. However, this just seems too hardcore for a class project like this and that would be too diffult for me.</p>\n\n<p>Any ideas where should I start?</p>\n', 'ViewCount': '65', 'Title': 'Algorithm playing modification of "Hey, That\'s My Fish!" game', 'LastActivityDate': '2013-10-23T22:03:08.947', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16380', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10941', 'Tags': '<algorithms><board-games>', 'CreationDate': '2013-10-23T21:46:21.323', 'Id': '16377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following problem:</p>\n\n<p>There are $n$ points in the plane.\nStarting from one of them I want to visit each of them once (except the starting node which has to be visited twice) but in a way that minimizes the cost of the total path.</p>\n\n<p>The weight of each edge changes depending on the path followed.\nFor example imagine $n=3$: $A$, $B$, $C$ and we start at $A$. \nThe weight of the edge $xy$ is $Sd_{xy}$, where $d_{xy}$ is the distance between the points $x$ and $y$ and $S$ is a given constant)\nIf I pick the edge $A\\to B$, the weight of the edge $B\\to A$ is now\n$(S-s_B)d_{AB}$ because $B$ changes $S$ by a constant amount $s_B$. \nSimilarly if I pick the edge $A\\to C$, the weight of the edge $B\\to C$ is now\n$(S-s_C)d_{AC}$ because visiting $C$ changes $S$ by a constant amount $s_C$. </p>\n\n<p>Developing the $n=3$ case, imagine S=11, $d_{AB}=5,d_{AC}=3,d_{BC}=4$ and $s_{A}=2,s_{B}=5,s_{C}=4$.\nThen there are 4 possible paths:</p>\n\n<p>A->B->C->A with cost $11*5+(11-5)*4+(11-5-4)*3$</p>\n\n<p>(we stop once we reach A because 11-5-4-2=0)</p>\n\n<p>A->B->A->C with cost $11*5+(11-5)*5+(11-4-2)*3$</p>\n\n<p>A->C->B->A with cost $11*3+(11-4)*4+(11-4-5)*5$</p>\n\n<p>A->C->A->B with cost $11*3+(11-4)*3+(11-4-2)*5$</p>\n\n<p>For $n=4$ there will be 3*3!=18 possible paths and so on.</p>\n\n<p>I know that $s_A+s_B+s_C=S$. This generalizes for $n$ points.\n What is an efficient algorithm for finding the minimum cost path?</p>\n', 'ViewCount': '186', 'Title': 'minimum cost path', 'LastEditorUserId': '10946', 'LastActivityDate': '2013-10-25T20:37:02.393', 'LastEditDate': '2013-10-25T20:37:02.393', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10946', 'Tags': '<dynamic-programming><greedy-algorithms>', 'CreationDate': '2013-10-24T01:54:35.210', 'FavoriteCount': '1', 'Id': '16391'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it practically possible or even near possible to make parity game to be solved in polynomial time? If yes, how? and if No, why?</p>\n', 'ViewCount': '113', 'Title': 'Is it possible to solve parity game problem in polynomial time?', 'LastActivityDate': '2013-10-24T15:35:31.673', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16401', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10700', 'Tags': '<algorithms><algorithm-analysis><polynomial-time>', 'CreationDate': '2013-10-24T14:53:19.353', 'Id': '16399'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have $k$ lists that I would like to combine to a single list. Each list's elements are unique and are sorted in a particular order, but there is no notion of an absolute order, and different items across lists can only be compared if they are equal. To give a specific example (here I use $&lt;$ to indicate the sorting inside each list):</p>\n\n<pre><code>l1 = a &lt; b &lt; c\nl2 = b &lt; d\n</code></pre>\n\n<p><code>l1 + l2</code> should result in one of the following (and I don't care which one it results in):</p>\n\n<pre><code>a &lt; b &lt; c &lt; d\na &lt; b &lt; d &lt; c\n</code></pre>\n\n<p>If the lists are incompatible, e.g. <code>a &lt; b</code> and <code>b &lt; a</code> then I want to get an error. The resulting list (if it exists) should respect all of the orderings of the sublists. Also all the items should be unique in the result. </p>\n\n<p>I thought about modifying k-way merge somehow but it's not clear to me how to do that without a global order. By the way, I'll probably be happy with a $O(nk)$ solution.</p>\n\n<hr>\n\n<p>Perhaps a better way to phrase the problem is this. I'd like to construct an order vector given $k$ pieces of that order vector (or get an error if that's impossible).</p>\n\n<p>Here's an algorithm that I think works for $k=2$. Find and mark all the duplicates - I think this is $O(n)$ - then start writing down elements from first list until you hit a duplicate, once you do add all elements from second list until you hit that same duplicate, then write down the duplicate and continue. The part where it's a bit more tricky with $k&gt;2$ is that the duplicate may not be in every list to do the above (and this process cannot be done sequentially, i.e. <code>l1+l2+l3</code> is generally not the same as <code>(l1+l2)+l3</code>, where <code>+</code> denotes this operation of finding the superset order list).</p>\n\n<p>It should be easy to extend the above to any $k$. Use a hash to find which lists each unique element belongs to. Then traverse along the first list using the above logic - writing down all elements until you hit first duplicate, in which case write down all elements (that haven't been written down yet) in all other lists before that duplicate and keep doing that until you reach the end of first list. If haven't reached the end of second list, continue same algorithm there and so on. Each list will be traversed twice - once to get the duplicates, and second time to do the above, making it $O(n)$. A compatibility error will be discovered if you iterate along a list but can't find the duplicate (because it was already written down in an earlier pass).</p>\n", 'ViewCount': '65', 'Title': '$k$-way merge but without an absolute order', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-26T09:38:28.027', 'LastEditDate': '2013-10-26T09:38:28.027', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '16413', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10958', 'Tags': '<algorithms>', 'CreationDate': '2013-10-24T21:39:18.260', 'Id': '16411'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following problem:</p>\n\n<blockquote>\n  <p>Give an algorithm to find the $1^{st}, 2^{nd}, 3^{th}$ fastest horses from 25 horses. In each round, at most 5 horses can race and you can get the exact position of these horses. Analyze the lower bound of this problem using <em>adversary argument</em>. One race is considered as one critical operation.</p>\n</blockquote>\n\n<p>I can figure out a solution using 7 races:</p>\n\n<blockquote>\n  <ol>\n  <li>Divide 25 horses into 5 groups with 5 horses for each: $A: a_1, a_2, a_3, a_4, a_5$; $B: b_1, b_2, b_3, b_4, b_5$; $C: c_1, c_2, c_3, c_4, c_5$; $D: d_1, d_2, d_3, d_4, d_5$; and $E: e_1, e_2, e_3, e_4, e_5$.</li>\n  <li>One race within each group. Suppose that the position of each horse in each group is consistent with its index: e.g., $A: a_1 &gt; a_2 &gt; a_3 &gt; a_4 &gt; a_5$.</li>\n  <li>One race for $a_1, b_1, c_1, d_1, e_1$ and get $a_1 &gt; b_1 &gt; c_1 &gt; d_1 &gt; e_1$. Thus, $a_1$ is the fastest horse.</li>\n  <li>The second and third ones are among $a_2, a_3, b_1, b_2, c_1$. So one more race is enough.</li>\n  </ol>\n</blockquote>\n\n<p>However I have difficulty in analyzing its lower bound using adversary argument.\nSo my problem is:</p>\n\n<blockquote>\n  <p>How to analyze its lower bound using <em>adversary argument</em>? What is the adversary strategy?</p>\n</blockquote>\n', 'ViewCount': '219', 'Title': 'How to analyze the lower bound of the horse racing problem using adversary argument?', 'LastActivityDate': '2013-10-25T07:05:30.847', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16420', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithms><algorithm-analysis><lower-bounds>', 'CreationDate': '2013-10-25T06:05:44.517', 'Id': '16417'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an Euclidean graph: each vertex is a point on the 2D plane, so the weight of each edge is the Euclidean distance between the vertices.</p>\n\n<p>I am randomly creating a path thru all the vertices and I want to know if there is any efficient way to find all the intersections on my path.</p>\n', 'ViewCount': '124', 'Title': 'Efficient way to find intersections', 'LastActivityDate': '2013-10-25T07:39:59.137', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16422', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><graphs><graph-traversal><weighted-graphs>', 'CreationDate': '2013-10-25T06:16:51.257', 'Id': '16418'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a table in which some values are repeated often as shown in the figure below. I want to encode that table such that it makes use of less memory. I have heard about run length encoding (RLE) but I would like to know if there are any other such encoding techniques or algorithms which can perform better than RLE or their performance is almost equivalent to RLE.</p>\n\n<p><img src="http://i.stack.imgur.com/rGPpF.png" alt="enter image description here"></p>\n', 'ViewCount': '83', 'Title': 'Encoding algorithms better than or equivalent to Run Length Encoding', 'LastEditorUserId': '4624', 'LastActivityDate': '2013-10-26T04:32:08.603', 'LastEditDate': '2013-10-26T04:32:08.603', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4624', 'Tags': '<algorithms><encoding-scheme>', 'CreationDate': '2013-10-25T20:55:20.093', 'Id': '16431'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an Euclidean graph: each vertex is a point on the 2D plane, so the weight of each edge is the Euclidean distance between the vertices.  I found a <a href="http://tsp-vs-world.blogspot.co.il/2013/10/intersections-optimal-tsp-route.html" rel="nofollow">geometric proof</a> that every optimal TSP solution contains no intersections.</p>\n\n<p>How many non-intersecting routes could be there? Or in other words: what is the probability to guess an optimal solution to a TSP problem if we just enumerate or sample non-intersecting routes?</p>\n\n<p><strong>Edit:</strong> I want to ignore the case that D.W. mentioned. For every path that you can swap between two neighbors vertices(If we represent the path as an array of vertices so neighbors will be two vertices with consecutive indexes) without changing its non-intersecting quality, all of those paths will be considered as one.</p>\n\n<p><strong>Edit</strong> I found that this kind of removing crossings from the graph also know as <a href="http://en.wikipedia.org/wiki/2-opt" rel="nofollow">2-OPT</a> </p>\n', 'ViewCount': '236', 'Title': 'Calculating the number of non-intersecting routes in an Euclidean graph', 'LastEditorUserId': '10572', 'LastActivityDate': '2013-11-03T12:37:57.193', 'LastEditDate': '2013-11-03T12:37:57.193', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><graphs><graph-traversal><traveling-salesman><weighted-graphs>', 'CreationDate': '2013-10-26T09:50:01.253', 'FavoriteCount': '1', 'Id': '16439'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I can't answer this question. It seems simple but I really don't know how to approach it.\nHere it is:</p>\n\n<p>A priority queue is said to be stable if deletions of items with equal priority value occur in the order in which they were inserted. Which of the following priority queue structures are stable:</p>\n\n<ul>\n<li>linked lists ordered in increasing priority (key)</li>\n<li>balanced search trees (e.g., 2-3 trees)</li>\n<li>heaps</li>\n<li>leftist heaps</li>\n</ul>\n\n<p>Explain why, or give counter-examples.</p>\n\n<p>I don't need a full solution just a way to approach this problem. I would prefer to solve it on my own.</p>\n\n<p>Any help is appreciated.</p>\n", 'ViewCount': '125', 'Title': 'Stable priority queue', 'LastActivityDate': '2013-10-27T18:18:07.780', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16480', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10511', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-10-26T22:42:54.990', 'Id': '16456'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a tree $T$ and a sequence of nodes $S$, with the only constraint on $S$ being that it's done through some type of recursion - that is, a node can only appear in $S$ if all of its ancestors have already appeared, what's a good algorithm to determine if $S$ is a breadth first visit, a depth first visit, or neither?</p>\n\n<p>A brute force approach is to compute every breadth first and depth first sequences and see if any is identical to $S$.  Is there a better approach?</p>\n\n<p>What if we don't want a yes or no answer, but a measure of distance.  E.g. $breadth &lt; S &lt; R &lt; U &lt; depth$; what's a good algorithm to determine distance? (A brute force approach for this is just $max(length(longest{\\_}common{\\_}subsequence(S, breadth))/length(breadth))$ over every BFS - again, is there a better way to do this?)</p>\n", 'ViewCount': '167', 'Title': 'Algorithm to determine if recursion was breadth first or depth first', 'LastEditorUserId': '10925', 'LastActivityDate': '2014-02-24T22:45:29.007', 'LastEditDate': '2013-10-27T11:20:32.830', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10925', 'Tags': '<algorithms><trees><recursion><discrete-mathematics>', 'CreationDate': '2013-10-27T00:54:38.727', 'FavoriteCount': '1', 'Id': '16459'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Theoretical Computer Science, which one is more important? Computability of a problem or Complexity of a problem?</p>\n', 'ViewCount': '72', 'ClosedDate': '2013-10-27T15:58:25.267', 'Title': 'Computability vs Complexity?', 'LastActivityDate': '2013-10-27T14:05:27.103', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '16471', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10700', 'Tags': '<algorithms><complexity-theory><computability>', 'CreationDate': '2013-10-27T10:35:47.770', 'Id': '16466'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Edit:</strong> found solution with $O(1)$ look for my answer at <a href="http://programmers.stackexchange.com/a/216217/104450">here</a> </p>\n\n<p>I have an array of elements, and I wish to reverse the order of elements between indices $i$ and $j$.</p>\n\n<p><strong>Example:</strong> Let the array contain elements $A,B,C,D,E,F$. Calling the function reverse(array, 1,4) will modify the array to be $A,E,D,C,B,F$.</p>\n\n<p>I want to do this operation multiple times on different indices. What would be the most efficient way to implement this (precalculations don\'t count)?</p>\n', 'ViewCount': '456', 'Title': 'How to reverse a subarray of an array', 'LastEditorUserId': '10572', 'LastActivityDate': '2013-11-01T12:27:12.877', 'LastEditDate': '2013-11-01T12:27:12.877', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><arrays>', 'CreationDate': '2013-10-27T11:58:05.780', 'FavoriteCount': '0', 'Id': '16467'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Why is it that Iterative-deepening A* is optimal, even without monotonicity? How can I be sure that the first goal reached is the optimal one?</p>\n', 'ViewCount': '282', 'Title': 'Why is Iterative-deepening A* optimal, even without monotonicity?', 'LastActivityDate': '2013-10-28T20:50:58.197', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '16514', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10575', 'Tags': '<search-algorithms><search-trees>', 'CreationDate': '2013-10-27T17:01:21.627', 'Id': '16477'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $\\Sigma$ be a small, finite alphabet.  Suppose we are given ${n\\choose 2}$ sets $S_{i,j}$, where $S_{i,j} \\subseteq \\Sigma \\times\\Sigma$.  I'd like to determine whether there exists a sequence $x_1,x_2,\\dots,x_n \\in \\Sigma$ such that $(x_i,x_j) \\in S_{i,j}$ for all $i,j$, and if so, find an example of such a sequence.</p>\n\n<p>Are there any good algorithms for this problem?</p>\n\n<p>Also: Suppose I model each set $S_{i,j}$ as a randomly chosen subset of $\\Sigma \\times \\Sigma$ where each of the possible elements is included in $S_{i,j}$ with probability $p$ (independently of everything else).  Thus, the expected size of each $S_{i,j}$ is $p \\cdot |\\Sigma|^2$.  Is there any characterization of the range of values of $p$ for which this problem should be efficiently solvable?</p>\n\n<p>(In my application, $|\\Sigma|=10$, if that helps.)</p>\n\n<p>This looks like some sort of 2-CSP (constraint satisfaction problem, where each constraint is on exactly 2 variables), but I don't know what more we might be able to say.</p>\n", 'ViewCount': '60', 'Title': 'Find sequence, given partial information about all pairs', 'LastActivityDate': '2013-10-28T14:06:30.490', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><constraint-programming>', 'CreationDate': '2013-10-28T14:06:30.490', 'Id': '16498'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '214', 'Title': 'Computing with the Monster', 'LastEditDate': '2013-10-31T08:54:40.460', 'AnswerCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11020', 'FavoriteCount': '1', 'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Monster_group" rel="nofollow">Monster</a> <strong>M</strong> is the largest of the finite sporadic groups that arises in the <a href="http://en.wikipedia.org/wiki/Classification_of_finite_simple_groups" rel="nofollow">classification of finite, simple groups</a> in mathematics. </p>\n\n<p><strong>M</strong> can be realized as a (very large!) set of <code>196882 X 196882</code> matrices with nothing more than entries of 1\'s and 0\'s, so long as we compute arithmetic as follows:</p>\n\n<p><code>1+1=0</code>\n<code>1+0=1+0=1</code>\n<code>1*1=1</code>\n<code>0*0=0</code>\n<code>1*0=0*1=0</code></p>\n\n<p>I have two simple questions for the reader. What is the minimum amount of bytes needed to store a single matrix? What is the computational cost (i.e., in FLOPS) of a single matrix multiplication in the most efficient implementation (i.e., taking into account that the entries are binary, not taking into account mathematical properties about the Monster)?</p>\n\n<p><em>This is a question purely at the computational side of the problem. In other words, treat the matrices as general <code>196882 x 196882</code> matrices with binary entries. This is not a question about the Monster. That was just added as motivation.</em></p>\n', 'Tags': '<algorithms><data-structures>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-31T08:54:40.460', 'CommentCount': '4', 'AcceptedAnswerId': '16694', 'CreationDate': '2013-10-28T20:30:19.927', 'Id': '16513'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a huge list of strings. The goal is to make a database of "relevant" words.</p>\n\n<p>Of course in the context of what I work with, "relevance" is something akin to the organization I work with. For e.g. if it is a automobile manufacturing industry they\'d buy products like </p>\n\n<pre><code>Automotive assembly equipment\nRadial tyres\nEtc\n</code></pre>\n\n<p>If it were an aircraft manufacturing industry:</p>\n\n<pre><code>Fabricated Aluminium Body\nHydraulic Fluid\nSkydrol\nJet Fuel\nEtc...\n</code></pre>\n\n<p>Issue is that I might come across so many spelling mistakes, abbreviations, and variations.</p>\n\n<p>So what is a technique I can prepare a database of words like these when I have nothing to start with, except a huge list of strings.</p>\n', 'ViewCount': '75', 'Title': 'techniques for finding relevant words in a huge list of strings', 'LastActivityDate': '2013-10-31T04:17:31.483', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2374', 'Tags': '<algorithms>', 'CreationDate': '2013-10-29T06:01:17.603', 'Id': '16531'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>According to the wiki-article, <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="nofollow">http://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> the term frequency of term $t$ in document $d$ is given as follows:\n$$\ntf(t,d) = 0.5 + \\frac{0.5 \\times f(t,d)}{\\max{\\{f(w,d): w~\\epsilon~d\\}}}\n$$</p>\n\n<p>This is the augmented way of computation. But what does $w$ stand for?</p>\n', 'ViewCount': '46', 'Title': 'clarification in tf-idf formula', 'LastActivityDate': '2013-10-29T13:46:21.610', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16544', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2374', 'Tags': '<search-algorithms>', 'CreationDate': '2013-10-29T11:51:18.663', 'Id': '16539'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an array of lines in 1D represented by coordination and weight, it can be only positive weight. I want to find all the lines that intersect a point in a given range.</p>\n\n<p>Is there any efficient way of doing it?</p>\n', 'ViewCount': '99', 'Title': 'Lines intersections with a point in 1D', 'LastActivityDate': '2014-01-05T03:04:28.580', 'AnswerCount': '2', 'CommentCount': '8', 'AcceptedAnswerId': '16577', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms>', 'CreationDate': '2013-10-30T08:59:16.970', 'Id': '16570'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '130', 'Title': 'Carry-free multiplication operation', 'LastEditDate': '2013-10-30T17:32:48.187', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'FavoriteCount': '3', 'Body': '<p>In long-multiplication, you shift and add, once for each $1$ bit in the lower number.</p>\n\n<p>Let $r = p \\otimes q$ be an operation similar to multiplication, but slightly simpler: when expressed via long-multiplication, the addition does not carry. Essentially you bitwise-<strong>xor</strong> the shifted numbers.</p>\n\n<p>Like so:</p>\n\n<p>$$\n  \\left[\\begin{matrix}\n    &amp;&amp;p_n &amp; ... &amp; p_i &amp; ... &amp; p_2 &amp; p_1 \\\\\n    &amp;&amp;q_n &amp; ... &amp; q_i &amp; ... &amp; q_2 &amp; q_1 &amp; \\otimes\\\\\n    \\hline\\\\\n    &amp;&amp;q_1 \\cdot p_n &amp; ... &amp; q_1 \\cdot p_i\n      &amp; ... &amp; q_1 \\cdot p_2 &amp; q_1 \\cdot p_1\\\\\n    &amp;q_2 \\cdot p_n &amp; ... &amp; q_2 \\cdot p_i\n      &amp; ... &amp; q_2 \\cdot p_2 &amp; q_2 \\cdot p_1\\\\\n    &amp;&amp;&amp;&amp;&amp;&amp;&amp;...\\\\\n    q_i \\cdot p_n &amp; ... &amp; q_i \\cdot p_i\n      &amp; ... &amp; q_i \\cdot p_2 &amp; q_i \\cdot p_1 &amp; \\stackrel{i}{\\leftarrow}\n      &amp;&amp;{\\Huge{\\oplus}} \\\\\n    \\hline \\\\\n    \\\\r_{2n}&amp; ... &amp; r_i\n      &amp; ... &amp;r_4&amp; r_3 &amp; r_2 &amp;r_1 &amp; =\n  \\end{matrix}\n  \\right]\n$$</p>\n\n<p>Using the long-multiplication-style formulation, this takes $\\mathcal O\\left(\\max\\left(\\left|p\\right|,\\left|q\\right|\\right)^2\\right)=\\mathcal O\\left(\\left|r\\right|^2\\right)$ time. Can we do better? Perhaps we can reuse some existing multiplication algorithms, or even better.</p>\n\n<hr>\n\n<h2>Followup: <a href="http://cs.stackexchange.com/q/16585/2755">Shift-and-or multiplication operation</a></h2>\n', 'Tags': '<algorithms><integers><number-theory><multiplication>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-30T17:32:48.187', 'CommentCount': '4', 'AcceptedAnswerId': '16581', 'CreationDate': '2013-10-30T16:33:37.543', 'Id': '16578'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Continuing in the same vein as <a href="http://cs.stackexchange.com/q/16578/2755">Carry-free multiplication operation</a>, a followup question is as follows (differences in bold):</p>\n\n<p>Let $r = p \\oplus q$ be an operation similar to multiplication, but slightly simpler: when expressed via long-multiplication the columns aren\'t summed up, but rather <strong>or</strong>\'d (not <strong>xor</strong>) together. Nothing is carried.</p>\n\n<p>$$\n  \\left[\\begin{matrix}\n    &amp;&amp;p_n &amp; ... &amp; p_i &amp; ... &amp; p_2 &amp; p_1 \\\\\n    &amp;&amp;q_n &amp; ... &amp; q_i &amp; ... &amp; q_2 &amp; q_1 &amp; \\otimes\\\\\n    \\hline\\\\\n    &amp;&amp;q_1 \\cdot p_n &amp; ... &amp; q_1 \\cdot p_i\n      &amp; ... &amp; q_1 \\cdot p_2 &amp; q_1 \\cdot p_1\\\\\n    &amp;q_2 \\cdot p_n &amp; ... &amp; q_2 \\cdot p_i\n      &amp; ... &amp; q_2 \\cdot p_2 &amp; q_2 \\cdot p_1\\\\\n    &amp;&amp;&amp;&amp;&amp;&amp;&amp;...\\\\\n    q_i \\cdot p_n &amp; ... &amp; q_i \\cdot p_i\n      &amp; ... &amp; q_i \\cdot p_2 &amp; q_i \\cdot p_1 &amp; \\stackrel{i}{\\leftarrow}\n      &amp;&amp;{\\bigvee} \\\\\n    \\hline \\\\\n    \\\\r_{2n}&amp; ... &amp; r_i\n      &amp; ... &amp;r_4&amp; r_3 &amp; r_2 &amp;r_1 &amp; =\n  \\end{matrix}\n  \\right]\n$$</p>\n\n<p>Using the long-multiplication-style formulation, this takes $\\mathcal O\\left(\\max\\left(\\left|p\\right|,\\left|q\\right|\\right)^2\\right)=\\mathcal O\\left(\\left|r\\right|^2\\right)$ time. Can we do better? Perhaps we can reuse some existing multiplication algorithms, or even better.</p>\n', 'ViewCount': '446', 'Title': 'Shift-and-or multiplication operation', 'LastActivityDate': '2013-10-30T19:44:37.417', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><integers><number-theory><multiplication>', 'CreationDate': '2013-10-30T17:35:19.933', 'FavoriteCount': '2', 'Id': '16585'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Its is known that <code>3SAT (all clauses have size 3)</code> is NP-complete. </p>\n\n<p>But how about <code>1SAT (all clauses have size 1)</code>, is it also NP-complete? I tried searching for it a lot, but couldnt find any proofs or explanations for the same.</p>\n\n<p>Like 3SAT has 3 clauses, what if I consider just one clause?</p>\n\n<p>Thanks for the help in advance.</p>\n', 'ViewCount': '156', 'ClosedDate': '2013-11-08T23:03:52.603', 'Title': '3SAT is NP-complete, however is 1SAT NP-complete?', 'LastActivityDate': '2013-11-03T00:24:29.170', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-3', 'PostTypeId': '1', 'OwnerUserId': '11108', 'Tags': '<algorithms><3-sat>', 'CreationDate': '2013-10-31T20:32:00.713', 'Id': '16606'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Some priority queues, like the <a href="https://en.wikipedia.org/wiki/Leftist_tree" rel="nofollow">height-based leftist tree</a> (or <a href="http://www.cse.ohio-state.edu/~gurari/course/cis680/cis680Ch8.html" rel="nofollow">here</a>) support merging in $\\mathcal O\\left(\\log n\\right)$ time.</p>\n\n<p>I am looking for a priority queue that merges in (expected|average|amortized|worst-case) <em>sub-linear</em> time, but also has the following properties:</p>\n\n<ul>\n<li>Elements are unique</li>\n<li><code>peek</code> and <code>pop</code> should work in (expected|average|amortized|worst-case) <em>sub-linear</em> time</li>\n</ul>\n\n<p>Is this impossible?</p>\n', 'ViewCount': '210', 'Title': 'Priority queue with unique elements and sublinear time merge?', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-04T02:12:44.287', 'LastEditDate': '2013-11-01T03:36:00.840', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><data-structures><trees><priority-queues>', 'CreationDate': '2013-11-01T03:30:02.627', 'FavoriteCount': '2', 'Id': '16611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider the next problem. Given a large oriented graph find its diameter: maximal length over all shortest directed paths between every pair of vertices. Graph is strongly connected. It is allowed to give lower and upper bounds on the answer. Time complexity should be linear or close to it.</p>\n\n<p>Suppose I solved the problem and I want to prove it. Moreover I want the proof can be checked in linear time: graph is large. </p>\n\n<p>Lower bound is easy. Just let me give two vertices on neccessary distance. Upper bound is harder. I can give a vertex and ask to estimate the furthest vertex reachable by forward, and then by backward, edges. Then I sum up these distances and call it upper bound.</p>\n\n<p>The problem that my upper bound doesn't look tight. Can anybody suggest something better?</p>\n", 'ViewCount': '143', 'Title': 'Upper bound on graph diameter. Linear certificate', 'LastActivityDate': '2013-11-01T11:34:39.277', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11114', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-11-01T11:34:39.277', 'FavoriteCount': '2', 'Id': '16617'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>At the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Time_complexity" rel="nofollow">time complexity</a>, only a PRAM example is given for polylogarithmic time.</p>\n\n<p>Let $T(n)$ denote the largest number of steps used by a machine to reach a final state on any input with size $n$ bits.</p>\n\n<blockquote>\n  <p>Is there a program for a standard sequential model of computation (e.g. a Turing machine or a sequential random-access machine), solving some natural problem, so that $T(n) \\in \\Theta((\\log n)^k)$ for some fixed $k&gt;1$?</p>\n</blockquote>\n', 'ViewCount': '88', 'Title': 'standard sequential algorithm with polylog runtime?', 'LastEditorUserId': '5323', 'LastActivityDate': '2013-11-06T16:34:41.877', 'LastEditDate': '2013-11-01T15:14:16.587', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16623', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5323', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2013-11-01T15:03:57.973', 'Id': '16622'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Are they used for <i>saving</i> the information; <i>sending</i> the information to the processor or <em>finding</em> the information? (or something else)</p>\n\n<p>Edit: I am talking about the <i>direct</i>,  <i>fully assotiative</i> and  <i>set associative </i> mapping. I hope this clears things up a bit.</p>\n', 'ViewCount': '39', 'Title': 'What are the cache mapping algorithms used for?', 'LastEditorUserId': '11137', 'LastActivityDate': '2013-11-03T13:52:04.777', 'LastEditDate': '2013-11-03T13:52:04.777', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11137', 'Tags': '<algorithms><computer-architecture>', 'CreationDate': '2013-11-02T15:33:48.060', 'Id': '16652'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '397', 'Title': 'Tiling an orthogonal polygon with squares', 'LastEditDate': '2013-11-05T17:00:30.150', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '2', 'Body': '<p>Given an orthogonal polygon (a polygon whose sides are parallel to the axes), I want to find the smallest set of interior-disjoint squares, whose union equals the polygon.</p>\n\n<p>I found several references to slightly different problems, such as:</p>\n\n<ul>\n<li><em>Covering</em> an orthogonal polygon with squares - similar to my problem, but the covering squares are allowed to overlap. This problem has a polynomial solution (<a href="http://cs.smith.edu/~orourke/Papers/ConnJORsquares.pdf">Aupperle, Conn, Keil and O\'Rourke, 1988</a>;  <a href="http://dx.doi.org/10.1142/S021819599600006X">Bar-Yehuda and Ben-Hanoch, 1996</a>).</li>\n<li>Tiling/decomposing/partitioning an orthogonal polygon to <em>rectangles</em>. This problem has a polynomial solution (<a href="http://dx.doi.org/10.1016/B978-044482537-7/50012-7">Keil, 2000</a>; <a href="http://dx.doi.org/10.1007/978-3-642-11409-0_1">Eppstein, 2009</a>).</li>\n<li><em>Covering</em> an orthogonal polygon with <em>rectangles</em> - this problem is known to be NP-complete (<a href="http://dx.doi.org/10.1109/sfcs.1988.21976">Culberson and Reckhow, 1988</a>).</li>\n</ul>\n\n<p>I am looking for an algorithm for minimal <em>tiling</em> with <em>squares</em>.</p>\n', 'Tags': '<algorithms><computational-geometry><tiling>', 'LastEditorUserId': '1342', 'LastActivityDate': '2013-12-10T07:45:32.940', 'CommentCount': '11', 'AcceptedAnswerId': '16801', 'CreationDate': '2013-11-02T23:30:11.247', 'Id': '16661'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is the variation:\nWe have some P piles of numbers,each having some pi numbers , and in each turn a player may choose a pile, and then a number from the chosen pile. All the numbers greater than equal to the chosen number are removed from the chosen pile. \nWe have to determine, if the player moving first has a winning strategy.</p>\n\n<p>What I have done is, read about the NIM game and how we could generate a winning strategy. But this variation seems to have a dynamic rule set. Is this a standard type? If so, where can I read about this.? If not, how to approach this? Help in any direction will be appreciated.</p>\n', 'ViewCount': '170', 'Title': 'Modification of Nim Game Winning Strategy', 'LastActivityDate': '2013-11-03T10:39:17.880', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8622', 'Tags': '<algorithms><game-theory>', 'CreationDate': '2013-11-03T10:39:17.880', 'FavoriteCount': '1', 'Id': '16668'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've got a grid consisting of squares that is 3 squares high and N squares long. \nSome of the squares are filled with numbers that are not greater than 3*N. You can move only to squares that are below, above, on right, or left from you. When you move on a square, you fill that square with a number of the amount of moves you have already taken + 1. If square is already filled with a number, you can move onto it, only if the sum steps you've have taken + 1 is equal to the number on the square.</p>\n\n<p>I need to find a continous path that leads through all the squares of the grid only once and output the same grid filled with numbers corresponding to the steps of your path. In other words, a path was previously laid out on the grid and then some numbers have been erased from squares. Now I have to retrace that path.</p>\n\n<p>I should be done in O(N^2), but I will be thankful for any solutions.</p>\n", 'ViewCount': '83', 'Title': 'Find a continuous path filling whole 3xN grid going through certain points in given order', 'LastActivityDate': '2013-11-03T23:11:59.523', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16683', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11149', 'Tags': '<algorithms><eulerian-paths><square-grid>', 'CreationDate': '2013-11-03T12:30:04.480', 'FavoriteCount': '1', 'Id': '16670'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given N numbers and M range queries (starting and ending points), I need to \ncompute majority (most frequent element, if it exists) in these ranges.</p>\n\n<p>I'm looking for an algorithm that would answer queries in logarithmic or even constant time.</p>\n", 'ViewCount': '91', 'Title': 'Range majority queries - most freqent element in range', 'LastActivityDate': '2013-11-03T18:09:21.813', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16676', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11149', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-11-03T12:55:59.207', 'Id': '16671'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a nice idea to optimize TSP by removing nodes, its going to be hard for me to explain my self, so please be patient with me and try to understand what I am saying.</p>\n\n<p>I have a symmetric(the weight of $AB$ is equal to $BA$), undirected graph where all the weights are 1 or 2.</p>\n\n<p>I am trying to find TSP optimal tour for this graph. Also I know that the optimal tour is only contains edges with the weight of 1.</p>\n\n<p>I been looking in to my data and I found that there is a node that only have two edges of 1 connected to it - lets call this node $B$ and those edges that connected to him $AB$ and $BC$. I know that $AB,BC$ must be part of the optimal tour because in any other way we will have an edge with the weight of 2, and by definition this can't be . So I am removing the node $B$ and adding extra edge $AC$ between the endpoints of the removed node, and I am giving him the weight of 1. Now when I will solve the TSP I can replace it by the two edges that I removed.</p>\n\n<p>Now I want to try something bigger and I am not sure that it will work, so this is what the question is about. Lets assume that we have not 2 but 3 edges connected to one node with the weight of 1, can we remove this node and add extra two edges using the same idea as before?</p>\n", 'ViewCount': '86', 'Title': 'Symmetric TSP optimization by removing nodes', 'LastEditorUserId': '10572', 'LastActivityDate': '2013-11-04T12:30:04.023', 'LastEditDate': '2013-11-04T10:43:43.307', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><graph-theory><traveling-salesman>', 'CreationDate': '2013-11-04T08:32:03.913', 'Id': '16696'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '109', 'Title': 'Which algorithms are usable for heatmaps and what are their pros and cons', 'LastEditDate': '2013-11-28T08:26:17.133', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11126', 'FavoriteCount': '1', 'Body': '<p>This is a <a href="http://stackoverflow.com/questions/19765076/which-algorithms-are-usable-for-heatmaps-and-what-are-their-pros-and-cons">cross post from Stack Overflow,</a> and <a href="http://dsp.stackexchange.com/questions/11463/which-algorithms-are-usable-for-heatmaps-and-what-are-their-pros-and-cons">DSP at Stackexchange</a> since I cannot really decide which part of Stackexchange is most fitting. If this is the wrong place please tell me and I\'ll remove the question.</p>\n\n<p>I have a matrix with numerical data. The matrix contains values from 0 to an arbitrary integer value.</p>\n\n<p>Each element of the matrix is equivalent to a coordinate on a map.</p>\n\n<p>I want to display that data as a heatmap overlayed the original map.</p>\n\n<p>The three approaches I have found so far are </p>\n\n<ol>\n<li><p>Linear interpolation. I guess the interpolation is don from the original datapoint to some set distance away from it in each direction. </p></li>\n<li><p>Average of surrounding cells. Each empty cell gets the average value of the eight adjacent cells. </p></li>\n<li><p>Gaussian blur as suggested on the SO thread.</p></li>\n<li><p>Box blur with 1..n passes.</p></li>\n</ol>\n\n<p>Are there any more methods? What are the pros and cons of the different approaches? What is a good source, online or print, for a discussion on heatmaps or similar problems?</p>\n', 'Tags': '<algorithms><image-processing><matrices><signal-processing>', 'LastEditorUserId': '11126', 'LastActivityDate': '2013-11-28T08:26:17.133', 'CommentCount': '6', 'AcceptedAnswerId': '18422', 'CreationDate': '2013-11-04T09:51:03.010', 'Id': '16699'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm currently taking an Algorithms course, and we are covering Backtracking, CSP, and heuristics. However, I'm getting confused on exactly the differences between these terms and their applications.</p>\n\n<p>When using Backtracking on a problem, I understand we are calculating the set of all allowable solutions to satisfy some property. This uses the idea of some form of constraint to make sure we can reduce the problem and not have to calculate every possible permutation.</p>\n\n<p>So is the Backtracking algorithm just an application of a CSP? \nBut when I saw an example of solving a sudoku puzzle with the Backtracking algorithm, it took many more steps than a CSP description of it. </p>\n\n<p>Also, how would the idea of using heuristics to solve a problem relate in with this?</p>\n\n<p>There is a good chance I'm getting confused with some of the terminology so any advice would be greatly appreciated. </p>\n", 'ViewCount': '83', 'Title': 'Heuristics & Constrain Satisfaction Problem Differences', 'LastActivityDate': '2013-11-05T10:36:09.383', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '16724', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8242', 'Tags': '<algorithms><heuristics>', 'CreationDate': '2013-11-04T22:22:10.640', 'Id': '16723'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '74', 'Title': 'Finding farthest item in an array with duplicates', 'LastEditDate': '2013-11-06T00:17:00.597', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9694', 'FavoriteCount': '1', 'Body': "<p>I have an array $A[]$ of size $L$, which contains numbers in the range $1 \\ldots N$.  Here $L&gt;N$, so the array will contain repetitions.  If $x,y$ are two numbers that are both present in the array, define the distance $d(x,y)$ to be the minimum difference in positions where $x,y$ appear, i.e.,</p>\n\n<p>$$d(x,y) = \\min \\{|i-j| : A[i]=x, A[j]=y\\}.$$</p>\n\n<p>Given a number $x$ that is present in the array, I need to find the number $y$ in the array distance from $x$ is as large as possible.  In other words, given $x$, I am trying to find $y$ that makes $d(x,y)$ as large as possible (subject to the restriction that $y$ is a number in $A[]$).</p>\n\n<p>Can this be done in $o(L)$ time per query?  It's OK to do some pre-processing of the array.</p>\n\n<p>For example, suppose the array is $[1,3,2,3,4,5,3]$.  Then $d(5,3)=1$, since there is a $5$ one position away from a $3$ (a $5$ appears at index $5$ and a $3$ appears at index $6$).  If the query is $x=5$, the correct answer is $y=1$, since this makes the value of $d(5,y)$ as large as possible.</p>\n", 'Tags': '<algorithms><arrays><amortized-analysis>', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-06T00:47:25.973', 'CommentCount': '4', 'AcceptedAnswerId': '16756', 'CreationDate': '2013-11-05T14:58:45.963', 'Id': '16739'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $S$ be a set of $n$ integers. Consider the following <em>weighted permutations problem</em>.</p>\n\n<blockquote>\n  <p>Let $m&lt;n$ be an integer. What is an efficient algorithm to enumerate all subsets of $m$ integers of $S$ such that they are listed in order of the sum of the integers in each subset?</p>\n</blockquote>\n\n<p>Each subset is a permutation, and each permutation has a total weight that is the sum of the integers in the permutation.</p>\n\n<p>The idea is to come up with an algorithm that is not the trivial algorithm of enumerating all subsets, and then sorting them, i.e. more of a "streaming" type algorithm. That is, efficiency in terms not so much of time but of "small" space.</p>\n\n<p>Maybe this is published somewhere in the literature although I have not seen it.</p>\n', 'ViewCount': '89', 'Title': 'Enumerating weighted permutations in sorted order problem', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-18T21:03:07.127', 'LastEditDate': '2013-11-18T21:03:07.127', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><reference-request><sorting><efficiency><enumeration>', 'CreationDate': '2013-11-05T17:32:02.033', 'FavoriteCount': '1', 'Id': '16744'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say we have a sport, where many competitions are organized throughout the year. In these events, a set of people compete, although not every person competes in every event.</p>\n\n<p>I would like to categorize these people, and the events, in such a way that people in category $A$ performed well in the events of category $A$ etc. This also means that people in category $A$ usually perform well at the same events. There would then be a one-to-one correspondence between event categories and people categories.</p>\n\n<p>It would also be OK for me if every event and every person is not uniquely put into one category, but is given a weight $w_A$ representing how much it fits into category $A$</p>\n\n<p>The data I have are the results from all the events.</p>\n\n<p>What do you think would be a good algorithm to categorize both events and people at the same time (since they are related)? I have been reading up on different machine learning algorithms, but would love some input on what to focus on.</p>\n", 'ViewCount': '40', 'Title': 'Categorizing sport events and competitors', 'LastActivityDate': '2013-11-05T17:56:41.147', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11208', 'Tags': '<algorithms><machine-learning>', 'CreationDate': '2013-11-05T17:56:41.147', 'Id': '16747'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have an undirected graph which is stored as an adjacency matrix. The graph contains a single cycle; all other vertices are isolated.</p>\n\n<p>How can I efficiently find the length of the cycle?</p>\n\n<p>The best I've been able to come up with:</p>\n\n<ol>\n<li>Starting at row 0 of the matrix, traverse through rows until an initial <code>1</code> is found, say at row <code>startVertex</code> and column <strong>k</strong>. Increment a counter.</li>\n<li>Search column <strong>k</strong> for its other <code>1</code>, say at row <strong>j</strong>. Increment the counter.</li>\n<li>Search row <strong>j</strong> for its other <code>1</code> value. Increment the counter.</li>\n<li>Repeat steps <strong>2</strong> and <strong>3</strong> until a row or column which matches <code>startVertex</code> is found.</li>\n</ol>\n\n<p>The complexity of this algorithm is $\\mathcal{O}(V^2)$.</p>\n\n<p>Is there a better algorithm out there?</p>\n", 'ViewCount': '49', 'Title': 'Find Cycle Length', 'LastActivityDate': '2013-11-06T01:35:39.257', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1025', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-11-05T23:39:17.893', 'Id': '16754'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm playing around with tournaments and currently have the problem that I need to check whether a given subset of the edges of a tournament is transitive (it need not be acyclic). I'm aware that I can always take the transitive closure of the edge set and see whether it terminates without adding a single edge or not, but I was wondering if there might be a simpler way than that.</p>\n\n<p>Note that I'm specifically going for simplicity, not efficiency; the tournaments I want to check are over a maximum of $7$ vertices, so complexity really isn't an issue. I would prefer simple, easy to implement ways. The simplest I could find so far is Floyd-Warshall, but maybe someone knows anything that's simpler still.</p>\n", 'ViewCount': '65', 'Title': 'Simplest way to check edge set for transitivity', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-08T12:59:44.767', 'LastEditDate': '2013-11-08T12:59:44.767', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16760', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6689', 'Tags': '<algorithms><graphs><transitivity>', 'CreationDate': '2013-11-06T00:54:33.107', 'Id': '16757'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '150', 'Title': 'Artificial Intelligence: Condition for BFS being optimal', 'LastEditDate': '2013-11-18T19:12:31.383', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6998', 'FavoriteCount': '1', 'Body': '<p>It is said in the book <em>Artificial Intelligence: A Modern Approach</em> for finding a solution on a tree using BFS that: </p>\n\n<blockquote>\n  <p>breadth-first search is optimal if the path cost is a nondecreasing function of the\n  depth of the node. The most common such scenario is that all actions have the same cost.</p>\n</blockquote>\n\n<p>From that I understand that if the path cost is non decreasing function of depth, the BFS algorithm returns an optimal solution, i.e., <strong>the only condition is the cost function being nondecreasing</strong>. But I think the only way for BFS to be optimal is the scenario in which all the path costs are identical, therefore a node found in a certain level is necessarily the optimal solution, as, if they exist, the others are. Therefore I think for BFS to be optimal, cost function should be non decreasing <strong>AND</strong> the costs of nodes should be identical. However, the book says only one of the conditions (former one) makes BFS optimal.</p>\n\n<p>Is there a situation in which the costs are not identical, the cost function is nondecreasing and the solution returned by BFS is guaranteed to be optimal?</p>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms><search-trees><search-problem>', 'LastEditorUserId': '6998', 'LastActivityDate': '2013-11-18T19:12:31.383', 'CommentCount': '0', 'AcceptedAnswerId': '16780', 'CreationDate': '2013-11-06T01:16:28.697', 'Id': '16758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '281', 'Title': 'CNF Generator for Factoring Problems', 'LastEditDate': '2013-11-07T21:03:05.140', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10572', 'FavoriteCount': '1', 'Body': '<p>I\'ve been reading these:</p>\n\n<ul>\n<li><a href="http://cstheory.stackexchange.com/questions/6755/fast-reduction-from-rsa-to-sat">Fast Reduction from RSA to SAT</a></li>\n<li><a href="http://cgi.cs.indiana.edu/~sabry/cnf.html" rel="nofollow">CNF Generator for Factoring Problems</a> (Also have C code implementation)</li>\n</ul>\n\n<p>I don\'t understand how the reduction from <a href="https://en.wikipedia.org/wiki/Integer_factorization" rel="nofollow">FACT</a> to <a href="https://en.wikipedia.org/wiki/Clausal_normal_form" rel="nofollow">$3\\text{-SAT}$</a> works. Are there any simple articles in which I can read about it?</p>\n\n<p>My final goal is to eventually implement a reduction from $3\\text{-SAT}$ to the <a href="https://en.wikipedia.org/wiki/Hamiltonian_path_problem" rel="nofollow">undirected Hamiltonian circuit problem</a>.</p>\n', 'Tags': '<algorithms><graph-theory><reductions><factoring><hamiltonian-path>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-13T22:15:26.360', 'CommentCount': '7', 'AcceptedAnswerId': '16805', 'CreationDate': '2013-11-07T06:57:01.380', 'Id': '16789'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have the following problem, which seems to be similar to Set Cover.</p>\n\n<p>We are given a set $U$ of elements (the universe, e.g., $U=\\{1,2,3,4,5\\}$).\nWe're also given a set $S$ of subsets (e.g., $S=\\{\\{1\\},\\{2\\},\\{3\\},\\{4\\},\\{5\\},\\{1,2,5\\},\\{1,3,4\\},\\{2,3,4,5\\}\\}$).</p>\n\n<p>The standard set cover problem asks for the minimum subset of $S$ that covers the whole universe $U$, i.e., the smallest set-cover.  In our case this would be $\\{\\{1\\},\\{2,3,4,5\\}\\}$.</p>\n\n<p>However, I'm interested in finding a collection of set-covers where each subset from $S$ is used at most once.  I want to find the largest such collection possible, i.e., to find as many different, disjoint set-covers of $U$ as possible.  Considering our example, the three set-covers $\\{\\{1\\},\\{2,3,4,5\\}\\}$ and $\\{\\{3\\},\\{4\\},\\{1,2,5\\}\\}$ and $\\{\\{2\\},\\{5\\},\\{1,3,4\\}\\}$ would be the output, since each one is a set-cover of $U$ and they are pairwise disjoint (no subset from $S$ is used in more than one set-cover).</p>\n\n<p>Just testing all possible combinations of set-covers is definitely not an option. I have tried using an algorithm for set cover (ILP implementation) repeatedly, eliminating all used subsets in between runs. However, I'm certain that this strategy does not actually maximize the number of all possible covers.</p>\n\n<p><strong>Edit 1:</strong> I will try to describe the size of my problem a bit better. The typical size of $U$ is about 4000. $S$ consists of about 50 subsets. Each element of $S$ covers on average about 70% of $U$. Please keep in mind that I don't want to find <em>all</em> subset combinations out of $S$ that cover $U$. I just want to determine the maximum number of set covers using each subset from $S$ <em>only once</em> (or not at all). As of now I'm guessing the maximum number of possible covers is approximately 5.</p>\n", 'ViewCount': '297', 'Title': 'Variation of Set Cover Problem: Finding a maximum-sized collection of disjoint set-covers', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-13T17:28:54.193', 'LastEditDate': '2013-11-13T17:28:54.193', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11255', 'Tags': '<algorithms><optimization><set-cover>', 'CreationDate': '2013-11-08T09:43:48.263', 'FavoriteCount': '1', 'Id': '16816'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I came across the following dynamic subtraction game:</p>\n\n<blockquote>\n  <p>There is one pile of n chips. The \ufb01rst player to move may remove as many chips as\n  desired, at least one chip but not the whole pile. Thereafter, the players alternate moving,\n  each player not being allowed to remove more chips than his opponent took on the previous\n  move. What is an optimal move for the \ufb01rst player if n = 44? For what values of n does\n  the second player have a win?</p>\n</blockquote>\n\n<p>Now, I know how to solve basic subtraction games, i.e., when both the players are allowed the same set of moves throughout the game (e.g., subtract only 1, 2, or 3 throughout the game). But in the game mentioned above, this set of possible numbers for subtraction is not fixed. I have no clue how to go about solving this question. Any kind of help would be appreciated.</p>\n', 'ViewCount': '195', 'Title': 'Dynamic subtraction game', 'LastActivityDate': '2013-11-08T22:24:52.493', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16834', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11119', 'Tags': '<algorithms><dynamic-programming><game-theory>', 'CreationDate': '2013-11-08T12:55:34.920', 'Id': '16820'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here is a variation of a job-scheduling Problem.\nLet $J = \\{j_1,...j_n\\}$ be a set of Jobs for $1 \\leq i \\leq n$. Given Job length $|j_i|\\in \\mathbb{N}$, deadline $f_i \\in \\mathbb{N}$, profit $p_i \\ge 0$ and starting-time $s_i  \\in \\mathbb{N}$. I am looking for a greedy approximation factor given that the Job length may only be distinguished by factor k. </p>\n\n<p>$$max_i|j_i| \\leq k \\cdot min_i|j_i|$$</p>\n\n<p>The Greedy algorithm of this Problem is fairly stupid. Greedy takes a job with the biggest profit.  I created an example (3-Job-Scheduling):</p>\n\n<p>Let $J = \\{j_1,j_2,j_3\\}$ with $|j_1| = 2, j_2 = j_3 = 1$ and </p>\n\n<p>$s_1 = 0; s_2 = 0; s_3 = 1$,</p>\n\n<p>$f_1 = 2;f_2 = 1; f_3 = 2$</p>\n\n<p>$p_1 = w; p_2 = p_3 = (w-1)$</p>\n\n<p>What I want to show is that Greedy gives us w while 2(w-1) is the optimal solution. </p>\n\n<p>My question: Is this valid for n-Job-Scheduling (the general case). Is this the worst-case? </p>\n\n<p>I can't think of anything worse. So I figured since the problem is a k-Matroid (is this a common term?) there will be a an approximation factor $\\frac{1}{k-\\epsilon}$ for  any $\\epsilon &gt; 0.$ I know this is not exactly a proof yet, but am I on the right way?</p>\n\n<p>Thanks for your help!</p>\n", 'ViewCount': '118', 'Title': 'Single machine job scheduling (Greedy heuristic)', 'LastActivityDate': '2013-11-08T13:06:23.300', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10940', 'Tags': '<approximation><scheduling><greedy-algorithms><check-my-proof>', 'CreationDate': '2013-11-08T13:06:23.300', 'Id': '16821'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '247', 'Title': 'Algorithm to check whether a complete, undirected graph is fullfilling the triangle inequality', 'LastEditDate': '2013-11-08T16:09:55.873', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11258', 'FavoriteCount': '2', 'Body': '<p>I am searching for an algorithm to check whether a complete, undirected graph is fullfilling the triangle inequality( $\\text{weight}(u,v) \\le \\text{weight}(u,w) + \\text{weight}(w,v)$ for all vertices $u, v, w$).</p>\n\n<p>My first naive try was to use an algorithm for solving the all-pairs-shortest-path-problem and compare the result to the vertices connecting two nodes directly.</p>\n\n<p>However, I think this might be overkill. Is there any better way to check?</p>\n\n<p>Thanks a lot.</p>\n', 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-09T03:27:41.780', 'CommentCount': '2', 'AcceptedAnswerId': '16830', 'CreationDate': '2013-11-08T16:04:04.330', 'Id': '16827'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Half a decade ago I was sitting in a data structures class where the professor offered extra credit if anyone could traverse a tree without using recursion, a stack, queue, etc. (or any other similar data structures) and just a few pointers.  I came up with what I thought was an obvious answer to that question which was ultimately accepted by the professor.  I was sitting in a discrete math class with another professor in the same department--and he asserted that it was impossible to traverse a tree without recursion, a stack, queue, etc., and that my solution was invalid.</p>\n\n<p>So, is it possible, or impossible?  Why or why not?</p>\n\n<p>Edit:  To add some clarification, I implemented this on a binary tree which had three elements-- the data stored at each node and pointers to two children.  My solution could be extended to n-ary trees with only a few changes.</p>\n\n<p>My data structures teacher did not put any constraints against mutating the tree, and indeed I found out later that his own solution was to use the child pointers to point back up the tree on his way down.  My discrete math professor said any mutation of a tree means that it is no longer a tree according to the mathematical definition of a tree, his definition would also preclude any pointers to parents--which would match the case where I solved it above.</p>\n', 'ViewCount': '726', 'Title': 'Can a tree be traversed without recursion, stack, or queue, and just a handful of pointers?', 'LastEditorUserId': '11262', 'LastActivityDate': '2013-11-14T13:15:47.923', 'LastEditDate': '2013-11-09T16:24:18.440', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11262', 'Tags': '<algorithms><trees><recursion>', 'CreationDate': '2013-11-08T22:19:23.803', 'FavoriteCount': '1', 'Id': '16833'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Two matrices can be stored in either row major or column major order in contiguous memory. Does the time complexity of computing their multiplication vary depending on the storage scheme? That is, I want to know whether it will work faster if stored in row major or column major order.</p>\n', 'ViewCount': '83', 'Title': 'Does the performance of matrix multiplication depend on the storage of the array?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-12T03:18:57.387', 'LastEditDate': '2013-11-10T23:05:57.370', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8533', 'Tags': '<algorithms><time-complexity><storage><multiplication>', 'CreationDate': '2013-11-10T04:00:59.373', 'Id': '17865'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>According to Wikipedia (<a href="https://en.wikipedia.org/w/index.php?title=Patience_sorting&amp;oldid=579146187" rel="nofollow">link</a>), patience sort finds the longest increasing subsequence of a given sequence. Lets say the longest increasing subsequence of seq has length <code>correct(seq)</code></p>\n\n<p>Patience sort works like this:</p>\n\n<pre><code>Parameters: A sequence of integers called seq\nOutput: A number between 0 and the length of seq. Lets call it patienceSort(seq)\nAlgorithm (in Python):\n\ndef patienceSort(seq):\n    piles = []\n    for el in seq:\n        for i in range(len(piles)):\n            # If there is a pile with a strictly greater element on top,\n            # place current element on that pile\n            if piles[i][-1] &gt; el:\n                piles[i].append(el)\n                break\n        # if there is no such pile, create a new one\n        else:\n            piles.append([el])\n    return len(piles)\n</code></pre>\n\n<h2>What I\'ve understood:</h2>\n\n<p>(I) We can guarantee that at every time the top of piles (<code>piles[i][-1]</code>) is ordered in increasing order.</p>\n\n<p>(II) For numbers in the same pile: Numbers are ordered...</p>\n\n<p>(II.1) ... by time (first one in the pile came first)</p>\n\n<p>(II.2) ... by value (first one in the pile has highest value)</p>\n\n<p>(III) The numbers on top of the piles are not necessarily an increasing sequence \nwithin seq</p>\n\n<p>(IV) The right pile is always the latest one that was created</p>\n\n<p>Example for (III):</p>\n\n<pre><code>      37\n20    38\n30    39\n40    40\n50    70\npile1 pile2\nseq = [50,40,70,30,40, 39,38,37,20]\n[20,37] is not an increasing sequence in seq\n</code></pre>\n\n<p>For each card (which is not on the first pile), you can store a pointer to the top of the pile before when you add it to the pile. This way, you can get an increasing sequence by following the pointers and inverting the list. So:</p>\n\n<pre><code>patienceSort(seq) &lt;= correct(seq)\n</code></pre>\n\n<h2>Question</h2>\n\n<p>Why is <code>patienceSort(seq) &gt;= correct(seq)</code> </p>\n', 'ViewCount': '167', 'Title': 'Why does Patience sorting find the longest increasing subsequence?', 'LastEditorUserId': '2914', 'LastActivityDate': '2013-11-10T21:59:58.327', 'LastEditDate': '2013-11-10T21:59:58.327', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2914', 'Tags': '<algorithms>', 'CreationDate': '2013-11-10T21:26:49.883', 'Id': '17883'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Solve the following recurrence equations</p>\n\n<p>a. T(n) = T(n/2) + 18</p>\n\n<p>b. T(n) = 2T(n/2) + 5n</p>\n\n<p>c. T(n) = 3T(n/2) + 5n</p>\n\n<p>d. T(n) = T(n/2) + 5n</p>\n\n<p>This is only a sample of what I was given but I am not sure what the question is asking me or how to solve it? Can someone please explain this to me?</p>\n\n<p>The only thing I really know about them is that it has something having to do with Big O and such. I was a bit sick when they gave this lecture so I couldn't really grasp the concept. Could someone please help me understand this?</p>\n", 'ViewCount': '84', 'ClosedDate': '2013-11-11T13:50:19.720', 'Title': 'How can I solve for T(n) recurrence equations?', 'LastActivityDate': '2013-11-10T23:57:30.613', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11305', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-11-10T22:32:41.520', 'Id': '17890'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is a problem that I was given for my data structures and algorithms class. I am not sure how to tackle this problem. Can I get some assistance?</p>\n\n<blockquote>\n  <p>At Hogwarts a new shipment of $n$ goblins has arrived.  To be of any use, a goblin must be completely truthful (never lies).  Unfortunately, not all of the $n$ goblins in the shipment are truth tellers.  Only some are truth-tellers, and some are deceivers.  It is your task to design an algorithm to separate the truth-teller goblins from the deceiver goblins.  To do this, you have one tool available:  You may combine any two goblins and have them state whether the other goblin is a truth-teller or a deceiver.  A truth-teller will always say correctly what the other goblin is, but a deceiver may lie (but also may sometimes tell the truth to REALLY confuse you).  For any two goblins that you test, the following can be concluded from the goblin responses:</p>\n  \n  <p>$$\n\\begin{array}{ccc}\n\\text{Goblin A says} &amp;\n\\text{Goblin B says} &amp;\n\\text{Conclusion} \\\\\\hline\n\\text{B is a truth-teller} &amp;\n\\text{A is a truth-teller} &amp;\n\\text{both are truth-tellers or both are deceivers} \\\\\n\\text{B is a truth-teller} &amp;\n\\text{A is a deceiver} &amp;\n\\text{at least one is a deceiver} \\\\\n\\text{B is a deceiver} &amp;\n\\text{A is a truth-teller} &amp;\n\\text{at least one is a deceiver} \\\\\n\\text{B is a deceiver} &amp;\n\\text{A is a deceiver} &amp;\n\\text{at least one is a deceiver}\n\\end{array}\n$$</p>\n  \n  <ul>\n  <li>Show that if more than $n/2$ goblins are deceivers, it is impossible to determine which goblins are the truth-tellers using only the pairwise testing strategy.</li>\n  <li>Consider the problem of identifying $1$ truth-teller goblin.  Show that if we assume that more than $n/2$ of the goblins are truth-tellers, then the problem of finding a single truth-teller from $n$ goblins can be reduced to a problem of about half the size using $n/2$ goblin comparisons.</li>\n  <li>Use a recurrence equation to show that if more than half of the goblins are truth-tellers, then the truth-tellers can all be identified within $O(n)$ goblin comparisons.</li>\n  </ul>\n</blockquote>\n', 'ViewCount': '128', 'Title': 'How can I tell who is a truth teller and who is a liar?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-11T15:40:35.343', 'LastEditDate': '2013-11-11T15:40:35.343', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '17910', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11305', 'Tags': '<algorithms>', 'CreationDate': '2013-11-11T03:51:36.600', 'Id': '17904'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we have a $n\\times n$ symmetric matrix $\\mathbf A$.</p>\n\n<p>I want to know if there exists two elements of a vector $\\mathbf x$, let's call them $x_i,x_j,i\\ne j$, such that $x_i +x_j+[A]_{i,j}\\ge y$ for some vector $\\mathbf x$ of size $n$. So:</p>\n\n<p>$$\nf\\left(\\mathbf x \\in \\mathbb N^n,y\\right)=\n\\begin{cases}\n1&amp;\\text{if }\\exists_{i,j,i\\ne j}x_i+x_j+[A]_{i,j}\\ge y\\\\\n0&amp;\\text{otherwise}\\\\\n\\end{cases}\n$$</p>\n\n<p>We can alternatively formulate it simpler, by subtractiong $\\frac y 2$ from all $x_i \\in \\mathbf x$, and flip the sign of the elements $\\mathbf A$ and formulate it like this:</p>\n\n<p>$$\ng\\left(\\mathbf x \\in \\mathbb N^n\\right)=\n\\begin{cases}\n1&amp;\\text{if }\\exists_{i,j,i\\ne j}x_i+x_j\\ge [A]_{i,j}\\\\\n0&amp;\\text{otherwise}\\\\\n\\end{cases}\n$$\n$\\mathbf A$ does not have to be stored as a matrix, and I am hoping some other form of storage can help here - some sort of precomputation.</p>\n\n<h3>Is there an efficient algorithm for this (subquadtratic in $n$)?</h3>\n\n<p>I am sort of struggling to name this, so:</p>\n\n<h3>Is this a known problem, or reducible to a known problem?</h3>\n", 'ViewCount': '46', 'Title': '2SUM with a weight', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-12T00:37:23.937', 'LastEditDate': '2013-11-11T14:59:50.627', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><data-structures><number-theory>', 'CreationDate': '2013-11-11T05:49:04.410', 'Id': '17907'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '172', 'Title': 'How do I find the shortest representation for a subset of a powerset?', 'LastEditDate': '2013-11-12T12:05:50.837', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1575', 'FavoriteCount': '1', 'Body': "<p>I'm looking for an efficient algorithm for the following problem or a proof of NP-hardness.</p>\n\n<p>Let $\\Sigma$ be a set and $A\\subseteq\\mathcal{P}(\\Sigma)$ a set of subsets of $\\Sigma$. Find a sequence $w\\in \\Sigma^*$ of least length such that for each $L\\in A$, there is a $k\\in\\mathbb{N}$ such that $\\{ w_{k+i} \\mid 0\\leq i &lt; |L| \\} = L$.</p>\n\n<p>For example, for $A = \\{\\{a,b\\},\\{a,c\\}\\}$, the word $w = bac$ is a solution to the problem, since for $\\{a,b\\}$ there's $k=0$, for $\\{a,c\\}$ there's $k=1$.</p>\n\n<p>As for my motivation, I'm trying to represent the set of edges of a finite automaton, where each edge can be labeled by a set of letters from the input alphabet. I'd like to store a single string and then keep a pair of pointers to that string in each edge. My goal is to minimize the length of that string.</p>\n", 'Tags': '<algorithms><formal-languages><encoding-scheme>', 'LastEditorUserId': '1575', 'LastActivityDate': '2013-11-18T21:37:48.757', 'CommentCount': '11', 'AcceptedAnswerId': '18093', 'CreationDate': '2013-11-11T10:54:26.300', 'Id': '17914'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an algorithm/systematic procedure to test whether a language is context-free?</p>\n\n<p>In other words, given a language specified in algebraic form (think of something like $L=\\{a^n b^n a^n : n \\in \\mathbb{N}\\}$), test whether the language is context-free or not.  Imagine we are writing a web service to help students with all their homeworks; you specify the language, and the web service outputs "context-free" or "not context-free".  Is there any good approach to automating this?</p>\n\n<p>There are of course techniques for manual proof, such as the pumping lemma, Ogden\'s lemma, Parikh\'s lemma, the Interchange lemma, and <a href="http://cs.stackexchange.com/q/265/755">more here</a>.  However, they each require manual insight at some point, so it\'s not clear how to turn any of them into something algorithmic.</p>\n\n<p>I see <a href="http://cs.stackexchange.com/a/282/755">Kaveh has written elsewhere</a> that the set of non-context-free languages is not recursively enumerable, so it seems there is no hope for any algorithm to work on all possible languages.  Therefore, I suppose the web service would need to be able to output "context-free", "not context-free", or "I can\'t tell".  Is there any algorithm that would often be able to provide an answer other than "I can\'t tell", on many of the languages one is likely to see in textbooks?  How would you build such a web service?</p>\n\n<hr>\n\n<p>To make this question well-posed, we need to decide how the user will specify  the language.  I\'m open to suggestions, but I\'m thinking something like this:</p>\n\n<p>$$L = \\{E : S\\}$$</p>\n\n<p>where $E$ is a word-expressions and $S$ is a system of linear inequalities over the length-variables, with the following definitions:</p>\n\n<ul>\n<li><p>Each of $x,y,z,\\dots$ is a word-expression.  (These represent variables that can hold any word in $\\Sigma^*$.)</p></li>\n<li><p>Each of $a,b,c,\\dots$ is a word-expression.  (Implicitly, $\\Sigma=\\{a,b,c,\\dots\\}$, so $a,b,c,\\dots$ represent a single symbol in the underlying alphabet.)</p></li>\n<li><p>Each of $a^\\eta,b^\\eta,c^\\eta,\\dots$ is a word-expression, if $\\eta$ is a length-variable.</p></li>\n<li><p>The concatenation of word-expressions is a word-expression.</p></li>\n<li><p>Each of $m,n,p,q,\\dots$ is a length-variable.  (These represent variables that can hold any natural number.)</p></li>\n<li><p>Each of $|x|,|y|,|z|,\\dots$ is a length-variable.  (These represent the length of a corresponding word.)</p></li>\n</ul>\n\n<p>This seems broad enough to handle many of the cases we see in textbook exercises.  Of course, you can substitute any other textual method of specifying a language in algebraic form, if you like.</p>\n', 'ViewCount': '119', 'Title': 'Algorithm to test whether a language is context-free', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-12T14:39:17.787', 'LastEditDate': '2013-11-12T14:39:17.787', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><formal-languages><context-free><decision-problem>', 'CreationDate': '2013-11-11T19:42:12.637', 'FavoriteCount': '3', 'Id': '17921'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Is it possible to modify Dijkstra\xb4s algorithm in order to get the longest path from $s$ to $t$ ?.</p>\n\n<blockquote>\n  <p>My intuition says that I\xb4ll need a different algorithm entirely. Finding the longest path is the same as finding the shortest path on a graph with negative weights. However, Dijkstra\u2019s algorithm requires that the weights are positive, so it cannot be modified to calculate the longest path. A better algorithm to use could be: <a href="http://en.wikipedia.org/wiki/Longest_path_problem" rel="nofollow">http://en.wikipedia.org/wiki/Longest_path_problem</a></p>\n</blockquote>\n\n<p>Any idea of how to modify it?</p>\n', 'ViewCount': '1734', 'ClosedDate': '2013-11-17T01:20:19.727', 'Title': 'Is it possible to modify dijkstra algorithm in order to get the longest path?', 'LastActivityDate': '2013-11-14T03:25:44.443', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs>', 'CreationDate': '2013-11-13T07:40:20.250', 'Id': '17980'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have two large sets of integers $A$ and $B$.  Each set has about a million entries, and each entry is a positive integer that is at most 10 digits long.  </p>\n\n<p>What is the best algorithm to compute $A\\setminus B$ and $B\\setminus A$? In other words, how can I efficiently compute the list of entries of $A$ that are not in $B$ and vice versa?  What would be the best data structure to represent these two sets, to make these operations efficient?</p>\n\n<p>The best approach I can come up with is storing these two sets as sorted lists, and compare every element of $A$ against every element of $B$, in a linear fashion.  Can we do better?</p>\n', 'ViewCount': '292', 'Title': 'Computing set difference between two large sets', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-14T13:47:21.917', 'LastEditDate': '2013-11-14T07:18:12.080', 'AnswerCount': '4', 'CommentCount': '11', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1985', 'Tags': '<algorithms><data-structures><sets>', 'CreationDate': '2013-11-13T13:50:12.773', 'FavoriteCount': '1', 'Id': '17984'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Is there an algorithm/systematic procedure to test whether a language is regular?</p>\n\n<p>In other words, given a language specified in algebraic form (think of something like $L=\\{a^n b^n : n \\in \\mathbb{N}\\}$), test whether the language is regular or not.  Imagine we are writing a web service to help students with all their homeworks; the user specifies the language, and the web service responds with "regular",  "not regular", or "I don\'t know".  (We\'d like the web service to answer "I don\'t know" as infrequently as possible.)  Is there any good approach to automating this?  Is this tractable?  Is it decidable (i.e., is it possible to guarantee that we never need to answer "I don\'t know")?  Are there reasonably efficient algorithms for solving this problem, and be able to provide an answer other than "don\'t know" for many/most languages that are likely to arise in practice?</p>\n\n<p>The classic method for proving that a language is not regular is the pumping lemma.  However, it looks like requires manual insight at some point (e.g., to choose the word to pump), so I\'m not clear on whether this can be turned into something algorithmic.</p>\n\n<p>A classic method for proving that a language is regular would be to use the  Myhill\u2013Nerode theorem to derive a finite-state automaton.  This looks like a promising approach, but it does requires the ability to perform basic operations on languages in algebraic form.  It\'s not clear to me whether there\'s a systematic way to symbolically perform all of the operations that may be needed, on languages in algebraic form.</p>\n\n<hr>\n\n<p>To make this question well-posed, we need to decide how the user will specify  the language.  I\'m open to suggestions, but I\'m thinking something like this:</p>\n\n<p>$$L = \\{E : S\\}$$</p>\n\n<p>where $E$ is a word-expression and $S$ is a system of linear inequalities over the length-variables, with the following definitions:</p>\n\n<ul>\n<li><p>Each of $x,y,z,\\dots$ is a word-expression.  (These represent variables that can take on any word in $\\Sigma^*$.)</p></li>\n<li><p>Each of $x^r,y^r,z^r,\\dots$ is a word-expression.  (Here $x^r$ represents the reverse of the string $x$.)</p></li>\n<li><p>Each of $a,b,c,\\dots$ is a word-expression.  (Implicitly, $\\Sigma=\\{a,b,c,\\dots\\}$, so $a,b,c,\\dots$ represent a single symbol in the underlying alphabet.)</p></li>\n<li><p>Each of $a^\\eta,b^\\eta,c^\\eta,\\dots$ is a word-expression, if $\\eta$ is a length-variable.</p></li>\n<li><p>The concatenation of word-expressions is a word-expression.</p></li>\n<li><p>Each of $m,n,p,q,\\dots$ is a length-variable.  (These represent variables that can take on any natural number.)</p></li>\n<li><p>Each of $|x|,|y|,|z|,\\dots$ is a length-variable.  (These represent the length of a corresponding word.)</p></li>\n</ul>\n\n<p>This seems broad enough to handle many of the cases we see in textbook exercises.  Of course, you can substitute any other textual method of specifying a language in algebraic form, if you have a better suggestion.</p>\n', 'ViewCount': '209', 'Title': 'Algorithm to test whether a language is regular', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-17T20:52:53.740', 'LastEditDate': '2013-11-14T22:53:44.637', 'AnswerCount': '1', 'CommentCount': '13', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><formal-languages><regular-languages><decision-problem>', 'CreationDate': '2013-11-14T07:13:17.670', 'FavoriteCount': '1', 'Id': '18010'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Im designing a program to play Connect 6, a variation of connect 4. I have narrowed down my options to the following:</p>\n\n<p>1) Minimax with Alpha-Beta Proning </p>\n\n<p>2) A Neural Net </p>\n\n<p>3) Machine Learning</p>\n\n<p>My program has one second to make a move, so I can only branch out 2 moves ahead with Minimax. Which solution would best perform under 1 second?</p>\n\n<p>I looked around the web, but couldn't find anything relevant.</p>\n\n<p>Also, are there any other additional resources you suggest I have a look at?</p>\n", 'ViewCount': '333', 'Title': 'Algorithms for Connect 4?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-15T23:35:49.010', 'LastEditDate': '2013-11-15T14:46:39.167', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '3', 'OwnerDisplayName': 'asd', 'PostTypeId': '1', 'Tags': '<algorithms><machine-learning><artificial-intelligence><board-games>', 'CreationDate': '2013-11-11T16:49:58.777', 'FavoriteCount': '1', 'Id': '18012'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am asked to check if a vector contains another vector as a subvector. For example:</p>\n\n<p>$$v_1 = (4, \\underline{3, 4}, 9, 10, 28, 5, 12, \\underline{3, 4})$$\n$$v_2 = (3, 4)$$</p>\n\n<p>The answer here will be two, since there are two instances of $v_2$ in $v_1$.</p>\n\n<p>I know I have to use "if" but I dont really know how to write it down (I have tried).</p>\n', 'ViewCount': '61', 'Title': 'Counting occurrences of one vector inside another', 'LastEditorUserId': '69', 'LastActivityDate': '2013-11-15T10:06:52.937', 'LastEditDate': '2013-11-14T17:13:13.220', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11377', 'Tags': '<algorithms><data-structures><lists>', 'CreationDate': '2013-11-14T13:54:23.277', 'Id': '18014'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '90', 'Title': 'Published or widely known failures of Page Rank algorithm', 'LastEditDate': '2013-11-17T17:21:13.813', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '96', 'FavoriteCount': '1', 'Body': '<p>I use Google a lot for searching articles and publications. Occasionally, I find Google totally misses what I am looking for and I get completely unexpected results for some search queries. </p>\n\n<blockquote>\n  <p>What are the published or widely known failures of PageRank algorithm?</p>\n</blockquote>\n\n<p>EDIT: Changed the post to address comments.</p>\n', 'ClosedDate': '2013-11-17T01:13:31.840', 'Tags': '<algorithms><reference-request><searching>', 'LastEditorUserId': '96', 'LastActivityDate': '2013-11-17T17:21:13.813', 'CommentCount': '8', 'CreationDate': '2013-11-14T15:48:56.063', 'Id': '18018'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to solve a 25k clauses 5k variables SAT problem. As it has been running for an hour (precosat) and I'd like to solve bigger ones afterwards, I'm looking for a multi-core SAT-Solver.</p>\n\n<p>As there seem to be many SAT-Solvers, I'm quite lost.</p>\n\n<p>Could anyone point me out the best one for my case?</p>\n\n<p>I'd also be happy if someone could give me the approximate time (if possible).</p>\n", 'ViewCount': '141', 'Title': 'Multicore SAT Solver', 'LastEditorUserId': '11382', 'LastActivityDate': '2014-02-05T14:17:31.637', 'LastEditDate': '2013-11-17T20:27:40.670', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '11382', 'Tags': '<algorithms><reference-request><parallel-computing><sat-solvers>', 'CreationDate': '2013-11-14T16:25:35.683', 'FavoriteCount': '1', 'Id': '18021'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My Problem is like this:</p>\n\n<ol>\n<li><p>I have a physical layout represented as a graph. The Nodes represents hooks/ducts where a wire can anchor and Edges are the possible connection between 2 nodes from where wire can go.</p></li>\n<li><p>There are some special Nodes, called splitters, from where a single wire can be splitted to  2 or more up to k. The k can be taken constant for now but it varies from node to node. Not all nodes are splitters. </p></li>\n<li><p>There is one source of power from where a wire will emerge. It is the source. The wire has to be taken to n sinks. </p></li>\n<li><p>An edge can take any number of wires traversing through it in either direction.</p></li>\n<li><p>The the total wire length has to be minimized. </p></li>\n<li><p>The nature of graph, planar or euclidean is not known. </p></li>\n</ol>\n\n<p><strong>Example</strong>: Below is a sample network. Nodes are named as numbers and edges are provided with equal weights of 1. Source is Node1 and Sinks are Node5, Node9 and Node13. In case 1 Node6 is Splitter node. In case 2 Node6 and Node4 are splitter nodes. The splitter node\'s k=3, i.e., it can take in one wire and split it out to 3 wires.</p>\n\n<p><strong>Case 1</strong>. Only one splitter Node. It makes sense to split at Node6.\n<img src="http://i.stack.imgur.com/IVvkw.jpg" alt="enter image description here"></p>\n\n<p><strong>Case 2</strong>. Two splitter Node. It makes sense to split at Node4 instead of Node6.\n<img src="http://i.stack.imgur.com/6ZKJD.jpg" alt="enter image description here"></p>\n\n<p>I am looking for different strategies to find out a generic solution for this problem. The graph presented here is of a smaller scale as compared to the problem in hand. The graph is static and can not be changed (i mean the solution should not suggest any new edge or propose new splitter location ). \nAny reference to research paper published on this kind of problem is also welcomed.</p>\n\n<p><strong>Case 3</strong>. Two splitter Node. It makes sense to split at Node4 and Node14. Note that this case has edge weights changed for Edge 8-12, 6-10 and 10-11. The important thing in this case is retracing of a wire after getting splitted from Node14.</p>\n\n<p><img src="http://i.stack.imgur.com/aYuPN.jpg" alt="enter image description here"></p>\n', 'ViewCount': '128', 'Title': 'Wiring Length Minimization', 'LastEditorUserId': '7976', 'LastActivityDate': '2014-01-20T06:19:48.023', 'LastEditDate': '2013-11-15T09:44:52.997', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7976', 'Tags': '<algorithms><graph-theory><optimization>', 'CreationDate': '2013-11-15T08:43:01.883', 'FavoriteCount': '3', 'Id': '18041'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This came up while I was trying to answer this question on  <a href="http://cs.stackexchange.com/questions/18041/wiring-length-minimization">Wiring Length Minimization</a>.  I was going to call this the "polygamous marriage" problem, but the internet, so kittens.  Yay!</p>\n\n<p>Suppose we have $M$ kittens that need to be adopted by $N$ people, $M &gt; N$.  For each kitten, $i$ and each person $j$ there is a cost $c_{ij}$.  We would like to minimize the total cost of getting all the kittens adopted.  There is also a set of constraints: each person $j$ is able to adopt no more than $u_j$ kittens.</p>\n\n<p>Without the constraints the problem is easy; each kitten $i$ goes with the person $j$ for which $c_{ij}$ is minimal.  With the constraints is there an efficient algorithm for this problem or is it NP-hard?</p>\n', 'ViewCount': '160', 'Title': 'Complexity of the Kitten Adoption problem', 'LastActivityDate': '2013-11-16T08:03:23.087', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '18065', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '7459', 'Tags': '<algorithms><complexity-theory>', 'CreationDate': '2013-11-15T15:27:48.693', 'FavoriteCount': '4', 'Id': '18051'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider a standard feasibility problem. The goal is to examine the state of feasible solutions for $Ax=b$ to find an $x$ that satisfies some property. Does the dual of this problem tell us anything about the solution? Since there is no objective function that we're examining I think the dual looks like $A^Tx \\leq 0$. Is this right?</p>\n", 'ViewCount': '24', 'Title': 'Does it make sense to examine the dual of a feasbility problem?', 'LastActivityDate': '2013-11-15T22:32:41.220', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11410', 'Tags': '<algorithms><optimization><linear-programming>', 'CreationDate': '2013-11-15T22:32:41.220', 'FavoriteCount': '1', 'Id': '18056'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This sub-problem is motivated by <a href="http://cs.stackexchange.com/q/18010/755">Algorithm to test whether a language is regular</a>.</p>\n\n<p>Suppose we have two languages $L_1,L_2$ that are expressed in "algebraic" form, as formalized below.  I want to determine whether $L_1 = L_2$.</p>\n\n<p>Is this decidable?  If yes, can anyone suggest an algorithm for this?  If not, can anyone suggest a semi-algorithm (one that returns "equal", "not equal", or "I\'m not sure"; where we want it to return "not sure" as rarely as possible).</p>\n\n<p>Motivation: A solution to this sub-problem would be helpful for solving <a href="http://cs.stackexchange.com/q/18010/755">Algorithm to test whether a language is regular</a></p>\n\n<hr>\n\n<p><strong>Algebraic form.</strong> Here\'s one possible definition of what I mean by a representation of a language in algebraic form.  Such a language is given by</p>\n\n<p>$$L = \\{E : S\\}$$</p>\n\n<p>where $E$ is a word-expression and $S$ is a system of linear inequalities over the length-variables, with the following definitions:</p>\n\n<ul>\n<li><p>Each of $x,y,z,\\dots$ is a word-expression.  (These represent variables that can take on any word in $\\Sigma^*$.)</p></li>\n<li><p>Each of $x^r,y^r,z^r,\\dots$ is a word-expression.  (Here $x^r$ represents the reverse of the string $x$.)</p></li>\n<li><p>Each of $a,b,c,\\dots$ is a word-expression.  (Implicitly, $\\Sigma=\\{a,b,c,\\dots\\}$, so $a,b,c,\\dots$ represent a single symbol in the underlying alphabet.)</p></li>\n<li><p>Each of $a^\\eta,b^\\eta,c^\\eta,\\dots$ is a word-expression, if $\\eta$ is a length-variable.</p></li>\n<li><p>The concatenation of word-expressions is a word-expression.</p></li>\n<li><p>Each of $m,n,p,q,\\dots$ is a length-variable.  (These represent variables that can take on any natural number.)</p></li>\n<li><p>Each of $|x|,|y|,|z|,\\dots$ is a length-variable.  (These represent the length of a corresponding word.)</p></li>\n</ul>\n\n<p>My goal is something rich enough to capture many of the examples we see in textbook exercises.  (Feel free to suggest modifications to this formalization if it makes the equality-testing problem easier while still remaining expressive enough to capture many of the examples we see in textbooks.)</p>\n\n<hr>\n\n<p><strong>Some easier cases.</strong> If the original problem is too hard, here are some sub-cases that would still be interesting:</p>\n\n<ul>\n<li><p>If $L_1,L_2$ are two languages specified in "algebraic" form as above, and $L_1\\ne L_2$, let $d(L_1,L_2)$ denote the length of the shortest word that is an element of one of $L_1,L_2$ but not the other.  Can we upper-bound $d(L_1,L_2)$ as a function of the length of the descriptions of $L_1,L_2$?  Is it guaranteed to always be polynomial? at most singly-exponential?</p>\n\n<p>Motivation: If we could prove it is always polynomial, this might help us exhibit a witness that $L_1 \\ne L_2$.</p></li>\n<li><p>If we omit the reversal operation (we do not allow $x^r,y^r,\\dots$), does the problem become easier?</p></li>\n</ul>\n', 'ViewCount': '77', 'Title': 'Test whether two languages are equal, when give in algebraic form', 'LastActivityDate': '2013-11-16T03:08:21.470', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><formal-languages><regular-languages><decision-problem>', 'CreationDate': '2013-11-16T03:08:21.470', 'Id': '18062'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>here is a competitive programming question:</p>\n\n<p>You have a number of chores to do. You can only do one chore at a time and some of them depend on others. \nSuppose you have four tasks to complete. For convenience, we assume the\ntasks are numbered from 1 to 4. Suppose that task 4 depends on both task 2 and task\n3, and task 2 depends on task 1. One possible sequence in which we can complete the\ngiven tasks is [3,1,2,4] - in this sequence, no task is attempted before any of the other\ntasks that it depends on. The sequence [3,2,1,4] would not be allowed because task\n2 depends on task 1 but task 2 is scheduled before task 1. In this example, you can\ncheck that there exactly three possible sequences compatible with the dependencies:\n[3,1,2,4], [1,2,3,4] and [1,3,2,4].\nIn each of the cases below, you have N tasks numbered 1 to N with some dependencies\nbetween the tasks. You have to calculate the number of ways you can reorder all N\ntasks into a sequence that does not violate any dependencies.</p>\n\n<p>[Task, Dependency(s)] : [1, NA], [2,1], [3,2], [4,1], [5,4], [6, 3 and 5], [7,6], [8,7], [9,6], [10, 8 and 9].</p>\n\n<p>I inferred the following:</p>\n\n<ul>\n<li>Any sequence will always start with 1, since it has no dependencies.</li>\n<li>6 will always be in the 6th position of any sequence.</li>\n<li>10 will always be at the last position.</li>\n</ul>\n\n<p>Then, by trial and error, and listing all possibilities for the two separate parts of the sequence (1 _ _ _ _ 6 and 6 _ _ _ 10), I got 6x3 = 18 possibilities. However, for a larger set of data, these deductions would not be easy. What is the way to calculate this logically and find the number of possibilities, and how can this be integrated into a program?</p>\n\n<p>(I have tried to represent the question as clearly as possible, but you can visit this link to view the question (Q. No 4): <a href="http://www.iarcs.org.in/inoi/2013/zio2013/zio2013-qpaper.pdf" rel="nofollow">http://www.iarcs.org.in/inoi/2013/zio2013/zio2013-qpaper.pdf</a>)</p>\n\n<p>I am a high school student preparing for a programming competition, and I haven\'t taken many courses on algorithm design, so this might be a trivial question - please excuse me!</p>\n', 'ViewCount': '89', 'Title': 'Task Dependency/Combinatorics', 'LastActivityDate': '2013-11-17T00:54:12.167', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11415', 'Tags': '<algorithms><algorithm-analysis><combinatorics>', 'CreationDate': '2013-11-16T09:24:13.547', 'FavoriteCount': '1', 'Id': '18067'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I will be given some kind of this graph as in the picture below. I\'ve searched some algorithms but it seams as if it is something impossible for me to figure them out. In fact using <a href="http://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm" rel="nofollow">Floyd\u2013Warshall algorithm</a> it is kinda of possible, but unfortunately I\'m only allowed to use stacks (instead of matrices). I also looked for <a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a> but I could not get the relationship with my problem.<img src="http://i.stack.imgur.com/z6bfG.png" alt="picture"></p>\n\n<p>Clearly my aim is to get all <strong>shortest paths</strong> from one point to another one. As I mentioned I will just output the solution from my <strong>stack</strong> in a vector string. I guess I have to visit each node and what I am most afraid is of getting stacked in a loop or even loosing the track during the search.\nAlso note that this is <strong>not a directed graph</strong>. If  Dijkstra\'s algorithm is applicable here I would be very grateful if anyone of you would guide me and I would really appreciate any help, suggestion, idea or even a vision for not getting stacked in a loop or loosing the track while searching.</p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '81', 'Title': 'Finding Shortest Paths of weighted graph using stacks', 'LastActivityDate': '2013-11-16T11:28:27.430', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11406', 'Tags': '<algorithms><graph-theory><graphs><shortest-path>', 'CreationDate': '2013-11-16T11:28:27.430', 'Id': '18071'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know when we access elements  in rows it will be much faster than if it is accessed column wise. In matrix multiplication one of the matrices must be accessed column wise. \nIn GPUs with CUDA/OpenCL I could resolve this issue by explicitly copying both matrices row-wise into user-managed shared(cache)-memory (or local memory in OpenCL), and then can access data for matrix multiplication in anyway- no penalty for column access.</p>\n\n<p>My question is how do I avoid such column access if I am implementing Matrix-multiplication on CPU, where I do not have any access to such shared memory as in GPU?        </p>\n', 'ViewCount': '84', 'Title': 'How to avoid column wise access in matrix multiplication?', 'LastActivityDate': '2013-11-17T01:45:10.273', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><cpu-cache><performance>', 'CreationDate': '2013-11-16T13:01:29.680', 'FavoriteCount': '1', 'Id': '18072'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>A natural number n represents the initial position in the game. When it is a players turn he/she is allowed to </p>\n\n<pre><code>I)  Subtract 2 from n\nII) Subtract 3 from n\nIII)    Subtract 5 from n\n</code></pre>\n\n<p>We call the player who begin the game Adam and the other player Berta. The players alternate by applying on of the three rules to the number 0 or a negative number his/her opponent. If a player manages to produce the number 0 or a negative number he/she wins the game.</p>\n\n<p>Here is an example of a game played by Adam and Berta (for n=15)</p>\n\n<pre><code>15 is given to Adam. He decides to subtract 5 leaving 15-5 = 10 to Berta\n10 is given to Berta. She decides to subtract 3 leaving 11-3=8 to Beta\n8 is given to Adam. He decides to subtract 2 leaving 8-2=6 to Berta\n6 is given to Berta. She decides to subtract 2 leaving 6-2=4 to Adam\n4 is given to Adam. He decides to subtract 5 producing -1 a negative number, Adam wins!\n</code></pre>\n\n<p>b) we define a one dimensional array X(1), X(2),X(3),..,X(n)</p>\n\n<pre><code>i)  X(j) =1 if Adam has a method to win when given the number j\nii) X(j)=0 if Adam has no method that guarantees that he wins when the given the number k\n</code></pre>\n\n<p>Calculate X(1),X(2),X(3)\u2026.,X(23),X(25)</p>\n\n<p>What is X(8), X(13) and X(24)?     Answer should be of the form\nboolean boolean boolean  so if X(8)=0 , X(13)=1 and X(24)=1 the correct answer is 011\nThus the correct answer is one of the following 000 001 010 011 100 101 110 111</p>\n\n<p>My attempt is </p>\n\n<pre><code>n=8\nAdam: 8-5=3\nBerta: 3-3 =0\nBerta wins  0\n\nn=13 \nAdam=13-5=8\nBerta: 8-3=5\nAdam 5- 5\nAdam wins 1\n\nI get really stuck with 24, so far I have 01 \n</code></pre>\n\n<p>Is there a method for this type of problem, i have been stuck on it for ages now. \nThanks in advance</p>\n', 'ViewCount': '176', 'Title': 'Applying dynamic programming to a simple two-person game of perfect information', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-16T08:01:39.513', 'LastEditDate': '2014-02-16T08:01:39.513', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7173', 'Tags': '<algorithms><complexity-theory>', 'CreationDate': '2013-11-16T21:17:32.187', 'Id': '18082'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>When using <a href="http://en.wikipedia.org/wiki/Resolution_%28logic%29" rel="nofollow">resolution</a>, if the empty set {\xd8} is derived from a formula like {\xacx,\xacy} {x,y}, does that mean the formula is unsatisfiable? </p>\n\n<p>If this is the case, why <strong>is</strong> <code>{x,y},{\xacx,\xacy},{x,\xacy}</code> satisfiable, and </p>\n\n<p><code>{x},{\xacx},{x,y,z,w}</code> <strong>is not</strong> satisfiable? </p>\n', 'ViewCount': '92', 'Title': 'Resolution and what it means to derive the empty set', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T07:53:18.520', 'LastEditDate': '2014-03-31T07:53:18.520', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7173', 'Tags': '<algorithms><logic><satisfiability><propositional-logic>', 'CreationDate': '2013-11-16T21:19:46.557', 'Id': '18083'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to find for any given vertex in a polygon stored in a doubly-connected edge list if the polygon is to its right or not. How do I do that without having a bunch of nested if statements?</p>\n\n<p>It seems reasonable to expect a more elegant solution that uses the DCEL.</p>\n\n<p>Thank you.</p>\n\n<p>EDIT: the vertices have coordinates and are stored in a normal DCEL data structure along with the half edges and polygons. </p>\n', 'ViewCount': '96', 'Title': 'How do you find out with a DCEL if the face is to the right of a vertex?', 'LastEditorUserId': '10269', 'LastActivityDate': '2013-11-20T15:10:47.230', 'LastEditDate': '2013-11-17T09:27:16.113', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '18165', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10269', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2013-11-17T06:39:35.590', 'Id': '18092'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to implement TSP algorithm by brute force for learning purposes.</p>\n\n<p>I\'ve understood there\'s a set of cities, let\'s call it <strong><em><code>V</code></em></strong> and it\'s possible to get a matrix representation for the costs for going from a <strong><em><code>v1</code></em></strong> city to a <strong><em><code>v2</code></em></strong> city. I\'ll assume there are not cycles, so  it\'s no possible to going from <strong><em><code>v1</code></em></strong> back to <strong><em><code>v1</code></em></strong></p>\n\n<p>Then, I should generate a matrix after these sum series:</p>\n\n<p><img src="http://i.stack.imgur.com/lXusa.png" alt="Formulas"></p>\n\n<p><strong>However, I really can\'t see in a practice way how a matrix would be outputed from restrictions.</strong></p>\n\n<p>Let\'s say we got 3 cities:</p>\n\n<blockquote>\n  <p>Madrid, Berlin and Malmo</p>\n</blockquote>\n\n<p>So the path costs are (and they\'re not forced to be the same for ways back):</p>\n\n<p><strong>From Madrid to:</strong></p>\n\n<p>Berlin: 12<br>\nMalmo:  20</p>\n\n<p><strong>From Berlin to</strong></p>\n\n<p>Madrid: 32<br>\nMalmo:  12</p>\n\n<p><strong>From Malmo to:</strong></p>\n\n<p>Madrid: 14<br>\nBerlin: 17</p>\n\n<p>So my input is:    </p>\n\n<p>$$\\begin{bmatrix} 0 &amp; 12 &amp; 20 \\\\ 32 &amp; 0 &amp; 12 \\\\ 14 &amp; 17 &amp; 0\\end{bmatrix}$$    </p>\n\n<p>How should the matrix be outputed according to the sum series?</p>\n\n<blockquote>\n  <p><em><strong>I assume the algorithm would need to generate a matrix from this exampple:</em></strong></p>\n</blockquote>\n\n<p>There are 3 cities <strong><em><code>x1</code></em></strong>, <strong><em><code>x2</code></em></strong> and <strong><em><code>x3</code></em></strong> and got the costs matrix shown below:</p>\n\n<p>$$\\begin{bmatrix} 10 &amp; 2 &amp; 1 \\\\ 3 &amp; 10 &amp; 2 \\\\ 11 &amp; 2 &amp; 10\\end{bmatrix}$$ </p>\n\n<p>Following, example shows the next matrix:</p>\n\n<p>$$\\begin{bmatrix} x_{11}+x_{12}+x_{13} &amp;  &amp;  \\\\  &amp; x_{21}+x_{22}+x_{23} &amp;  \\\\  &amp;  &amp; x_{31}+x_{32}+x_{33}\\\\x_{11} &amp; x_{21} &amp; x_{31} \\\\ x_{12} &amp; x_{22} &amp; x_{32} \\\\  x_{13}&amp; x_{23} &amp; x_{33} \\end{bmatrix}$$ </p>\n\n<p>Which would be the same as:</p>\n\n<p>$$\\begin{bmatrix} 1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1\\\\ 1&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;1&amp;0\\\\ 0&amp;0&amp;1&amp;0&amp;0&amp;1&amp;0&amp;0&amp;1\\\\ 0&amp;1&amp;0&amp;0&amp;1&amp;0&amp;0&amp;1&amp;0\\\\ \\end{bmatrix}$$ </p>\n\n<p>Then, considering the power ser of the three cities, matrix got the following rows added:</p>\n\n<p>$$\\begin{bmatrix} 0&amp;1&amp;1 &amp; 0&amp;0&amp;0 &amp; 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 &amp; 1&amp;0&amp;1 &amp; 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 &amp; 0&amp;0&amp;0 &amp; 1&amp;1&amp;0 \\\\ 0&amp;0&amp;1 &amp; 0&amp;0&amp;1 &amp; 0&amp;0&amp;0 \\\\ 0&amp;1&amp;0 &amp; 0&amp;0&amp;0 &amp; 0&amp;1&amp;0 \\\\ 0&amp;0&amp;0 &amp; 1&amp;0&amp;0 &amp; 1&amp;0&amp;0  \\end{bmatrix}$$ </p>\n\n<p>Finally, the whole generated matrix is:</p>\n\n<p>$$\\begin{bmatrix} 1&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1&amp;1\\\\ 1&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;1&amp;0\\\\ 0&amp;0&amp;1&amp;0&amp;0&amp;1&amp;0&amp;0&amp;1\\\\ 0&amp;1&amp;0&amp;0&amp;1&amp;0&amp;0&amp;1&amp;0\\\\ 0&amp;1&amp;1 &amp; 0&amp;0&amp;0 &amp; 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 &amp; 1&amp;0&amp;1 &amp; 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 &amp; 0&amp;0&amp;0 &amp; 1&amp;1&amp;0 \\\\ 0&amp;0&amp;1 &amp; 0&amp;0&amp;1 &amp; 0&amp;0&amp;0 \\\\ 0&amp;1&amp;0 &amp; 0&amp;0&amp;0 &amp; 0&amp;1&amp;0 \\\\ 0&amp;0&amp;0 &amp; 1&amp;0&amp;0 &amp; 1&amp;0&amp;0 \\end{bmatrix}$$ </p>\n', 'ViewCount': '119', 'Title': 'Proper TSP implementation by brute force', 'LastEditorUserId': '5066', 'LastActivityDate': '2013-11-19T19:58:20.210', 'LastEditDate': '2013-11-19T19:58:20.210', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5066', 'Tags': '<algorithms><traveling-salesman>', 'CreationDate': '2013-11-17T20:39:11.493', 'Id': '18104'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m building a load test harness for a distributed system. Currently I\'m using the "<a href="http://www.cs.rutgers.edu/~muthu/bquant.pdf" rel="nofollow">Cormode, Korn, Muthukrishnan, and Srivastava</a>" method to estimate latency quantiles of system responses.</p>\n\n<p>I\'m now testing systems larger than can be adequately stressed using a single load generator. I\'m looking for a method similar to the one cited above that can be extended to a cluster of load generators. </p>\n\n<p>I would like to measure the latency quantiles of responses to the cluster as a whole, and to individual nodes in the cluster without recording/streaming the latency of every event in the system to a central node. </p>\n\n<p>Does such a method exist? If so, are there any public implementations?</p>\n', 'ViewCount': '24', 'Title': 'Efficiently estimating latency quantiles of a distributed system', 'LastActivityDate': '2013-11-18T17:31:59.213', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11457', 'Tags': '<algorithms><distributed-systems><statistics>', 'CreationDate': '2013-11-18T17:31:59.213', 'Id': '18118'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need some help understanding the problem statement in CLRS 11.3-4 pg 269.  It says:</p>\n\n<blockquote>\n  <p>Consider a hash table of size $m = 1000$ and a corresponding hash function \n   $ h(k) =\\left \\lfloor m(kA \\bmod 1) \\right \\rfloor $\n  for $A = (\\sqrt5 - 1)/2$. Compute the locations to which the keys\n  61, 62, 63, 64, and 65 are mapped.</p>\n</blockquote>\n\n<p>Is $\\mod 1$ a misprint in my copy? Also $A$ is a real number.  Is modulus defined for non-integer numbers?</p>\n', 'ViewCount': '122', 'Title': 'How does taking modulo 1 make sense?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-20T14:54:20.567', 'LastEditDate': '2013-11-18T21:36:32.413', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18201', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4251', 'Tags': '<algorithms><terminology><hash>', 'CreationDate': '2013-11-18T21:11:51.583', 'Id': '18130'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>consider a program that generates a <a href="http://en.wikipedia.org/wiki/Random_walk" rel="nofollow">random walk</a> using a <a href="http://en.wikipedia.org/wiki/Pseudorandom_number_generator" rel="nofollow">PRNG</a>, as in following pseudocode. it uses <a href="http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic" rel="nofollow">arbitrary precision arithmetic</a> such that there is no limit on variable values (ie no overflow).</p>\n\n<pre><code>srand(x)\nz = 0\nwhile (z &gt;= 0)\n{\n  r = rand(100)\n  if (r &lt;= 50) z -= 1\n  else z += 1\n}\n</code></pre>\n\n<p>the PRNG is inited with seed <code>x</code> <em>(also arbitrary precision).</em> the PRNG <code>rand(100)</code> generates a value between <code>0..99</code>. hence for 51 values the accumulator var <code>z</code> is decremented, for 49 values it is incremented.</p>\n\n<p>it is expected due to the <a href="http://en.wikipedia.org/wiki/Law_of_large_numbers" rel="nofollow">law of large numbers</a> that this program will halt for all initial seeds <code>x</code>. however, </p>\n\n<blockquote>\n  <p>how does one prove it will halt for all initial seeds <code>x</code>?</p>\n</blockquote>\n\n<p>it seems such a proof must depend on the details of the construction of the PRNG. am assuming there exist PRNGs such that a different random sequence is generated for every initial seed <code>x</code> (ie the infinite set of naturals). that in itself may be up for question. are such PRNGs known? where are they used? etc.. so an answer may come up with an arbitrary PRNG for the purposes of the question. a single example fulfilling the criteria would be an acceptable answer.</p>\n\n<p>looking for related literature, similar problems/proof considered, etc.</p>\n', 'ViewCount': '58', 'Title': 'proof of convergence in arbitrary precision PRNGs', 'LastActivityDate': '2013-11-19T00:38:41.127', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><reference-request><probability-theory><pseudo-random-generators><random-walks>', 'CreationDate': '2013-11-18T21:52:05.343', 'FavoriteCount': '1', 'Id': '18132'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have asked this question in StackOverflow. I was asked to move in here. so here it is:</p>\n\n<p>I need some clarifications and inputts regarding <code>Dijktra\'s algorithm</code> vs <code>breath first search</code> in directed graphs, if these are correct.</p>\n\n<p><code>Dijktra\'s</code> algorithm finds the shortest path from Node <code>A</code> to Node <code>F</code> in a <code>weighted</code> graph regardless of if there is a cycle or not (as long as there are no negative weights)</p>\n\n<p>but for that, All paths from <code>A to all other Nodes in the graph are calculated and we grab the path from</code>A<code>to</code>F<code>by reversing the sequences of nodes in</code>prev`.</p>\n\n<p>BFS: finds the shortest path from <code>Node A</code> to <code>Node F</code> in a non-weighted graph, but if fails if  a cycle detected. </p>\n\n<p>however, <code>BFS</code> just calculates the path from Node A to Node F and not necessarily all path from Node A.\nif Node F is reached early, it just returns the path.<img src="http://i.stack.imgur.com/A0hho.png" alt="enter image description here"></p>\n', 'ViewCount': '216', 'Title': 'Dijktra algorithm vs breath first search for shortest path in graph', 'LastActivityDate': '2013-11-19T22:06:59.357', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11469', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-11-19T02:18:27.230', 'Id': '18138'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm given this problem:</p>\n\n<p>Consider the following closest-point heuristic for building an approximate traveling-salesman  tour. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex u that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest u is vertex v. Extend the cycle to include u by inserting u just after v. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour. </p>\n\n<p>This is the same as Prim's algorithm.  Unless I'm missing something, this is not an approximate traveling salesman tour since the traveling salesman requires a Hamiltonian path where we don't revisit any nodes, but on many graphs this algorithm seems to require revisiting nodes to get back to the source node.  Am I wrong or is this problem unclearly worded?</p>\n", 'ViewCount': '120', 'Title': "Traveling Salesman's Tour Approx Algorithm: is this really a Hamiltonian Path?", 'LastActivityDate': '2013-11-19T10:58:35.200', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '18152', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11474', 'Tags': '<traveling-salesman><spanning-trees><approximation-algorithms>', 'CreationDate': '2013-11-19T06:43:05.060', 'Id': '18147'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the linear programs </p>\n\n<p>\\begin{array}{|ccc|}\n\\hline\nPrimal: &amp; A\\vec{x} \\leq \\vec{b} \\hspace{.5cm} &amp; \n\\max \\vec{c}^T\\vec{x} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|ccc|}\n\\hline\nDual: &amp; \\vec{c} \\leq \\vec{y}^TA \\hspace{.5cm} &amp;\n\\min \\vec{y}^T\\vec{b} \\\\\n\\hline\n\\end{array}</p>\n\n<p>The weak duality theorem states that \nif $\\vec{x}$ and $\\vec{y}$ satisfy the constraints then\n$\\vec{c}^T\\vec{x} \\leq \\vec{y}^T\\vec{b}$.\nIt has a short and slick proof using linear algebra:\n$\\vec{c}^T\\vec{x} \\leq  \\vec{y}^T A \\vec{x} \\leq \\vec{y}^T\\vec{b}$.</p>\n\n<p>The strong duality theorem states that if the $\\vec{x}$ is an optimal solution for the primal then there is $\\vec{y}$ which is a solution for the dual and \n$\\vec{c}^T\\vec{x} = \\vec{y}^T\\vec{b}$.</p>\n\n<p>Is there a similarly short and slick proof for the strong duality theorem?</p>\n', 'ViewCount': '213', 'Title': 'Short and slick proof of the strong duality theorem for linear programming', 'LastActivityDate': '2013-11-20T00:00:15.410', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<algorithms><reference-request><linear-programming><linear-algebra><duality>', 'CreationDate': '2013-11-19T09:39:14.263', 'Id': '18151'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm having trouble understanding the mechanics of the midpoint algorithm. I understand the gist of what it does; it keeps us within a half a pixel of where the actual line should be printed. It does this by updating this $d$ value for each pixel that we traverse. </p>\n\n<p>However, even after drawing some samples and observing what the $d$ value does, I can't figure out the true inner workings of it, and how updating it by these $\\Delta_e$ and $\\Delta_{ne}$ symbols (which are also a bit of a mystery to me) keeps our pixel placement in check.</p>\n\n<p>Can someone summarize it in simple terms for the good of all humanity?</p>\n\n<p><strong>The Algorithm:</strong></p>\n\n<pre><code>Line (x1, y1, x2, y2)\n    begin\n    int x, y, dx, dy, d, deltaE, deltaNE;\n    x &lt;- x1;        y  &lt;- y1;\n    dx &lt;- x2 - x1;  dy &lt;- y2 - y1;\n    d &lt;- 2*dy - dx;\n    deltaE &lt;- 2*dy;     deltaNE &lt;- 2*(dy - dx);\n\n    PlotPixel(x, y);\n    while ( x &lt; x2) do\n        if (d &lt; 0) then\n        begin\n            d &lt;- d + deltaE;\n            x &lt;- x + 1;\n        end;\n        else begin\n            d &lt;- d + deltaNE;\n            x &lt;- x + 1;\n            y &lt;- y + 1;\n        end;\n        PlotPixel (x, y);\n    end;\nend;\n</code></pre>\n", 'ViewCount': '146', 'Title': 'How does the midpoint line drawing algorithm work?', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-20T00:32:33.500', 'LastEditDate': '2013-12-20T00:32:33.500', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><graphics>', 'CreationDate': '2013-11-19T14:34:50.693', 'Id': '18156'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a distance to get to, and square tiles that have a cost and length. EX: a 1 unit block that costs 1 unit to purchase.</p>\n\n<p>So if I was trying to get 10 units away. I would require 10 of the 1 unit blocks at that point for a total cost of 10.</p>\n\n<p>So another example would be a distance of 10 units away except I have two tiles to pick from of [1,1], [5,4], [x,y], x = length, y = cost.</p>\n\n<p>The cheapest cost would be 8, (2 of 5 blocks = 10 distance).</p>\n\n<p>So, at first I thought this would be a shortest path problem, but seeing how I can re-use tiles that didn't work out the best.</p>\n\n<p>I then tried to implement a rod-cutting algorithm, using the length of the rod as distance and the price for the length of cuts. This algorithm though kept trying to maximize the distance, which wasn't what I wanted.</p>\n\n<p>My final method, was to divide the cost by length for each tile to find the most cost efficient tile (shown below). This worked for about 9% of my unit-tests, it was failing most because it couldn't see that it might be more cost effective to use a more expensive tile, etc.</p>\n\n<p>I started with a greedy algorithm using cost / length for most efficient cost tiles, when I couldn't get that to work. I moved towards a dynamic programming algorithm using the cutting rod example.</p>\n\n<p>This required a lot of hacky work, since I didn't really want the maximized value via cutting. I wanted the minimal cost value of that distance, though it also only had about a 2% success rate.</p>\n\n<p>I then went into recursion, with help from another tip. Using a tiny little Tile class to help me pass variables around, this had lots of success with my self test values, but once again only had a 10% success rate.</p>\n\n<p>So after trying DFS, Rod-Cutting, Recursion, and Dynamic Programming. I am lost. Did I overlook one of these algorithms? </p>\n", 'ViewCount': '129', 'Title': 'shortest cost tiling of path to x distance', 'LastEditorUserId': '11483', 'LastActivityDate': '2013-11-20T02:46:29.247', 'LastEditDate': '2013-11-20T02:46:29.247', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '18184', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11483', 'Tags': '<algorithms>', 'CreationDate': '2013-11-19T16:08:54.803', 'Id': '18160'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a $n\\times n$ matrix <code>A[0...n-1][0....n-1]</code> where all entries are non-negative integers, and a non-negative integer <code>K</code>, I want to find the number of submatrices whose entries sum to <code>K</code>.</p>\n\n<p>The best solution I can find has $O(n^4)$ running time.  Any ideas how to improve the running time?</p>\n', 'ViewCount': '208', 'Title': 'Number of submatrices with a particular sum', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-20T02:15:50.597', 'LastEditDate': '2013-11-20T01:59:10.917', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '3', 'OwnerDisplayName': 'hello', 'PostTypeId': '1', 'Tags': '<algorithms><arrays><matrices>', 'CreationDate': '2013-11-19T16:37:23.323', 'Id': '18173'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I participated in one algorithmic competition. I got stuck in one problem, I am asking the same here.</p>\n\n<p><strong>Problem Statement</strong></p>\n\n<p>XOR-sum array is to XOR all the numbers of that sub-array.\nAn array is given to you, you have to add all possible such XOR-sub-array.</p>\n\n<p><strong>Example</strong></p>\n\n<p><strong>Input</strong></p>\n\n<p>Array :- 1 2</p>\n\n<p><strong>Output</strong> :- 6</p>\n\n<p><strong>Explanation</strong></p>\n\n<p>F(1, 1) = A[1] = 1, F(2, 2) = A[2] = 2 and F(1, 2) = A[1] XOR A[2] = 1 XOR 2 = 3. Hence the answer is 1 + 2 + 3 = 6.</p>\n\n<p>I found an $O(N^2)$ solution, but it was too inefficient one and wasn\'t accepted in the competition.</p>\n\n<p>I saw the best solution of this problem <a href="http://www.codechef.com/viewplaintext/2928504" rel="nofollow">on Code Chef</a>. But in this code, I didn\'t understand below module, please help me to understand that.</p>\n\n<pre><code>for (int i=0, p=1; i&lt;30; i++, p&lt;&lt;=1) {\n    int c=0;\n    for (int j=0; j&lt;=N; j++) {\n        if (A[j]&amp;p) c++;\n    }\n    ret+=(long long)c*(N-c+1)*p;\n}\n</code></pre>\n\n<p>In pseudocode:</p>\n\n<ul>\n<li>Set $r = 0$</li>\n<li>For $i$ from $0$ to $29$, let $p = 2^i$.\n<ul>\n<li>Set $c=0$</li>\n<li>Iterate over the array $A$. For each element $A[j]$:\n<ul>\n<li>If $A[j] \\mathbin\\&amp; p \\ne 0$ then increment $c$. ($\\&amp;$ is bitwise and.)</li>\n</ul></li>\n<li>Add $c \\times (N-c+1) \\times p$ to $r$</li>\n</ul></li>\n</ul>\n', 'ViewCount': '374', 'Title': 'Algorithm to add sum of every possible xor-sum sub-array', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-21T12:19:12.663', 'LastEditDate': '2013-11-21T12:19:12.663', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2715', 'Tags': '<algorithms><efficiency>', 'CreationDate': '2013-11-20T10:40:38.763', 'Id': '18194'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '185', 'Title': 'Maxima of diagonals in a column wise and row wise sorted matrix', 'LastEditDate': '2013-11-22T12:24:54.567', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '220', 'FavoriteCount': '1', 'Body': '<p>Let $\\{a_i\\}$ and $\\{b_i\\}$ be non-decreasing sequences of non-negative integers.</p>\n\n<p>How fast can one find \n$$c_j=\\max_{0 \\leq i&lt; j}\\{a_i+b_{j-i-1}\\}$$\nfor all $0\\leq j\\leq n-1$?</p>\n\n<p>Naively, it takes $O(n^2)$ time, but I\'m hoping monotonicity can help here.</p>\n\n<p>It\'s easy to observe $\\{c_i\\}$ is also non-decreasing. If we consider the matrix $M$ where $M_{i,j} = a_i+b_j$, then it is a matrix sorted in both row and column direction, and we are searching for the maximum element in every diagonal. </p>\n\n<p>However, if it\'s an arbitrary column wise and row wise sorted matrix, this problem require $\\Omega(n^2)$ time.</p>\n\n<p>Proof: Let all the numbers below the main diagonal be $\\infty$. The elements in the $k$th diagonal are randomly numbers from $(k,k+1)$. Reading any entry provides no information for any other entry. </p>\n\n<p>Edit: This problem is much harder than I anticipated. We can model this problem as a convolution problem over the semiring $(\\min,+)$ (take the dual, search for min instead of max), and it can be solved in $O(\\frac{n^2}{\\log n})$ time according to a <a href="http://mathoverflow.net/a/11606/6886">Ryan Williams\'s answer on mathoverflow</a>. It doesn\'t use the information that the sequence is non-decreasing though.</p>\n', 'Tags': '<algorithms>', 'LastEditorUserId': '220', 'LastActivityDate': '2013-11-22T16:16:46.693', 'CommentCount': '0', 'AcceptedAnswerId': '18252', 'CreationDate': '2013-11-20T23:35:03.953', 'Id': '18211'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>CS sometimes seems take for granted that $\\mathcal O(\\text{poly}(n))$ is "easy", while $\\mathcal O\\left(2^{poly(n)}\\right)$ is "difficult". I am interested in research into "difficult" polynomial-time algorithms, where the best-case solution to the constructed problem runs in $\\Theta(n^c)$, where $c$ can be chosen to be large; but the solution could be tested in $O(n)$ time.</p>\n\n<p><b>Question:</b></p>\n\n<blockquote>\n  <p>Given an integer $c$, can we construct problems that would:</p>\n  \n  <ul>\n  <li>Take $\\Theta\\left(n^c\\right)$ best-case-time to solve,</li>\n  <li>While taking $\\tilde{\\mathcal O}(n)$ time, and $\\tilde{\\mathcal O}(n)$ space, to test a solution?</li>\n  </ul>\n</blockquote>\n\n<p>($\\tilde{\\mathcal O}(n)$ is <a href="https://en.wikipedia.org/wiki/Big_O_notation#Extensions_to_the_Bachmann.E2.80.93Landau_notations" rel="nofollow">soft-big-oh</a>, meaning $O(n \\log^k n)$ for some $k$)</p>\n\n<hr>\n\n<p>Something I note - I might be mistaken somewhere here - is that presumably, if there is a $\\mathcal O(n)$ algorithm to test the solution, then perhaps there can be a $\\mathcal O(n)$ reduction to $\\rm k\\text{-}SAT$. If so, and, if $\\rm P=NP$, and there was a polynomial-time algorithm, say ${\\rm S{\\small OLVE}}\\left(\\Phi(\\mathbf x)\\right) \\in O({|\\mathbf x|}^{\\alpha})$ time, then I think this might contradict our $\\Theta(n^c)$ problem, if $\\alpha &lt; c$.</p>\n\n<hr>\n\n<p>The motivation would be to research the possibility of having a "one-way-function", that is linear(ithmic)-time computable, and best-case "difficult"-polynomial-time invert-able, where "difficult" means a high degree polynomial, instead of the usual exponential-time definition of "difficult"; perhaps this might be able to be used for cryptography even if $\\rm P=NP$ (like "post-P-equals-NP-cryptography", similar to how there is a field of "post-quantum-cryptography").</p>\n', 'ViewCount': '156', 'Title': 'Can we construct problems that can be solved in $\\Theta(n^c)$ time, and tested in $O(n)$ time', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-12-10T15:31:18.290', 'LastEditDate': '2013-12-10T15:31:18.290', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><reference-request><time-complexity><algorithm-synthesis>', 'CreationDate': '2013-11-21T14:40:26.493', 'FavoriteCount': '3', 'Id': '18223'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>One can, for example, represent 2d arrays such as:</p>\n\n<pre><code>[[1,2],[3,4],[4,5]]\n</code></pre>\n\n<p>as flat arrays:</p>\n\n<pre><code>[1,2,3,4,5,6]\n</code></pre>\n\n<p>as long as he transforms the indices before accessing:</p>\n\n<pre><code>index(x,y) = x + y*2 // because internal width=2\n</code></pre>\n\n<p>This is often faster. My question is: is it possible to use a similar approach of representing an structure as a flat array, for, instead of n-dimensional tables, free-form trees such as:</p>\n\n<pre><code>[[1,2,3],4,[5,[6,7]],8]\n</code></pre>\n', 'ViewCount': '141', 'Title': 'Is there an structure that allows for a flat representation of trees with constant access to any element?', 'LastActivityDate': '2013-11-27T16:26:44.323', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-11-22T01:24:15.343', 'Id': '18247'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>During the process of deadlock detection, the wait-for graph can be obtained from the resource-allocation graph. To detect whether there is a deadlock using the wait-for graph, can topological sort be used? </p>\n\n<p>By making a slight modification, i.e., instead of removing the source as done in usual topological sort, we can remove the node (here, node represents a process) which does not point to another node first and proceed in this manner. As soon as the algorithm detects that it cannot proceed further and there are still nodes existing in the wait-for graph, it can be concluded as deadlock.</p>\n\n<p>Other tags are topological-sort and deadlock-detection.</p>\n', 'ViewCount': '85', 'Title': 'Algorithm for wait-for graph', 'LastActivityDate': '2013-11-26T08:54:56.227', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18360', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10120', 'Tags': '<algorithms><operating-systems>', 'CreationDate': '2013-11-23T12:56:55.543', 'Id': '18274'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Below is a homework problem where we have been asked to alter a greedy algorithm to return n element instance of a set problem. The original algorithm is also below. I was thinking that I could alter line 3 so that it would run until the size of C was equal to n, and change the logic in line 4 so that it would pick and remove vertices until the size of n. A vertex would be removed when the size of C doesn't equal to n but the cover is complete. I can't really think of any other way to do it. The real problem is that I'm not entirely sure how to make the algorithm run in exponential time like they are asking. </p>\n\n<blockquote>\n  <p>GREEDY-SET-COVER can return a number of different solutions, depending on\n  how we break ties in line 4. Give a procedure BAD-SET-COVER-INSTANCE.n/\n  that returns an n-element instance of the set-covering problem for which, depending\n  on how we break ties in line 4, GREEDY-SET-COVER can return a number of\n  different solutions that is exponential in n.</p>\n  \n  <p>$X$ \u2014 some finite set<br>\n  $F$ \u2014 a family of subsets of $X$<br>\n  $C$ \u2014 cover being constructed    </p>\n  \n  <p>GREEDY-SET-COVER($n$)<br>\n  1    let $U = X$<br>\n  2    let $C = \\varnothing$<br>\n  3    while $U \\ne \\varnothing$<br>\n  3a            select an $S \\in F$ that maximizes $\\left|S \\cap U\\right|$<br>\n  3b            set $U = U \\setminus S$<br>\n  3c            set $C = C \\cup \\{S\\}$<br>\n  4    return $C$   </p>\n</blockquote>\n\n<p>Could it be said that since the number of subsets a set has is $2^n$ and that in the worst case this algorithm will end up finding all of those subsets before settling on an n-instance set to return?</p>\n", 'ViewCount': '112', 'Title': 'Finding an instance of an n-element set cover', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-28T21:30:30.133', 'LastEditDate': '2013-11-28T21:30:30.133', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11580', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms><set-cover><approximation-algorithms>', 'CreationDate': '2013-11-23T23:48:31.680', 'Id': '18287'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve got 30 elements which has to be  grouped/sorted into 10 ordered 3-tuple. \nThere are several rules and constraints about grouping/sorting.\nFor example: Element $A$ must not be in the same tuple same unit $B$. \nElement $C$ must not be right in front of element $A$, etc.</p>\n\n<p>I am searching for an approximated algorithm:</p>\n\n<ol>\n<li>We don\'t need to achieve the exact optimum </li>\n<li>It is OK for some rules not to be satisfied, if it helps to fulfill more rules.</li>\n</ol>\n\n<p>Do you know of any algorithm/proceeding that solve this problem or a similar one?\nI fear to solve it in an optimal way, you have to try out every possible solution-> $2 ^ {30}$</p>\n\n<p><strong>EDIT</strong>: Sorry for the bad explanation. I am trying to make it a bit clearer:\nI got 30 elements for example: $\\{1,2,3,\\ldots,30\\}$.\nI need to group them into 3-tuples so that i get something like: $(1,2,3)$, $(4,5,6)$,$\\ldots$,$(28,29,30)$.</p>\n\n<p>There are several constraints. For example: </p>\n\n<ul>\n<li>1 cannot precede 2 in an ordered tuple, so, for instance  $(1,2,3)$ is not a valid tuple.</li>\n<li>5 must be together with 4. </li>\n</ul>\n\n<p>Those constraints can be broken and its possible that there is no solution where all rules can be fulfilled. <br />An solution is considered as good if the amount of rules broken is "low".</p>\n\n<p>Hope that makes it clearer and thanks for the help so far.</p>\n', 'ViewCount': '81', 'Title': 'Algorithm for sorting with constraints', 'LastEditorUserId': '157', 'LastActivityDate': '2013-12-16T01:48:30.027', 'LastEditDate': '2013-12-16T00:37:15.273', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '18345', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11601', 'Tags': '<algorithms><sorting><randomized-algorithms><greedy-algorithms>', 'CreationDate': '2013-11-25T00:14:58.160', 'Id': '18312'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '110', 'Title': 'Chernoff bounds and Monte Carlo algorithms', 'LastEditDate': '2013-11-25T14:30:48.973', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'FavoriteCount': '1', 'Body': "<p>One of Wikipedia examples of use of Chernoff bounds is the one where an algorithm $A$ computes the correct value of function $f$ with probability $p &gt; 1/2$. Basically, Chernoff bounds are used to bound the error probability of $A$ using repeated calls to $A$ and majority agreement.</p>\n\n<p>I don't understand how, to be frank. It would be nice if somebody could break it down piece by piece. Moreover, does it matter whether $A$ is a decision algorithm or can return more values? How are Chernoff bounds in general used for Monte Carlo algorithms?</p>\n", 'Tags': '<randomized-algorithms><randomness><chernoff-bounds>', 'LastEditorUserId': '8508', 'LastActivityDate': '2013-11-25T19:22:19.073', 'CommentCount': '2', 'AcceptedAnswerId': '18329', 'CreationDate': '2013-11-25T06:16:42.047', 'Id': '18321'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The <a href="https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol#Timestamp" rel="nofollow">ICMP timestamp</a> protocol is useful for determining which path \u2014 forward or reverse \u2014 is contributing to the jitter on the line.</p>\n\n<p>In an ideal world, all computers would have an excellent ntpd, and time accuracy of about 1ms (the granularity of ICMP timestamp) should not be a problem:</p>\n\n<pre><code>0       145.5   146 = 75 + 71\n1       142.7   142 = 72 + 70\n2       140.7   140 = 70 + 70\n3       146.7   146 = 76 + 70\n4       148.3   148 = 77 + 71\n5       157.5   157 = 87 + 70\n6       167.1   167 = 96 + 71\n7       166.3   166 = 95 + 71\n8       167.7   167 = 97 + 70\n9       159.0   159 = 88 + 71\n</code></pre>\n\n<p>However, in reality, as per <a href="http://stackoverflow.com/questions/20172028/awk-hping-print-difference-between-icmp-originate-receive/20186781#20186781">http://stackoverflow.com/questions/20172028/awk-hping-print-difference-between-icmp-originate-receive/20186781#20186781</a>, the data could be like this:</p>\n\n<pre><code>0       165.9   166 = -142113 + 142279\n1       160.2   160 = -142118 + 142278\n2       155.2   155 = -142122 + 142277\n3       156.5   156 = -142121 + 142277\n4       164.7   165 = -142112 + 142277\n5       164.4   164 = -142111 + 142275\n6       160.9   161 = -142114 + 142275\n7       158.1   158 = -142117 + 142275\n8       155.6   156 = -142119 + 142275\n9       143.0   143 = -142131 + 142274\n10      153.2   153 = -142120 + 142273\n11      157.1   157 = -142115 + 142272\n12      158.3   158 = -142114 + 142272\n13      148.6   149 = -142123 + 142272\n14      144.3   144 = -142127 + 142271\n15      145.3   145 = -142125 + 142270\n</code></pre>\n\n<p>Which still shows that only one path is responsible for the jitter, since only one value jumps up and down randomly, whereas the other one is decreasing monotonically (probably due to an actively-running ntpd, which is correcting the time as we ping).</p>\n\n<p>Another example could be less wrong-looking of the time not being synchronised, say:</p>\n\n<pre><code>0       165.9   166 = -113 + 279\n</code></pre>\n\n<p>Or, better yet:</p>\n\n<pre><code>0       165.9   166 = 7 + 159\n</code></pre>\n\n<p>Or, say, still wrong by some 10ms to 40ms, on a landline link from Alberta to Vogtland, but much less obvious:</p>\n\n<pre><code>0       165.9   166 = 59 + 107\n</code></pre>\n\n<p>How to make scientific sense of this data, for it to be easily presentable, and not blatantly wrong?</p>\n\n<p>Feel free to assume to have between 10 to 1000 measurement points, measured over a couple of seconds or minutes.</p>\n', 'ViewCount': '50', 'Title': 'How to present ping data from ICMP timestamp', 'LastActivityDate': '2013-11-28T06:12:49.997', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11551', 'Tags': '<algorithms><reference-request><computer-networks><synchronization>', 'CreationDate': '2013-11-25T20:05:35.750', 'Id': '18332'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Considering two sets $A, B$ containing some $p$-dimensional points $x \\in \\mathbb{R}^p$. Let $d_x^S = \\min_{x' \\in S \\setminus \\{x\\}} \\lVert \\mathbf{x} - \\mathbf{x'} \\rVert$ denote the Euclidean distance from $x$ to its nearest point in $S$. We have a very simple algorithm:</p>\n\n<ol>\n<li>$\\forall x \\in A$, if $d_x^A &gt; d_x^B$ then move $x$ from $A$ to\n$B$.</li>\n<li>$\\forall x \\in B$, if $d_x^A &lt; d_x^B$ then move $x$ from $B$ to\n$A$.</li>\n<li>Repeat (1) and (2) until convergence</li>\n</ol>\n\n<p>Convergence is when there is no more $x \\in A$ such that $d_x^A &gt; d_x^B$, and there is no more $x \\in B$ such that $d_x^A &lt; d_x^B$.</p>\n\n<p>How could I figure out which function does this algorithm minimize or maximize at each iteration ? The function $\\Phi(A)+\\Phi(B) = \\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^B$ does not seem to decrease at each iteration.</p>\n\n<p>Note: another version of this algorithm is when we define $d_x^S$ as the mean distance from $x$ to its $k$ nearest points in $S$, instead of the distance to its nearest point in $S$. I don't know if $k &gt; 1$ would make the proof more complicated or not.</p>\n", 'ViewCount': '87', 'Title': 'Which potential function does this algorithm minimize or maximize?', 'LastEditorUserId': '2895', 'LastActivityDate': '2013-11-26T16:29:35.140', 'LastEditDate': '2013-11-26T16:29:35.140', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2895', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><optimization>', 'CreationDate': '2013-11-25T20:40:26.780', 'FavoriteCount': '2', 'Id': '18333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been trying to implement an algorithm to detect cycles (probably how many of them) in a <code>directed and undirected graph</code>. That is the code should apply for both directed and undirected graphs.</p>\n\n<p>Using <code>DFS or topological sort</code> is mostly recommended in various posts. But largely, everything is addressed for undirected graph. </p>\n\n<p><a href="http://pages.cs.wisc.edu/~vernon/cs367/notes/13.GRAPH.html" rel="nofollow">This link</a> describes one approach for cycle detection. To my understanding this works for directed graphs.</p>\n\n<p><a href="http://algs4.cs.princeton.edu/41undirected/CC.java.html" rel="nofollow">This link</a> has the code for cycle detection in undirected graphs. but I fail to understand how it ignores the back edge. That is it must ignore any cycles with two nodes, say D to C and C to D.\nwhich means it must remember it parent as the DFS recurses. But the code does not seem take care of that.</p>\n\n<p>Any suggestions welcome..</p>\n\n<p><img src="http://i.stack.imgur.com/YA7NX.png" alt="enter image description here"></p>\n', 'ViewCount': '120', 'Title': 'single algorithm to work on both directed and undirected graph to detect cycles?', 'LastActivityDate': '2013-11-27T16:11:53.180', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11469', 'Tags': '<algorithms><graphs><recursion>', 'CreationDate': '2013-11-26T00:35:32.907', 'Id': '18342'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '120', 'Title': 'More efficient algorithm for determining if one list is a sublist of another list', 'LastEditDate': '2013-11-26T01:14:57.783', 'AnswerCount': '2', 'Score': '4', 'OwnerDisplayName': 'Panarit', 'PostTypeId': '1', 'OwnerUserId': '11640', 'FavoriteCount': '1', 'Body': "<p>I'm trying to build an algorithm which takes two lists of natural numbers and finds if every element of the first list is displayed at least once in the second list.</p>\n\n<p>What if the list is sorted? </p>\n\n<p>An algorithm that can do this is by comparing every element of the first list with every element from the second list. I think there is an algorithm with a better complexity. Can anyone give me any idea?</p>\n", 'Tags': '<algorithms><data-structures><sorting><lists><comparison>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-26T21:23:38.760', 'CommentCount': '1', 'AcceptedAnswerId': '18350', 'CreationDate': '2013-11-24T14:49:37.500', 'Id': '18346'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am studying the following paper for understanding next-word prediction using n-gram &amp; trie: - <a href="http://nlp.cs.berkeley.edu/pubs/Pauls-Klein_2011_LM_paper.pdf" rel="nofollow">http://nlp.cs.berkeley.edu/pubs/Pauls-Klein_2011_LM_paper.pdf</a> Before this, I did some brief study on what are n-grams. And, I know trie data structure. The issue is - the algorithm in paper is not so intuitive to understand. Anyone can provide some better/intuitive explanation of this algorithm, or some similar Language Model Implementation. I am new to this site, so if this question structure is inappropriate to this site, please guide. Thanks in advance.</p>\n', 'ViewCount': '128', 'Title': 'Next Word Prediction using n-gram & Tries', 'LastActivityDate': '2013-11-26T04:08:32.257', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11628', 'Tags': '<algorithms><machine-learning><artificial-intelligence><natural-lang-processing>', 'CreationDate': '2013-11-26T04:08:32.257', 'Id': '18351'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '75', 'Title': 'For how large graphs can we still check for isomorphism?', 'LastEditDate': '2013-11-26T14:45:57.957', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11147', 'FavoriteCount': '1', 'Body': u'<p>Let $G_1,$ $G_2$  be two arbitrary graphs with $n$ vertices and we want to check if they are isomorphic.</p>\n\n<p><strong>Question 1.</strong>  For what maximal value of $n$ we \u200b\u200bcan actually perform such verification? </p>\n\n<p><strong>Question 2.</strong>\nIs there any result like as  a list of all non-isomorphic graphs up to $n$, and what is the value of $n$ \u2014 10, 20?</p>\n', 'Tags': '<graph-theory><graph-algorithms>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-27T04:19:37.950', 'CommentCount': '3', 'AcceptedAnswerId': '18373', 'CreationDate': '2013-11-26T08:44:24.090', 'Id': '18358'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Assume you have an object moving at a constant velocity(up, down, left, right) in a grid. You have unlimited resources (memory, time). At any given time step in the grid, you can "guess" the location of the object, and you win if it is currently at that position. Without relying on probability techniques, what would be the best way to approach catching this object?</p>\n', 'ViewCount': '30', 'Title': 'finding a an object with constant velocity on an infinite grid in discrete time steps', 'LastActivityDate': '2013-11-26T10:28:16.433', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11632', 'Tags': '<algorithms><search-algorithms><probabilistic-algorithms>', 'CreationDate': '2013-11-26T09:27:30.743', 'Id': '18366'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to come up with a good algorithm to merge a set of Trails. I have described what is meant by a Trail and the conditions which\ndetermine if the merge is good or bad.</p>\n\n<p><strong>Trail - Linear sequence of n nodes obtained by traversing a graph.</strong></p>\n\n<p>E.g., </p>\n\n<pre><code>A--B--C--D--E--F\nB--C--D--E\nP--Q--R--S--T\nD--E--F--G--H\n</code></pre>\n\n<p>Each Trail has a maximum capacity of 500. There is a cost of 2 associated with each Trail.</p>\n\n<p>2 Trails can be merged if they overlap and their combined capacity is &lt; 500</p>\n\n<hr>\n\n<p>Examples of merging</p>\n\n<p>Format below is Trail followed by capacity and cost in brackets.</p>\n\n<p>Example 1</p>\n\n<pre><code>A--B--C--D--E--F : 200(2)\nA--B--C : 200(2)\nD--E--F : 200(2)\n</code></pre>\n\n<p>Total cost = 6</p>\n\n<p>We can merge A--B--C--D--E--F and A--B--C because they overlap and their combined capacity 400 &lt; 500. </p>\n\n<p>Similarly we can merge A--B--C--D--E--F and D--E--F. We end up with</p>\n\n<pre><code>A--B--C : 400(2)\nD--E--F : 400(2)\n</code></pre>\n\n<p>Total cost  = 4\nThis is a good merge and we have reduced total cost.</p>\n\n<hr>\n\n<p>Example 2</p>\n\n<pre><code>A--B--C--D--E--F : 200(2)\nB--C--D--E : 200(2)\n</code></pre>\n\n<p>Total cost = 4</p>\n\n<p>Merging the Trails we have</p>\n\n<pre><code>A--B : 200(2)\nB--C--D--E : 400(2)\nE--F : 200(2)\n</code></pre>\n\n<p>Total cost = 6\nThis merge is not good because it increase the cost.</p>\n\n<p>We can have merges that keep the cost the same. They are also acceptable</p>\n\n<p>Currently I have a simple 2 loop solution. The loops iterate over a sorted(in decreasing Trail length) list and check one by one if merges can be done.</p>\n\n<p>I am not aware if this falls into any known algorithm category. If anyone can point me to them or give an idea for a better solution, that will be great.</p>\n', 'ViewCount': '34', 'Title': 'Efficient way to merge a set of Trails (sequence of nodes in a Graph)', 'LastActivityDate': '2013-11-26T10:38:05.267', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'OwnerDisplayName': 'user3030022', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><graph-traversal>', 'CreationDate': '2013-11-25T14:39:45.633', 'Id': '18374'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>According to CLRS, each iteration of the outermost loop (on $k$) makes a new copy of the adjacency matrix. Is it safe not to copy the matrix on every iteration?</p>\n\n<p>What I mean is, according to CLRS:</p>\n\n<p>$d_{ij}^K = \\min(d_{ij}^{K-1}, d_{ik}^{K-1} + d_{kj}^{K-1})$</p>\n\n<p>Is the following possible?</p>\n\n<p>$d_{ij} = \\min(d_{ij}, d_{ik} + d_{kj})$</p>\n\n<p>I have tried not copying the matrix, and got the same result before as the one which makes a copy after each iteration, but did I just get lucky?</p>\n', 'ViewCount': '82', 'Title': 'Can Floyd-Warshall be used to solve an APSP problem without copying the matrix?', 'LastEditorUserId': '11637', 'LastActivityDate': '2013-11-26T18:09:21.957', 'LastEditDate': '2013-11-26T16:10:04.703', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18392', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11637', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2013-11-26T15:49:45.417', 'Id': '18389'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So given an input of lets say 10 strings, what way can we input these so we get the best or worst case for these two given sorts?</p>\n\n<pre><code>Heap sort:\nbest case - nlogn\nworst case - nlogn\n\nQuick sort:\nbest case - nlogn\nworst case - n^2\n</code></pre>\n\n<p>Where I get confused on these two is:</p>\n\n<ul>\n<li><strong>heap</strong>- Since the best and worst case are the same does it not matter\nthe input order? The number of comparisons and assignments will\nalways be the same? I imagine in a heap sort it may be the same since\nthe real work is done in the insertion, but the sorting only uses the\nremoval of the max/min heap? Is that why?</li>\n<li><strong>quick sort</strong>- This one I don't know for sure. I'm not sure what the\nbest case and worst case situations are for this. If its a already\nsorted list of 10 strings for example wouldn't we always have to\nchoose the same amount of pivots to get complete the recursive\nalgorithm? Any help on this explanation would really help.</li>\n</ul>\n", 'ViewCount': '416', 'Title': 'Best and worse case inputs for heap sort and quick sort?', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-11-26T21:03:49.457', 'LastEditDate': '2013-11-26T18:52:40.720', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11638', 'Tags': '<algorithms><complexity-theory><sorting><heaps>', 'CreationDate': '2013-11-26T17:43:11.983', 'Id': '18391'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '230', 'Title': 'Easy way to prove that this algorithm eventually terminates', 'LastEditDate': '2013-11-26T19:24:53.680', 'AnswerCount': '1', 'Score': '10', 'OwnerDisplayName': 'user995434', 'PostTypeId': '1', 'OwnerUserId': '2895', 'FavoriteCount': '3', 'Body': '<h2>Introduction and notations:</h2>\n\n<p>Here is a new and simple version of my algorithm which seems to terminates (according to my experiments), and now I would like to prove that.</p>\n\n<p>Let the notation $x_i \\in \\mathbb{R}^p$ refer to a $p$ dimensional data point (a vector). I have three sets A, B and C, such that $|A| = n$, $|B| = m$, $|C| = l$:\n$$A = \\{x_i | i = 1, .., n\\}$$\n$$B = \\{x_j | j = n+1, .., n+m\\}$$\n$$C = \\{x_u | u = n+m+1, .., n+m+l\\}$$</p>\n\n<p>Given $k \\in \\mathbb{N^*}$, let $d_{x_i}^A$ denote the mean Euclidean distance from $x_i$ to its $k$ nearest points in $A$; and $d_{x_i}^C$ denote the mean Euclidean distance from $x_i$ to its $k$ nearest points in $C$.</p>\n\n<h2>Algorithm:</h2>\n\n<p>I have the following algorithm which iteratively modifies the sets A and B by moving some selected elements from A to B and vis versa, and C remains always the same (do not change). To make it simple: the purpose of the algorithm is to better separate the sets $A$ and $B$ such that "the points of $B$ are more similar to those of a known fixed set $C$" and "the points of $A$ are finally self-similar and farther from those of $C$ and the final set $B$":</p>\n\n<ul>\n<li>$A\' = \\{ x_i \\in A \\mid d_{x_i}^A &gt; d_{x_i}^C \\}$ ... (1)</li>\n<li>$A = A \\setminus A\'$; $B = B \\cup A\'$ ... (2)</li>\n<li>$B\' = \\{ x_i \\in B \\mid d_{x_i}^A &lt; d_{x_i}^C$ } ... (3)</li>\n<li>$B = B \\setminus B\'$; $A = A \\cup B\'$ ... (4)</li>\n<li>Repeat (1), (2), (3), and (4) until: (no element moves from $A$ to $B$ or from $B$ to $A$, that is A\' and B\' become empty) or ($|A| \\leq k$ or $|B| \\leq k$)</li>\n</ul>\n\n<p>The algorithm terminates in two cases:</p>\n\n<ul>\n<li>when $|A|$ or $|B|$ becomes less than or equals to $k$</li>\n<li>or the most standard case, when $A\' = B\' = \\emptyset$, which means that no more elements moves between A and B.</li>\n</ul>\n\n<h2>Question:</h2>\n\n<p>How to prove that this algorithm eventually terminates ? I didn\'t found a convenient potential function which can be strictly minimized or maximized by the algorithm.\nI have unsuccessfully tried some functions: the function $\\sum_{x \\in A} d_x^C + \\sum_{x \\in B} d_x^A$ but it is not increasing at each iteration. The function $\\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^C$ but it is not decreasing at each iteration. The function $\\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^B$ seems not to be decreasing at each iteration. The function $\\sum_{x \\in A} d_x^B + \\sum_{x \\in B} d_x^A$ seems not to be increasing at each iteration. So what is the convenient potential function which can be show to either increase or decrease at each iteration ? Or should we show that the function decreases but not at each iteration (after some iterations rather) ? How ?</p>\n\n<h2>Notes:</h2>\n\n<ul>\n<li>The $k$ nearest points to $x$ in a set $S$, means: the $k$ points\n(others than $x$) in $S$, having the smallest Euclidean distance to\n$x$. You can just take $k = 1$ to simplify the analysis.</li>\n<li>I don\'t know if this may help or not, but I have the following\nproperty for my initial sets $A, B, C$: initially $\\forall x_i \\in B,\n   x_j \\in A$, if $x_b \\in C$ is the nearest point to $x_i$ and $x_a \\in\n   C$ is the nearest point to $x_j$ then always $distance(x_i, x_b) &lt;\n   distance(x_j, x_a)$. This intuitively means that points in $B$ are\ncloser to $C$ than points in $A$.</li>\n<li>If that makes the analysis easier: it is totally possible to consider a slightly different version of the Algorithm where as soon as a point from $A$ should be moved to $B$, it is moved from $A$ to $B$ (without passing by $A\'$), and vis versa for $B$.</li>\n</ul>\n', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><optimization>', 'LastEditorUserId': '2895', 'LastActivityDate': '2013-11-27T22:36:33.060', 'CommentCount': '11', 'CreationDate': '2013-11-22T10:53:45.843', 'Id': '18393'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My question is about numerical methods for inverting integral transforms;</p>\n\n<p>I'm trying to numerically invert the following integral transform:</p>\n\n<p>$$F(y) = \\int_{0}^{\\infty} y\\exp{\\left[-\\frac{1}{2}(y^2 + x^2)\\right]} I_0\\left(xy\\right)f(x)\\;\\mathrm{d}x$$</p>\n\n<p>So for a given $F(y)$ I need to approximate $f(x)$\nwhere:</p>\n\n<ul>\n<li><strong>$f(x)$ and $F(y)$ are real and positive</strong> (they are continuous probability distributions)</li>\n<li><strong>$x,y$ are real and positive</strong> (they are magnitudes)</li>\n</ul>\n\n<p>I have a very messy and brute force method for doing this at the minute: </p>\n\n<p>I define $f(x)$ and the spline over a series of points, the values of the splined points are 'guessed' by random sampling, which yields a predicted $F(y)$. A basic genetic algorithm I wrote up minimises the difference between the predicted and measured $F(y)$ array. I then take the $f(x)$ which the algorithm converges to as my answer for the inversion.</p>\n\n<p>This approach works fairly well for some simple cases, but it feels messy to me and not particularly robust.</p>\n\n<p><strong>Can anyone give me guidance on better ways of solving this problem?</strong></p>\n\n<p>Thanks for your time &amp; help!</p>\n", 'ViewCount': '40', 'Title': 'Numerically approximating an inverse integral transform?', 'LastActivityDate': '2013-11-27T05:40:37.663', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user11649', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><numerical-analysis><numerical-algorithms>', 'CreationDate': '2013-11-27T05:40:37.663', 'Id': '18406'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have a genetic algorithm in Java and I'm testing new types of selections.</p>\n\n<p>For my tests I'm using the De Jong Half Sphere, my fitness function is $x^2 + y^2$.</p>\n\n<p>The selection method used is Sigma scaling : </p>\n\n<pre><code>fscaled = S + (fraw \u2212fmean)/2\u03c3\n</code></pre>\n\n<p>Example using S=1</p>\n\n<pre><code> - a) Fitness = 5.927822124 |  After Scaling = 0.900756351532265  | 17.74410992 %\n - b) Fitness = 3.431749363 |  After Scaling = 0.5720553077878213 | 10.27246375 %\n - c) Fitness = 8.428103101 |  After Scaling = 1.2300115637030606 | 25.22835279 %\n - d) Fitness = 2.548825744 |  After Scaling = 0.455785494144756  | 7.629554871 %\n - e) Fitness = 13.07076638 |  After Scaling = 1.841391282832097  | 39.12551868 %\n</code></pre>\n\n<p>I know that sigma scaling should be used with a proportional selection.</p>\n\n<ul>\n<li><p>Is this the right way to implement this type of selection?</p></li>\n<li><p>If I wanted to minimize the function instead of maximize it how can I accomplish it, should I subtract the mean to each fitness and get the percentage this way or is there a better way?</p></li>\n</ul>\n", 'ViewCount': '145', 'Title': 'How to use Sigma Scaling in a genetic algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-04T02:20:08.893', 'LastEditDate': '2014-02-03T00:18:32.670', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11665', 'Tags': '<algorithms><genetic-algorithms>', 'CreationDate': '2013-11-27T15:44:34.027', 'FavoriteCount': '0', 'Id': '18418'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For clarity, I attach below a concise implementation of the algorithm in Python. I understand that it checks all possible element swaps, but I don't see how that necessarily means that all possible orderings of the elements will be reached, and/or that no ordering will be duplicated.</p>\n\n<p>For example, what if the elements at index 0 and 1 are swapped, then 1 and 2 are swapped?  How does the algorithm guarantee this won't result in a duplicate ordering?</p>\n\n\n\n<pre><code>P = []\ndef permute(l, n=0):\n    if n == len(l):\n        return P.append(list(l))\n    for i in xrange(n, len(l)):\n        l[i], l[n] = l[n], l[i]\n        permute(l, n+1)\n        l[i], l[n] = l[n], l[i]\n</code></pre>\n", 'ViewCount': '53', 'Title': 'Is there a proof of the recursive algorithm for generating all permutations of a sequence?', 'LastActivityDate': '2013-11-28T02:19:16.190', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10768', 'Tags': '<algorithms><combinatorics><correctness-proof>', 'CreationDate': '2013-11-27T21:26:26.657', 'Id': '18423'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I got an amount of numbers, they are shuffled and represent the individuals.\nFor example (3,1,3,2,5,22,5) is one individual or (22,3,1,3,5,5,2).</p>\n\n<p>Mutation is done quite easy by permutation within an individual.</p>\n\n<p>The problem is to find a crossover method between 2 individuals, in which the quantity of the values stay the same(for the above example 2 times 3, 2 times 5, 1 time 22 and so on).<br/> I searched a lot on it but only found crossover methods for either unordered / ordered lists. Those for ordered lists seemed promising but they require that every value only occur ones.</p>\n\n<p>Do you know a method to solve this problem or got an idea for altering an crossover for ordered list method like PMX?</p>\n', 'ViewCount': '57', 'Title': 'Crossover method for genetic algorithm', 'LastActivityDate': '2013-11-27T23:32:23.200', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11601', 'Tags': '<algorithms><genetic-algorithms>', 'CreationDate': '2013-11-27T23:32:23.200', 'Id': '18429'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it common to try to improve an algorithm by decomposing its action on a topological piece of data (e.g. graphs, geometric data) into a series of steps, each of which only makes a local change/perturbation in a confined neighborhood of the data while fixing the rest?</p>\n\n<p>One benefit of this "breakdown" is improved conceptual understanding, and it also facilitates induction proofs (possibly based on invariants that are preserved by each step). Does this "decomposition" have a name in algorithm design? The idea might be similar to Divide and Conquer.</p>\n\n<p>One might say that every individual step in an algorithm, like deleting an edge or labeling a vertex of a graph, is small enough to be considered "local", but I\'m talking more about the "decomposition" in a higher-level description, like how truncating a polyhedron is local but sorting a list is not (even though at the lowest level, comparisons, insertions and deletions are local).</p>\n\n<p><strong>Examples of decomposing into local transformations</strong></p>\n\n<ul>\n<li><p>Augustin Cauchy\'s proof of his <a href="http://en.wikipedia.org/wiki/Cauchy%27s_theorem_%28geometry%29" rel="nofollow">Rigidity Theorem</a> relied on his "<a href="http://www.cs.mcgill.ca/~cs507/projects/1998/sfreel/cauchylemma.html" rel="nofollow">Arm Lemma</a>" that opened up a convex planar chain by sequentially opening at each vertex. Although his proof was found to be wrong, he decomposed the opening algorithm into a series of "local opening moves", and he tried to prove that each local move preserved convexity of the chain. By induction, convexity would still hold after iterating over all vertices.</p></li>\n<li><p>The <em>pivot algorithm</em> for <a href="http://en.wikipedia.org/wiki/Self-avoiding_walk" rel="nofollow">self-avoiding walks</a> (SAW) transforms a SAW into another by rotating or reflecting part of the original SAW. <a href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="nofollow">Markov Chain Monte Carlo</a> can employ this transformation as a transition between SAW states to sample the space of SAWs.</p></li>\n<li><p>As an application of the above, one technique of <a href="http://en.wikipedia.org/wiki/Protein_folding#Computational_methods_for_studying_protein_folding" rel="nofollow">protein folding prediction</a> is to force an amino acid chain into several known protein structures, choose the one with the lowest energy (due to intermolecular forces), then let the chosen structure "relax" into a configuation with lower energy. Relaxation involves the prescription of a few "local moves", where small parts of the chain can "wriggle around" (e.g. subchains pivoting around their ends), and random wriggles are allowed for a period of time to let the chain "stabilize". Modelling the chain as an SAW allows pivots to be used as a "wriggle".</p></li>\n</ul>\n', 'ViewCount': '171', 'Title': 'Decomposing an Algorithm into Local Transformations/Perturbations', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-13T22:04:18.290', 'LastEditDate': '2013-12-13T13:27:28.013', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '11678', 'Tags': '<algorithms><terminology><reference-request><algorithm-design>', 'CreationDate': '2013-11-28T08:34:39.540', 'Id': '18438'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m looking for a way to position nodes on a 2-dimensional plane in such a way that the distances between the nodes, which are entered exogenously, are represented visually in a way that as good as possible communicates the logical distance between those nodes. Basically, I have a set of nodes A, B, C etc. and between some of them I have a distance: A is 10 from B, 20 from C etc. Some nodes are not connected at all. The distances are not internally geometrically consistent: A may be 10 from B, and 10 from C, but B and C may be 100 from each other.</p>\n\n<p>I want to do this in a way that is \'stable\' - i.e. generates the exact same layout each time the algorithm is run. Multi-dimensional scaling algorithms seem to be the way to go about this, but I\'m having a hard time finding practical properties of the various MDS algorithms that exist, and the one I\'ve tried is based on iterating from a random start position and therefore doesn\'t generate the same result every time. It also cannot handle not-connected nodes, afaik.</p>\n\n<p>The practical application for this, and more detail on what I\'ve tried, is described in this question: <a href="http://stackoverflow.com/questions/20263829/stable-multi-dimensional-scaling-algorithm">http://stackoverflow.com/questions/20263829/stable-multi-dimensional-scaling-algorithm</a></p>\n\n<p>(Original post:\nI\'m not sure what the etiquette is on this (repost question here or just link), but I asked a question on Stackoverflow that I was advised might fit better here. The question is here: <a href="http://stackoverflow.com/questions/20263829/stable-multi-dimensional-scaling-algorithm">http://stackoverflow.com/questions/20263829/stable-multi-dimensional-scaling-algorithm</a> . I can repost here if that is the custom, but I thought I\'d err on the side of fewest fragmentation and duplication. Thanks.)</p>\n', 'ViewCount': '77', 'Title': "'Stable' multi-dimensional scaling algorithm", 'LastEditorUserId': '11681', 'LastActivityDate': '2013-11-28T13:55:38.880', 'LastEditDate': '2013-11-28T13:55:38.880', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11681', 'Tags': '<algorithms><graphs>', 'CreationDate': '2013-11-28T12:37:03.300', 'Id': '18439'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When can an algorithm be said to have $O(1)$ complexity? My doubt is if $n$ is specified to be a large number but constant and we cannot implement it in reality without a loop even then can we call it to have $O(1)$ time complexity? Consider the following examples.</p>\n\n<ol>\n<li><p>Algorithm to add first 1000 natural numbers (that is I mean to say if n is specified directly). Then can we say this has $O(1)$ time complexity?</p></li>\n<li><p>Finding the $7$th smallest element in a min heap. This element is present in anywhere in the first 6 levels of the heap (considering root at level 0). So to find the element we need to check $2^7 - 1$ elements. Then can we say this has $O(1)$ time complexity?</p></li>\n</ol>\n', 'ViewCount': '223', 'Title': 'When does an algorithm run in $O(1)$ time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-02T12:21:30.010', 'LastEditDate': '2013-12-02T12:21:30.010', 'AnswerCount': '4', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8533', 'Tags': '<algorithms><asymptotics><runtime-analysis>', 'CreationDate': '2013-11-28T19:12:48.417', 'Id': '18451'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is NP-hard to approximate <a href="http://link.springer.com/chapter/10.1007/3-540-44849-7_21" rel="nofollow">maximum 3D matching problem</a> even if each element occurs exactly in two triples. I\'m interested in the following decision version of 3D matching. </p>\n\n<p>Informally, Given a set of triples $F$ of elements such that each element occurs exactly in two triples, Is there a subset of $F$ such that each element occurs in exactly one triple?</p>\n\n<blockquote>\n  <p>Is this decision problem solvable in polynomial time? Is it $NP$-complete?</p>\n</blockquote>\n', 'ViewCount': '30', 'Title': 'Bounded occurrence 3D matching problem', 'LastActivityDate': '2013-11-28T20:25:59.877', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '96', 'Tags': '<algorithms><complexity-theory>', 'CreationDate': '2013-11-28T20:25:59.877', 'Id': '18455'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we are given an array of numbers representing lengths of line segments.  Find which three of these segments can be assembled into a triangle with maximum area.</p>\n\n<p>I can compute the areas of all $O(n^3)$ possible triangles in $O(n^3)$ time using Heron's formula, and then return the largest. Can this be improved to $O(n^2)$ time? Or even faster?</p>\n", 'ViewCount': '276', 'Title': 'Given the set of length of triangle, find the Maximum Area Triangle', 'LastActivityDate': '2013-11-29T00:51:20.767', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'Chuang Fu', 'PostTypeId': '1', 'OwnerUserId': '11626', 'Tags': '<algorithms>', 'CreationDate': '2013-11-26T00:37:15.057', 'Id': '18459'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would you approach the knapsack problem in a dynamic programming situation if you now have to limit the number of item in the knapsack by a constant $p$ ? This is the same problem (max weight of $W$, every item have a value $v$ and weight $w$) but you can only add $p$ item(s) to the knapsack and obviously need to optimize the value of the knapsack.</p>\n\n<p>Do we need a 3rd dimension or we could find an other approach without it. I tried to simply add the number of item in the knapsack in the cell and taking the max value at the end with the number of item &lt;= $p$ but it is not the BEST solution. </p>\n', 'ViewCount': '238', 'Title': 'Variant of the knapsack problem', 'LastActivityDate': '2013-12-02T08:43:27.143', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18529', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11536', 'Tags': '<algorithms><optimization><dynamic-programming><knapsack-problems>', 'CreationDate': '2013-11-30T19:46:10.817', 'FavoriteCount': '1', 'Id': '18492'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '110', 'Title': 'Peer grading design - choosing a graph, to get accurate rankings/ratings', 'LastEditDate': '2013-12-02T23:49:34.417', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11721', 'FavoriteCount': '3', 'Body': "<p><strong>Background.</strong> I am writing some code for semi-automated grading, using peer grading as part of the grading process. Students are given pairs of essays at a time, and the students have a slider to choose which is better and how much better it is.  e.g., the slider might look something like this:</p>\n\n<p><code>A---X-B</code></p>\n\n<p>Based on the results of the peer grading, essays are ranked and the teacher will then grade the top X% and bottom X% and scores for all essays will be automatically calculated based on this.  I have already come up with methods for doing this ranking/scoring process; that part works well.</p>\n\n<p><strong>My question.</strong> How should I select which pairs of essays to give to students?</p>\n\n<p>Simulations suggest we need an essay to be peer-graded at least 3 times, to get an accurate ranking.  Thus, each essay should appear in at least 3 of the pairs that are presented for peer grading.</p>\n\n<p>We can think of this as a graph problem.  Think of the essays as nodes.  Each edge represents a pair of essays that are presented during the peer grading process.  The accuracy results above suggest that the degree of each node (or of most nodes) should be at least 3.  What sort of graph should I use?  How should I generate the graph to be used during peer grading?</p>\n\n<p>One challenge is that if you have clusters in the graph, this will skew the peer-gradings.  For example, we wouldn't want to have high-quality essays peer-graded mostly against high-quality essays, because that would skew the results of the peer grading.</p>\n\n<p>What would you recommend?</p>\n\n<p>I think this problem could be modelled with a undirected graph using something like the following:</p>\n\n<ul>\n<li>Start by taking the node with the least degree and link it with the next least</li>\n<li>Continue until your average degree is at least 3</li>\n<li>Maximise node connectivity</li>\n<li>Minimise number of cliques</li>\n</ul>\n\n<p>Is this a good approach? If not what would you recommend instead?</p>\n", 'Tags': '<algorithms><graphs><modelling>', 'LastEditorUserId': '755', 'LastActivityDate': '2013-12-02T23:49:34.417', 'CommentCount': '5', 'AcceptedAnswerId': '18494', 'CreationDate': '2013-11-30T22:26:52.017', 'Id': '18493'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an ensemble of points in 3D space, represented by their coordinates $\\mathbf{c_i}\\equiv(x_i,y_i,z_i)^\\top$ . I need to calculate</p>\n\n<ul>\n<li>the distance between all these points: $\\quad\\forall i,j\\quad d_{ij} \\equiv |\\mathbf{d_{ij}}|\\equiv |\\mathbf{c_j}-\\mathbf{c_i}|$</li>\n<li>the scalar product between all distances that share a common coordinate. $\\quad\\forall i,j,k\\quad \\mathbf{d_{ij}}\\cdot\\mathbf{d_{ik}}$</li>\n</ul>\n\n<p>What is the fastest way to do this on a single thread? Is Fourier space going to be of any use? Can it be parallelized to make it even faster? If approximations are proposed, they should come with an error bound.</p>\n', 'ViewCount': '26', 'Title': 'fastest way to compute scalar product of an ensemble of vectors', 'LastActivityDate': '2013-12-01T01:22:24.163', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11724', 'Tags': '<algorithms><graphs><parallel-computing><fourier-transform>', 'CreationDate': '2013-12-01T01:22:24.163', 'Id': '18497'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given is the following graph which is logically divided into layers (with Dijkstra's shortest paths algorithm):</p>\n\n<pre><code> Vertices   Layer\n\n    Root      0\n   /   \\\n  A     B     1\n / \\    |\nC   D   E     2\n \\  |  /\n  \\ | /\n    F         3\n</code></pre>\n\n<p>Now I'm looking for an algorithm which groups vertices when they have a (single) common ancestor in the previous layer, e.g. for the graph in the example the groups would be:</p>\n\n<pre><code>0: A, B\n1: C, D\n2: E\n3: F\n</code></pre>\n\n<p>I know that this is doable by visiting vertices and comparing ancestors but I was wondering whether there is a well known algorithm for it.</p>\n\n<p><strong>Update:</strong> My question is really only related to find groups. I'm aware of the fact, that I can traverse vertices and test for incoming edges and group those vertices. Furthermore, the graph is fully constructed.</p>\n\n<p>One (now deleted) answer mentioned DFS, which creates a search forest (as BFS creates a search tree which I basically used for levels, though I mentioned Dijkstra). So, I assume that combining BFS and DFS could give me the desired result.</p>\n", 'ViewCount': '140', 'Title': 'Algorithm to Group Vertices of Graph', 'LastEditorUserId': '11734', 'LastActivityDate': '2014-03-07T23:10:15.287', 'LastEditDate': '2013-12-07T20:39:47.837', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11734', 'Tags': '<algorithms><graph-theory><graph-traversal>', 'CreationDate': '2013-12-01T16:02:07.553', 'Id': '18504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I need to modify the Dijkstra's algorithm to get the shortest path in a directed graph and get the one with the least amount of edges if there are equal paths. </p>\n\n<p>I am thinking to add another data field to count the number of edges from start and compare them in same way as weights. Would that work?</p>\n", 'ViewCount': '143', 'Title': u'Modifying Dijkstra\u2019s algorithm to favor the path with least amount of edges', 'LastEditorUserId': '755', 'LastActivityDate': '2013-12-02T04:28:20.353', 'LastEditDate': '2013-12-02T04:28:20.353', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18516', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '11747', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2013-12-02T00:29:23.490', 'Id': '18515'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '137', 'Title': 'Widest path algorithm steps', 'LastEditDate': '2013-12-02T06:02:56.707', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11747', 'FavoriteCount': '1', 'Body': u'<p>I need to compute the bottleneck shortest paths from s to all vertices of a graph by modifying the Dijkstra\u2019s algorithm. I found this explanation on Wikipedia(<a href="http://en.wikipedia.org/wiki/Widest_path_problem" rel="nofollow" title="Link">Link to Wikipedia</a>) but I would appreciate if you can elaborate it a bit for me.</p>\n\n<blockquote>\n  <p>If the edges are sorted by their weights, then a modified version of\n  Dijkstra\'s algorithm can compute the bottlenecks between a designated\n  start vertex and every other vertex in the graph, in linear time. The\n  key idea behind the speedup over a conventional version of Dijkstra\'s\n  algorithm is that the sequence of bottleneck distances to each vertex,\n  in the order that the vertices are considered by this algorithm, is a\n  monotonic subsequence of the sorted sequence of edge weights;\n  therefore, the priority queue of Dijkstra\'s algorithm can be replaced\n  by an array indexed by the numbers from 1 to m (the number of edges in\n  the graph), where array cell i contains the vertices whose bottleneck\n  distance is the weight of the edge with position i in the sorted\n  order. This method allows the widest path problem to be solved as\n  quickly as sorting; for instance, if the edge weights are represented\n  as integers, then the time bounds for integer sorting a list of m\n  integers would apply also to this problem</p>\n</blockquote>\n\n<p>So I need to sort my vertex by weight starting from A? I would appreciate if you can explain me the steps in this algorithm. </p>\n\n<p>Thanks</p>\n', 'ClosedDate': '2014-01-19T02:13:52.897', 'Tags': '<algorithms><graph-theory><graphs>', 'LastEditorUserId': '11747', 'LastActivityDate': '2013-12-02T06:09:22.450', 'CommentCount': '4', 'AcceptedAnswerId': '18527', 'CreationDate': '2013-12-02T03:04:42.707', 'Id': '18522'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question is inspired by <a href="http://www.spoj.com/problems/HAYBALE/" rel="nofollow">this SPOJ problem</a>. That problem asks median of the final array, that can be done in $O(k+n)$, where <em>k</em> is the number of update queries and <em>n</em> is the size of the array.</p>\n\n<p>The variant : Given an array of length <strong>N</strong> (of the order $10^5$), initially all elements are 0. There are <strong>K</strong> (of the order $10^5$) queries of type <strong>A B</strong>, meaning array elements in range $[A,B]$ must be increased by 1. After <em>every</em> query operation, find the median of the array.<br>\nCan this be solved efficiently?</p>\n', 'ViewCount': '172', 'Title': 'Algorithm : Median of a dynamic array', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T12:35:44.540', 'LastEditDate': '2014-04-10T12:35:44.540', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1742', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-12-02T10:58:40.747', 'FavoriteCount': '4', 'Id': '18532'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am a physicist, with little formal training in computer science - please don\'t assume I know even obvious things about computer science! </p>\n\n<p>Within the context of data analysis, I was interested in identifying clusters within a $d$-dimensional list of $n$ data-points, for which the dimensionality $d$ could be $\\sim100$, whilst the number of data-points could be $\\sim 1,000,000$, or perhaps more.</p>\n\n<p>I wanted the points with a cluster to be close together, with distance measured in the Euclidean manner,\n$$\nd(\\vec x,\\vec y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2} \n$$ \nAs long as the clustering was reasonably accurate, I wasn\'t bothered if the exactly correct result was obtained. i.e. if of my $\\sim1,000,000$, $\\sim1,000$ points were wrongly categorized, it wouldn\'t matter much.</p>\n\n<p>I have written a short algorithm that can perform typically at $\\mathcal{O}(n)$ (from trials of up to $n\\sim5,000,000$ and some theoretical analysis) and worst-case $\\mathcal{O}(n^2)$ (from my theoretical evaluation of the algorithm). The nature of the algorithm sometimes (but not always) avoids the so-called chaining problem in clustering, where dissimilar clusters are chained together because they have a few data-points that are close.</p>\n\n<p>The complexity is, however, sensitive to the <em>a priori</em> unknown number of clusters in the data-set. The typical complexity is, in fact, $\\mathcal{O}(n\\times c)$, with $c$ the number of clusters. </p>\n\n<p>Is that better than currently published algorithms? I know naively it is a  $\\mathcal{O}(n^3)$ problem. I have read of SLINK, that optimizes the complexitiy to $\\mathcal{O}(n^2)$. If so, is my algorithm useful? Or do the major uses of clustering algorithms require exact solutions?</p>\n\n<p>In real applications is $c\\propto n$?, such that my algorithm has no advantage. My naive feeling is that for real problems, the number of "interesting" clusters (i.e. not including noise) is a property of the physical system/situation being investigated, and is in fact a constant, with no significant dependence on $n$, in which case my algorithm looks useful.</p>\n', 'ViewCount': '83', 'Title': 'Is an $\\mathcal{O}(n\\times \\text{Number of clusters})$ clustering algorithm useful?', 'LastEditorUserId': '11757', 'LastActivityDate': '2013-12-06T00:49:12.167', 'LastEditDate': '2013-12-05T13:11:18.563', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '18668', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11757', 'Tags': '<algorithms><complexity-theory><cluster>', 'CreationDate': '2013-12-02T11:38:24.923', 'Id': '18534'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have come across many sorting algorithms during my high school studies. However, I never know which is the fastest (for a random array of integers). So my questions are:</p>\n\n<ul>\n<li>Which is the fastest currently known sorting algorithm?</li>\n<li>Theoretically, is it possible that there are even faster ones? So, what's the least complexity for sorting?</li>\n</ul>\n", 'ViewCount': '2193', 'Title': 'What is a the fastest sorting algorithm for an array of integers?', 'LastActivityDate': '2013-12-03T19:50:58.720', 'AnswerCount': '4', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8870', 'Tags': '<algorithms><time-complexity><optimization><sorting>', 'CreationDate': '2013-12-02T16:15:25.337', 'Id': '18536'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $n$ be an integer, and let $\\mathbb{Z}$ denote the set of all integers.  Let $[a,b]$ denote the interval of integers $\\{a,a+1,a+2,\\dots,b\\}$.</p>\n\n<p>I am looking for a data structure to represent a map $f:[1,n] \\to \\mathbb{Z}$.  I want the data structure to support the following operations:</p>\n\n<ul>\n<li><p>$\\text{get}(i)$ should return $f(i)$.</p></li>\n<li><p>$\\text{set}([a,b],y)$ should update $f$ so that $f(a)=f(a+1)=\\cdots=f(b)=y$, i.e., update $f$ to a new map $f'$ such that $f'(i) = y$ for $i \\in [a,b]$ and $f'(i) = f(i)$ for $i \\notin [a,b]$.</p></li>\n<li><p>$\\text{stab}(i)$ should return the largest interval $[a,b]$ such that $i \\in [a,b]$ and $f$ is constant on $[a,b]$ (i.e., $f(a)=f(a+1)=\\cdots=f(b)$).</p></li>\n<li><p>$\\text{add}([a,b],\\delta)$ should update $f$ to a new map $f'$ such that $f'(i) = f(i) + \\delta$ for $i \\in [a,b]$ and $f'(i) = f(i)$ for $i \\notin [a,b]$.</p></li>\n</ul>\n\n<p>I want each of these operations to be efficient.  I would count $O(1)$ or $O(\\lg n)$ time as efficient, but $O(n)$ time is too slow.  It's OK if the running times are amortized running times.  Is there a data structure that simultaneously makes all of these operations efficient?</p>\n\n<p>(I've noticed a similar pattern come up in a several programming challenges.  This is a generalization that would suffice for all of those challenge problems.)</p>\n", 'ViewCount': '145', 'Title': 'Data structure for map on intervals', 'LastEditorUserId': '755', 'LastActivityDate': '2013-12-03T07:49:12.067', 'LastEditDate': '2013-12-02T20:33:03.120', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-structures><trees><intervals>', 'CreationDate': '2013-12-02T18:19:49.227', 'FavoriteCount': '1', 'Id': '18542'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '111', 'Title': 'Clarification on Tabu Search', 'LastEditDate': '2013-12-03T22:03:19.313', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '170', 'FavoriteCount': '1', 'Body': u'<p>I need some help in understanding the \'<em>Tabu Search</em>\' Algorithm. (<a href="http://en.wikipedia.org/wiki/Tabu_search" rel="nofollow">Wikipedia</a>)</p>\n\n<p>I miss a simple explanation to Tabu Search. Anyway, I\'m trying to refer to available resources and build an understanding. </p>\n\n<p><em><strong>This is what I\'m trying to \'digest\':</em></strong></p>\n\n<ul>\n<li><p><em>Tabu Search</em> is an improvement over the <em>Hill Climbing</em> algorithm (Ref-1).</p></li>\n<li><p>The problem with Hill Climbing is that it does not guarantee about reaching the global optimum, because it only searches on a subset of the whole solution space. It will find the local optimum.</p></li>\n<li>To get rid of this issue, Tabu Search maintains a \'Tabu List\' of previously visited states that cannot be revisited (Ref-2). </li>\n<li>If the tabu list is too large, the oldest candidate solution is removed and it\u2019s no\nlonger tabu to reconsider (Ref-3).</li>\n</ul>\n\n<p><em><strong>My questions are,</em></strong> </p>\n\n<ol>\n<li><p>How does Tabu Search cure the problem of getting stuck in a local\noptimum? Does it increase the search-space?</p></li>\n<li><p>What is the need of maintaining a list (i.e. Tabu List)? Why not\njust remember the optimum solution found so far?</p></li>\n<li><p>When the Tabu List is too large, the oldest candidate will be\nremoved. What if this oldest candidate is the global optimum?</p></li>\n</ol>\n\n<p><em>If anyone could explain Tabu Search algorithm using an example, I\'m sure these questions would be automatically answered.</em></p>\n\n<p>References:</p>\n\n<ul>\n<li><p>(Ref-1) Hill Climbing, Wikipedia Article (<a href="http://en.wikipedia.org/wiki/Hill_climbing" rel="nofollow">link</a>)</p></li>\n<li><p>(Ref-2) Russell, Stuart Jonathan, et al. Artificial intelligence: a modern approach. Vol. 74. Englewood Cliffs: Prentice hall, 1995. (<a href="http://www.worldcat.org/oclc/31288015" rel="nofollow">WorldCat</a>)</p></li>\n<li><p>(Ref-3) Luke, Sean. "Essentials of Metaheuristics.". (<a href="http://cs.gmu.edu/~sean/book/metaheuristics/Essentials.pdf" rel="nofollow">pdf</a>)</p></li>\n</ul>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms>', 'LastEditorUserId': '268', 'LastActivityDate': '2013-12-04T11:19:56.827', 'CommentCount': '0', 'AcceptedAnswerId': '18606', 'CreationDate': '2013-12-03T20:36:30.533', 'Id': '18582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What type of algorithm would you suggest me to use for this problem? I want to implement an algorithm that minimize the total distance in a graph (TSP) but for only X nodes. Also, we can go as many times we want on every vertices and/or edges. Let\'s say, on my graph, that I want the minimum distance for visiting every blue node. What algorithm and heuristic would you recommand me? An approximation running in reasonable time would be acceptable.</p>\n\n<p>For this example, this is an undirected graph, the vertices are points in the plane and the cost of an edge is the distance between its endpoints.</p>\n\n<p><img src="http://i.imgur.com/YheHQPT.jpg" alt="graph"></p>\n', 'ViewCount': '74', 'Title': 'Algorithm to use for a TSP variant', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-04T00:26:11.793', 'LastEditDate': '2013-12-04T00:26:11.793', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18593', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11797', 'Tags': '<algorithms><graphs><heuristics><traveling-salesman>', 'CreationDate': '2013-12-03T23:17:30.113', 'Id': '18589'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I came across following question:</p>\n\n<p>There is a building with N floors. We have a balls of same property and have a breaking point of X floor. i.e, If ball dropped from X+1 floor then the ball will break. if ball is dropped from X or less than X then it wont break. Now design an algorithm to find X if N is given.</p>\n\n<p>My Solution:</p>\n\n<p>Drop the ball from N/2 floor. If ball breaks then X &lt; N/2. Else X > N/2. Repeat this process till X is found. \nbut If X is N/2+1 or N/2-1 the number of steps to find X are high. </p>\n\n<p>Is there a better way to Find X?   </p>\n', 'ViewCount': '77', 'Title': 'Ball Break Point Problem', 'LastActivityDate': '2013-12-04T21:17:09.277', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18621', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11783', 'Tags': '<algorithms>', 'CreationDate': '2013-12-04T10:33:59.543', 'Id': '18603'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is an $O(n \\log n)$ algorithm to find how big the largest subset of $n$ axis-aligned rectangles (in the plane) that contain a common point is? Perhaps by reducing this to a problem with such runtime?</p>\n\n<p>An $O(n^{3})$ algorithm could be </p>\n\n<pre><code>for each of the n rectangles      \n    for each of the 4 sides of the rectangle\n        find the points where it intersects other rectangles and keep the set of those points \n        loop through the intersection points set and update the count\n</code></pre>\n', 'ViewCount': '348', 'Title': 'Efficient algorithm for finding maximum subset of intersecting rectangles', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-06T06:43:42.183', 'LastEditDate': '2014-01-06T06:43:42.183', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10950', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-12-04T14:50:43.893', 'FavoriteCount': '1', 'Id': '18611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have written an algorithm for clustering data-points with their nearest neighbours with a Euclidean distance metric. I estimate its complexity to be $\\mathcal{O}(n \\times k)$ for $n$ data-points with $k$ genuine clusters. The dimension $d$ of the data results in $\\mathcal{O}(e^d)$ complexity, though I believe this can be mitigated.</p>\n\n<p>The algorithm is Monte-Carlo, resulting in an approximate solution. In pseudo-code, for <code>data</code> transformed to a unit hypercube:</p>\n\n<pre><code>radius = 0.15 # This should be fine-tuned for problem.\nmode_number = 0 # Label for mode currently being investigated.\n\nwhile length(data) &gt; 0:\n\n    mode_number += 1\n    branching = True\n    point = data[0] # i.e. first data point.\n\n    while branching:\n        within_radius = [] # Make this list empty.\n        for item in data:\n            if Euclidean_Distance(item,point) &lt; radius:\n                within_radius += point # List of points within a sphere.\n\n        if length(within_radius) == 0: \n            branching = False\n            continue \n\n        point = Pick_Random_From(within_radius) # Next point.\n        data = data.Remove_From_Data(within_radius) # Remove neighbouring points from data.\n        mode[mode_number] += within_radius # Add points to mode.\n</code></pre>\n\n<p>Although the <code>for item in data</code> loop contributes a complexity $\\times$ <code>length(data)</code> each pass, <code>length(data)</code> shrinks rapidly, resulting in only $\\times n$ complexity. In fact, the <code>length(data)</code> shrinks linearly with iteration (with a saw-tooth behaviour), e.g. </p>\n\n<p><img src="http://i.stack.imgur.com/Eo20S.png" alt="enter image description here"></p>\n\n<p>where branch is incremented every time <code>while branching</code> is passed, and number of deleted is the number of points removed at that pass of <code>while branching</code>. That result is from 3 Gaussians with 1000 points sampled from each Gaussian. My code identified six modes:</p>\n\n<pre><code>Modes: 6\nMode 5 , points: 1000\nMode 3 , points: 932\nMode 0 , points: 829\nMode 1 , points: 168\nMode 4 , points: 68\nMode 2 , points: 2\n</code></pre>\n\n<p>though only three substantial, which wasn\'t too far off.</p>\n\n<p>The algorithm starts at a point, finds its nearby neighbours, and deletes them from the data list, and repeats from a point within those nearest neighbours. If a point has no nearby neighbours, it begins a new mode. If the data-points are spread in many modes, the algorithm requires more iterations to clear all the modes, resulting in $\\times k$ complexity.</p>\n\n<p>The dimension of the data limits performance. In higher dimensions of space, points are more "spread-out", e.g. a sphere radius $r$ takes up an $(r/R)^d$ fraction of the space, which shrinks exponentially with increasing $d$. This, I think, could be mitigated somewhat by increasing <code>radius = 0.15</code> in my algorithm. This is justifiable - with larger $d$, more of the space in a unit hypercube is "around the edges". </p>\n\n<p>So overall I estimate complexity: $\\mathcal{O}(n\\times k\\times e^d)$. Is my analysis reasonable? Is the working of the algorithm clear?</p>\n', 'ViewCount': '65', 'Title': 'Monte-Carlo algorithm for clustering', 'LastActivityDate': '2013-12-06T18:34:11.897', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11757', 'Tags': '<algorithms>', 'CreationDate': '2013-12-06T13:58:34.970', 'Id': '18683'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I need to evaluate a polynomial of degree n at the n cube roots of unity. Simple evaluation would take $O(n^2)$ time. I know that polynomial evaluation can be done in $O(n\\log n)$ time using FFT.</p>\n\n<p>But the problem that I am facing is this. FFT only works when $n$ is a power of 2. If I extend my polynomial by appending zeroes at the end, that would not solve my problem because then I would be evaluating my polynomial at cube roots of $m$ (where $m$ is a power of $2$ greater than $n$) which I don't want.</p>\n\n<p>Any ideas on this?</p>\n", 'ViewCount': '40', 'Title': 'Discrete fourier transform of a polynomial whose degree is not a power of 2', 'LastEditorUserId': '683', 'LastActivityDate': '2013-12-11T07:55:55.970', 'LastEditDate': '2013-12-11T07:55:55.970', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11899', 'Tags': '<algorithms><divide-and-conquer><polynomials>', 'CreationDate': '2013-12-07T09:42:01.607', 'Id': '18710'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I can say that $x * y$ is a problem, but also I can say that $x * y$ is an algorithm for finding the area of rectangle.</p>\n\n<p>I been reading Wikipedia about an <a href="http://en.wikipedia.org/wiki/Algorithm">algorithm</a> and a <a href="http://en.wikipedia.org/wiki/Computational_problem">problem</a>, but I am not sure about their definitions. Could you please explain it to me in more detail?</p>\n', 'ViewCount': '219', 'Title': 'What is the difference between a problem and an algorithm?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-07T14:08:16.373', 'LastEditDate': '2013-12-07T13:13:20.887', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '18718', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><terminology>', 'CreationDate': '2013-12-07T13:03:18.480', 'Id': '18717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $ G=(V,E) $ be a directed graph with a real weight function $w$ defined on the edges and $ a,b \\in V$. Let $\\alpha$ denote the minimal weight of all paths from $a$ to $b$ and $\\beta$ denote the minimal weight of all paths from $b$ to $a$. How do you find two paths $l_1,l_2$  such that:</p>\n\n<ol>\n<li>$l_1=(a,v_1,..,v_n,b)$</li>\n<li>$l_2=(b,u_1,..,u_m,a)$</li>\n<li>$w(l_1) + w(l_2)\\leq 1.1(\\alpha + \\beta)$</li>\n<li>From all the paths holding the above, bring to minimum the sum of weights on the edges $e=(u,v)\\in l_1$ such that $(v,u)\\in l_2$</li>\n</ol>\n\n<p>Less formally, I want to find a path starting at $a$ ending with $b$ and returning to $a$ such that the path is not too long (at most 10% longer than the optimal solution) and tries to use as much as different roads as possible (if it used some road $(x,y)$ when going from $a$ to $b$ , it would try to avoid the road $(y,x)$ when going back to $a$)</p>\n', 'ViewCount': '88', 'Title': 'Path finding under constraints', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T19:10:15.007', 'LastEditDate': '2014-01-07T10:00:27.157', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7706', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2013-12-07T22:12:38.000', 'Id': '18725'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a rough idea that the time complexity is O(mn) where m and n are based on the length of signals, as per this <a href="http://en.wikipedia.org/wiki/Dynamic_time_warping" rel="nofollow">http://en.wikipedia.org/wiki/Dynamic_time_warping</a>. </p>\n\n<p>How can I determine the time complexity of the following code (<a href="http://www.mathworks.com/matlabcentral/fileexchange/16350-continuous-dynamic-time-warping" rel="nofollow">source</a>)</p>\n\n<pre><code>function [Dist,D,k,w,rw,tw]=dtw3(t,r)\n%\n% [Dist,D,k,w,rw,tw]=dtw(r,t,pflag)\n%\n% Dynamic Time Warping Algorithm\n% Dist is unnormalized distance between t and r\n% D is the accumulated distance matrix\n% k is the normalizing factor\n% w is the optimal path\n% t is the vector you are testing against\n% r is the vector you are testing\n% rw is the warped r vector\n% tw is the warped t vector\n\n\n[row,M]=size(r); \nif (row &gt; M) \n     M=row; \n     r=r\'; \nend;\n[row,N]=size(t); \nif (row &gt; N) \n     N=row; \n     t=t\'; \nend;\n\n\nd=((repmat(r\',1,N)-repmat(t,M,1)).^2);\n\nD=zeros(size(d));\nD(1,1)=d(1,1);\n\nfor m=2:M\n    D(m,1)=d(m,1)+D(m-1,1);\nend\nfor n=2:N\n    D(1,n)=d(1,n)+D(1,n-1);\nend\nfor m=2:M\n    for n=2:N\n        D(m,n)=d(m,n)+min(D(m-1,n),min(D(m-1,n-1),D(m,n-1))); \n    end\nend\n\nDist=D(M,N);\nn=N;\nm=M;\nk=1;\nw=[M N];\nwhile ((n+m)~=2)\n    if (n-1)==0\n        m=m-1;\n    elseif (m-1)==0\n        n=n-1;\n    else \n      [values,number]=min([D(m-1,n),D(m,n-1),D(m-1,n-1)]);\n      switch number\n      case 1\n        m=m-1;\n      case 2\n        n=n-1;\n      case 3\n        m=m-1;\n        n=n-1;\n      end\n    end\n    k=k+1;\n    w=[m n; w]; \nend\n\n% warped waves\nrw=r(w(:,1));\ntw=t(w(:,2));\nend\n</code></pre>\n', 'ViewCount': '87', 'Title': 'How can I analyze the time complexity of this Dynamic Time Warping algorithm implemented in MATLAB?', 'LastActivityDate': '2013-12-08T07:54:28.550', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18733', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11922', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-12-08T06:33:52.807', 'Id': '18732'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>According to Wikipedia, the <a href="http://en.wikipedia.org/wiki/Independent_set_%28graph_theory%29" rel="nofollow">Independent Set</a> problem is a special case of the <a href="http://en.wikipedia.org/wiki/Set_packing" rel="nofollow">Set Packing</a> problem. But, it seems to me that these problems are equivalent.</p>\n\n<p>The <a href="http://en.wikipedia.org/wiki/Independent_set_%28graph_theory%29" rel="nofollow">Independent Set</a> search problem is: given a graph $G(V,E)$ and an integer $n$, find $n$ vertices no two of which are adjacent.</p>\n\n<p>The <a href="http://en.wikipedia.org/wiki/Set_packing" rel="nofollow">Set Packing</a> search problem is: given a finite collection $C$ of finite sets and an integer $n$, find $n$ sets that are pairwise disjoint.</p>\n\n<p>I think they are equivalent based on the following bidirectional reduction:</p>\n\n<p>&rarr;: Given an independent set problem on a graph $G(V,E)$, create a collection of $C$ of sets, where for each vertex $v \\in V$ there is a set $S_v \\in C$ containing all edges adjacent to $v$. Now, every set packing in $C$ corresponds to a set of vertices no two of which have an edge in common, i.e., this is an independent set in $G$ of the same size.</p>\n\n<p>&larr;: Given a set packing problem on a collection $C$, create a graph $G(V,E)$ where for every set $S \\in C$ there is a vertex $v_S \\in V$, and there is an edge between $v_{S_1}$ and $v_{S_2}$ iff the sets $S_1$ and $S_2$ intersect. Now, every independent vertex set in $G$ corresponds to a set of sets from $C$ no two of which intersect, i.e., this is a set packing in $C$ of the same size.</p>\n\n<p>My question is: is my reduction correct? If so, are these problem equivalent? Is it possible to use approximation algorithms for one problem on the other problem?</p>\n', 'ViewCount': '145', 'Title': 'Equivalence of independent set and set packing', 'LastActivityDate': '2013-12-08T11:18:43.767', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18741', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><graphs><sets><packing>', 'CreationDate': '2013-12-08T08:50:02.567', 'FavoriteCount': '1', 'Id': '18736'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<pre><code>There are n collections of M sets.\nPick a single set from each collection,\nsuch that all n picked sets are pairwise disjoint.\n</code></pre>\n\n<p>This problem can be converted to the standard <a href="http://en.wikipedia.org/wiki/Set_packing" rel="nofollow">Set Packing</a> problem in the following way: add a unique element $e_i$ to all $M$ sets in each collection $C_i$. Then find a set packing of size $n$ in the resulting collection. Each set in the returned set packing must belong to a different collection.</p>\n\n<p>So, the variant is not more difficult than the original set packing problem.</p>\n\n<p>MY QUESTION IS: is the variant easier than the original problem? In particular:</p>\n\n<ul>\n<li>Is it possible to solve the variant problem in time polynomial in $n$ (assuming $M$ is constant)?</li>\n<li>Is it possible to approximate the variant problem in a more efficient way than the approximations known for the general set packing problem (i.e. $O(\\sqrt{nM})$)?</li>\n</ul>\n', 'ViewCount': '40', 'Title': 'Set packing variant', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-12-08T11:02:12.180', 'LastEditDate': '2013-12-08T10:39:51.820', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18740', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><np-complete><sets><packing>', 'CreationDate': '2013-12-08T09:03:16.310', 'Id': '18737'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The algorithm (from <a href="http://en.wikipedia.org/wiki/Mastermind_%28board_game%29" rel="nofollow">here</a>) - </p>\n\n<blockquote>\n  <ol>\n  <li><p>Create a set S of remaining possibilities (at this point there are 1296). The first guess is aabb.</p></li>\n  <li><p>Remove all possibilities from S that would not give the same score of colored and white pegs if they were the answer.</p></li>\n  <li><p>For each possible guess (not necessarily in S) calculate how many possibilities from S would be eliminated for each possible\n  colored/white score. The score of the guess is the least of such\n  values. Play the guess with the highest score (minimax).</p></li>\n  <li><p>Go back to step 2 until you have got it right.</p></li>\n  </ol>\n</blockquote>\n\n<p>I confused about the 3nd step - </p>\n\n<p>what is mean -  </p>\n\n<blockquote>\n  <p>how many possibilities from S would be eliminated for each possible\n  colored/white score</p>\n</blockquote>\n\n<p>what is the "correct answer" and the "guess" here  ? </p>\n\n<p>Can someone clear it some more ? </p>\n', 'ViewCount': '317', 'Title': 'Mastermind (board game) - Five-guess algorithm', 'LastEditorUserId': '4409', 'LastActivityDate': '2013-12-08T23:49:43.557', 'LastEditDate': '2013-12-08T18:02:05.260', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18756', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4409', 'Tags': '<algorithms><game-theory><board-games>', 'CreationDate': '2013-12-08T13:56:57.047', 'Id': '18749'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '16', 'Title': 'Why generally to find a Euler cycle is easier than Hamilton cycle for the same set of nodes?', 'LastEditDate': '2013-12-09T19:57:24.237', 'AnswerCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11964', 'FavoriteCount': '1', 'Body': "<p>To find a Hamilton cycle is a NPC problem, but Euler is not. Considering one can always transform the vertex as edge or vice versa conceptually. Then the vertex can be used to describe the information which originally edge does.</p>\n\n<p>What properties make the Euler graph more easily to be resolved?</p>\n\n<p>For example, in genome assembly problem, one can either consider a Kmer as vertex or edge(de bruijn graph), it's just two perspectives to look at the same thing. I think their should be additional or missing information between two kind of explanation.</p>\n", 'ClosedDate': '2013-12-10T07:16:43.247', 'Tags': '<algorithms><graph-theory><eulerian-paths><hamiltonian-path>', 'LastEditorUserId': '11964', 'LastActivityDate': '2013-12-09T19:57:24.237', 'CommentCount': '2', 'CreationDate': '2013-12-09T19:48:17.363', 'Id': '18785'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $M$ be a finite set of even cardinality. Define $C=\\{\\{a,b\\}:a,b \\in M, a \\neq b\\}$ the set of all pairs over $M$. Let $w:C \\rightarrow \\mathbb{R}^+_0$ be a function.</p>\n\n<p>Now find $C' \\subset C$ with the following constraints:</p>\n\n<p>$$\n\\bigcup C' = M \\\\\n\\forall x,y \\in C': x \\cap y = \\emptyset \\\\\n\\sum_{x \\in C'}w(x) \\text{ minimal}\n$$</p>\n\n<p>In words: Find a subset $C'$ of pair-wise disjunctive pairs over $M$ that covers $M$, with the sum of these pairs being minimal. Any element of $M$ must appear in exactly one pair.</p>\n\n<p>I could not find an efficient algorithmic solution, and I also fail to relate this to any other known (optimization) problem. I was thinking of the subset sum problem, but I don't see any relation.</p>\n\n<p>So the questions is: Can you find an efficient algorithm to find $C'$? A good approximation might also be sufficient. If not, can you reduce this to any other known computer science problem? </p>\n", 'ViewCount': '87', 'Title': 'Find subset with minimal sum under constraints', 'LastActivityDate': '2013-12-10T00:52:40.407', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18801', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11970', 'Tags': '<algorithms><reductions><optimization>', 'CreationDate': '2013-12-10T00:01:00.097', 'Id': '18799'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a weighted digraph $G=V,E$, and a weight function, $d(u,v)$, one can normally use Dijkstra's algorithm to obtain the shortest path. What I am interested in, is how to obtain the $2^{nd}$-shortest path, the $3^{rd}$-shortest, and so on.</p>\n\n<p>Questions:</p>\n\n<blockquote>\n  <p>Is there an efficient algorithm to get the i-th-most-shortest-path between two nodes in a weighted graph?</p>\n  \n  <p>Is there an efficient algorithm to get the k-most-shortest-paths between two nodes in a weighted graph?</p>\n</blockquote>\n\n<p>An answer to either one is OK, though I wonder if an answer to the second question can be done more efficiently than $k$ calls to an answer to the first question.</p>\n", 'ViewCount': '67', 'Title': 'k-shortest paths', 'LastActivityDate': '2013-12-11T00:25:44.833', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><graphs><shortest-path><graph-traversal>', 'CreationDate': '2013-12-11T00:25:44.833', 'FavoriteCount': '1', 'Id': '18849'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following problem: given a set of $m$ red points and $n$ blue points in the plane, find a minimum length cycle that separates the red points from the blue points. That is, the red points are inside the cycle and the blue points are outside the cycle, or vice versa. This problem is called the <em>red blue separation problem</em>.</p>\n\n<p>I am trying to reduce the Traveling Salesman Problem (TSP) to this but I am getting nowhere. Can you please help me with this? Any help is appreciated.</p>\n', 'ViewCount': '122', 'Title': 'Prove the red blue separation problem is NP-complete', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-13T18:02:00.340', 'LastEditDate': '2013-12-13T18:02:00.340', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18872', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12004', 'Tags': '<algorithms><algorithm-analysis><np-complete><np-hard><np>', 'CreationDate': '2013-12-11T00:44:22.820', 'Id': '18852'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This problem is from the book <a href="http://cseweb.ucsd.edu/users/dasgupta/book/index.html" rel="nofollow">Algorithms, Chapter 5: Greedy algorithms</a>. In case of being closed as a duplication to that in <a href="http://stackoverflow.com/questions/13127446/minimum-spanning-tree-subgraph">Minimum Spanning tree subgraph</a>, I will first make a defense:</p>\n\n<ul>\n<li>The accepted solution at <a href="http://stackoverflow.com/questions/13127446/minimum-spanning-tree-subgraph">Minimum Spanning tree subgraph</a> <strong>is</strong> still in dispute.</li>\n<li>The solution given by <code>eh9</code> seems to be related with the Kruskal MST algorithm.</li>\n<li>I am seeking for a general (and easy) solution.</li>\n</ul>\n\n<p>The problem is as follows:</p>\n\n<blockquote>\n  <p>Let $T$ be an MST of graph $G$. Given a connected subgraph $H$ of $G$, show that $T \\cap H$ is contained in some MST of $H$.</p>\n</blockquote>\n\n<p>My partial trial is <em>by contradiction</em>:</p>\n\n<blockquote>\n  <p>Suppose that $T \\cap H$ is not contained in any MST of $H$. That is to say, for any MST of $H$ (denoted $MST_{H}$), there exists an edge $e$ such that $e \\in T \\cap H$, and however, $e \\notin MST_{H}$.<br>\n  Now we can add $e$ to $MST_{H}$ to get $MST_{H} + {e}$ which contains a cycle (denoted $C$). </p>\n  \n  <ul>\n  <li>Because $MST_{H}$ is a minimum spanning tree of $H$ and $e$ is not in $MST_{H}$, we have that every other edge $e\'$ than $e$ in the cycle $C$ has weight no greater than that of $e$ (i.e., $\\forall e\' \\in C, e\' \\neq e. w(e\') \\le w(e)$).</li>\n  <li>There exists at lease one edge (denoted $e\'\'$) in $C$ other than $e$ which is not in $T$. Otherwise, $T$ contains the cycle $C$. </li>\n  </ul>\n  \n  <p>Now we have $w(e\'\') \\le w(e)$ and $e \\in T \\land e\'\' \\notin T$, $\\ldots$</p>\n</blockquote>\n\n<p>As you see, I failed to continue with the above argument. Therefore, my problem here is:</p>\n\n<blockquote>\n  <ul>\n  <li>Is my argument by contradiction feasible so far? </li>\n  <li>And, how to draw the contradiction to complete the argument?</li>\n  </ul>\n</blockquote>\n', 'ViewCount': '178', 'Title': 'Minimum spanning tree and its connected subgraph', 'LastEditorUserId': '4911', 'LastActivityDate': '2013-12-12T23:34:15.087', 'LastEditDate': '2013-12-11T15:25:56.723', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithms><graph-theory><spanning-trees>', 'CreationDate': '2013-12-11T08:02:14.180', 'Id': '18867'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '75', 'Title': 'Is radix sort a greedy algorithm?', 'LastEditDate': '2014-01-19T02:27:32.353', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8870', 'FavoriteCount': '1', 'Body': '<p>I was thinking of radix sort, and at a sudden thought that it uses de facto the paradigm  of dynamic programming, but I soon changed my mind to greedy algorithm. Is it really a greedy algorithm? </p>\n', 'Tags': '<algorithms><terminology><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T02:27:32.353', 'CommentCount': '3', 'AcceptedAnswerId': '18895', 'CreationDate': '2013-12-11T19:37:58.497', 'Id': '18889'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading <a href="http://en.wikipedia.org/wiki/Subgraph_isomorphism_problem" rel="nofollow">Subgraph isomorphism problem</a> </p>\n\n<p>I am having trouble understanding how they prove that the subgraph isomorphism problem is NP-Complete using the Hamiltonian cycles problem in the article.  </p>\n\n<p>Can someone help me explain what is happening in more laymen terms?  </p>\n', 'ViewCount': '187', 'Title': 'Reducing from Hamiltonian Cycle to Subgraph Isomorphism', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-12T09:18:51.053', 'LastEditDate': '2013-12-12T09:18:51.053', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18907', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12045', 'Tags': '<algorithms><graph-theory><np-complete>', 'CreationDate': '2013-12-12T00:15:59.803', 'Id': '18906'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have following problem: I have a sorted sequence of $N$ integers (assume they are monotonically increasing).  I want to check whether there is any subsequence of length $\\ge N/4$, such that consecutive elements of the subsequence all differ by the same value.</p>\n\n<p>For example, in the sequence [3,4,5,8,12] there are two such subsequences: [3,4,5] (the difference is 1) and [4,8,12] (the difference is 4).  Thus, the length of longest such subsequence is 3 for this example.  Since $3 \\ge 5/4$, the answer is yes, there is a subsequence of length $\\ge N/4$ with the desired property.</p>\n\n<p>In my real-life situation, the sequence is of length $N\\approx 10^6$, and the elements are all 9-digit numbers.  Is there an efficient algorithm to solve this problem?</p>\n\n<hr>\n\n<p>My naive approach was to create Cartesian product with absolute differences between numbers:</p>\n\n<p>$$\n\\left( \\begin{array}{ccccc}\n0 &amp; 1 &amp; 2 &amp; 5 &amp; 9 \\\\\n1 &amp; 0 &amp; 1 &amp; 4 &amp; 8 \\\\\n2 &amp; 1 &amp; 0 &amp; 3 &amp; 7 \\\\\n5 &amp; 4 &amp; 3 &amp; 0 &amp; 4 \\\\\n9 &amp; 8 &amp; 7 &amp; 4 &amp; 0 \\end{array} \\right) $$</p>\n\n<p>And then focus on top-right part and compute number of occurrences of each difference, so:</p>\n\n<p>$$\n ||\\text{diff-by-1}|| = 2 =&gt; \\text{3 numbers diff by 1}\\\\\n ||\\text{diff-by-4}|| = 2 =&gt; \\text{3 numbers diff by 4}\n$$</p>\n\n<p>This is very simple and very ineffective. It requires lot of comparisons and does not scale (at all): its running time is $\\Theta(N^2)$. In my real life scenario my sequence is ~10^6 long, so this is too slow.</p>\n\n<p>To give you wider picture as maybe there is much better (probabilistic) approach to this problem: after largest sub-sequence is found I want to compute simple ratio: </p>\n\n<p>$$\nr:=\\frac{\\text{largest sub-sequence length}}{\\text{sequence length}}\n$$</p>\n\n<p>and if $r$ is greater then some fixed value I want to raise alarm (or do whatever I have to do ;-)).</p>\n\n<p>Thanks for any help, references, pointers, etc.</p>\n\n<p>BTW: here are things that I was/am looking at:</p>\n\n<ul>\n<li><a href="http://link.springer.com/article/10.1007/s00453-009-9376-2" rel="nofollow">http://link.springer.com/article/10.1007/s00453-009-9376-2</a></li>\n<li><a href="http://en.wikipedia.org/wiki/Longest_increasing_subsequence_problem" rel="nofollow">http://en.wikipedia.org/wiki/Longest_increasing_subsequence_problem</a></li>\n<li><a href="http://en.wikipedia.org/wiki/Longest_common_subsequence_problem" rel="nofollow">http://en.wikipedia.org/wiki/Longest_common_subsequence_problem</a></li>\n<li><a href="http://en.wikipedia.org/wiki/Kalman_filter" rel="nofollow">http://en.wikipedia.org/wiki/Kalman_filter</a></li>\n</ul>\n\n<p><strong>Update</strong>: was thinking a little bit more about it and started from the end, so instead of computing all differences between numbers (top-right corner of the matrix) I can derive small $k$ value from "fixed value" I mentioned at the end of original question. For instance if I am going to raise the alarm when 25% of all numbers are in some sequence I need to focus on small "triangles" in matrix and number of computations required is smaller (much smaller). \nWhen I add some sampling then it should be simple enough to implement at scale.</p>\n\n<p><strong>Update 2</strong> - Implemented @D.W. algorithm, sample run below:</p>\n\n<pre><code>    11:51:06 ~$ time nodejs progression.js \n    L: 694000000,694000002,694000006,694000007,694000009,694000010,\n        694000013,694000015,694000018,694000019,694000021,694000022,694000023,\n    694000026,694000028,694000030,694000034,694000036,694000038,694000040,\n    694000043,694000045,694000046,694000048,694000051,694000053,694000055,\n    694000057,694000060,694000061,694000063,694000067,694000069,694000072,\n    694000074,694000076,694000077,694000079,694000080,694000082,694000083,\n    694000084,694000086,694000090,694000091,694000093,694000095,694000099,\n    694000102,694000103,694000105,694000108,694000109,694000113,694000116,\n    694000118,694000122,694000125,694000128,694000131,694000134,694000137,\n    694000141,694000143,694000145,694000148,694000152,694000153,694000154,\n    694000157,694000160,694000162,694000163,694000166,694000170,694000173,\n    694000174,694000177,694000179,694000180,694000181,694000184,694000185,\n    694000187,694000189,694000193,694000194,694000198,694000200,694000203,\n    694000207,694000211,694000215,694000219,694000222,694000226,694000228,\n    694000232,694000235,694000236\n    N: 100\n    P: 0.1\n    L: 10 (min)\n    D: 26 (max)\n    [ 9, 18, 27, 36, 45, 54, 63, 72, 81, 90 ]\n    Found progression of 10 elements, difference: 16 starts: 694000045, ends: 694000189.\n\n    real    0m0.065s\n    user    0m0.052s\n    sys 0m0.004s\n</code></pre>\n', 'ViewCount': '249', 'Title': "Detecting a subsequence that's an arithmetic progression, in a sorted sequence", 'LastEditorUserId': '8380', 'LastActivityDate': '2013-12-16T10:54:37.677', 'LastEditDate': '2013-12-16T10:54:37.677', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '19002', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8380', 'Tags': '<algorithms><decision-problem><subsequences>', 'CreationDate': '2013-12-13T13:35:39.720', 'Id': '18951'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we want to arrange n numbers stored in an array such that all negative value occur before the positive ones. What will be the minimum number of exchanges in the worst case ? </p>\n', 'ViewCount': '98', 'Title': 'Minimum number of exchanges needed to get all negative values left of all positive ones', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T21:01:41.950', 'LastEditDate': '2014-03-14T21:01:41.950', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11984', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2013-12-13T13:48:34.530', 'Id': '18952'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>(This may be more fitting for CSTheory, I\'m not sure.)</p>\n\n<p>I\'m looking for an practical or theoretical work (that is, academic papers, online jots, pseudocode or code) regarding efficient algorithms for the following problem:</p>\n\n<h2>Unknown-Number-of-Bins Histogram</h2>\n\n<p><strong>Inputs:</strong></p>\n\n<ul>\n<li>An array of integers $a$, of length $n$.</li>\n</ul>\n\n<p><strong>Outputs:</strong> </p>\n\n<ul>\n<li>An array of integers $\\text{bins}$ of length $m &lt;= n$.</li>\n<li>An array of unsigned integers $\\text{counts}$, also of length $m$.</li>\n</ul>\n\n<p><strong>Output Requirements:</strong> </p>\n\n<ul>\n<li><p>For every $i \\in \\{0...m-1\\}$ it must be the case that </p>\n\n<p>$\\bigl|\\bigl\\{ j \\in \\{0...n-1\\} \\mid a_j = \\text{bins}_i \\bigr\\}\\bigr|$\n$ = \\text{counts}_i$</p>\n\n<p>In other words, $\\text{bins}$ and $\\text{counts}$ constitute a histogram of $a$, with one bin for every unique value in $a$.</p></li>\n<li>It is <strong>not</strong> required for $\\text{bins}$ or $\\text{counts}$ to be sorted.</li>\n</ul>\n\n<p><strong>Other Notes:</strong></p>\n\n<ul>\n<li>Complexity is considered as a function of <strong>both $n$ and $m$</strong>. </li>\n<li>Low time complexity is required both asymptotically for relatively low values $m$ - but not for low values of $n$.</li>\n<li>No hiding monstrosities in the $\\mathop{O}(\\cdot)$ constants please!</li>\n<li>A parallel(izable) approach? You are most welcome :-)</li>\n<li>Low space complexity is a benefit.</li>\n<li>Deterministic algorithms preferred, and barring that, go easy on those coin flips.</li>\n</ul>\n\n<p>Clearly, there are many way to go about this, some very straightforward, e.g. "sort the input, then build a sorted histogram in a single pass", in $\\mathop{O}(n \\log{n})$ time. Of course I wanted something better....</p>\n', 'ViewCount': '62', 'Title': 'Computing a histogram with the number of extant values not known in advance', 'LastEditorUserId': '11796', 'LastActivityDate': '2013-12-16T09:13:58.833', 'LastEditDate': '2013-12-16T09:13:58.833', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11796', 'Tags': '<algorithms><combinatorics><integers><counting>', 'CreationDate': '2013-12-14T12:42:42.897', 'Id': '18979'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider this tree:</p>\n\n<p><img src="http://i.stack.imgur.com/cMslZ.png" alt="Simple binary tree"></p>\n\n<p>If I traverse it using post-order, I\'d start at <em>B</em> (as it is the leftmost leaf) and that\'s where my misunderstanding begins. I know <em>B</em> is the first and <em>A</em> will be the last node in post order, as the rule is left-right-root. One of my university professors said the correct answer for the post-order traversal of a tree similar to the one above would be <strong><em>B</em>, <em>C</em>, <em>D</em>, <em>E</em>, <em>A</em></strong>, but in my understanding, it should be <strong><em>B</em>, <em>D</em>, <em>E</em>, <em>C</em>, <em>A</em></strong>.  </p>\n\n<p>Am I getting it wrong? Shouldn\'t I evaluate <em>(C,D),(C,E)</em> as a subtree and then go back to the parent tree?</p>\n', 'ViewCount': '40', 'Title': "Doesn't post-order traversal require subtrees to be evaluated separately?", 'LastEditorUserId': '12111', 'LastActivityDate': '2013-12-14T19:07:44.407', 'LastEditDate': '2013-12-14T19:07:44.407', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18988', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12111', 'Tags': '<data-structures><binary-trees><search-algorithms><trees><search-trees>', 'CreationDate': '2013-12-14T17:40:35.087', 'Id': '18987'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it possible to use a sorting algorithm with a non-transitive comparison, and if yes, why is transitivity listed as a requirement for sorting comparators?</p>\n\n<p>Background:</p>\n\n<ul>\n<li><p><em>A sorting algorithm generally sorts the elements of a\nlist according to a comparator function C(x,y), with</em></p>\n\n<p>\\begin{array}{ll} C(x,y) = \\begin{cases}\n    -1  &amp; {\\text{if}}\\ x\\prec y   \\\\ 0 &amp; {\\text{if}}\\ x\\sim y \\\\ \n    +1 &amp; {\\text{if}}\\ x\\succ y  \\\\ \\end{cases} \\end{array}</p>\n\n<p><em>The requirements for this comparator are, as far as I\nunderstand them:</em></p>\n\n<ul>\n<li><em>reflexive:</em> $\\forall x: C(x,x)=0$ </li>\n<li><em>antisymmetric:</em> $\\forall x,y: C(x,y) = - C(y,x)$ </li>\n<li><em>transitive:</em> $\\forall x,y,z, a: C(x,y)=a \\land C(y,z)=a \\Rightarrow C(x,z)=a$</li>\n<li><em>C(x,y) is defined for all x and y, and the results depend only on x and y</em></li>\n</ul>\n\n<p><em>(These requirements are always listed differently accross different\nimplementations, so I am not sure I got them all right)</em></p></li>\n</ul>\n\n<p>Now I am wondering about a "tolerant" comparator function, that accepts numbers x,y as similar if$ |x - y| \\le 1$:\n\\begin{array}{ll}\nC(x,y) = \\begin{cases}\n-1  &amp; {\\text{if}}\\ x\\lt y-1   \\\\\n0 &amp; {\\text{if}}\\ |x - y| \\le 1 \\\\ \n+1 &amp; {\\text{if}}\\ x\\gt y+1  \\\\\n\\end{cases}\n\\end{array}</p>\n\n<p>Examples: both <code>[ 1, 2, 3, 4, 5]</code> and <code>[1, 4, 3, 2, 5]</code> are correctly sorted  in ascending order according to the tolerant comparator ($C(x,y) \\le 0$ if x comes before y in the list)<br>\nbut <code>[1, 4, 2, 3, 5]</code> is not, since C(4,2)=1</p>\n\n<p>This tolerant comparator is reflexive and antisymmetric, but not transitive.</p>\n\n<p>i.e. C(1,2) = 0 , c(2,3) = 0, but C(1,3) = -1, violating transitivity</p>\n\n<p>Yet I cannot think of any sorting algorithm that would fail to produce a "correctly sorted" output when given this comparator and a random list.</p>\n\n<p>Is transitivity therefore not required in this case? And is there a less strict version of transitivity that <em>is</em> required for the sorting to work?</p>\n\n<p>Related questions:</p>\n\n<ul>\n<li><a href="http://math.stackexchange.com/q/276907/34541">Why is antisymmetry necessary for comparison sort?</a> (about antisymmetry)</li>\n<li><a href="http://cs.stackexchange.com/q/2336/2932">Sorting algorithms which accept a random comparator</a> (about a random C(x,y))</li>\n<li><a href="http://stackoverflow.com/q/20363810/145999">OrderBy with a non-transitive IComparer</a> (about the c# sort algorithm, by me)</li>\n</ul>\n', 'ViewCount': '234', 'Title': 'Is transitivity required for a sorting algorithm', 'LastActivityDate': '2014-02-12T23:38:27.540', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2932', 'Tags': '<algorithms><sorting><quicksort><transitivity>', 'CreationDate': '2013-12-15T17:19:24.000', 'Id': '19013'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So, I have a list of cities (A, B, C, etc) with weighted edges (two-way, undirected) between them with a list of cities that already have a store.</p>\n\n<p>My task is to place a one new store at either A, B, C, etc so that I minimize the travel distance from any city to a store.</p>\n\n<p>So my head is thinking</p>\n\n<ol>\n<li>Perform Dijkstra\'s on every non-city vertex</li>\n<li>Find the max distance among that node -> all other nodes.</li>\n<li>Store that distance associated with the store it originated from.</li>\n</ol>\n\n<p>The problem I encountered was</p>\n\n<p>The easiest example of A, B, and C cities with a store already existing at A, has roads of\nAB = 1, BC = 2.</p>\n\n<p>Placing a store at B, would make C have to travel 2 units to the nearest store (at B). \nWhile placing a store at C, would make B have to travel 1 unit (since it can backtrack to A where a store is at).</p>\n\n<p>This "backtracking" to other nodes is causing my mix-ups. Since I\'m only iterating from the point of view of either City B or C. B\'s point of view thinks the max distance to a store is 2, while C does as well. (Even though C should find the max value of 1 unit).</p>\n', 'ViewCount': '126', 'Title': 'all pairs shortest path using Dijkstra', 'LastEditorUserId': '11483', 'LastActivityDate': '2013-12-18T05:57:22.477', 'LastEditDate': '2013-12-17T15:25:25.707', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '19085', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11483', 'Tags': '<algorithms>', 'CreationDate': '2013-12-16T17:19:25.133', 'Id': '19048'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For my homework I have a problem that I can't solve and it makes me wonder about 2 different MST:</p>\n\n<blockquote>\n  <p>Let $G=(V,E)$ be a graph that has a minimum spanning tree $T$.</p>\n  \n  <p>I want to find another minimum spanning tree $T'$ that has at least 1 different edge $e'$\n  such that the weight of $e'$ is differ from any weight of edges in $T$.</p>\n</blockquote>\n\n<p>If $T'$ doesn't exist I can claim that every 2 different MST must have the same weight for each edge. </p>\n\n<p>My intuition says that this claim is wrong but on the other hand I can't find example of $T'$ to contradict this claim.</p>\n", 'ViewCount': '72', 'Title': 'Find a diffrent minimal spanning tree for a graph', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-19T10:57:54.237', 'LastEditDate': '2013-12-19T10:57:54.237', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19116', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9292', 'Tags': '<algorithms><graph-theory><graphs><spanning-trees>', 'CreationDate': '2013-12-16T17:31:17.433', 'Id': '19049'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given two NP NP-hard functional problems, A and B, one can find a reduction of A to B. Is it possible to find a reduction that would honour approximations? That is, if you have an approximation algorithm for B that yield approximate solutions within accuracy $\\delta$, is it possible to reduce A to B in such a way that one would be able to derive an approximate solution of A within accuracy $\\epsilon = \\epsilon(\\delta)$?</p>\n', 'ViewCount': '85', 'Title': 'Approximation algorithms for NP-complete problems', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-19T05:48:21.523', 'LastEditDate': '2013-12-16T20:06:37.607', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10447', 'Tags': '<complexity-theory><reductions><approximation-algorithms>', 'CreationDate': '2013-12-16T18:26:00.343', 'Id': '19050'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a question regarding recursion in <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm" rel="nofollow">Viterbi algorithm</a>.</p>\n\n<p>Define $\\pi(k; u; v)$  which is the maximum probability for any sequence of length $k$, ending in the tag bigram $(u; v)$.</p>\n\n<p>The base case if obvious $\\pi(0,*,*)=1$</p>\n\n<p>The general case.</p>\n\n<p>$\\pi(k,u,v) = max_{w \\in K_{k-2} } \\pi(k-1,w,u) \\cdot q(v|w,u) \\cdot e(x_k|v)$</p>\n\n<p>The author justifies the recursion as folllows: </p>\n\n<blockquote>\n  <p>How can we justify this recurrence? Recall that $\\pi(k, u, v)$ is the highest probability for any sequence $y_{\u22121}...y_k$ ending in the bigram $(u, v)$. Any such sequence must have $y_{k\u22122} = w$ for some state $w$. The highest probability for any sequence of length $k \u2212 1$ ending in the bigram $(w, u)$ is $\\pi(k \u2212 1, w, u)$, hence the highest probability for any sequence of length $k$ ending in the trigram $(w, u, v)$ must be $\\pi(k \u2212 1,w, u) \\cdot q(v|w, u) \\cdot e(x_k |v)$</p>\n</blockquote>\n\n<p>I do not understand why it\'s actually true, I think it\'s possible to reach $\\pi(n,u, v)$ from any $(n-1,w, u)$ not actually the maximum one $\\pi(n-1,w, u)$ just because $q(v|w, u) \\cdot e(x_k |v)$ might have a higher influence on the resulting $(n,u, v)$ than any $\\pi(n-1,w, u)$.</p>\n\n<p>I would appreciate if anyone could explain me why it\'s true.</p>\n', 'ViewCount': '94', 'Title': 'Viterbi algorithm recursive justification', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-04T18:05:36.053', 'LastEditDate': '2014-02-04T18:05:36.053', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19109', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8473', 'Tags': '<algorithms><dynamic-programming><recursion><correctness-proof><hidden-markov-models>', 'CreationDate': '2013-12-18T13:44:43.103', 'Id': '19093'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Relevant Information:<br>\n<a href="http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes" rel="nofollow">Sieve of Eratosthenes</a><br>\n<a href="http://en.wikipedia.org/wiki/Sieve_of_Sundaram" rel="nofollow">Sieve of Sundaram</a></p>\n\n<p>Suppose I want to generate all primes in <code>[2,n]</code>, and I have both of these algorithms at my disposal to get the job done. Which is preferable under what conditions?</p>\n\n<p>I read that Sundaram runs in O(n log n) time, whereas Eratosthenes runs in O(n log log n) time, so it seems that Eratosthenes is preferable. However, that is just a very superficial evaluation. Are there other factors (aside from ease of implementation) to be considered? Which is the \'better\' algorithm?</p>\n', 'ViewCount': '81', 'Title': 'Sieve of Eratosthenes vs. Sieve of Sundaram', 'LastActivityDate': '2013-12-19T10:41:54.903', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11093', 'Tags': '<algorithms><time-complexity><primes>', 'CreationDate': '2013-12-19T10:41:54.903', 'Id': '19115'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I came across an interesting procedure that ranks (sorts) a set of tuples, <em>not</em> by comparisons between tuples, but by the proximity between <em>next</em> tuple(s) and the set of tuples already ranked.</p>\n\n<p>Specifically, consider the ranking procedure as follows,</p>\n\n<p>Input: $D=\\{p_i\\mid p_i \\in \\mathbb{R}^2, i=1, 2,\\ldots,n\\}$, $i_{start} \\in \\{1, 2,\\ldots, n\\}$, and distance metric $f: 2^D \\times D \\mapsto \\mathbb{R}_{\\ge 0}$<br>\nOutput: $\\Pi = \\left[ i_1, i_2, \\ldots, i_n\\right]$</p>\n\n<ol>\n<li>$\\Pi \\gets \\left[~ \\right]$; // empty sequence</li>\n<li>$A \\gets \\emptyset$;  </li>\n<li>$i_{next} \\gets i_{start}$;  </li>\n<li>while $D \\neq \\emptyset$  </li>\n<li>&nbsp;&nbsp;&nbsp;&nbsp;$\\Pi \\gets \\Pi \\oplus i_{next} $ // append $i_{next}$ to sequence</li>\n<li>&nbsp;&nbsp;&nbsp;&nbsp;$A \\gets A \\bigcup \\left\\{ p_{i_{next}} \\right\\}$; // $p_{i_{next}}$ is a tuple from $D$ identified by subscript ${i_{next}}$  </li>\n<li>&nbsp;&nbsp;&nbsp;&nbsp;$D \\gets D \\setminus \\left\\{p_{i_{next}}\\right\\}$;  // same $p_{i_{next}}$ as in line 6  </li>\n<li>&nbsp;&nbsp;&nbsp;&nbsp;${i_{next}} \\gets \\min \\bigl\\{\\arg_j\\,\\min_{p_j\\in D}\\,f(A, p_j) \\bigr\\}$; // not defined when $D = \\emptyset$   </li>\n</ol>\n\n<p>An example of the distance metric $f(A, p)$ is, say, the distance between 2D point $p$ and the centroid of 2D points in set $A$. As such, the procedure is literally the expansion of a cluster of 2D points, starting from a given point $p_{start}$, until all $n$ points from $D$ have been included. And the sequence $\\Pi$ records the order by which points from $D$ are included in the cluster.</p>\n\n<p>Could anyone shed some light on the literature, background, or well-known examples, of such ranking procedures? In particular, are there any previous results on the complexity bounds of such a procedure perhaps under different types of distance metrics?</p>\n', 'ViewCount': '93', 'Title': 'On ranking (sorting) by a varying distance metric', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-23T13:08:49.810', 'LastEditDate': '2013-12-23T13:08:49.810', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7644', 'Tags': '<algorithms><complexity-theory><reference-request><sorting><ranking>', 'CreationDate': '2013-12-19T18:41:36.297', 'FavoriteCount': '1', 'Id': '19128'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am curious whether the following problems has been studied before, but wasn't able to find any papers about it:</p>\n\n<p>Given a planar graph G, and two vertices s and t, find an st-path $P$ which minimizes the number of distinct faces of G which contain vertices of $P$ on their boundary.</p>\n\n<p>Does anybody know any references?  </p>\n", 'ViewCount': '70', 'Title': 'Finding an st-path in a planar graph which is adjacent to the fewest number of faces', 'LastActivityDate': '2013-12-20T00:57:12.100', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '12250', 'Tags': '<algorithms><graph-theory><reference-request>', 'CreationDate': '2013-12-20T00:57:12.100', 'Id': '19137'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm using machine-learning algorithms to solve binary classification problem (i.e. classification can be 'good' or 'bad'). I'm using <code>SVM</code> based algorithms, <code>LibLinear</code> in particular. When I get a classification result, sometimes, the probability estimations over the result are pretty low. For example, I get a classification 'good' with probability of 52% - those kind of results I rather throw away or maybe classify them as 'unknown'.</p>\n\n<p><strong>EDITED - by D.W.'s suggestion</strong></p>\n\n<p>Just to be more clear about it, my output is not only the classification 'good' or 'bad', I also get the confidence level (in %). For example, If I'm the weather guy, I'm reporting that tomorrow it will be raining, and I'm 52% positive at my forecast. In this case, I'm sure you won't take your umbrella when you leave home tomorrow, right? So in those cases where my model does not have a high confidence level I throw away this prediction and don't count it in my estimations.</p>\n\n<p>Unfortunately, I can't find articles regarding thresholding the probability estimations... </p>\n\n<p>Does anyone have an idea what is a normal threshold that I can set over the probability estimations? or at least can refer me to a few articles about it?</p>\n", 'ViewCount': '69', 'Title': 'What would be a decent threshold for classification problem?', 'LastEditorUserId': '11754', 'LastActivityDate': '2014-04-20T10:08:52.793', 'LastEditDate': '2013-12-21T08:24:16.580', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11754', 'Tags': '<machine-learning><probabilistic-algorithms><statistics><classification>', 'CreationDate': '2013-12-20T07:53:10.210', 'Id': '19146'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let\'s assume that I\'ve implemented a method for the extraction of the principal lines of the palmprint. For instance, let\'s say that I\'m able to do a transformation like the following one (image from <a href="http://kst.buu.ac.th/proceedings/KST2010/docs/en08.pdf" rel="nofollow">Patprapa Tunkpien, Sasipa Panduwadeethorn and Suphakant Phimoltares</a>):</p>\n\n<p><img src="http://i.stack.imgur.com/vc7YJ.png" alt="Example of extraction of principal lines of the palmprint"></p>\n\n<p>Now, I want to make an evaluation of the accuracy level of my algorithm. In order to do this, I have only the algorithm, and a big set of palmprint images (on which I can execute the extraction procedure to obtain the corresponding set of binary images of principal lines of the palmprint).</p>\n\n<p>How can I evaluate the accuracy level of my algorithm, without having any kind of reference extraction method?</p>\n', 'ViewCount': '34', 'Title': 'How to evaluate the accuracy of a method for the extraction of the principal lines of the palmprint', 'LastEditorUserId': '12301', 'LastActivityDate': '2013-12-22T20:05:22.463', 'LastEditDate': '2013-12-22T16:43:22.697', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19197', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12301', 'Tags': '<algorithms><algorithm-analysis><image-processing>', 'CreationDate': '2013-12-22T16:35:11.107', 'Id': '19192'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question was originally posted here: <a href="http://stackoverflow.com/q/20735339/2305618">http://stackoverflow.com/q/20735339/2305618</a></p>\n\n<p>I am surely not the first to have implemented code to perform the following graph transformation. But try as I might, I can\'t find a previous reference to it. </p>\n\n<p>The transformation arises when creating a single graph that simultaneously models hierarchical inclusion relationships and other pairwise relationships between nodes.</p>\n\n<p>The algorithm transforms a rooted directed tree DAG into an equivalent hierarchical network DAG. Equivalence in this sense - if the meaning of the tree structure is that parents \'consist of\' or \'contain\' their children the equivalent network structure would carry the same \'consisting of\' or \'containment\' information.</p>\n\n<p>The tree hierarchy is represented in the network as sub-networks and sub-sub-networks. The tree\'s exterior nodes are copied across and each interior node X is represented by the pair: X\' and X\'\'</p>\n\n<p>Does anyone know the name/reference of this transformation/ algorithm? :-)</p>\n\n<p>Illustration image of this transformation:\n<img src="http://davidpratten.com/wp-content/uploads/2013/12/example-algorithm.png" alt="Illustration image of this transformation"></p>\n\n<p>Thanks</p>\n\n<p>David</p>\n', 'ViewCount': '56', 'Title': "What is this algorithm? Create a tree's equivalent hierarchical network", 'LastEditorUserId': '12306', 'LastActivityDate': '2013-12-23T05:21:06.803', 'LastEditDate': '2013-12-23T05:21:06.803', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12306', 'Tags': '<algorithms><graph-theory><trees>', 'CreationDate': '2013-12-23T00:45:35.480', 'Id': '19200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am working on a special case of the longest path problem. For a cyclic directed graph $G=(V, E)$, where the edge-weights are probability values (i.e., $P(\\_) = w(s, q)$ with $s,q \\in V$), my aim is to find the least 'probable' path between two vertices. </p>\n\n<p>My initial approach is to generate an graph $G'$ where the weights are the complementary probabilities $1- w(s, q)$ (with strictly positive values), and compute Dijkstra's shortest path on $G'$. Is this reasoning sound? Or am I getting myself into an NP-hard disaster?</p>\n", 'ViewCount': '141', 'Title': 'Find least probable path in graph', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T02:10:13.287', 'LastEditDate': '2014-01-19T02:10:13.287', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '19217', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12300', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2013-12-23T12:08:42.880', 'Id': '19207'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an algorithm to solve the following problem:</p>\n\n<p>INPUT: a set of $n$ points in the plane, $(x_1,y_1),...,(x_n,y_n)$.</p>\n\n<p>OUTPUT: a set of $n-1$ axis-parallel interior-disjoint squares, such that the boundary of each square contains at least two points. (*)</p>\n\n<p>Here is an example of input (left) and possible output (right) for $n=4$:</p>\n\n<p><img src="http://i.stack.imgur.com/jBzRL.png" alt="enter image description here"></p>\n\n<p>Efficiency is not a concern right now - the time complexity may be exponential in $n$ (since I will usually use this algorithm with small $n$\'s).</p>\n\n<p>My first thought was along the following lines:</p>\n\n<ol>\n<li>For every pair of points, find a square that touches these two points.</li>\n<li>In the resulting set of $n(n-1)/2$ squares, find a subset with $n-1$ squares that are interior-disjoint, or report that such a set does not exist.</li>\n</ol>\n\n<p>The problem is that, for every two points, there may be infinitely many squares that touch them. For example, the points $(0,0)$ and $(10,7)$ are touched by any square with a side-length of 10 and lower-left corner in $0 \\times [-3,0]$. </p>\n\n<p>How would you approach this problem?</p>\n\n<p>(*) NOTE: I don\'t know whether such a set of $n-1$ squares always exists. I want to play with points in order to gain some intuition about this question and maybe find a counter-example.</p>\n', 'ViewCount': '69', 'Title': 'Finding squares touching points', 'LastEditorUserId': '1342', 'LastActivityDate': '2013-12-23T21:46:05.337', 'LastEditDate': '2013-12-23T20:34:40.807', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19224', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2013-12-23T20:17:00.867', 'Id': '19222'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Self-stabilizing algorithms are extremely useful in distributed systems,\n  but can the algorithm be applied concurrently to each computational node?</p>\n\n<p>My instinct is to say yes, but I can\'t help but imagine a \'Game Of Life\' scenario\n  where the state of the graph toggles between two states.\nWhat keeps the question alive in my mind is the possibility that both such states would be considered \'correct\', but then if the state were correct, wouldn\'t the graph stabilize?</p>\n\n<p>If they are not guaranteed to work in parallel,\n  can you provide an example where absolute convergence doesn\'t exist?</p>\n\n<hr>\n\n<p>A self-stabilizing algorithm is an algorithm (in all my experience, applied to arbitrary graphs and graph structures exclusively) that, upon repeated applications, guarantees <em>convergence</em> onto a correct state\u2014one that meets a set of requirements.  Consider a simple example:</p>\n\n<blockquote>\n  <p>You maintain a network of hospitals and data centers.\n  Each hospital must be connected to exactly one data center, and each data center must be connected to exactly two other data centers.  (You can imagine also attempting to minimize latency.)\n  When you set up the network, it is in a correct state\u2014all requirements are met.</p>\n  \n  <p>Weeks pass, and a giant storm takes out one of your data centers.\n  Being the forth-dimensional thinker you are, you planned for situations like this by putting into place a self-stabilizing algorithm that will automatically reconfigure the network to get back to a correct state.\n  You set up a daemon to oversee the operation.  It picks arbitrary nodes in your network and asks them if they are in a correct state.  If not, the daemon corrects that part of the network.</p>\n  \n  <p>All is well and you can stay asleep in bed (because everything catastrophic happens at 3am).</p>\n</blockquote>\n\n<p>The trick is, the daemon isn\'t a necessary part of this system\u2014it\'s just easier to think about it like this.\nIn reality, each node is its own computational unit\u2014constantly evaluating its own state and taking steps to correct itself if necessary.</p>\n\n<p>The introductory paragraphs of the <a href="http://en.wikipedia.org/wiki/Self-stabilization" rel="nofollow">Wikipedia article</a> on the subject give a very good technical overview:</p>\n\n<blockquote>\n  <p>Self-stabilization is a concept of fault-tolerance in distributed computing. A distributed system that is self-stabilizing will end up in a correct state no matter what state it is initialized with. That correct state is reached after a finite number of execution steps.</p>\n</blockquote>\n\n<p>Just as a piece of requested information that doesn\'t fit in well above:</p>\n\n<blockquote>\n  <p>When the graph is stable, no node has any faults with it.  Since no predicates apply to it, no actions are taken, and the graph remains stable.</p>\n</blockquote>\n', 'ViewCount': '47', 'Title': 'Are self-stabilizing algorithms guaranteed to work in parallel?', 'LastEditorUserId': '1854', 'LastActivityDate': '2014-05-03T22:37:43.923', 'LastEditDate': '2013-12-24T19:46:13.590', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '19259', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1854', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-12-24T04:06:36.440', 'Id': '19230'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose a parameter $\\hat{k}$ is larger than another parameter $k$, assume that $k$ is bounded \nby a function $f$ of $\\hat{k}$. </p>\n\n<p>How can we prove that if a problem is FPT with respect to $k$ implies it is FPT w.r.to $\\hat{k}$.</p>\n', 'ViewCount': '39', 'Title': 'Fixed Parameter Algorithms', 'LastActivityDate': '2013-12-24T10:15:26.153', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<algorithms><complexity-theory>', 'CreationDate': '2013-12-24T05:19:04.430', 'Id': '19233'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given simple, udirected and connected graph with $n$ verticies. Every edge in this graph has some weight. I have to find (in polynomial time) a set of edges such that :  </p>\n\n<p>1.every simple cycle in graph contains at least one edge from this set<br>\n2.sum of weights of these edges is the least possible  </p>\n\n<p>I have derived the following algorithm to solve this problem.  </p>\n\n<p>First, apply DFS algorithm, starting from 1. vertex, to get all back edges. It is known that every simple cycle in this graph will contain at least one back edge, so 1. condition will be satisfied. To satisfy the 2. condition, we must apply DFS algorithm starting from 2. vertex, 3. vertex .... n. vertex, to get different sets of back edges, that all satisfy 1. condition, and choose set with the least sum of weights.  </p>\n\n<p>My question is will this algorithm give the right answer ? And if it won't, then why?  </p>\n\n<p>I have some doubdts about it, for example, is the set of back edges, given by DFS algorithm, is least possible to satisfy the condition that every simple cycle in the graph contain at least one edge from this set? For example, DFS has given me that $BE =  \\left\\{ e_1, e_2, e_3, e_4 \\right\\}$ are the back edges of graph. So every simple cycyle will contain at least one edge from $BE$. But, is it possible that we can throw out, say, $e_3$ and still every simple cycle in graph will contain at least one edge from $\\left\\{ e_1, e_2, e_4 \\right\\}$ ?</p>\n", 'ViewCount': '86', 'Title': 'Finding edges with minimal weight sum, such that every simple cycle contain at least one edge', 'LastActivityDate': '2013-12-24T15:46:23.037', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '19251', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11315', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2013-12-24T10:19:44.943', 'Id': '19237'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let G be a weighted undirected graph and e be an edge with maximum weight in G.Suppose there is a minimum weight spanning tree in G containing the edge e.Which of the following statements is always TRUE?</p>\n\n<p>1.There exists a cut in g having all edges if maximum weight</p>\n\n<p>2.There exists a cycle in G having all edges of maximum weight</p>\n\n<p>3.Edge e can not be contained in a cycle </p>\n\n<p>4.All edges in G have the same weight </p>\n\n<p>i think Option 4 and 1 is correct but which is always true .and what is the meaning of option 3.Can any body remove my confusion ??</p>\n', 'ViewCount': '46', 'Title': 'Satisfying condition to be in minimum spanning tree of an edge (maximum weight)', 'LastActivityDate': '2013-12-24T10:44:53.143', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19241', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<algorithms><graph-theory><trees><spanning-trees>', 'CreationDate': '2013-12-24T10:26:51.827', 'Id': '19239'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to construct all inequivalent $8\\times 8$ matrices (or $n\\times n$ if you wish) with elements 0 or 1. The operation that gives equivalent matrices is the simultaneous exchange of the i and j row AND the i and j column. eg. for $1\\leftrightarrow2$\n\\begin{equation}\n\\left( \\begin{array}{ccc}\n0 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 1 \\\\\n1 &amp; 0 &amp; 0 \\end{array} \\right) \\sim\n\\left( \\begin{array}{ccc}\n1 &amp; 0 &amp; 1 \\\\\n0 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 \\end{array} \\right)\n\\end{equation}</p>\n\n<p>Eventually, I will also need to count how many equivalent matrices there are within each class but I think Polya's counting theorem can do that. For now I just need an algoritmic way of constructing one matrix in each inequivalence class. Any ideas?</p>\n", 'ViewCount': '127', 'Title': 'Constructing inequivalent binary matrices', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-03T13:13:25.200', 'LastEditDate': '2014-01-28T22:50:40.620', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '12348', 'Tags': '<algorithms><combinatorics>', 'CreationDate': '2013-12-24T15:30:07.377', 'FavoriteCount': '1', 'Id': '19250'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I want to run Dijkstra's algorithm on a graph whose edge weights are integers in the\nrange 0, ..., W, where W is a relatively small number.\nHow can I modify that algorithm so that it takes time just O((|V| + |E|) logW) and relatively easy implement that in C/C++?</p>\n", 'ViewCount': '259', 'Title': "Dijkstra's algorithm for edge weights in range 0, ..., W", 'LastActivityDate': '2013-12-25T17:06:47.797', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12351', 'Tags': '<algorithms><algorithm-analysis><data-structures><shortest-path><weighted-graphs>', 'CreationDate': '2013-12-24T16:15:19.157', 'Id': '19252'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>T (n) = T(\u221an) + 1\nThe easy way to do this is with a change of variables. Let m = lg n and\nS(m) = T (2^m).</p>\n\n<p>T(2^m) = T (2^(m/2)) + 1</p>\n\n<p>S(m) = S(m/2) + 1</p>\n\n<p>Can any one explain why 1 and 2 are same and this works ??</p>\n', 'ViewCount': '52', 'Title': "Can't under stand change of variable", 'LastActivityDate': '2013-12-25T11:55:52.397', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19273', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-12-24T21:23:15.767', 'Id': '19258'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have got some equipment of standard lengths, say: </p>\n\n<blockquote>\n  <p>equipment_lengths = {60, 48, 36, 29}</p>\n</blockquote>\n\n<p>that I have to place on a given length of, say 100. I have to place this equipment so as to minimize material waste while covering the given length. I am allowed to exceed the given length of 100 but keep the excess minimized.  I am not allowed to drop short of the given length. Also I have an unlimited number of each equipment.</p>\n\n<p>So, one solution to the above example would be:</p>\n\n<blockquote>\n  <p>{60, 48} (excess = 60 + 48 - 100 = 8)</p>\n</blockquote>\n\n<p>Another could be </p>\n\n<blockquote>\n  <p>{60, 60} (excess = 60 + 60 - 100 = 20)</p>\n</blockquote>\n\n<p>The first solution is preferable to the second one as it minimizes the excess. </p>\n\n<p>My first approach has been to implement a greedy algorithm that takes the longest equipment and keeps placing those till I exceed the given length. Once that happens I remove the last equipment I placed and place the next smaller ones till I exceed again. In the end I select the combination with the least amount of excess. For example the algorithm will successively do the following:</p>\n\n<ul>\n<li>60, 60 (excess = 20)</li>\n<li>60, 48 (excess = 8)</li>\n<li>60, 36, 36 (excess = 32)</li>\n<li>60, 36, 29 (excess = 25)</li>\n</ul>\n\n<p>So, now it will select {60, 48} as the optimal solution when you remove last one equipment on excess. The algorithm then repeats the exercise by removing the last two equipments and finds another optimal solution. Finally it chooses the best one among the two optimal solutions it found.</p>\n\n<p>Now, I have been studying bin packing and knapsack problem variations and while these problems look similar, I haven't been able to apply them to my problem. Does my problem have a well known solution? If not, how should I go about it to efficiently find the globally optimum solution?</p>\n\n<p>Additional information: I think giving preference to longer equipments should be a good idea as the equipments need to be joined with screws or bolts so joining two longer equipments would be less work than three shorter ones, but I don't think it's more important than material wastage.</p>\n", 'ViewCount': '26', 'Title': 'optimal placement of fixed length items on a given length', 'LastActivityDate': '2013-12-25T19:18:09.683', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19285', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12367', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-12-25T15:34:12.247', 'Id': '19277'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we are given matrices $A_1, \\ldots, A_k$ which are $n \\times n$ matrices with rational entries and are asked to determine whether the polynomial ${\\rm det}(\\alpha_1 A_1 + \\alpha_2 A_2 + \\cdots + \\alpha_k A_k)$ is identically zero. How can we do this <strong>deterministically</strong> in polynomial time in $n$ and $k$?</p>\n\n<p>I'm aware that black-box polynomial identity testing is a difficult problem, but then this is not quite a black box. </p>\n", 'ViewCount': '150', 'Title': 'Testing whether a determinant polynomial is identically zero', 'LastEditorUserId': '12370', 'LastActivityDate': '2013-12-29T04:06:42.227', 'LastEditDate': '2013-12-29T04:06:42.227', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '12370', 'Tags': '<algorithms><linear-algebra><polynomials>', 'CreationDate': '2013-12-25T16:01:54.260', 'Id': '19278'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>As described in <a href="http://courses.engr.illinois.edu/cs421/sp2012/project/PracticalEarleyParsing.pdf" rel="nofollow">this paper</a>, you can use an pre-computed automaton to speed up an Earley parse.  I\'m not interested in the rigorous proof of this, but just how the basic algorithm works so that I can implement it.  Understanding this paper on my own would take a long long time, and I don\'t think it\'s justified for this algorithm because it seems simple enough, but the paper is accademic and is required to be over my head, so they didn\'t write it for a reader like me.</p>\n\n<p>For instance, what does a transition labeled with a variable in the automaton mean?  How does this thing work?</p>\n', 'ViewCount': '35', 'Title': 'How does Earley parsing using an automaton work?', 'LastActivityDate': '2013-12-25T17:26:45.727', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12373', 'Tags': '<algorithms><automata><parsers>', 'CreationDate': '2013-12-25T17:26:45.727', 'Id': '19284'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that standard Bloom Filters only have operations like inserting elements and checking if an element belongs to filter, but are also some modification of Bloom filters which enable a delete operation--for example: counting Bloom filters. I heard also about another method, which uses a second filter. If I want to remove an element I have to 'insert' it into this second filter. I can't find how this proposed structure operates, any article about it, or even the name of the originator. Maybe someone can share with me with a link to any interesting articles about this method? I found a lot of articles about counting Bloom filters and other methods, but I can't find any description of this one.</p>\n", 'ViewCount': '101', 'Title': 'Deleting in Bloom Filters', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:49.423', 'LastEditDate': '2014-04-29T11:35:49.423', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12038', 'Tags': '<reference-request><data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2013-12-25T22:52:19.973', 'Id': '19292'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given are a 2D plane and a array of points in this plane, with every point having an integer value assigned.</p>\n\n<p>Is there an algorithm which, when given a ratio a/b, divides the plane with a straight line, so that the values of the points are distributed as close as possible to the given ratio? </p>\n\n<p>Points may be on the dividing line, then the are counted to the 'left/upper' partition.</p>\n", 'ViewCount': '53', 'Title': 'Partition points in a plane with a straigth line', 'LastActivityDate': '2013-12-28T20:30:41.743', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19346', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11258', 'Tags': '<algorithms><computational-geometry><partition>', 'CreationDate': '2013-12-26T12:46:45.640', 'FavoriteCount': '0', 'Id': '19299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Does anyone know a good algorithm for quickly finding an approximate solution to the following problem?</p>\n\n<p>Given two square matrices $A$ and $B$, minimize $\\| P A P^\\top - B \\|$ over all permutation matrices $P$.</p>\n\n<p>I have heard that there are several types of algorithms for these kinds of problems, like iterative improvement, simulated annealing, tabu search, genetic algorithms, evolution strategies, ant algorithms, and scatter search. I am looking for existing software.</p>\n', 'ViewCount': '34', 'Title': 'Quadratic programming problem involving permutation matrices', 'LastActivityDate': '2013-12-26T14:22:58.533', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12383', 'Tags': '<algorithms><optimization><permutations><approximation-algorithms>', 'CreationDate': '2013-12-26T14:22:58.533', 'Id': '19303'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Is there any example that anybody could come up  with that shows Prim's algorithm does not always give the correct result when it comes knowing the minimal spanning tree.</p>\n", 'ViewCount': '137', 'Title': "Minimal Spanning tree and Prim's Algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-05T17:28:24.357', 'LastEditDate': '2014-01-05T17:28:24.357', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '19309', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7269', 'Tags': '<algorithms><greedy-algorithms><spanning-trees>', 'CreationDate': '2013-12-26T17:49:25.153', 'Id': '19306'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have some algorithm and I wonder whether there is a way to optimize it beyond the naive approach.</p>\n\n<p>Basically I have time steps and the current time is $t_c$. At any given time step, a couple of "ranges" are active from now up to some given time $t_i$ (i.e. $\\forall t\\leq t_i: r_i\\in R(t)$).</p>\n\n<pre><code>   now       -time-&gt;\n   v\nr1 ****\nr2 ***************\nr3 *******\nr4 *****************************\nr5 ************\n</code></pre>\n\n<p>At each time $t$ I have to calculate some <em>expensive</em> function of all currently active ranges $f(\\{r|r\\in R(t)\\})$ (in the diagram I\'d have to calculated $f(r_1, r_2, r_3, r_4, r_5)$ now and given nothing changes I need $f(r_2, r_4, r_5)$ 9 steps later). Afterwards <em>new</em> ranges are generated and the time moves one step further. Therefore the existing ranges slowly change. Of course, past ranges are irrelevant. That\'s it.</p>\n\n<pre><code>    now      -time-&gt;\n    v\nr1 ****\nr2 ***************\nr3 *******\nr4 *****************************\nr5 ************\nr6 **********\n</code></pre>\n\n<p>My function is associative and cumutative, which means I could make use of any cached function results on subsets of the currently active set. It also time-scales with the number of arguments. Therefore given $f(a,b,c)$ I can calculate $f(d,a,b,c,e)$ in only two calculations rather than five.</p>\n\n<p>Can you see any way to optimize this which is faster than calculating the function at each time for each active range?</p>\n', 'ViewCount': '47', 'Title': 'Any way to optimize this with subset caching?', 'LastActivityDate': '2013-12-29T01:38:34.720', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '19348', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12124', 'Tags': '<algorithms>', 'CreationDate': '2013-12-28T16:00:42.080', 'FavoriteCount': '1', 'Id': '19342'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is a perhaps naive question that has been tingling me: Is there a O($\\sqrt[3]{n}$) asymptotic lower bound for addressing arbitrarily large memory randomly? My cause of belief is that the shortest path to any memory stored physically must be through three-dimensional space, and the diagonal here must have some minimal length.</p>\n\n<p>For example, when sorting arbitrarily many elements, then addressing these elements must eventually cost something proportional to the distance, and even if you have high-speed cable between every single point in some space, it seems as if there is a geometric limit bounded at higher than O(n lg n).</p>\n\n<p>What is wrong with my argument?</p>\n', 'ViewCount': '119', 'Title': '$O(\\sqrt[3]{n})$ lower bound on random-access memory?', 'LastEditorUserId': '6890', 'LastActivityDate': '2014-01-01T08:22:52.070', 'LastEditDate': '2013-12-30T10:21:14.250', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '19370', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9213', 'Tags': '<algorithms>', 'CreationDate': '2013-12-29T18:42:30.607', 'Id': '19367'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question is inspired by <a href="http://cs.stackexchange.com/q/19250/755">Constructing inequivalent binary matrices</a>.</p>\n\n<p>Define the equivalence relation $\\sim$ as follows: If $M,N$ are two $8\\times 8$ binary matrices (all elements are $0$ or $1$), say that $M \\sim N$ if you can transform $M$ into $N$ by a sequence of moves, where each move picks some pair $(i,j)$ and swaps rows $i$ and $j$ and then swaps columns $i$ and $j$.  For example,\n\\begin{equation}\n\\left( \\begin{array}{ccc}\n0 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 1 \\\\\n1 &amp; 0 &amp; 0 \\end{array} \\right) \\sim\n\\left( \\begin{array}{ccc}\n1 &amp; 0 &amp; 1 \\\\\n0 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 \\end{array} \\right).\n\\end{equation}\nThis equivalence relation induces a set of equivalence classes.</p>\n\n<p>Is there a way to define a canonical representative for each equivalence class, so that given any matrix $M$, we can efficiently compute the canonical representative $M^*$ corresponding to the equivalence class containing $M$?  I\'m hoping for a simple and efficient algorithm to compute the canonical representative.</p>\n\n<p>For instance, one way to define a canonical representative for matrix $M$ would be as follows: among all matrices $N$ that are equivalent to $M$, choose the one that is lexicographically first.  However, I don\'t know of any fast way to compute the canonical representative corresponding to a given matrix $M$.  (One could enumerate all matrices that are equivalent to $M$ by trying all $8!$ possible permutations, and then check which one is lexicographically first, but this is inefficient: it requires $8! \\approx 2^{15.3}$ steps of computation, which is too much.)  Is there a better approach?</p>\n\n<p>Alternatively, is there a way to define a canonical representative for each equivalence class, so that we can quickly test any given matrix $M$ to determine whether it is in canonical form?  (i.e., there is an efficient algorithm to check this)</p>\n\n<p>A good answer to this question might help solve <a href="http://cs.stackexchange.com/q/19250/755">Constructing inequivalent binary matrices</a>.</p>\n', 'ViewCount': '39', 'Title': 'A canonical representative, for this equivalence relation on matrices', 'LastActivityDate': '2013-12-30T17:44:51.860', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><matrices><permutations>', 'CreationDate': '2013-12-30T06:08:00.250', 'FavoriteCount': '2', 'Id': '19371'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am reading a unpublished paper describing an algorithm. In one step of the algorithm, there is a bipartite graph $G(X,Y,E)$, where $X=\\{1,...,n\\}$.</p>\n\n<p>For every subset $X' \\subseteq X$, they define</p>\n\n<p>$$f(X')=\\bigcup_{i \\in X'} \\{j \\in Y| (i,j)\\in E\\}.$$</p>\n\n<p>In other words, $f(X')$ is the set of neighbors of vertices in $X'$.</p>\n\n<p>Then they define:</p>\n\n<p>$$ X^+ = \\arg \\max_{X' \\subseteq \\{2,3,...,n\\}\\ s.t.\\ |X'|\\geq|f(X')|}\\{|X'|\\}$$</p>\n\n<p>i.e., $X^+$ is a largest subset of $\\{2,3,...,n\\}$ such that $X^+$ is at least as large as the set of its neighbors in $Y$ (i.e., the largest subset such that $|X^+| \\ge |f(X^+)|$).</p>\n\n<p>And then they do some stuff with this $X^+$.</p>\n\n<p>MY QUESTION IS: Can this $X^+$ be found in polynomial time? </p>\n\n<p>The authors do not prove that it can, but this is implied by the paper (otherwise the algorithm itself cannot be polynomial). Maybe it is so obvious that only I haven't seen this?</p>\n\n<p>EDIT: The following similar problem can be solved easily:</p>\n\n<pre><code>Find the largest set of vertices in X with less than |Y| neighbours.\n</code></pre>\n\n<p>Solution: find a $y \\in Y$ with a minimal number of neighbours in $X$. Return the set that includes all vertices in $X$ except $y$'s neighbours.</p>\n\n<p>What about the following similar problem?</p>\n\n<pre><code>Find the largest set of vertices in X with less than |Y|/2 neighbours.\n</code></pre>\n", 'ViewCount': '126', 'Title': 'Largest set of vertices that is larger than its set of neighbors', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-01-01T15:40:34.427', 'LastEditDate': '2014-01-01T15:40:34.427', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><graph-theory><time-complexity><runtime-analysis>', 'CreationDate': '2013-12-30T14:31:23.103', 'Id': '19377'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>what is the reason for the correctness proof of Prim's Algorithm for the undirected case cannot carry over to the directed case?</p>\n\n<p>Is it because of after any number of steps, $S$ might not be in a sub tree of an MST since it depends upon the direction of the edge of the directed graph, unlike the undirected one?</p>\n", 'ViewCount': '111', 'ClosedDate': '2014-01-05T17:29:58.513', 'Title': "Proof of Correctness of Prim's algorithm", 'LastEditorUserId': '7269', 'LastActivityDate': '2013-12-31T16:22:53.543', 'LastEditDate': '2013-12-31T13:52:39.337', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7269', 'Tags': '<graph-theory><correctness-proof><greedy-algorithms><spanning-trees>', 'CreationDate': '2013-12-31T12:55:44.063', 'Id': '19405'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a list of webpages and I must download them frequently, each webpage got a different download frequency. Based on this frequency we group the webpages in 5 groups:</p>\n\n<pre><code>Items in group 1 are downloaded once per 1 hour\nitems in group 2 once per 2 hours\nitems in group 3 once per 3 hours\nitems in group 4 once per 12 hours\nitems in group 5 once per 24 hours\n</code></pre>\n\n<p>This means, we must download all the group 1 webpages in 1 hour, all the group 2 in 2 hours etc.</p>\n\n<p>I am trying to make an algorithm. As input, I have:</p>\n\n<p>a) <code>DATA_ARR</code> = one array with 5 numbers. Each number represents the number of items in this group.</p>\n\n<p>b) <code>TIME_ARR</code> = one array with 5 numbers (1, 2, 3, 12, 24) representing how often the items will be downloaded.</p>\n\n<p>b) <code>X</code> = the total number of webpages to download per hour. This is calculated using items_in_group/download_frequently and rounded upwards. <code>If we have 15 items in group 5, and 3 items in group 4, this will be 15/24 + 3/12 = 0.875 and rounded is 1.</code></p>\n\n<p>Every hour my program must download at max <code>X</code> sites. I expect the algorithm to output something like:</p>\n\n<pre><code>Hour 1: A1 B0 C4 D5\nHour 2: A2 B1 C2 D2\n...\n</code></pre>\n\n<p>A1 = 2nd item of 1st group<br/>\nC0 = 1st item of 3rd group</p>\n\n<p>My algorithm must be as efficient as possible. This means that I should never download items more often than the update frequency of their group (unless I have absolutely no other choice)(see example)</p>\n\n<hr>\n\n<p><strong>Example:</strong></p>\n\n<pre><code>group 1: 0 items | once per 1 hour\ngroup 2: 3 items | once per 2 hours\ngroup 3: 4 items | once per 3 hours\ngroup 4: 0 items | once per 12 hours\ngroup 5: 0 items | once per 24 hours\n</code></pre>\n\n<p>We calculate the number of items we can take per hour: <code>3/2+4/3 = 2.83. We round this upwards and it\'s 3.</code></p>\n\n<p>Using pencil and paper, we can found the following solution:</p>\n\n<pre><code>Hour 1: B0 C0 B1\nHour 2: C1 B2 C2\nHour 3: B0 C3 B1\nHour 4: C0 B2 C2\nHour 5: B0 C1 B1\nHour 6: C3 B2 C2\nand repeat the above.\n</code></pre>\n\n<p>We take <code>C0</code>, <code>C1</code> and <code>C3</code> once every 3 hours. We also take <code>B0</code>, <code>B1</code> and <code>B2</code> once every 2 hours.</p>\n\n<p>We take <code>C2</code> once every 2 hours. This is more often than needed, wasting bandwidth, but if there is no other way around (like here), we will take it every 2 hours. My question on SO <a href="http://stackoverflow.com/questions/20857363/is-there-a-solution-for-this">here</a> and Math Overflow <a href="http://math.stackexchange.com/questions/623173/is-there-a-solution-for-this">here</a> made me understand that sometimes you are forced to download items more often than the absolutely minimum.</p>\n\n<hr>\n\n<p><strong>Question: Please, explain to me, how to design an algorithm able to download the items, while using the absolute minimum number of downloads?</strong> Brute force is <strong>NOT</strong> a solution and the algorithm must be efficient CPU wise because the number of elements can be huge.</p>\n', 'ViewCount': '131', 'Title': 'Scheduling Algorithm with limitations', 'LastEditorUserId': '12497', 'LastActivityDate': '2014-01-01T20:20:16.847', 'LastEditDate': '2013-12-31T17:34:43.363', 'AnswerCount': '1', 'CommentCount': '10', 'AcceptedAnswerId': '19422', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12497', 'Tags': '<algorithms>', 'CreationDate': '2013-12-31T14:47:36.400', 'Id': '19411'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '127', 'Title': 'NP-complete decision problems - how close can we come to a solution?', 'LastEditDate': '2014-01-02T15:28:30.023', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '2', 'Body': '<p>After we prove that a certain <strong>optimization</strong> problem is NP-hard, the natural next step is to look for a polynomial algorithm that comes close to the optimal solution - preferrably with a constant approximation factor.</p>\n\n<p>After we prove that a certain <strong>decision</strong> problem is NP-complete, what is the natural next step? Obviously we cannot "approximate" a boolean value...</p>\n\n<p>My guess is that, the next step is to look for a randomized algorithm that returns the correct solution with a high probability. Is this correct?</p>\n\n<p>If so, what probability of being correct can we expect to get from such a randomized algorithm?</p>\n\n<p>As far as I understand from Wikipedia, <a href="https://en.wikipedia.org/wiki/PP_%28complexity%29" rel="nofollow">PP contains NP</a>. This means that, if the problem is in NP, it should be easy to write an algorithm that is correct more than $0.5$ of the times.</p>\n\n<p>However, <a href="https://en.wikipedia.org/wiki/Bounded-error_probabilistic_polynomial" rel="nofollow">it is not known whether BPP contains NP</a>. This means that, it may be difficult (if not impossible) to write an algorithm that is correct more than $0.5+\\epsilon$ of the times, for every positive $\\epsilon$ independent of the size of input.</p>\n\n<p>Did I understand correctly?</p>\n', 'Tags': '<np-complete><approximation><randomized-algorithms>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-01-03T01:18:29.533', 'CommentCount': '2', 'AcceptedAnswerId': '19419', 'CreationDate': '2013-12-31T16:03:52.760', 'Id': '19412'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to come up with a good algorithm for the following decision problem:</p>\n\n<p>Let $G=(V,A)$ be a directed graph and let $s,t \\in V$. Are there at-least 2 non-intersecting paths from $s$ to $t$?</p>\n\n<p>By non-intersecting I mean that if $P_1$ and $P_2$ are non-intersecting paths from $s$ to $t$ then $\\forall e\\in P_1\\in A, e\\notin P_2$ (can share vertices but not edges).</p>\n\n<p>I thought of the following algorithm:</p>\n\n<ol>\n<li>Find the shortest path $P\\in A$ in $G$ from $s$ to $t$. if no such path exist, return false;</li>\n<li>Find the shortest path $P'\\in A-P$ in $G$ from $s$ to $t$. if no such path exists, return false; else return true.</li>\n</ol>\n\n<p>I didn't find a contradicting example but so far I was unable to prove its correctness.</p>\n\n<p>Ideas about proof approaches for this algorithm or suggesting other algorithms is welcome.</p>\n", 'ViewCount': '69', 'Title': 'Non intersecting paths in a graph', 'LastEditorUserId': '10438', 'LastActivityDate': '2014-01-01T16:49:59.463', 'LastEditDate': '2013-12-31T16:24:44.257', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19418', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10438', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2013-12-31T16:06:10.790', 'Id': '19413'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to apply <a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a> to the <a href="http://projecteuler.net/problem=83" rel="nofollow">Problem 83</a> on projecteuler.net. The problem reads:</p>\n\n<blockquote>\n  <p>In the 5 by 5 matrix below, the minimal path sum from the top left to\n  the bottom right, by moving left, right, up, and down, is indicated in\n  bold red and is equal to 2297.</p>\n\n<pre><code>131   673   234   103    18\n201    96   342   965   150\n630   803   746   422   111\n537   699   497   121   956\n805   732   524    37   331\n</code></pre>\n  \n  <p>Find the minimal path sum, in matrix.txt (right click and \'Save\n  Link/Target As...\'), a 31K text file containing a 80 by 80 matrix,\n  from the top left to the bottom right by moving left, right, up, and\n  down.</p>\n</blockquote>\n\n<p>I wonder if the original algorithm can be simplified due to the fact that the graph is represented as a matrix?</p>\n\n<p>In particular, I\'ve noticed that every edge in the graph is only relaxed once (i.e. the node\'s distance is changed only once from infinity to some value). Can I rely on this fact in my code?</p>\n', 'ViewCount': '247', 'Title': "How to optimize Dijkstra's algorithm for a grid graph?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-06T11:39:11.420', 'LastEditDate': '2014-01-06T11:39:11.420', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'OwnerDisplayName': 'user12525', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2014-01-02T02:19:25.053', 'Id': '19446'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to organise a large reunion though facebook. Over the years we've all drifted apart, and so there isn't anyone who is still facebook friends with everyone.</p>\n\n<p>You need to be facebook friends to invite someone to the facebook event and I have invited all the people that I can. I also know that each of the uninvited people has at least one friend amongst the invited people, and by checking for 'mutual friends' I can know who they are.</p>\n\n<p>In order to get everyone invited to the reunion I'm going to ask some of the invited people to invite the uninvited people.</p>\n\n<p>My question is, does anyone happen to know an algorithm to find the minimum number of invited people that are required to message all of the uninvited people? I don't need anything super detailed, just something to point me in the right direction.</p>\n", 'ViewCount': '57', 'Title': 'Minimum number of people required to invite everyone to a reunion', 'LastActivityDate': '2014-01-03T04:23:46.813', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12557', 'Tags': '<algorithms>', 'CreationDate': '2014-01-03T03:57:50.800', 'Id': '19474'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<pre><code>i=n; \nwhile(i&gt;0) {\n  k=1;\n  for(j=1;j&lt;=n:j+=k)\n    k++;\n  i=i/2;\n}\n</code></pre>\n\n<p>The while loop has the complexity of $\\lg(n)$ the j value of inner loop runs 1,3,6,10,15...\nincrease like 2,3,4,5,...</p>\n\n<p>But how to find the overall complexity ?</p>\n', 'ViewCount': '136', 'ClosedDate': '2014-01-03T15:56:24.630', 'Title': 'How to find the asymptotic runtime of these nested loops?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-04T19:36:17.830', 'LastEditDate': '2014-01-04T19:36:17.830', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-03T10:45:35.863', 'Id': '19480'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem whose solution can be written as a binary string with a given length $N$, where $N$ is a given parameter. Standard GA works well on this problem. From runs of small values $N$, I found that optimal solutions are binary strings that contain only 01 or 011. For example, the optimal solution for $N=16$ is <code>0101011010101101</code>. So I think it would be a good idea to only search the solutions space where all solutions/chromosomes only contain 01 or 011. But clearly the standard GA cannot restrict the search in the subspace I desire. One mutation or one crossover will make the new solution go into the larger solution space.</p>\n\n<p>My question is: is there a way to adapt the standard GA to restrict its solution space to one where the chromosomes contain only 01 or 011?</p>\n', 'ViewCount': '99', 'Title': 'A genetic algorithm modified for a specific problem', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-30T15:55:12.573', 'LastEditDate': '2014-01-30T15:55:12.573', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12564', 'Tags': '<algorithms><optimization><genetic-algorithms>', 'CreationDate': '2014-01-03T12:57:36.797', 'Id': '19484'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In the last 2 paragraphs of the paper about Hopcroft\u2013Karp algorithm to find the maximum cardinality matching in bipartite graph:</p>\n\n<blockquote>\n  <p><a href="https://dl.dropboxusercontent.com/u/64823035/04569670.pdf" rel="nofollow">https://dl.dropboxusercontent.com/u/64823035/04569670.pdf</a></p>\n  \n  <p>The execution time of a phase is O(m+n), where m is the number of\n  edges in G, and n is the number of vertices. Hence the execution time\n  of the entire algorithm is O((m+n)s), where s is the cardinality of a\n  maximum matching.</p>\n  \n  <p>If G has n vertices then m &lt;= n^2 / 4 and s &lt; n / 2 so that the\n  execution time is bounded by O(n^(5/2)).</p>\n</blockquote>\n\n<p>I don\'t understand given:</p>\n\n<pre><code>m &lt;= n^2 / 4\ns &lt;= n / 2\n</code></pre>\n\n<p>why they concluded:</p>\n\n<pre><code>O((m+n)s) = O(n^(5/2))\n</code></pre>\n\n<p>Shouldn\'t it be:</p>\n\n<pre><code>O((m+n)s) = O(n^3)\n</code></pre>\n\n<p>Any idea?</p>\n', 'ViewCount': '78', 'Title': u'Hopcroft\u2013Karp algorithm time complexity', 'LastEditorUserId': '12572', 'LastActivityDate': '2014-01-03T14:30:49.710', 'LastEditDate': '2014-01-03T14:30:49.710', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12572', 'Tags': '<algorithms><graph-theory><graphs><graph-traversal><bipartite-matching>', 'CreationDate': '2014-01-03T13:58:38.513', 'Id': '19486'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The following array occupies 10000 slots in memory:</p>\n\n<pre><code>a = [0,1,2,3,4,5,6,7,8,9,10,...,10000]\n</code></pre>\n\n<p>But one could easily represent the same array as:</p>\n\n<pre><code>a = {len:10000, get: \u03bb idx -&gt; idx}\n</code></pre>\n\n<p>Which is much more compact. Similarly, there are several arrays that can be represented compactly:</p>\n\n<pre><code>a = {a:1000, get: \u03bb idx -&gt; idx * 2}\nIs a description for [0,2,4,6,8,10,...,2000]\n\na = {a:1000, get \u03bb idx -&gt; idx ^ 2}\nIs a description for [0,1,2,4,9,...1000000]\n\nAnd so on...\n</code></pre>\n\n<p>Providing so many arrays can be represented in much shorter ways than storing each element on memory, I ask:</p>\n\n<ol>\n<li>Is there a name for this phenomena?</li>\n<li>Is there a way to find the minimal representation for a specific array?</li>\n<li>Considering this probably depends on the language of description (in this case, I used an imaginary programming language with functions, objects and math operators). Is there a specific language that is optimal for finding that kind of minimal description for objects?</li>\n</ol>\n', 'ViewCount': '91', 'Title': 'How to find the minimal description for an array?', 'LastActivityDate': '2014-01-28T15:05:22.407', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><complexity-theory><formal-languages><programming-languages><data-compression>', 'CreationDate': '2014-01-04T23:53:15.990', 'Id': '19501'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of elements U = {1, 2, .... , n} and a set S of k sets whose union form the whole universe. Each of these sets is associated with a cost.</p>\n\n<p>I have a fixed number of colors, C = {1 , 2, ... , m}. Some of the sets mentioned above interfere with each other. I cannot assign same color to both those sets together.</p>\n\n<p>I want to pick the sets and color them from my available color list in the following way:</p>\n\n<p>**Objective: Minimize the total cost of the selected sets</p>\n\n<p>Constraints:</p>\n\n<ol>\n<li><p>All elements of the universe are covered</p></li>\n<li><p>No two sets that interfere with each other is assigned the same color**</p></li>\n</ol>\n\n<p>If the second constraint, i.e., coloring constraint, is taken out, the problem reduces to standard weighted set covering problem. I can solve that using a greedy manner. For example, greedy unweighted set covering will work in the following way: -- 1. pick the set with the highest number of elements at first, 2. Remove that set and the associated elements from the universe, 3. Repeat step 1 until all elements of the universe are covered.</p>\n\n<p>But the coloring constraint in the presence of interference among sets and a fixed number of colors complicates the issue.</p>\n\n<p>For example, let\'s assume,</p>\n\n<p>U = {1, 2, 3, 4, 5}.</p>\n\n<p>There are three sets, {1, 2, 3}; {2, 3, 4, 5} ; {4, 5}.</p>\n\n<p>Assume {2, 3, 4, 5} interferes with both {1, 2, 3} and {4, 5}. {1, 2, 3} and {4, 5} do not interfere with each other. Assume that there is only one color in the system.</p>\n\n<p>A standard greedy unweighted set coloring solution will pick {2, 3, 4, 5} at first and {1, 2, 3} in the second round. But that is an infeasible solution to my problem since they interfere with each other and I have only one color in my system. A feasible solution will be the selection of {1, 2, 3} and {4, 5}.</p>\n\n<p>I wonder how I can minimize the total cost while meeting the color constraint. Any hint on the unweighted version of the problem (where all sets have equal cost) will be very helpful, too.</p>\n\n<p>Thanks,</p>\n\n<p>Nazmul</p>\n\n<p>Additional information: The coloring of the sets and the interference can be understood by my application scenario. </p>\n\n<p>I am looking at a wireless cell. I have a set of frequencies, possible base station locations and their associated users. Each set is associated with a station and it shows the set of users that the base station can serve. </p>\n\n<p>"Interference" from one set to the other means that the users\' signals of one set reaches the other set\'s base station. In a wireless setting, this interference is not symmetric. But assumption of symmetry is OK in the algorithm because if set A interferes with set B, A &amp; B both should get different colors if they are selected. </p>\n\n<p>The interference among sets is not transitive. Set A may interfere with set B (A\'s users may interfere with B\'s base station) and set B may interfere with set C (B\'s users may interfere with C\' base station) but that does not mean that set A will interfere with set C.</p>\n\n<p>The current example is showing that two sets interfere if they intersect. This is just a coincidence. I will have a pre-generated look up table that shows which set interferes with which set before the algorithm starts. This pre-generated table will be an input to the optimization problem</p>\n', 'ViewCount': '76', 'Title': 'Weighted Set covering problem with a fixed number of colors', 'LastEditorUserId': '12596', 'LastActivityDate': '2014-03-06T18:52:06.767', 'LastEditDate': '2014-01-05T07:54:04.203', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '19516', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12596', 'Tags': '<algorithms><graphs><optimization>', 'CreationDate': '2014-01-05T01:30:35.303', 'Id': '19503'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How do I iterate over all the $k$-element subsets of $\\{1,2,\\dots, n\\}$ by switching one element at a time?</p>\n\n<pre><code>123\n134\n234\n124\n145\n245\n345\n135\n235\n125\n</code></pre>\n\n<p>This comes from Ch2 of  <a href="http://www.math.upenn.edu/~wilf/website/CombinatorialAlgorithms.pdf" rel="nofollow">Combinatorial Algorithms</a> by Nienhuis and Wilf.</p>\n\n<p>Equivalently I am asking for a Hamiltonian circuit on the <a href="http://en.wikipedia.org/wiki/Johnson_graph" rel="nofollow">Johnson graph</a> of $k$ element subsets of a set of $n$ elements connected if their intersection has $k-1$ elements. </p>\n\n<hr>\n\n<p>I am trying to understand how the equation $$A(n,k) = A(n-1,k), \\overline{ A(n-1,k-1)}\\otimes \\{n\\}$$ from Nienhuis-Wilf leads to a type of "gray code" for subsets.  In fact, it is the gray code when you restruct to $k$-element sets. </p>\n\n<p>Here, $A(n,k)$ is an ordering, looping over the $k$-element subsets of $\\{1,2,\\dots, n\\}$.   The notation $\\overline{ A(n-1,k-1)}\\otimes \\{n\\}$ means we should list the $k-1$-element substs of $\\{1,2,\\dots, n-1\\}$ and append the element $n$ to each element of that list.</p>\n\n<p>This equation can also be thought of a set theoretic version of the binomial coefficient identity</p>\n\n<p>$$ \\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}$$</p>\n\n<p>Using this formation I came up a means of listing all the subsets in order. </p>\n\n<p>Here <code>gc(n,k)</code> is returning an array of $k$-element arrays, enumerating the $k$-element subsets of $\\{1,2,\\dots, n\\}$.</p>\n\n\n\n<pre><code>def gc(n,k):\n    if(k==1):\n        return [[i+1] for i in range(n)]\n    elif(n == 0):\n        return []\n    else:\n        L = [ x+ [n] for x in gc(n-1,k-1)]\n        return gc(n-1,k)+ L[::-1]\n</code></pre>\n\n<p>How do I find the predecessor or successor of a given subset without generating all the subsets?  I wrote some python code for this, which is different from what is in the textbook.  It still doesn\'t return the correct answer.</p>\n\n<pre><code>def S(n,k,a):\n    if k == 1:\n        return [(a[0] + 1)%n]\n    elif(a[-1] == n-1):\n        return P(n-1,k-1, a[:-1]) + [n-1]\n    else:\n        return S(n-1,k,a)\n\ndef P(n,k,a):\n    if k == 1:\n        return [(a[0] - 1)%n]\n    elif(a[-1] == n-1):\n        return S(n-1,k-1, a[:-1]) + [n-1]\n    else:\n        return P(n-1,k,a)\n</code></pre>\n\n<p>This looks pretty close to the recursion in Nienhuis-Wilf but I would like to understand where I am going wrong in my implementation.</p>\n', 'ViewCount': '99', 'Title': 'iterating over subsets by switching one element at a time', 'LastEditorUserId': '3131', 'LastActivityDate': '2014-01-08T21:29:35.157', 'LastEditDate': '2014-01-05T18:08:51.710', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><graph-theory><finite-automata>', 'CreationDate': '2014-01-05T03:35:26.093', 'Id': '19504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I just read the <a href="https://en.wikipedia.org/wiki/Simon%27s_problem" rel="nofollow">Wiki article for Simon\'s Problem</a> but I don\'t fully understand it because I don\'t follow the symbolic notation used to describe functions (I am not a computer scientist). </p>\n\n<p>Can someone just briefly explain it in simple language, like is it just XOR\'ing two binary strings and trying to isolate each string? Also, if this problem is known to take exponential time to solve, why isn\'t it being used as a cryptographic primitive?</p>\n', 'ViewCount': '105', 'Title': "Simple explanation of Simon's Problem", 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-06T14:47:26.853', 'LastEditDate': '2014-01-06T06:21:48.723', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '19521', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8367', 'Tags': '<algorithms><quantum-computing>', 'CreationDate': '2014-01-05T23:55:30.393', 'Id': '19519'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have read about the hash function MD5. I found an online calculator which counts the value of MD5 for different data. But the MD5 function returns as result strings like <code>735b90b4568125ed6c3f678819b6e058</code>, so I can't apply any numerical operation to this value, e.g. <code>mod</code>.</p>\n\n<p>How can I transform values like these into numbers?</p>\n", 'ViewCount': '51', 'Title': 'Transform values of hash functions like MD5 into numbers', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-06T06:17:29.300', 'LastEditDate': '2014-01-06T06:14:46.820', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19523', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12038', 'Tags': '<algorithms><hash>', 'CreationDate': '2014-01-06T00:15:57.237', 'Id': '19520'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Foldl and folr are 2 very important functions for FP and Haskell, but I have never heard much about the unsided fold:</p>\n\n<pre><code>fold f [a,b,c,d] = (f (f a b) (f c d))\n</code></pre>\n\n<p>That is, a fold that operates on binary associative functions (so the order of application doesn't matter). If I recall correctly, this is very common in databases as it can be parallelized. So, about it, I ask:</p>\n\n<ol>\n<li>Is it, like foldr, universal?</li>\n<li>Like foldr, can you define every important function using it?</li>\n<li>Is there a fusion rule for it, similar to those for foldr/build and unfoldr/destroy?</li>\n<li>Why is it barely mentioned?</li>\n<li>Any consideration worth mentioning?</li>\n</ol>\n", 'ViewCount': '29', 'Title': 'What are the properties of the unsided fold?', 'LastActivityDate': '2014-01-06T02:07:05.993', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><functional-programming>', 'CreationDate': '2014-01-06T02:07:05.993', 'Id': '19524'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I would like to be able to represent circles in x-y coordinates.</p>\n\n<p>Each circle contains an x and y coordinates and radius in double data type.</p>\n\n<p>My goal is to compare circles with each other whether they are partially or completely overlapping.</p>\n\n<p>I am looking for efficient ideas. Honestly the only idea that comes to my mind is draw a line(let's say l1) from x1,y1 to x2,y2 and the length of this line is larger than addition of r1 and r2 then it does not overlap, if r1+r2 =&lt; l1 then it overlaps, but I don't know how to find whether it is completely overlapping or partially. Also this wouldn't work for cases where I am combining more than one circle.</p>\n", 'ViewCount': '74', 'Title': 'How to represent circles in x-y coordinates', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-07T09:54:25.833', 'LastEditDate': '2014-01-07T09:54:25.833', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><computational-geometry><modelling>', 'CreationDate': '2014-01-06T03:30:29.157', 'Id': '19525'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Conc lists are similar to cons lists. In contrast to folds, mapreduce is the main "iterating" operation used on it. Composed mapreduces produce intermediate lists. Is there a fusion law for them, similar to those for foldr/build and unfoldr/destroy?</p>\n', 'ViewCount': '15', 'Title': 'Is there a fusion law for the mapreduce operation used on conc-lists?', 'LastActivityDate': '2014-01-06T04:00:14.660', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><data-structures><programming-languages><functional-programming>', 'CreationDate': '2014-01-06T04:00:14.660', 'Id': '19526'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have the following problem:</p>\n\n<blockquote>\n  <p>In a 2D space with polygonal obstacles, find the shortest path between two given point.</p>\n</blockquote>\n\n<p>Without additional constraints, we can reduce it to a graph problem by constructing a visibility graph and then solve it by searching.</p>\n\n<p>If the following constraints are added, how can we solve it?</p>\n\n<p>Let's define a path as a set of line segments.</p>\n\n<ol>\n<li>Each segment must be longer than a given value.</li>\n<li>Each segment must be horizontal or vertical. (simplified version)<br/>\nEach segment must be parallel or perpendicular to other segments. (general version)</li>\n</ol>\n\n<p><strong>Clarification</strong></p>\n\n<ul>\n<li>The problem is in continuous 2D space.</li>\n<li>To simply the problem, the direction of the first and the last segments is given.</li>\n</ul>\n\n<p><strong>Application</strong></p>\n\n<p>This can be considered as a routing problem. I am going to use the algorithm for MEP services routing (e.g. duct, pipe, cable tray).</p>\n\n<p>We need a connector to connect two services when the direction is changed.</p>\n\n<ul>\n<li>As the connector consumes some space, we have constraint (1).</li>\n<li>As we only have limited choices of the connector (90\xb0, 60\xb0, 45\xb0, 30\xb0), we have constraint (2). To simplified the problem, I only use connector for right angle.</li>\n</ul>\n", 'ViewCount': '99', 'Title': 'Any algorithm for finding Euclidean shortest path with specific constraints in 2D?', 'LastEditorUserId': '12662', 'LastActivityDate': '2014-01-09T01:47:04.127', 'LastEditDate': '2014-01-09T01:47:04.127', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12662', 'Tags': '<algorithms><computational-geometry><shortest-path>', 'CreationDate': '2014-01-08T06:21:37.013', 'Id': '19573'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Assume having a graph $G_{variables}=(V,U)$ where $V=\\{v_1,v_2,\u2026,v_n\\}$ is a set of variables; each variable $v_i\\in V$ is associated with a set of possible values (it's domain) $dom(v_i)$. </p>\n\n<p>Let $P$ be a search problem (i.e reachability problem) over graph $G=(O,E)$ where $O$ is the cartesian product of the variables domains. Let $T$ be a junction tree resulted from $G_{variables}$. $P$ can be also solved through searching every clique in $T$. I am looking for keywords/examples of such problems. $G_{variables}$ preferably to be DAG.  </p>\n", 'ViewCount': '30', 'Title': 'Search problems that can also be solved by junction trees and searching cliques', 'LastEditorUserId': '4598', 'LastActivityDate': '2014-01-10T04:15:00.893', 'LastEditDate': '2014-01-10T04:15:00.893', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<graph-theory><reference-request><search-algorithms><search-problem>', 'CreationDate': '2014-01-09T17:48:23.083', 'Id': '19602'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="https://github.com/gurgeh/CodeSpace">This Github repo</a> hosts a very cool project where the creator is able to, give an integer sequence, predict the most likely next values by searching the smallest/simplest programs that output that integer sequence. I was trying to approach the same idea using lambda-calculus instead of a stack-based language, but I was stuck on the enumeration of valid programs on LC\'s grammar.</p>\n\n<p>Anyway, what is the field studying that kind of idea and how can I grasp the current state-of-art?</p>\n', 'ViewCount': '94', 'Title': 'What is the field studying the search and generation of computer programs?', 'LastActivityDate': '2014-01-12T11:12:54.350', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><complexity-theory><formal-languages><computability><regular-languages>', 'CreationDate': '2014-01-09T20:37:06.207', 'Id': '19605'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '317', 'Title': 'Which algorithms can not be parallelized?', 'LastEditDate': '2014-01-11T10:29:16.157', 'AnswerCount': '3', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '11108', 'FavoriteCount': '5', 'Body': "<p>Is there any algorithm which is very difficult to parallelize or the research is still active?</p>\n\n<p>I wanted to know about any algorithm or any research field in parallel computing.</p>\n\n<p>Anything, I searched for, has a 'parallel' implementation done. Just want to do some study on any unexplored parallel computing field.</p>\n", 'Tags': '<algorithms><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-12T18:17:54.130', 'CommentCount': '4', 'AcceptedAnswerId': '19674', 'CreationDate': '2014-01-11T02:59:29.227', 'Id': '19643'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a simple directed graph $G(V,E)$ that has a source $s$ and sink $t$. Each edge $e$ of $G$ has positive integer capacity $c(e)$ and positive integer cost $a(e)$. I am trying to find the minimum cost maximum flow from $s$ to $t$ using the <a href="http://wwwhome.math.utwente.nl/~uetzm/do/DO_Lecture4.pdf" rel="nofollow">well-known Dijkstra potential method for finding augmenting paths</a>. It goes something like this:</p>\n\n<pre><code>Initialize all edge flows to 0.\nInitialize all potentials pi[v] to 0.\nWhile there exists an augmenting path in G_f (the residual network):\n    Set the costs of all edges e = uv to be:\n        b(e) = a(e) + pi[u] - pi[v], if e exists in G or\n        b(e) = -a(e_reverse) + pi[u] - pi[v], where e_reverse = vu otherwise\n    # We are now assured all edges have nonnegative costs\n    Using Dijkstra method with costs b(e) in G_f:\n        Find the cheapest augmenting path from s to t\n        Calculate dist(v), the cost of cheapest path from s to v\n    Augment the cheapest path to t to current flow\n    Set pi[v] = pi[v] + dist(v) for all vertices v\nThe current flow gives the minimum cost maximum flow.\n</code></pre>\n\n<p>Obviously, if all costs $a(e) \\le a_{max}$ and all capacities $c(e) \\le c_{max}$, then there is a loose bound $|E|c_{max}a_{max}$ for cost of minimum cost maximum flow. However, the bound on the potentials $\\pi(v)$ and Dijkstra distances $dist(v)$ is not so obvious. In fact, judging by how it adds $dist(v)$ to $\\pi(v)$ each iteration, $\\pi(v)$ can possibly be multiplied by $|V|$ each iteration!</p>\n\n<p><strong>My question is</strong>, is there a way to calculate a non-exponential bound for $\\pi(v)$? If not, say all capacities and costs are at most $10^4$, $|V| = 200$, $|E| = 5000$. The minimum cost of the maximum flow is at most $5000 \\times 10^4 \\times 10^4 = 5 \\times 10^{11}$. But is it possible that $\\pi(v)$ and $dist(v)$ exceeds 64-bit integers? How do so many implementations not use Big Integers?</p>\n', 'ViewCount': '104', 'Title': 'Potential values of minimum cost maximum flow algorithm', 'LastEditorUserId': '7137', 'LastActivityDate': '2014-01-15T14:37:32.410', 'LastEditDate': '2014-01-15T14:37:32.410', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7137', 'Tags': '<algorithms><graph-theory><algorithm-analysis><asymptotics><network-flow>', 'CreationDate': '2014-01-11T04:25:09.883', 'Id': '19645'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading through Skiena\'s "The Algorithm Design Manual". He analyses the number of copy operations performed in a dynamic array.</p>\n\n<blockquote>\n  <p>"Suppose we start with an array of size 1, and double its size from $m$ to $2m$ each time we run out of space. This doubling process involves allocating a new contiguous array of size $2m$, copying the contents of the old array into the lower half of the new one. The apparent waste is in the recopying of the old contents, on each expansion."</p>\n</blockquote>\n\n<p>Skiena then asks:</p>\n\n<blockquote>\n  <p>"How many times might an element have to be recopied after a total of $n$ insertions?".</p>\n</blockquote>\n\n<p>He then gives a formula for calculating the total number of movements $M$, for $n$ insertions, which is</p>\n\n<p>$$\\mathbf{M}=\\sum_{i=1}^{\\log n} i\\cdot\\frac{n}{2^i}.$$</p>\n\n<p>His explanation is:</p>\n\n<blockquote>\n  <p>"Well, the first inserted element will have been recopied when the array expands after the first, second, fourth, eighth, ...insertions. It will take $\\log_{2}n$ doublings until the array gets to have n positions. However, most elements do not suffer much upheaval. Indeed, the $(n/2 + 1)$st through $n$th elements will move at most once and might never have to move at all."</p>\n</blockquote>\n\n<p>But I really don\'t understand how he draws the formula from that. Can anyone help me understand how he reaches it? Mainly what I can\'t understand is why he is multiplying $\\frac{n}{2^i}$ by $i$.</p>\n', 'ViewCount': '44', 'Title': 'How is this formula for dynamic array reached?', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-11T11:01:41.227', 'LastEditDate': '2014-01-11T11:01:41.227', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19650', 'Score': '1', 'OwnerDisplayName': 'patchwork', 'PostTypeId': '1', 'Tags': '<algorithms>', 'CreationDate': '2014-01-09T12:47:07.277', 'Id': '19649'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In directed graph, to find strongly connected components why do we have to transpose adjacency matrix (reverses the direction of all edges) if we could use reversed list of nodes by they finishing time and then traverse original graph.\nIn other words, we would find finish times of all vertices and start traversing from lowest finish time to greatest (by increasing finish time)? </p>\n\n<p>Additionally, if we do topological sorting on some DAG, and then reverse edges (transpose adjacency matrix) and do topological sorting again - should we get to equal arrays, just in reversed order?</p>\n\n<p>EDIT:\nAlgorithm description from other topic:\n<a href="http://cs.stackexchange.com/questions/11232/correctness-of-strongly-connected-components-algorithm-for-a-directed-graph?rq=1">Correctness of Strongly Connected Components algorithm for a directed graph</a></p>\n', 'ViewCount': '137', 'Title': u'Kosaraju\u2019s Algorithm - why transpose?', 'LastEditorUserId': '12752', 'LastActivityDate': '2014-04-07T20:21:21.567', 'LastEditDate': '2014-01-11T11:01:38.880', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12752', 'Tags': '<algorithms><graphs>', 'CreationDate': '2014-01-11T09:48:32.243', 'Id': '19652'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I am having a plane in N dimension. Th distance between 2 points (a1,a2,...,aN) and (b1,b2,...,bN) is max{|a1-b1|, |a2-b2|, ..., |aN-bN|}.</p>\n\n<p>I need to to know how many K-sets exist(here K-set refers to set of points whose distance between 2 points of set is K).But as there can be infinite number of these K-sets. Thus, we would only like to count the number of classes of K-sets, such that any two K-sets which belong to the same class are equivalent if they follow given conditions.Two K-sets X and Y are considered equivalent (and belong to the same class) if:</p>\n\n<pre><code>They contain the same number of points \nThere exists N integer numbers (t1, ..., tN) such that by translating each point of X  \n by the amount ti in dimension i (1\u2264i\u2264N) we obtain the set of points Y.\n</code></pre>\n\n<p>Let's consider N=2, K=4 and the following sets of points X={(1,2), (5,5), (4,3)} and Y={(2,5), (5,6), (6,8)}. Let's consider now the tuple (1,3). By translating each point of X by the amounts specified by this tuple we obtain the set {(2,5), (6,8), (5,6)}, which is exactly the set Y. Thus, the two sets X and Y are equivalent and belong to the same class.</p>\n\n<p>Example let say N=2 and K=1 .</p>\n\n<p>There are 9 classes of K-sets. One K-set from each class is given below:</p>\n\n<pre><code>{(0,0), (0,1)}\n{(0,0), (1,0)}\n{(0,0), (1,1)}\n{(0,1), (1,0)}\n{(0,0), (0,1), (1,0)}\n{(0,0), (0,1), (1,1)}\n{(0,0), (1,0), (1,1)}\n{(0,1), (1,0), (1,1)}\n{(0,0), (0,1), (1,0), (1,1)}\n</code></pre>\n\n<p>So answer here will be 9.</p>\n", 'ViewCount': '76', 'ClosedDate': '2014-02-12T09:15:23.047', 'Title': 'Number of K-sets', 'LastActivityDate': '2014-02-12T08:29:00.027', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12759', 'Tags': '<algorithms><computational-geometry><sets><number-theory>', 'CreationDate': '2014-01-11T15:36:25.203', 'Id': '19656'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Two strings $S$ and $T$ are said to be <em>conjugate</em> if there are two non-empty strings $A$ and $B$ such that $S = A+B$ and $T = B+A$ ($+$ is concatenation). How can I find if two strings are conjugate or not?</p>\n\n<p>Example: if <code>S="tokyo"</code> and <code>T="kyoto"</code>, then the pair $(S,T)$ is conjugate, because we can find <code>A="to"</code> and <code>B="kyo"</code>.</p>\n', 'ViewCount': '64', 'Title': 'Check if there exist A and B such that S=A+B and T=B+A', 'LastEditorUserId': '755', 'LastActivityDate': '2014-01-13T07:38:45.797', 'LastEditDate': '2014-01-13T07:38:45.797', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12759', 'Tags': '<algorithms><strings>', 'CreationDate': '2014-01-11T17:17:17.660', 'Id': '19657'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's have an array where first half are of value 50 and the second half 100. What would be the asymptotic performance when sorting using Quicksort.</p>\n\n<p>I think it it should be $O(n^2)$ as for array of same elements the complexity is $O(n^2)$ and this particular problem could be rewritten as sorting the first half + sorting the second hald $O(2*(\\frac{n}{2})^2 + n)$ which is still $O(n^2)$.</p>\n\n<p>But my schoolmates claim it should be $O(n log(n))$.. so which one is correct?</p>\n", 'ViewCount': '87', 'LastEditorDisplayName': 'user12779', 'Title': "Quicksort's asymptotic performance for array of [50,...,50,100,...100]", 'LastActivityDate': '2014-01-29T21:37:14.940', 'LastEditDate': '2014-01-12T18:30:54.990', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'user12779', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><performance><quicksort>', 'CreationDate': '2014-01-12T18:18:39.200', 'Id': '19675'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In a directed graph i want to call bfs on some of the vertices so that all of the vertices will be met.</p>\n\n<p>(in other words all of the other vertices are reachable from these chosen vertices.)</p>\n\n<p>I want to find the minimum number of such vertices.</p>\n\n<p>Actually this problem arises in social networks when we want to find the minimum number of people to which if we send a message then all of the network members will get that.(suppose that we know when someone gets the message he/she will send that to all of his/her followers.)</p>\n\n<p>Can anyone help?</p>\n', 'ViewCount': '158', 'Title': 'find the minimum number of vertices in a directed graph from which the other vertices are reachable', 'LastEditorUserId': '12820', 'LastActivityDate': '2014-04-05T15:47:19.383', 'LastEditDate': '2014-01-14T00:00:43.170', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12820', 'Tags': '<algorithms><graph-theory><social-networks>', 'CreationDate': '2014-01-13T23:54:48.383', 'Id': '19707'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $\\mathbb{M}$ be the set of all 3D triangle meshes.</p>\n\n<p>Let $a:\\mathbb{M} \\rightarrow \\mathbb{R}$ be a function that computes surface area of the mesh.</p>\n\n<p>Let $\\mathbb{T}$ be the set of 3D affine transformation matrices.</p>\n\n<p>Let $t:\\mathbb{M} \\times \\mathbb{T} \\rightarrow \\mathbb{M}$ be a function that transforms mesh $M$ with matrix $T$.</p>\n\n<p>Let $\\mathbb{C}$ be a set of values of unknown (at the moment) structure, with the following properties:</p>\n\n<ul>\n<li>There is a function $c:\\mathbb{M} \\rightarrow \\mathbb{C}$ that maps meshes to these values.</li>\n<li>There is a function $a:\\mathbb{C} \\rightarrow \\mathbb{R}$, such that $(\\forall M \\in \\mathbb{M})(a(c(M)) = a(M))$, i.e. computes surface area of the mesh indirectly, via first mapping it to $\\mathbb{C}$.</li>\n<li>There is a function $t:\\mathbb{C} \\times \\mathbb{T} \\rightarrow \\mathbb{C}$ such that $(\\forall M \\in \\mathbb{M})(\\forall T \\in \\mathbb{T})(t(c(M), T) = c(t(M, T)))$, i.e. $C$s can be transformed with the same effect on computed surface area as if transformation was applied to the mesh itself.</li>\n</ul>\n\n<p>So, is there such $\\mathbb{C}$? Obviously I don't want $\\mathbb{C} = \\mathbb{M}$, and prefer $a(C)$ and $t(C)$ to be of constant asymptotic complexity with respect to amount of triangles in the mesh.</p>\n", 'ViewCount': '63', 'Title': 'Triangle mesh surface area after affine transformation', 'LastActivityDate': '2014-01-14T16:45:09.260', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12837', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2014-01-14T16:45:09.260', 'Id': '19718'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm a researcher working with a language that has gone through phonological changes through time.  I would like to tag parts of a word (i.e. prefix, stem, suffix) and then apply those phonological changes and then see what is left or different about the stuff that I tagged.</p>\n\n<p>I'm currently using Python with some regex stuff to apply the changes so if I can do this using its NPL toolkit that would be perfect.  I've start to mess around with it but I haven't found anything that will work just yet.  I'm also not sure if this toolkit would be the best for this.</p>\n\n<p>For example, I apply the following transformation to tag <code>re</code>, <code>peat</code> and <code>ed</code> in <code>repeated</code>:\n$$\n\\begin{align}\n  \\mathtt{repeated}\n  &amp; \\xrightarrow{\\mathtt{rep} \\mapsto \\mathtt{rp}} \\mathtt{rpeated} \\\\\n  &amp; \\xrightarrow{\\mathtt{ea} \\mapsto \\mathtt{e}} \\mathtt{rpeted} \\\\\n  &amp; \\xrightarrow{\\mathtt{d}\\$ \\mapsto \\epsilon} \\mathtt{rpete} \\\\\n\\end{align}\n$$</p>\n\n<p>I would like to be able to find out what is left of the stuff I tagged.  So I'd like to see that <code>r</code> is all that is left of the prefix, <code>pe</code> is all that is left of the stem, etc.  Any help or direction is greatly appreciated!</p>\n", 'ViewCount': '25', 'Title': 'Trying to tag parts of a word and keep track of any changes that happen to those parts', 'LastEditorUserId': '39', 'LastActivityDate': '2014-01-29T15:41:39.147', 'LastEditDate': '2014-01-29T15:41:34.770', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12849', 'Tags': '<algorithms><strings><natural-lang-processing><computational-linguistics>', 'CreationDate': '2014-01-14T19:56:01.060', 'Id': '19725'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that I want to optimize a unimodal function defined on some real interval.  I can use the well-known algorithm as described in Wikipedia under the name of <a href="http://en.wikipedia.org/wiki/Ternary_search" rel="nofollow">ternary search</a>.</p>\n\n<p>In case of the algorithm that repeatedly halving intervals, it is common to reserve the term <em>binary search</em> for discrete problems and to use the term <em>bisection method</em> otherwise.  Extrapolating this convention, I suspect that the term <em>trisection method</em> might apply to the algorithm that solves my problem.</p>\n\n<p>My question is whether it is common among academics, and is safe to use in, e.g., senior theses, to apply the term <em>ternary search</em> even if the algorithm is applied to a continuous problem.  I need a reputable source for this.  I\'m also interested whether the term <em>trisection method</em> actually exists.</p>\n', 'ViewCount': '96', 'Title': 'Is "ternary search" an appropriate term for the algorithm that optimizes a unimodal function on a real interval?', 'LastEditorUserId': '12861', 'LastActivityDate': '2014-04-20T03:07:25.000', 'LastEditDate': '2014-01-16T03:45:59.587', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12861', 'Tags': '<algorithms><terminology><numerical-analysis><numerical-algorithms>', 'CreationDate': '2014-01-15T02:37:13.467', 'Id': '19734'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for an algorithm to convert a digraph (directed graph) to an undirected graph in a reversible way, ie the digraph should be reconstructable if we are given the undirected graph. I understand that this will come in expense of the undirected graph having more vertices but I do not mind.</p>\n\n<p>Does one know how to do this or can suggest any references? Thanks in advance.</p>\n\n<hr>\n\n<p>Update: Regarding the answer of AdrianN below. It might be a good starting point but I don\'t think it works in its current form. Here is an image of why I think it doesn\'t:\n<img src="http://i.stack.imgur.com/Kh4eq.png" alt="enter image description here"></p>\n\n<hr>\n\n<p>Update after D.W.\'s comment: I consider the vertices of the graphs to be unlabeled. If a solution involves labeling the vertices (like AdrianN\'s does), then it should give the same (isomorphic) undirected graph no matter how the labeling is done. My definition of "isomorphic" for graphs with labeled vertices is that there is a permutation of the labeling that relates the two graphs, but I am not sure of the exact definition for unlabeled graphs...</p>\n', 'ViewCount': '127', 'Title': 'Converting a digraph to an undirected graph in a reversible way', 'LastEditorUserId': '12348', 'LastActivityDate': '2014-01-16T18:45:39.390', 'LastEditDate': '2014-01-15T19:07:50.007', 'AnswerCount': '4', 'CommentCount': '4', 'AcceptedAnswerId': '19758', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12348', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2014-01-15T10:30:23.683', 'Id': '19744'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to write an algorithm that detects the most common subset of at least size $k$, from a collection of sets.  If there are ties for the most common subset, I want the one of them whose size is as large as possible.</p>\n\n<p>For example if I have:</p>\n\n<pre><code>s1 = {A, B, C   }\ns2 = {A, B, C, D}\ns3 = {   B, C, D}\n</code></pre>\n\n<p>Then the most common subset of size $\\ge k=2$ is {B, C}. As another example, if I have:</p>\n\n<pre><code>s1 = {A, B, C  D}\ns2 = {A, B, C, D}\ns3 = {   B, C, D}\n</code></pre>\n\n<p>Then the most common subset of size $\\ge k=2$ is {B, C, D}. It's important that in this instance the algorithm would give me {B, C, D} and not {B, C}, {B, D} etc. Note that I'm not interested in the longest common subset (a different problem), I'm interested in the longest most common subset if you will. I also don't care about enumerating all the different subsets, I just want to find the most common.</p>\n\n<p>Is there an efficient algorithm for this problem?</p>\n\n<p>I have an algorithm for this problem, but I don't think it's very efficient. For $k=2$ I enumerate all subsets of size 2 and count how many times each one appears in the collection. If the most-frequently occurring pair is more frequently occurring than any other pair then that must be the most common subset. If there is more than one with the same (maximum) frequency then I look at the sets they are contained in. If these overlap exactly then I take the union of the pairs and that gives me the most common subset (with size > 2).</p>\n\n<p>I think this could be related to the maximum clique problem but I'm not certain.</p>\n\n<p>Note that just taking the intersection does not give the correct answer.  For instance, if I have</p>\n\n<pre><code>s1 = {A, B      }\ns2 = {      C, D}\ns3 = {A,    C, D}\n</code></pre>\n\n<p>then the intersection is the empty set, but the most common subset is {C, D}.</p>\n", 'ViewCount': '69', 'Title': 'Most common subset of size $k$', 'LastEditorUserId': '755', 'LastActivityDate': '2014-01-16T22:42:41.270', 'LastEditDate': '2014-01-16T22:42:41.270', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19762', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12876', 'Tags': '<algorithms><graphs><sets><data-mining>', 'CreationDate': '2014-01-15T21:58:59.930', 'Id': '19755'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have to generate all possible paths in a directed, acyclic weighted graph with edge costs. I also have to sort them in order of shortest path. </p>\n\n<p>The simplest way that comes to mind is to do a depth-first search (DFS) for all paths, accumulating their edge costs as I traverse the paths, and then doing an NlogN sort on the result. </p>\n\n<p>But I wondering if there is a better way to do this task. Are there any algorithms that could optimize this problem (combining DFS and a shortest path algorithm such as Dijkstra's maybe?). </p>\n", 'ViewCount': '261', 'Title': 'Optimal algorithm to traverse all paths in the order of shortest path', 'LastEditorUserId': '10519', 'LastActivityDate': '2014-01-19T16:10:53.177', 'LastEditDate': '2014-01-16T17:08:00.273', 'AnswerCount': '1', 'CommentCount': '13', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10519', 'Tags': '<algorithms><graphs><shortest-path><search-algorithms>', 'CreationDate': '2014-01-16T01:40:27.343', 'FavoriteCount': '1', 'Id': '19761'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '331', 'Title': "Why does Dijkstra's algorithm fail on a negative weighted graphs?", 'LastEditDate': '2014-01-16T22:51:53.690', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '11972', 'FavoriteCount': '1', 'Body': '<p>I know this is probably very basic, I just can\'t wrap my head around it.<br>\nWe recently studied about Dijkstra\'s algorithm for finding the shortest path between two vertices on a weighted graph.<br>\nMy professor said this algorithm will not work on a graph with negative edges, so I tried to figure out what could be wrong with shifting all the edges weights by a positive number, so that they all be positive, when the input graph has negative edges in it.<br>\nFor example, let\'s consider the following input graph:<br>\n<img src="http://i.stack.imgur.com/GlrNb.png" alt="input graph"><br>\nNow if I\'ll add 3 to all edges, it\'s obvious that the shortest path (between $s$ and $t$) has changed:\n<img src="http://i.stack.imgur.com/NgPpM.png" alt="graph after adding 3"><br>\nThus this kind of operation might result in wrong output. \nAnd this, basically, what I don\'t get. Why does this happen? Why is shifting the values has such a dramatic effect on the shortest path? This is totally counter-intuitive, at least for me.<br>\nIn probability, when you have some (discrete) distribution probability given by some random variable $X$, and you want to calculate the variance, then for every constant $c$, it holds that $Var(X+c)=Var(X)$, and this happens, of-course, because shifting the distribution values left or right does not effect how $X$ spreads.<br>\nNow, I\'m well aware that these are two different things here, and finding the shortest path is not exactly the same as calculating the variance of a random distribution function, but I\'m just saying that this is why to me it seems so counter-intuitive.</p>\n\n<p>Your thoughts?</p>\n', 'Tags': '<algorithms><graphs><shortest-path><weighted-graphs>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-16T22:51:53.690', 'CommentCount': '3', 'AcceptedAnswerId': '19775', 'CreationDate': '2014-01-16T18:47:20.783', 'Id': '19771'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for papers/methods (or at least problem examples) where the original search problem $P$ can be solved by either:</p>\n\n<ol>\n<li>Searching through the original graph. or</li>\n<li>By decomposing it into several subset of problems $P_1,P_2, \\dots,P_n$.</li>\n</ol>\n\n<p>Ideally $sol(P )=sol(P_1)\\cup sol(P_2)\\cup \\ldots \\cup(P_n)$ with no preprocessing (i.e the union of the subproblems correspond directly to the solution of the problem).  </p>\n\n<p>I have no constraint; though prefer the underlying graph to be a DAG and the problem to be a reachability problem. Google seems to fail on finding such papers. </p>\n', 'ViewCount': '74', 'Title': 'Decomposing the search problem into several small problems', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T12:32:29.493', 'LastEditDate': '2014-01-17T21:36:52.663', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '19804', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<graph-theory><reference-request><search-algorithms><search-problem>', 'CreationDate': '2014-01-17T18:59:51.023', 'Id': '19792'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '508', 'Title': 'How does the 3-opt algorithm for TSP work?', 'LastEditDate': '2014-01-19T16:11:51.803', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12945', 'FavoriteCount': '1', 'Body': u'<p>I understand that the 3-Opt Heuristic for solving the Traveling Salesman problem involves removing three edges from a graph and adding three more to recomplete the tour. However, I\'ve seen many papers that mention that when three edges are removed, there remain only 2 possible ways to recombine the tour - this doesn\'t make sense to me.</p>\n\n<p>For example, I found a paper [1] that says:</p>\n\n<blockquote>\n  <p>The 3-opt algorithm works in a similar fashion, but instead of removing two edges we remove three. This means that we have two ways of reconnecting the three paths into a valid tour1 (\ufb01gure 2 and \ufb01gure 3). A 3-opt move can actually be seen as two or three 2-opt moves.</p>\n</blockquote>\n\n<p>However, I count 8 different ways to reconnect the tour (7 if not counting the permutation before removing the edges). What am I missing here? <strong>Edit: 3 different ways, not 8</strong></p>\n\n<p>Also, can someone link me to an algorithm for 3-opt if possible? I\'m just trying to understand it but I haven\'t come across any clear algorithms yet; all resources I find simply say "remove three edges, reconnect them". That\'s it, which is sort of vague.</p>\n\n<p>Here are the 3 tours that seem to me to be 3-opt moves after removing three edges.</p>\n\n<p><img src="http://i.stack.imgur.com/KynPB.png" alt="enter image description here"></p>\n\n<hr>\n\n<ol>\n<li><a href="http://web.tuke.sk/fei-cit/butka/hop/htsp.pdf" rel="nofollow">Heuristics for the Traveling Salesman Problem</a> by C. Nilsson</li>\n</ol>\n', 'Tags': '<algorithms><optimization><heuristics><traveling-salesman>', 'LastEditorUserId': '12945', 'LastActivityDate': '2014-03-03T18:36:23.367', 'CommentCount': '6', 'AcceptedAnswerId': '19810', 'CreationDate': '2014-01-18T16:42:44.363', 'Id': '19808'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><img src="http://i.stack.imgur.com/bNQ68.png" alt="enter image description here"></p>\n\n<p>I\'m working on semi automatic texture segmentation using level set and gabor feature vector described by Manjunath und Ma [1]. I consider their feature representation is better than others since I can get good result on several images but somehow performs really bad on other images data. So I guess there\'s something wrong with my understanding.</p>\n\n<p>They describe that the feature vectors consist of $\\mu_{mn}$ (mean) and $\\sigma_{mn}$ (standard deviation). I use scale $S = 4$ and orientation $K = 6$, thus my feature vector will be 48 dimension (4 * 6 * 2 = 48):</p>\n\n<p>$f = \\{  \\mu_{00}, \\sigma_{00}, \\mu_{01}, \\sigma_{01}, \\mu_{02}...\\mu_{35}, \\sigma_{35}  \\} \\tag1$</p>\n\n<p>I am confused with their equation:</p>\n\n<p>$W(x, y) = \\int I(x_1,y_1)g_{mn} * (x - x_1, y - y_1) dx_1 dy_1 \\tag2$</p>\n\n<p>$\\mu_{mn} = \\int \\int |W_{mn}(x,y)|dxdy \\tag3$</p>\n\n<p>$\\sigma_{mn} = \\sqrt{ \\int \\int (|W_{mn}(x,y) - \\mu_{mn}|)^2 dxdy } \\tag4$</p>\n\n<p>So my first question is: <strong>are $\\mu$ and $\\sigma$ simply mean and average?</strong> So far, I just average every pixel value of the magnitude to obtain $\\mu$ and compute the standard deviation to obtain $\\sigma$.</p>\n\n<p>After I compute the feature vector, I need to do distance measure between two vectors $f_i$ and $f_j$</p>\n\n<p>$d(i, j) = \\sum_{}^{m}\\sum_{}^{n} d_{mn}(i, j) \\tag5$</p>\n\n<p>$d_{mn}(i, j) = |\\frac{\\mu_{mn}^i - \\mu_{mn}^j}{\\alpha(\\mu_{mn})}|  + |\\frac{\\sigma_{mn}^i - \\sigma_{mn}^j}{\\alpha(\\sigma_{mn})}| \\tag6$</p>\n\n<p>I\'m really puzzled with $\\alpha(\\mu_{mn})$ and $\\alpha(\\sigma_{mn})$ ? <strong>What are they?</strong> I have tried to calculate 48 standard deviations from $ (1)$</p>\n\n<p>$\\alpha = \\{ \\alpha(\\mu_{00}), \\alpha(\\sigma_{00}), \\alpha(\\mu_{01}), \\alpha(\\sigma_{01}), \\alpha(\\mu_{02})...\\alpha(\\mu_{35}), \\alpha(\\sigma_{35}) )\\} \\tag7$</p>\n\n<p>and use it to calculate the distance function but I\'m not convinced with my calculation since <strong>it seems not better than simple euclidean distance.</strong></p>\n\n<hr>\n\n<ol>\n<li><a href="http://vision.ece.ucsb.edu/publications/96PAMITrans.pdf" rel="nofollow">Texture Features for Browsing and Retrieval of Image Data</a> by B.S. Manjunath and W.Y. Ma (1996)</li>\n</ol>\n', 'ViewCount': '32', 'Title': 'How to compute Gabor feature vector?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-20T18:13:21.637', 'LastEditDate': '2014-01-20T11:42:09.483', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12978', 'Tags': '<algorithms><image-processing><computer-vision>', 'CreationDate': '2014-01-20T10:59:27.953', 'FavoriteCount': '1', 'Id': '19847'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a timeseries dataset of a quantity measured over the period of a week. I want to verify if the data is varying in a diurnal fashion with the help of some mathematical measure. Does any such measure exist?</p>\n', 'ViewCount': '21', 'ClosedDate': '2014-02-02T11:28:36.667', 'Title': 'Model for diurnal nature of data', 'LastActivityDate': '2014-01-31T05:44:09.183', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9396', 'Tags': '<algorithms><data-mining>', 'CreationDate': '2014-01-21T17:35:09.867', 'Id': '19872'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an m x n matrix which is sparse with N non-zero entries. A modified version of Kadane\'s 2-d algorithm can find the maximum sum subrectangle in O(m N log n) time, which beats traditional Kadane\'s 2-d algorithm of O(m^2 n) for sufficiently sparse matrices. The sparse matrix algorithm, which is also O(m^2 n) time for dense matrices, matching Kadane\'s algorithm, can be found here <a href="http://stackoverflow.com/questions/17558028/maximum-sum-subrectangle-in-a-sparse-matrix">http://stackoverflow.com/questions/17558028/maximum-sum-subrectangle-in-a-sparse-matrix</a> .Now I want to know if the optimal solution can be updated quickly if one entry in the matrix is changed. By "quickly" I mean something like O(m log n) time or better. It\'s possible that perhaps the matrix does not have to be sparse to work out a solution, however a solution when N = O(min(m,n)) would be ok. Preprocessing is also ok as long as the amortized cost of preprocessing time per element changed matches or beats something like O(m log n) time for m changes.</p>\n', 'ViewCount': '46', 'Title': 'Updating maximum sum subrectangle in a sparse matrix when one element is changed', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-01-21T20:34:10.013', 'LastEditDate': '2014-01-21T19:52:44.193', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><dynamic-programming><matrices>', 'CreationDate': '2014-01-21T17:38:51.647', 'Id': '19873'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What is the most efficient algorithm for finding the median of an array when the available operations are limited to Max(), Min(), Multiply, Add, and no conditionals are allowed. Pivots and sorts are not allowed. The algorithm I have come up with so far seems very inefficient and goes like so:</p>\n\n<ol>\n<li>For an array of length n, create a working array of length 2n by\ndoing pairwise Max() comparisons for every pair of elements in the input\narray. This 2n array is guaranteed not to contain the lowest value from the original (or one fewer of the lowest value if there were ties for the lowest value)</li>\n<li>Recursively call (1) n/2 times </li>\n<li>Take min() of final array</li>\n</ol>\n\n<p>There's got to be a faster algorithm!</p>\n", 'ViewCount': '56', 'Title': 'Efficient Median Algorithm With Very Constrained Operators', 'LastEditorUserId': '13052', 'LastActivityDate': '2014-01-24T06:02:32.310', 'LastEditDate': '2014-01-23T06:13:42.257', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19909', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13052', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2014-01-23T05:15:52.513', 'Id': '19908'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>So, I've got this problem:</p>\n\n<blockquote>\n  <p>Given a string $\\omega\\in\\{0,\\ldots,9\\}^*$, find the smallest prime number (in base 10) that contains that string, or otherwise returns 0. </p>\n</blockquote>\n\n<p>What I'm asking is a fast algorithm for the problem?</p>\n", 'ViewCount': '107', 'Title': 'Fast ways to compute the smallest prime with a given substring?', 'LastEditorUserId': '31', 'LastActivityDate': '2014-01-23T20:58:12.430', 'LastEditDate': '2014-01-23T15:24:19.347', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11695', 'Tags': '<algorithms><strings><primes>', 'CreationDate': '2014-01-23T12:32:05.913', 'Id': '19910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Recently i\'ve been dealing with a problem that led me to the following questions:</p>\n\n<ul>\n<li>Is there a good algorithm to enumerate all maximum/perfect matchings in a general graph?</li>\n<li>Is there a good algorithm for finding all maximum/perfect matchings in a general graph?</li>\n<li>Are these two problems equivalent in their complexity?</li>\n</ul>\n\n<p>I\'ve stumbled upon the following references:</p>\n\n<ul>\n<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.8179&amp;rep=rep1&amp;type=pdf" rel="nofollow">Algorithms for Enumerating All Perfect Maximum and Maximal Matchings In Bipartite Graphs</a>- Suggesting an algorithm for enumerating all maximum matchings in a bipartite graph.</li>\n<li><a href="http://www.sciencedirect.com/science/article/pii/0893965994900450" rel="nofollow">Finding All the Perfect Matchings\nin Bipartite Graphs</a>- Suggesting an algorithm for finding all perfect matchings in bipartite graphs</li>\n</ul>\n\n<p>Both algorithms\' complexity depend on the number of perfect matchings in the graph (meaning exponential running time in the worst case).</p>\n\n<p>Moreover, both articles deal with bipartite graphs, I couldn\'t find similar articles dealing with the same problem in general graphs.</p>\n\n<p>I\'d appreciate information and references about the above problems.</p>\n', 'ViewCount': '131', 'Title': 'Counting and finding all perfect/maximum matchings in general graphs', 'LastEditorUserId': '10438', 'LastActivityDate': '2014-01-24T19:06:05.883', 'LastEditDate': '2014-01-24T16:12:01.350', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19926', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10438', 'Tags': '<algorithms><complexity-theory><graph-theory><reference-request><matching>', 'CreationDate': '2014-01-23T21:28:38.760', 'Id': '19924'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was trying to understand better the definition of a strong PSRG and I came across this expression which I am trying to understand better:</p>\n\n<p>$$ Pr_{r \\in \\{0,1\\}^l}[A(r) = \\text{"yes"}]$$</p>\n\n<p>Where r is a truly random bit string and A is a polynomial time deterministic machine.\nI\'ve been having some problems understanding what this expression means conceptually (or intuitively). </p>\n\n<p>So far these are some of my thoughts and I will try to point out my doubts too.</p>\n\n<p>A is just a standard TM so we can image that on l steps, it will yield $2^l$ branches. Each branch has a chance of occurring depending on which r occurs. Therefore, I was wondering if the above probability expression just mean "the fraction of branches that out yes"? Is that basically the same as the chance that A will output yes on the given random bit string? The thing that was confusing me and I was not sure how to deal with it was that, A(r) always outputs the same thing ("yes" or "no") on a given r (say it always accepts or rejects if r = 1010100 or something), it didn\'t seem to me that it a probabilistic sense, unless we randomly choose r. So I was wondering how the community interpreted this equation and what it mean.</p>\n\n<p>Also, since this is a probability, it seems to me that A(r) is just r.v. that only takes two values (yes or no), right? So this distribution only has two probability values, the one that A outputs yes or no, right? I was wondering how that related to the string r and I was not sure how to resolve this.</p>\n', 'ViewCount': '32', 'Title': 'Interpreting probabilistic time turning machines', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-01-24T19:23:21.040', 'LastEditDate': '2014-01-24T19:23:21.040', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19943', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<terminology><probability-theory><randomized-algorithms><randomness>', 'CreationDate': '2014-01-24T04:54:38.433', 'Id': '19932'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '105', 'Title': 'Find the centre of a circle given two points lying on it and its radius', 'LastEditDate': '2014-01-25T21:53:25.530', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6823', 'FavoriteCount': '1', 'Body': '<p>We have been given 2 points on a circle and its radius. Now I want to find out the centre point of such a circle. How can I code this efficiently without solving the quadratic equations?</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T13:19:19.007', 'CommentCount': '3', 'AcceptedAnswerId': '19974', 'CreationDate': '2014-01-25T20:05:04.313', 'Id': '19970'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm having trouble figuring out how to create admissible heuristics from cost functions.  For example, if I was trying to create an admissible heuristic from a cost function that takes in starting position and ending position and returns the cost based on differences in height, I don't understand how to create an admissible heuristic from such a basic function.</p>\n\n<p>Any help would be greatly appreciated.</p>\n", 'ViewCount': '86', 'Title': 'Creating admissible heuristics from functions?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T14:46:11.440', 'LastEditDate': '2014-01-26T02:02:13.660', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13134', 'Tags': '<algorithms><heuristics>', 'CreationDate': '2014-01-25T23:24:20.117', 'Id': '19976'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Firstly, I am revising for my Concurrent Programming exam and have come across the following question from a previous exam paper. I have attempted to answer it, and will try and convey this effort; I'm very stuck with this question and not sure how to further progress.</p>\n\n<h3>Question</h3>\n\n<p>This question is from a 2011 Past Paper from my University created by the Examining Body that year.\nExaminers: Professor E K Burke\nDr P Sage\nand the internal examiners</p>\n\n<blockquote>\n  <p>A car park has 100 parking spaces. Because of building work max cars must be accommodated in an overflow area which is accessed via the main car park. Entrance to the main and overflow areas is controlled by two automatic barriers as follows.</p>\n  \n  <ul>\n  <li><p>When the car park is empty both barriers are closed.</p></li>\n  <li><p>Normally, the main barrier is raised as a car approaches and is lowered immediately the car has entered.</p></li>\n  <li>An exception occurs immediately after the main car park is full i.e. when it has 100-max cars in it. As the next car approaches the overflow barrier is raised first, then the main barrier is raised. Once the car has entered the main car park the overflow barrier remains raised and the main barrier is lowered. The normal main barrier action described above then resumes.</li>\n  </ul>\n  \n  <p>Consider the following program which is intended to control the two barriers. </p>\n  \n  <p>All instructions, o1, o2, o3, m1, m2, m3 and m4, are atomic. You may assume that $0 &lt; \\max \\leq 100$.</p>\n\n<pre><code>int #cars=0;\n\nprocess Main {\nwhile (true)\n {\n m1: &lt;#cars++&gt;;\n m2: &lt;openMainBarrier&gt;;\n m3: &lt;closeMainBarrier&gt;;\n m4: &lt;if (#cars==100)\n     break&gt;;\n }\n}\n\nprocess Overflow {\nint max; \no1: &lt;input(max)&gt;;\n while (true) \n {\n   o2: &lt;if (#cars==100-max+1)  break&gt;; \n   o3: &lt;openOverflowBarrier&gt;; \n }\n}\n</code></pre>\n  \n  <ol>\n  <li><p>The program terminates only if both processes terminate. Explain why the program may not terminate.</p></li>\n  <li><p>Explain why, even if the program does terminate, it may not operate as specified.</p></li>\n  <li><p>By introducing the use of semaphores, ensure that program does terminate and operates as specified. You must only use atomic instructions. You may introduce new additional non-semaphore variables but you must not alter the scope of #cars and max.</p></li>\n  </ol>\n</blockquote>\n\n<h3>My attempt</h3>\n\n<p>Most semaphore exercises I have looked at so far often have the main process in a <code>while(true){...}</code> infinite loop, thus, termination has never been an issue before - it is normally not addressed in these short exercises.</p>\n\n<p>It is because of this I am having difficulty trying to produce an answer for 2) and 3). The question does not even appear to address cars leaving the car park, which leads me to understand this isn't an infinite ongoing cycle type process, but will terminate when both car parks reach maximum capacity?</p>\n\n<p>In this instance, the only thing I can think of is that the process will not terminate in the event the main carpark or overflow carpark does not reach capacity, however I feel this answer is not 'clever' enough and I'm worried I may be missing something. </p>\n\n<p>I have attempted 3) as follows but I am worried this is incorrect as I'm not sure if I am supposed to handle cars leaving the car park, or if I am supposed to ignore that in order to force it to terminate.</p>\n\n<pre><code>OverflowCarPark{\nint max;\n&lt;input (max)&gt;\nwhile(true){\n down(maxFull)\n if(#cars &lt;= 100 + max){\n    down(openMainBarrier)\n    up(openOverflowBarrier)\n }\n  else { break; }\n}\n\nMainCarPark{\n&lt;cars++;&gt;\nwhile(true){\n if(#cars &lt;= 100){\n    up(openMainBarrier)\n  } else if (#cars &gt; 100 + max) { break; \n  } else { \n    up(maxFull);\n    down(openOverflowBarrier);\n }\n}\n</code></pre>\n\n<p>What is the recommended approach to dealing with termination here? </p>\n", 'ViewCount': '38', 'Title': 'Finding issues in concurrent implementation of carpark overflow control', 'LastEditorUserId': '12785', 'LastActivityDate': '2014-01-26T09:17:05.963', 'LastEditDate': '2014-01-26T02:40:05.850', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12785', 'Tags': '<algorithms><concurrency><synchronization><mutual-exclusion>', 'CreationDate': '2014-01-26T00:47:28.777', 'Id': '19977'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have a list of distinct integers. I'm looking for a data structure that will support the operations <code>search</code>, <code>insert</code>, <code>delete</code> and <code>closest_pair</code> in $O(\\log n)$ time. </p>\n\n<p>I know that a sorted array supports <code>search</code>, <code>insert</code> and <code>delete</code> in $O(\\log n)$ time. So, all we need to do is maintain information about the closest pair during the <code>insert</code> and <code>delete</code> operations. </p>\n\n<p>The only problem is that the <code>closest_pair</code> would perform in better than $O(\\log n)$ time since information about the closest pair would already be available. So, I'm stuck at this point.</p>\n", 'ViewCount': '227', 'Title': 'Data Structure For Closest Pair Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T21:35:15.070', 'LastEditDate': '2014-01-26T15:56:11.007', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '19998', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<data-structures><search-algorithms>', 'CreationDate': '2014-01-26T05:43:37.223', 'Id': '19984'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Hello I am trying to solve the AlienTiles problem described at <a href="http://www.alientiles.com" rel="nofollow">alientiles.com</a> using the A* algorithm but I cannot find any good heuristic function so far.</p>\n\n<p>In AlienTiles you have a board with $N \\times N$ tiles, all coloured red. By clicking on a tile, all tiles in the same row and column advance to the next color, with the colour order being red $\\rightarrow$ green $\\rightarrow$ blue $\\rightarrow$ purple, resetting to red after purple. A goal state is a state where every tile has the same colour, as long as its not red.</p>\n\n<p>Is there any good point to start? I am completely frustrated about how I am supposed to handle the problem. An easy function that I came up with was the distance of the colour of the current tile with the target tile, but it is very slow.</p>\n', 'ViewCount': '145', 'Title': 'Solving AlienTiles with an A* heuristic', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-29T02:29:56.353', 'LastEditDate': '2014-01-28T14:09:31.010', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '20054', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13168', 'Tags': '<algorithms><search-algorithms><heuristics><board-games>', 'CreationDate': '2014-01-27T21:01:28.317', 'Id': '20017'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>When talking about general data structure design, my lecture notes talk about one of the concerns being cost of operations. As well as the individual cost, it mentions amortized cost. But then it goes on to say:</p>\n\n<blockquote>\n  <p>Amortized cost can be:</p>\n  \n  <ul>\n  <li>absolute (e.g. for every sequence \u03c3 of operations (O(n log n))</li>\n  <li>competitive (e.g. for every sequence \u03c3 of operations O(OPT(\u03c3)))</li>\n  </ul>\n  \n  <p>where OPT(\u03c3) is the optimal cost of clairvoyant algorithms</p>\n</blockquote>\n\n<p>I can\'t really make any sense of this. Googling <a href="https://en.wikipedia.org/wiki/Page_replacement_algorithm#The_theoretically_optimal_page_replacement_algorithm" rel="nofollow">throws up this</a> but I can\'t see the relevance to data structures more generally. Can anyone help me understand the quoted text?</p>\n', 'ViewCount': '134', 'Title': 'What is a clairvoyant algorithm?', 'LastEditorUserId': '10036', 'LastActivityDate': '2014-02-11T22:58:46.517', 'LastEditDate': '2014-02-11T22:58:46.517', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20037', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10036', 'Tags': '<algorithms><terminology><data-structures><amortized-analysis>', 'CreationDate': '2014-01-28T10:31:46.363', 'Id': '20035'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '191', 'Title': 'detect closed shapes formed by points', 'LastEditDate': '2014-02-02T13:48:57.277', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13186', 'FavoriteCount': '0', 'Body': '<p>I plot several arrays containing xy-coordinates of points (using plot(x,y)) and obtain a plot with some curves. The curves form some very distinctive closed shapes (that is, the points describing the curves lie close to each other).</p>\n\n<p>Now I need to find the (possibly approximate) centers of the closed shapes. Alternatively, it\'s good to "recognize" the closed shapes and to fill them. I don\'t know what is easier given the coordinates of points forming the shapes.</p>\n\n<p>A possible example with 3 closed shapes to detect is given below.</p>\n\n<p><img src="http://i.stack.imgur.com/qDacQ.jpg" alt="enter image description here"></p>\n\n<p>Points can be also added along the image\'s borders, thus, closing all open shapes. Then all "regions" in the figure will be closed, but the question persists.</p>\n', 'Tags': '<algorithms><computational-geometry>', 'LastEditorUserId': '13186', 'LastActivityDate': '2014-02-05T09:43:25.447', 'CommentCount': '7', 'AcceptedAnswerId': '20247', 'CreationDate': '2014-01-28T14:19:16.247', 'Id': '20039'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have tried hard , but i'm unable to come up with the expected running time for the number of comparisons to find the Randomized Median (find the median of an unsorted array). \nAlso i wanted to make sure that we CANNOT take expectation of the recurrence that we use to find the randomized mean , or any other recurrence in any other problem as they belong to different probability spaces? Is this statement right?</p>\n", 'ViewCount': '32', 'ClosedDate': '2014-01-29T17:01:16.563', 'Title': 'Randomised Median', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T17:01:04.093', 'LastEditDate': '2014-01-29T17:01:04.093', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13212', 'Tags': '<algorithms><randomized-algorithms>', 'CreationDate': '2014-01-29T05:10:11.183', 'Id': '20055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<blockquote>\n  <p>The algorithm task is to find an integer (range is not known). the function <code>guess(num)</code> returns one of three chars: \'>\',\'&lt;\' or \'=\'.<br>\n  Find the secret number with <code>O(logS)</code> guesses, where <code>S</code> is the secret number.  You can use <code>find_secret(N1, N2)</code> which operate with <code>O(log(N2-N1))</code>.. What it does is simply a binary search.   </p>\n</blockquote>\n\n<p>So, the algorithm implemented (with Python) as follows:  </p>\n\n<pre><code>def find_secret2():\n    low = 1\n    high = 2\n    answer = guess(high)\n    while answer == "&gt;":\n        low *= 2\n        high *= 2\n        answer = guess(high)\n\n    if answer == "=":\n        return high\n    return find_secret(low, high)\n</code></pre>\n\n<p>my thoughts about the complexity of this algorithm:  </p>\n\n<p>it takes <code>O(logS)</code> to reach the range where <code>low &lt; secret &lt; high</code>.<br>\nthen, it takes <code>O(log(high-low))</code> - because we\'re using <code>find_secret(N1, N2)</code> method.</p>\n\n<p>I\'ll be glad if you could help me explain why the algorithm\'s complexity is <code>O(logS)</code> in a mathematical/rigorous way using the O-notation.<br>\nThanks!</p>\n', 'ViewCount': '55', 'Title': 'Runtime analysis of a "find the secret number" algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T20:39:11.993', 'LastEditDate': '2014-01-29T20:39:11.993', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20080', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13229', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-01-29T19:33:20.053', 'Id': '20078'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a directed graph with $N$ vertices. Every pair of vertices is connected by two edges (one in each direction), and each of these edges has a weight which may be negative.</p>\n\n<p>On various occasions 'edge update' operations occur, where the weight of an edge is modified (although the edge is never deleted, and no new edges or vertices are added). After each of these operations I wish to know the length of the shortest path between one 'root' vertex and every other vertex in the graph. I don't care what the path is, and I already know it must exist. I don't mind how negative weight cycles are handled.</p>\n\n<p>I care about optimizing the running time of the single update operation and the $N$ queries that follow.</p>\n\n<p>The obvious approach would be Bellman-Ford, which would take $O(|V||E|)$ which is $O(n^3)$.</p>\n\n<p>Is there a faster way to do it?</p>\n", 'ViewCount': '37', 'Title': 'Update SSSPP solution on complete digraph on weight changes', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-30T13:06:42.687', 'LastEditDate': '2014-01-30T13:06:42.687', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6986', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2014-01-29T23:13:29.900', 'FavoriteCount': '2', 'Id': '20089'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a social network in the form of an undirected graph $G = (V,E)$ with distinct non-negative integer keys. For each node $u \\in V$, let the set $\\Gamma(u) = \\{ v \\in V : (u,v) \\in E \\}$ be the <strong><em>neighbourhood</em></strong> of $u$. Clearly, I got that $d(u) = \\left | \\Gamma(u) \\right |$, where $d(u)$ is the <strong><em>degree</em></strong> of $u$.</p>\n\n<p>I now want to split $\\Gamma(u)$ into two sets of nodes</p>\n\n<p>$$\\Gamma_{\\text{low}}(u)  = \\{ v \\in V : v \\in \\Gamma(u) \\text{ and } v &lt; u \\}$$\n$$\\Gamma_{\\text{high}}(u) = \\{ v \\in V : v \\in \\Gamma(u) \\text{ and } v &gt; u \\}$$</p>\n\n<p>i.e. $\\Gamma_{\\text{low}}(u)$ and $\\Gamma_{\\text{high}}(u)$ contain neighbours of $u$ whose key is less than or greater than $u$'s, respectively.</p>\n\n<p>Assuming that the graph has got $m$ edges, is there a way to obtain an upper bound for $d_{\\text{low}}(u) = \\left | \\Gamma_{\\text{low}}(u) \\right |$ and $d_{\\text{high}}(u) = \\left | \\Gamma_{\\text{high}}(u) \\right |$ for varying $m$? I just can obtain the following relationship</p>\n\n<p>$$\\sum_{u \\in V} d_{\\text{low}}(u) + \\sum_{u \\in V} d_{\\text{high}}(u) = 2m,$$</p>\n\n<p>because the graph is undirected. I read somewhere that $d(u) = O\\left ( \\sqrt{m} \\right )$ is a good bound for social networks, but how do I prove it?</p>\n", 'ViewCount': '45', 'Title': 'Need an upper bound for node degree', 'LastActivityDate': '2014-01-30T14:19:42.420', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13243', 'Tags': '<algorithms><graph-theory><space-complexity><social-networks>', 'CreationDate': '2014-01-30T14:19:42.420', 'Id': '20108'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Background</strong></p>\n\n<p>I need to find a largest set of non-overlapping axis-parallel squares, out of a given collection of candidate squares.</p>\n\n<p>This problem is NP-complete. Many papers suggest approximation algorithms (see <a href="https://en.wikipedia.org/wiki/Maximum_disjoint_set" rel="nofollow">Maximum Disjoint Set in Wikipedia</a>), but I need an exact algorithm. </p>\n\n<p>My current solution uses the following divide-and-conquer strategy:</p>\n\n<ul>\n<li>Calculate all horizontal and vertical lines that pass through corners of the candidate squares. Each such line separates the candidates into three groups: candidates that are entirely at one side of the line, candidates that are entirely at the other side of the line, and candidates that are intersected by the line. Now there are two cases:\n<ul>\n<li><em>Easy Case</em>: There is a separator line $L$ that does not intersect any candidate square. Then, recursively calculate the maximum-disjoint-set among the squares on one side of $L$, recursively calculate the maximum-disjoint-set among the squares on the other side of $L$, and return the union of these two sets. The separator line guarantees that the union is still a disjoint set.</li>\n<li><em>Hard Case</em>: All separator lines intersect one or more candidate squares. Select one of the separator lines, $L$; suppose that $L$ intersects $k$ squares. Calculate all $2^k$ subsets of these intersected squares. For each subset $X$ that is in itself a disjoint set, calculate the maximum-disjoint-set recursively as in the Easy Case, under the assumption that $X$ is in the set. I.e., recursively calculate the maximum-disjoint-set among the squares on one side of $L$ that do not intersect $X$, recursively calculate the maximum-disjoint-set among the squares on the other side of $L$ that do not intersect $X$, and calculate the union of these two sets with $X$. Out of all $2^k$ unions, return the largest one.</li>\n</ul></li>\n</ul>\n\n<p><strong>Question</strong></p>\n\n<p>My question is: <em>What is the best way to select the separator line $L$</em>?</p>\n\n<p>There are two conflicting considerations: On one hand, we want $L$ to intersect as few squares as possible, so that the power set is not too large. On the other hand, we want $L$ to separate the candidate squares to subsets of balanced size, preferrably equal size, so that the recursion ends as fast as possible. What is the best way to balance these conflicting considerations?</p>\n\n<p>EDIT: <strong>Additional details</strong></p>\n\n<p>My current heuristic is to pick the separator line that intersects the least number of squares. This heuristic allows the algorithm to process input sets with up to $n=30$ candidates, in several seconds. The optimal solution in these cases has about 10 squares. In general, the number of squares in the optimal solution is near $2\\cdot\\sqrt{n}$.</p>\n\n<p>When the input grows beyond 30 candidates, the running time becomes much slower (several minutes and more). My goal is to find a heuristic that will allow me to process larger sets of candidates.</p>\n', 'ViewCount': '143', 'Title': 'A heuristic for finding a maximum disjoint set', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-02-04T14:43:12.340', 'LastEditDate': '2014-01-31T10:01:52.057', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '20140', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><efficiency><heuristics>', 'CreationDate': '2014-01-30T18:29:37.270', 'Id': '20126'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been looking into computer assisted music composition lately for my school project. While searching for literature I came across <a href="http://igm.rit.edu/~jabics/GenJam94/Paper.html" rel="nofollow">GenJam</a>, an interactive jazz improvisation software which uses genetic algorithms to produce musical phrases. </p>\n\n<p>I was wondering If anyone has done some work on computer generated music and could suggest term papers, books or other reading material I should look into. </p>\n', 'ViewCount': '72', 'Title': 'Computer Music Composition', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T22:36:37.800', 'LastEditDate': '2014-01-31T10:52:11.543', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13272', 'Tags': '<algorithms><reference-request><genetic-algorithms>', 'CreationDate': '2014-01-31T10:24:17.680', 'FavoriteCount': '1', 'Id': '20150'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '80', 'Title': 'Genetic algorithm: What is the expected number of strings that are explored?', 'LastEditDate': '2014-01-31T12:10:35.870', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'Remi.b', 'PostTypeId': '1', 'OwnerUserId': '13275', 'Body': "<p>My question concerns genetic algorithm searching along bit strings.</p>\n\n<p>Given:</p>\n\n<ul>\n<li>$N$ = population size</li>\n<li>$l$ = length of bit strings</li>\n<li>$p_c$ = probability that a single crossover occur (double crossover never occur)</li>\n<li>$p_m$ = probability for a given bit that a mutation occur</li>\n</ul>\n\n<p>$w(x)$, the fitness function is equal to the number of 1 in the strings. Therefore, the fitness can take any integer value between 0 and $l$ (the length of the strings).</p>\n\n<p>My question is (three ways of formulating the same question):</p>\n\n<ul>\n<li>What is the expected total number of possibilities explored in $G$ generations?</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>What is the expected proportion of the total possibility space (which equals $2^l$) that is explored in $G$ generations?</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>What is the expected size of the subset of strings that have ever existed in the population during a simulation that last $G$ generations?</li>\n</ul>\n\n<p>Secondary questions:</p>\n\n<ul>\n<li>How does the frequency distribution - of the total number of possibilities explored in $G$ generation - looks like?\n<ul>\n<li>is it a normal (Gauss) distribution?</li>\n<li>Is it skewed?</li>\n<li>...</li>\n</ul></li>\n</ul>\n\n<hr>\n\n<p>I don't quite know how complex is my question. Here are two assumptions that one would like to consider in order to ease the problem.</p>\n\n<ul>\n<li><p>one might want to assume that the population at start is not randomly drawn from the possibility space. He could assume that the whole population is made of identical strings (only one instance). For example, the string <code>000000000</code> (which length equals $l$).</p></li>\n<li><p>one might want to assume that $p_c = 0$</p></li>\n</ul>\n", 'Tags': '<algorithms><optimization><average-case><genetic-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T17:51:05.043', 'CommentCount': '3', 'AcceptedAnswerId': '20153', 'CreationDate': '2014-01-31T00:26:34.640', 'Id': '20152'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a set of line segments, how do we identify a subset of maximal cardinality where all line segments are pairwise non-intersecting?</p>\n\n<p>Brute force we would get $2^n$ sets to check where $n$ is the number of line segments, so that isn\'t viable. Anyone got a bright idea how the do this efficiently? I tried doing it this way: remove a line segment that intersects with the most other line segments, iterate until no line segments intersect anymore; but that didn\'t work.</p>\n\n<hr>\n\n<p><a href="http://jsfiddle.net/afaucogney/RwNXL/" rel="nofollow">Here</a> is a "ready to help me" place, where you can test your solution; it visualizes the set of line segments.</p>\n\n<p>To try it out, please implement your attempt in the following function on the linked site:</p>\n\n<pre><code>function showAnalysis() {\n    debug("Just do it");\n}\n</code></pre>\n\n<p>and then click on the top canvas. The fiddle generates randoms segments in the top canvas, and the bottom canvas is the place where an optimal subset will be displayed.</p>\n', 'ViewCount': '117', 'Title': 'Efficiently pick a largest set of non-intersecting line segments', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-07T07:22:28.420', 'LastEditDate': '2014-02-07T07:22:28.420', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '1', 'OwnerDisplayName': 'Anthony', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><np-hard><efficiency>', 'CreationDate': '2014-01-22T14:04:07.477', 'Id': '20263'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been reading about Dijkstra\'s algorithm and I think I understand it. I followed the algorithm in pseudo-code from Wikipedia, and now I wonder:</p>\n\n<ol>\n<li><p>If my graph is bi-directional and I add each edge to my graph <em>twice</em> (once "forwards", once "backwards"), will the "standard" Dijkstra\'s algorithm work?</p></li>\n<li><p>Is it ok that some of my edges are zero cost? (the rest are all positive - none are negative)</p></li>\n</ol>\n\n<p>And finally, what is a Dijkstra "heap" algorithm? Is it the same as Dijkstra\'s algorithm using a PriorityQueue?</p>\n', 'ViewCount': '106', 'Title': 'Does "standard" Dijkstra\'s algorithm work with bi-directional edges and zero cost edges?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-04T10:06:37.453', 'LastEditDate': '2014-02-04T10:06:37.453', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21281', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14379', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2014-02-04T07:55:47.223', 'Id': '21280'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '54', 'Title': 'Why is the PageRank vector also the eigenvector of the web adjacency matrix?', 'LastEditDate': '2014-02-04T23:08:39.537', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7706', 'FavoriteCount': '1', 'Body': '<p>From <a href="http://en.wikipedia.org/wiki/PageRank#Damping_factor" rel="nofollow">wikipedia</a>:</p>\n\n<blockquote>\n  <p>The PageRank values are the entries of the dominant eigenvector of the\n  modified adjacency matrix. This makes PageRank a particularly elegant\n  metric</p>\n</blockquote>\n\n<p>Can anyone please elaborate on the connection between the eigenvector and the PR vector? Why are they related?</p>\n', 'Tags': '<graph-theory><search-algorithms>', 'LastEditorUserId': '11946', 'LastActivityDate': '2014-02-04T23:08:39.537', 'CommentCount': '0', 'AcceptedAnswerId': '21291', 'CreationDate': '2014-02-04T11:09:29.440', 'Id': '21283'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a genetic algorithm for an optimization problem.\nI plotted the running time of the algorithm on several runs on the same input and the same parameters (population size, generation size, crossover, mutation). </p>\n\n<p>The execution time changes between executions.\nIs this normal?</p>\n\n<p>I also noticed that against my expectation the running time sometimes decreases in place of increasing when I run it on a larger input.\nIs this expected?</p>\n\n<p>How can I analyze the performance of my genetic algorithm experimentally?</p>\n', 'ViewCount': '93', 'Title': 'How to analyze the performance of a genetic algorithm experimentally?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T23:51:30.663', 'LastEditDate': '2014-02-09T22:59:55.253', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'user2963216', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><genetic-algorithms><algorithm-engineering>', 'CreationDate': '2014-01-07T13:02:11.520', 'FavoriteCount': '1', 'Id': '21352'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In order to plot a curve a set of points is usually calculated based on some formula. The function FPLOT in MATLAB also supports plotting with some error tolerance. Its help says the following about this functionality:</p>\n\n<pre><code>The FPLOT function begins with a minimum step of size (XMAX-XMIN)*TOL.\nThe step size is subsequently doubled whenever the relative error\nbetween the linearly predicted value and the actual function value is\nless than TOL.\n</code></pre>\n\n<p>So, if I plot a curve based on some expression and with some predefined tolerance TOL, is the error of the line segment approximation between any two calculated points always lower than TOL? Unfortunately, this is not obvious for me.</p>\n\n<p>If not, how could one achieve a guaranteed (small) error of a piecewise-linear curve approximation (compared with the "exact" curve).</p>\n', 'ViewCount': '14', 'Title': 'Error estimates of piecewise-linear curve approximations', 'LastActivityDate': '2014-02-06T14:32:19.993', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '21379', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13186', 'Tags': '<approximation><approximation-algorithms><error-estimation>', 'CreationDate': '2014-02-06T14:20:58.570', 'Id': '21376'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '64', 'Title': 'Finding all paths with lengths in a fixed interval in sparse graphs', 'LastEditDate': '2014-02-06T16:31:43.573', 'AnswerCount': '2', 'Score': '1', 'OwnerDisplayName': 'Andrew S.', 'PostTypeId': '1', 'OwnerUserId': '13327', 'Body': '<p>What is the most efficient way to find all paths of length M to N in a large sparse graph?</p>\n\n<p>Some general information:</p>\n\n<ul>\n<li>Graph has 30,000 to 50,000 nodes</li>\n<li>Average number of edges per node ~ 10</li>\n<li>M=4, N=7</li>\n<li>Graph has cycles</li>\n</ul>\n', 'Tags': '<algorithms><graphs><shortest-path>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T19:11:38.457', 'CommentCount': '2', 'AcceptedAnswerId': '21382', 'CreationDate': '2014-02-01T17:56:44.383', 'Id': '21381'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is a popular question: </p>\n\n<blockquote>\n  <p>What is the most efficient (in time complexity) way to sort 1 million 32-bit integers? </p>\n</blockquote>\n\n<p>Most answers seem to agree that one of the best ways would be to use radix sort since the number of bits in those numbers is assumed to be constant. This is also a very common thought exercise when CS students are first learning non-comparison based sorts. However, what I haven\'t seen described in detail (or at least clearly) is how to optimally choose the radix (or number of buckets) for the algorithm.</p>\n\n<p><a href="http://www.quora.com/Sorting-Algorithms/What-is-the-most-efficient-way-to-sort-a-million-32-bit-integers" rel="nofollow">In this observed answer</a>, the selection of the radix/number of buckets was done empirically and it turned out to be $2^8$ for 1 million 32-bit integers. I\'m wondering if there is a better way to choose that number? In "Introduction to Algorithms" (p.198-199) it explains Radix\'s run-time should be $\\Theta(d(n+k))$ (d=digits/passes, n=number of items, k=possible values). It then goes further and says that given n b-bit numbers, and any positive integer $r \\leq b$, radix-sort sorts the number in $\\Theta((b/r)(n+2^r))$ time. It then says: </p>\n\n<blockquote>\n  <p>If $b \\geq \\lfloor \\lg(n) \\rfloor$, choosing $r = \\lfloor \\lg(n) \\rfloor$ gives the best time to within a constant factor.</p>\n</blockquote>\n\n<p>But, if we choose $r = \\lg(10^6) =20$, which is not $8$ as the observed answer suggests.</p>\n\n<p>This tells me that I\'m very likely misinterpreting the "choosing of $r$" approach the book is suggesting and missing something (very likely) or the observed answer didn\'t choose the optimal value.</p>\n\n<p>Could anyone clear this up for me?</p>\n', 'ViewCount': '130', 'Title': 'Choosing the optimal radix/number-of-buckets when sorting n-bit integers using radix sort', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T16:28:50.420', 'LastEditDate': '2014-02-06T16:28:50.420', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'OwnerDisplayName': 'sigue', 'PostTypeId': '1', 'Tags': '<algorithms><sorting><efficiency>', 'CreationDate': '2014-01-13T03:18:00.663', 'FavoriteCount': '1', 'Id': '21385'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of piecewise-linear curves (i.e. "points connected by line segments") as shown in the figure below:</p>\n\n<p><img src="http://i.stack.imgur.com/pHNJY.jpg" alt="enter image description here"></p>\n\n<p>additional points are added on the borders (drawn in red) "closing" the open regions of the figure. In fact, the figure can be any plot with some closed piecewise-linear shapes on it (i.e. there are no lines or curves with open ends).</p>\n\n<p>I need to find some (one is enough, but more is OK too if it doesn\'t significantly slow down the algorithm) testing points inside of <strong>EVERY</strong> closed contour (see e.g. the figure above). By adding the border lines all "open" regions become closed ones, so the whole figure consists of simple "closed" regions. However, this condition is not necessary in general.</p>\n\n<p>The test points should also lie on some minimal distance from the boundary (this condition assures the meaningful results).</p>\n\n<p><em>I also have a possible solution to the problem:</em></p>\n\n<p>1) By a curve intersection algorithm all intersection points of any curve (incl. the "borders") with any other curve were found. By this procedure every curve was divided into <strong>segments</strong> by the intersection points lying on the curve.</p>\n\n<p>2) Next, two normals are drawn from some linear piece of a segment to "infinity" (i.e. the length of the normals is big enough to place their endpoints outside of the plotted/bounded region). </p>\n\n<p>3) The two nearest intersections are found, and the two test points are chosen at the half of the distance between the normals\' origins and their two nearest intersections. If the distance is too small - choose another piece of the segment and repeat (2) and (3).\nThis should work since every curve is a border of some closed contour. </p>\n\n<p>4) repeat (2) and (3) for all segments found in (1)</p>\n\n<p>The proposed solution doesn\'t require that the closed contours are "detected" (i.e. that they are available as ordered lists of points). However, it can be assumed that the closed contours are "known" (their detection was discussed e.g. in <a href="http://cs.stackexchange.com/questions/20039/detect-closed-shapes-formed-by-points">detect closed shapes formed by points</a>)</p>\n\n<p><strong>The main concern about the proposed solution is that it might be not quite efficient</strong>, because we need first N*N/2 intersection checks (since the curves are checked pair by pair) in order to find all intersections. N is the number of curves. And then we need at least M*N/2 intersection checks in order to analyze every segment. M is the number of curve segments.</p>\n\n<p>However, in this problem the speed IS an issue.</p>\n', 'ViewCount': '60', 'Title': 'test points inside of polygons/piecewise-linear contours', 'LastEditorUserId': '13186', 'LastActivityDate': '2014-02-11T14:55:30.343', 'LastEditDate': '2014-02-11T14:55:30.343', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13186', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-02-07T09:59:43.807', 'Id': '21426'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Based on the Wikipedia implementation of insertion sort:</p>\n\n<p>Given an input array $A$:</p>\n\n<pre><code>for i \u2190 1 to length(A)\n    j \u2190 i\n    while j &gt; 0 and A[j-1] &gt; A[j]\n        swap A[j] and A[j-1]\n        j \u2190 j - 1\n</code></pre>\n\n<p>is there a way to quantify how many swaps are needed to sort the input list? It's $O(n^2)$, but I want a more precise bound. A perfectly sorted array clearly needs no swaps (so insertion sort is $\\Omega(n)$), while a completely reversed array requires something on the order of $n^2$ swaps, but how do we quantify the number of swaps?</p>\n", 'ViewCount': '50', 'Title': 'How can I quantify the number of swaps required for insertion sort?', 'LastActivityDate': '2014-02-08T19:17:31.727', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21456', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><sorting>', 'CreationDate': '2014-02-08T18:42:15.473', 'Id': '21455'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From the proof of <a href="http://en.wikipedia.org/wiki/Miller-Rabin_primality_test">Miller-Rabin</a>, if a number passes the <a href="http://en.wikipedia.org/wiki/Fermat_primality_test">Fermat primality test</a>, it must also pass the Miller-Rabin test with the same base $a$ (a variable in the proof). And the computation complexity is the same.</p>\n\n<p>The following is from the <a href="http://en.wikipedia.org/wiki/Fermat_primality_test">Fermat primality test</a>:</p>\n\n<blockquote>\n  <p>While Carmichael numbers are substantially rarer than prime\n  numbers,<a href="http://en.wikipedia.org/wiki/Miller-Rabin_primality_test">1</a> there are enough of them that Fermat\'s primality test is\n  often not used in the above form. Instead, <strong>other more powerful\n  extensions</strong> of the Fermat test, such as Baillie-PSW, Miller-Rabin, and\n  Solovay-Strassen are more commonly used. </p>\n</blockquote>\n\n<p>What is the benefit of Miller-Rabin and why it is said to be more powerful than the Fermat primality test?</p>\n', 'ViewCount': '133', 'Title': u'Why Miller\u2013Rabin instead of Fermat primality test?', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-09T11:22:04.147', 'LastEditDate': '2014-02-09T09:41:15.330', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '14516', 'Tags': '<algorithms><primes>', 'CreationDate': '2014-02-09T07:38:07.713', 'Id': '21462'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We are given an array and numerous subarrays. What we can do is add or multiply some element to all elements of a subarray. The subarrays are given as start index and the end index. We have to find the largest number of independent subarrays from the subarrays given to us. For example:</p>\n\n<p>If we have the array as given below :</p>\n\n<p>$\\qquad A = [a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,\\dots]$</p>\n\n<p>Now we are given numerous subarrays as :</p>\n\n<p>$\\qquad\\begin{align*}\n  S_1 &amp;= (a_1, \\dots, a_6) \\\\\n  S_2 &amp;= (a_1, \\dots, a_8) \\\\\n  S_3 &amp;= (a_7, \\dots, a_8) \n\\end{align*}$</p>\n\n<p>So in this case all manipulations that can be done by the subarray $S_2$ is actually captured by the subarrays $S_1$ and $S_3$, i.e we want to add $7$ to all elements in $S_2$ then we can do it by adding $7$ to all elements in $S_1$ and $S_3$. So $S_2$ can be removed. In this similar way we need to find out the maximum number of independent subarrays possible.</p>\n\n<p><strong>Independence</strong> here means that if any subarray can be expressed as a combination of two or more subarrays then any changes that can be made to the subarray like adding a specific element to all elements in the subarray can also be make  by applying the same operation to all the subarrays which make up the given  subarray. For example if we have an array of size n. Then we have a given subarray from A11(11th element) to A20(20th element) and there are two more subarrays given from A11 to A15 and A16 to A20. Then if we want to add 5 to all elements from A11 to A20. Then we can add 5 to subarray from A11 to A15 and also A16 to A20. Hence A11 to A20 is not important.</p>\n\n<p>Here we just need to find the total number of independent subarrays.</p>\n', 'ViewCount': '54', 'ClosedDate': '2014-02-10T11:52:12.307', 'Title': 'Find the largest number of independent subarrays given an array', 'LastEditorUserId': '12101', 'LastActivityDate': '2014-02-10T13:55:36.340', 'LastEditDate': '2014-02-10T13:55:36.340', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12101', 'Tags': '<algorithms><arrays>', 'CreationDate': '2014-02-10T09:36:04.417', 'Id': '21490'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an equation $y=2x^4-20x^2-50x$ within a range of -4 to 6 </p>\n\n<p>I already have an algorithim that tell me the y value when I plug in the $x$. It simply uses a loop.</p>\n\n<p>What I am trying to do is determine what will be be the y max value and what $x$ corresponds to it.\nIs there some sort of loop or counter I can use. I am program this on SPARC machine but any sort of pseucode loop will be appreciated.</p>\n\n<p>I tried to do the following</p>\n\n<p>I have a counter</p>\n', 'ViewCount': '65', 'Title': 'What is an algorithm for what I am trying to do?', 'LastEditorUserId': '3094', 'LastActivityDate': '2014-02-10T22:30:37.267', 'LastEditDate': '2014-02-10T22:05:26.020', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '21508', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithms>', 'CreationDate': '2014-02-10T22:00:20.603', 'Id': '21507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to find a algorithm that will do the following:</p>\n\n<blockquote>\n  <p>Given two sets $A, B \\subseteq \\mathbb{R}$, where $|B| &gt; |A|$, find the largest subset $C \\subseteq B$, such that:</p>\n  \n  <p>$\\qquad |\\operatorname{mean}(A) - \\operatorname{mean}(C) |&lt; \\delta$ and</p>\n  \n  <p>$\\qquad |\\operatorname{std}(A) - \\operatorname{std}(C) | &lt; \\epsilon$ </p>\n</blockquote>\n\n<p>I think this problem is NP-hard, but I would like a good approximation that does reasonably well. An even harder version of this is to find the larget subset $C$ that matches not just the moments, but the histogram of $A$ to some quality metric. If you have a solution, or can point to same papers that would great!</p>\n\n<p>I am a neuroscientist, and this is for my research.</p>\n', 'ViewCount': '61', 'Title': 'Finding largest subset that matches moments', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T23:46:11.150', 'LastEditDate': '2014-02-12T08:37:57.857', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14589', 'Tags': '<algorithms><reference-request><optimization>', 'CreationDate': '2014-02-11T23:42:08.537', 'Id': '21545'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am somewhat confused with the running time analysis of a program here which has recursive calls which depend on a RNG.  (Randomly Generated Number)</p>\n\n<p>Let\'s begin with the pseudo-code, and then I will go into what I have thought about so far related to this one.</p>\n\n<pre><code>    Func1(A, i, j)\n    /* A is an array of at least j integers */\n\n 1  if (i \u2265 j) then return (0);\n 2  n \u2190 j \u2212 i + 1 ; /* n = number of elements from i to j */\n 3  k \u2190 Random(n);\n 4  s \u2190 0; //Takes time of Arbitrary C\n\n 5  for r \u2190 i to j do\n 6      A[r] \u2190 A[r] \u2212 A[i] \u2212 A[j]; //Arbitrary C\n 7      s \u2190 s + A[r]; //Arbitrary C\n 8  end\n\n 9  s \u2190 s + Func1(A, i, i+k-1); //Recursive Call 1\n10  s \u2190 s + Func1(A, i+k, j); //Recursive Call 2\n11  return (s);\n</code></pre>\n\n<p>Okay, now let\'s get into the math I have tried so far.  I\'ll try not to be too pedantic here as it is just a rough, estimated analysis of expected run time.  </p>\n\n<p>First, let\'s consider the worst case.  Note that the K = Random(n) must be at least 1, and at most n.  Therefore, the worst case is the K = 1 is picked.  This causes the total running time to be equal to T(n) = cn + T(1) + T(n-1).  Which means that overall it takes somewhere around cn^2 time total (you can use Wolfram to solve recurrence relations if you are stuck or rusty on recurrence relations, although this one is a fairly simple one).  </p>\n\n<p>Now, here is where I get somewhat confused.  For the expected running time, we have to base our assumption off of the probability of the random number K.  Therefore, we have to sum all the possible running times for different values of k, plus their individual probability.  By lemma/hopefully intuitive logic: the probability of any one Randomly Generated k, with k between 1 to n, is equal 1/n.  </p>\n\n<p><strong>Therefore, (in my opinion/analysis) the expected run time is:</strong></p>\n\n<p><strong>ET(n) = cn + (1/n)*Summation(from k=1 to n-1) of (ET(k-1) + ET(n-k))</strong></p>\n\n<p>Let me explain a bit.  The cn is simply for the loop which runs i to j.  This is estimated by cn.  The summation represents all of the possible values for k.  The (1/n) multiplied by this summation is there because the probability of any one k is (1/n).  <strong>The terms inside the summation represent the running times of the recursive calls of Func1.</strong>  The first term on the left takes ET(k-1) because this recursive call is going to do a loop from i to k-1 (which is roughly ck), and then possibly call Func1 again.  The second is a representation of the second recursive call, which would loop from i+k to j, which is also represented by n-k.</p>\n\n<p><strong>Upon expansion of the summation, we see that the overall function ET(n)  is of the order n^2.</strong>  <em>However</em>, as a test case, plugging in k=(n/2) gives a total running time for Func 1 of roughly nlog(n).  <em>This</em> is why I am confused.  How can this be, if the estimated running time is of the order n^2?  Am I considering a "good" case by plugging in n/2 for k?  Or am I thinking about k in the wrong sense in some way?  </p>\n', 'ViewCount': '66', 'Title': 'Algorithm Analysis: Expected Running Time of Recursive Function Based on a RNG', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-17T19:50:53.527', 'LastEditDate': '2014-02-12T09:12:25.723', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14596', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><average-case>', 'CreationDate': '2014-02-12T05:31:29.490', 'Id': '21558'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know that this is a known theorem but I can't find its proof. The theorem is: </p>\n\n<blockquote>\n  <p>The write-contention of any $n$-process wait-free consensus algorithm (implemented from any read-modify-write operations) is $n$.</p>\n</blockquote>\n\n<p>Can someone link me to a proof or an explanation?</p>\n", 'ViewCount': '38', 'Title': 'Proof of contention of the wait-free consensus algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-12T13:30:08.410', 'LastEditDate': '2014-02-12T13:30:08.410', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14602', 'Tags': '<algorithms><reference-request><proof-techniques><concurrency>', 'CreationDate': '2014-02-12T10:02:18.260', 'Id': '21566'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have two algorithms which I would like to implement:</p>\n\n<p>First, given a (very long) list $\\{\\mathbf{r}_{i}\\}_{i=1}^{n}\\subset \\mathbb{R}^{3}$, a point $p \\in \\mathbb{R}^{3}$, and a distance $d&gt;0$, find all $i$ such that $|\\mathbf{r}_{i} - \\mathbf{p}| &lt; d$.</p>\n\n<p>Currently, I sort the list over the $x$-coordinate, so that if $x_{i}$ is the $x$-coordinate of $\\mathbf{r}_{i}$, then $x_{1} \\le x_{2} \\le \\cdots \\le x_{n}$. Then I use the condition $(x_{i}-\\mathbf{p}_{x})^{2} &gt; d^2 \\implies |\\mathbf{r}_{i} - \\mathbf{p}|^2 &gt; d^{2}$. This allows me to filter the list by binary search. This leaves me with a subset $\\{\\mathbf{r}_{i}\\}_{i=j_{1}}^{i=j_{2}}$ of the original list where for all $i \\in [j_{1}, j_{2}]$, $(x_{i}-\\mathbf{p}_{x})^{2} &lt;= d^2$. Then I have to bite the bullet and do an explicit test on each one of these to see if $|\\mathbf{r}_{i} - \\mathbf{p}| &lt; d$.</p>\n\n<p>This algorithm strikes me as weird and not optimal; is there a better way?</p>\n\n<p>Next: A related problem: Given a list $\\{\\mathbf{r}_{i}\\}_{i=1}^{n}\\subset \\mathbb{R}^{3}$ and a point $\\mathbf{p} \\in \\mathbf{R}^{3}$, find $j \\in [1,n]$ such that $d_{j}:=|\\mathbf{r}_{j} - \\mathbf{p}| = \\min_{i} |\\mathbf{r}_{i} - \\mathbf{p}|$.  Is there an efficient algorithm to find $j$?</p>\n\n<p>Finally, I should note that any one-time ``sorting'' expense is tolerable, since this operation is repeated many times. </p>\n", 'ViewCount': '65', 'Title': 'Find all neighbors at a certain distance, in 3 dimensions', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-13T22:18:22.163', 'LastEditDate': '2014-02-13T22:18:22.163', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21615', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14616', 'Tags': '<algorithms><computational-geometry><search-algorithms>', 'CreationDate': '2014-02-12T22:55:41.277', 'Id': '21580'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm a CS undergrad so my math/CS knowledge is not that deep so please correct me if my premise is flawed or I have made some incorrect assumptions.</p>\n\n<p>So I was thinking, much in the way that some primality testers are probabilistic(they give you yes or no but have a chance to be wrong). Would it be possible to build a probabilistic halting problem solver? One that reports within a certain degree of error, whether a problem halts or not?</p>\n", 'ViewCount': '70', 'Title': 'Possible to construct a probabilistic halting problem solver?', 'LastActivityDate': '2014-02-13T02:24:54.360', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '13245', 'Tags': '<probability-theory><halting-problem><probabilistic-algorithms>', 'CreationDate': '2014-02-12T23:13:06.320', 'FavoriteCount': '1', 'Id': '21581'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I believe the following question in summary is: How to approximate Euclidean distance in a digital plane?</p>\n\n<blockquote>\n  <p>When a pebble falls on a calm surface of water a circular wave propagates. I want to color the pixels with time to show this effect. So I discretize the time and in each time step, starting  from the center, I color one pixel away in all directions. But this gives a square wave. I guess what is wrong is that I have approximated the Euclidean distance with the infinity norm.</p>\n</blockquote>\n\n<p>How do I approximate Euclidean distance to get the a circular wave on the pixels? I don't want to measure the distance from each pixel to the center in each time slot. That will be very heavy. I am looking for a simple algorithm like coloring the next pixel adjacent to last colored pixel.</p>\n", 'ViewCount': '38', 'Title': 'Wave propagation in digital image', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-13T16:56:21.790', 'LastEditDate': '2014-02-13T16:56:21.790', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21589', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12321', 'Tags': '<algorithms><computational-geometry><approximation>', 'CreationDate': '2014-02-13T02:10:38.453', 'Id': '21585'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been reading text books about distributed algorithms. They usually present a large collection of algorithms under various hypothesis and guarantees, with their proof of correctness. But they often fail to mention the practical application of those algorithms, and their relative usefulness and importance. For instance, various forms of broadcast, snapshot, election, consensus...</p>\n\n<p>My questions are :</p>\n\n<ol>\n<li>What are the most fundamental distributed algorithms (the few ones\nthat you really need to know)</li>\n<li>Could you give me very concrete cases where they are used</li>\n<li>What are the most widely used distributed algorithms </li>\n</ol>\n\n<p>Thanks in advance. I'm feeling overwhelmed :)</p>\n", 'ViewCount': '50', 'ClosedDate': '2014-02-15T04:33:47.070', 'Title': 'Widely-used and fundamental distributed algorithms', 'LastEditorUserId': '867', 'LastActivityDate': '2014-02-13T19:56:23.207', 'LastEditDate': '2014-02-13T19:56:23.207', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14642', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2014-02-13T16:23:37.980', 'Id': '21601'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $G=(V,E)$ be a directed graph with a positive weight function $w$ defined on its edges and $s\\in V$.  Let $d$ be $G's$ diameter. We also know that for every  $ u\\in V, $ if $\\delta(s,v) &lt; \\delta(s,u)$ then for every pair of edges $\\{(v,v'),(u,u')\\}$, we have $w(v,v') &lt; w(u,u')$.\nHow do you find the shortest path from $s$ to every other vertex $v$ in time $O(V+E\\cdot d)$ ?</p>\n\n<p>We came up with a linear algorithm since we figured that the longer the path is the heavier it gets, therefore we simply started from $s$, scanned all its neighbors and assigned $d(s,v) = w(s,v)$ and continued iteratively (similar to $BFS$ only that we considered the weights) until we covered all vertices. Yet I'm pretty sure we were wrong since the time it takes is linear which is better than the question asks.</p>\n", 'ViewCount': '33', 'Title': 'Shortest path with weight constriants', 'LastEditorUserId': '7706', 'LastActivityDate': '2014-02-13T22:08:19.560', 'LastEditDate': '2014-02-13T22:08:19.560', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7706', 'Tags': '<algorithms>', 'CreationDate': '2014-02-13T22:01:33.813', 'Id': '21612'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $G=(V,E)$ a directed graph. We define $v \\in V$ to be "nearly root" of there\'s a path from $v$ to at least $|V|/2$ vertices. How do you find the set of all "nearly roots" in linear time?</p>\n\n<p>What we\'ve tried is to build the strongly connected graph $G_{scc}$ obtained from $G$ and then to do a topological sort. Now we went backwards and for each $U\\in G_{scc}$ we count how many vertices it can reach by scanning all outgoing edges and summing up all vertices reachable from it\'s successors. This seems wrong since we may count vertices more than once.</p>\n', 'ViewCount': '94', 'Title': 'How to find all "nearly roots"?', 'LastActivityDate': '2014-02-27T09:30:34.527', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7706', 'Tags': '<algorithms>', 'CreationDate': '2014-02-13T22:07:18.663', 'FavoriteCount': '1', 'Id': '21614'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>It seems to me to be incorrect to say that lexicographic DFS is P-complete, since it isn't a decision problem. There is a corresponding decision problem, first DFS ordering, which is known to be P-complete. However, I want to talk about complexity of DFS, not it's decision problem. What complexity class should I say DFS belongs to?</p>\n", 'ViewCount': '36', 'Title': 'lexicographic depth-first search complexity class', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-24T19:24:05.510', 'LastEditDate': '2014-02-24T19:24:05.510', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'OwnerDisplayName': 'Adam Kurkiewicz', 'PostTypeId': '1', 'OwnerUserId': '11718', 'Tags': '<complexity-theory><search-algorithms>', 'CreationDate': '2014-02-13T14:39:10.070', 'Id': '21620'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm currently trying to find an efficient algorithm to solve a discrete optimization problem that arises when constructing decision trees. The problem is as follows:</p>\n\n<p>Say we are given $N$ ordered data points on the real line: $x_1,\\ldots,x_N \\in \\mathbb{R}$ such that $x_1 &lt; x_2 &lt; \\ldots &lt; x_N$. </p>\n\n<p>Each of the points $x_i$ has a label $y_i \\in \\{0,1\\}$. We are asked to predict this label using a <em>decision stump classifier</em>, $h(x;t) = \\mathbb{1}[x&gt;t]$. The decision stump classifier predicts $\\hat{y}_i = 1$ if $x_i &gt; t$. Thus, the accuracy of this classifier depends on the threshold parameter $t$. </p>\n\n<p>I am looking for an efficient algorithm to determine the value of $t$ that maximizes the number of correctly classified points. That is, solve the problem:</p>\n\n<p>$$\\max_{t\\in\\mathbb{R}} A(t)=\\sum_{i=1}^N \\mathbb{1}[y_i=0]\\mathbb{1}[x_i\\leq t] + \\mathbb{1}[y_i=1]\\mathbb{1}[x_i&gt;t]$$\nSome insights:</p>\n\n<ul>\n<li><p>Since there are $N$ data points, we only need to consider $N+1$ values of $t$. These are values that lie in the $N+1$ intervals $(-\\infty,x_1), [x_1,x_2),\\ldots, [x_{N-1},x_N), [x_N,\\infty]$.</p></li>\n<li><p>When $t &lt; \\min_{i}x_i$, the decision stump predicts $\\hat{y}_i=1$ for all $i=1,\\ldots,N$. Here $A(t) = \\sum_{i=1}^N{\\mathbb{1}[y_i=1]}=N^+$.</p></li>\n<li><p>When $t \\geq \\max_{i}x_i$, the decision stump predicts $\\hat{y}_i=0$ for all $i=1,\\ldots,N$. Here $A(t) = \\sum_{i=1}^N{\\mathbb{1}[y_i=0]}=N^-$.</p></li>\n<li><p>The number of correctly classified points $A(t)$ can be decomposed as $B(t)+C(t)$, where\n$$B(t) = \\sum_{i=1}^N{\\mathbb{1}[y_i=0]\\mathbb{1}[x_i\\leq t]}$$\nis the number of correctly classified points with negative labels and\n$$C(t) = \\sum_{i=1}^N{\\mathbb{1}[y_i=1]\\mathbb{1}[x_i&gt;t]}$$\nand the number of correctly classified points with positive labels. Note that $B(t)$ is monotonically increasing in $t$ while $C(t)$ is monotonically decreasing in $t$.</p></li>\n</ul>\n", 'ViewCount': '80', 'Title': 'Determining the optimal threshold value for a one-dimensional decision stump classifier', 'LastEditorUserId': '2046', 'LastActivityDate': '2014-02-16T20:30:00.237', 'LastEditDate': '2014-02-14T17:52:01.077', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2046', 'Tags': '<algorithms><machine-learning><discrete-mathematics>', 'CreationDate': '2014-02-14T02:54:49.967', 'Id': '21623'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have Java function as input and want to get it's order for example O( n ) \nI search in the internet very hard but didn't find any thing can help me, is there any algorithms or package may help me.</p>\n", 'ViewCount': '100', 'ClosedDate': '2014-02-17T10:37:05.783', 'Title': 'Code to get java code complexity', 'LastActivityDate': '2014-02-15T01:05:39.453', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14684', 'Tags': '<algorithms><java>', 'CreationDate': '2014-02-15T00:59:16.727', 'FavoriteCount': '1', 'Id': '21650'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '50', 'Title': 'Find all non-decreasing sequences given lenght and size', 'LastEditDate': '2014-02-17T17:57:13.627', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14696', 'Body': "<p>I'm trying to find a solution for this exercise:</p>\n\n<blockquote>\n  <p>Give the pseudocode of an algorithm which takes two positive integers\n  n and k and prints all the non-decreasing sequences of length k\n  (1,2,...,n).</p>\n  \n  <p>For example n=4, k=3:</p>\n  \n  <p>111 112 113 114 122 123 124 133 134 144 222 223 224 233 234 244 333\n  334 344 444</p>\n  \n  <p>the complexity must be O(n S(n,k)) with S(n,k) the number of the\n  sequences to print for n and k.</p>\n</blockquote>\n\n<p>i think from the complexity required that a backtracking algorithm it's needed  but i could'nt solve it.</p>\n\n<p>i tried something like this:</p>\n\n<pre><code>P(n,k,h: prefix length, S: sequence)\n\n      if h == k then\n           OUTPUT S\n      else\n         for i=1 to n do\n             if(i&gt;=S[h]) \n                   S[h+1]=i;\n                   P(n,k,h+1,S);\n</code></pre>\n", 'ClosedDate': '2014-02-17T19:49:23.877', 'Tags': '<algorithms><backtracking>', 'LastEditorUserId': '14696', 'LastActivityDate': '2014-02-17T17:57:13.627', 'CommentCount': '6', 'AcceptedAnswerId': '21671', 'CreationDate': '2014-02-15T14:23:38.727', 'Id': '21664'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Earlier this week a <a href="http://arxiv.org/pdf/1402.3036.pdf" rel="nofollow">paper</a> was released describing an algorithm for building optimal alphabetic ternary trees. The alphabetic property as described on <a href="http://en.wikipedia.org/wiki/Binary_search_tree#Optimal_binary_search_trees" rel="nofollow">Wikipedia</a> as </p>\n\n<blockquote>\n  <p>Alphabetic trees are Huffman trees with the additional constraint on\n  order, or, equivalently, search trees with the modification that all\n  elements are stored in the leaves. Faster algorithms exist for optimal\n  alphabetic binary trees (OABTs).</p>\n</blockquote>\n\n<p>From this, I understand that an alphabetic binary tree will produce shorter paths from root to element for elements that have been inserted into the tree more times than elements that haven\'t, and the alphabetic property would be described the same way for a ternary tree. </p>\n\n<p>I haven\'t been able to find a lot of discussion about applications for the alphabetic property outside of Huffman coding but it is not a requirement to implement the algorithm.  Are there any applications where the alphabetic property would be a requirement, and if not, are there any benefits to guaranteeing the alphabetic property that justifies increased implementation complexity?</p>\n', 'ViewCount': '49', 'Title': 'What are applications of alphabetic trees?', 'LastActivityDate': '2014-02-15T21:47:02.083', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14703', 'Tags': '<algorithms><data-structures><trees>', 'CreationDate': '2014-02-15T21:47:02.083', 'FavoriteCount': '1', 'Id': '21680'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The Cheriton-Tarjan MST algorithm finds MSTs in time O(m log log n) in arbitrary graphs by using a cleverly-modified version of a leftist heap data structure to store edges. It was developed in 1976. The algorithm relies on the fact that leftist heaps can be merged in time O(log n), but also uses the fact that leftist heaps are binary trees and therefore that every node has at most two children.</p>\n\n<p>In 1978, binomial heaps were invented as a cleaner type of heap that supports merging in O(log n) time. Since then, most algorithms that need mergable heaps either use binomial heaps or some other related structure. However, since binomial heaps don't use binary trees, the Cheriton-Tarjan algorithm's main optimization (namely, eliminating unnecessary edges by doing a top-down DFS over the heap) won't work in the same time bounds when run on a binomial heap rather than a leftist heap.</p>\n\n<p>Has there been any work done to update the Cheriton-Tarjan MST algorithm to use binomial heaps rather than leftist heaps?</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '39', 'Title': 'Updating the Cheriton-Tarjan MST algorithm to use binomial heaps?', 'LastActivityDate': '2014-02-16T00:48:54.857', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2014-02-16T00:48:54.857', 'FavoriteCount': '1', 'Id': '21687'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '51', 'Title': 'An algorithm for making 2 carts meet', 'LastEditDate': '2014-02-17T08:37:43.170', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14724', 'FavoriteCount': '0', 'Body': '<p>Say I have 2 carts on an infinite railroad, each cart is initially under a lamp.\nThere are only 2 lamps, and they are at a fixed location, hence they don\'t change their location. The distance between them is D, but its not known.\nEach cart has a processing unit, both will execute a copy of the same program once the whole system is "initiated".</p>\n\n<p>The task: to write a code in pseudo-code, which will make the 2 carts collide.</p>\n\n<p>Restrictions:\nThe following commands can be used:</p>\n\n<blockquote>\n  <ol>\n  <li>move left/right *insert_number_here* steps //each movement takes 1 clock cycle, meaning that "move left C steps takes C clock cycles</li>\n  <li>if underlamp *put_instruction_here* //if the cart is under a lamp</li>\n  <li>goto *number_of_line_here*</li>\n  <li>stop</li>\n  </ol>\n</blockquote>\n\n<p>Variables, loops and not (!) are usable.</p>\n\n<p>the given solution is:</p>\n\n<blockquote>\n  <p>for i=1 to infinity:</p>\n  \n  <ol>\n  <li><p>go left i steps.</p></li>\n  <li><p>if underlamp stop.</p></li>\n  <li><p>go right i steps.</p></li>\n  </ol>\n</blockquote>\n\n<p>Now, I need a hint to help me make a simple improvement to the algorithm that is given in the trivial solution, so when the distance between the lamps is D, the total number of steps of both carts will be a first order polynomial function of D.</p>\n\n<p>My lecturer gave the following solution, and told me to improve the given code using a different idea.\nThis is his solution:</p>\n\n<blockquote>\n  <ol>\n  <li>go right.</li>\n  <li>go right.</li>\n  <li>go left.</li>\n  <li>if underlamp goto 6.</li>\n  <li>goto 1.</li>\n  <li>go right.</li>\n  <li>goto 6.</li>\n  </ol>\n</blockquote>\n\n<p>My improvement of the given code, but I\'m not sure that in this one, the total number of steps is a first order polynomial function of D.</p>\n\n<blockquote>\n  <p>for i=1 to infinity:</p>\n  \n  <ol>\n  <li><p>go right i steps.</p></li>\n  <li><p>if underlamp goto 4.</p></li>\n  <li><p>go left.</p></li>\n  </ol>\n  \n  <p>4.for j=1 to infinity:</p>\n  \n  <ol>\n  <li>go right j steps.</li>\n  </ol>\n</blockquote>\n', 'ClosedDate': '2014-02-24T16:45:28.990', 'Tags': '<algorithms><computer-algebra><pseudo-polynomial>', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-02-17T19:03:35.367', 'CommentCount': '3', 'AcceptedAnswerId': '21720', 'CreationDate': '2014-02-16T20:29:05.613', 'Id': '21705'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am a high school student computationally studying the 3-dimensional structure of chromosomes by 40 kilobase loci. In a nutshell, loci that are close in space tend to express their genes at the same time --- loci are different stops on a 3D-winding DNA chain.</p>\n\n<p>The best way to understand the 3D structure is by gathering what are basically distances between loci.</p>\n\n<p>Now I have an $n\\times n$ ($n$ = number of loci studied) matrix where $(i,j)$ is the distance between locus $i$ and locus $j$. I also have a (somewhat miraculous) 3-dimensional of the same chromosome that maps each locus to a certain point in a 3D $(x,y,z)$ coordinate system.</p>\n\n<p>My task is to find all of the loci within a certain radius of locus $L$. With the matrix, I would have to go to $L$ and traverse many nearby locus-distance chains, possibly for a long time, before being any bit certain that I had everything I wanted (i.e. brute force). With the spatial model, I would only have to conduct a simple search within that radius.</p>\n\n<p>Here is my question. What is the complexity of finding nearby loci in the 3D model and the 2D matrix with respect to loci count and radius size (whichever you think is more complex)? (Compare the two complexities and give both)</p>\n\n<p>I am not very studied in CS, but here is what I guess:</p>\n\n<p>$$\nC_{2D search best-case} = O(n^2)\n$$\n$$\nC_{2D search worst-case} = O(2^n)\n$$</p>\n\n<p>Best-case is what you'd expect, and worst-case would be going through every permutation of the distance.</p>\n\n<p>$$\nC_{3D search any case} = O(n)\n$$</p>\n\n<p>This is just my rather fallible intuition.</p>\n", 'ViewCount': '57', 'Title': 'Time complexity of proximity search in distance matrix', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-17T19:14:00.307', 'LastEditDate': '2014-02-17T09:59:29.453', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21721', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14736', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><ralgorithms>', 'CreationDate': '2014-02-17T00:27:33.090', 'Id': '21711'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '130', 'Title': 'Asymptotic lower bound on the number of comparisons needed to find the intersection of unsorted arrays', 'LastEditDate': '2014-02-17T18:28:17.103', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14740', 'FavoriteCount': '1', 'Body': u"<p>A homework problem in my current CS class asks us to produce a <em>comparison-based</em> procedure for taking (essentially\u2014there are some poorly-specified rules about duplicates) the set intersection of $k$ unsorted arrays of at most $n$ elements each. For full credit, we are supposed to do this in $O((k-1)n)$ comparisons. (Specifically, we are given a Java array of arrays of Comparable elements.)</p>\n\n<p>I'm pretty thoroughly convinced that this is impossible, and that the best worst-case comparison bound for such a procedure is $\\Theta(N\\log n_0)$, where $N$ is the sum of the lengths of the arrays and $n_0$ is the length of the shortest array. I don't, however, know how to <em>prove</em> this is the best.</p>\n\n<p>Since producing such an algorithm is current homework, please adhere to the following restriction in your answers/comments: if I am <em>wrong</em>, and it <em>is</em> possible to do better, do not reveal the algorithm unless it is very difficult (in which case a link to a relevant paper would be appreciated).</p>\n\n<h3>What I've tried so far</h3>\n\n<p>The shortest array has $2^{n_0}$ subsets. This gives an immediate information-theoretic lower bound of $\\Omega(\\log_2(2^{n_0}))$. Unfortunately, this is just $\\Omega(n_0)$, and $O(n_0)$ obviously can't be obtained.</p>\n\n<h3>Edit</h3>\n\n<p>I missed a line in the (rather long) assignment. It looks like what he's looking for is actually a lot less interesting than what I thought he wanted. However, I'm still curious about how to prove a lower bound of $\\Omega(N \\log n_0)$, if that is the lower bound.</p>\n", 'Tags': '<algorithms><complexity-theory><sets>', 'LastEditorUserId': '14740', 'LastActivityDate': '2014-02-17T18:28:17.103', 'CommentCount': '16', 'AcceptedAnswerId': '21717', 'CreationDate': '2014-02-17T03:29:15.263', 'Id': '21714'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been reading about the No Free Lunch Theorem, but I can\'t quite understand what it is about.  I\'ve heard this theorem described elsewhere as the claim that "no general purpose universal optimiser exists". On the other hand, the <a href="https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization" rel="nofollow">Wikipedia article</a> talks about \'candidate solutions" that are "evaluated one by one" - if we only consider algorithms of a particular form, then that is a much more limited claim.</p>\n\n<p>Can anyone explain what this theorem actually claims?</p>\n', 'ViewCount': '459', 'Title': 'What is the no free lunch theorem?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-18T13:02:10.723', 'LastEditDate': '2014-02-18T12:58:43.690', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '644', 'Tags': '<algorithms><terminology><optimization><heuristics>', 'CreationDate': '2014-02-18T10:48:00.223', 'FavoriteCount': '3', 'Id': '21758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we had 2 arrays of the same size with positive numbers and we wanted to pair up the elements of each array such that the total difference between the pairs is minimized.</p>\n\n<p>The first thought would be to choose pairs with the minimum difference and so on. But it turns out the correct algorithm is to sort them and them pair accordingly.</p>\n\n<p>Any ideas on how to prove that the latter algorithm correctly minimizes the sum of differences?</p>\n', 'ViewCount': '85', 'Title': 'How can we minimize the total distance of cross pairs in an array', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-21T16:31:05.690', 'LastEditDate': '2014-02-21T16:31:05.690', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21815', 'Score': '2', 'OwnerDisplayName': 'user14805', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><optimization><correctness-proof><permutations>', 'CreationDate': '2014-02-18T14:51:18.950', 'Id': '21767'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose in a given plain there are  fixed number of lines. A point P lies on one of the line. How to find which line intersects the point P ? I am giving an example<img src="http://i.stack.imgur.com/0CN2J.png" alt="enter image description here"> </p>\n\n<p>In the above graph point P is on the line CE. We can determine it visually.But my problem is how to make the computer understand it. Is there any algorithm available to make it so ?</p>\n', 'ViewCount': '134', 'Title': 'How to find whether a point is in a line or not', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-18T16:53:15.867', 'LastEditDate': '2014-02-18T16:36:45.730', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14806', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-02-18T15:15:49.593', 'Id': '21770'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to write some little code (POC for the selection/mutation operators) that uses a genetic algorithm to solve a global maximum for a function.</p>\n\n<pre><code>f(x_1...x_n) = M - (x_1 - a_1)^2 - (x_2 - a_2)^2 - ... - (x_n - a_n)^2\n</code></pre>\n\n<p>M a_i are constants. I have to find x_i such that f(x_i) = max(f) = M</p>\n\n<p>My selection method is truncation (I select the top 100 fittest of a population of 500).\nMy crossover method is average. there is a 80% chance for crossover, other wise one of the parents is passed on.\nMy elite count is 5 (1% of the population)\nThere is a 3% chance for a mutation for an individual, the range of the mutation is [-0.3, 0.3]</p>\n\n<p>My fitness function is f it self and my stopping condition is ABS(previous best fitness - current best fitness) &lt;= 10^(-21)</p>\n\n<p>You can find the code I wrote <a href="https://bitbucket.org/nocgod/ga-testing" rel="nofollow">here</a>.</p>\n\n<p>The problem is that it converges before it reaches even an approximate solution.</p>\n\n<p>What can I change in the solution approach so that the algorithm would converge on the maximum(f)?\n(This is not my algorithm, it\'s a reduction of a problem I have at work.)</p>\n', 'ViewCount': '49', 'ClosedDate': '2014-02-18T21:38:38.293', 'Title': 'Genetic algorithm fitness function', 'LastEditorUserId': '14811', 'LastActivityDate': '2014-02-19T12:52:37.320', 'LastEditDate': '2014-02-19T12:52:37.320', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14811', 'Tags': '<algorithms><optimization><heuristics><genetic-algorithms>', 'CreationDate': '2014-02-18T16:16:47.953', 'Id': '21776'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was <em>convinced</em> that my idea for a solution to sort $k$ sorted lists into one list would work with a 'variation' on MergeSort. I was told this would not work and had to use Heapsort, but didn't get any explanation or intuition behind it. (I believe the assumption is that each of the $k$ lists had size $\\frac{n}{k}$)</p>\n\n<p>Essentially, my intuition behind using Mergesort was that we have $n$ total elements, but all the steps below height $\\log k$ in our recursion tree had already been solved. So at height $\\log k$ each list is of size $\\frac{n}{k}$, so we perform $\\frac{n}{k}$ comparisons and $2\\frac{n}{k}$ inserts (to form a new list from the two $\\frac{n}{k}$ lists) $k$ times in total, which seems to be on the scale of $O(n)$. </p>\n\n<p>We are now one step up on the recursion tree with $\\frac{k}{2}$ lists and we will eventually perform $O(n)$ operations $\\log k$ times.</p>\n\n<p>Can anyone provide insight as to why this intuition is wrong? Or if right, how I should go about formally proving it?</p>\n", 'ViewCount': '97', 'Title': 'Given $k$ sorted lists, $O(n \\log k)$ complexity, Mergesort rather than Heapsort', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-18T21:28:30.520', 'LastEditDate': '2014-02-18T21:28:30.520', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '21788', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12906', 'Tags': '<algorithms><sorting>', 'CreationDate': '2014-02-18T20:13:25.730', 'Id': '21786'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a question regarding a graph algorithm which is as follows:</p>\n\n<p>Given a graph $G = (V,E)$ whose vertices are uniquely labeled $\\{1, 2,\\dots ,n\\}$ we want to determine the smallest integer $k$ such that deleting vertices $1$ through $k$ results in a graph whose largest connected component has at most $n/2$ vertices (when we delete a vertex we also delete all edges incident to that vertex). Give an $O(m \\log^* n)$ algorithm that determines $k$.</p>\n\n<p>The graph at the beginning could be disconnected, nor at the end need it be connected.\nSince $O(m \\log^*n)$ is almost linear, $\\log^*n$ grows very very slowly like in union-find data structure so this algorithm is almost linear time like union-find. I am trying to solve it through union find but it seems I am doing something wrong. Now I think that I should use union-find as a black box, but I ca'nt figure it out.</p>\n\n<p>This is a practice problem, not homework.</p>\n", 'ViewCount': '83', 'Title': 'Deleting vertices so that largest connected component has at most $n/2$ vertices', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-19T07:53:23.507', 'LastEditDate': '2014-02-19T07:53:23.507', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6688', 'Tags': '<algorithms><graphs>', 'CreationDate': '2014-02-19T02:02:46.737', 'Id': '21795'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we were using a priority queue(PQ) to implement Prim's algorithm. My understanding is that initially the weight of all vertices is set to $\\infty$. The weight of the starting vertex is then set to 0. All of the vertices are then inserted into the PQ. </p>\n\n<p>1) Does that mean that we can insert the vertices in any order into the PQ? </p>\n\n<p>2) Given that we can insert the vertices in any order, suppose we have a graph with the<br>\n$\\quad$following vertices a, b and c and the following weights w(a,$\\,$b) = 1, w(a,$\\,$c ) = 2. Once we set a.key = 0 $\\quad$and then extract a from PQ, we have b.key = 1 and c.key = 2. Given the answer to (1) was yes,my<br>\n$\\quad$understanding would be that in a binary tree representation of the heap, b would now be the root and c<br>\n$\\quad$would be a child of b. However, depending on the order in which the a, b and<br>\n$\\quad$c were added to the heap, c could be either the left or right child of b, right?</p>\n\n<p>3) Suppose now that the graph has vertices a, b, c, d and edges (a, b), (a, c), (a, d). Suppose they were<br>\n$\\quad$inserted into the heap in the order a, b, c, d. The binary tree representation of the heap should then be:       </p>\n\n<p>$\\quad\\quad$ a is the root, left(a) = b, right(a) = c, left(b) = d    </p>\n\n<p>$\\quad$ So, that would mean that the parent-child relationships do not correspond to the parent-child<br>\n$\\quad$ relationships in the original graph and they don't have to, right? </p>\n", 'ViewCount': '98', 'Title': "Prim's Algorithm - Building the Priority Queue", 'LastEditorUserId': '8639', 'LastActivityDate': '2014-02-19T15:22:41.730', 'LastEditDate': '2014-02-19T15:22:41.730', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<algorithms><graphs><spanning-trees>', 'CreationDate': '2014-02-19T05:32:02.400', 'FavoriteCount': '1', 'Id': '21801'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is my problem: I have a directed weighted graph with a substantial amount of vertices (few thousands), no cycles, in fact, it includes a starting node, a final node and an $m \\times n$ grid between them, where edges can be directed from the left to the right only. The weights of the edges depend on the path in which they are included (for example, if the path includes v.15, then the weights of several edges change).</p>\n\n<p>I tried to get all possible paths and then calculate their final sum post factum, but that turned out to be very inefficient method due to the number of paths. Is there an effective method which allows to find shortest paths in these kind of graphs?</p>\n', 'ViewCount': '74', 'Title': 'Finding shortest path in a graph when edge weights depend on the chosen vertices', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-19T09:41:29.690', 'LastEditDate': '2014-02-19T09:41:29.690', 'AnswerCount': '0', 'CommentCount': '13', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14837', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2014-02-19T09:31:23.923', 'Id': '21807'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been looking into diploid genetic algorithms for a while.</p>\n\n<p>Although, it seems like an implementation which includes diploid (dominant/recessive) genes is closer to the implementation that has been running successfully for billions of years in nature, <strong>does it offer any <em>real</em> advantages in GA?</strong></p>\n\n<p>In nature, 'Diploidness' of genes ensures that recessive genes are preserved, even when they are not expressed for generations: Just in case a scenario occurs where organisms having the genes would be more successful than the organisms with the dominant gene.</p>\n\n<p>In GA, by implementing elitism and bringing back organisms from past generations to the current population at random (perhaps based on a '<em>resurrection factor</em> ') should replicate the effects of diploidness, Right?</p>\n\n<p>Or is there some other advantage that diploid nature of genes offers?</p>\n\n<p>Please cite your sources, if possible.</p>\n", 'ViewCount': '26', 'ClosedDate': '2014-04-01T21:57:53.547', 'Title': 'Does using diploid (dominant/recessive) genes in genetic algorithm offer any advantage?', 'LastEditorUserId': '9479', 'LastActivityDate': '2014-02-25T03:14:55.333', 'LastEditDate': '2014-02-25T03:14:55.333', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<optimization><artificial-intelligence><genetic-algorithms>', 'CreationDate': '2014-02-19T14:41:52.503', 'Id': '21814'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $G=(V,E)$ be a directed graph. The <strong>connectivity</strong> of a graph is the defined as the cardinality of a smallest separator of $G$. A <strong>separator</strong> of $G$ is a subset $U$ of $V$, such that $G-U$ is not strongly connected.</p>\n\n<p>Why does the following algorithm compute the connectivity of a graph correctly?</p>\n\n<p>\\begin{equation}\n\\begin{split}\n&amp;\\text{Connectivity}(\\text{graph }G=(V,E)) \\\\\n&amp;\\;\\;\\;\\;01\\text{:}\\;\\;k=\\infty \\\\\n&amp;\\;\\;\\;\\;02\\text{:}\\;\\;\\text{for }i=1,\\ldots,|V| \\\\\n&amp;\\;\\;\\;\\;03\\text{:}\\;\\;\\{\\\\\n&amp;\\;\\;\\;\\;04\\text{:}\\;\\;\\;\\;\\;\\;\\text{for each }v\\in V\\\\\n&amp;\\;\\;\\;\\;05\\text{:}\\;\\;\\;\\;\\;\\;\\{\\\\\n&amp;\\;\\;\\;\\;06\\text{:}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{compute a minimum }v_i,v\\text{-seperator }U_{v_i,v}\\\\\n&amp;\\;\\;\\;\\;07\\text{:}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;k=\\min\\left\\{k,\\left|U_{v_i,v}\\right|\\right\\}\\\\\n&amp;\\;\\;\\;\\;08\\text{:}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{if }(i&gt;k+1) \\text{ return }k\\\\\n&amp;\\;\\;\\;\\;09\\text{:}\\;\\;\\\\\n&amp;\\;\\;\\;\\;10\\text{:}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{compute a minimum }v,v_i\\text{-seperator }U_{v,v_i}\\\\\n&amp;\\;\\;\\;\\;11\\text{:}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;k=\\min\\left\\{k,\\left|U_{v,v_i}\\right|\\right\\}\\\\\n&amp;\\;\\;\\;\\;12\\text{:}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{if }(i&gt;k+1) \\text{ return }k\\\\\n&amp;\\;\\;\\;\\;13\\text{:}\\;\\;\\;\\;\\;\\;\\}\\\\\n&amp;\\;\\;\\;\\;14\\text{:}\\;\\;\\}\n\\end{split}\n\\end{equation}</p>\n\n<p>More precisely, why can we return $k$ in line 08 (resp. 12) without concerning the other $u,v$-seperators?</p>\n', 'ViewCount': '41', 'Title': 'Proving the correctness of an algorithm, which computes the connectivity of a directed graph', 'LastActivityDate': '2014-02-19T23:39:35.870', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21828', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12502', 'Tags': '<algorithms><graph-theory><graphs><correctness-proof>', 'CreationDate': '2014-02-19T15:58:02.890', 'FavoriteCount': '1', 'Id': '21819'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>What is the problem in \u201cclosest pair of points algorithm\u201d if all points share the same x-coordinate or the same y-coordinate? and how the algorithm will change?</p>\n', 'ViewCount': '16', 'ClosedDate': '2014-02-20T11:13:49.043', 'Title': 'Closest pair of points on a line', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-20T09:33:33.917', 'LastEditDate': '2014-02-20T09:33:33.917', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '-1', 'OwnerDisplayName': 'Roy411', 'PostTypeId': '1', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-02-19T19:57:03.630', 'Id': '21838'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know that depth-first search can be used to produce a depth-first spanning tree, which classifies all edges as tree edges, forward edges, backward edges or cross edges. Are there any algorithms that make use of the depth-first spanning tree?</p>\n', 'ViewCount': '78', 'Title': 'Applications of Depth-First Spanning Tree', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-20T15:01:48.333', 'LastEditDate': '2014-02-20T11:08:14.503', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14872', 'Tags': '<algorithms><graphs><graph-traversal><spanning-trees>', 'CreationDate': '2014-02-20T07:12:38.657', 'Id': '21839'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '86', 'Title': 'minimizing the summed cardinality of set unions', 'LastEditDate': '2014-02-24T17:46:51.283', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14883', 'FavoriteCount': '1', 'Body': '<p>this optimization problem, I am working on, is kind of making me crazy. ;)</p>\n\n<p>Given is a list <code>o</code> of sets (with finite cardinality) of strictly positive integer values (Z>0), e.g.:</p>\n\n<pre><code>o_without_sizes =\n[ {1, 2, 3, 4}\n, {5, 6}\n, {2, 3, 4, 5}\n, {5, 6, 7}\n, {7, 8}\n. {9} ]\n</code></pre>\n\n<p>Every set has a name <code>n</code> (also in Z>0, but only for identification) and a fixed independent size value <code>s</code> (also in Z>0), e.g.:</p>\n\n<pre><code>type O = [(Name, Size, Values)]\no =\n[ (1, 2, {1, 2, 3, 4})\n, (2, 1, {5, 6})\n, (3, 2, {2, 3, 4, 5})\n, (4, 3, {5, 6, 7})\n, (5, 2, {7, 8})\n. (6, 1, {9}) ]\n</code></pre>\n\n<p>These sets are to be combined to unions <code>b</code> of a maximum size value sum <code>h (&gt;= max s, that means that no set has a size making it too big to fit into a single union)</code>, e.g. 4.</p>\n\n<p>The goal is to find the <code>b</code> so that the sum of cadinalities of the unions in it is as small as possible.\nhere is a bad <code>b</code>:</p>\n\n<pre><code>size:   3,  cardinality:   6,   sets: [1,2]  ,  values: [1,2,3,4,5,6]\nsize:   2,  cardinality:   4,   sets: [3]    ,  values: [2,3,4,5]\nsize:   3,  cardinality:   3,   sets: [4]    ,  values: [5,6,7]\nsize:   3,  cardinality:   3,   sets: [5,6]  ,  values: [7,8,9]\ncardinality sum:  16\n</code></pre>\n\n<p>and the optimum <code>b</code> for this example:</p>\n\n<pre><code>size:   4,  cardinality:   5,   sets: [3,1]  ,  values: [1,2,3,4,5]\nsize:   4,  cardinality:   3,   sets: [2,4]  ,  values: [5,6,7]\nsize:   3,  cardinality:   3,   sets: [5,6]  ,  values: [7,8,9]\ncardinality sum:  11\n</code></pre>\n\n<p>Until now I only implemented a naive brute force solution (Haskell code): <a href="http://lpaste.net/7204008959806537728" rel="nofollow">http://lpaste.net/7204008959806537728</a></p>\n\n<p>I was hoping to find a dynamic programming solution like it exists for the (Z>0) 0-1 knapsack problem, but did not yet succeed.\nIs my problem perhaps NP-hard? If so, is it many-one-reducible to SAT or something? Or is there a good approximation?</p>\n\n<p>Of course, if there exists a known efficient optimal algorithm, it would be awesome if you could enlighten me. :)</p>\n', 'Tags': '<algorithms><np-complete><optimization><dynamic-programming><np-hard>', 'LastEditorUserId': '14883', 'LastActivityDate': '2014-02-24T17:46:51.283', 'CommentCount': '2', 'AcceptedAnswerId': '21867', 'CreationDate': '2014-02-20T19:25:23.093', 'Id': '21857'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have thought about the most useful way of checking an array for 2 elements that sum to X.\nThe trivial solution is to check the sum of every element with every element, and the complexity of this solution is $O(n^2)$.</p>\n\n<p>My solution is:\nSay the array is A.\nIt's length is N.\nElements are from A[0] to A[N-1]</p>\n\n<p>Pseudo-Code is:</p>\n\n<pre><code>Check_Sum(A,left,right) {\n  mid &lt;-- floor( (left+right)/2 )\n\n  if(A[left]+A[right]=X)\n    return true\n\n  return Check_Sum(A,left,mid)||Check_Sum(A,mid,Right)\n}\n</code></pre>\n\n<p>My question is: Is the complexity of my solution equal to $O(n \\lg n)$?</p>\n", 'ViewCount': '247', 'Title': 'Checking if there are 2 elements in an array that sum to X in O(n lg n)', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-24T16:47:29.490', 'LastEditDate': '2014-02-25T20:26:49.787', 'AnswerCount': '7', 'CommentCount': '2', 'AcceptedAnswerId': '22042', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<algorithms><arrays><complexity>', 'CreationDate': '2014-02-20T20:19:07.547', 'Id': '21858'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Say $A'$ is the output of $\\mathrm{Bubblesort}(A)$ on an array of length $N$.\nTo prove that Bubblesort works, we have to prove that it always terminates and that\n$$A'[0]\\leq A'[1] \\leq \\dots \\leq A'[N-1].$$\nIs there anything else that needs to be proven to show that Bubblesort actually sorts?</p>\n\n<p>(I have found this question in a textbook about algorithms.)</p>\n", 'ViewCount': '170', 'Title': 'Proving the Bubblesort actually sorts', 'LastEditorUserId': '683', 'LastActivityDate': '2014-02-21T14:37:04.937', 'LastEditDate': '2014-02-21T14:08:46.187', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<algorithms><sorting><arrays>', 'CreationDate': '2014-02-21T12:53:59.497', 'Id': '21883'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '178', 'Title': 'Algorithm Request: "Shortest non-existing substring over given alphabet"', 'LastEditDate': '2014-02-21T18:13:37.423', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14918', 'FavoriteCount': '1', 'Body': "<p>I'm looking for an (efficient) algorithm to solve the following problem:</p>\n\n<blockquote>\n  <p>Given a string $S$ and a set of characters $M$, find the shortest string composed only of characters in $M$ that is <em>not</em> contained in $S$.</p>\n</blockquote>\n\n<p>Try as I might, I can't seem to map this problem to any of the standard CS string problems.</p>\n", 'Tags': '<algorithms><data-structures><strings><substrings>', 'LastEditorUserId': '14918', 'LastActivityDate': '2014-02-22T09:38:12.063', 'CommentCount': '3', 'AcceptedAnswerId': '21901', 'CreationDate': '2014-02-21T17:58:42.620', 'Id': '21896'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '216', 'Title': 'Parallel algorithm for finding the maximum in $\\log n$ time using $n / \\log n$ processors', 'LastEditDate': '2014-02-22T11:43:18.023', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '14907', 'FavoriteCount': '1', 'Body': "<p>We were presented in class with an algorithm for finding the maximum in an array in parallel in $O(1)$ time complexity with $n^2$ computers.</p>\n\n<p>The algorithm was:</p>\n\n<blockquote>\n  <p>Given an array A of length n:</p>\n  \n  <ol>\n  <li>Make a flag array B of length n and initialize it with zeroes with $n$ computers.</li>\n  <li>Compare every 2 elements and write 1 in B at the index of the minimum with $n^2$ computers.</li>\n  <li>find the index with the 0 in A with $n$ computers.</li>\n  </ol>\n</blockquote>\n\n<p>The lecturer teased us it could be done with $\\frac{n}{\\log n}$ computers and with $\\log n$ time complexity.</p>\n\n<p>After alot of thinking I couldn't figure out how to do it.\nAny idea?</p>\n", 'Tags': '<algorithms><search-algorithms><parallel-computing>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-22T11:43:18.023', 'CommentCount': '0', 'AcceptedAnswerId': '21911', 'CreationDate': '2014-02-21T21:02:53.517', 'Id': '21910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How can one select the proper number of parameters for a genetic algorithm to model a given system?</p>\n\n<p>For example, say you want to optimize production of cars, and you have 1,000 measurements of hourly efficiency at various tasks for each of 1,000 different employees.  So, you have 1,000,000 data points.  Most of these are likely to be weakly correlated to the overall efficiency of your factory, but not <em>so</em> weakly that you can say they are irrelevant with statistical confidence.  How do you go about picking inputs for your GA so that you don't have 1,000,000+ degrees of freedom, resulting in very slow convergence or no convergence at all?</p>\n\n<p>Specifically, what are the algorithms one could use to pre-select or selectively eliminate features?</p>\n\n<p>One approach I have used myself in this scenario is to evolve the parameter selection itself, so I might have parents like <code>{a,b,c}</code>, <code>{b,d,e,q,x,y,z}</code>, and so on. I would then mutate the children to add or drop features. This works well for a few dozen features. But the problem is that it is inefficient if there is a large number of degrees of freedom. In that case, you are looking at <code>10^n</code> combinations (in the example above, <code>10^1,000,000</code>), which makes some pre-filtering of features critical to get any kind of useful performance.</p>\n", 'ViewCount': '20', 'Title': 'Selection of parameters for genetic algorithm', 'LastActivityDate': '2014-02-24T18:15:42.303', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '21976', 'Score': '8', 'OwnerDisplayName': 'Ed Cottrell', 'PostTypeId': '1', 'OwnerUserId': '11946', 'Tags': '<genetic-algorithms>', 'CreationDate': '2014-02-05T21:37:40.020', 'Id': '21974'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '57', 'Title': 'A scoring approach to computer opponents that needs balancing', 'LastEditDate': '2014-03-22T15:34:07.950', 'AnswerCount': '2', 'Score': '13', 'OwnerDisplayName': u'Simon Andr\xe9 Forsberg', 'PostTypeId': '1', 'OwnerUserId': '14926', 'Body': '<p>Update: The (Java) code used for this approach has now been <a href="http://codereview.stackexchange.com/questions/45088/a-scoring-approach-to-computer-opponents">posted at Code Review</a>.</p>\n\n<hr>\n\n<p>This question is about an approach to computer opponents that I have created and are either currently being used, or are planned to be used, in several computer games.</p>\n\n<h3>Background</h3>\n\n<p>Last year, when trying to improve a computer opponent for a game called "Minesweeper Flags" <em>(short description: A turn-based multiplayer version of Minesweeper where you have to <strong>take</strong> more mines than your opponent)</em>, I strongly changed the way my algorithms worked. Instead of using an approach like if-else-if-else, I am using a set of "scorers" with specified weights to determine what the best move is.</p>\n\n<p>You might think that for a game like Minesweeper Flags, it\'s only about making moves that gives you the highest probability of taking a mine, but it\'s not that simple. Which move the computer will make usually depends on several features for that specific move in the current game state. Examples of features: </p>\n\n<ul>\n<li>What\'s the probability of this move scoring a mine?</li>\n<li>What\'s the probability of revealing anything to my opponent here?</li>\n</ul>\n\n<h3>Description of the system</h3>\n\n<p>The system basically works like this:</p>\n\n<ol>\n<li>"Pre-scorers": Some pre-analysis is done for the current game state (in terms of Minesweeper Flags, this is usually: Calculating all the probabilities)</li>\n<li>"Scorers": A set of ordinary scorers are asked to determine the score for each possible move, each scorer applies scores according to it\'s own criteria. The scorers can check the results of the pre-analysis that was made.</li>\n<li>The scores calculated in the above step is summed together and is set to be the score for a move.</li>\n<li>The moves are sorted according to their score and ranked so that all moves with the same score gets the same rank.</li>\n<li>"Post-scorers": The result of the above can be sent to "Post-scorers" that have the possibility to modify the scores of any fields in any way they want, according to the post-scorer\'s own rules.</li>\n</ol>\n\n<p>When combining a bunch of pre-scorers, scorers (with their weights) and post-scorers, it\'s becomes what I call a <strong>score configuration</strong>.</p>\n\n<h3>Example result</h3>\n\n<p>This is an example of scores having been applied to Minesweeper Flags. This is the map that was scored:</p>\n\n<p><img src="http://i.stack.imgur.com/aIRiz.png" alt="Minesweeper Flags map that was scored"></p>\n\n<p>And this is the output of an actual score configuration. It is showing the rank of the possible moves, where 1 is the best rank and has been highlighted in white:</p>\n\n<p><img src="http://i.stack.imgur.com/ArJG4.png" alt="Example output of scoring approach"></p>\n\n<p>Thanks to having written highly flexible code, this approach to AIs can be inserted into other games as well.</p>\n\n<h3>Advantages and disadvantages</h3>\n\n<p>Below are some advantages and disadvantages of this system that I can think of myself</p>\n\n<p><strong>Advantages</strong></p>\n\n<ul>\n<li>It\'s very easy to create a whole lot of different configurations for AIs.</li>\n<li>It is possible to use with Genetic Algorithms: Each scorer has an associated weight, the weight can become the gene.</li>\n<li>Using some tools, it is possible to check why a specific move was made and which scorers were mainly responsible for that move</li>\n<li>Using tools, it is possible to create a map of the overall score/rank of possible moves (like the screenshot above)</li>\n<li>By applying scores to the way the human plays, it is possible to create an "#AI_Mirror" which tries to make moves that it thinks the human would make</li>\n</ul>\n\n<p><strong>Disadvantages</strong></p>\n\n<ul>\n<li>It can be extremely difficult to adjust a score configuration "correctly", to make the AI play as good as possible.</li>\n</ul>\n\n<h3>Questions</h3>\n\n<ul>\n<li><p>Is the system I have built here widely known in the AI world? What would it be called in real AI terms?</p></li>\n<li><p>Does this approach make sense or is there a different approach that you would recommend?</p></li>\n<li><p>What ways are there that could make the process of tweaking a score configuration easier?</p></li>\n</ul>\n\n<p>Regarding the last question, I am aware of the possibility of using genetic algorithms, I am also mildly aware of <a href="http://artint.info/html/ArtInt_272.html" rel="nofollow">SARSA</a> (and I do think my scorers resembles that site\'s description of features with weights, but from my understanding that\'s not exactly what I have created here). I think that a problem with SARSA is that you don\'t know the reward until the game is over, the best move is often a move which doesn\'t give a reward (a mine) at all. Your current chances of winning depends on both the current score (how many mines you and your opponent have taken) and what the current map looks like.</p>\n', 'Tags': '<algorithms><artificial-intelligence><computer-games>', 'LastEditorUserId': '14926', 'LastActivityDate': '2014-03-22T15:34:07.950', 'CommentCount': '0', 'AcceptedAnswerId': '21982', 'CreationDate': '2014-02-12T19:44:49.750', 'Id': '21981'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '157', 'Title': 'Basic action for every data structure O(1)', 'LastEditDate': '2014-02-24T18:34:26.200', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Body': '<p>My lecturer for Algorithms said that most of the data structures I will encounter in the algorithms course I am taking have a basic action which is of O(1).</p>\n\n<p>Ex: Binary heap.</p>\n\n<p>Basic action is:</p>\n\n<blockquote>\n  <ol>\n  <li>Compare 2 childen.</li>\n  <li>Compare the "winner" with his parent.</li>\n  <li>Replace when needed.</li>\n  <li>Do 1-3 with the "winner", until and including the root.</li>\n  </ol>\n</blockquote>\n\n<p>How is O(1) even possible?</p>\n', 'ClosedDate': '2014-02-27T05:50:30.873', 'Tags': '<algorithms><data-structures>', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-02-24T19:28:48.693', 'CommentCount': '2', 'AcceptedAnswerId': '21997', 'CreationDate': '2014-02-24T18:17:38.903', 'Id': '21985'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In most implementations of genetic algorithms, the focus is on crossover and mutation. But somehow, most of them leave out diploid (dominant/recessive) nature of genes. As far as my (limited) understanding goes dominant/recessive nature of genes is a very important factor in deciding the actual characteristics of an organism.</p>\n\n<p>So my question is why is the diploid nature of genes left out of genetic algorithms in most implementation?</p>\n\n<p>Is it because:</p>\n\n<ul>\n<li>it doesn't provide much benefit </li>\n<li>it adds unnecessary complexity to an otherwise simple algorithm</li>\n<li>it's difficult to implement</li>\n</ul>\n\n<p>Or something else entirely? </p>\n", 'ViewCount': '40', 'Title': 'Why are diploid (dominant/recessive) genes not used widely in genetic algorithms?', 'LastActivityDate': '2014-02-24T18:17:57.267', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '11', 'OwnerDisplayName': 'Shayan RC', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<machine-learning><genetic-algorithms>', 'CreationDate': '2014-02-18T13:17:26.417', 'Id': '21991'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '46', 'Title': 'What are the state-of-the-art algorithms for pathfinding on a continuous map of the Earth?', 'LastEditDate': '2014-02-24T19:22:56.753', 'AnswerCount': '2', 'Score': '12', 'OwnerDisplayName': 'Deer Hunter', 'PostTypeId': '1', 'OwnerUserId': '14799', 'Body': "<p>Suppose I have got a solar-powered autonomous surface vessel somewhere in the fjords of Norway, supplied with a fairly recent set of maps, a GPS receiver, and no means of downlinking detailed commands from me. This vessel has to reach, say, the island of Hainan at the earliest possible moment.</p>\n\n<ul>\n<li>What are the <strong>deterministic</strong> algorithms for finding a maritime route on a globe?</li>\n<li><p>What is their time and memory complexity?</p></li>\n<li><p>Can I, for instance, use A* after transforming the map of the globe into a diagram with connected polygons (i.e. Delaunay triangulation on a sphere/ellipsoid) and what are other feasible approaches?</p></li>\n</ul>\n\n<p>Answers should ideally provide references to papers with discussion of the above-mentioned questions.</p>\n\n<p>As pointed out by <em>Rob Lang</em>, the algorithms must fit the usual criteria: in the absence of time constraints, lead to the shortest path between any two points on Earth's oceans and seas, or indicate pathfinding failure otherwise.</p>\n\n<p>There are interesting sub-topics here (trading pre-computation time/storage for online computations, providing slightly suboptimal routes before a deadline kicks in etc.), but these are ancillary to the main issue.</p>\n", 'Tags': '<artificial-intelligence><search-algorithms>', 'LastEditorUserId': '6980', 'LastActivityDate': '2014-02-24T19:22:56.753', 'CommentCount': '10', 'AcceptedAnswerId': '21994', 'CreationDate': '2014-02-14T05:02:33.373', 'Id': '21993'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '58', 'Title': 'Advantage of the Monte Carlo method over a regular periodic sampling', 'LastEditDate': '2014-02-27T10:10:16.087', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15001', 'Body': "<p>I am unclear on when to use the Monte Carlo random sampling method for algorithm design. The classic example that I keep seeing is using random points within some bounding rectangle to determine the area of some irregular figure. Wouldn't a regular periodic sampling provide more repeatable results for this application then using the Monte Carlo (random sampling) method?</p>\n", 'ClosedDate': '2014-02-27T07:44:07.457', 'Tags': '<algorithms><randomized-algorithms><monte-carlo>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-27T10:10:16.087', 'CommentCount': '1', 'AcceptedAnswerId': '22004', 'CreationDate': '2014-02-24T20:37:44.313', 'Id': '22002'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a list of intervals $[s_1, e_1], [s_2, e_2], \\ldots$, what's the most efficient way to determine if an interval $[a, b]$ can be covered by the intervals in the list?</p>\n", 'ViewCount': '52', 'ClosedDate': '2014-02-25T20:19:48.823', 'Title': 'Algorithm to determine if an interval is covered by a list of intervals?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-25T20:20:38.797', 'LastEditDate': '2014-02-25T20:20:38.797', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15029', 'Tags': '<algorithms><intervals>', 'CreationDate': '2014-02-25T14:27:16.193', 'Id': '22026'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When generating a board for <a href="http://en.wikipedia.org/wiki/Zobrist_hashing" rel="nofollow">Zobrist hashing</a>, why are the initial elements random? How can you detect changes to elements later if the initial values are random?</p>\n', 'ViewCount': '54', 'Title': 'Why is the initial state of Zobrist hashing random?', 'LastEditorUserId': '15038', 'LastActivityDate': '2014-02-25T19:37:57.503', 'LastEditDate': '2014-02-25T18:24:38.183', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22036', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15038', 'Tags': '<algorithms><artificial-intelligence><hash>', 'CreationDate': '2014-02-25T18:16:03.587', 'Id': '22033'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Reading this <a href="http://classes.soe.ucsc.edu/cmps102/Spring10/lect/17/SAT-3SAT-and-other-red.pdf" rel="nofollow">http://classes.soe.ucsc.edu/cmps102/Spring10/lect/17/SAT-3SAT-and-other-red.pdf</a>, I came to know that reducing a clause $C_i$ from a $SAT$ instance containing more than 3 literals to a $3-SAT$ instance is done this way,</p>\n\n<p>Suppose $C_1$ is $\\{x_1, x_2, x_3, x_4\\}$. It\'s equivalent representation in 3-literal clauses is,</p>\n\n<p>$C_{3-SAT} = \\{\\{x_1, x_2, y_1\\},\\{\\bar{y_1}, x_3,x_4\\}\\}$</p>\n\n<p>The issue lies here. Assume for $C_1$, all the literals are $False$ except for $x_2$.\nFor $C_{3-SAT}$, the first clause would indeed be $True$, but the boolean value for the second clause depends on the choice of $y_1$. What if we chose $y_1 = True$? then, $C_{3-SAT}$ will be $False$, yet $C_1$ is $True$, realizing a false reduction.</p>\n', 'ViewCount': '185', 'Title': 'Issue understanding the reduction of SAT to 3-SAT in poly time', 'LastActivityDate': '2014-02-26T13:45:34.337', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22053', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><np-complete><satisfiability><3-sat><boolean-algebra>', 'CreationDate': '2014-02-26T13:28:05.573', 'FavoriteCount': '1', 'Id': '22052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having an AI exam in two weeks, and I am still figuring out certain concepts and ideas, related to Bayesian Nets, Hidden Markov Chains, Conditional Random Fields and Neural Nets (yes it is all going to be tested and yes we have a text (AI - A Modern Approach), but no, we did not cover everything in class).</p>\n\n<p>I "know" a few things about the mathematical descriptions of all of those, but I know pretty much nothing about their usage or practical applications.</p>\n\n<p>Here are my questions (and I apologize for my naivety):</p>\n\n<ol>\n<li>What kind of machine learning algorithm classes do they belong to? Since they all need training, does it mean that they are all supervised learning algorithms?</li>\n<li>They all have an underlying structure that allows a graph to represent them, where directed edges denote dependencies between states. The probability of being in a state is computed as a conditional probability from ancestors of the state. Does that sound about right?</li>\n<li>In what kind of situation do you want to use which of the algorithms? Is it possible to some it up, or does it require subtle differentiation and expert-level knowledge?</li>\n<li>Why do Neural Nets get special treatment? I heard of many classes teaching Neural Nets, but I have heard of no such thing for the other guys.</li>\n</ol>\n', 'ViewCount': '52', 'Title': 'How are Bayesian Nets, Hidden Markov Chains, Conditional Random Fields and Neural Nets related?', 'LastActivityDate': '2014-02-28T16:11:53.907', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15080', 'Tags': '<machine-learning><artificial-intelligence><neural-networks><probabilistic-algorithms><markov-chains>', 'CreationDate': '2014-02-26T20:00:12.453', 'Id': '22062'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>The GSAT (Greedy Satisfiability) algorithm can be used to find a solution to a search problem encoded in CNF. I\'m aware that since GSAT is greedy, it is incomplete (which means there would be cases where a solution might exist, but GSAT cannot find it). From the following link, I learned that this can happen when flipping variables greedily traps us in a cycle such as I \u2192 I\' \u2192 I\'\' \u2192 I. </p>\n\n<p><a href="http://www.dis.uniroma1.it/~liberato/ar/incomplete/incomplete.html" rel="nofollow">http://www.dis.uniroma1.it/~liberato/ar/incomplete/incomplete.html</a></p>\n\n<p>I\'ve been trying quite hard to come up with an actual instance that can show this, but have not been able to (and could not find examples elsewhere). Any help would be much appreciated. Thanks :)</p>\n\n<p>P.S. I\'m not talking about "hard" k-SAT problems in which the ratio of variables to clauses approaches 4.3. I\'m just looking for a simple example, possibly involving the least number of variables and/or clauses required.</p>\n', 'ViewCount': '76', 'Title': 'GSAT incompleteness example', 'LastActivityDate': '2014-02-27T05:59:46.950', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15094', 'Tags': '<algorithms><satisfiability><greedy-algorithms><3-sat>', 'CreationDate': '2014-02-27T05:07:36.670', 'Id': '22079'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I read that determining the size of the maximum independent set (and also a clique of maximum size) is in P. The versions that find the actual solution are known to be NP-hard. </p>\n\n<p>With respect to finding clique size, you can sort the node degrees, decrement $i$ from $|V|$ to $0$, and each time check if you have $i$ elements of node degree $i$, pick the power set of those $\\geq i$ elements and verify the clique. However, picking the power set is exponential, and this algorithm would give you the solution itself. I have a hard time figuring out how you can construct an algorithm that decides the presence of a clique (or independent set) of a certain size in polytime, but doesn't give you the solution.</p>\n", 'ViewCount': '101', 'Title': 'Why is determining the size of a maximum independent set or a clique in P?', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-27T07:22:20.957', 'LastEditDate': '2014-02-27T07:22:20.957', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22083', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<algorithms><complexity-theory><graph-theory><graphs><time-complexity>', 'CreationDate': '2014-02-27T05:41:09.613', 'Id': '22080'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Are there dynamic programming examples that run in exponential time? Every example that I've seen so far constructs the top half of a matrix in a bottom-up fashion ($n^2$) from the base case and evaluates $n$ expressions to optimize each entry.</p>\n", 'ViewCount': '104', 'Title': 'Are there dynamic programming examples that run in exponential time?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-28T10:04:36.073', 'LastEditDate': '2014-02-28T08:38:41.003', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '22101', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<algorithms><runtime-analysis><dynamic-programming>', 'CreationDate': '2014-02-27T16:33:50.553', 'Id': '22094'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If we were to intuitively construct a lower bound for searching an element in a list $A$ containing $n$ integers, it would be in $\\Omega(n)$.</p>\n\n<p>But with the decision tree model, the number of leafs is $n$, so we conclude that the lower bound is $\\Omega(\\log{n})$.</p>\n\n<p>This is the same as finding the maximum element in a list. Intuitively, it is in $\\Omega(n)$, but with the decision tree model it is $\\Omega(\\log{n})$.</p>\n\n<p>Can someone help me understand this discrepancy ?</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '17', 'Title': 'Doubt in the correctness of decision tree models for constructing a lower bound', 'LastActivityDate': '2014-02-27T19:13:00.927', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22104', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><search-algorithms><decision-problem><trees><lower-bounds>', 'CreationDate': '2014-02-27T17:39:15.747', 'Id': '22099'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>In the words of (<a href="http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf" rel="nofollow">http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf</a>, section 17.2), "Each $f(x)$ can be interpreted as de\ufb01ning a hyperplane in $R^n$. Thus, tracing a path through the tree computes the intersection of the half-planes de\ufb01ned by the nodes touched by the path."</p>\n\n<p>I fail to visualize how path tracing is done? I would be glad to see it explained through the presentation of the path in a 2-dimensional space.</p>\n\n<p>I do understand that $x_1,x_2,...,x_n$ is a point in the $R^n$ dimensional space. But I don\'t get how Figure 17.1 in (<a href="http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf" rel="nofollow">http://www.cs.utah.edu/~suresh/5962/lectures/17.pdf</a>) helped in proving the lower bounds of Element Uniqueness as $\\Omega(n\\log n)$. I also don\'t get the implication of $\\#F$ being connected components; why can\'t they simply be called solutions?</p>\n\n<p>Unfortunately, reading online resources did not help me much understand the aforementioned concepts.</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '34', 'Title': 'Help in geometrically understanding "Linear Decision Trees"', 'LastEditorUserId': '15072', 'LastActivityDate': '2014-02-27T22:05:37.933', 'LastEditDate': '2014-02-27T19:22:01.350', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '22113', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><search-trees><lower-bounds>', 'CreationDate': '2014-02-27T19:09:05.800', 'Id': '22102'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am currently developing a math-physics program and I need to calculate if two vectors overlap one another, the one is vertical and the other is horizontral. Is there any fast algorithm to do this because what I came up so far, has a lot of repeats. Eg lets say we have vector V1((0,1),(2,1)) and a V2((1,0),(1,2)) where the first parenthesis is the coordinates starting and the second coordinates that the vector reaches. I want as a result to take that they overlap at (1,1)</p>\n\n<p>So far the only idea I came up is to ''expand'' each vector to a list of points and then compare the lists e.g for V1 its list would be (0,1) (1,1) (2,1)</p>\n", 'ViewCount': '22', 'Title': 'Algorithm for detection of overlaping between vectors', 'LastActivityDate': '2014-02-27T21:10:39.893', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22112', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13168', 'Tags': '<algorithms>', 'CreationDate': '2014-02-27T21:05:05.853', 'Id': '22111'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Taking <a href="http://www.geeksforgeeks.org/backtracking-set-7-suduku/" rel="nofollow">this</a> as an example:</p>\n\n<pre><code>bool SolveSudoku(int grid[N][N])\n{\n    int row, col;\n\n    // If there is no unassigned location, we are done\n    if (!FindUnassignedLocation(grid, row, col))\n       return true; // success!\n\n    // consider digits 1 to 9\n    for (int num = 1; num &lt;= 9; num++)\n    {\n        // if looks promising\n        if (isSafe(grid, row, col, num))\n        {\n            // make tentative assignment\n            grid[row][col] = num;\n\n            //RUN ARC CONSISTENCY HERE ......?\n\n            // return, if success, yay!\n            if (SolveSudoku(grid))\n                return true;\n\n            // failure, unmake &amp; try again\n            grid[row][col] = UNASSIGNED;\n\n            ////REMOVE ARC CONSISTENCY HERE ......?\n        }\n    }\n    return false; // this triggers backtracking\n}\n</code></pre>\n\n<p>Given the backtracking algorithm with CSP\'s, I would like to add ARC consistency to make it smarter.</p>\n\n<p><a href="http://www.codeproject.com/Articles/34403/Sudoku-as-a-CSP" rel="nofollow">For example</a>:</p>\n\n<blockquote>\n  <p>"When we want to assign the digit \'d\' to cell s1, we use assign(cells,\n  s, d).  ...but I also want to eliminate this possibility from its\n  peers  (like Forward Checking does, tell me something new!). If the\n  elimination causes one (or some) of the peers going down to only one \n  possibility, which we call d2, we want to assign d2 to it, and by\n  doing that,  eliminate d2 from all of its peer\'s peers, and that could\n  make another chain  reaction. This chain reaction is simply called\n  constraint propagation:  placing a constraint on one cell can cause\n  further constraints to be placed on  other cells."</p>\n</blockquote>\n\n<ul>\n<li>Can the process of arc propagation end up leading to a solution by\nitself or even a false solution? What is done in those cases?</li>\n<li>In the likely event the next recursive call on the next variable returns false (no values worked out for that variable), <strong>how do I\nundo all the changes the arc consistency did?</strong></li>\n</ul>\n', 'ViewCount': '21', 'Title': 'How is ARC consistency un-done after a recursive failure condition?', 'LastActivityDate': '2014-02-28T02:44:32.343', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '22119', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15116', 'Tags': '<algorithms><artificial-intelligence><recursion><constraint-programming><backtracking>', 'CreationDate': '2014-02-28T00:42:17.027', 'FavoriteCount': '3', 'Id': '22116'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have thought about it for a while, and I'm not really sure what is the best way to:</p>\n\n<blockquote>\n  <p>1.Implement a stack using 2 queues.</p>\n  \n  <p>2.Implement a queue using 2 stacks.</p>\n</blockquote>\n\n<p>I have only though about something trivial that takes O(n) time for dequeue and enqueue, and O(n) time for push and pop.</p>\n\n<p>Can I do better than that?\nAre there more efficient ways to do 1 and 2?</p>\n", 'ViewCount': '16', 'ClosedDate': '2014-02-28T13:23:58.910', 'Title': 'Impelementing a stack using a 2 queues, and a queue using 2 stacks', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-02-28T13:21:49.683', 'LastEditDate': '2014-02-28T13:21:49.683', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<algorithms><time-complexity><stack>', 'CreationDate': '2014-02-28T13:14:28.070', 'Id': '22131'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have two functions $S$ and $T$ which are interrelated and I want to find the asymptotic worst case runtime. The fact that they are interrelated is stumping me...</p>\n\n<p>How would I find the asymptotic runtime $S(n)$ and $T(n)$?</p>\n\n<p>$$\n\\begin{align*}\nS(n) &amp;= 2S(n/4) +  T(n/4) \\\\\nT(n) &amp;=  S(n/2) + 2T(n/2)\n\\end{align*}\n$$</p>\n', 'ViewCount': '55', 'Title': 'Asymptotic Runtime of Interrelated Functions', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-01T23:51:07.287', 'LastEditDate': '2014-03-01T23:51:07.287', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11204', 'Tags': '<algorithms><algorithm-analysis><asymptotics><search-algorithms><master-theorem>', 'CreationDate': '2014-03-01T02:34:54.417', 'Id': '22149'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How many times does the statement count in line 5 executes in terms of $n$?</p>\n\n<pre><code>1.  count=0; \n2.  for (i=1; i&lt;=n; i++) { \n3.  for (j=1; j&lt;=n; j*=2) { \n4.  for (k=1; k&lt;=j; k++) {\n5.        count = count + 1;\n6.      }\n7.    }\n8.  }\n</code></pre>\n\n<p>For the loop in line 3, we can list the numbers for $j$ as, $2^0,2^1,...,2^{\\log n}$\nTherefore, we can refer to the iterations of the exponents as $r$. Doing so would lead to the summations below for counting the number of executions.</p>\n\n<p>$\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}\\sum_{k=1}^{j}1 =\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}j $</p>\n\n<p>I am stuck here, because $\\sum_{r=0}^{\\log n}$ depends on $j$ but I am not sure how to incorporate them.</p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '20', 'Title': 'The number of executions of the count statement; how many?', 'LastActivityDate': '2014-03-01T05:18:02.957', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22154', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2014-03-01T04:34:54.633', 'Id': '22150'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '126', 'Title': 'Selecting random points at general position', 'LastEditDate': '2014-03-01T22:33:17.750', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '1', 'Body': '<p>How will you find a random collection of $n$ points in the plane, all with integer coordinates in a specified range (e.g. -1000 to 1000), such that no 3 of them are on the same line?</p>\n\n<p>The following algorithm eventually works, but seems highly inefficient:</p>\n\n<ol>\n<li>Select $n$ points at random.</li>\n<li>Check all $O(n^3)$ triples of points. If any of them are on the same line, then discard one of the points, select an alternative point at random, and go back to 2.</li>\n</ol>\n\n<p>Is there a more efficient algorithm?</p>\n', 'Tags': '<algorithms><computational-geometry><randomized-algorithms>', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-03-01T22:50:12.840', 'CommentCount': '1', 'AcceptedAnswerId': '22171', 'CreationDate': '2014-03-01T20:49:34.203', 'Id': '22167'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>consider the pseudo code for calculating failure function:\nI partial understand the algorithm.\nKNUTH-MORRIS-PRATT FAILURE (P)</p>\n\n<p>Input:    Pattern with m characters\nOutput: Failure function f for P[i . . j]</p>\n\n<pre><code>i \u2190 1\nj \u2190 0\nf(0) \u2190 0\nwhile i &lt; m do\n    if P[j] = P[i]\n        f(i) \u2190 j +1\n        i \u2190 i +1\n        j\u2190 j + 1\nelse if j!=0\n     j \u2190 f(j - 1)\nelse\n    f(i) \u2190 0\n    i \u2190 i +1\n</code></pre>\n\n<p>The j in above code signifies the length of longest equal perfect prefix and suffix. So when P[i]==P[j], j is increased by 1, signifying that prefix suffix length has increased by 1(by appending P[i] to suffix and P[j] to prefix).</p>\n\n<p>But when P[i]!=P[j] and j!=0, why does the algo assign f(j-1) to j. What does that signify?</p>\n\n<p>I felt that when P[i]!=P[j], then we can't use the previous j value, so we have to assume lps[i]=0 and compare all possible prefixes to all possible suffixes and then find the longest prefix suffix match. I realise that the algorithm's way of dealing with P[i]!=P[j] is more efficient O(string len), but I just can't understand how it works. </p>\n", 'ViewCount': '38', 'Title': 'O(pattern_length) failure function in kmp algorithm', 'LastActivityDate': '2014-03-02T06:13:21.870', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15186', 'Tags': '<algorithms><algorithm-analysis><strings>', 'CreationDate': '2014-03-02T06:13:21.870', 'FavoriteCount': '1', 'Id': '22182'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My question is very similar to <a href="http://cs.stackexchange.com/questions/1825/maximum-enclosing-circle-of-a-given-radius">another</a> solved question. As the title indicates, the major difference in my question is that I need to find the <strong>convex polygon</strong> that encloses the maximum number of points with a given <strong>area</strong> (<em>instead</em> of finding the <strong>circle</strong> that encloses the maximum number of points with a given <strong>radius</strong>).</p>\n\n<p>So my question is as the following:</p>\n\n<blockquote>\n  <p>Given $n$ points $p_1,\\dots,p_n$ in the plane and an area $A$, find a convex polygon $\\mathcal{P}$ whose area is $\\le A$ and that contains as many of the $n$ points as possible.</p>\n</blockquote>\n\n<p>I already checked on the <a href="http://en.wikipedia.org/wiki/Minimum_bounding_box_algorithms" rel="nofollow">Minimum bounding box algorithms</a> and <a href="http://en.wikipedia.org/wiki/Convex_hull_algorithms" rel="nofollow">Convex hull algorithms</a>. I also found some <a href="http://alienryderflex.com/smallest_enclosing_polygon/" rel="nofollow">implementations</a> for Minimum bounding polygon. However, those previous algorithms don\'t consider the concept of determining the convex polygon that encloses <strong>as many points as possible</strong> of a given set of points and <strong>a given area</strong>. Finally I invented an approach to solve the problem but it doesn\'t work in some cases.</p>\n\n<p>The idea of my approach is to start by finding <strong>the minimum bounding box</strong> for all the points. If the resulting polygon\'s area is greater than the given area, I cut off one of the edge points according to some technique, then I re-compute the minimum bounding box and test the area. I keep looping until I reach the desired area. I won\'t get into details with my approach as it\'s not working properly. </p>\n\n<p><strong>EDIT</strong></p>\n\n<p>As a simplified solution, finding <strong>any rectangle</strong> (it doesn\'t have to be axis-aligned) that encloses the maximum number of points with a given area would be very beneficial and works as a temporary solution for my problem. </p>\n', 'ViewCount': '101', 'Title': 'Maximum Enclosing Convex Polygon of a Given Area', 'LastEditorUserId': '15187', 'LastActivityDate': '2014-03-04T00:28:51.230', 'LastEditDate': '2014-03-04T00:28:51.230', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15187', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-03-02T07:18:47.567', 'Id': '22183'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '34', 'Title': 'Barnes-Hut algorithm and recursion limit', 'LastEditDate': '2014-03-02T09:38:00.273', 'AnswerCount': '0', 'Score': '1', 'OwnerDisplayName': 'user3058846', 'PostTypeId': '1', 'OwnerUserId': '15188', 'Body': '<p>I\'m running the <a href="http://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation" rel="nofollow">Barnes-Hut simulation algorithm</a> for an $n$-body simulation.</p>\n\n<p>If while distributing each particles to their corresponding nodes, two particles comes closer to a level smaller than the machine precision (or $\\Delta d \\to 0$), how is the situation to be treated? Should the recursive distribution stop and add both of them to a single node?\nThis happens because of treating particles as point objects rather than 2D objects, yet imagine a machine having a precision level of 3 decimal points (millimeters in the SI system) and the bodies came closer to $10^{-6}$ (micro) level.</p>\n', 'ClosedDate': '2014-03-02T19:03:05.587', 'Tags': '<algorithms><recursion>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-02T09:38:00.273', 'CommentCount': '2', 'CreationDate': '2014-03-01T18:44:27.993', 'Id': '22184'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Minimizing deterministic finite automata (DFAs) is a problem that has been thoroughly studied in the literature, and several algorithms have been proposed to solve the following problem: \nGiven a DFA $\\mathscr{A}$, compute a corresponding minimal DFA accepting the same language as $\\mathscr{A}$.\nMost of these algorithms run in polynomial time.</p>\n\n<p>However, I wonder whether the decision variant of this problem - "given a DFA $\\mathscr{A}$, is $\\mathscr{A}$ minimal?" - can be solved more efficiently than actually computing the minimal automaton.\nObviously, this can also be done efficiently by running for example <a href="http://en.wikipedia.org/wiki/DFA_minimization#Hopcroft.27s_algorithm">Hopcroft\'s partition-refinement algorithm</a> and then deciding whether all partitions contain precisely one state.</p>\n\n<p>As Yuval Filmus suggests <a href="http://cs.stackexchange.com/a/3046/7486">in his answer</a>, the decidability variant can be solved faster, possibly by using the standard algorithms.\nUnfortunately, I cannot see how (I hope I am not missing an obvious point here).</p>\n\n<p>Yuval points out in the comments here that the best known algorithms (like the one above) run in time $\\mathcal{O}(n \\log n)$ for constant-sized alphabets. Therefore, I am not only interested in asymptotically significant gains in runtime, as these seem rather unlikely. What bothers me most is that I cannot imagine any "shortcut" that might be drawn from the fact that we are only interested in a yes-no-answer - not even a shortcut that allows for saving an asymptotically negligible amount of time. I feel that every sensible algorithm that decides the minimality of a DFA would have to actually minimize the DFA and see if anything changes during the process. </p>\n', 'ViewCount': '121', 'Title': 'How fast can we decide whether a given DFA is minimal?', 'LastEditorUserId': '7486', 'LastActivityDate': '2014-03-02T20:05:21.393', 'LastEditDate': '2014-03-02T18:51:48.217', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7486', 'Tags': '<algorithms><formal-languages><finite-automata>', 'CreationDate': '2014-03-02T12:41:01.930', 'FavoriteCount': '2', 'Id': '22191'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '482', 'Title': 'How is this problem related to the study of algorithms and big O notation?', 'LastEditDate': '2014-03-02T18:06:17.933', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '15202', 'FavoriteCount': '2', 'Body': "<p>I'm taking a graduate computer science course on algorithms and analysis. The current subject is big O notation and recursion. How is the following problem related to the study of algorithms, recursion, and big O notation? I know and understand the solution to the problem, but I just don't see how this is relevant to the subject matter.</p>\n\n<p><em>Given an $x$, show that $x^{62}$ can be computed with only eight multiplications (A general algorithm can not accomplish it).</em></p>\n", 'Tags': '<algorithms><asymptotics>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-03T10:57:00.337', 'CommentCount': '1', 'AcceptedAnswerId': '22201', 'CreationDate': '2014-03-02T17:56:54.607', 'Id': '22200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can we add new words to DFA, maintaining minimality?</p>\n\n<p>Maybe, you can advise some completely different approach to solve the following problem:</p>\n\n<blockquote>\n  <p>A program must process two types of queries:</p>\n  \n  <ol>\n  <li>Add new word to the language.</li>\n  <li>Check if some word of the language occurs in the input string as a substring.</li>\n  </ol>\n  \n  <p>Time and space complexity should be as low as possible.</p>\n</blockquote>\n\n<p>The only solution that I know so far is to rebuild an automaton from scratch every time, using Aho-Corasick algorithm, but, you know, it\'s really-really inefficient.</p>\n\n<p>I\'ve performed some search on Internet and found some papers like <a href="http://acl.ldc.upenn.edu/J/J00/J00-1002.pdf" rel="nofollow">"Incremental Construction of Minimal Acyclic Finite-State Automata"</a>, but, unfortunately, algorithm described there produces and maintains just a minimal trie (not a complete FSM with suffix links, like an automaton produced by Aho-Corasick algo).</p>\n', 'ViewCount': '66', 'Title': 'Adding words to DFA, maintaining minimality', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-03T15:22:33.553', 'LastEditDate': '2014-03-03T10:08:34.447', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8991', 'Tags': '<algorithms><finite-automata>', 'CreationDate': '2014-03-03T10:06:53.027', 'FavoriteCount': '1', 'Id': '22215'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've written a genetic algorithm (GA) that solves a 7-dimensional optimisation problem.  All seven variables are floating point numbers.  The problem is that the entire population seems to converge to very nearly the same point in the solution space within about 20 generations, even if I increase the population size by 10x.</p>\n\n<p><strong>Attempted solutions</strong></p>\n\n<p>Starting with a parent population $\\vec{x}^{(j)}$, where $j$ is the individual's index in the population, I take the best 10% (i.e. highest fitness scores) to reproduce.  First I perform cross-over producing 2 children for every randomly selected pair of parents.  The parents can be re-used (is that a problem?).  About 5% of the children get mutated randomly.</p>\n\n<p>I've tried two variations of cross-over but both have the same problem:</p>\n\n<ol>\n<li><p>Calculating a 7 element weights vector $[w_i]$ and calculating the children's elements as $c_i^{(1)}=w_i x_i^{(1)} + (1-w_i)x_i^{(2)}$ and $c_i^{(2)}=(1-w_i)x_i^{(1)} + w_i x_i^{(2)}$.  The parents ith elements are $x_i^{(1)}$ for parent 1 and $x_i^{(2)}$ for parent 2.  Each weight is a sample from a uniform random variable in $[0.0;1.0)$.</p></li>\n<li><p>Confining the weights $w_i$ to be either $0.0$ or $1.0$, i.e. randomly exchanging elements of the two parents genomes to create the children.</p></li>\n</ol>\n\n<p>The mutation operation consists of randomly choosing one of the child's 7 elements and adding some Gaussian noise to it.  The standard deviation of that noise differs for each element since the elements have different physical units.</p>\n\n<p>Afterwards, I evaluate the fitness of the children and keep the best few hundred or thousand (setting that I choose at the start of the algorithm) out of the combined population of parents and children to give the next generation.  Note that the parents do not get mutated, especially since I want to preserve the best from the parent generation in case none of the children improve on it.</p>\n\n<p>I've tried population sizes from 1024 to 20480 and have also tried increasing the probability of mutating a child from 5% to 50% but I still have the problem that all the individuals in the population become very similar within the first 20 or so iterations.  Please advise on what I'm doing wrong.</p>\n\n<p>I should point out that the algorithm, despite this problem, does get fairly close to the optimal solution.  I know this because the quantity to maximise is the correlation between between two things (no more details, sorry) and I can get to about 0.94 (the maximum physically possible is always 1.0).  However, I am concerned that the GA is not covering enough of the solution space, causing it to miss the global maximum.</p>\n\n<p><strong>My questions</strong></p>\n\n<ol>\n<li>Are either of the above cross-over methods correct?</li>\n<li>Is it ok to re-use parents in cross-over?  Stated another way, is polygamy a good idea in this algorithm or should I change that part to ensure that no parent gets used more than once?</li>\n<li>Should I mutate the parents as well?</li>\n<li>What should I do with the 90% of parents that did not get used in the reproduction?</li>\n</ol>\n", 'ViewCount': '72', 'Title': 'How to stop genetic algorithm population converging to a single value', 'LastActivityDate': '2014-03-03T21:17:23.897', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22238', 'Score': '2', 'OwnerDisplayName': 'chippies', 'PostTypeId': '1', 'OwnerUserId': '15228', 'Tags': '<optimization><genetic-algorithms>', 'CreationDate': '2014-02-19T15:05:28.010', 'Id': '22216'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Lets say you have a data model that consists of a 2D grid of integer points. This grid is sparsely populated and boundless in x and y (up to the max of a 32-bit integer).</p>\n\n<p>What is the best way to index these points in order to have an optimised lookup on an arbitrary (x,y) coordinate? Is an O(1) lookup solution possible?</p>\n', 'ViewCount': '105', 'Title': 'What is the best way to index lookups on a 2D array of integers that is boundless in x and y?', 'LastActivityDate': '2014-03-11T19:28:28.083', 'AnswerCount': '5', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15254', 'Tags': '<algorithms><optimization><databases><data-sets>', 'CreationDate': '2014-03-04T02:39:54.287', 'Id': '22251'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a sphere (radius R) on a rectilinear grid. Some cells intersect the edge of that sphere, call them \'edge cells\'. Designate a given cell by indices [i,j,k] which refer to the lowest-index vertex of that cell (i.e. the inner-most cell, in the [+x,+y,+z] octant would be designated as [0,0,0]). For any given \'edge cell\', how can I calculate what fraction of its volume is within the sphere of radius R?</p>\n\n<p>In this figure, the purple surface is the surface of the sphere passing through some arbitrary cell, with origin [x0,y0,z0]. I want to know what fraction of the cell\'s volume is within this surface.</p>\n\n<p><img src="http://i.stack.imgur.com/vF22q.png" alt="enter image description here"></p>\n\n<p>I think \'edge cells\' can be calculated by finding the cells whose vertex [i,j,k] is at a radius R.</p>\n\n<p>I think there should be an analytic formula to calculate the volume fraction, but I can\'t figure out how to setup the integral. Any help would be greatly appreciated!</p>\n', 'ViewCount': '19', 'ClosedDate': '2014-03-06T12:01:43.700', 'Title': 'fraction of volume of a rectilinear grid cell within some radius of the origin', 'LastActivityDate': '2014-03-04T17:36:17.757', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15276', 'Tags': '<algorithms><computational-geometry><simulation>', 'CreationDate': '2014-03-04T17:36:17.757', 'Id': '22276'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Basically, I using an algorithm called 'miranda' to look at miRNA targets and it only runs on a single thread. It compares everything in one file against everything in another file, produces a file as an output and runs off of the command line in terminal. The process took roughly 20 hours to create the output file.</p>\n\n<p>I was advised by my supervisor that if i split one of the files up into say 4 equally sized parts, and ran them in four separate terminal windows this would decrease the overall time it took for the process to be completed.</p>\n\n<p>I found that when I was using a single terminal window, the process would take up about 100-120% of the CPU. However, when running four terminal windows, each individual process only takes between 30-40% of the CPU.</p>\n\n<p>How much effect does splitting the file up like this have in the overall time it takes to run the process? Although I split it across four threads, will the effect only be an increase in speed of about 1.5 times?</p>\n", 'ViewCount': '44', 'Title': 'Does splitting a process across 4 terminal windows decrease the time it takes?', 'LastActivityDate': '2014-03-05T18:02:23.337', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9799', 'Tags': '<algorithms><cpu-pipelines><threads>', 'CreationDate': '2014-03-04T17:57:45.533', 'Id': '22277'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I understand the basis of A* as being a derivative of Dijkstra, however, I recently found out about D*. From Wikipedia, I can understand the algorithm. What I do not understand is why I would use D* over Dijkstra. To my understanding, Dijkstra gives a best path and D* works backwards from the end goal, but unlike A* it seems to do many calculations, so it doesn't seem as efficient.</p>\n", 'ViewCount': '116', 'Title': 'Why choose D* over Dijkstra?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-05T09:28:49.120', 'LastEditDate': '2014-03-05T07:08:16.793', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22299', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '15288', 'Tags': '<algorithms><graphs><search-algorithms><efficiency>', 'CreationDate': '2014-03-05T00:30:16.657', 'Id': '22284'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been working on a challenge i found on the internet. It is as follows:</p>\n\n<blockquote>\n  <p>You\'ve stumbled onto a significant vulnerability in a commonly used cryptographic library. It turns out that the random number generator it uses frequently produces the same primes when it is generating keys.</p>\n  \n  <p>Exploit this knowledge to factor the (hexadecimal) keys below, and enter your answer as the last six digits of the largest factor you find (in decimal).</p>\n  \n  <p>Key 1: 1c7bb1ae67670f7e6769b515c174414278e16c27e95b43a789099a1c7d55c717b2f0a0442a7d49503ee09552588ed9bb6eda4af738a02fb31576d78ff72b2499b347e49fef1028182f158182a0ba504902996ea161311fe62b86e6ccb02a9307d932f7fa94cde410619927677f94c571ea39c7f4105fae00415dd7d</p>\n  \n  <p>Key 2: \n   2710e45014ed7d2550aac9887cc18b6858b978c2409e86f80bad4b59ebcbd90ed18790fc56f53ffabc0e4a021da2e906072404a8b3c5555f64f279a21ebb60655e4d61f4a18be9ad389d8ff05b994bb4c194d8803537ac6cd9f708e0dd12d1857554e41c9cbef98f61c5751b796e5b37d338f5d9b3ec3202b37a32f</p>\n</blockquote>\n\n<p>These seem to be common RSA 1024-bit keys.</p>\n\n<p>My approach to the problem was to implement <a href="http://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm" rel="nofollow">Pollard\'s rho algorithm</a> to find factors, then once a factor is found, try dividing the decimal form of the keys by that factor until it is not divisible anymore. Iterate. </p>\n\n<p>Now, i used Pollard\'s rho and tried to divide until the key is not divided anymore because of the information the problem gave: the keys are not completely random. </p>\n\n<p>But here comes the question: assuming the algorithm generates two primes and multiplies them to get a co-prime, which is the key, the low randomness doesn\'t help much, does it? I mean, even if both keys share a common factor, finding it the first time would take the regular-impractical-exponential time.</p>\n\n<p>That seems to be the case, as my Python algorithm is running for about 5 hours now and has not found any factor to the second key, which i decided to start with.</p>\n\n<p>As it is a challenge, i assume there is a practical way of finding the answer. \nSo what im i doing wrong? Is just the algorithm choice wrong, as Pollard\'s rho is intended mainly for integer with small factors? Is my assumption that i can only use the lack of randomness after i find the first of the four factor, to then try to break the other key with the same factor, wrong?</p>\n\n<p>I would like if someone could just point me in right direction, instead of just giving the answer. Thank you. </p>\n', 'ViewCount': '33', 'Title': 'Finding prime factors of non-random key generator', 'LastActivityDate': '2014-03-05T05:41:29.360', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22291', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15293', 'Tags': '<algorithms><cryptography><randomness><primes>', 'CreationDate': '2014-03-05T03:22:05.833', 'Id': '22289'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm having trouble thinking about the following. If we have two machines 1 and 2 that evenly split a set of data points, does $k$-means separately, then averages the result, does this agree with just $k$-means on one machine?</p>\n\n<p>To be more specific, consider the following routine:</p>\n\n<blockquote>\n  <ol>\n  <li>Split an input dataset evenly between 1 and 2.</li>\n  <li>Initialize $k$ centroids that are the same on both 1 and 2.</li>\n  <li>Use $k$-means algorithm to find updated centroids.</li>\n  <li>Find a new set of $k$ centroids by averaging the corresponding centroid from 1 and 2. </li>\n  <li>Loop over 2. </li>\n  </ol>\n</blockquote>\n\n<p>Will this agree with just running $k$-means? I think that it is the averaging part that might work, but then I'm not sure if this routine finishes running. </p>\n", 'ViewCount': '34', 'Title': 'parallelizing $k$-means', 'LastActivityDate': '2014-03-05T07:58:00.607', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15297', 'Tags': '<algorithms><machine-learning><parallel-computing>', 'CreationDate': '2014-03-05T06:16:31.567', 'Id': '22292'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've implemented a program for computing eigenvectors of some random, symmetric, $N$x$N$ matrix using the power method.  I have found difficulty in calculating all $N$ eigenvectors consistently, almost every time the algorithm fails to converge for all $N$.  The Wikipedia page on the power method tells me this algorithm is not guaranteed to converge for all $N$ eigenvectors, can someone suggest a way for me to encourage convergence, at least in a majority of the cases?  Is this possible?  If not, can someone suggest a better algorithm for computing eigenvectors?</p>\n", 'ViewCount': '44', 'Title': 'Power method to calculate eigenvectors', 'LastActivityDate': '2014-03-06T05:48:26.150', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15331', 'Tags': '<algorithms><linear-algebra><matrices>', 'CreationDate': '2014-03-06T05:02:27.497', 'Id': '22326'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let an integer n be given. Write the integers from 1 to n in binary notation successively from left to right. In the resulting string consisting of zeros and ones, choose a palindrome substring of maximal length. It is required to find the length of this substring. For example, if n=5, we can get a string which is 1 10 11 100 101. And we can get the longest substring that is palindromic: 11011 or 01110. So the answer to the question is 5. The n can be very big. Are there any good algorithm to solve this problem? Thanks in advance!</p>\n', 'ViewCount': '53', 'Title': 'Binary Palindrome', 'LastActivityDate': '2014-03-06T11:42:28.540', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9042', 'Tags': '<algorithms>', 'CreationDate': '2014-03-06T08:20:14.780', 'FavoriteCount': '1', 'Id': '22332'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '248', 'Title': 'QuickSort Dijkstra 3-Way Partitioning: why the extra swapping?', 'LastEditDate': '2014-03-18T10:08:54.497', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15391', 'FavoriteCount': '1', 'Body': u'<p><img src="http://i.stack.imgur.com/qeA2F.png" alt="Algorithm"></p>\n\n<p>Given the algorithm above (taken from the <a href="https://d396qusza40orc.cloudfront.net/algs4partI/slides/23Quicksort.pdf" rel="nofollow">slides</a> (p. 35) of the <a href="http://coursera.org" rel="nofollow">Coursera</a> course \u201cAlgorithms Part I\u201d by Robert Sedgewick and Kevin Wayne), look at the scenario where i is at "X", the following happens:</p>\n\n<p><strong>Scenario:</strong> i -> "X", "X" > "P"</p>\n\n<pre><code>1. swap("X", "Z"), gt--;   // the value at i is now "Z", which is still &gt; "P"\n2. swap("Z", "Y"), gt--;   // the value at i is now "Y", which is still &gt; "P"\n3. swap("Y", "C"), gt--;    // Now we finally get a value at i "C" which is &lt; "P"\n// Now we can swap values at i and lt, and increrement them\n4. swap("P", "C"), i++, lt++;\n</code></pre>\n\n<p>Why don\'t we just decrement gt until gt points to a value that is &lt; the value at lt ("P" in this case), then we swap this value with the value at i. This will save swapping operations.</p>\n\n<p>So if we do that for the scenario mentioned above, we\'ll do:</p>\n\n<pre><code>1. gt--\n2. gt--\n3. swap("X", "C"), gt--;   \n// Now we can swap values at i and lt, and increrement them\n4. swap("P", "C"), i++, lt++;\n</code></pre>\n\n<p>Is this excessive swapping needed for the algorithm? does it improve performance in some way?\nIf it does improve performance, how? </p>\n\n<p>If it doesn\'t affect performance, please give a proper explanation or a proof as to why this it does not affect performance. </p>\n\n<p>Also, would the second method I mentioned affect performance in any way? please explain why.</p>\n\n<p>P.S. "Affect performance" as used above means either improve/degrade performance.</p>\n', 'Tags': '<algorithms><sorting><quicksort>', 'LastEditorUserId': '133', 'LastActivityDate': '2014-03-24T05:53:55.407', 'CommentCount': '7', 'AcceptedAnswerId': '22395', 'CreationDate': '2014-03-08T05:27:19.077', 'Id': '22389'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The Stable Roommates Problem matches 2n participants into n sets of roommates based off of each participant's list of preferences. I was wondering if there was a variant of this problem where the number of roommates is different. For example, matching 10n participants into n sets. Thanks for the help.</p>\n\n<p>Edit: The Hospital Resident problem is also similar to this. Each hospital can take a certain number of residents. The problem is the residents list their preferences by hospital instead of other residents.</p>\n", 'ViewCount': '31', 'Title': 'variant of the stable roommates problem', 'LastEditorUserId': '15410', 'LastActivityDate': '2014-03-08T06:10:34.393', 'LastEditDate': '2014-03-08T06:10:34.393', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15410', 'Tags': '<algorithms><matching>', 'CreationDate': '2014-03-08T05:35:54.230', 'Id': '22391'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is a shop which consists of N items and there are M buyers. Each buyer wants to buy a specific set of items. However, the cost of all transactions is same irrespective of the number of items sold. So, the shopkeeper needs to maximize the number of buyers. The buyers will buy only if all the items are being sold. Items are unique. All items need not be sold.</p>\n\n<p>So, basically, we have a bipartite graph. We need to find a set of edges which maximize the number of nodes on Buyer vertex set such that each node on item set has only one edge. Any suggestions?</p>\n', 'ViewCount': '28', 'Title': 'How to maximize the number of buyers in a shop?', 'LastActivityDate': '2014-03-08T15:42:28.667', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15420', 'Tags': '<graph-theory><greedy-algorithms><bipartite-matching>', 'CreationDate': '2014-03-08T15:42:28.667', 'Id': '22399'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am wondering what is the difference between Bottleneck Travelling Salesman Problem and normal Travelling Salesman Problem?</p>\n\n<p>Thank you</p>\n', 'ViewCount': '43', 'ClosedDate': '2014-03-11T06:27:58.377', 'Title': 'Difference between BTSP and TSP', 'LastActivityDate': '2014-03-09T15:09:06.390', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '22431', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15429', 'Tags': '<algorithms><traveling-salesman>', 'CreationDate': '2014-03-08T20:06:43.840', 'Id': '22409'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '72', 'Title': 'Approximation algorithms for Euclidean Traveling Salesman', 'LastEditDate': '2014-04-13T09:51:49.107', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15429', 'FavoriteCount': '1', 'Body': "<p>I am trying to find a way to solve Euclidean TSP in a polynomial time. I looked at some papers but I couldn't decide which one is better. What is the general approximation algorithm for solving this problem in polynomial time?</p>\n", 'ClosedDate': '2014-04-13T09:54:22.080', 'Tags': '<algorithms><reference-request><approximation><traveling-salesman>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-13T09:51:49.107', 'CommentCount': '5', 'CreationDate': '2014-03-09T18:25:51.543', 'Id': '22438'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the Shamos-Hoey algorithm for finding whether or not any two of $n$ line segments intersect, which is available at this site: <a href="http://geomalgorithms.com/a09-_intersect-3.html" rel="nofollow">http://geomalgorithms.com/a09-_intersect-3.html</a>, there is use of "nearest line above" and "nearest line below". The algorithm is supposed to run in time $O(n\\log n)$. Here is their pseudocode:</p>\n\n<pre><code>Initialize event queue EQ = all segment endpoints;\nSort EQ by increasing x and y;\nInitialize sweep line SL to be empty;\n\nWhile (EQ is nonempty) {\n    Let E = the next event from EQ;\n    If (E is a left endpoint) {\n        Let segE = E\'s segment;\n        Add segE to SL;\n        Let segA = the segment Above segE in SL;\n        Let segB = the segment Below segE in SL;\n        If (I = Intersect( segE with segA) exists) \n            return TRUE;   // an Intersect Exists\n        If (I = Intersect( segE with segB) exists) \n            return TRUE;   // an Intersect Exists\n    }\n    Else {  // E is a right endpoint\n        Let segE = E\'s segment;\n        Let segA = the segment above segE in SL;\n        Let segB = the segment below segE in SL;\n        Delete segE from SL;\n        If (I = Intersect( segA with segB) exists) \n            return TRUE;   // an Intersect Exists\n    }\n    remove E from EQ;\n}\nreturn FALSE;      // No  Intersections\n</code></pre>\n\n<p>If one studies the C++ code provided at the bottom of the webpage, we see that this is simply a "next" and "previous" in a BST, however, I can\'t seem to tell which information is being used as the BST key.</p>\n\n<p>My issue is the following: if we are considering all $y$-values at the current $x$-value of the sweep line, this is not merely a check for next or previous endpoint value in a BST, and can not take $O(\\log n)$ time. However, if we are checking per endpoint coordinate, this could not possibly be correct, since the following situation would lead to an incorrect execution:</p>\n\n<p><img src="http://i.stack.imgur.com/cgl2r.png" alt="Counterexample"></p>\n\n<p>The algorithm should find that $B$ and $C$ intersection on insertion of $B$ into the search tree ("SweepLine" / "SL") while sweeping. However, if we are ordering by endpoint coordinates, $A$ is $B$\'s previous, $C$ is not, and this would run into problems.</p>\n', 'ViewCount': '37', 'Title': 'Shamos-Hoey Line segment intersection runtime', 'LastEditorUserId': '15463', 'LastActivityDate': '2014-03-09T23:52:12.310', 'LastEditDate': '2014-03-09T23:52:12.310', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15463', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-03-09T23:24:01.920', 'Id': '22443'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm wondering why the following argument doesn't work for showing that the existence of a Las Vegas algorithm also implies the existence of a deterministic algorithm:</p>\n\n<p>Suppose that there is a Las Vegas algorithm $A$ that solves some graph problem $P$, i.e., $A$ takes an $n$-node input graph $G$ as input (I'm assuming the number of edges is $\\le n$) and eventually yields a correct output, while terminating within time $T(G)$ with some nonzero probability.</p>\n\n<p>Suppose that there is no deterministic algorithm that solves $P$. Let $A^\\rho$ be the deterministic algorithm that is given by running the Las Vegas algorithm $A$ with a fixed bit string $\\rho$ as its random string. \nLet $k=k(n)$ be the number of $n$-node input graphs (with $\\le n$ edges).\nSince there is no deterministic algorithm for $P$, it follows that, for any $\\rho$, the deterministic algorithm $A^\\rho$ fails on at least one of the $k$ input graphs. Returning to the Las Vegas algorithm $A$, this means that $A$ has a probability of failure of $\\ge 1/k$, a contradiction to $A$ being Las Vegas. </p>\n", 'ViewCount': '194', 'Title': 'Relationship between Las Vegas algorithms and deterministic algorithms', 'LastActivityDate': '2014-03-10T21:34:04.607', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15471', 'Tags': '<algorithms><complexity-theory><randomized-algorithms>', 'CreationDate': '2014-03-10T02:22:57.270', 'Id': '22448'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $G$ be a DAG. We know that some nodes in $G$ are "bad", while the others are "good"; a descendant of a bad node is bad while the ancestors of a good node are good. We also know that bad nodes have a unique minimal element in $G$ which we\'d like to find querying as few nodes as possible with queries of the type "Are you good or bad?".</p>\n\n<p>This problem is solved in Git, the popular version control system, by the command <code>git-bisect</code>, which helps a programmer find the first commit in which a bug was introduced.</p>\n\n<p>At the start, the algorithm implemented by Git assumes to know a single bad commit and one or more good commits. At each step of its execution, the algorithm finds a commit using the following steps (taken from <a href="https://github.com/git/git/blob/master/Documentation/git-bisect-lk2009.txt#L407" rel="nofollow">here</a>):</p>\n\n<ol>\n<li><p>Keep only the commits that:</p>\n\n<p><em>a)</em> are an ancestor of the bad commit (including the bad commit itself), and</p>\n\n<p><em>b)</em> are not an ancestor of a good commit (excluding the good commits).</p></li>\n<li><p>Starting from the good ends of the resulting graph, associate to each\ncommit the number of ancestors it has plus one.</p></li>\n<li><p>Associate to each commit $\\min(X, N-X)$, where $X$ is the value associated to the commit in step 2, and $N$ is the total number of commits in the graph (after it was reduced in step 1).</p></li>\n<li><p>The best bisection point is the commit with the highest associated\nnumber.</p></li>\n</ol>\n\n<p>This algorithm is essentially finding the commit that achieves the "worst best case": in fact, $\\min(X,N-X)$ is the number of nodes in the DAG at the next iteration in the best case, thus $\\max\\min(X,N-X)$ is the worst best case.</p>\n\n<p>I\'m wondering:</p>\n\n<ul>\n<li>Does it make any difference if we select the "best worst case", that is, the node that achieves $\\min\\max(X,N-X)$?</li>\n<li>Is this algorithm worst-case optimal?</li>\n</ul>\n\n<p>EDIT: I\'ve noticed that this problem has a $\\Omega(N)$ bound. Consider the DAG formed by a single node $b$ with $N-1$ parents called $g_1,\\dots,g_{N-1}$. If we know that $b$ is bad then we have check each of the parents to see if they are the minimal bad node.</p>\n\n<p>EDIT 2: The previous is actually a $\\Omega(w)$ bound, where $w$ is the width of the poset. An alternative algorithm for this problem is given in <a href="http://cstheory.stackexchange.com/a/8943/12419">this answer</a> on cstheory.stackexchange that uses $O(w\\log{n})$ queries.</p>\n', 'ViewCount': '132', 'Title': 'Is the algorithm implemented by git bisect optimal?', 'LastEditorUserId': '4511', 'LastActivityDate': '2014-03-16T15:12:21.663', 'LastEditDate': '2014-03-16T15:12:21.663', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4511', 'Tags': '<algorithms><graphs>', 'CreationDate': '2014-03-10T03:11:55.213', 'FavoriteCount': '1', 'Id': '22451'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a set of buyers, houses, agents with the following constraints: </p>\n\n<ol>\n<li>Agents only know a subset of buyers</li>\n<li>Agents only know a subset of houses</li>\n<li>Agents can only do some amount of transactions</li>\n</ol>\n\n<p>Construct a flow network to model the problem.\n<em>Disclaimer this is homework</em></p>\n\n<p>I have put together some thoughts on a solution to this problem.  First I realized that agents should be right before the sink node with a max flow equal to the number of possible transactions.  This would then limit the number of transactions that can flow through an agent since the max flow can't be exceeded.  </p>\n\n<p>I was then thinking that it would be possible to put an edge between each person and house that an agent knows (example if agent knows person A, B and house 1, 2 there are edges (A,1), (A,2), (B,1), (B,2)). Finally, connect each house that an agent knows to an agent with max flow of 1. Then by running a Ford-Fulkerson algorithm across it the inflow of the sink node would indicate max number of transactions.</p>\n\n<p>However, I am concerned that there is no limit of stopping a house from being purchased more than once. I have tried building the network with various configurations (swapping houses and clients, building nodes of client/house pairs) and nothing seems to solve this specific problem.</p>\n\n<p>I am mostly looking for hints to solve this problem.</p>\n", 'ViewCount': '30', 'Title': 'Network Flow with multiple connected subsets', 'LastActivityDate': '2014-04-10T02:41:15.647', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7887', 'Tags': '<algorithms><network-flow>', 'CreationDate': '2014-03-10T07:12:56.767', 'Id': '22455'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is a question I have stumbled upon in my textbook, and didn't really know how to approach:</p>\n\n<p>Given a $k$-bit binary counter.\nWe have an operation Increment, which adds 1 to the counter.\nWe add a new operation Decrement, which subtracts 1 from the counter.\nProve that the cost for executing $n$ operations is $\\Theta(nk)$.</p>\n", 'ViewCount': '32', 'ClosedDate': '2014-03-12T09:25:27.037', 'Title': 'Binary counter amortized analysis', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-10T12:12:13.747', 'LastEditDate': '2014-03-10T11:34:25.743', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<algorithms><algorithm-analysis><amortized-analysis>', 'CreationDate': '2014-03-10T10:34:20.833', 'Id': '22460'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For probabilistic algorithms such as <a href="http://en.wikipedia.org/wiki/PageRank" rel="nofollow">PageRank</a> and <a href="http://kamvar.org/assets/papers/eigentrust.pdf" rel="nofollow">EigenTrust</a>, the stopping case is given as $|R_{t+1} - R_{t}| &lt; \\epsilon$ (i.e. convergence is assumed). Neither the papers on EigenTrust or PageRank, or the PageRank wiki page, give any <em>clear</em> indication of what $\\epsilon$ should be. </p>\n\n<p>I believe it might be something to do with the damping factor $d = 0.85$; specifically $\\epsilon = 1 - d = 0.15$, but  I can\'t be sure. </p>\n\n<p>How is $\\epsilon$ determined, and if it\'s nothing more than an abitrary value $0 \\leq \\epsilon \\leq 1$, how would you choose a sensible value?</p>\n', 'ViewCount': '41', 'Title': 'PageRank and EigenTrust: How small should epsilon be?', 'LastEditorUserId': '755', 'LastActivityDate': '2014-03-11T02:45:24.150', 'LastEditDate': '2014-03-11T02:45:24.150', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10926', 'Tags': '<probabilistic-algorithms><markov-chains>', 'CreationDate': '2014-03-10T11:57:49.503', 'Id': '22467'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If we have a FPRAS for approximating the quantity c, can we get another FPRAS for estimating (c-1) using the estimation of c?</p>\n', 'ViewCount': '17', 'Title': 'Estimating (c-1) from approximation of c', 'LastActivityDate': '2014-03-10T16:49:39.253', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15499', 'Tags': '<approximation-algorithms>', 'CreationDate': '2014-03-10T16:00:59.913', 'Id': '22473'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How different are string matching and integer matching in terms of time complexity?</p>\n\n<p>I'm asking this especially in relation to Rabin Karp algorithm. Why is it faster to compute hash code for every substring and check for equality of hashcode with the hashcode of the given string than the naive method of just checking if amy of the substrings match with the given string?</p>\n", 'ViewCount': '45', 'Title': 'string matching vs integer matching', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-11T00:18:50.563', 'LastEditDate': '2014-03-10T20:58:37.677', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22485', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15506', 'Tags': '<algorithms><strings><substrings>', 'CreationDate': '2014-03-10T18:51:36.317', 'Id': '22480'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '874', 'Title': 'Why is Quicksort described as "in-place" if the sublists take up quite a bit of memory? Surely only something like bubble sort is in-place?', 'LastEditDate': '2014-03-14T09:28:19.193', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '15572', 'FavoriteCount': '2', 'Body': '<p>Quicksort is described as "in-place" but using an implementation such as:</p>\n\n<pre><code>def sort(array):\n    less = []\n    equal = []\n    greater = []\n    if len(array) &gt; 1:\n        pivot = array[0]\n        for x in array:\n            if x &lt; pivot:\n                less.append(x)\n            if x == pivot:\n                equal.append(x)\n            if x &gt; pivot:\n                greater.append(x)\n        return sort(less) + equal + sort(greater)\n    else:\n        return array\n</code></pre>\n\n<p>You have to create a copy of the list for each recursion. By the first return, in memory we have:</p>\n\n<ul>\n<li>array</li>\n<li>greater+equal+less</li>\n</ul>\n\n<p>Then by the second recursion across all sub-lists we have:</p>\n\n<ul>\n<li>array</li>\n<li>greater, equal, less from first recursion</li>\n<li>greater+equal+less from less1, greater+equal+less from greater1</li>\n</ul>\n\n<p>etc...</p>\n\n<p>Is this just badly written code or am I correct in thinking that for a big list, you actually have to have, proportionally, a fair amount of extra space to store thse?</p>\n\n<p>When I think of something that is "in-place", I think of bubble sort, which simply swaps elements in the list like: <a href="http://en.wikipedia.org/wiki/File:Bubble-sort-example-300px.gif">http://en.wikipedia.org/wiki/File:Bubble-sort-example-300px.gif</a></p>\n\n<p>BubbleSort only requires 1 extra variable to store a potentially swapped element.</p>\n', 'Tags': '<algorithms><sorting><quicksort>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T11:57:21.597', 'CommentCount': '2', 'AcceptedAnswerId': '22517', 'CreationDate': '2014-03-11T22:51:08.497', 'Id': '22516'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Let\'s say we have a directed graph $G = (V, E)$ for which $(v, w) \\in E$ and/or $(w,v) \\in E$ holds true for all $v, w \\in V$. My feeling is that this graph most definitely is Hamiltonian, and I want to find a Hamiltonian path in it (from any vertex to any other vertex, I don\'t care where to start or stop).</p>\n\n<p>I wanted to refer to <a href="http://en.wikipedia.org/wiki/Hamiltonian_path#Bondy.E2.80.93Chv.C3.A1tal_theorem" rel="nofollow">Meylien\'s theorem</a> for this:</p>\n\n<blockquote>\n  <p>A strongly connected simple directed graph with $n$ vertices is Hamiltonian if the sum of full degrees of every pair of distinct non-adjacent vertices is greater than or equal to $2n \u2212 1.$</p>\n</blockquote>\n\n<p>There are two subtleties that I\'m not sure about with this theorem:</p>\n\n<ul>\n<li>What is meant by "adjcent vertices". Does the order matter here? Is the pair $(v,w)$ adjacent even if $(w,v) \\in E$ but not $(v,w)$ itself? If that is the case and the graph is strongly connected, than it must obviously be Hamiltonian, since there are no non-adjacent pairs of vertices at all.</li>\n<li>A graph with the above property is not necessarily strongly connected. I think this is easy to solve: We can just decompose the graph into SCCs. We will still have no non-adjacent pairs of vertices in the components and all of them are Hamiltonian. We can then construct a Hamiltonian path of the whole graph by connecting them in topological order.</li>\n</ul>\n\n<p>Is the above reasoning correct and the theorem applicable? Or is there some other argument we can use to show that there is a Hamiltonian path in the graph?</p>\n\n<p>In the end I want to actually <em>find</em> the Hamiltonian cycles in the SCCs, but haven\'t had much luck finding a constructive proof of the theorem, let alone an algorithm that solves this. Can it be done in a straightforward way? I feel like some kind of greedy approach could work, where we take the nodes in decreasing order of outdegree or something similar.</p>\n', 'ViewCount': '27', 'Title': "Meyniel's theorem + finding a Hamiltonian path for a specific graph family", 'LastEditorUserId': '13167', 'LastActivityDate': '2014-03-12T03:08:19.193', 'LastEditDate': '2014-03-12T03:02:15.303', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '22527', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13167', 'Tags': '<algorithms><graph-theory><graphs><hamiltonian-path>', 'CreationDate': '2014-03-12T02:51:07.913', 'Id': '22525'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a variant of bidding problem at hand.\nThere are N bidders(~20) who bid for items from a pool of many items(~10K). Each bidder can bid many items. I want to maximize the number of bidders who are satisfied. A bidder is satisfied if he gets all the items that he bid for in the first place. For eg-</p>\n\n<pre><code>Bidders = A,B,C\nItems = 1,2,3,4\n\nBidder    Bids\nA         1,2\nB         2,3\nC         3,4\n</code></pre>\n\n<p><img src="http://i.stack.imgur.com/Xcxlu.png" alt="enter image description here">\nIn this case its only possible to satisfy 2 bidders at max.</p>\n\n<p>I\'ve tried to model the problem to a maxflow problem and have taken several approaches but to no avail\nMy approaches so far-</p>\n\n<ol>\n<li><p>Tried to model this problem as a bipartite matching problem. The only problem being that instead of a one-one mapping I have a one-many mapping with an AND condition. </p></li>\n<li><p>A maxflow problem with edges going from source to each vertex with a capacity of number of bids. Problem here being ensuring that all edges from a bidder are selcted.</p></li>\n<li><p>A maxflow problem with both upper bounded and lower bounded edge capacities.</p></li>\n</ol>\n', 'ViewCount': '155', 'Title': 'Maximum number of matched vertexes in a one-to-many bipartite graph', 'LastActivityDate': '2014-03-12T17:58:04.897', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '22548', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '15606', 'Tags': '<algorithms><graph-theory><network-flow><bipartite-matching><max-cut>', 'CreationDate': '2014-03-12T16:10:00.057', 'Id': '22542'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Efficiency must be maximised and accuracy must be at least 97% for a algorithm doing image recognition in a database.</p>\n\n<p>In question (as stated in the question) is the dimension of the objectivness, also see:</p>\n\n<p>"Multiobjective optimization deals with solving problems having not only one, but multiple, often conflicting, criteria. Such problems can arise in practically every field of science, engineering and business, and the need for efficient and reliable solution methods is increasing. The task is challenging due to the fact that, instead of a single optimal solution, multiobjective optimization results in a number of solutions with different trade-offs among criteria, also known as Pareto optimal or efficient solutions. Hence, a decision maker is needed to provide additional preference information and to identify the most satisfactory solution. Depending on the paradigm used, such information may be introduced before, during, or after the optimization process. Clearly, research and application in multiobjective optimization involve expertise in optimization as well as in decision support."</p>\n\n<p>-- Multiobjective Optimization, Springer. Interactive and Evolutionary Approaches, Series: Lecture Notes in Computer Science, Vol. 5252</p>\n', 'ViewCount': '9', 'ClosedDate': '2014-04-01T22:09:33.723', 'Title': 'Is this problem a multi-objective optimisation problem?', 'LastEditorUserId': '13282', 'LastActivityDate': '2014-03-12T22:27:47.977', 'LastEditDate': '2014-03-12T22:27:47.977', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-4', 'PostTypeId': '1', 'OwnerUserId': '13282', 'Tags': '<algorithms><optimization>', 'CreationDate': '2014-03-12T21:25:09.297', 'Id': '22555'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>(This is based on <a href="http://math.stackexchange.com/questions/699864/perfect-play-in-1-dimensional-minesweeper">my earlier MSE question</a> that has been stagnant for just over a week, since the\n<br>\nbest answer only addresses the case of two mines and does not even completely resolve that.)</p>\n\n<p>What is the computational complexity of playing 1-dimensional Minesweeper in a way that\n<br>\nmaximizes the probability of winning, where the board length is given in unary, the number\n<br>\nof mines is known, and their position is chosen uniformly from among all possibilities?</p>\n\n<p>I believe the reasoning used in the proof of $\\:$<a href="http://en.wikipedia.org/wiki/IP_%28complexity%29#Proof_of_IP_.3D_PSPACE" rel="nofollow">IP $\\subseteq$ PSPACE</a>\n<br>\nshows that the complexity is at most PSPACE.\n<br><br></p>\n', 'ViewCount': '51', 'ClosedDate': '2014-03-13T01:42:56.530', 'Title': 'How hard is it to play 1-dimensional minesweeper optimally?', 'LastEditorUserId': '12859', 'LastActivityDate': '2014-03-13T02:01:40.077', 'LastEditDate': '2014-03-13T02:01:40.077', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12859', 'Tags': '<algorithms><computer-games>', 'CreationDate': '2014-03-12T23:07:07.527', 'Id': '22563'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading the paper: "Maximum Weight Independent Set Of Circular-Arc Graphs and It\'s Applications" (<a href="http://link.springer.com/article/10.1007%2FBF02832044" rel="nofollow">http://link.springer.com/article/10.1007%2FBF02832044</a>). And I had a question regarding the proof of Lemma 3 in the paper. It is stated as follows.</p>\n\n<p>If $X$ is the maximum weight independent set of the circular-arc graph $A$, then for any clique $Z$ of $A$, we have that $\\| X \\cap Z\\| = 1$.</p>\n\n<p>The paper then proves this by contradiction in two cases, the first when $\\| X \\cap Z\\| \\geq 2$ and the second when $\\| X \\cap Z\\| = 0$. I buy the argument for the first case, but I am concerned about the argument they make for the case equal to 0. Their argument goes as follows: If $\\| X \\cap Z\\| = 0$, then including any vertex in $Z$ into $X$ will remain independent and the weight will be greater than the previous set, contradicting the fact that $X$ was a maximum weight independent set. To me, this statement does not seem true. </p>\n\n<p><img src="http://i.stack.imgur.com/aqVfQ.jpg" alt="Circular Graph Example"></p>\n\n<p>If we pick vertices 8, 6, and 3 to have weight 1000, and all other vertices to have weight 1, then clearly {8, 6, 3} is a maximum weight independent set. However, their exists cliques of size two (edges) that have an empty intersection with {8, 6, 3}... such as the edge {1, 2}. You can also extend this to cliques of size 3 by adding an arc that intersects with arcs {8, 2, 1} then the clique formed by this new arc and arcs 1 and 2 will have an empty intersection as well.</p>\n\n<p>So where have I gone wrong in my understanding? I feel like I must be missing something really obvious. Clarifications would be greatly appreciated!</p>\n', 'ViewCount': '34', 'Title': 'Maximum Weight Independent Set in Circular-Arc Graphs (Proof of A Lemma)', 'LastActivityDate': '2014-03-14T04:31:41.060', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4734', 'Tags': '<algorithms><graph-theory><correctness-proof>', 'CreationDate': '2014-03-14T04:31:41.060', 'FavoriteCount': '0', 'Id': '22608'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '45', 'Title': 'Examples of algorithms that have runtime O(N + M) resp O(NM)', 'LastEditDate': '2014-03-14T09:40:03.310', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '544', 'FavoriteCount': '1', 'Body': "<p>I'm looking for examples of loops that have running time $O(nm)$, $O(n+m)$ and $O(n\\log m)$ to help me understand these concepts.  Could anybody give some examples and explain why they have the given running time?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T09:40:03.310', 'CommentCount': '4', 'AcceptedAnswerId': '22612', 'CreationDate': '2014-03-14T05:44:15.117', 'Id': '22611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '39', 'Title': "What's the time complexity of this append method?", 'LastEditDate': '2014-03-14T16:03:43.027', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15687', 'Body': '<p>I made a method that appends a sequence to another sequence.</p>\n\n<p>So: (append [1,2,3] [4,5,6]) = [1,2,3,4,5,6]</p>\n\n<p><strong>CODE In C#</strong></p>\n\n<pre><code>IEnumerable&lt;int&gt; Append(IEnumerable&lt;int&gt; xs,IEnumerable&lt;int&gt; ys)\n{\n    using(var iteratorX = xs.GetEnumerator())\n    using(var iteratorY = ys.GetEnumerator())\n    {\n        bool isTrueForX = false;\n        bool isTrueForY = false;\n        while((isTrueForX = iteratorX.MoveNext()) || (isTrueForY = iteratorY.MoveNext()))\n        {\n            if(isTrueForX) yield return iteratorX.Current;\n            else if(isTrueForY) yield return iteratorY.Current;\n        }\n    }\n}\n</code></pre>\n\n<p>I would like to know what is the time-complexity of this algorithm.</p>\n', 'ClosedDate': '2014-03-14T16:28:00.820', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'LastEditorUserId': '15687', 'LastActivityDate': '2014-03-14T16:03:43.027', 'CommentCount': '2', 'AcceptedAnswerId': '22617', 'CreationDate': '2014-03-14T11:37:38.200', 'Id': '22615'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Union-By-Rank and Path Compression is supposed to improve the performance of a tree implementation of a disjoint set.   </p>\n\n<p>However, in looking at the UNION(x, y) operation, I noticed that if x and y are actually the roots of the 2 trees being merged, no path compression actually takes place. The resulting tree would simply have a depth equal to the larger depth of the 2 trees.</p>\n\n<p>Is my understanding of the algorithm correct?</p>\n', 'ViewCount': '73', 'Title': 'Disjoint Set - Tree Implementation with Union-By-Rank and Path Compression', 'LastActivityDate': '2014-03-14T14:38:42.007', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2014-03-14T14:04:46.157', 'Id': '22620'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading the proof of PCP theorem in <a href="http://people.csail.mit.edu/madhu/papers/1992/almss-conf.pdf" rel="nofollow">Proof Verication and Hardness of Approximation Problems</a>. The following paragraph appears in section 3 (page 4), <em>"Outline of the Proof of the Main Theorem"</em>.</p>\n\n<blockquote>\n  <p>The results of these sections show that $NP \\subset OPT (poly(n), 1)$ (Theorem 5) and $NP \\subset OPT (\\log n, poly \\log n)$ (Theorem 8). Theorems 9 and 10 show that the recursion idea applies to these proof systems, and in particular shows the following:</p>\n  \n  <ol>\n  <li>$OPT (f (n), g(n)) \\subset OPT (f (n) + O(\\log g(n)), (\\log g(n))^{O(1)} )$ and</li>\n  <li>$OPT (f (n), g(n)) \\subset OPT (f (n) + (g(n))^{O(1)} , 1)$.</li>\n  </ol>\n  \n  <p>This allows us to conclude that $NP \\subset OPT (\\log n, poly \\log \\log n)$ (<strong>by composing</strong> two $OPT (\\log n, poly \\log n)$ proof systems) and then <strong>by composing</strong> this system with the $OPT (poly(n), 1)$ proof system we obtain $OPT (\\log n, 1)$ proof system for $NP$.</p>\n</blockquote>\n\n<p><strong>Edit.</strong> Composition of verifiers is defined in <a href="http://www.cs.umd.edu/~gasarch/pcp/AS.pdf" rel="nofollow">Probabilistic Checking of Proofs: A New Characterization of NP</a>  section 3 (page 13) <em>"Normal Form Verifiers and Their Use in Composition"</em>.</p>\n\n<blockquote>\n  <p>Let $r, q, s, t$ be any functions defined on the natural integers. Suppose there is a normal-form verifier $V_2$ that is $(r(n), s(n), q(n), t(n))$-constrained. Then, for all functions $R, Q, S, T$, $$RPCP(R(n), S(n), Q(n), T(n)) \\subseteq \\\\RPCP(R(n) + r(\\tau), s(\\tau), Q(n) + q(\\tau), Q(n)t(\\tau))$$ where $\\tau$ is a shorthand for $O((T(n))^2)$.</p>\n</blockquote>\n\n<p>Where $RPCP(r, s, q, t)$ is a $PCP(r, s \\cdot q)$ which takes $t$ time to accept of reject <em>after</em> reading the $s \\cdot q$ bits.</p>\n\n<p>I still don\'t see how this composition works. Is $OPT(r, q) = RPCP(r, q, 1, q)$ and a normal form verifier? In that case it seems to work, but then just composing $OPT(poly(n), 1)$ with $OPT(\\log n, poly \\log n)$ is enough, so why bother with $OPT(\\log n, poly \\log \\log n)$ or relations $1.$ and $2.$?</p>\n', 'ViewCount': '83', 'Title': 'Proof of PCP theorem', 'LastEditorUserId': '5167', 'LastActivityDate': '2014-03-23T10:41:07.547', 'LastEditDate': '2014-03-23T10:41:07.547', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '5167', 'Tags': '<complexity-theory><proof-techniques><probabilistic-algorithms>', 'CreationDate': '2014-03-15T07:51:32.813', 'Id': '22644'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '137', 'Title': 'Solve the recurrence $f(n+1)=f(n)^2,\\, f(0)=2$', 'LastEditDate': '2014-03-17T02:31:23.060', 'AnswerCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15733', 'Body': "<p>I have a problem with an exercise asking me to solve the following recurrence:\n$$f(n+1)=f(n)^2, \\quad f(0)=2$$\nCan someone explain how to solve this? I tried but couldn't.</p>\n", 'ClosedDate': '2014-03-17T15:58:41.800', 'Tags': '<algorithms><recurrence-relation>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-17T14:25:31.523', 'CommentCount': '4', 'AcceptedAnswerId': '22659', 'CreationDate': '2014-03-15T19:30:10.380', 'Id': '22657'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '179', 'Title': 'Why does merge sort run in $O(n^2)$ time?', 'LastEditDate': '2014-03-16T15:08:35.860', 'AnswerCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12448', 'FavoriteCount': '1', 'Body': '<p>I have been learning about Big O, Big Omega, and Big Theta. I have been reading many SO questions and answers to get a better understanding of the notations. From my understanding, it seems that <strong>Big O is the upper bound</strong> running time/space of the algorithm, <strong>Big Omega is the lower bound</strong> running time/space of the algorithm and <strong>Big Theta is like the in between of the two</strong>.</p>\n\n<p>This particular <a href="http://stackoverflow.com/questions/10376740/big-theta-notation-what-exactly-does-big-theta-represent?lq=1">answer</a> on SO stumbled me with the following statement </p>\n\n<blockquote>\n  <p>For example, merge sort worst case is both ${\\cal O}(n\\log n$) and $\\Omega(n\\log n)$ -\n  and thus is also $\\Theta(n\\log n)$, but it is also ${\\cal O}(n^2)$, since $n^2$ is\n  asymptotically "bigger" than it. However, it is NOT $\\Theta(n^2)$, Since\n  the algorithm is not $\\Omega(n^2)$</p>\n</blockquote>\n\n<p>I thought merge sort is ${\\cal O}(n\\log n)$ but it seems it is also ${\\cal O}(n^2)$ because $n^2$ is asymptotically bigger than it. Can someone explain this to me?</p>\n', 'Tags': '<algorithms><terminology><asymptotics><landau-notation>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-16T15:08:35.860', 'CommentCount': '5', 'AcceptedAnswerId': '22674', 'CreationDate': '2014-03-16T00:41:55.560', 'Id': '22662'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to use discrete differential evolutionary algorithm for assigning discrete values from set size $L$ to vectors of size $D$ where $L$ could be smaller, equal or larger than $D$. Elements of vector $X$ could take the same values of other elements.  My question is if we have a population of size $NP$ with each vector $X$ in the population of size $D$. How do we actually apply the mutation operand:</p>\n\n<p>$$V_{j,i}^{G+1} = X_{j, r_1}^{G} + F\\cdot (X_{j, r_2}^{G}-X_{j, r_3}^{G})$$</p>\n\n<p>where $i$, $r_1$, $r_2$, $r_3$ are references to vectors in $NP$ and none is equal to the other, $J$ is an index in vector $X$, and $F$ is a random number between $0$ and $1.2$.</p>\n\n<p>Suppose $X_{r_1}^{G}$ is equal to $\\{4, 1, 3, 2, 2, 0\\}$ and  $X_{r_2}^{G}$ is equal to $\\{2, 2, 3, 0, 4, 2\\}$ and $X_{r_3}^{G}$ is equal to $\\{1, 2, 3, 3, 0, 1\\}$\nCould anyone explain in detail the steps (through example if possible) on how to get the mutant vector $V_{j,i}^{G+1}$</p>\n', 'ViewCount': '25', 'Title': 'Mutation and crossover operations in discrete differential evolutionary operations?', 'LastEditorUserId': '15742', 'LastActivityDate': '2014-03-17T01:45:19.773', 'LastEditDate': '2014-03-17T01:45:19.773', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15742', 'Tags': '<algorithms><optimization><heuristics><evolutionary-computing>', 'CreationDate': '2014-03-16T07:22:44.630', 'Id': '22668'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '43', 'Title': 'Solve recurrence', 'LastEditDate': '2014-03-16T16:11:33.010', 'AnswerCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15733', 'FavoriteCount': '0', 'Body': "<p>![enter image description here][1]</p>\n\n<p>I have a problem to solve this recurrence. I tried by myself but it doesn't look understandable. Solve the following recurrence</p>\n", 'ClosedDate': '2014-03-16T16:14:27.257', 'Tags': '<algorithms><recurrence-relation>', 'LastEditorUserId': '15733', 'LastActivityDate': '2014-03-16T16:11:33.010', 'CommentCount': '6', 'AcceptedAnswerId': '22678', 'CreationDate': '2014-03-16T13:53:38.090', 'Id': '22675'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The is the running time of an algorithm : $0.5 X^2 + 3X$ , </p>\n\n<p>Q1) i dont understand why my lecturer says that the Big O is $O(X^3)$, shouldnt it be $O(X^2)$ as it is bounded by the quadratic power of $X$.</p>\n\n<p>Who is correct , me or my teacher ???</p>\n\n<p>Q2) Why is the Theta of the equation : $X^2$, i cant understand why</p>\n', 'ViewCount': '66', 'ClosedDate': '2014-03-17T11:30:36.683', 'Title': 'Why is Big $O(X^3)$ instead of Big $O(X^2)$ for this algorithm', 'LastEditorUserId': '15763', 'LastActivityDate': '2014-03-17T11:25:46.300', 'LastEditDate': '2014-03-17T11:25:46.300', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-03-17T01:08:52.797', 'Id': '22688'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let me clarify:</p>\n\n<p>Given a scatterplot of some given number of points n, if I want to find the closest point to any point in the plot mentally, I can immediately ignore most points in the graph, narrowing my choices down to some small, constant number of points nearby.</p>\n\n<p>Yet, in programming, given a set of points n, in order to find the closest point to any one, it requires checking every other point, which is ${\\cal O}(n)$ time.</p>\n\n<p>I am guessing that the visual sight of a graph is likely the equivalent of some data structure I am incapable of understanding; because with programming, by converting the points to a more structured method such as a quadtree, one can find the closest points to $k$ points in $n$ in $k\\cdot\\log(n)$ time, or ammortized ${\\cal O}(\\log n)$ time.</p>\n\n<p>But there is still no known ${\\cal O}(1)$ ammortized algorithms (that I can find) for point-finding after data restructuring.</p>\n\n<p>So why does this appear to be possible with mere visual inspection?</p>\n', 'ViewCount': '11181', 'Title': 'Why can I look at a graph and immediately find the closest point to another point, but it takes me O(n) time through programming?', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-03-24T14:59:51.437', 'LastEditDate': '2014-03-17T11:56:21.750', 'AnswerCount': '13', 'CommentCount': '13', 'Score': '67', 'PostTypeId': '1', 'OwnerUserId': '15762', 'Tags': '<algorithms><computer-vs-human>', 'CreationDate': '2014-03-17T06:05:56.817', 'FavoriteCount': '23', 'Id': '22693'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My teacher pointed out to us during lectures that we could use <strong>Graph Search</strong> to help us solve Sudoku puzzles which has left me puzzled . </p>\n\n<p>I dont see how this is possible as <strong>Graph Search</strong> is mostly about getting from Node A to Node B. He mentioned about how its a directed graph where the nodes correspond to partially completed puzzle</p>\n\n<p>What is the general idea behind using <strong>Graph Search</strong> to solve Sudoku Puzzle</p>\n', 'ViewCount': '108', 'Title': 'How to implement graph search to solve Sudoku puzzle', 'LastEditorUserId': '12448', 'LastActivityDate': '2014-03-17T12:30:24.120', 'LastEditDate': '2014-03-17T07:52:15.367', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs>', 'CreationDate': '2014-03-17T07:45:30.290', 'FavoriteCount': '1', 'Id': '22695'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\u2019m trying to find an algorithm that can give me an approximate solution for a wiring problem that I have been asked to look at. I believe this is closely related to finding a node weighted Steiner tree \u2013 e.g. <a href="http://www.cs.umd.edu/~samir/grant/gk98b.ps" rel="nofollow">http://www.cs.umd.edu/~samir/grant/gk98b.ps</a>. </p>\n\n<p>I have a number of connectors which have a fixed location in space and are connected together by wires. This can be represented as a graph where the connectors are nodes and the wires are edges:</p>\n\n<p><img src="http://i.stack.imgur.com/yQwNf.png" alt="Original layout"></p>\n\n<p>Each wire needs to be surrounded by a tube to protect it and a tube can contain many wires. Two or more tubes can be joined together at a junction. In the sketch below, the black lines show the outside of the tubes and the grey circles show junctions, but with each connector still connected by the same wires as before:</p>\n\n<p><img src="http://i.stack.imgur.com/7U9jO.png" alt="Layout showing junctions and tubes"></p>\n\n<p>Both the tubes and the junctions have a cost associated with them \u2013 for the tubes this is proportional to the length of the tube, for the junctions this is a fixed cost per junction. For example, if the tubes cost \\$10 per metre and the junctions cost \\$5 each, 1 metre of tubes with two junctions would be 10 + 5 + 5 = \\$20.</p>\n\n<p>I would like to find a layout that minimises the cost of the total length of tube + the cost of the junctions. I don\u2019t think it\u2019s quite the same problem as the reference above \u2013 I need to ensure that that the wires connecting between the connectors do not change, only the intermediate junctions between them. My real application has approximately 300 nodes and 1500 edges.</p>\n\n<p>Thanks,</p>\n\n<p>Rich</p>\n', 'ViewCount': '31', 'Title': 'Steiner tree wiring problem', 'LastActivityDate': '2014-03-17T18:14:12.507', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15786', 'Tags': '<algorithms><graph-theory>', 'CreationDate': '2014-03-17T18:14:12.507', 'Id': '22717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I plugged in a <a href="http://en.wikipedia.org/wiki/Hardware_random_number_generator" rel="nofollow">hardware true-random number generator (TRNG)</a> to my computer, then wrote programs with output that depends on the TRNG\'s output. Can it do anything non-trivial that a Turing machine with a psuedo-random number generator can\'t do? (a trivial thing it can do would be generating truly random numbers)</p>\n', 'ViewCount': '37', 'Title': 'Are there any practical differences between a Turing machine with a PRNG and a probabilistic Turing machine?', 'LastActivityDate': '2014-03-17T20:49:58.900', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15790', 'Tags': '<turing-machines><randomness><probabilistic-algorithms>', 'CreationDate': '2014-03-17T19:39:13.557', 'Id': '22720'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Suppose that I have a set of $N$ points in $k$-dimensional space ($k&gt;1$), such as in <a href="http://cs.stackexchange.com/q/22693/15794">this question</a>, and that I need to find all pairs with a distance\xb9 smaller than a certain threshold $t$.  The brute-force method would require $N(N-1)$ distance calculations, which is not acceptable.  I attack the problem by first sorting the cells in a grid\xb9, such as in <a href="http://cs.stackexchange.com/a/22707/15794">this answer</a>, followed by brute-force within each grid cell and a number of neighbours (which is easily calculated from the cell size $w*h$ and the maximum distance $t$).</p>\n\n<p>My solution seems to work acceptably well for my purposes, and the results appear to be correct.  However I\'m neither a computer scientist nor a mathematician, and I\'m not sure what tools I could use to calculate the optimal cell size.  In fact, I developed the aforementioned possibly naive algorithm because it seemed like a reasonably okay method.  I <em>guess</em> the optimal cell size depends in some way on $N$, $t$, on the cost of the distance function, and on the implementation of the sorting in cells, on the distribution of points, and on other things.  How would I make a guess of the optimal values of $w$ and $h$, with or without a priori knowledge on the approximate number of pairs I expect to find?</p>\n\n<p>Does the answer change if the N points are divided in two sets $S_1$ and $S_2$, and each pair shall consist of one element from each set?</p>\n\n<hr>\n\n<p>\xb9<sup>Not necessarily euclidian.  The points may, for example, be locations on a sphere, i.e. on Earth, with latitude and longitude.</sup></p>\n', 'ViewCount': '172', 'Title': 'How do I choose an optimal cell size when searching for close pairs of points, and using cells to implement this?', 'LastEditorUserId': '15794', 'LastActivityDate': '2014-03-17T23:35:41.607', 'LastEditDate': '2014-03-17T21:42:18.320', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15794', 'Tags': '<algorithms><computational-geometry><matching>', 'CreationDate': '2014-03-17T21:25:27.787', 'FavoriteCount': '1', 'Id': '22722'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have to solve this problem: We have weighted $n$-node undirected graph $G = (V,E)$ and a positive integer $k$. We can reach all vertices from vertex 1 (the root). We need to find the weight of minimal spanning tree in which the degree of vertex 1 is at most $k$ (we don\'t care about other vertices\' degrees). We can assume that such a tree exists.</p>\n\n<p>Can someone give an idea how to approach the solution?</p>\n\n<p>What I\'ve already tried:</p>\n\n<p>1) I know how to find essential edges from vertex 1. We can use dfs and start from a random edge of vertex 1. When we return to vertex 1 we can check if this edge (another vertex 1 edge) has lower weight than the previous one. If yes, than the previous one is not essential.</p>\n\n<p>2) After that I wanted to use Kruskal\'s algorithm (adding in the beginning of the algorithm all essential edges). But the problem is that sometimes we should not take an edge with minimal weight to construct the required tree. </p>\n\n<p>For example: 9-node undirected graph, $k = 3$</p>\n\n<pre><code>(vertex1 vertex2 weight)\n1 2 1\n2 3 5\n3 4 6\n4 5 7\n5 1 1\n1 6 1\n6 7 8\n7 8 9\n8 9 10\n9 1 2\n</code></pre>\n\n<p>So essentials will be (1,2) and (1, 6) (or (1, 5) and (1,6)). Kruskal will take (1,5) (or (1,2)) anyway. And the weight will be 41, but the correct answer is 39.\nSo I don\'t know how to use Kruskal\'s algorithm here. </p>\n\n<p>(The same example visualized, vertex 1 = vertex A)</p>\n\n<p><img src="http://i.stack.imgur.com/ud2Xc.png" alt="The same example visualized"></p>\n\n<p>I thought that we may construct a minimal spanning tree without constraints and after that try to transform it to the required one, but I don\'t know how to do this (how to transform without brute force).</p>\n', 'ViewCount': '146', 'Title': 'Minimal spanning tree with degree constraint', 'LastEditorUserId': '15839', 'LastActivityDate': '2014-03-19T20:17:49.750', 'LastEditDate': '2014-03-19T11:20:15.890', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '15839', 'Tags': '<algorithms><graph-theory><spanning-trees>', 'CreationDate': '2014-03-18T21:45:52.923', 'FavoriteCount': '2', 'Id': '22775'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have two shapes in a 2D space, not necessarily convex, and I\'d like to compare how similar they are. How can I define a robust distance metric to measure their similarity, and how can I compute it?</p>\n\n<p><img src="http://i.stack.imgur.com/COt2d.png" alt="Distance between two shapes"></p>\n\n<p>I am looking for a method which provides a short distance in case of:</p>\n\n<ol>\n<li>scaling;</li>\n<li>rotation;</li>\n<li>perhaps local scaling or rotation.</li>\n</ol>\n\n<hr>\n\n<p>I see two possible solutions:</p>\n\n<ol>\n<li>transform the shapes into <strong>pixel-based matrices</strong> (bitmap) and compute a Levenshtein distance (but without enough robustness in the distance, in case of rotation for instance);</li>\n<li>transform the shapes into <strong>graphs</strong> and try to define a distance between them.</li>\n</ol>\n', 'ViewCount': '58', 'Title': 'Similarity between two geometric shapes', 'LastEditorUserId': '755', 'LastActivityDate': '2014-03-19T00:40:49.737', 'LastEditDate': '2014-03-19T00:40:49.737', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15843', 'Tags': '<algorithms><graph-theory><computational-geometry><computer-vision>', 'CreationDate': '2014-03-18T23:43:32.437', 'FavoriteCount': '2', 'Id': '22781'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to solve this problem:</p>\n\n<blockquote>\n  <p>Given a collection of cities and the number of commuters between cities, design a network of roads for minimal cost where cost includes the cost of building the roads, and traveling of commuters. (So if more commuters use one edge, that edge will have a modified cost since the commuters want to get to their destination quickly)</p>\n</blockquote>\n\n<p>I've been treating the cities as nodes in a graph and the roads as edges. I want the weight of the edges to be dependent on the length of the edge (longer edges cost more) and somehow dependent on how many commuters are using that edge.</p>\n\n<p>My thought was to try doing something like a Minimum Steiner Tree, which if cost didn't depend on the commuters would minimize the cost of building the roads, but I'm not convinced that the solution to this should be a tree. And if I try to find a Minimal Spanning Tree or Minimum Steiner Tree I'm not sure how to deal with the fact that I don't know the edge cost until I know the full tree/graph. That is if people want to travel from A to B and we remove the edge from A to B, those people will now have to travel from A to C then from C to B which will modify the edge cost between A and C and C and B.</p>\n\n<p>Does anyone have any ideas on how to deal with the changing edge costs or know of a better way to solve this problem?</p>\n\n<p><strong>edit:</strong> You can make any sort of network you want, the roads can go straight from one city to another, or to any intermediate node(s) which can be placed anywhere in the plane. I'm planning on making the cost of building a road just a constant times the length of the road. For the cost of commuters I think another fractional constant times the distance they have to travel along the roads times the number of commuters traveling along that section of road.</p>\n", 'ViewCount': '87', 'ClosedDate': '2014-03-27T07:59:50.773', 'Title': 'Algorithm to determine a minimal cost graph', 'LastEditorUserId': '15844', 'LastActivityDate': '2014-03-19T16:50:47.457', 'LastEditDate': '2014-03-19T16:50:47.457', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15844', 'Tags': '<algorithms><graph-theory><graphs><weighted-graphs>', 'CreationDate': '2014-03-18T23:49:12.033', 'Id': '22782'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For a binary tree we define horizontal distance as follows:</p>\n\n<pre><code>    Horizontal distance(hd) of root = 0\n    If you go left then hd = hd(of its parent)-1, and \n    if you go right then hd = hd(of its parent)+1.\n</code></pre>\n\n<p>The bottom view of a tree then consists of all the nodes of the tree, where there is no node with the same <code>hd</code> and a greater level. (There may be multiple such nodes for a given value of <code>hd</code>. In this case all of them belong to the bottom view.) I'm looking for an algorithm that outputs the bottom view of a tree.</p>\n\n<hr>\n\n<p>Examples:</p>\n\n<p>Suppose the binary tree is:</p>\n\n<pre><code>         1\n        /  \\\n       2    3\n      / \\  / \\\n     4   5 6  7\n            \\\n             8\n</code></pre>\n\n<p>Bottom view of the tree is: 4 2 5 6 8 7</p>\n\n<pre><code>    Ok so for the first example,\n    Horizontal distance of node with value 1: 0, level = 1\n    Horizontal distance of node with value 2: 0 - 1 = -1, level = 2\n    Horizontal distance of node with value 3: 0 + 1 = 1, level = 2\n    Horizontal distance of node with value 4: -1 - 1 = -2, level = 3\n    Horizontal distance of node with value 5: -1 + 1 = 0, level = 3\n    Horizontal distance of node with value 6: 1 - 1 = 0, level = 3\n    Horizontal distance of node with value 7: 1 + 1 = 2, level = 3\n    Horizontal distance of node with value 8: 0 + 1 = 1, level = 4\n\n    So for each vertical line that is for hd=0, print those nodes which appear in the last level of that line.\n    So for hd = -2, print 4\n    for hd = -1, print 2\n    for hd = 0, print 5 and 6 because they both appear in the last level of that vertical line\n    for hd = 1, print 8\n    for hd = 2, print 7\n</code></pre>\n\n<p>One more example for reference :</p>\n\n<pre><code>         1\n      /     \\\n    2         3\n   / \\       / \\\n  4   5     6     7 \n / \\ / \\   / \\    / \\\n8  9 10 11 12 13 14 15     \n</code></pre>\n\n<p>So the output for this will be :\n8 4 9 10 12 5 6 11 13 14 7 15</p>\n\n<pre><code>Similarly for this example\nhd of node with value 1: 0, , level = 1\nhd of node with value 2: -1, level = 2\nhd of node with value 3: 1, level = 2\nhd of node with value 4: -2, level = 3\nhd of node with value 5: 0, , level = 3\nhd of node with value 6: 0, level = 3\nhd of node with value 7: 2, level = 3\nhd of node with value 8: -3, level = 4\nhd of node with value 9: -1, level = 4\nhd of node with value 10: -1, level = 4\nhd of node with value 11: 1, level = 4\nhd of node with value 12: -1, level = 4\nhd of node with value 13: 1, level = 4\nhd of node with value 14: 1, level = 4\nhd of node with value 15: 3, level = 4\n\nSo, the output will be:\nhd = -3, print 8\nhd = -2, print 4\nhd = -1, print 9 10 12\nhd = 0, print 5 6\nhd = 1, print 11 13 14\nhd = 2, print 7\nhd = 3, print 15 \n\nSo the ouput will be:\n8 4 9 10 12 5 6 11 13 14 7 15\n</code></pre>\n\n<hr>\n\n<p>I already know a method in which I can do it using a lot of extra space (a map, and a 1-D array for storing the level of the last element in that vertical line) and with time complexity of $O(N \\log N)$.\nAnd this is the implementation of this method:</p>\n\n<pre><code>void printBottom(Node *node, int level, int hd, int min, map&lt; int, vector&lt;int&gt; &gt;&amp; visited, int lev[], int l)\n{\n     if(node == NULL)\n             return;\n     if(level == 1){\n              if(lev[hd-min] == 0 || lev[hd-min] == l){\n                      lev[hd-min] = l;\n                      visited[hd-min].push_back(node-&gt;data);\n              }\n     }\n     else if(level &gt; 1)\n     {\n          printBottom(node-&gt;left, level-1, hd-1, min, visited, lev, l);\n          printBottom(node-&gt;right, level-1, hd+1, min, visited, lev, l);\n     }\n}\n\n\nint main()\n{\n    find the minimum and maximum values for hd via DFS\n\n    int lev[max-min+1]; //lev[hd] contains the maximum level for which we have found nodes with this value of hd; initialized with 0's\n\n    map &lt; int, vector&lt;int&gt; &gt; visited; //the nodes in the bottom view\n\n    int h = height(root);\n\n    for (int i=h; i&gt;0; i--){\n        printBottom(root, i, 0, min, visited, lev, i);\n    }\n\n    output visited\n}\n</code></pre>\n\n<p>I am seeking help to do this in more optimized way, which used less space or time. Is there any other efficient method for this problem?</p>\n", 'ViewCount': '279', 'Title': 'Print bottom view of a binary tree', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-03-21T11:07:00.120', 'LastEditDate': '2014-03-20T11:18:18.890', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22899', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15862', 'Tags': '<algorithms><binary-trees>', 'CreationDate': '2014-03-19T10:10:45.850', 'Id': '22801'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I am reading Prim's MST for the first time and wanted to implement the fast version of it . </p>\n\n<p>$m$ - The number of edges in the graph </p>\n\n<p>$n$ - The number of vertices in the graph </p>\n\n<p>Here's the algorithm :</p>\n\n<p>1) Create a Min Heap of size $V$ where $V$ is the number of vertices in the given graph. Every node of min heap contains vertex number and key value of the vertex.</p>\n\n<p>2) Initialize Min Heap with first vertex as root (the key value assigned to first vertex is $0$ ). The key value assigned to all other vertices is $\\infty$ .</p>\n\n<p>3) While Min Heap is not empty, do following</p>\n\n<p>\u2026..a) Extract the min value node from Min Heap. Let the extracted vertex be u.</p>\n\n<p>\u2026..b) For every adjacent vertex $v$ of $u$, check if $v$ is in Min Heap (not yet included in MST). If $v$ is in Min Heap and its key value is more than weight of $u-v$, then update the key value of $v$ as weight of $u-v$.</p>\n\n<p>Now my point is during implementation ( I am doing in C++) in step 3(b) I have to check whether the vertex is there in the heap or not . As we know , searching in a heap is done in $O(n)$ time . So in the main while loop which will run ( $n$ number of times ) although extract-min is $O(\\log n)$ but the search ( whether $v$ is min heap or not takes time proportional to size of the heap ( although it is decreasing in each step ) . </p>\n\n<p>So is it correct to say that the above algorithm is $O(m+n\\log n)$</p>\n", 'ViewCount': '86', 'Title': "Prim's Minimum Spanning Tree implementation $O(mn)$ or $O(m+n \\log n)$?", 'LastEditorUserId': '15879', 'LastActivityDate': '2014-03-19T19:06:06.820', 'LastEditDate': '2014-03-19T18:56:58.260', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15879', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs><runtime-analysis>', 'CreationDate': '2014-03-19T18:20:22.067', 'Id': '22817'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose an algorithm goes through a list of n integers and for every iteration of the loop it is needs to check if the current evaluated element of the list is even. If it is even, return the index of the integer that is evaluated as even.</p>\n\n<p>How come the algorithm would have 2n+1 comparison?</p>\n\n<p>I thought linear search would have n comparision because it is going through n elements. +1 comparison for the if statement. So that would make the algorithm O(n+1) comparison, no?. Where did the extra n come from?</p>\n\n<p>Pseudo-code:</p>\n\n<pre><code>procedure last_even_loc(a1,a2,...,an:integers);\nlocation = 0;\nfor i = 1 to n\n\n    if (a_i = 0) (mod 2) then location = i\n\nreturn location;\n</code></pre>\n', 'ViewCount': '66', 'Title': 'Why is there a 2n+1 comparison for a linear search algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-23T11:52:41.557', 'LastEditDate': '2014-03-23T11:52:41.557', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22853', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15555', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2014-03-20T03:45:36.143', 'Id': '22849'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://cs.stackexchange.com/q/11263/8660">This</a> link provides an algorithm for finding the diameter of an undirected tree <strong>using BFS/DFS</strong>. Summarizing:</p>\n\n<blockquote>\n  <p>Run BFS on any node s in the graph, remembering the node u discovered last. Run BFS from u remembering the node v discovered last. d(u,v) is the diameter of the tree. </p>\n</blockquote>\n\n<p>Why does it work ?</p>\n\n<p>Page 2 of <a href="http://courses.csail.mit.edu/6.046/fall01/handouts/ps9sol.pdf" rel="nofollow">this</a> provides a reasoning, but it is confusing. I am quoting the initial portion of the proof:</p>\n\n<blockquote>\n  <p>Run BFS on any node s in the graph, remembering the node u discovered last. Run BFS from u remembering the node v discovered last. d(u,v) is the diameter of the tree.</p>\n  \n  <p>Correctness: Let a and b be any two nodes such that d(a,b) is the diameter of the tree. There is a unique path from a to b. Let t be the first node on that path discovered by BFS. If the paths $p_1$ from s to u and $p_2$ from a to b do not share edges, then the path from t to u includes s. So</p>\n  \n  <p>$d(t,u) \\ge d(s,u)$</p>\n  \n  <p>$d(t,u) \\ge d(s,a)$</p>\n  \n  <p>....(more inequalities follow ..)</p>\n</blockquote>\n\n<p><img src="http://i61.tinypic.com/rji9uq.png" alt=""> </p>\n\n<p>The inequalities do not make sense to me.</p>\n', 'ViewCount': '232', 'Title': 'Algorithm to find diameter of a tree using BFS/DFS. Why does it work?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T23:37:16.420', 'LastEditDate': '2014-03-20T13:30:18.160', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8660', 'Tags': '<algorithms><graphs><search-algorithms><graph-traversal>', 'CreationDate': '2014-03-20T07:09:10.287', 'Id': '22855'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to make a recursive relationship for a function <code>f(m, n)</code> nonrecursive to make it more efficient and succinct in my code.</p>\n\n<p>I stumbled across an important recurrence relationship dealing with the number of vertices, edges, faces, and solids (<a href="http://en.wikipedia.org/wiki/Polytope" rel="nofollow">m-polytopes</a>) in an n-cube which is based off of a simpler algorithm for an n-simplex which uses Pascal\'s triangle:</p>\n\n<p>For a simplex:\nnCm gives you the number of m-polytopes (m = 1 for points, 2 for lines, 3 for faces) in an n-simplex (1-simplex is a line, 2-simplex is a triangle, 3-simplex is a tetrahedron).</p>\n\n<p>The pattern between the n-simplex m-polytopes and the n-cube m-polytopes are very similar:</p>\n\n<pre><code>Here is the n-cube up to 10\n10-polytopes:                                                            \n9-polytopes:                                                           1\n8-polytopes:                                                     1    16\n7-polytopes:                                               1    14   112\n6-polytopes:                                         1    12    84   448\n5-polytopes:                                   1    10    60   280  1120\n4-polytopes:                             1     8    40   160   560  1792\n3-polytopes:                       1     6    24    80   240   672  1792\n2-polytopes:                 1     4    12    32    80   192   448  1024\n1-polytopes:           1     2     4     8    16    32    64   128   256\nHere is the n-simplex up to 10\n10-polytopes:                                                            \n9-polytopes:                                                           1\n8-polytopes:                                                     1     9\n7-polytopes:                                               1     8    36\n6-polytopes:                                         1     7    28    84\n5-polytopes:                                   1     6    21    56   126\n4-polytopes:                             1     5    15    35    70   126\n3-polytopes:                       1     4    10    20    35    56    84\n2-polytopes:                 1     3     6    10    15    21    28    36\n1-polytopes:     1     1     2     3     4     5     6     7     8     9\n</code></pre>\n\n<p>And here is the c code that generated that:</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\n#define TOP 10\n\n// nothing to see here\nint factorial(int n)\n{\n    if (n &lt; 2)\n        return 1;\n    else\n        return n * factorial(n - 1);\n}\n// a recursive implementation for the number of \n// m-polytopes in an n-cube\nint ncubeRecursive(int n, int m)\n{\n    if (n == 0 &amp;&amp; m == 0)\n        return 1;\n    else if(n &lt; 0 || m &lt; 0)\n        return 0;\n    else\n    {\n        return (ncubeRecursive(n - 1, m - 1) + 2 * ncubeRecursive(n - 1, m));\n    }\n}\n// missing a nonrecursive algorithm\n// YOUR JOB TO FIX THIS\n\n\n// a recursive implementation for the number of\n// m-polytopes in an n-simplex\nint nsimplexRecursive(int n, int m)\n{\n    if (n == 0 &amp;&amp; m == 0)\n        return 1;\n    else if(n &lt; 0 || m &lt; 0)\n        return 0;\n    else\n    {\n        return (nsimplexRecursive(n - 1, m - 1) + nsimplexRecursive(n - 1, m));\n    }\n}\n// a nonrecursive algorithm\nint nsimplexNonrecursive(int n, int m)\n{\n    return factorial(n)/(factorial(n - m) * factorial(m));\n}\n\nint main()\n{\n    printf("Here is the n-cube up to %i\\n", TOP);\n    for (int n = TOP; n &gt; 0; --n)\n    {\n        printf("%i-polytopes:", n);\n        for (int m = 0; m &lt; TOP; ++m)\n        {\n            int val = ncubeRecursive(m, n);\n            if (val == 0)\n                printf("%6c", \' \');\n            else\n                printf("%6i", val);\n        }\n        printf("\\n");\n    }\n    printf("Here is the n-simplex up to %i\\n", TOP);\n    for (int n = TOP; n &gt; 0; --n)\n    {\n        printf("%i-polytopes:", n);\n        for (int m = 0; m &lt; TOP; ++m)\n        {\n            int val = nsimplexNonrecursive(m, n);\n            if (val == 0)\n                printf("%6c", \' \');\n            else\n                printf("%6i", val);\n        }\n        printf("\\n");\n    }\n\n    return 0;\n}\n</code></pre>\n\n<p>Does anyone here see a non-recursive pattern? I just don\'t know how to analyze a recursive relationship like this for a function that takes to inputs like <code>f(m, n)</code> instead of just <code>f(x)</code>. </p>\n', 'ViewCount': '44', 'ClosedDate': '2014-03-24T18:49:20.950', 'Title': 'How to make this recursive relationship nonrecursive?', 'LastActivityDate': '2014-03-20T18:03:35.190', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '22874', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15918', 'Tags': '<algorithms><combinatorics><computational-geometry><recurrence-relation>', 'CreationDate': '2014-03-20T15:28:41.100', 'Id': '22869'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m currently studying deadlocks and I\'m trying to learn myself the bankers algorithm. </p>\n\n<p>In this example my teacher goes to B first, then C and finally A.</p>\n\n<p>He adds (I think) 2 (row b, Has) with 3 (free) and then gets the new Free value 5.\nAfter that he proceeds to row C where he adds 2 (row c, Has) with the free value 5 which gives us a new free value of 7.</p>\n\n<p>After this he goes to the last row A and adds 3 to seven which gives him the free value of 10.</p>\n\n<p>This does not cause a dead lock and I\'m wondering why, Isn\'t he over the Max value of row A?</p>\n\n<p><img src="http://i.stack.imgur.com/ESBZx.png" alt="err"></p>\n\n<p><img src="http://i.stack.imgur.com/nwIqq.png" alt="enter image description here"></p>\n\n<p>Here is another example in which the first image is included, but there is also one scenario when we get a dead lock. I don\'t really understand why since we still get 10 free like the rest of the examples. </p>\n\n<p>Sorry if this is a bit confusing. I can simplify if its needed.</p>\n', 'ViewCount': '93', 'Title': 'Bankers algorithm - How does it work?', 'LastActivityDate': '2014-03-21T01:04:41.087', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '22889', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11661', 'Tags': '<algorithms><deadlocks>', 'CreationDate': '2014-03-20T18:53:49.317', 'Id': '22877'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Imagine we have N houses, on a standard euclidean 2D plane. We also have N "packages", each of which contains several "objects" of different types, let\'s call them A, B, C, etc. We know the content of all boxes beforehand, but can\'t change them (they\'re randomly generated).</p>\n\n<p>We need to send one package to every house, and we know that the person who lives in the house will need to use one or more of the objects, but we can\'t know which of them they will need. To use the object, they will travel to the nearest house that has it (ideally their own), use it there, and go back to their house. We need to assign a package to each house to minimize the potential distance they will have to travel.</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Package 1 contains B, C, D, E  </li>\n<li>Package 2 contains A, B, D, F  </li>\n<li>Package 3 contains A, E, F, G  </li>\n<li>House 1 is at (0,0)  </li>\n<li>House 2 is at (0, 5)</li>\n<li>House 3 is at (12, 0)</li>\n</ul>\n\n<p>We send packages 1, 2 and 3 to houses 1, 2 and 3: </p>\n\n<ul>\n<li>Owner of house 1 needs A and B: travels 5 to house 2 to get A (and back, but we\'re not counting that).</li>\n<li>Owner of house 2 needs C and G: travels 5 to house 1 to get C, goes back, then 13 to house 3 to get G (total = 18)</li>\n<li>Owner of house 3 needs A and G: travels 0 to their own house.</li>\n</ul>\n\n<p>Total distance = 23</p>\n\n<p>Since we can\'t know what they will want, we assume everyone needs everything. This means I need to minimize the sum of the shortest distance between every location and every type of object. Is this a known problem? How can I make an approximation algorithm for it? It sounds simple enough but I\'m stumped, I have no idea how to search for it.</p>\n\n<p>I was thinking along the lines of calculating a "differentness" of every package to every other one, then trying to place the "most different" packages closer to each other and the more similar ones further, but I don\'t really know how to do that either. </p>\n', 'ViewCount': '22', 'Title': 'Assigning packages to different points by minimizing distance: is this a known problem?', 'LastActivityDate': '2014-03-20T20:39:36.420', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15940', 'Tags': '<algorithms><optimization><combinatorics>', 'CreationDate': '2014-03-20T20:39:36.420', 'Id': '22879'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In unification, given a set of equations, a standard problem is to compute a most general unifier (mgu). I am interested in a somewhat reversed problem. Imagine having a set of equations that do not have an mgu, like this one:</p>\n\n<pre><code>x = a\nx = b\n</code></pre>\n\n<p><code>x</code> here is a variable, whereas <code>a</code> and <code>b</code> are terms. I am interested are there any algorithms that could find a possible replacement for <code>a</code> and <code>b</code> such that the resulting equations have mgu? In the above example, that would be <code>a -&gt; y, b -&gt; y</code>, <code>y</code> being a variable. Lets call this a <em>fix</em>. I am particularly interested in most specific fixes. I could not find anything so far, but this seems like a natural problem, or not?</p>\n', 'ViewCount': '55', 'Title': 'Unification --- most specific unifier', 'LastActivityDate': '2014-03-27T22:32:18.053', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23148', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8508', 'Tags': '<algorithms><logic><unification><equality>', 'CreationDate': '2014-03-21T19:28:11.560', 'Id': '22910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm a having trouble analyzing this algorithm.\nThis is a binary counter that supports only increments in $2^i$ values\nit's implemented in this way:\nstarting from the $i$-th location change all the straight $1$'s to $0$'s and the first $0$ to $1$.</p>\n\n<p>So I analyzed the W.C to be $O(\\log n)$ because the worst case is we need to increment by $1$ a $2^i-1$ number.\nNow for the amortized I thought using the accounting method, charging for each change from $0$ to $1$ $2\\$$ amortized cost, since each time we increment we flip at most one $0$ to $1$. and put $1\\$$ on each $1$ bit to pay from flipping it back to $0$. so at most the amortized cost is $2\\$$ which means amortized $O(n)$. if it's correct than what's the difference from a regular binary counter? I don't think I understand...</p>\n", 'ViewCount': '39', 'Title': 'Custom binary counter supports only increment in $2^i$ values amortized analysis', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-03-22T06:26:40.153', 'LastEditDate': '2014-03-22T06:26:40.153', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15693', 'Tags': '<algorithms><algorithm-analysis><amortized-analysis>', 'CreationDate': '2014-03-21T22:43:51.283', 'Id': '22916'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to code a solver for nonsingular systems of $N$ linear equations in $N$ unknowns (say up to $N=100$) with an asymmetric Toeplitz matrix. I know that the Levinson algorithm can solve it in time $O(N^2)$ and I am looking for a solution with this complexity. I have seen mentions of alternative approaches such as Schur decomposition, LDU decomposition, Bareiss, Cholesky...</p>\n\n<p>The equations are established in a Galois field, so that stability is not at all an issue here.</p>\n\n<p>I am seeking advice for a good method to implement. My priorities are</p>\n\n<p>1) ease of implementation,</p>\n\n<p>2) low memory requirements.</p>\n\n<p>I am not specially looking for superfast methods ($o(N^2)$), unless they are appropriate for moderate $N$, and simple.</p>\n\n<p>What do you recommend ?</p>\n', 'ViewCount': '16', 'Title': 'Solution of a Toepltiz system of linear equations', 'LastActivityDate': '2014-03-23T17:43:45.287', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16034', 'Tags': '<algorithms><linear-algebra>', 'CreationDate': '2014-03-23T17:43:45.287', 'Id': '22976'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I face this problem a lot while searching phone numbers and bank account numbers, when I do remember it partially. </p>\n\n<p>I save a draft in gmail with the content <code>I am mango</code>. Then I search it, by entering just <code>mango</code> and it gets me to the draft. </p>\n\n<p>But when I save a draft with some number such as <code>123987645</code> and try to search by entering <code>12398764</code> i.e just one character missing I fail to find it. Also I failed when I just typed  <code>87645</code>. </p>\n\n<p>Out of curiosity I am asking are the algorithms for finding numbers and text fundamentally different? Or I am missing something?</p>\n', 'ViewCount': '67', 'Title': 'Are algorithms for searching text vs searching numbers fundamentally different?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-23T12:58:54.220', 'LastEditDate': '2014-03-24T12:30:44.410', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><search-algorithms><strings><matching>', 'CreationDate': '2014-03-24T12:01:50.507', 'Id': '22997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How can we add n positive integers with binary expansion $l_1$, $l_2$,...$l_n$ bits so that the total complexity is $O (\\sum l_i)$ for $i = {1,...,n}$ ? More importantly, how can show this complexity using amortized analysis (the potential method)?</p>\n\n<p>I know the elementary school addition of 2 numbers of length $s$ and $r$ is $O(r+s)$ and hence, the addition of n integers is $O(\\sum l_i)$. However, what potential function would you use to prove that bound? I don't seem to have any intuition on that... Maybe use a function similar to the standard binary counter example (number of 1's in the binary representation of the number)..?</p>\n", 'ViewCount': '57', 'Title': 'Amortized Analysis for Addition of $n$ numbers', 'LastActivityDate': '2014-03-26T09:43:41.157', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13063', 'Tags': '<algorithms><algorithm-analysis><amortized-analysis>', 'CreationDate': '2014-03-25T02:55:52.887', 'Id': '23027'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have been attempting to learn parameterized complexity on my own, and decided to go through all of the FPT race problems, and defining easy FPT algorithms for them, using concepts such as bounded search tree. I am stuck on figuring out an FPT algorithm for edge dominating set, defined as follows:</p>\n\n<p><strong>EdgeDominatingSet</strong></p>\n\n<p>Instance: A graph $G=(V,E)$; a positive integer $k$. </p>\n\n<p>Question: Is there a subset $D\\subseteq E$ with $|D|\\leq k$ such that for each $e\\in E$, either $e\\in D$ or $e$ shares an endpoint with an $e'\\in D$. </p>\n\n<p>Parameter: $k$</p>\n\n<p>I'm not looking to define anything fancy, just a simple FPT result. Any help would be great! </p>\n", 'ViewCount': '40', 'Title': 'FPT algorithm for edge dominating set', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-25T08:46:17.463', 'LastEditDate': '2014-03-25T08:46:17.463', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16086', 'Tags': '<algorithms><complexity-theory><parametrized-complexity>', 'CreationDate': '2014-03-25T04:47:51.053', 'Id': '23028'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have been trying to understand the difference between normal polynomial evaluation and horner's method. usually it takes 3n-1 operations while horner's method reduces it to 2n operations. I tried a couple of explanations but they were too theoritical. I would be glad if somebody comes up with a decent and simple explanation.</p>\n", 'ViewCount': '73', 'Title': "Can somebody explain Horner's method of evaluating polynomials and how does it reduce the time complexity to 2n operations?", 'LastActivityDate': '2014-03-26T15:13:03.730', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '23040', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16105', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2014-03-25T17:30:37.520', 'FavoriteCount': '1', 'Id': '23037'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '360', 'Title': 'Is this method really uniformly random?', 'LastEditDate': '2014-03-25T22:35:47.140', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16107', 'FavoriteCount': '2', 'Body': "<p>I have a list and want to select a random item from the list.</p>\n\n<p>An algorithm which is said to be random:</p>\n\n<blockquote>\n  <p>When you see the first item in the list, you set it as the selected item.</p>\n  \n  <p>When you see the second item, you pick a random integer in the range [1,2]. If it's 1, then the new item becomes the selected item. Otherwise you skip that item.</p>\n  \n  <p>For each item you see, you increase the count, and with probability 1/count, you select it. So at the 101st item, you pick a random integer in the range [1,101]. If it's 100, that item is the new selected node.</p>\n</blockquote>\n\n<p>Is it really uniformly random?</p>\n\n<p><strong>My thoughts are:</strong></p>\n\n<p>As the number of nodes increases, the probability for them being selected \ndecreases, so the best chance of selection is for items 1, 2, 3, ..., not for 20, 21, ..., 101.</p>\n\n<p>Each node will not have equal probability of being selected.</p>\n\n<p>Please clarify, as I have trouble understanding this.</p>\n", 'Tags': '<algorithms><probability-theory><randomized-algorithms><sampling>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-25T22:35:47.140', 'CommentCount': '5', 'AcceptedAnswerId': '23041', 'CreationDate': '2014-03-25T17:55:00.713', 'Id': '23038'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://en.wikipedia.org/wiki/Maximum_subarray_problem" rel="nofollow">Kadane Algorithm</a> is used to solve the maximum subarray problem which in simpler terms is to find the highest possible value of a continuous sub array in an array. </p>\n\n<p>One often cited application of Kadane Algorithm is that given the prices of the stock over X days , on which days should we buy and sell the stock in order to maximize given profit. These example is not practical at all as it is impossible to predict future prices of stocks and hence impossible to apply it . </p>\n\n<p>What are some real world application of Kadane algorithm?</p>\n', 'ViewCount': '30', 'Title': 'Practical Application of Kadane algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T08:20:17.887', 'LastEditDate': '2014-03-26T08:20:17.887', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><reference-request><optimization>', 'CreationDate': '2014-03-26T01:38:44.523', 'Id': '23055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>We have a 0-1 knapsack in which the increasing order of items by weight is the same as the decreasing order of items by value. Design a greedy algorithm and prove that the greedy choice guarantees an optimal solution.</p>\n\n<p>Given the two orders I imagined that we could just choose the first k elements from either sequence and use them to fill knapsack until it was full. This would be similar to choosing the items with the greatest ratio of value to weight. But I don't think that is an optimal solution. </p>\n\n<p>So what I need help with is whether or not this solution is optimal. And how would I prove the correctness of a greedy algorithm. </p>\n", 'ViewCount': '63', 'Title': 'Correctness proof of greedy algorithm for 0-1 knapsack problem', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T16:45:47.887', 'LastEditDate': '2014-03-26T08:38:55.053', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23060', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15512', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><greedy-algorithms><knapsack-problems>', 'CreationDate': '2014-03-26T03:23:03.477', 'Id': '23058'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having trouble in understanding <a href="http://en.wikipedia.org/wiki/Order_statistic_tree" rel="nofollow">order-statistics tree</a> .   </p>\n\n<p><strong>Definition :</strong><br>\nEvery node in tree stores the number of descendants of itself .<br>\nCan you please explain the Algorithm or pseudocode how to <strong>Insert</strong> and <strong>Delete</strong> a node in the tree <strong>when tree is to be balanced</strong> .  </p>\n\n<p>Basically i want to find the rank of an element in the tree in <strong>O(log N)</strong> time complexity and nodes can be inserted and deleted in between querries .  </p>\n\n<p>P.S. I am able to understand the <strong>rank(x)</strong> operation .</p>\n', 'ViewCount': '36', 'Title': 'insert and delete in order statistic tree', 'LastEditorUserId': '635', 'LastActivityDate': '2014-03-27T03:22:10.707', 'LastEditDate': '2014-03-27T03:22:10.707', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '16107', 'Tags': '<algorithms><data-structures><binary-trees>', 'CreationDate': '2014-03-26T13:08:12.103', 'Id': '23073'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '89', 'Title': 'What are efficient ways to compute the derivatives of iterated functions?', 'LastEditDate': '2014-03-26T18:45:29.407', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16143', 'FavoriteCount': '1', 'Body': u'<p>The derivatives of iterated functions at a fixed point $z_0$ are useful in constructing a Taylors series of iterated analytic functions - in other words, the Taylors series of a dynamical system $f^t(z_0)$. </p>\n\n<p>A simpler version of the problem casts light on possible approaches. Instead of iterated functions, consider composite functions and <a href="http://en.wikipedia.org/wiki/Fa%C3%A0_di_Bruno%27s_formula" rel="nofollow">Fa\xe0 di Bruno\'s formula</a> The combinatorial structure <a href="http://en.wikipedia.org/wiki/Integer_partition" rel="nofollow">integer partitions</a> serves as the index to the summations in Fa\xe0 di Bruno\'s formula. So the first step of evaluation is to enumerate all integer partitions of a given order.  </p>\n\n<p>While combinatorial structure <a href="http://oeis.org/A000041" rel="nofollow">integer partitions</a> are associated with the derivatives of composite functions, <a href="http://oeis.org/A000669" rel="nofollow">the unlabeled version of the labeled combinatorial structure total partitions</a> are associated with iterated functions. See <a href="http://tetration.org/Combinatorics/PartitionDiagrams.htm" rel="nofollow">Partition Diagrams</a> for more information on the different relevant combinatorial structures. The following Mathematica code does what I need, but the cost of simplicity is that it first enumerates the labeled total partitions to compute the unlabeled total partitions. So for $n=10$, $D^{10}f^t(z_0)$, there are 282,137,824 labeled total partitions, while there are only 2,312 unlabeled partitions. </p>\n\n<h2>Combinatorial Examples</h2>\n\n<p>a(4)=5 unlabeled total partitions: (oooo),(oo(oo)),(o(ooo)),(o(o(oo))),((oo)(oo)).</p>\n\n<p>b(4)=26 labeled total partitions: ((1,4),2,3), (1,(2,4),3), (1,2,(3,4)), (((1,4),2),3), ((1,(2,4)),3), ((1,2),(3,4)), (((1,4),3),2), ((1,(3,4)),2), ((1,3),(2,4)), ((1,4),(2,3)), (1,((2,4),3)), (1,(2,(3,4))), (1,2,3,4), ((1,2,4),3), ((1,2),3,4), ((1,3,4),2), ((1,3),2,4), (1,(2,3,4)), (1,(2,3),4), ((1,2,3),4), (((1,2),4),3), (((1,2),3),4), (((1,3),4),2), (((1,3),2),4), (1,((2,3),4)), ((1,(2,3)),4)</p>\n\n<p>b(4) = 12 (o(o(oo))) + 3 ((oo)(oo)) + 6 ((oo(oo)) + 4 (o(ooo)) + (oooo) = 26</p>\n\n<p>I would like to have an efficient way to reproduce the preceding results.</p>\n\n<h2>Mathematica code</h2>\n\n<pre><code>TotalPartitions[0] = {{}};\nTotalPartitions[1] = {{1}};\nTotalPartitions[2] = {{1, 2}};\nMatch[l_List, pattern_] := Extract[l, Position[l, pattern]];\nTP1[l_List, next_Integer] := \n  Map[( l /. # -&gt; {#, next}) &amp;, Match[l, _Integer] ];\nTP2[l_List, next_Integer] := \n  Map[( l /. # -&gt; Append[#, next]) &amp;, Match[l, _List] ];\nTP3[l_List, next_Integer] := \n  Map[( l /. # -&gt; {#, next}) &amp;, Match[l, _List] ];\nTotalPartitions[n_Integer] := \n  TotalPartitions[n] = \n   Flatten[ {Map[(TP1[#, n] ) &amp;, TotalPartitions[n - 1]], \n     Map[(TP2[#, n] ) &amp;, TotalPartitions[n - 1]], \n     Map[(TP3[#, n] ) &amp;, TotalPartitions[n - 1]]}, 2];\n\nu = TotalPartitions[4] /. _Integer -&gt; 1\n&gt; {{{{1, 1}, 1}, 1}, {{1, {1, 1}}, 1}, \n&gt;  {{1, 1}, {1, 1}}, {{1, 1}, {1, 1}}, \n&gt;  {1, {{1, 1}, 1}}, {1, {1, {1, 1}}}, \n&gt;  {{1, 1}, 1, 1}, {1, {1, 1}, 1}, \n&gt;  {1, 1, {1, 1}}, {{{1, 1}, 1}, 1}, \n&gt;  {{1, {1, 1}}, 1}, {{1, 1}, {1, 1}}, \n&gt;  {{1, 1, 1}, 1}, {{1, 1}, 1, 1}, \n&gt;  {1, {1, 1, 1}}, {1, {1, 1}, 1}, \n&gt;  {1, 1, 1, 1}, {{1, 1, 1}, 1}, \n&gt;  {{1, 1}, 1, 1}, {{{1, 1}, 1}, 1}, \n&gt;  {{{1, 1}, 1}, 1}, {1, {{1, 1}, 1}}, \n&gt;  {{1, {1, 1}}, 1}, {{1, 1, 1}, 1}, \n&gt;  {{{1, 1}, 1}, 1}, {{{1, 1}, 1}, 1}}\n\nSetAttributes[Z, Orderless];\nTally[Apply[List, Apply[Z, u, Infinity], Infinity]]\n&gt;  {{{1, {1, {1, 1}}}, 12}, {{{1, 1}, {1, 1}}, 3}, {{1, 1, {1, 1}}, 6}, \n&gt;   {{1, {1, 1, 1}}, 4}, {{1, 1, 1, 1}, 1}}\n</code></pre>\n\n<p>Note that the Tally function displays the number of occurrences of labeled total partitions for each unlabeled total partition, so that $12+3+6+4+1=26$ shows how the 5 unlabeled total partitions of order 4 map to the 26 labeled total partitions. I\'ve tried pure analytic approaches, combinatorial approaches and a hybrid of the two in my Mathematica programs <a href="http://tetration.org/Resources/Files/Mathematica/SchroederSummations.nb" rel="nofollow">Schroeder Summations and Iterate.</a> I believe this is a useful problem in dynamics and combinatorics and merits an efficient answer.</p>\n', 'Tags': '<algorithms><combinatorics><recursion>', 'LastEditorUserId': '16143', 'LastActivityDate': '2014-03-30T07:36:14.487', 'CommentCount': '5', 'AcceptedAnswerId': '23108', 'CreationDate': '2014-03-26T14:00:31.227', 'Id': '23076'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The question is as follows:\nTrue or False: For every non-directed connected non-weighted graph and for every spanning tree T of the graph there exists a vertex v such that T is a DFS tree with the root v.</p>\n\n<p>What about if instead of DFS I used BFS?</p>\n\n<p>I have no clue where to begin with this one. I feel like I'm overlooking some basic characteristic of the algorithm or the tree that it produces. Any help would be appreciated!</p>\n", 'ViewCount': '28', 'Title': 'Question about spanning trees and creating them through BFS and/or DFS algorithms', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-26T23:45:56.843', 'LastEditDate': '2014-03-26T23:45:56.843', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15977', 'Tags': '<algorithms><graph-theory><spanning-trees>', 'CreationDate': '2014-03-26T22:08:28.540', 'FavoriteCount': '2', 'Id': '23101'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Help would be much appreciated.  </p>\n\n<p>"Receive a number and reorder it from the largest to the smallest.<br>\nInput: 13252<br>\nOutput: 53221<br>\nCant use arrays...\nOnly while, for, if/else  ...</p>\n\n<p>any idea? i\'m clueless.<br>\nThanks for the help.</p>\n', 'ViewCount': '52', 'ClosedDate': '2014-03-27T08:39:14.710', 'Title': 'A home assignment. C language', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-27T08:39:56.697', 'LastEditDate': '2014-03-27T08:39:56.697', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16170', 'Tags': '<algorithms><sorting>', 'CreationDate': '2014-03-26T22:20:53.537', 'Id': '23102'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I can solve problem $A$ in polynomial time using only $\\log n$ bits of randomness, with a $\\ge \\frac{2}{3}$ chance of a correct answer.  Then surely I can solve $A$ determistically by running my algorithm for $A$ over all random strings of length $\\log n$ (of which there are a polynomial number) and take a popular vote of the outcomes.</p>\n\n<p>I don't understand, then, why we would ever talk about $O(\\log n)$ amounts of randomness in complexity classes that are closed under polynomial factors.  More specifically, the PCP theorem says $NP = PCP[O(\\log n), O(1)]$ - why isn't that the same as $PCP[0, O(1)]$?</p>\n", 'ViewCount': '25', 'Title': "Why can't we derandomize the PCP theorem by iterating over all possible $\\log n$ random strings?", 'LastActivityDate': '2014-03-26T23:35:06.183', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23107', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16172', 'Tags': '<randomized-algorithms>', 'CreationDate': '2014-03-26T23:13:50.927', 'Id': '23104'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I found an intersting problem. I have to compute <code>n! modulo p</code> and can't figure out a way of doing this. For small <code>n</code>s I can actually compute the factorial, but for large ones, <code>n!</code> doesn't fit a known type.</p>\n\n<p>So, my question is.. how can I compute <code>n! modulo p</code>?</p>\n\n<p><code>p</code> might be prime, but I can't remember the problem exactly.</p>\n", 'ViewCount': '32', 'ClosedDate': '2014-03-27T14:24:28.367', 'Title': 'Computing n! modulo p', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-27T14:22:28.643', 'LastEditDate': '2014-03-27T14:22:28.643', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16193', 'Tags': '<algorithms><number-theory><arithmetic>', 'CreationDate': '2014-03-27T12:49:15.957', 'Id': '23122'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My question is that I'm trying to find the sparsest cut in a connected, undirected graph (all weights are = 1). Basically, I am looking trying to find the smallest cut (i.e., number of edges cut since all weights = 1) while maximizing the number of vertices in the resulting subgraph. How do I approach this problem with flow?</p>\n", 'ViewCount': '20', 'ClosedDate': '2014-04-16T23:02:28.523', 'Title': 'Minimum cut versus sparsest cut?', 'LastActivityDate': '2014-04-14T02:32:51.203', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16201', 'Tags': '<algorithms><graphs><network-flow>', 'CreationDate': '2014-03-27T15:31:57.593', 'Id': '23133'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array $A$ of integers in ascending order, how efficiently can it be decided whether there exists an integer $i$ such that $A[i] = i$? How would an optimal algorithm for this problem work?</p>\n', 'ViewCount': '78', 'ClosedDate': '2014-04-01T22:03:12.943', 'Title': 'Given a sorted array $A$, how can it be efficiently determined whether $\\exists i . A[i] = i$?', 'LastEditorUserId': '69', 'LastActivityDate': '2014-03-27T17:46:44.490', 'LastEditDate': '2014-03-27T17:38:23.723', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15702', 'Tags': '<algorithms><time-complexity>', 'CreationDate': '2014-03-27T16:03:43.970', 'Id': '23135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have points of self-intersecting polygon, its edges and also I am able to find points where it intersects itself using Bentley\u2013Ottmann algorithm.</p>\n\n<p>I planned to build non-self intersecting polygons by editing edges around intersection points, but problem is when you have two edges that intersect, you don't know which two of the four sides are inside, and which are outside of polygon.</p>\n\n<p>I could use ray crossing algorithm to resolve this, but it is too slow. Its time complexity is O(n), and I'd have two do it at least two times for every intersection point. So it would be extremely slow with around 200k points of polygon.</p>\n\n<p>So what I'm asking is, is there any faster way to divide self intersecting polygon into non-self intersecting ones.</p>\n\n<p>I need this because I am doing polygon triangulation. I already done sweep-line triangulation algorithm for triangulating non-self intersecting polygons with holes. So I just need tho have array of these polygons as input.</p>\n", 'ViewCount': '27', 'Title': 'Divide self-intersecting polygon', 'LastActivityDate': '2014-03-27T16:46:50.083', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16207', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-03-27T16:46:50.083', 'Id': '23137'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading <em>The Design of Approximation Algorithms</em> (Williamson and Schmoys; page 50 of <a href="http://www.designofapproxalgs.com/book.pdf" rel="nofollow">this PDF</a>) about solving the minimum-degree spanning tree problem of maximum degree 2 in polynomial time. The text states that we are going to find a tree $T$ with maximum degree at most $2\\mathrm{OPT} +\\lceil\\log_{2} n\\rceil$ in polynomial-time. $n$ is the number of the vertices in the graph.  Could someone clarify why $\\log_{2} n$ is placed there? The text gives no explanation what it is.</p>\n', 'ViewCount': '47', 'Title': 'Error bound for minimum-degree spanning tree', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-27T19:06:31.367', 'LastEditDate': '2014-03-27T19:06:31.367', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23144', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16209', 'Tags': '<algorithms>', 'CreationDate': '2014-03-27T17:08:12.697', 'Id': '23139'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Context</strong></p>\n\n<p>In a 2-dimensional space, suppose $p_0$ is the origin - the lowest point of the Convex Hull (CH), and suppose $p_1, ..., p_{n-1}$ are sorted by their polar angles. When applying Graham scam, I always observe that $p_1$ and $p_{n-1}$ are vertices of the Convex Hull (CH). For instance, consider the <a href="http://kiharalab.org/3d-surfer/v1.0/convex_hull.gif" rel="nofollow">graph</a>, for which $p_{n-1}$ is $p_{12}$.</p>\n\n<p><strong>Question</strong></p>\n\n<p>How can I show this formally, that $p_{n-1} \\in CH$ and $p_1 \\in CH$ for all instances?</p>\n\n<p><strong>My answer</strong></p>\n\n<ul>\n<li><p>$p_1 \\in CH$ because $\\angle p_{n-1}p_0p_1$ must be a left turn, since $p_0$ is the lowest point and $p_1$ has the smallest polar angle above $p_0$; similarly,</p></li>\n<li><p>$p_{n-1} \\in CH$ because $\\angle p_{n-2}p_{n-1}p_{0}$ must be a left turn, since $p_0$ is the lowest point and $p_{n-1}$ has the largest polar angle above $p_0$.</p></li>\n</ul>\n\n<p>Does this sound like a convincing proof?</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '41', 'Title': 'Graham Scan - Why does the first and last points always belong to the convex hull?', 'LastEditorUserId': '657', 'LastActivityDate': '2014-03-27T18:41:48.727', 'LastEditDate': '2014-03-27T18:41:48.727', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-03-27T17:24:56.727', 'Id': '23142'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For a class project we're developing a software that solves a common optimisation problem. After some research we've found out that our problem is called QAP (Quadratic Asssignment Problem) and the algorithm that is commonly used is Branch and Bound. I understand the basics of the problem and I see the need of a lower bound to compute the sollution. I came up with a trivial bound example but our teacher told us lower bounds were no trivial matter and we should do some research. After a while we've found out that the Gilmore-Lawler bound is a good one to solve our problem (or at least good enough for learning purposes). </p>\n\n<p>Although I have read a couple of papers I can't get the grasp of it. The idea seems to be to convert the QAP into an LAP combining the two matrices of the original problem. I've got completely lost after that. How is the number I'm supposed to find as the lower bound calculated? </p>\n\n<p>Also, I'm aware that the lower bound has to be calculated for partial solutions, but how do I do that? The lower bound, as I understood, it's calculated from the program's matrices, which are a parameter for the branch and bound algorithm and are fixed from the beginning aren't they? I'd also need an explanation for that.</p>\n", 'ViewCount': '110', 'Title': 'Trying to understand the Gilmore-Lawler lower bound', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-28T12:34:17.467', 'LastEditDate': '2014-03-28T12:34:17.467', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12801', 'Tags': '<algorithms><optimization><heuristics><lower-bounds><branch-and-bound>', 'CreationDate': '2014-03-27T17:56:59.077', 'Id': '23146'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was wondering, what is so special about a majority of nodes having a value? Why is that the key fact for consensus to work on Paxos?</p>\n\n<p>It says on the paper that "any two majorities have at least one acceptor in common, this works if an acceptor can accept at most one value" (page 2). Its true that if we have two majorities then at least one node between the two must be the same one, but I was not entirely sure if I fully appreciated why that was special for consensus. How does this guarantee that a single value gets accepted? Or how does it aid in that goal?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '29', 'Title': 'What is so special about a majority and why is it the key for Paxos to work? (Paxos made simple)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:34:51.380', 'LastEditDate': '2014-04-30T16:34:51.380', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T06:45:51.627', 'FavoriteCount': '2', 'Id': '23165'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If given that all edges in a graph $G$ are of equal weight $c$, can one use breadth-first search (BFS) in order to produce a minimal spanning tree in <strong>linear time</strong>?</p>\n\n<p>Intuitively this sounds correct, as BFS does not visit a node twice, and it only traverses from vertex $v$ to vertex $u$ iff it hasn't visited $u$ before, such that there aren't going to be any cycles, and if $G$ is connected it will eventually visit all nodes. Since the weight of all edges is equal, it doesn't matter which edges the BFS chose.</p>\n\n<p>Does my reasoning make any sense?</p>\n", 'ViewCount': '318', 'Title': 'If all edges are of equal weight, can one use BFS to obtain a minimal spanning tree?', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-29T19:58:59.877', 'LastEditDate': '2014-03-29T19:58:59.877', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23181', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11424', 'Tags': '<algorithms><graphs><spanning-trees>', 'CreationDate': '2014-03-28T13:34:40.343', 'Id': '23179'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>On page 3 it says "Since all numbers are totally ordered, condition P2 guarantees the crucial safety property that only a single value is chosen."</p>\n\n<p>Where P2 = "If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v."</p>\n\n<p>What I was a little confused was what "totally ordered" meant and how does it guarantee p2?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '34', 'Title': 'What does it mean that numbers are "totally ordered"? (paxos made simple)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:34:05.207', 'LastEditDate': '2014-04-30T16:34:05.207', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23186', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T16:57:03.753', 'Id': '23185'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was reading page 4 of the paper where it says condition $P2^c$:</p>\n\n<p>"For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that </p>\n\n<ul>\n<li><p>(a) no acceptor in S has accepted any proposal numbered less than n, or </p></li>\n<li><p>(b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S."</p></li>\n</ul>\n\n<p>(Where S = any set consisting of a majority of acceptors. \nC = the set of acceptors that have accepted some value c, the letter C stands for the majority that has Chosen a value).</p>\n\n<p>I was trying to understand better the conditions for issuing a proposal, specifically (b) is the one causing troubles for me.</p>\n\n<p>For me (a), makes sense because we don\'t want to issue a new proposal with a higher sequence number if there has been any proposal that has been chosen from the majority we are able to see (i.e. S). Since anything that has been chosen is accepted and since something chosen is part of the majority C, if we issue a new proposal, we could risk confusing the current paxos instance when its already chosen a value.</p>\n\n<p>However, (b) is less clear to me why we want it to hold. Recall (b) is:</p>\n\n<p>(b) = "v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S".</p>\n\n<p>Why are we interested in having that condition? Which safety properties does that condition help us maintain?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '54', 'Title': 'Rational of why Paxos only issues new values if its value is the largest one in the majority that he can see?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:33:18.723', 'LastEditDate': '2014-04-30T16:33:18.723', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T17:50:41.200', 'FavoriteCount': '1', 'Id': '23187'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>On page 4 of the paper, at the bottom where they outline what a prepare request looks like it says:</p>\n\n<blockquote>\n  <p>"A proposer chooses a new proposal number n and sends a request to\n  each member of some set of acceptors, asking it to respond with:</p>\n  \n  <p>(a) A promise never again to accept a proposal numbered less than n, </p>\n  \n  <p>(b) ... "</p>\n</blockquote>\n\n<p>Why does prepare message extract such promise from the set of acceptors that receive such a prepare message? What is the goal and what safety guarantee does it provide? How does it aid Paxos into reaching a consensus to a certain value? How is it crucial so that a majority can reach agreement?</p>\n\n<p>I did some further research to try to answer this question and went to the following yale university notes:</p>\n\n<p><a href="http://pine.cs.yale.edu/pinewiki/Paxos" rel="nofollow">http://pine.cs.yale.edu/pinewiki/Paxos</a></p>\n\n<p>These notes try to justify this point by saying:</p>\n\n<blockquote>\n  <p>"... proposer tests the waters by sending a prepare(n) message to all\n  accepters where n is the proposal number. An accepter responds to this\n  with a promise never to accept any proposal with a number less than n\n  (<strong>so that old proposals don\'t suddenly get ratified</strong>)... "</p>\n</blockquote>\n\n<p>Basically, the way they justify that step is by saying so that old proposals don\'t get ratified/accepted. Which I kind of see why they said that, since if a decision has happened, that rule definitively avoids the consensus from being reversed, which is nice. But, however, the reason I did not feel very convinced about it yet is because it seems to damage termination a lot. What if if a majority was about to be formed in the past and the proposer was ready to send the propose(n) and now because a different prepare(n\') was sent its not able to reach agreement? I can\'t seem to convince myself that that this step is 100% a good idea. </p>\n\n<p>Unless the only reason for that step is because they just want to guarantee that once a value has been chosen/decided, by adopting this rule, its impossible that any old proposal reverses the decision?</p>\n\n<p>Again, I did some further research by reading the yale article and now it says:</p>\n\n<blockquote>\n  <p>"The rule that an acceptor doesn\'t accept any proposal earlier than a\n  round it has acknowledged means that the value v in an ack(n, v, n_v)\n  message never goes out of date\u2014there is no possibility that an\n  acceptor might retroactively accept some later value in round n\' with\n  nv &lt; n\' &lt; n. So the ack message values tell a consistent story about\n  the history of the protocol, even if the rounds execute out of order."</p>\n</blockquote>\n\n<p>However, its not obvious to me why retroactivity might be bad (unless in the case that I already mentioned). Retroactivity could be good if it somehow manages our protocol to reach consensus. Still the only reason I see for such a rule is so that one doesn\'t reverse a decision by accident once a value has been agreed on. </p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '55', 'Title': 'Why does a prepare message wants a promise that an acceptor is never to accept a proposal numbered less than its propose sequence value? (Paxos)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T17:25:43.177', 'LastEditDate': '2014-04-30T17:25:43.177', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T18:47:38.740', 'Id': '23190'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><strong>Context</strong></p>\n\n<p>Consider this algorithm. If the set $\\{\\angle p_ip_{i+1}p_{i+2} : i=0,...,n-1\\}$  does not contain left and right turns, output "yes the polygon is convex"; otherwise, "no".</p>\n\n<p><strong>My answer</strong>  </p>\n\n<p>Consider this nonsimple polygon having 4 vertices; the algorithm above will output "yes" as the set of points does not contain both left and right turns, yet the polygon is not convex. Is this a good counterexample rendering the above algorithm incorrect? </p>\n\n<p><img src="http://i.stack.imgur.com/hHEck.png" alt="enter image description here"></p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '37', 'Title': 'Show that this algorithm does not work for determining convex polygons', 'LastEditorUserId': '15072', 'LastActivityDate': '2014-03-29T11:42:03.130', 'LastEditDate': '2014-03-29T11:42:03.130', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '23202', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><graph-theory><computational-geometry>', 'CreationDate': '2014-03-28T21:33:44.823', 'Id': '23193'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m writing a balanced $n_d$-Hyperoctree data structure in which the only fundamental operations I provide are edge traversals between parent and child nodes. I\'m storing the nodes using a Morton z-curve.</p>\n\n<p>Using only this information:</p>\n\n<ul>\n<li>The level of a node can be computed by traversing to the root of the tree. </li>\n<li>Neighbor searches are performed by traversing the tree up until a common parent is found, and then traversing the tree back down again. </li>\n</ul>\n\n<p>I can compute the following constants depending on the number of spatial dimensions $n_d$:</p>\n\n<ul>\n<li>the #of children of a node: $2^{n_d}$,</li>\n<li><p>the #of $n_d - m$ dimensional neighbors for each node (in 3D, $n_d - 1$ neighbors are faces, $n_d - 2$ neighbors are edges, $n_d - 3$ neighbors are corners):</p>\n\n<p>$2^{n_d} \\begin{pmatrix} n_d \\\\ m \\end{pmatrix}$ </p></li>\n</ul>\n\n<p>The neighbors sharing a $n_d - 1$-dimensional face with each Hypercube form the set $\\mathcal{N}_{n_d - 1}$. They are numerated within this set as follows. The neighbor at the negative side from the node center comes first, then it comes the neighbor at the positive side. That is, for $n_d = 1$ I just have $\\mathcal{N}_{n_d - 1} = \\lbrace 0, 1 \\rbrace = \\lbrace \\mathrm{Left}, \\mathrm{Right} \\rbrace$. </p>\n\n<ul>\n<li><p>In $n_d = 2$, $\\mathcal{N}_{n_d - 1} = \\lbrace 0, 1, 2, 3 \\rbrace = \\lbrace \\mathrm{L}, \\mathrm{R}, \\mathrm{Top}, \\mathrm{Bottom} \\rbrace$. </p></li>\n<li><p>In $n_d = 3$ it is $\\mathcal{N}_{n_d - 1} = \\lbrace 0, 1, 2, 3, 4, 5 \\rbrace = \\lbrace \\mathrm{L}, \\mathrm{R}, \\mathrm{T}, \\mathrm{B}, \\mathrm{Front}, \\mathrm{Back} \\rbrace$</p></li>\n</ul>\n\n<p>and so on. </p>\n\n<p>I haven\'t been able yet to generalize the algorithm for finding the $n_d - 1$ neighbor located at a given position to arbitrary dimensions. My current algorithm traverses the tree up until a common parent node is found. During the up traversal it stores the child positions w.r.t. their parents of the nodes traversed. Then it traverses the tree back down using the reversed path of the up traversal. The child positions are found by finding the siblings of the child positions during the up traversal in the inverted neighbor direction, which can be computed as:</p>\n\n<p>$\\mathrm{inverted\\_neighbor\\_position}_{n_d - 1}(p) = (p + 1) * (p \\% 2 = 0) + (p - 1) * (p \\% 2 \\neq 0)$</p>\n\n<p>However, to know if a common parent has been found, it checks if a parent node has a childe at a given relative position of another child which expressed as a neighbor position, i.e., it checks if a node has a sibling in a given neighbor direction within its parent. I have only a hand-coded stencil for this check and haven\'t been able to generalized.</p>\n\n<ul>\n<li><p>Can this check for a "common parent" be generalized to arbitrary dimensions? How?</p></li>\n<li><p>Example up to 3D: for a child and a neighbor position, returns the child position of the sibling:</p>\n\n<pre><code>// i means, there is no sibling for that child in that direction\n//0  1  2  3  4  5     &lt;&lt; nghbr position\n{ i, 1, i, 2, i, 4},  // child 0\n{ 0, i, i, 3, i, 5},  // child 1\n{ i, 3, 0, i, i, 6},  // child 2\n{ 2, i, 1, i, i, 7},  // child 3\n{ i, 5, i, 6, 0, i},  // child 4\n{ 4, i, i, 7, 1, i},  // child 5\n{ i, 7, 4, i, 2, i},  // child 6\n{ 6, i, 5, i, 3, i}   // child 7\n</code></pre></li>\n</ul>\n\n<p>Assuming that the $n_d - 1$ dimensional neighbors can be found:</p>\n\n<ul>\n<li><p>How can I find $n_d - m$ dimensional neighbors?</p>\n\n<ul>\n<li>Right now, I hand-coded traversals that perform $m$ times searches for $(n_d - 1)$ neighbors. Is there a general way to generate these traversals for $n_d$ dimensions and the $n_d - m$ neighbors?</li>\n</ul></li>\n<li><p>What is a suitable order for $n_d - m$ dimensional neighbors where $m &gt; 1$ ?</p></li>\n</ul>\n\n<p>Most of the research I\'ve found in the literature uses hand coded stencils for, 1,2,3 and 4 dimensions, but no general ways of generating these stencils. So if anyone can point me to relevant literature I would appreciate it.</p>\n', 'ViewCount': '50', 'Title': 'Finding nd - m dimensional neighbors for a given node within a balanced hyperoctree', 'LastEditorUserId': '16250', 'LastActivityDate': '2014-04-14T14:53:55.267', 'LastEditDate': '2014-04-14T14:53:55.267', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16250', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2014-03-28T21:36:29.753', 'Id': '23194'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I have two vectors $V_1, V_2 \\in R^l$. Can they be converted into bit vectors $B_1,B_2 \\in \\{0,1\\}^l $ such that if $V_1, V_2$ is close in Euclidean distance, $B_1,B_2$ is close in hamming distance?</p>\n\n<p>Please note that this is a little different from locality sensitive hashing, where    each vector will be hashed into a bucket with certain probability and $l$ such hash functions will be combined to generate $B_i$. The problem appears to be simpler than that as I am not concerned with dimensionality reduction, which is a goal of LSH. </p>\n\n<p>Side note: Please feel free to add new tags as I am not sure which tags should go here. </p>\n', 'ViewCount': '35', 'Title': 'Distance preserving projection for Euclidean distance', 'LastActivityDate': '2014-03-28T22:09:00.227', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16255', 'Tags': '<algorithms><hash><edit-distance>', 'CreationDate': '2014-03-28T22:09:00.227', 'Id': '23196'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and was wondering, what guarantees Paxos to converge and not run forever without a consensus/agreement on a value? Is it guaranteed to converge always or is it a probabilistic bound that the probability that it does not converge is really really small?</p>\n\n<p>Because I was reading condition $P2^c$ and it seems possible to me that because of that condition, Paxos might loop forever if we are not careful (I think, maybe I am wrong, but I would love to know why!) Take the following case:</p>\n\n<p>I was a little worried about the case when, say that a majority was close to forming but there is a new node that wants to propose his new value but was unlucky and did not happen to communicate with that growing set that nearly formed a majority (its not a majority yet, but it was reaalllyyy close!). That node was ready to prepare his value but was that unlucky and reasoned "Ok, none of the nodes I spoke to had a value, thus, time to propose my value!" but his sequence number is much higher than the previous one that was forming on the other side and it starts to spread, wouldn\'t it be because we want to satisfy (b) in $P2^c$? i.e. doesn\'t it damage the convergence to a decision (never mind the run time, it might never converge...)? Even in the case of a very good quality network..?</p>\n\n<p>Because now this new node has a higher value and his value spreads like a disease and it <em>could</em> happen again, right before it was about to form a majority with his own value. That situation could happen again and again and again no value is decided! Right? What prevents Paxos from being in this situation? Or what is the argument to convince me that its really unlikely to happen many times and that it converges in polynomial time most of the time?</p>\n\n<p>Recall condition $P2^c$ is the following:</p>\n\n<p>$P2^c$</p>\n\n<p>"For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that\n(a) no acceptor in S has accepted any proposal numbered less than n,\nor\n(b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S."</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '36', 'Title': 'What guarantees Paxos to converge (terminate)? (i.e. not run forever without a consensus)', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T10:20:41.300', 'LastEditDate': '2014-04-29T10:20:41.300', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><distributed-systems>', 'CreationDate': '2014-03-29T03:28:29.840', 'Id': '23208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Say that for a particular problem, e.g., the independent set problem, it has been shown that no polynomial-time algorithm exists to solve it.</p>\n\n<p>Could we get around this by finding an algorithm which approximates the solution to a certain accuracy?</p>\n\n<p>That is, would the result above bar the existence of an algorithm which finds a maximum independent set to an accuracy of 0.5? I.e., it is guaranteed to be less than 0.5 away from the size of a maximum set? (And hence implying that it actually <em>is</em> a maximum independent set.)</p>\n\n<p>It seems to me that the latter wouldn't violate our proofs of non-tractability, which are discrete in nature, while still giving an answer that satisfies the problem from a practical perspective.</p>\n", 'ViewCount': '252', 'Title': 'Approximating NP-complete problems', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-30T16:47:19.050', 'LastEditDate': '2014-03-30T16:47:19.050', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '23239', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12588', 'Tags': '<algorithms><graph-theory><algorithm-analysis><time-complexity>', 'CreationDate': '2014-03-30T01:43:13.067', 'Id': '23236'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was confused about one specific part. How does or why does property $P2^b$ satisfy property $P2^c$?</p>\n\n<p>These are the properties:</p>\n\n<p>$P2^b = $ If a proposal with value $v$ is chosen, then every higher-numbered (i.e. approx. later in time)  proposal issue by any proposer has value $v$.</p>\n\n<p>$P2^c$ = For any $v$ and $n$, if a proposal with value $v$ and number $n$ is issued, then there is a set $S$ (some majority) of acceptors such that either:</p>\n\n<p>(a)no acceptor in $S$ has accepted any proposal numbered less than $n$, or</p>\n\n<p>(b)$v$ is the value of the highest-numbered proposal among all proposals numbered less than $n$ accepted by the acceptors in $S$ (some majority).</p>\n\n<p>The paper uses $S$ to denote some majority and $C$ to denote some majority that has actually <em>chosen</em> a value.</p>\n\n<p>The thing that I am confused about is, for me $P2^b$ is saying, ok once a value has been chosen, say at sequence number $n$ (i.e. roughly time $n$), then after that time, we want to make sure that any proposer is only able to propose the value of the majority (chosen value). If we have that, then, we do not risk the already formed majority from reverting weirdly. i.e. once we have formed a majority, we want it to stick and stay like that. However, it was not 100% clear to me why property $P2^c$ satisfied that requirement. I kind of see why (a) is a nice property to have, since, having (a) means that its safe to issue a new proposal $(n, v)$ since we contacted some majority $S$ and none of them had accepted anything in a time earlier than now $n$. So, if a majority had formed we would have seen at least one value and we did not see anything accepted, its safe to propose something since a majority has not formed.  </p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '16', 'Title': "How do we make sure in Paxos that we don't propose a different value if a majority has formed?", 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:35:20.363', 'LastEditDate': '2014-04-30T16:35:20.363', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-30T06:57:57.560', 'FavoriteCount': '1', 'Id': '23247'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When getting source array length, I want to generate the array of swaps that need to be performed in order to sort the source array. I want to make this array <strong>as small as possible</strong>. Swaps will be performed only if necessary for sorting.</p>\n\n<p>Example </p>\n\n<ul>\n<li>input: <strong>3</strong>; </li>\n<li>Output: <strong>[0,1 , 1,2 , 0,1];</strong></li>\n</ul>\n\n<p>I want to understand how to calculate the number of such swaps and how exactly to generate such array.</p>\n\n<p><strong>Edit:</strong> Some thing like <a href="http://pages.ripco.net/~jgamble/nw.html" rel="nofollow">network sorting</a>.</p>\n', 'ViewCount': '84', 'Title': 'Find the minimum amount of swaps to sort array', 'LastEditorUserId': '10572', 'LastActivityDate': '2014-03-30T15:07:02.737', 'LastEditDate': '2014-03-30T14:30:19.207', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '23259', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<algorithms><sorting>', 'CreationDate': '2014-03-30T10:44:53.997', 'Id': '23251'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have to craft up a number made of 19 digits so that, after some mathematical operations made 11 times, the resulting sum of the remainders of the first ten operations is equal to the the remainder of the eleventh operation. The eleven calculations are as follows. I need to reverse them in order to be able to craft up the x.</p>\n\n<p>op_10 : (x / 256) mod 256 => This is the remainder that must be equal to the sum of the ten remainders below.</p>\n\n<p>op_0 : (x / 72057594037927936) mod 256 => 1st remainder</p>\n\n<p>op_1 : (x / 281474976710656) mod 256 => 2nd remainder</p>\n\n<p>op_2 : (x / 65536) mod 16 => 3rd remainder</p>\n\n<p>op_3 : (x / 1048576) mod 16 => 4th remainder</p>\n\n<p>op_4 : (x / 16777216) mod 16 => 5th remainder</p>\n\n<p>op_5 : (x / 268435456) mod 16 => 6th remainder</p>\n\n<p>op_6 : (x / 4294967296) mod 16 => 7th remainder</p>\n\n<p>op_7 : (x / 68719476736) mod 16 => 8th remainder</p>\n\n<p>op_8 : (x / 1099511627776) mod 16 => 9th remainder</p>\n\n<p>op_9 : (x / 17592186044416) mod 16 => 10th remainder</p>\n\n<p>Is there such an algorithm?\nx is the number i need to craft up.</p>\n', 'ViewCount': '39', 'Title': 'Reversing Key Algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T07:46:16.000', 'LastEditDate': '2014-03-31T07:46:16.000', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16290', 'Tags': '<algorithms><number-theory>', 'CreationDate': '2014-03-30T12:57:15.223', 'Id': '23256'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I am trying to get an idea of how the <a href="https://en.wikipedia.org/wiki/Aks_primality_test" rel="nofollow">AKS primality test</a> should be interpreted as I learn about it, e.g. a corollary for proving that PRIMES \u2286 P, or an actually practical algorithm for primality testing on computers.</p>\n\n<p>The test has polynomial runtime but with high degree and possible high constants. So, in practive, at which $n$ does it surpass other primality tests?\nHere, $n$ is the number of digits of the prime, and "surpass" refers to the approximate runtime of the tests on typical computer architectures.</p>\n\n<p>I am interested in functionally comparable algorithms, that is deterministic ones that do not need conjectures for correctness.</p>\n\n<p>Additionally, is using such a test over the others practical given the test\'s memory requirements?</p>\n', 'ViewCount': '294', 'Title': 'When is the AKS primality test actually faster than other tests?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-02T22:41:02.053', 'LastEditDate': '2014-03-31T07:48:51.077', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '16292', 'Tags': '<algorithms><efficiency><primes>', 'CreationDate': '2014-03-30T13:40:07.150', 'FavoriteCount': '3', 'Id': '23260'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I came across the following problem in a exam. </p>\n\n<p>We choose a permutation of n elements $[1,n]$ uniformly at random. Now a variable MIN holds the minimum value seen so far at it is defined to $\\infty$ initially. Now during our inspection if we see a smaller value than MIN, then MIN is updated to the new value. </p>\n\n<p>For example, if we consider the permutation, </p>\n\n<p>$$5\\ 9\\ 4\\ 2\\ 6\\ 8\\ 0\\ 3\\ 1\\ 7$$\nthe MIN is updated 4 times as $5,4,2,0$. Then the expected no. of times MIN is updated?</p>\n\n<p>I tried to find the no. of permutations, for which MIN is updated $i$ times, so that I can find the value by $\\sum_{i=1}^{n}iN(i)$, where $N(i)$, is the no. of permutations for which MIN is updated $i$ times.</p>\n\n<p>But for $i\\geq2$, $N(i)$ is getting very complicated and unable to find the total sum.</p>\n', 'ViewCount': '190', 'Title': 'Expected number of updates of minimum', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T22:33:27.977', 'LastEditDate': '2014-03-31T17:47:01.783', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16323', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms>', 'CreationDate': '2014-03-31T15:20:35.840', 'Id': '23295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Please can anyone explain me the worst ,average and best case running time for the Unifrom binary search .. Also how can the lookup table be explained?</p>\n', 'ViewCount': '19', 'ClosedDate': '2014-04-01T07:40:45.420', 'Title': 'Uniform Binary Search explanation and lookup table', 'LastEditorUserId': '16347', 'LastActivityDate': '2014-04-01T08:37:14.043', 'LastEditDate': '2014-04-01T08:37:14.043', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16347', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><binary-search>', 'CreationDate': '2014-04-01T06:48:15.420', 'Id': '23313'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been researching on AdaBoost and GentleBoost classifiers, but can't seem to find a clear answer to the question:</p>\n\n<ul>\n<li>What is Adaboost better at classifying in computer vision?</li>\n<li>What is GentleBoost better at classifying?</li>\n</ul>\n\n<p>I've been told that AdaBoost is good for things with soft edges, like facial recognition, while GentleBoost is good for things with harder and more symmetrical features and edges, like vehicles. Is this true? Is there any proof for this or any evidence to back up this claim?</p>\n", 'ViewCount': '26', 'Title': 'Advantages of adaboost over gentleboost in applications, or vice versa?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T13:37:38.917', 'LastEditDate': '2014-04-01T13:37:38.917', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<algorithms><reference-request><machine-learning><classification><computer-vision>', 'CreationDate': '2014-04-01T10:34:09.543', 'FavoriteCount': '1', 'Id': '23316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the problem:</p>\n\n<blockquote>\n  <p>Given an undirected graph and two of its vertices, is there a path between them? </p>\n</blockquote>\n\n<p>I often read that this problem can be solved in linear time in the number of vertices! I am not sure why this claim holds.</p>\n\n<p>Can this really be done in linear time (not amortized) without preprocessing?</p>\n', 'ViewCount': '189', 'Title': 'Can we test whether two vertices are connected in time linear in the number of nodes?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T17:56:51.647', 'LastEditDate': '2014-04-01T17:55:46.217', 'AnswerCount': '3', 'CommentCount': '10', 'AcceptedAnswerId': '23323', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16358', 'Tags': '<algorithms><graphs>', 'CreationDate': '2014-04-01T15:36:52.380', 'Id': '23320'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there any algorithm which takes edges (given by its two end points), and determines in which cell (or cells) of grid it is?</p>\n\n<p>Grid has fixed dimensions and number of cells. Grid is represented by its cells with matrix. And every cell has list of edges that intersect that cell.</p>\n\n<p>Input is set of pair points, but I can also transform it in just set of points, or any other needed representation.\nOutput should be the mentioned grid with cells who contain list of edges that intersect that cell.</p>\n\n<p>Algorithm should be fast and robust, and by that I mean it covers special (degenerated) cases and that its time complexity is good.</p>\n\n<p>What I want to be able is to use that grid later for search, for example to answer me question like "Which cells does given edge AB(with end points A and B) intersect?" or "Give me all edges that intersect cell 12"(First row, second column).</p>\n', 'ViewCount': '44', 'Title': 'Algorithm for storing polygon edges into grid', 'LastEditorUserId': '16207', 'LastActivityDate': '2014-04-02T12:08:49.930', 'LastEditDate': '2014-04-02T12:08:49.930', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16207', 'Tags': '<reference-request><computational-geometry><search-algorithms>', 'CreationDate': '2014-04-01T16:03:33.280', 'Id': '23321'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The algorithms for the problem I am working on become more and more complex as I try to improve their performance. They already span several pages with cases and sub-cases, and will probably become even longer. I am worried that there might be mistakes that are difficult to notice.</p>\n\n<p>As a programmer, I am used to writing detailed test-cases to test my programs, but the algorithm is written in pseudo-code (it is not easy to implement). What ways can you recommend for testing the algorithm?</p>\n', 'ViewCount': '108', 'Title': 'How can I debug my pseudocode algorithm?', 'LastEditorUserId': '472', 'LastActivityDate': '2014-04-03T17:43:22.733', 'LastEditDate': '2014-04-02T11:41:24.963', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '23348', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><software-testing>', 'CreationDate': '2014-04-02T11:32:10.490', 'Id': '23338'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I read in the artificial-intelligence book of Russel and Norvig that The tree-search version of A* is optimal if heuristic function is admissible, while the graph-search version is optimal if heuristic function is consistent(monotone).\nAn admissible heuristic is one that never overestimates the cost to reach the goal.\nA heuristic $h(n)$ is consistent if, for every node $n$ and every successor $n\'$ of $n$ generated by any action $a$, the estimated cost of reaching the goal from $n$ is no greater than the step cost of getting to $n\'$  plus the estimated cost of reaching the goal from $n\'$ :\n$h(n) \u2264 c(n, a, n\') + h(n\')$.</p>\n\n<p>My question is about this graph and heuristic.</p>\n\n<p><img src="http://i.stack.imgur.com/HBX9A.png" alt="enter image description here"></p>\n\n<p>Suppose this graph is a state space of a problem in Artificial intelligence.  $A$ is the start node(initial state),and $D$ is the goal. Numbers on the edges are path costs. Numbers on the nodes are value of heuristic function for this problem.\nI think this heuristic function is consistent. So A* can find the optimal path from start to goal.</p>\n\n<pre><code>step 1: g(A)=0, h(A)=5, so f(A)=5\n        Expand A : B, C\n        add A to close list.\n        add B and C to open list.\n\nstep 2: g(B)=10, h(B)=1, so f(B)=11\n        g(C)=1, h(C)=8, so f(C)=9\n        f(C) &lt; f(B) so:\n            Expand C : D\n            add C to close list\n            add D to open list.\n\nstep 3: g(D)=1+16=17,h(D)=0, so f(D)=17\n        f(B) &lt; f(D) so:\n            Expande B : nothing because D is already in open list.\n\nstep 4: Just D in open list so\n             Expand D : D is goal\n\nResult: path:ACD, cost=17\n</code></pre>\n\n<p>A* found the path ACD but optimal path is ABD.</p>\n', 'ViewCount': '53', 'Title': 'Optimality of A*', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-02T18:54:04.303', 'LastEditDate': '2014-04-02T18:54:04.303', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16375', 'Tags': '<algorithms><artificial-intelligence><search-algorithms>', 'CreationDate': '2014-04-02T16:58:12.097', 'FavoriteCount': '1', 'Id': '23351'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to find a linear solution with a small constant factor but I\'m not sure what to search for, or even how to succinctly describe it.  The best I\'ve come up with is:</p>\n\n<blockquote>\n  <p>Given a set of rectangles on a plane find the set(s) which allow the same <code>y</code> value for some largest contiguous set of <code>x</code> values.  All rectangles are axis aligned, the same width, and do not overlap.</p>\n</blockquote>\n\n<p>I find this much easier to visualize so below is an example problem instance and solution.</p>\n\n<p><img src="http://i.stack.imgur.com/uKphj.png" alt="Longest horizontal intersection of contiguous blocks"></p>\n\n<p>Edit:</p>\n\n<p>We have a linear solution that has a constant factor on the order $y_{max} - y_{min}$ which can be pretty big.  Here is an idea of the algorithm I\'ve been trying to work out since originally posting this.</p>\n\n<ul>\n<li>Rectangles are already sorted by $x$ position.</li>\n<li>Maintain an ordered list $I$ of the intervals currently allowing a contiguous line.</li>\n<li>Maintain two variables $(i_{min}, i_{max})$ which are the min and max\n$y$ value of the current intersection (in the example solution $(5, 5.25)$).</li>\n<li>Iterate from $x_0$ to $x_{max}$</li>\n<li><p>At each $x$ position test if any of the current rectangle(s) intersect $(i_{min}, i_{max})$.</p>\n\n<ul>\n<li>1) If yes, add the rectangle to $I$ and update $(i_{min}, i_{max})$.</li>\n<li>2) If no, find the longest suffix of $I$ s.t. it allows overlap with current rectangle.</li>\n<li>3) If no suffix exists or the current $x$ position has no rectangles skip to the next $x$ position with rectangles and reinitialize $I$, and $(i_{min}, i_{max})$.</li>\n</ul></li>\n<li><p>In #2 and #3 save the current $I$ if it allows the widest contiguous line so far.</p></li>\n</ul>\n', 'ViewCount': '51', 'Title': 'What is this problem? Largest set of contiguous x values for which the same y value can be held', 'LastEditorUserId': '527', 'LastActivityDate': '2014-04-03T00:34:55.823', 'LastEditDate': '2014-04-02T23:59:58.480', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '527', 'Tags': '<algorithms><data-structures><computational-geometry><intervals>', 'CreationDate': '2014-04-02T19:00:40.687', 'FavoriteCount': '1', 'Id': '23361'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is an assignment of an introductory course of complexity theory and I need to <em>find</em> a way to do the following:</p>\n\n<p>Given $n,m \\in \\Bbb N$, is $n \\le m!$ ?</p>\n\n<p>The idea is to provide a <strong>Post Machine</strong> that can decide this in an <em>efficient way</em>, using $n,m$ in a binary codification.</p>\n\n<p>We know that the factorial isn't efficient, so the problem actually is just to find a way to decide this, if it's possible.</p>\n\n<p>I know how to compare if $n\\le m$, but the factorial is my problem. \n<strong>I don't how how to compute $m!$ with a Post Machine</strong>, if possible, in polynomial-time.</p>\n\n<p>I guess that the most simple way to do this is comparing $n$ with factorials of numbers that are lower than $m$, but the factorial it's still my problem.</p>\n\n<p>My question, is there an algorithm that can help me?</p>\n", 'ViewCount': '115', 'Title': 'Algorithm to decide if $n \\le m!$', 'LastEditorUserId': '11936', 'LastActivityDate': '2014-04-04T23:09:10.580', 'LastEditDate': '2014-04-02T21:19:47.417', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '23439', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11936', 'Tags': '<algorithms><decision-problem><integers>', 'CreationDate': '2014-04-02T19:06:07.690', 'Id': '23363'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array, say A, with a finite definite length like N (e.g. 1000) can we define a problem to be NP-Complete without any intentional injection of NP-Completeness by something else :</p>\n\n<p>for example questions like this are not acceptable:</p>\n\n<p>"Solve this, NP-Complete, problem using A as an auxiliary array."<hr/></p>\n\n<p>Please note that N is a predefined constant.</p>\n', 'ViewCount': '48', 'ClosedDate': '2014-04-04T12:30:27.823', 'Title': 'NP-Complete algorithm defined on a fixed size array', 'LastActivityDate': '2014-04-04T15:23:01.813', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '15050', 'Tags': '<algorithms><np-complete>', 'CreationDate': '2014-04-03T11:27:32.477', 'Id': '23390'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>You are given two things: A fixed initial 'model' partition of an interval, e.g.</p>\n\n<pre><code>I------I---I-----I-------I----...\n</code></pre>\n\n<p>where each <code>-</code> or <code>I</code> represents an element in a discrete time series and the <code>I</code>s are the partition boundaries.  This can also be represented as a sequence of subinterval lengths, i.e. 7, 4, 6, 8, ...</p>\n\n<p>Then, you're given a new set of subinterval lengths; and the task is to arrange these in such a way as to get as many coincident <code>I</code>s as possible.  Or equivalently, you are given a new partition on an interval of the same length (though, critically, the new partition may have greater or fewer elements) and the task is to shuffle the subintervals around to maximize alignment.  So if the model was</p>\n\n<pre><code>I------I---I-----I-------I----I\n</code></pre>\n\n<p>and you are given 2, 11, 5, 12, i.e.</p>\n\n<pre><code>I-I----------I----I-----------I\n</code></pre>\n\n<p>then the solution would be 11, 2, 12, 5, </p>\n\n<pre><code>I----------I-I-----------I----I\n           *             *\n</code></pre>\n\n<p>achieving alignment at 2 locations (marked with an asterisk, compare to model).</p>\n\n<p>There is an additional constraint: The locations of the aligned subintervals must be distributed approximately randomly throughout the length of the solution.  The simplest means of getting a partition with at least some alignment to the model would be to build the new partition segment by segment, drawing without replacement from the collection of test segments, aligning where possible.  But this would strongly bias the occurrences of alignment towards the beginning of the time series, and is therefore not allowed.  There is of course also the brute force O(n!) enumeration but my series are little too long for that.</p>\n\n<p>Naturally a solution that finds the optimal permutation would be great, but one that is efficient and gets a permutation with a substantial fraction of the possible alignment would also be good.  My current version is a variation on the 'simple' algorithm derived above, except only drawing from a small subcollection of subintervals so as to avoid bias.  I know it can be improved upon!</p>\n", 'ViewCount': '66', 'Title': 'Permute the subintervals of an interval partition to most closely align with a model partition', 'LastEditorUserId': '16410', 'LastActivityDate': '2014-04-03T23:56:25.543', 'LastEditDate': '2014-04-03T12:56:05.663', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '23405', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16410', 'Tags': '<algorithms><permutations><partitions>', 'CreationDate': '2014-04-03T11:28:45.423', 'Id': '23391'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem that I encountered that boils down to the following:</p>\n\n<p>Considered this directed graph I found on Google: <img src="http://i.stack.imgur.com/BTt0Z.png" alt="enter image description here"></p>\n\n<p>I have the following information available to me</p>\n\n<pre><code>Node: Ancestors\n\n1 : 3\n2 : 1 3 5 7\n3 : Null\n4 : 3 5\n5 : 3\n6 : 1 2 3 4 5 7 \n7 : 1 3\n8 : 1 2 3 4 5 6 7\n</code></pre>\n\n<p>How can I re-construct the original graph in a reasonably efficient manner? I basically have large sets of data that I would like to have visualized as branches and merges(similar to a code repository, but not quite).</p>\n\n<p>Note: While I believe my data shouldn\'t be disjoint, I\'m somewhat certain my data is incomplete and will produce disjoint graphs, or at the very least have many separate "roots". There is no ordering to the data, everything must be considered random, the lists can also be thought of as sets. </p>\n', 'ViewCount': '39', 'Title': 'Reconstruct directed graph from list of ancestors for each node', 'LastEditorUserId': '16433', 'LastActivityDate': '2014-04-04T13:27:17.890', 'LastEditDate': '2014-04-04T02:55:04.440', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16433', 'Tags': '<algorithms><graph-theory><graphs><sets>', 'CreationDate': '2014-04-04T02:30:23.000', 'Id': '23408'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On the following picture, we have overlapping polygons: we know the positions of vertices and the edges for each polygons, and the intersections are exactly known (vertices at the intersection are represented on the figure). Is it possible to \'partition\' the domain spanned by the union of these polygons into \'disjoint\' polygons (with contacts only at the boundary)? To be more precise, I\'d like to associate to all edges the surfaces for which the edge is contained in its boundary.</p>\n\n<p><img src="http://i.imgur.com/rQ3uxmf.png" alt="try"></p>\n\n<p>Please advise if I\'m not clear enough and thanks in advance for any suggestions!</p>\n', 'ViewCount': '25', 'Title': 'Partition overlapping polygons', 'LastActivityDate': '2014-04-04T13:08:20.263', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16439', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-04-04T11:38:42.093', 'FavoriteCount': '1', 'Id': '23416'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '79', 'Title': 'How to compute a curious inverse', 'LastEditDate': '2014-04-05T16:42:34.627', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16449', 'FavoriteCount': '1', 'Body': '<p>Let $M$ be a square matrix with entries that are $0$ or $1$ and let $v$ be a vector with values that are also $0$ or $1$.  If we are given $M$ and $y = Mv$, we can computer $v$ if $M$ is non-singular.  </p>\n\n<p>Now let us take the second bit (from the right) of the binary representation of each $y_i$ as another vector $z$. So $z$ also has entries which are $0$ or $1$. If $y_i$ has fewer than two bits we just let $z_i=0$.  </p>\n\n<blockquote>\n  <p>If we are given $z$ and $M$, how (and when) can you find a $v$ so that\n  $Mv$ would produce $z$ under this operation?</p>\n</blockquote>\n\n<p>Here is an example</p>\n\n<p>$$M = \\begin{pmatrix}\n  0 &amp; 0 &amp; 1 &amp; 0\\\\\n  1 &amp; 1 &amp; 0 &amp; 1\\\\\n  1 &amp; 1 &amp; 1 &amp; 0\\\\\n  0 &amp; 1 &amp; 1 &amp; 1\\\\\n\\end{pmatrix}\n, v = \\begin{pmatrix}\n  0 \\\\ \n  1 \\\\ \n  1 \\\\\n   1\\\\\n\\end{pmatrix}\n\\implies Mv=\\begin{pmatrix}\n  1 \\\\ \n  2 \\\\\n  2 \\\\ \n  3\\\\\n\\end{pmatrix}\n.$$</p>\n\n<p>So in this case </p>\n\n<p>$$z = \n\\begin{pmatrix}\n0 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n$$</p>\n\n<hr>\n\n<p>Is this problem in fact NP-hard?</p>\n', 'Tags': '<algorithms><np-hard><linear-algebra>', 'LastEditorUserId': '10359', 'LastActivityDate': '2014-04-05T16:42:34.627', 'CommentCount': '0', 'AcceptedAnswerId': '23429', 'CreationDate': '2014-04-04T17:13:48.063', 'Id': '23428'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>(Re-posted from StackOverflow as suggested)</p>\n\n<p>I have the following problem.</p>\n\n<p>The functions $f(x),g(x)$ are defined as\n$$\nf(x) = \\begin{cases} f_1(x) &amp; 0 \\leq x \\leq 10, \\\\ f_2(x) &amp; 10 &lt; x \\leq 20, \\\\ 0 &amp; \\text{otherwise}, \\end{cases} \\qquad\ng(x) = \\begin{cases} g_1(x) &amp; 0 \\leq x \\leq 5, \\\\ g_2(x) &amp; 5 &lt; x \\leq 20, \\\\ 0 &amp; \\text{otherwise}, \\end{cases}\n$$\nIn addition, we require the constraints\n$$\n\\int_0^{20} f(x) dx \\geq K, \\quad \\int_0^{20} g(x) dx \\geq Q, \\quad f(x)+g(x) \\leq R \\text{ for all $x$}.\n$$\nwhere $K,Q,R$ are parameters.</p>\n\n<p>I assume there is quite some elaborate theory behind it, and was wondering if anybody could point me in the right direction to devise an algorithm that can generate $f_1(x), f_2(x), g_1(x), g_2(x)$?</p>\n\n<p>I would like to add that for a given $K$ and $Q$, the interest is to keep $R$ as low as possible.</p>\n', 'ViewCount': '46', 'Title': 'Solving System of Equations', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-05T16:56:57.373', 'LastEditDate': '2014-04-05T16:53:20.290', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '23454', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16474', 'Tags': '<algorithms>', 'CreationDate': '2014-04-05T16:00:18.533', 'Id': '23452'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '62', 'Title': 'Automatic seat assignment algorithm', 'LastEditDate': '2014-04-05T20:43:50.023', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16478', 'Body': '<p>I am looking for articles relating to algorithms that deal with automatic selection of seating assignment.\nI need an algorithm (preferably more than one) that can automatically select a seating place while enforcing certain constraints that are predefined.\nOriginally I was planning on having the seats selected on the fly, meaning whenever a new person comes, the system selects the optimal seat for him based on the seats which were already taken, but I guess it is not a must.\nif there is a more general algorithm that can also present an approach fit for my problem that is also great.</p>\n\n<p>Lets call the seated people "players" , and our seating domain lets picture as a 2d matrix. lets say we have several groups among our "players" and you can set your "players" anywhere within the matrix as long as they are not seated next to other "players" from their own group . I am not claiming there is a perfect solution, I am looking for articles that are dealing with some approach for giving a solution - if you can direct me to an article or even give me a name for that kind of problem it is also good for me.</p>\n\n<p>Thanks,\nOlaf</p>\n', 'ClosedDate': '2014-04-07T07:05:22.693', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '16478', 'LastActivityDate': '2014-04-06T07:37:45.630', 'CommentCount': '6', 'AcceptedAnswerId': '23467', 'CreationDate': '2014-04-05T17:48:27.070', 'Id': '23458'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '477', 'Title': 'Best solutions to 6 degrees of separation', 'LastEditDate': '2014-04-06T21:52:34.007', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11306', 'FavoriteCount': '2', 'Body': '<p>From purely my knowledge of computer science a simple breadth first search from root A in search of node B, while keeping track of the depth of the tree, would be the most effective way to check whether A and B have 6 degrees of separation. If I simply wanted to check whether B is within 6 degrees I could also limit my depth to 6. </p>\n\n<p>I have heard however that there are better ways of doing this using bidirectional methods which involve some heuristics. I was wondering if someone could explain the most effective way of doing this and compare space and time complexity between the different approaches. Thanks!</p>\n\n<p>And as a followup, what would be a good algorithm for finding the degree of separation between two arbitrary nodes A and B and the path between them?</p>\n', 'Tags': '<algorithms><graphs><search-algorithms>', 'LastEditorUserId': '11306', 'LastActivityDate': '2014-04-06T21:52:34.007', 'CommentCount': '3', 'AcceptedAnswerId': '23479', 'CreationDate': '2014-04-06T18:03:22.393', 'Id': '23477'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m studying for my finals and I can across this statement.</p>\n\n<p>"For a fixed set of (unique) keys, any binary search tree containing those keys can\nbe converted to any other BST on the same set of keys via a sequence of left- and/or right-\nrotations."</p>\n\n<p>I\'m interested in a proof. Does anyone know any references?</p>\n', 'ViewCount': '78', 'Title': 'Unique keys in a binary search tree', 'LastActivityDate': '2014-04-10T16:49:59.190', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15826', 'Tags': '<algorithms><algorithm-analysis><binary-trees>', 'CreationDate': '2014-04-06T18:43:38.257', 'Id': '23481'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have two <code>True or False</code> questions in my practice test that are related but I am unsure about:</p>\n\n<pre><code>1. If an optimization problem can be solved using a greedy algorithm, \nthere must be a solution for this optimization problem using dynamic programming as well.\n\n2. If an optimization problem can be solved using dynamic programming, \nthere must be a solution for this problem using a greedy algorithm as well.\n</code></pre>\n\n<p>I think the answers are <code>1. True</code> and <code>2. False</code> is this correct?</p>\n', 'ViewCount': '96', 'ClosedDate': '2014-04-14T17:37:38.693', 'Title': 'Dynamic programming VS Greedy Algroithms', 'LastActivityDate': '2014-04-07T02:28:55.033', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '23495', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16514', 'Tags': '<algorithms><dynamic-programming><greedy-algorithms>', 'CreationDate': '2014-04-07T01:29:57.837', 'Id': '23493'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for public algorithm which gives the engine these abilities:</p>\n\n<ul>\n<li>Query by ranked terms</li>\n<li>Limit outcome by date/time range</li>\n</ul>\n\n<p>Basically, i'd like to concentrate articles (generally <code>title|text|timestamp</code>) identify the source and make N-N correlation to terms (is term for datasource same marking as term for dataentry?)</p>\n\n<p>Given the database of such information</p>\n\n<pre><code>entry_data_type:[type_id|title|description]\nentry_data:[entry_id|data_type_id|data_content]\nentry:[id|entry_type(data,source)|parent_entry_id|created|updated]\nterms(keywords):[id|keyword]\nentry2term:[entry_id|term_id|term_weight]\n</code></pre>\n\n<p>Where keywords are both automatically defined (text frequency analysis) and manually assigned (probably abstract terms in context to entry contents)</p>\n\n<p>I should be able to query by keywords like this: <code>kw1:3 kw2:10 kw3:-2 [range:-7 days]</code><br>\nand output shall be entries sorted by given keyword weights (pattern <code>keyword:weight</code>)</p>\n\n<p>I thought about something similar to EdgeRank, but that is social-graph-oriented, and I'm looking for more straight-forward solution (more selfish, meaning input filter is given by personal preferences, not social-graph-near preferences or social-score ranking)</p>\n\n<p>Also TF-IDF would have to be limited by time, so the document base to calculate the entry score is inserted in given date/time range only. Is there any possible break-down of TF-IDF ranking, eg. to pre-calculate raw-data for each day and then, based on query, merge them for given date-range?</p>\n\n<p>This question is independent of any particular programming language, platform, etc. I'm generally looking for keywords to look for, papers to read or ready implementations to study, but accepted are only answers not using paid or closed-source software parts or non-public-domain patents.</p>\n", 'ViewCount': '16', 'Title': 'TF-IDF query engine in context of terms weight', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-07T16:53:08.100', 'LastEditDate': '2014-04-07T16:53:08.100', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16077', 'Tags': '<reference-request><search-algorithms><statistics><search-problem><ranking>', 'CreationDate': '2014-04-07T01:30:29.197', 'Id': '23494'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What I understated is this:</p>\n\n<p>Probabilistic algorithm: an algorithm that produces result in determined time with a <code>confidence</code> value about the probability of the produced result being correct.</p>\n\n<p>Randomized algorithm: an algorithm that uses some sort of randomization to reach the result, that too with absolute surety of reaching the correct result.</p>\n\n<p>Is this correct? And what exactly is the difference between these two?</p>\n', 'ViewCount': '25', 'ClosedDate': '2014-04-07T05:51:07.407', 'Title': 'Difference between probabilistic and randomized algorithm', 'LastActivityDate': '2014-04-07T05:20:31.250', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16518', 'Tags': '<algorithms>', 'CreationDate': '2014-04-07T05:02:33.260', 'Id': '23500'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A town has $N$ people.\nAt Day 0, a person has a secret. At Day 1, he calls a random person and tells him the secret. At Day 2, every person who knows the secret calls a person at random to tell the secret. In this way secret propagates.</p>\n\n<p>Let X be the number of days till everybody knows the secret. What will be the Expectation of X? If $X_i$ be the number of people who know the secret at end of day $i$, what will be the Expectation of $X_i$?</p>\n\n<p>$Z$ = min{$k$|$X_k = N$}. Then what will be the bound of $Z$? What is the expected number of phone calls required so that $X_i = N$?</p>\n', 'ViewCount': '20', 'ClosedDate': '2014-04-07T13:54:25.063', 'Title': 'Expected time taken to spread message in gossip-based protocol', 'LastActivityDate': '2014-04-07T11:47:03.013', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'user3505352', 'PostTypeId': '1', 'OwnerUserId': '16533', 'Tags': '<randomized-algorithms>', 'CreationDate': '2014-04-07T05:21:23.793', 'Id': '23507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>i\'m a little bit lost ... can you help me ?</p>\n\n<p>So I have this table of date (each row give a point with its group)</p>\n\n<p><img src="http://i.stack.imgur.com/h3wef.png" alt="data table"></p>\n\n<p><img src="http://i.stack.imgur.com/RnEgL.jpg" alt="enter image description here"></p>\n\n<p>So i took a random weight let\'s say : [1, -2]\nH = 1 if n =&lt; 0\n    0 otherwise</p>\n\n<p>a= H([1,-2][6,3]) = H(0) = 1 but the target output is 0 ... so we have to update the weight:\nw -> w - p = [5 , -5] .\nNext : </p>\n\n<p>a= H([5,-5][3,3]= H(0)=1 the problem is : there the target output is 1 so we don\'t have to update the weight , but that\'s strange because the [5 -5] vector doesn\'t draw a linear separation between the 2 groups ...</p>\n\n<p>Thanks for your help ? :) </p>\n', 'ViewCount': '55', 'ClosedDate': '2014-04-14T17:40:03.533', 'Title': "Perceptron learning rule doesn't work", 'LastActivityDate': '2014-04-08T12:53:53.387', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16532', 'Tags': '<algorithms><machine-learning><neural-networks>', 'CreationDate': '2014-04-07T14:53:01.470', 'Id': '23518'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m reading <a href="http://download-software.intel.com/sites/default/files/article/165685/clmul-wp-rev-2.01-2012-09-21.pdf">this Intel white paper on carry-less multiplication</a>. It describes multiplication of polynomials in $\\text{GF}(2^n)$. On a high level, this is performed in two steps: (1) multiplication of polynomials over $\\text{GF}(2)$, and (2) reducing the result modulo an irreducible polynomial. We use the "standard" bitstring representation of polynomials, i.e. $x^3+x+1 = [1011]$.</p>\n\n<p>The paper gives an algorithm for calculation of the remainder polynomial on page 16 in Algorithm 3. However, I\'m having trouble understanding the reduction algorithm on pages 16-17 (Algorithm 4). Essentially, I think we need Algorithm 4 for larger fields when our or partial results don\'t fit 128 bits anymore. They give an example for multiplication of two polynomials in $\\text{GF}(2^{128})$.</p>\n\n<blockquote>\n  <p>Where do the "magic constants" 63, 62, and 57 for right shifts, and the "magic constants" 1, 2, and 7 for left shifts come from?</p>\n</blockquote>\n\n<p>For example, how does one generalize the algorithm for smaller fields, say $\\text{GF}(2^{32})$? Would the corresponding shift values then be 15, 14, 9 and 1, 2, 7?</p>\n\n<blockquote>\n  <p>In the final step 4, the algorithm tells you to "XOR $[E_1:E_0]$, $[F_1:F_0]$, and $[G_1:G_0]$ with each other and $[X_3:D]$".</p>\n</blockquote>\n\n<p>Why do we do this? As far as I can see, the result of this XOR operation is neither stored anywhere nor used anywhere. Is it somehow used for computing $[H_1 : H_0]$?</p>\n', 'ViewCount': '79', 'Title': "Understanding Intel's algorithm for reducing a polynomial modulo an irreducible polynomial", 'LastEditorUserId': '16379', 'LastActivityDate': '2014-04-10T16:46:42.737', 'LastEditDate': '2014-04-08T12:51:39.823', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23574', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '16379', 'Tags': '<algorithms><integers><polynomials><multiplication>', 'CreationDate': '2014-04-08T12:44:30.947', 'Id': '23549'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '140', 'Title': 'If $\\log xy=\\log x+\\log y$ then why multiplication is harder than addition?', 'LastEditDate': '2014-04-08T17:51:22.997', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '16535', 'FavoriteCount': '1', 'Body': "<p>Someone told me that the $\\log$ function was introduced to make the calculation easier. If we have to calculate $xy$, we can calculate instead $\\log x+\\log y$ since $\\log xy=\\log x+\\log y$. How this can make the calculation easier? Maybe from a mathematician point of view but what about a computer scientist's point of view?</p>\n\n<p>If it makes the calcualtion easier then why people do not use it to simplify the complexity of the multiplication algorithms?</p>\n\n<p>From my own thinking, this transformation makes the calculation more difficult. How can we calculate the $\\log x$ and $\\exp x$ functions in a computer?</p>\n\n<p>Am I right? Any suggestions please? Thank you for your time.</p>\n", 'Tags': '<algorithms><complexity-theory><reference-request><education>', 'LastEditorUserId': '16535', 'LastActivityDate': '2014-04-08T20:37:30.863', 'CommentCount': '2', 'AcceptedAnswerId': '23564', 'CreationDate': '2014-04-08T16:13:52.080', 'Id': '23554'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am coding a procedure that takes an integer $d$, and generates $d$ finite lists $X_1 \\ldots, X_d$ of elements. I would then like for it to output a list of the elements in the product set $X_1 \\times \\cdots \\times X_d$. </p>\n\n<p>I can't use nested for-loops because $d$ can vary so I wouldn't know how many to nest. I'm sure there's a totally standard solution to this problem, but I don't know enough to search for it successfully either here on online.</p>\n\n<p>For what it's worth, here's one dumb solution I came up with. Let $b$ be the maximum cardinality of the sets $X_i$. Then run a single loop for $n$ running from $0$ to $b^d$; for each $n$, write it in base $b$ and use the $i^{\\rm th}$ digit to read off the element of $X_i$ corresponding to that digit (and ignore if any of those digits are too big for the cardinality of the corresponding set). This will work, but feels like a pretty stupid solution. </p>\n\n<p>What's the standard way of doing this? </p>\n", 'ViewCount': '79', 'Title': 'How to enumerate a product set?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T22:42:28.783', 'LastEditDate': '2014-04-09T21:28:51.973', 'AnswerCount': '3', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16561', 'Tags': '<algorithms><combinatorics><sets>', 'CreationDate': '2014-04-08T17:45:31.770', 'Id': '23556'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have a source node $S$, destination node $D$ and a set $A$ of intermediate nodes $P_1, P_2, \\dots$ in an edge-weighted undirected graph. I want to find the vertex $P_i\\in A$ that minimizes $\\mathrm{dist}(S, P_i) + \\mathrm{dist}(D, P_i)$?  In addition, the overall path from $S$ to $D$ should contain only one node from the set $A$.  What is an efficient algorithm for this? I don't want to go with brute-force approach.</p>\n", 'ViewCount': '114', 'Title': 'Minimum path between two vertices passing through a given set exactly once', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T19:05:53.140', 'LastEditDate': '2014-04-09T13:09:03.500', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16563', 'Tags': '<algorithms><graphs><shortest-path>', 'CreationDate': '2014-04-08T18:26:55.787', 'FavoriteCount': '1', 'Id': '23558'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been reading up on boosting algorithms.</p>\n\n<p>I understand that the main crux of the algorithm is to build weak classifiers that are slightly better than random guessing, and then to add them up so that we end up with a strong classifier. This is done via "boosting rounds".</p>\n\n<p>With different papers using different variables, like <em>k</em>, <em>n</em>, <em>m</em>, etc, it\'s gotten a bit confusing.</p>\n\n<p>I just want to confirm, if I run a boosting algorithm for, say 150 rounds, is that equivalent to saying that I\'m training 150 <em>weak classifiers</em>? I mean, is a weak classifier the output of one boosting round?</p>\n', 'ViewCount': '25', 'Title': 'Boosting algorithms: Confusion between "weak classifiers" versus "number of boosting rounds"', 'LastActivityDate': '2014-04-09T07:23:23.323', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23585', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10418', 'Tags': '<algorithms><classification>', 'CreationDate': '2014-04-08T19:14:37.247', 'Id': '23561'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '37', 'Title': 'Construct a digraph given its in-degree and out-degree distribution', 'LastEditDate': '2014-04-09T09:53:02.030', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16571', 'FavoriteCount': '1', 'Body': "<p>Could anyone help me with this algorithmic problem:</p>\n\n<p>Given the in and out degrees of a set of vertices, is it possible to determine if there exist a valid graph respecting this constraint? The graph can allow self loops but not parallel edges.</p>\n\n<p>Here's an example: </p>\n\n<pre><code>Vertex A: in=1 out=1\nVertex B: int=2 out=2\n</code></pre>\n\n<p>For which we can construct this graph:</p>\n\n<pre><code>A =&gt; B\nB =&gt; A\nB =&gt; B\n</code></pre>\n\n<p>Here's another example:</p>\n\n<pre><code>Vertex A: in=0 out=1\nVertex B: in=1 out=1\n</code></pre>\n\n<p>Here, we obviously cannot construct such graph.</p>\n\n<p>I have been scratching my head around this problem. \nFor an undirected graph, there exist a simple algorithm to solve this problem but I cannot find any way to derive a solution for directed graphs.</p>\n\n<p>I have the intuition that we could find a matching algorithm in the bipartite graph representing the in and out edges of the graph and where each out-edge would be matched to an in-edge. </p>\n\n<p>However the usual approach can produce a graph with parallel edges.</p>\n\n<p>For example 1, a valid solution could be:</p>\n\n<pre><code>A- A+: A =&gt; A\nB- B+: B =&gt; B\nB- B+: B =&gt; B\n</code></pre>\n\n<p>Which is not a valid graph.</p>\n\n<p>Also, please note that I am more interested in determining if a valid solution exists. It's not necessary to provide or construct such solution.  </p>\n", 'Tags': '<algorithms><graphs>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T09:53:02.030', 'CommentCount': '2', 'AcceptedAnswerId': '23578', 'CreationDate': '2014-04-09T00:39:42.843', 'Id': '23575'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Students will identify certain students they want to work with. I have therefore decided to split them into two groups where I want to minimize the number of people in Group 1 who want to work with students from Group 2. </p>\n\n<p>I was thinking about creating a source node <em>s</em>, and creating a node for each person ($p_i$) - followed by hooking up the <em>s</em> to each $p_i$. Then I would create, another series of nodes for each person ($q_i$) and hook up each $p_i$ to each $q_i$ if $p_i$ <strong>doesn\'t</strong> want to work with $q_i$. Then, I would hook up each $q_i$ to a terminal node <em>t</em>. Each of the edges would have weight 1.</p>\n\n<p>I was thinking about running Edmonds\u2013Karp on it. Now, the solution would yield the maximum bipartite matching of the group (see e.g. <a href="https://www.youtube.com/watch?v=c9uLwB6aUVQ" rel="nofollow">here</a>). For each active arc from $p_i$  to $q_i$ in the final diagram, I would separate those two students.</p>\n\n<p>However, I have a bad taste in my mouth after running this algorithm; the bad taste stems from modeling the instance with respect to my intention: If I maximize the complement (the desire not to work with someone), do I really minimize the desire of students to work with each other across the two groups?</p>\n\n<p>If my hunch is correct (in that I\'m wrong), please point me in the right direction.</p>\n', 'ViewCount': '53', 'Title': 'How to optimally seperate a student body?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T09:54:48.057', 'LastEditDate': '2014-04-09T09:53:59.487', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23586', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<algorithms><optimization><polynomial-time><bipartite-matching>', 'CreationDate': '2014-04-09T06:15:38.270', 'Id': '23582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How do people measure performance overhead? Whenever someone is bragging about how their program or application performs better than another, they talk about particular measurements, eg time, performance etc. </p>\n\n<p>Whilst I understand how time to perform a task can be calculated, I cannot understand how performance is measured, and I can't find out thought Google.</p>\n\n<p>Are there multiple ways? Are they all accepted or do people have issues with certain methods?</p>\n", 'ViewCount': '51', 'Title': 'How do people measure performance overhead?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T23:26:53.140', 'LastEditDate': '2014-04-09T21:43:47.333', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '23614', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16580', 'Tags': '<algorithms><terminology><performance>', 'CreationDate': '2014-04-09T11:25:17.617', 'Id': '23591'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There are lots of <a href="/questions/tagged/algorithm-analysis" class="post-tag" title="show questions tagged &#39;algorithm-analysis&#39;" rel="tag">algorithm-analysis</a> questions around. Many are similar, for instance those asking for an analysis of nested loops or divide &amp; conquer algorithms, but most answers seem to be tailor-made.</p>\n\n<p>On the other hand, the answers <a href="http://cs.stackexchange.com/questions/192/how-to-come-up-with-the-runtime-of-algorithms">to another general question</a> explain the larger picture (in particular regarding asymptotic analysis) with some examples, but not how to get your hands dirty.</p>\n\n<p>Is there a structured, general method for analysing algorithms?</p>\n\n<p><sup>This is supposed to become a <a href="http://meta.cs.stackexchange.com/questions/599/reference-questions">reference question</a> that can be used to point beginners to; hence its broader-than-usual scope. Please take care to give general, didactically presented answers that are illustrated by at least one example but nonetheless cover many situations. Thanks!</sup></p>\n', 'ViewCount': '2051', 'Title': 'Is there a system behind the magic of algorithm analysis?', 'LastActivityDate': '2014-04-10T15:08:23.810', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '32', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><reference-question>', 'CreationDate': '2014-04-09T12:59:52.003', 'FavoriteCount': '30', 'Id': '23593'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading up on <a href="http://en.wikipedia.org/wiki/Dynamic_programming">Dynamic Programming</a> when I came across the following quote</p>\n\n<blockquote>\n  <p>A dynamic programming algorithm will examine all possible ways to\n  solve the problem and will pick the best solution. Therefore, we can\n  roughly think of dynamic programming as an <strong>intelligent, brute-force\n  method that enables us to go through all possible solutions to pick\n  the best one</strong>. If the scope of the problem is such that going through\n  all possible solutions is possible and fast enough, dynamic\n  programming guarantees finding the optimal solution</p>\n</blockquote>\n\n<p>The following example was given </p>\n\n<blockquote>\n  <p>For example, let\'s say that you have to get from point A to point B as\n  fast as possible, in a given city, during rush hour. A dynamic\n  programming algorithm will look into the entire traffic report,\n  looking into all possible combinations of roads you might take, and\n  will only then tell you which way is the fastest. Of course, you might\n  have to wait for a while until the algorithm finishes, and only then\n  can you start driving. The path you will take will be the fastest one\n  (assuming that nothing changed in the external environment)</p>\n</blockquote>\n\n<p><a href="http://en.wikipedia.org/wiki/Brute-force_search">Brute Force</a> is trying every possible solution before deciding on the best solution . </p>\n\n<p>How is Dynamic Programming different from Brute Force if it also <strong>goes through all possible solutions before picking the best one</strong> , the only difference i see is that Dynamic Programming takes into account the additional factors ( traffic conditions in this case).</p>\n\n<p>Am  i correct to say that Dynamic Programming is a subset of Brute Force method ??</p>\n', 'ViewCount': '1511', 'Title': 'How is Dynamic programming different from Brute force', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T13:32:34.613', 'LastEditDate': '2014-04-09T22:36:34.710', 'AnswerCount': '6', 'CommentCount': '5', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><terminology><optimization><dynamic-programming>', 'CreationDate': '2014-04-09T15:58:08.683', 'FavoriteCount': '1', 'Id': '23599'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a> is often quoted as being used to find the shortest path route however I was surprised to know that there exist A* search which is a extension of Dijkstra\'s algorithm. </p>\n\n<p>How is it that A* search algorithm is able to perform better compared to Dijkstra\'s algorithm , what sort of technique does it used that Dijkstra\'s algorithm did not use ???</p>\n', 'ViewCount': '84', 'Title': "How is A* search superior to Dijkstra's algorithm", 'LastActivityDate': '2014-04-13T01:53:10.240', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2014-04-10T02:18:11.010', 'Id': '23619'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The <a href="http://en.wikipedia.org/wiki/K-d_tree" rel="nofollow">link</a> in wikipedia about kd-trees store points in the inner nodes. I have to perform NN queries and I <strong>think</strong> (newbie here), I am understanding the concept.</p>\n\n<p>However, I was said to study Kd-trees from Computational Geometry Algorithms and Applications (De Berg, Cheong, Van Kreveld and Overmars), section 5.2, page 99. The main difference I can see is that Overmars places the splitting data in the inner nodes and the actual points of the dataset in the leaves. For example, in 2D, an inner node will hold the splitting line.</p>\n\n<p>Wikipedia on the other hand, seems to store points in inner nodes and leaves (while Overmars only on leaves).</p>\n\n<p>In this case, how do we perform nearest neighbour search? Moreover, why there is this difference?</p>\n', 'ViewCount': '36', 'Title': 'kd-tree stores points in inner nodes? If yes, how to search for NN?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T14:43:00.437', 'LastEditDate': '2014-04-10T14:43:00.437', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16638', 'Tags': '<data-structures><computational-geometry><search-algorithms><search-trees><nearest-neighbour>', 'CreationDate': '2014-04-10T13:20:27.113', 'Id': '23636'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am working on an algorithm that requires me to interpolate a couple trillion positive discrete points with f(x) having low finite value (for example 0 - 5). It there a specialized algorithm specific for this reduced problem that can provide 0 error interpolation?\nCould you maybe point me to a few papers?</p>\n\n<p>EDIT: I noticed that i can make the trillions of points become less, but then the max value of f(x) rises exponentially (if i cut them by half, the max value gets squared). If i can get for example 4 points and interpolate with a 3rd degree function, is it guaranteed to pass through all points?</p>\n', 'ViewCount': '31', 'Title': '0 error interpolation for discrete finite value points', 'LastEditorUserId': '16647', 'LastActivityDate': '2014-04-10T15:24:00.573', 'LastEditDate': '2014-04-10T15:24:00.573', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16647', 'Tags': '<numerical-analysis><numerical-algorithms>', 'CreationDate': '2014-04-10T14:49:27.107', 'Id': '23642'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I read that in a 2D space, the two points farthest away must be in the convex hull (CH).</p>\n\n<p>Intuitively, I can see why. If the two farthest points are not in the convex hull, then there must be a point that is outside the convex hull (contradiction). I know that a vertex of CH has two adjacent edges that converge at that point, which is of further distance from any other vertex or point within CH, than  the non-vertices beside it. What I mean is shown in the image below: a vertex (p) with two adjacent edges,</p>\n\n<p><img src="http://i.stack.imgur.com/AU0YX.png" alt="enter image description here"></p>\n\n<p><strong>Problem is</strong>, I don\'t see how I can prove this more formally.  I am looking for a more concrete proof.</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '62', 'Title': 'Why are the two farthest points vertices of the Convex Hull?', 'LastEditorUserId': '15072', 'LastActivityDate': '2014-04-21T19:03:15.407', 'LastEditDate': '2014-04-10T15:55:19.067', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23647', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-04-10T15:48:47.490', 'Id': '23646'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Has the following problem been studied before? If yes, what approaches/algorithms were developed to solve it?</p>\n\n<blockquote>\n  <p><strong>Problem ("Maximum Stacking Height Problem")</strong></p>\n  \n  <p>Given $n$ polygons, find their stable, non-overlapping arrangement\n  that <strong>maximizes their stacking height</strong> on a fixed floor under the\n  influence of gravity.</p>\n</blockquote>\n\n<p><br></p>\n\n<h2>Example</h2>\n\n<p>Three polygons:</p>\n\n<p><img src="http://i.stack.imgur.com/SbCt3.png" alt="enter image description here"></p>\n\n<p>and three of their infinitely many stable, non-overlapping arrangements, with different stacking heights:</p>\n\n<p><img src="http://i.stack.imgur.com/h938i.png" alt="enter image description here"></p>\n\n<p><br></p>\n\n<h2>Clarifications</h2>\n\n<ul>\n<li>All polygons have uniform mass and equal density</li>\n<li>Friction is zero</li>\n<li>Gravity is acting on every point into the downwards direction (i.e. the force vectors are all parallel)</li>\n<li>A configuration is not considered stable if it rests on an unstable equilibrium point (for example, the green triangle in the pictures can not balance on any of its vertices, even if the mass to the left and the right of the balance point is equal)</li>\n<li>To further clarify the above point: A polygon is considered unstable ("toppling") <em>unless</em> it rests on at least one point <em>strictly to the left</em> <strong>and</strong> at least one point <em>strictly to the right</em> of its center of gravity (this definition greatly simplifies simulation and in particular makes position integration etc. unnecessary for the purpose of evaluating whether or not an arrangement is stable.</li>\n<li>The problem in its "physical" form is a continuous problem that can only be solved approximately for most cases. <strong>To obtain a discrete problem that can be tackled algorithmically, constrain both the polygon vertices and their placement in the arrangement to suitable lattices.</strong></li>\n</ul>\n\n<p><br></p>\n\n<h2>Notes</h2>\n\n<ul>\n<li>Brute force approaches of any kind are clearly infeasible. Even with strict constraints on the placement of polygons inside the lattice (such as providing a limited region "lattice space") the complexity simply explodes for more than a few polygons.</li>\n<li>Iterative algorithms must bring some very clever heuristics since it is easy to construct arrangements where removing any single polygon results in the configuration becoming unstable and such arrangements are unreachable by algorithms relying on every intermediate step being stable.</li>\n<li>Since the problem smells at least NP- but more likely EXPTIME-complete in the total number of vertices, even heuristics would be of considerable interest. <strong>One thing that gives hope is the fact that most humans will recognize that the third arrangement in the example is optimal.</strong></li>\n</ul>\n', 'ViewCount': '46', 'Title': 'Maximum Stacking Height Problem', 'LastActivityDate': '2014-04-10T18:20:35.550', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16652', 'Tags': '<algorithms><time-complexity><optimization><computational-geometry><heuristics>', 'CreationDate': '2014-04-10T17:14:32.580', 'Id': '23651'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<h2>Task and terminology</h2>\n\n<p>Assume we have a set $X$ and two sequences $S_1 = (a_1, a_2, \\ldots,a_n)$ and $S_2 = (b_1, b_2, \\ldots,b_n)$, where $a_i \\in X, b_i \\in X, \\forall i \\in [1..n]$.</p>\n\n<p>We define, that two sequences are <em>congruent</em> and denote them $S_1 \\cong S_2$ if $\\exists k,l \\in [1..n]$ such that $a_{k+i \\pmod{n}}=b_{l+i \\pmod{n}}, \\forall i \\in [1..n]$. (Plainly said, they would define the same cycle.) If $S_1 \\cong S_2$ then we say that $S_1$ is a <em>rotation</em> of $S_2$ and vice-versa.</p>\n\n<p>The task is to find an algorithm that determines if $S_1 \\cong S_2$.</p>\n\n<h2>Problem</h2>\n\n<p>In the case that $X$ is a <strong>total order set</strong> the task can be solved quite easily with a linear algorithm. (Finding a lexicographical extremum of all rotations for both sequences and then comparing those extrema.)</p>\n\n<p>The question is, is there linear algorithm if $X$ is a <strong>setoid</strong>? I.e. members of $X$ can be checked only for equivalence. If not, does at least exist an algorithm with a better complexity than brute force?</p>\n', 'ViewCount': '30', 'ClosedDate': '2014-04-11T06:43:48.323', 'Title': 'How to check if two sequences of setoid members are mutual rotations?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-11T06:43:42.743', 'LastEditDate': '2014-04-11T06:43:42.743', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16669', 'Tags': '<algorithms><discrete-mathematics>', 'CreationDate': '2014-04-11T01:30:03.200', 'Id': '23663'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '102', 'Title': 'EM algorithm for two Gaussian models', 'LastEditDate': '2014-04-11T15:34:11.077', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16682', 'FavoriteCount': '1', 'Body': '<p>This is about basic machine learning which I do not understand clearly.</p>\n\n<p>I have 2 Gaussian models $G_1$ and $G_2$ and given a list of data as below.</p>\n\n<p>(1, 3, 4, 5, 7, 8, 9, 13, 14, 15, 16, 18, 23)</p>\n\n<p>I do not know which numbers were generated by which $G_i$ model, but I want to do Expectation Maximization for clustering.</p>\n\n<p>So, what are the hidden variables? and how to perform Expectation and Maximization steps?</p>\n\n<p>Could you show some calculation or short Python code of the likelihood metric? (Perhaps 2 iteration)</p>\n\n<p>I got the basics of EM algorithm from here, but I cannot apply to this question. <a href="http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html" rel="nofollow">http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html</a></p>\n', 'Tags': '<algorithms><machine-learning>', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-18T00:00:26.993', 'CommentCount': '0', 'AcceptedAnswerId': '23681', 'CreationDate': '2014-04-11T13:50:46.657', 'Id': '23673'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the time complexity of Halley\'s Method? </p>\n\n<p>I am thinking ${\\cal O}(\\log(n)F(n))$, or something very similar to Newton-Raphson, but I feel as though there should be some change to the complexity in order to yield the greater convergence. However, I can\'t seem to find any solid ground to support any of my ideas (as nobody has ever seemed to have asked this question on the internet before)...</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Halley%27s_method" rel="nofollow">Halley\'s Method - Wikipedia</a></p>\n', 'ViewCount': '29', 'Title': "Time Complexity of Halley's Method", 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-16T01:10:02.363', 'LastEditDate': '2014-04-11T15:25:57.560', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16683', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-04-11T14:05:23.880', 'Id': '23674'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Using gradient descent in <em>d</em> dimensions to find a local minimum requires computing gradients, which is computationally much faster than Newton's method, because Newton's method requires computing both gradients and Hessians.</p>\n\n<p>However, gradient descent generally requires many more iterations than Newton's method to converge within the same accuracy.</p>\n\n<p>My question, then, is:</p>\n\n<p>Assuming they both converge, in terms of the <em>number of elementary floating-point operations</em>, which is usually faster:  Newton's method or gradient descent?  Why?</p>\n", 'ViewCount': '32', 'Title': "Gradient descent vs. Newton's method: which is more efficient?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T13:38:17.120', 'LastEditDate': '2014-04-12T13:38:17.120', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<algorithms><algorithm-analysis><optimization><numerical-algorithms>', 'CreationDate': '2014-04-12T11:27:02.383', 'Id': '23701'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the following algorithm</p>\n\n<pre><code>void NQueen(int k,int n)\n{\n  int i;\n  for(i=1;i&lt;=n;i++)\n  {\n    if(place(k,i)==1)\n    {     x[k]=i;\n            if(k==n)\n            {\n                printf("Solution\\n");\n                printboard(n);\n            }\n            else\n                NQueen(k+1,n);\n    }\n  }\n}\n\nint place(int k,int i)\n{\n  int j;\n  for(j=1;j&lt;k;j++)\n  {\n    if((x[j]==i)||abs(x[j]-i)==abs(j-k))\n        return 0;\n  }\n  return 1;\n}\n\nvoid printboard(int n)\n{\n  int i;\n  for(i=1;i&lt;=n;i++)\n    printf("%d  ",x[i]);\n}\n\nvoid main()\n{\n    int n;\n    printf("Enter Value of N:");\n    scanf("%d",&amp;n);\n    NQueen(1,n);\n}\n</code></pre>\n\n<p>I am having trouble understanding the time complexity of the following algorithm.It has time complexity: $O(n^n)$, As NQueen function is recursively called n times.But is there is any tighter bound possible for this program? what about best case, and worst case time complexity?</p>\n\n<p>Can someone help me understand the time complexity of the algorithm?</p>\n', 'ViewCount': '24', 'ClosedDate': '2014-04-12T21:38:52.353', 'Title': 'How to get time complexity of n queen puzzle algorithm(Using Backtracking)?', 'LastEditorUserId': '16713', 'LastActivityDate': '2014-04-14T13:12:49.753', 'LastEditDate': '2014-04-14T13:12:49.753', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16713', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-04-12T20:06:22.897', 'Id': '23713'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the fastest way to get one of the value from set of intervals which occurs most frequently?</p>\n\n<p>For example if I have interval (0, 3) and (2, 4) the most frequent value can be 2 or 3.</p>\n', 'ViewCount': '30', 'ClosedDate': '2014-04-13T09:59:15.193', 'Title': 'The most frequent integer value from set of intervals', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-13T07:54:29.607', 'LastEditDate': '2014-04-13T07:54:29.607', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16727', 'Tags': '<algorithms><intervals>', 'CreationDate': '2014-04-12T21:41:53.963', 'Id': '23717'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I always thought that TSP currently requires time exponential in the number of cities to solve.</p>\n\n<blockquote>\n  <p>How, then, has <a href="http://www.math.uwaterloo.ca/tsp/concorde.html" rel="nofollow">Concorde</a> optimally solved a TSP instance with\n  <strong>85,900 cities</strong>?!?</p>\n</blockquote>\n\n<p>Is this a typo? Is the base of the exponential 1.0000000000000001 or similar? Was it an instance specifically constructed to be solvable easily? What <em>is</em> the asymptotic runtime of the best known TSP solving algorithm?</p>\n', 'ViewCount': '93', 'Title': 'What is the asymptotic runtime of the best known TSP solving algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-13T10:00:05.303', 'LastEditDate': '2014-04-13T10:00:05.303', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23732', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16732', 'Tags': '<algorithms><np-hard><traveling-salesman>', 'CreationDate': '2014-04-13T07:43:43.813', 'Id': '23731'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>According to  <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">Introduction to algorithms by Cormen et al</a>,\n$$T(n)=2T(n/2)+n\\log n$$ is not case 3 of Master Theorem. Can someone explain me why?</p>\n\n<p>And which case of master theorem is it?</p>\n', 'ViewCount': '35', 'ClosedDate': '2014-04-13T13:12:06.137', 'Title': '$T(n)=2T(n/2)+n\\log n$ and the Master theorem', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-13T13:49:05.647', 'LastEditDate': '2014-04-13T12:18:48.650', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16715', 'Tags': '<algorithms><algorithm-analysis><master-theorem>', 'CreationDate': '2014-04-13T12:01:52.337', 'Id': '23735'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to think about dynamic algorithms which allows negative integers in input (where it\'s problematic, because obviously it\'s not always the case)?</p>\n\n<p>Examples:</p>\n\n<ul>\n<li><a href="http://en.wikipedia.org/wiki/Partition_problem" rel="nofollow" title="Partition problem">Partition Problem</a> with negative numbers in set allowed</li>\n<li><a href="http://en.wikipedia.org/wiki/Knapsack_problem" rel="nofollow" title="Knapsack problem">Knapsack problem</a> with negative weights are allowed</li>\n</ul>\n\n<p>Is there some general idea to handle these or other similar dynamic algorithms?</p>\n', 'ViewCount': '49', 'ClosedDate': '2014-04-29T23:36:13.323', 'Title': 'Knapsack problem, partition problem, or in general dynamic algorithm with negative numbers allowed', 'LastActivityDate': '2014-04-13T22:55:43.990', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '23756', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16743', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2014-04-13T15:15:36.993', 'FavoriteCount': '1', 'Id': '23740'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What is the difference between maximal flow and maximum flow. I am reading these terms while working on Ford Fulkerson algorithms and they are quite confusing. I tried on internet, but couldn't get a reasonable answer. I believe maximum flow is quite clear as it means maximum amount of flow that can be transferred from source to sink in a network, but what exactly is maximal flow.</p>\n\n<p>Please answer in layman terms if possible.</p>\n\n<p>Thanks.</p>\n", 'ViewCount': '100', 'Title': 'What is the difference between maximal flow and maximum flow?', 'LastActivityDate': '2014-04-14T01:38:17.857', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16750', 'Tags': '<algorithms><graph-theory><graphs><network-flow>', 'CreationDate': '2014-04-14T00:31:01.150', 'Id': '23763'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we have an $N \\times N \\times N$ 3-d sorted array meaning that every row,column, and file is in sorted order. Searching for an element in this structure can be done using $O(N^2)$ comparisons. However are $\\Omega(N^2)$ comparisons needed in the worst-case? For an $N \\times N$ 2-d sorted array I recall a proof that $\\Omega(N)$ comparisons are needed; I'm having trouble seeing how to extend to the 3-d case though</p>\n", 'ViewCount': '24', 'Title': 'Lower bound on number of comparisons needed to search for a number in a sorted 3-d array', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-14T18:45:49.407', 'LastEditDate': '2014-04-14T18:28:24.510', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23793', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><arrays><lower-bounds>', 'CreationDate': '2014-04-14T17:58:35.807', 'Id': '23791'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm writing an algorithm to solve a research problem involving searching for numbers on very large arrays.</p>\n\n<p>I encountered a sub-problem that requires me to break up sums of numbers which are power of 2, i.e. how to find combinations of these numbers that sum up to a certain value. The sum and the combination size are given. Also only 4 power of 2s can be present in a combination: <code>2,4,8,16</code>.</p>\n\n<p>Lets suppose I have a sum of <code>8</code> and I need to break up into a set of <code>3</code> numbers. The result would invariably be <code>[2,2,4]</code>.</p>\n\n<p>If I need to break up <code>20</code> into 3 power of 2s, I could have <code>[2,2,16]</code> or <code>[4,8,8]</code>.</p>\n\n<p>Naturally larger sets will result in many possible combinations.</p>\n\n<p>My aim is to write an algorithm that obtains all possible combinations as efficiently as possible. I'm not looking for the algorithm itself, but some leads into how to partition the number.\nWhat are some properties of power of 2s that I need to use to partition a sum into a possible combination?</p>\n", 'ViewCount': '77', 'LastEditorDisplayName': 'user16784', 'Title': 'Breaking up sum of power of 2s', 'LastActivityDate': '2014-04-15T18:19:50.530', 'LastEditDate': '2014-04-15T10:51:52.943', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'user16784', 'PostTypeId': '1', 'Tags': '<algorithms><number-theory>', 'CreationDate': '2014-04-15T08:41:24.113', 'Id': '23806'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In section 9.2 of CLRS (<em>Introduction to Algorithms; page 185 in the 2nd edition and page 215 in the 3rd edition</em>), a randomized selection algorithm is presented. </p>\n\n<p>For its analysis, $T(n)$ is a random variable denoting the time required on an input array $A[p \\cdots r]$ of $n$ elements and $X_k$ is an indicator random variable $X_k = I \\{ \\text{the subarray } A[p \\cdots q] \\text{ has exactly } k \\text{ elements (due to the pivot)} \\}$. </p>\n\n<p>It has been claimed that $X_k$ and $T(\\max(k-1, n-k))$ are independent (page 187 in the 2nd edition and page 218 in the 3rd edition). However, I find it quite counter-intuitive to understand. How to verify it?</p>\n', 'ViewCount': '26', 'Title': 'Why are the two random variables independent in the analysis of Randomized Selection algorithm in CLRS?', 'LastActivityDate': '2014-04-15T17:13:42.643', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23814', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithm-analysis><randomized-algorithms>', 'CreationDate': '2014-04-15T13:18:26.367', 'Id': '23811'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to figure out what the big o estimate of the following for loop is.</p>\n\n<pre><code>for(int i = 10; i to 100; i++){\nfor(int j = 1; j to i; j++){\nfor(int k = 1; k to j; k++){\ns=s+i+j;\n}\n}\n}\n</code></pre>\n\n<p>I was thinking that the two inner loops run n times but I am unsure about the first loop. </p>\n\n<p>What would the Big O estimate be for this code?</p>\n', 'ViewCount': '49', 'Title': 'What is the Big O estimate of a nested for loop?', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-15T21:50:41.567', 'LastEditDate': '2014-04-15T21:50:41.567', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16798', 'Tags': '<algorithms><loops>', 'CreationDate': '2014-04-15T17:28:03.380', 'Id': '23815'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have a array of real numbers with $n$ rows and $m$ columns.  I want to consider possible ways of dividing that array into rectangular regions of three different possible types: a constant value over the entire region, a linear gradient across rows or a linear gradient across columns.</p>\n\n<p>For example, the array</p>\n\n<p>$\\begin{bmatrix}1 &amp; 2 &amp; 3 &amp; 10 \\\\ 1 &amp; 2 &amp; 3 &amp; 12 \\\\ 4 &amp; 4 &amp; 4 &amp; 14\\end{bmatrix}$</p>\n\n<p>can be decomposed into a horizontal gradient [1-3], a vertical gradient [10-14] and a constant region of value 4.</p>\n\n<p>There are potentially many different ways to decompose any given array (in the extreme case, you can always divide it into $n$ by $m$ constant single-cell regions), so I'm interested in finding a 'simplest' solution (i.e. one with as few regions as possible.)</p>\n\n<p>Has this problem been studied or is it related to another kind of problem that has been studied?  It seems like it could be approached using some sort of tree representation where I start with all cells as individual constant regions, then combine them together under larger regions as long as they meet the desired constraints, then evaluate using some fitness function to decide whether it is an optimal (or at least 'good enough') decomposition.</p>\n\n<hr>\n\n<p>Regarding the optimality constraint, I have some flexibility here, so I can choose a constraint that helps make the problem easier to solve.  Ultimately the goal is to have a decomposition that appeals to human sensibilities of the 'patterns' present in the data, so my first thought is that minimizing the number of regions is probably a good constraint.</p>\n\n<p>Some interesting boundary cases to consider:</p>\n\n<p>$\\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\end{bmatrix}$</p>\n\n<p>The preferred decomposition here would be three constant 2x2 regions of '1' and three constant regions of '0'.  This is preferable to the alternate 6 region decomposition with three 2x2 '0-1' gradients and three constant '1' regions, as constant regions are preferable to gradients if the number of regions is the same.</p>\n\n<p>$\\begin{bmatrix}1 &amp; 2 &amp; 3 \\\\ 2 &amp; 2 &amp; 2 \\\\ 3 &amp; 2 &amp; 1\\end{bmatrix}$</p>\n\n<p>Due to symmetry, this can decompose into either horizontal or vertical gradients.  I don't have any preference in this case for which should be chosen as the decomposition.</p>\n\n<p>Regarding distribution of size, it's probably better to avoid at the minimum 1x2 gradients.  Any array can be distributed arbitrarily into various single cell constant blocks and 1x2 gradient blocks, so this kind of interpretation is not particularly interesting.  In the case where there are no other patterns in a region of the data, I'd rather treat it as single cells, as I can do some post-processing to extract these 'complex' regions and treat them as lookup tables.</p>\n\n<hr>\n\n<p>Another case:</p>\n\n<p>$\\begin{bmatrix}1 &amp; 1 &amp; 2 &amp; 3 &amp; 4\\\\ 1 &amp; 1 &amp; 2 &amp; 3 &amp; 4\\\\ 1 &amp; 1 &amp; 2 &amp; 2 &amp; 2\\\\ 1 &amp; 1 &amp; 2 &amp; 2 &amp; 2\\end{bmatrix}$</p>\n\n<p>Here it would be preferable to take the 4x2 '1' region and 2x3 '2' region, with a 2x3 '2-4' gradient region.  The alternative '1-4' gradient is worse because it requires splitting the '1' region into two different regions.</p>\n", 'ViewCount': '47', 'Title': 'Sub-dividing a 2d array into regions', 'LastEditorUserId': '16797', 'LastActivityDate': '2014-04-16T12:39:59.740', 'LastEditDate': '2014-04-16T12:39:59.740', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16797', 'Tags': '<algorithms><arrays>', 'CreationDate': '2014-04-15T17:38:24.487', 'Id': '23816'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here's the code for the algorithm:</p>\n\n<pre><code>Foo(n)\n   lcm = 1\n   for i = 2 to n\n       lcm = lcm*i/Euclid(lcm,i)\nreturn lcm\n</code></pre>\n\n<p>The running time of <code>Euclid</code>$(a, b)$ is given as $O(\\log(\\min(a, b)))$</p>\n\n<p>So the running time of the for loop will be $O(n)$, so would this be the final running time? or do I have to take the $O(\\log(\\min(a, b)))$ into account as well?</p>\n", 'ViewCount': '82', 'Title': 'Big O running time for this algorithm?', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-16T07:27:49.683', 'LastEditDate': '2014-04-16T07:27:49.683', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '23836', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16810', 'Tags': '<algorithms><runtime-analysis>', 'CreationDate': '2014-04-15T23:26:54.843', 'Id': '23835'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have two sound clips $C_1,C_2$ which are pretty similar.  I'd like to measure how perceptually similar they are, i.e., how similar a human would perceive the two to be.  Is there a way to use a lossy compression algorithm (e.g., a MP3, AAC, or Ogg Vorbis encoder) to compare the two clips?</p>\n\n<p>It occurs to me that that audio compressors already contain a good deal of knowledge about psychoacoustics and human perception of sound, built into them.  Is there a good way to use them to measure how similar the two clips are?</p>\n\n<p>Maybe something like $L(C_1 || C_2) / (L(C_1) + L(C_2))$, where $L(x)$ is the length of the compression of sound clip $x$, and $C_1 || C_2$ is the result of concatenating the two clips?  Or maybe find the highest bitrate such that \n$F(C_1)$ is close to $F(C_2))$ via a simple metric (e.g., L2 norm applied to the FFT spectrum), where $F(C)$ is the result of compressing $C$ at that bitrate followed by decompressing it?  Or something else along these lines?  Has anyone studied this?</p>\n\n<p>If it is relevant, the two clips are fairly similar: one was obtained by a transformation of the other.  They are aligned in time and have the same length.  Each is relatively short (at most a few seconds).  I've done some searching but haven't found any reference or research paper that discusses this sort of approach -- maybe I just haven't found it yet, though.</p>\n", 'ViewCount': '19', 'ClosedDate': '2014-04-16T08:19:31.513', 'Title': 'Measuring similarity of music, using lossy compression', 'LastActivityDate': '2014-04-16T00:54:43.233', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-compression>', 'CreationDate': '2014-04-16T00:54:43.233', 'Id': '23838'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>On math.stackexchange, someone asked how to count the number of ways to place $1$'s into a $10 \\times 10$ matrix so that every row and column has $5$ $1$'s.  Each element of the matrix must be either zero or one.</p>\n\n<p>I came up with a recursive solution for an $N \\times 10$ matrix.  Subproblems are indexed by the counts $c_k$ of how many columns have $k$ $1$'s, for $k =0, 1,2,3,4,5$. The counts $c_k$ have to satisfy $\\sum_k c_k = 10$, and they also have to satisfy $\\sum_k kc_k = 5N$ and $c_k = 0$ for $k &gt; N$. The complexity of this algorithm basically boils down to how many distinct sets of valid indices $(c_k)_k$ there are.</p>\n\n<p>For a $10 \\times 10$ matrix I think this approach should work out nicely, but I worry the complexity might get prohibitively large if we wanted to count how many ways to get $M/2$ $1$'s in every row and column of an $M \\times M$ matrix. So I'm wondering, is there a more efficient way to solve this counting problem?  In other words, a better way than solving for $N \\times M$ in increasing order of $N$ and keeping track of subcases indexed by $(c_k)_k$ such that $\\sum_k c_k = M$ and $\\sum_k k c_k = NM/2$? Also, for my solution, can anybody work out a good bound for how many sub-cases I have as a function of $M$?</p>\n", 'ViewCount': '75', 'Title': 'Count number of ways to place ones in an $M \\times M$ matrix so that every row and column has $k$ ones?', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-18T18:41:58.363', 'LastEditDate': '2014-04-18T15:35:14.763', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><time-complexity><combinatorics>', 'CreationDate': '2014-04-16T19:56:14.717', 'FavoriteCount': '1', 'Id': '23869'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have N events, each repeats after some time t<sub>i</sub>. I want a queue where I can pop out next event.</p>\n\n<p>For example, I have events A,B,C with intervals 2,3,5 At the beginning they all are in the queue with values:</p>\n\n<p>A-2, B-3, C-5</p>\n\n<p>When I take out event, it should be A, after that I put it back, but add +2.</p>\n\n<p>B-3, A-4, C-5</p>\n\n<p>Next is B. I add +3</p>\n\n<p>A-4, C-5, B-6</p>\n\n<p>Next is A, I add +2</p>\n\n<p>C-5, B-6, A-6</p>\n\n<p>and so on.</p>\n\n<p>What could be the best algorithm for that?</p>\n', 'ViewCount': '14', 'Title': 'Algorithm for queue with multiple repeated different interval events', 'LastActivityDate': '2014-04-17T10:00:49.813', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16855', 'Tags': '<algorithms>', 'CreationDate': '2014-04-17T10:00:49.813', 'Id': '23882'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading a paper for recognizing interval graphs. Here is an excerpt from the paper:</p>\n\n<blockquote>\n  <p>Each interval graph has a corresponding interval model in which two intervals overlap if\n      and only if their corresponding vertices are adjacent. Such a representation is usually far\n      from unique. To eliminate uninteresting variations of the endpoint orderings, we shall\n      consider the following block structure of endpoints: Denote the right (resp. left) endpoint of\n      an interval $u$ by $R(u)$ (resp. $L(u)$). In an interval model, define a maximal contiguous set of\n      right (resp. left) endpoints as an R-block (resp. L-block). Thus, the endpoints can be grouped\n      as a left-right block sequence. Since an endpoint block is a set, the endpoint orderings within\n      a block are ignored. It is easy to see that the overlapping relationship does not change if one\n      permute the endpoint order within each block. Define two interval models for $G$ to be\n      equivalent if either their left-right block sequences are identical or one is the reversal of the\n      other. </p>\n</blockquote>\n\n<p>I am unable to understand the notion of equivalent intervals. Can someone help me?</p>\n', 'ViewCount': '37', 'Title': 'Recognizing interval graphs--"equivalent intervals"', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-18T06:40:51.167', 'LastEditDate': '2014-04-18T06:40:51.167', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '23890', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11703', 'Tags': '<algorithms><graph-theory><graphs><intervals>', 'CreationDate': '2014-04-17T11:39:56.467', 'Id': '23885'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have done the proof until the point when $T(n) \\leq cn^{\\log7}$.</p>\n\n<p>But when it comes to finding the value of constant $c$, I am getting stuck.</p>\n\n<p>The given recurrence relation is $T(n) = 7T(n/2) + n^2$. </p>\n\n<p>Since we already calculated the solution above which is $cn^{\\log 7}$.</p>\n\n<p>Inductive step:</p>\n\n<p>Now we have to prove that $T(n) \\leq c n^{\\log7}$ where $c$ is a positive constant.\nIf we consider that the solution holds good for $n/2$ then we can prove that it works  for $n$ also: \n$$T(n/2) \\leq c(n/2)^{\\log7}.$$\nSubstituting these values in the recurrence relation:</p>\n\n<p>$$\n\\begin{align*}\nT(n) &amp;\\leq 7c/(2)^{\\log7} \\times (n)^{\\log7} + n^2 \\\\\n     &amp;\\leq cn^{\\log7}, \\text{ since $7/(2)^{\\log7}$  is constant so can be ignored and $cn^{\\log7} \\gg n^2$ for large $n$} \\\\\n     &amp;\\leq cn^{\\log7} \\text{ assuming $c$ is a constant $\\geq 1$.}\n\\end{align*}\n$$</p>\n\n<p>Finally to find constant $c$, </p>\n\n<p>$$(7/(2)^{\\log7}) \\times cn^{\\log7} + n^2 \\leq cn^{\\log7}. $$</p>\n\n<p>I am not able to find appropriate $c$ for which the condition holds true. </p>\n', 'ViewCount': '59', 'Title': 'To prove the recurrence by substitution method $T(n) = 7T(n/2) + n^2$', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-18T02:50:15.323', 'LastEditDate': '2014-04-17T17:55:46.163', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16666', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation>', 'CreationDate': '2014-04-17T17:22:36.890', 'Id': '23892'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am in an entry-level algorithms class, and for our final project we are coding and thoroughly analyzing 6 different sorting methods. Part of the analyzation is timing the methods and comparing the runtime results depending on the original order of the array (in order to more fully grasp the concept of constant costs, I suppose). I coded the bubble sort in Java, and when I run it on an array that is in descending order, it returns a sorted array FASTER than when I run it on an array of random ints, even though it is doing, on average, twice as many swaps. It seems to me that doing twice as many operations should result in taking much longer to finish. I have NO idea what could be causing this discrepancy, and any help would be appreciated.</p>\n', 'ViewCount': '56', 'Title': 'Why is my bubble sort taking longer to sort a random array as opposed to a descending array?', 'LastActivityDate': '2014-04-18T03:09:06.577', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16861', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-04-17T17:34:49.310', 'Id': '23893'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading The Algorithm Design Manual, 2nd Edition. \nThe book gives an example task and then explains how to solve it step by step.\n(The task and solution is detailed <a href="http://www8.cs.umu.se/kurser/TDBA77/VT06/algorithms/BOOK/BOOK2/NODE51.HTM" rel="nofollow">here</a>)</p>\n\n<p><em>But I don\'t follow one step from the solution. That step doesn\'t contain enough details for me so I don\'t understand it. I\'m asking if someone can explain it.</em></p>\n\n<p><strong>Below I rewrote the task in a shorter form than in the book:</strong></p>\n\n<p>Given n ordered strings (lets call them rules) of m characters. The strings are indexed starting from 1 (not 0)</p>\n\n<p>The goal is to construct the trie with the minimum possible number of edges. The leaves of the resulting trie (each leaf represents a string) have to be in the same exact order as the given collection of strings.</p>\n\n<p>We are free to pick an arbitrary character position on each step of constructing the trie (e. g., we are free to start constructing the trie from character position 2 as opposed to starting from 1). Thanks to this ability we can minimize the built trie. Lets call picking a character - probing.</p>\n\n<p>Example:</p>\n\n<p>From the four rules below it\'s possible to build different tries depending on the order of picking characters from the strings.</p>\n\n<p><code>\nS1: (a,a,a)\nS2: (b,a,a)\nS3: (c,b,b)\nS4: (d,b,b)\n</code></p>\n\n<p><img src="http://i.stack.imgur.com/xrCvv.gif" alt="tries"></p>\n\n<p>The book says that</p>\n\n<blockquote>\n  <p>Probing at the p-th position, 1 &lt;= p &lt;= m, partitioned the rules into runs R1,...,Rr, where each rule in a given run Rx = Si,...,Sj had the same character value of Si[p].</p>\n  \n  <p>Since the rules were ordered, each node in the subtree must represent the root of a run of consecutive rules, so there were only ${{n}\\choose{2}}$ possible nodes to choose from for this tree...</p>\n  \n  <p>...</p>\n  \n  <p>The rules in each run must be consecutive, so there are only ${{n}\\choose{2}}$ possible runs to worry about.</p>\n</blockquote>\n\n<p><strong>My question: How does the fact the rules are consecutive lead to the inference that there are ${{n}\\choose{2}}$ possible nodes/runs after probing at the p-th position?</strong></p>\n', 'ViewCount': '71', 'Title': u'Studying Skiena. War Story: What\u2019s Past is Prolog', 'LastEditorUserId': '16874', 'LastActivityDate': '2014-04-20T22:48:07.310', 'LastEditDate': '2014-04-19T22:09:42.227', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16874', 'Tags': '<algorithms><dynamic-programming>', 'CreationDate': '2014-04-18T10:29:52.833', 'Id': '23908'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Say I want to compute a covnex hull of given points on the plane. I would like to write an algorithm, that only compares the points and doesn't do any arithmetic operations. Wikipedia states, that:</p>\n\n<blockquote>\n  <p>The standard $\\Omega(n \\log n)$ lower bound for sorting is proven in the decision tree model of computing, in which only numerical comparisons but not arithmetic operations can be performed; however, in this model, convex hulls cannot be computed at all. </p>\n</blockquote>\n\n<p>Why is it so? I can't find any justification for it anywhere, I know it to be intuitively true, but how come it's a necessity?</p>\n", 'ViewCount': '331', 'Title': "Convex Hull algorithm - why it can't be computed using only comparisons", 'LastEditorUserId': '657', 'LastActivityDate': '2014-04-18T14:32:41.873', 'LastEditDate': '2014-04-18T14:15:34.753', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23911', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16876', 'Tags': '<algorithms><computational-geometry><computation-models>', 'CreationDate': '2014-04-18T11:55:18.920', 'Id': '23910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have $n$ balls and $n$ bins. We put the balls into the bins randomly. If we count the maximum number of balls in any bin, the expected value of this is  $\\Theta(\\ln n/\\ln\\ln n)$. How can we derive this fact? Are Chernoff bounds helpful?</p>\n', 'ViewCount': '63', 'ClosedDate': '2014-04-23T16:47:14.277', 'Title': 'Expected maximum bin load, for balls in bins with equal number of balls and bins', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-19T00:38:01.780', 'LastEditDate': '2014-04-19T00:38:01.780', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15406', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><probabilistic-algorithms><chernoff-bounds>', 'CreationDate': '2014-04-18T22:43:53.940', 'Id': '23925'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A polynomial map is equal to another polynomial map iff they take on the same values at each point.  So this is different from formal polynomials.   So since in $\\Bbb{Z}_p$, $x^{p-1} = 1$ for all $x \\neq 0$, and is $0$ on $0$, we have that there are a finite number of polynomial maps in $\\Bbb{Z}_p[x_1, \\dots, x_k]$.  For now let's work in $\\Bbb{Z}_2$ for simplicity.</p>\n\n<p>The coefficients are arbitrarily chosen $c_i \\in \\Bbb{Z}_2$, and polynomials are in $\\Bbb{Z}_2[x_1, \\dots, x_k]$.  Then </p>\n\n<p>$c_0 + c_1x_1$ obviously has complexity $2$ (2 operations explicitly required).</p>\n\n<p>$c_0 + c_1x_1 + c_2 x_2$ needs $4$ ops.</p>\n\n<p>$c_0 + c_1x_1 + (c_2+ c_3 x_1) x_2$ needs $6$ ops max.</p>\n\n<p>$c_0 + c_1x_1 + c_2 x_2 + c_3 x_3 + c_4 x_1 x_2 + c_5 x_1 x_3 + c_6 x_2 x_3 + c_7 x_1 x_2 x_3 = \\\\ c_0 + x_1(c_1 + x_2 (c_4 + c_7 x_3) + c_5 x_3) + x_2 (c_2 + c_6 x_3) + c_3 x_3$ </p>\n\n<p>needs $14$ ops.  The pattern seems to be</p>\n\n<p>$1 + (k + (k-1) + \\dots + 1) + ((k-2) + (k-1) + \\dots + 1) + \\dots + 1$ which is $O(k^3)$, so is polynomial complexity polynomial?</p>\n\n<p>Where did I make any mistake?  Thanks.</p>\n\n<p>This doesn't make sense as there are about $p^k$ coefficients, and each one must be visited.</p>\n\n<p>$* + x_1(* + x_2(* + x_3(* + *x_4) + *x_3 + *x_4)) + x_2 (* + x_3(* + *x_4) + *x_4) + x_3(* + *x_4) + *x_4$</p>\n", 'ViewCount': '28', 'Title': 'Complexity of general polynomial map evaluation is polynomial?', 'LastEditorUserId': '12373', 'LastActivityDate': '2014-04-20T23:51:52.963', 'LastEditDate': '2014-04-19T20:31:32.287', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23943', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12373', 'Tags': '<algorithms><complexity-theory><polynomial-time><polynomials>', 'CreationDate': '2014-04-19T20:08:43.153', 'Id': '23941'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to make an algorithm to calculate what combination of cards from what buyers I should get to get the cheapest deal. Taking the shipping costs into consideration. It's for a website called MagicCardMarket, which is kind of an Ebay for trading cards. The problem is that the site doesn't have a proper optimization tool, so I'm trying to make it myself. (I found some other site that does this, but you have to pay for it and I'm learning a lot already doing it myself.)</p>\n\n<p>The first thing I do is gather a bunch of information. The most important parts being how many and what cards the buyer want. Then I gather all the articles that comply with these cards and throw away all the bad ones (those who's quality is too bad, or language is incorrect, ...) Articles are basically instances of those cards being sold, with info about their seller and cost. So now I have a list of cards, and for each card a list of articles for those cards, and for each article a cost and seller (and from this seller I can get the shipping cost, this can depend on how many cards are bought from a single seller, though most of the time it stays almost constant.)</p>\n\n<p>So now I'm looking for an algorithm to get all the cards you want, as cheap as possible. Taking into consideration that when I buy from more sellers, the shipping cost will go up.</p>\n\n<p>Are there any algorithms I should look at, that might lend themselves to this problem very nicely? Or perhaps some general tips for these kinds of algorithms? It's the first time I'm trying to make this kind of algorithm, so I wasn't sure what to search for either.</p>\n\n<p>The easiest way would probably be to just get all possible combinations, and check which is the cheapest, though most cards will have several hundreds of articles associated with them, and most of the time you'll have 60 to 80 cards you want to buy (a full deck.) So I don't think this is a good way, it uses enormous amount of ram and takes a lot of time. I'm also writing it as a web-app, preferably something that can also work on mobile phones.</p>\n\n<p>Thanks a lot!</p>\n\n<p>PS. I wasn't sure if this is the proper forum to ask this, please redirect me if I was wrong.</p>\n", 'ViewCount': '97', 'Title': 'Card-buying algorithm', 'LastActivityDate': '2014-04-22T18:55:37.893', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '23968', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16932', 'Tags': '<algorithms>', 'CreationDate': '2014-04-20T12:27:43.090', 'Id': '23950'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to solve a particular algorithm puzzle, <a href="https://icpcarchive.ecs.baylor.edu/index.php?option=com_onlinejudge&amp;Itemid=8&amp;page=show_problem&amp;problem=1512" rel="nofollow">explained here</a></p>\n\n<p>It asks:</p>\n\n<blockquote>\n  <p>Given an m x n matrix of zeros and ones, rearrange the columns such\n  that the ones on every line are grouped together.</p>\n</blockquote>\n\n<p>So for example, if the input is:</p>\n\n<pre><code>0110\n0001\n1101\n</code></pre>\n\n<p>the output should be</p>\n\n<pre><code>0011\n0100\n1110\n</code></pre>\n\n<p>I\'m puzzled even where to begin. This reminds me of a Rubiks cube. Each time I move a column to satisfy the conditions of one row, I am affecting all the other rows.</p>\n\n<p><strong>What type of algorithm does this problem require to be solved?</strong></p>\n\n<p>And more generally, are there any references out there that provide classification for different types of problems and offer solution ideas?</p>\n', 'ViewCount': '24', 'Title': 'How to solve an algorithm puzzle where moving one item changes all the others', 'LastActivityDate': '2014-04-20T19:45:32.213', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms>', 'CreationDate': '2014-04-20T19:45:32.213', 'Id': '23957'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given $n$ points $x_1, x_2, ..., x_n \\in \\mathbb{R}^k$, where $\\mathbb{R}^k$ can be high dimensional. Is it possible to devise a fast algorithm </p>\n\n<p>(1) Preparation: first take the n points as an input, do whatever preprocessing necessary. </p>\n\n<p>(2) Query: for each query $f_i = w^\\top x$ being a hyperplane and a constant $b$, find the subset of points $\\{x_i | w^\\top x_i \\geq b\\}$.</p>\n\n<p>such that the asymptotic complexity for the query is less than $\\Theta(k \\cdot n)$ ?</p>\n', 'ViewCount': '40', 'Title': 'Fast algorithm to find points on one side of hyperplane?', 'LastActivityDate': '2014-04-21T19:33:24.670', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '24003', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<algorithms><complexity-theory><computational-geometry>', 'CreationDate': '2014-04-20T20:49:00.997', 'FavoriteCount': '1', 'Id': '23961'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let the following datatype be defined:</p>\n\n<pre><code> data T = A | B T | C T T\n</code></pre>\n\n<p>That is, <code>B, B T, B (B T), C A A, C (B T) A</code> and so on all are members of T. Now, suppose we define two functions that operate on that type:</p>\n\n<pre><code>f :: T -&gt; T\nf A = A\nf (B x) = B (B (f x))\n\ng :: T -&gt; T\ng A = A\ng (B x) = B (B (B (g x)))\n</code></pre>\n\n<p>There are restrictions on the definition of <code>f</code> and <code>g</code>: first, recursive calls can only be applied directly to a subterm of one of the inputs (guaranteeing termination), and second, they can't use any datatype other than T on their bodies (consider T is the only existing type). In this case, we know that the following function:</p>\n\n<pre><code>h :: T -&gt; T\nh A = A\nh (B x) = B (B (B (B (B (B (h x))))))\n</code></pre>\n\n<p>works as the composition <code>f . g</code>. My question is, is it possible/decidable to find the composition of <code>f</code> and <code>g</code> in this form - that is, without any reference to <code>f</code> and <code>g</code> themselves? What is the name of the problem I am trying to solve?</p>\n", 'ViewCount': '48', 'Title': 'Is there a decidable algorithm to compose two well-behaved recursive functions that work on a recursive tree datatype?', 'LastEditorUserId': '16949', 'LastActivityDate': '2014-04-21T19:21:04.403', 'LastEditDate': '2014-04-21T15:45:57.793', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16949', 'Tags': '<algorithms><complexity-theory><computability><programming-languages>', 'CreationDate': '2014-04-21T15:36:19.557', 'Id': '23992'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So here is the challenge problem statement: <a href="https://icpcarchive.ecs.baylor.edu/index.php?option=com_onlinejudge&amp;Itemid=8&amp;page=show_problem&amp;problem=1512" rel="nofollow">https://icpcarchive.ecs.baylor.edu/index.php?option=com_onlinejudge&amp;Itemid=8&amp;page=show_problem&amp;problem=1512</a> </p>\n\n<p>Basically, given a 0/1 matrix, you need to permute the columns so that the first column is fixed and after the permutation of columns, then for each row the 1\'s in the row occur contiguously (without counting wrap-around). I thought about this problem and came up with a conjecture that would make it easy to solve, but I\'m not sure if it\'s true.</p>\n\n<p>Conjecture: After the first $k$ columns have been chosen, let $S$ be the set of rows that end with a $1$ in the $k$th column, such that there exists an unchosen column that has a $1$ in that row. Then the $k+1$th column can be chosen to be the column that has (i) 1\'s for all positions in $S$, and (ii) which has a minimum total number of $1$\'s.</p>\n\n<p>Is this true? If so an optimal solution could be constructed very quickly. I know that the $k+1$th column has to satisfy condition (i), which reduces the possibilities, but I\'m really hoping we can ensure condition (ii) also holds so that the choice of $k+1$th column is essentially unique.</p>\n', 'ViewCount': '26', 'Title': 'Conjecture about a matrix column swapping challenge problem', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-21T18:53:34.960', 'LastEditDate': '2014-04-21T18:53:34.960', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><correctness-proof><greedy-algorithms><permutations>', 'CreationDate': '2014-04-21T17:15:13.210', 'Id': '23997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the Wagner Fischer algorithm to calculate the Levenshtein distance for two input strings, the first step is to compare each input string with the empty string for each input. Why is this step necessary? Is the purpose simply to keep the algorithm consistent when checking the strings for matches and mismatches to determine the distance between two prefixes? </p>\n\n<p>Thanks!</p>\n', 'ViewCount': '19', 'Title': 'Wagner Fischer Algorithm and the Empty String', 'LastActivityDate': '2014-04-21T22:21:39.417', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16968', 'Tags': '<algorithms>', 'CreationDate': '2014-04-21T21:11:32.500', 'Id': '24005'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a bipartite graph similar to the marriage problem, where there are M males and N females, and a 1:1 matching between males and females is desired (with the remainder of the more populous gender remaining unmatched).</p>\n\n<p>The differences between my problem and existing research I've found:</p>\n\n<ol>\n<li>Want to maximize the number of pairings at the expense of stability. Stability is nice as a secondary objective, but not required.</li>\n<li>Preference lists are not exhaustive. Eg. m_1 only wants n_3 and cannot be matched with any other.</li>\n</ol>\n\n<p>Can anyone point me in some possible directions?</p>\n", 'ViewCount': '33', 'Title': 'Marriage algorithm that maximizes number of pairings', 'LastActivityDate': '2014-04-21T22:40:51.340', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16969', 'Tags': '<algorithms><graphs><bipartite-matching>', 'CreationDate': '2014-04-21T22:40:51.340', 'Id': '24008'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How would one search for a string of digits in a large digit sequence? For example, I'd like to search for <code>351814</code> in Euler's number. I'm not too keen on computer science, I'm a pure math major, so I don't really know how to begin. I also wouldn't know how to run said code.   </p>\n\n<p><strong>Any help would be greatly appreciated.</strong></p>\n", 'ViewCount': '85', 'ClosedDate': '2014-04-27T11:41:19.013', 'Title': 'Searching for a string of numbers in a large digit sequence', 'LastEditorUserId': '17106', 'LastActivityDate': '2014-04-27T11:41:04.027', 'LastEditDate': '2014-04-27T05:40:11.397', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16971', 'Tags': '<algorithms><search-algorithms><strings><binary-search>', 'CreationDate': '2014-04-22T00:40:36.540', 'Id': '24009'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When simulating the clock page replacement algorithm, when a reference comes in which is already in memory, does the clock hand still increment?</p>\n\n<p>Here is an example:</p>\n\n<p>With 4 slots, using the clock page replacement algorithm</p>\n\n<p>Reference list: 1 2 3 4 1 2 5 1 3 2 4 5</p>\n\n<p>Initial list would look like this: </p>\n\n<pre><code>-&gt; [1][1]\n   [2][1]\n   [3][1]\n   [4][1]\n</code></pre>\n\n<p>The next reference to insert would be 1, then 2. Would the hand still point at 1 after 1, and after 2 ? In other words, after inserting the 5, would the clock look like this :</p>\n\n<pre><code>-&gt; [5][1]\n   [2][0]\n   [3][0]\n   [4][0]\n</code></pre>\n\n<p>?</p>\n', 'ViewCount': '31', 'Title': 'Clock page replacement algorithm - Already existing pages', 'LastActivityDate': '2014-04-22T03:16:05.973', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16972', 'Tags': '<algorithms><paging><virtual-memory>', 'CreationDate': '2014-04-22T00:46:50.387', 'Id': '24011'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have heard of machine learning systems that can learn from trials of data what effect it should have, but I was wondering is it possible to make a learning appliance that builds on past data? Instead of only receiving new entries, once the database is big enough to begin going over it and doing pseudo consideration, drawing conclusions about the data and adding yet more entries to its database purely from looking at its past results? I suppose such a thing would probably eat up memory quickly unless it had a threshold of entries it was allowed to create. (In theory it would take a long time to get it to a state where it can become sentient in that way, through many trial sets before it begins thinking on its own.)</p>\n\n<p>An application of such a thing would go beyond just determining what the best way to optimize code is, or how to beat a player in a game, but a machine that can actually build its own personality.</p>\n', 'ViewCount': '55', 'Title': 'Can a self-learning appliance be developed?', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-22T08:58:56.877', 'LastEditDate': '2014-04-22T04:23:14.357', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16973', 'Tags': '<algorithms><machine-learning>', 'CreationDate': '2014-04-22T02:36:04.760', 'Id': '24016'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm not sure if this is the appropriate forum for asking the following question. Please guide me to the right forum.</p>\n\n<p>I am trying to <strong>understand the algorithms</strong> used by kernels(such as Integer Sort) in <strong>NAS parallel benchmarks</strong>. I could not find any source online that explained the algorithms used by the kernels. I could try to infer the algorithms from the code, but that would be really messy. The algorithms are not explained in the paper published by the authors of the benchmark either. </p>\n\n<p>Please point me to some tutorials/documents describing these algorithms. Thanks</p>\n", 'ViewCount': '24', 'Title': 'Explanation of algorithms used by NAS benchmark kernels', 'LastActivityDate': '2014-04-23T14:47:55.357', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7203', 'Tags': '<algorithms><benchmarking>', 'CreationDate': '2014-04-23T14:47:55.357', 'FavoriteCount': '1', 'Id': '24047'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<blockquote>\n  <p>How many arithmetic operations are required to find a <a href="http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse">Moore\u2013Penrose\n  pseudoinverse\n  matrix</a> of a arbitrary field?</p>\n</blockquote>\n\n<p>If the matrix is invertible and complex valued, then it\'s just the inverse. Finding the inverse takes $O(n^\\omega)$ time, where $\\omega$ is the matrix multiplication constant. It is Theorem 28.2 in Introduction to Algorithms 3rd Edition. </p>\n\n<p>If the matrix $A$ has linearly independent rows or columns and complex valued, then the pseudoinverse matrix can be computed with $A^*(A A^*)^{-1}$ or $(A A^*)^{-1}A^*$ respectively, where $A^*$ is the conjugate transpose of $A$. In particular, this implies an $O(n^\\omega)$ time for finding the pseudoinverse of $A$.</p>\n\n<p>For general matrix, the algorithms I have seen uses QR decomposition or SVD, which seems to take $O(n^3)$ arithmetic operations in the worst case. Is there algorithms that uses fewer operations? </p>\n', 'ViewCount': '115', 'Title': 'Complexity of finding the pseudoinverse matrix', 'LastActivityDate': '2014-04-23T22:00:05.707', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24065', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<algorithms><linear-algebra>', 'CreationDate': '2014-04-23T19:49:06.803', 'FavoriteCount': '1', 'Id': '24060'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am studying for an exam I have and I am looking over the Knuth-Morris-Pratt algorithm. What is going to be on the exam is the Fail table and DFA construction. I understand DFA construction, but I don\'t really understand how to make the fail table.</p>\n\n<p>If I have an example of a pattern "abababc" how do I build a fail table from this? The solution is:</p>\n\n<p>Fail table:</p>\n\n<p>0 1 2 3 4 5 6 7</p>\n\n<p>0 0 0 1 2 3 4 0</p>\n\n<p>but how do I get that? No code just an explanation of how to get that is necessary.</p>\n', 'ViewCount': '18', 'ClosedDate': '2014-04-24T06:35:49.383', 'Title': 'Knuth-Morris-Pratt fail table construction', 'LastActivityDate': '2014-04-24T06:05:49.097', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17038', 'Tags': '<algorithms>', 'CreationDate': '2014-04-24T06:05:49.097', 'Id': '24073'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a doubt about the complexity of the Euclidean algorithm; the slide of my Professor says:</p>\n\n<blockquote>\n  <p>The calculation of $\\mathrm{GCD} (a, b)$ stops at the most after $2\\log_2 a$ iterations. (1) Since $\\log_2 a$ is the size of the input, (2) calculation requires a linear number of iterations. (3) The algorithm therefore has polynomial complexity.</p>\n</blockquote>\n\n<p>I don't get how he could deduce (2) from (1), and (3) from (2); until now, the only concept of theory that he gave us is that to represent a positive integer, we need $1+\\log_2 x$ bits.</p>\n\n<p>Thanks in advance for the help :)</p>\n", 'ViewCount': '39', 'Title': 'Complexity of the Euclidean algorithm', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-24T10:11:29.850', 'LastEditDate': '2014-04-24T10:05:07.710', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24077', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17046', 'Tags': '<algorithms><complexity>', 'CreationDate': '2014-04-24T09:58:40.473', 'Id': '24076'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '41', 'Title': 'Dual-pivot Quicksort reference implementation?', 'LastEditDate': '2014-04-25T06:56:55.333', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7533', 'FavoriteCount': '2', 'Body': "<p>Has some sort of canonical - or reference - implementation of Dual-pivot Quicksort been posted anywhere?</p>\n\n<p>I would like to include that algorithm in a comparison among sorting algorithms for a specialized need that I have, but the Java versions I've seen appear to have various kinds of tweaks applied to them, like using Insertion Sort for small (sub-) arrays, which makes it harder to compare the fundamentals.</p>\n", 'Tags': '<algorithms><reference-request><sorting><quicksort>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-25T09:18:42.890', 'CommentCount': '7', 'AcceptedAnswerId': '24099', 'CreationDate': '2014-04-25T00:47:08.157', 'Id': '24092'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Does anyone have (or can easily write) an optimal inline assembly function for the M0+ processor in Thumb mode to multiply two 32-bit numbers and return a 64-bit number?</p>\n\n<p>As the M0+ does not have long multiply, the only way this can be accomplished is through primitive multiplication, for which the compiler calls __aeabi_lmul which performs 64x64=64 multiplication in 34 instructions. I'm hoping a significantly faster algorithm exists, given that the inputs are only 32 bits.</p>\n", 'ViewCount': '4', 'ClosedDate': '2014-04-25T07:00:02.017', 'Title': 'Fastest M0+ Thumb 32x32=64 Function?', 'LastActivityDate': '2014-04-25T03:25:01.580', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17060', 'Tags': '<algorithms><multiplication>', 'CreationDate': '2014-04-25T03:25:01.580', 'Id': '24093'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $G$ be a undirected simple graph with 1000 nodes. My aim to enumerate all minimum cuts in the graph, but we have exponential many minimum cuts. So instead of enumerating all minimum cuts can we do the following task in polynomial time.</p>\n\n<ol>\n<li>Enumerating first $n^2$ mincuts in increasing order of size. </li>\n</ol>\n', 'ViewCount': '18', 'ClosedDate': '2014-04-25T07:05:28.190', 'Title': 'Finding the minimum size of mincut in graph', 'LastEditorUserId': '6522', 'LastActivityDate': '2014-04-27T16:37:41.930', 'LastEditDate': '2014-04-27T16:37:41.930', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<algorithms><graphs><search-algorithms>', 'CreationDate': '2014-04-25T06:13:20.707', 'Id': '24096'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to study some basic logspace algorithms. I have studied about the some of the problems, but most of the problems I am unable to follow in detai. Can some one suggest me the best references (books or lecture notes) where these explained clearly. </p>\n\n<p>Thank you.</p>\n', 'ViewCount': '32', 'ClosedDate': '2014-04-26T16:00:56.397', 'Title': 'On Logspace algorithms', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-26T16:01:06.850', 'LastEditDate': '2014-04-26T16:01:06.850', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<algorithms><reference-request><space-complexity>', 'CreationDate': '2014-04-25T13:56:42.783', 'Id': '24102'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to find a graph that does not get optimally colored by the Briggs coloring algoritm. Any suggestions on a type of graph to look for?</p>\n', 'ViewCount': '18', 'ClosedDate': '2014-04-29T22:38:59.250', 'Title': 'Briggs algorithm non optimal situation', 'LastActivityDate': '2014-04-25T16:11:04.377', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17080', 'Tags': '<algorithms><graph-theory><graphs><colorings>', 'CreationDate': '2014-04-25T16:11:04.377', 'Id': '24110'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking on several algorithm. Usuall O(n log n) algorithms compare numbers as they are, there is not a change to get better than log performance.</p>\n\n<p>Fast nearly O(n) algorithms like Radix sort, Counting sort, Bitonic sort.. are using different approach and therefore can run "faster". The faster is usually joined with bigger memory usage. I have been looking / trying to create some algorithm, that can sort numbers in nearly O(n) but with usage of bit representation of numbers. So something similar to Radix sort, but in binary mode. Is there any algorithm like this, since I have not found any reference ?</p>\n', 'ViewCount': '28', 'ClosedDate': '2014-04-26T12:18:44.577', 'Title': 'Sorting algorithms for integer numbers - "binary mode"', 'LastActivityDate': '2014-04-25T18:25:24.487', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17083', 'Tags': '<algorithms><sorting>', 'CreationDate': '2014-04-25T18:25:24.487', 'Id': '24114'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The 1-D <strong>distinct</strong> closest pair of points problem is as follows: Given a set of n <strong>distinct integer</strong> points on real line, find a pair of points with the smallest distance between them, here the distance between two points p_i and p_j is absolute difference of their values, i.e, |p_i-p_j|. </p>\n\n<p>A naive way to solve this problem is to sort the points and check distance of each consecutive points in sorted order and take the minimum of such distances and this takes O(nlog n) time.</p>\n\n<p>My question is can we do better than that? Can we solve it in o(n log n)(note the little-oh) time? If not, then how to show an omega(nlog n) time bound for this problem?</p>\n\n<p>Note that, if the <strong>distinctness</strong> criteria was not there we could have shown a lower bound using Element Distinctness Problem.</p>\n', 'ViewCount': '20', 'Title': 'Linearithmic lower bound for 1-D "distinct" closest pair of points problem', 'LastActivityDate': '2014-04-25T20:06:05.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24116', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '15697', 'Tags': '<algorithms><data-structures><sorting><lower-bounds>', 'CreationDate': '2014-04-25T19:47:38.497', 'Id': '24115'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider a random $n$ dimensional vector $v$ where $v_i \\in \\{0,1\\}$.  For each $i$ we know $p_i = P(v_i = 1)$ and let us assume the $v_i$ are independent.  Using these probabilities, is there an efficient way to iterate over all possible binary $n$ dimensional vectors in order from most likely to least likely (with arbitrary choices for ties)?  </p>\n\n<p>Take for example $p = \\{0.8, 0.3, 0.6\\}$. The most likely vector is $(1,0,1)$ and the least likely is $\\{0,1,0\\}$.  </p>\n\n<p>For small $n$ we could label each vector with its probability and sort although this isn't very efficient. However, consider $n=100$ for example. This is no longer an option.</p>\n", 'ViewCount': '70', 'Title': 'How to iterate over vectors in order of probability', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-28T20:21:46.513', 'LastEditDate': '2014-04-26T17:58:45.207', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '17089', 'Tags': '<algorithms><enumeration>', 'CreationDate': '2014-04-26T08:23:55.233', 'FavoriteCount': '2', 'Id': '24123'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to implement Tabu search, as shown <a href="http://www.cleveralgorithms.com/nature-inspired/stochastic/tabu_search.html" rel="nofollow">here</a>, for my problem. I am now stuck on how to generate the tabu list. In algorithm it mentions that if cost of the candidate is better than the best, then make candidate same as best, and populate tabu list with FeatureDifference(best, candidate). Thus the function gets duplicate copies of argument with different name best and candidate. Also I am not sure how to interpret or implement the function FeatureDifference(best, candidate), because except for name there is no description.</p>\n\n<p>I do understand that this function will differ from problem to problem, but an example would make it clearer. The ruby code on the page does show an implementation of traveling salesman problem in ruby, but what they do is just add edges of the best candidate to the tabu list, and I am not able to make the connection.</p>\n\n<p>If anybody can explain what is FeatureDifference(best, candidate), with an example that would be great. Because I am not able to get given only the current and best solution what goes into the tabu list.</p>\n\n<p>Think found <a href="http://docs.jboss.org/drools/release/latest/optaplanner-docs/html_single/index.html#tabuSearch" rel="nofollow">something</a> that could help, thanks to <a href="http://stackoverflow.com/users/472109/geoffrey-de-smet">Geoffrey De Smet</a> from the <a href="http://stackoverflow.com/questions/20368048/solving-travelling-salesman-with-tabu-search">discussion</a>.</p>\n', 'ViewCount': '32', 'Title': 'How to populate the tabu list in tabu search?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-27T11:38:54.203', 'LastEditDate': '2014-04-27T11:38:54.203', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17094', 'Tags': '<algorithms><search-algorithms><heuristics>', 'CreationDate': '2014-04-26T15:40:00.323', 'Id': '24130'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a set of $N$ $n \\times n$ matrices $A_1,\\ldots,A_N$, and two vectors $x,y$, the problem is to find a product of up to $K$ matrices $A = A_{j_1}A_{j_2}\\cdots A_{j_k}$ so that $Ax$ is as close to $y$ as possible in terms of euclidean distance. The problem can be shown to be NP-hard e.g. by reduction from subset sum. However the problem seems so hard that I have a hard time believing there is even a polynomial time strong approximation algorithm. Also there is the issue of how to define the quality of the approximation. The problem is invariant under scaling $x$ and $y$ so probably we should assume $x$ has length $1$ and an approximation gets within distance $\\epsilon ||y||$ of the optimal solution, where $y \\neq 0$ and $\\epsilon &gt; 0$ can be chosen arbitrarily small. </p>\n\n<p>Anyway like I said, I believe there is no polynomial time strong approximation algorithm but I'm having a hard time thinking of how a proof might go. If someone could help me resolve this question about whether a polynomial time strong approximation algorithm exists, that'd be great.</p>\n", 'ViewCount': '14', 'Title': 'Approximation scheme for finding best product of matrices that minimizes $||Ax - y||$ for given $x,y$', 'LastActivityDate': '2014-04-26T18:15:06.397', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><approximation><matrices>', 'CreationDate': '2014-04-26T18:15:06.397', 'Id': '24136'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '28', 'Title': 'Is This Property (Representative Property) Can Be Generalized?', 'LastEditDate': '2014-04-27T23:48:57.763', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17125', 'FavoriteCount': '1', 'Body': u'<p>I recently came across with a question that asks for the greatest subset of a given set, which includes relatively prime elements.(Randomly selected item from a set is always relatively prime to all others.)\nMy approach was eliminating the subsets depending on if the subset satisfies the rule.\nWhich means testing one by one all possible subsets. In order to determine which subset is going to be tested, I have choosen the following way:</p>\n\n<p>Let S be a set and "n" is the size of the set. Since there are 2^n possible subsets, all computer needs to do is to check all possible subsets. However, the question is in which way and regarding what property. In other words what would be the "rule" for computer to do it.</p>\n\n<p>I came up with a method which I call "Representative Property". By using ones and zeroes I could represent all the elements of the set to indicate their existance or absence in the subset which is going to be tested.</p>\n\n<p>\u0130f n=5, then There is 32 possible subsets. \nStarting from 2^n -1 in which case 31, converting 31 to binary we get "11111". What binary 11111 represent is all the elements of the set is included in the subset which is going to be tested.</p>\n\n<p>By incrementing that value by one I could get 11110,11101,11100,...00010,00001 where "1" stands for existance of the element and the "0" is vice versa.</p>\n\n<p>This is the code for who wonders all work...</p>\n\n<pre><code>import java.util.Scanner;\nimport java.util.ArrayList;\n\npublic class Quest_1 {\n\nprivate static boolean isRelativePrime(ArrayList&lt;Integer&gt; newInfo) {\n    int j;\n    for(int i=newInfo.size()-1;i&gt;=0;i--){\n\n        for(int k=i-1;k&gt;=0;k--){\n            j=2;\n            while (j&lt;newInfo.get(k)||j&lt;newInfo.get(i)){\n                if(newInfo.get(k)%j == 0 &amp;&amp; newInfo.get(i)%j== 0)\n                    return false;\n                j++;\n            }\n        }\n    }\n    return true;\n}\npublic static String reverse(String source) {\n    int i, len = source.length();\n    StringBuffer dest = new StringBuffer(len);\n\n    for (i = (len - 1); i &gt;= 0; i--)\n        dest.append(source.charAt(i));\n    return dest.toString();\n}\npublic static String binary(int i, int k){\n\n\n    int count = 0;\n    String empty ="";\n\n    while(i&gt;0){\n\n        if (i%2==0)\n            empty = empty +"0";\n        else\n            empty = empty +"1";\n        i = i/2;\n        count++;\n\n    }\n\n    while(count &lt; k){\n        empty = empty+"0";\n        count++;\n    }\n    empty = reverse(empty);\n    return empty;\n}\n\npublic static void main(String[] args) {\n    Scanner scan = new Scanner(System.in);\n    System.out.print("Ka\xe7 eleman? ");\n    String binary;\n    int max = 0;\n    int componentSize = scan.nextInt();\n\n    ArrayList&lt;Integer&gt; info = new ArrayList&lt;Integer&gt;();\n    ArrayList&lt;Integer&gt; newInfo = new ArrayList&lt;Integer&gt;();\n    for (int i = 0;i&lt;componentSize;i++){\n\n        info.add(scan.nextInt());\n\n    }\n    for(int l=0;l&lt;info.size();l++)\n        System.out.println("***----****------****** "+info.get(l));\n    for(int k=(int)Math.pow(2, componentSize)-1; k&gt;0; k--){\n        // takes the string.if "1" add,then compare.if relatPrime is true omit, else leave it\n        binary = binary( k, componentSize);\n        int index=binary.length()-1;\n        while(index &gt;= 0){\n\n            if(binary.charAt(index)==\'1\')\n                newInfo.add(info.get(index));\n\n            index--;\n        }\n        if(isRelativePrime(newInfo) &amp;&amp; max&lt;newInfo.size())\n            max = newInfo.size();\n        index=newInfo.size()-1;\n        while(index&gt;=0){\n            newInfo.remove(index);\n            index--;\n        }\n    }\n    System.out.println("here is the max: "+max);\n}\n\n}\n</code></pre>\n\n<p>Now here comes the question. Can this way, I mean representing the computations with numbers , be generalized?</p>\n\n<p>For example: there is a password whose length is 5.\n"1" stands for a, "2" for b and etc.</p>\n\n<p>or\nfor chess games, by decrementing the test case number, Can all the possibilities be evaluated ? And what is this process called,checking all the possibilities?</p>\n\n<p>I am majoring CS, first semester. Therefore, I am open to all suggestions.</p>\n\n<p>(I am really sorry for everything wrong, I\'d really appreciated,if you kindly warned me.)</p>\n', 'Tags': '<algorithms><decision-problem><numeral-representations>', 'LastEditorUserId': '17125', 'LastActivityDate': '2014-04-28T01:09:02.173', 'CommentCount': '1', 'AcceptedAnswerId': '24165', 'CreationDate': '2014-04-27T23:19:38.823', 'Id': '24160'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>How would one write code to find and add the prime numbers between 4 and 5 million? I need a code that can find all such numbers, and then add them together. I'm not too keen on computer science, so I wouldn't know even where to begin. (I'm a math major).</p>\n\n<p>Any help is appreciated. Also any advice on how to compile and run the code would be helpful.  </p>\n\n<p><strong>PS</strong> I know the answer is one of the following: 294185048443, 303141243820, 294095048847, 277319432363, 453190021165, or 210569014847.</p>\n", 'ViewCount': '59', 'Title': 'Algorithm to find and add prime numbers', 'LastActivityDate': '2014-04-28T01:14:22.950', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17126', 'Tags': '<algorithms><primes>', 'CreationDate': '2014-04-27T23:26:30.997', 'Id': '24162'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am working on acyclic orientations of undirected graphs and have the following questions: </p>\n\n<ol>\n<li>Given connected undirected simple graph $G$, how to find all possible acyclic orientations of $G$ ? </li>\n<li>What is the number of acyclic orientations? \nIt is known (from <a href="http://math.mit.edu/~rstan/pubs/pubfiles/18.pdf">here</a>) to be $(-1)^p\\ \\chi(G,-\\lambda)$ for a graph $G$ with $p$ vertices where $\\chi$ is the chromatic polynomial evaluated at $-\\lambda$; but I wasn\'t successful in understanding how to evaluate $\\chi$ at a negative value ($-\\lambda$).  </li>\n</ol>\n', 'ViewCount': '62', 'Title': 'Algorithm to find all acyclic orientations of a graph', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-28T17:24:29.400', 'LastEditDate': '2014-04-28T03:02:59.220', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<algorithms><graph-theory><counting>', 'CreationDate': '2014-04-28T02:49:32.510', 'Id': '24171'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Please consider the following Divide-And-Conquer Problem:</p>\n\n<blockquote>\n  <p>You\u2019re consulting for a small computation-intensive investment\n  company, and they have the following type of problem that they want to\n  solve over and over. A typical instance of the problem is the\n  following. They\u2019re doing a simulation in which they look at n\n  consecutive days of a given stock, at some point in the past. Let\u2019s\n  number the days i=1,2,...,n; for each day i, they have a price p(i)\n  per share for the stock on that day. (We\u2019ll assume for simplicity that\n  the price was fixed during each day.) Suppose during this time period,\n  they wanted to buy 1,000 shares on some day and sell all these shares\n  on some (later) day. They want to know: When should they have bought\n  and when should they have sold in order to have made as much money as\n  possible? (If there was no way to make money during the n days, you\n  should report this instead.)</p>\n</blockquote>\n\n<p>There already is a <a href="http://cs.stackexchange.com/questions/10050/confusion-related-to-a-divide-and-conquer-problem">discussion about the same problem</a>, but I don\'t understand why they try to make it that complicated. I think the following Algorithm will yield the result they ask for:</p>\n\n<pre><code>DivideAndConquer(1,..,n)\n(i1,j1) := DivideAndConquer(1,..,n/2)\n(i2,j2) := DivideAndConquer(n/2 + 1,..,n)\nreturn (min(p(i1),pi2),max(p(j1),p(j2)))\n</code></pre>\n\n<p>Even if they ask for the range, we can get it from <code>(i,j) := DivideAndConquer(1,..,n)</code> by <code>j-i</code>. So, what am I missing?</p>\n\n<p>PS: I don\'t see the point of using a DivideAndConquer approch at all. Why not sorting the input twice. Once in ascending and once in descending order induced by <code>p(i)</code>.</p>\n\n<p>EDIT: According to the explanation of Yuval Filmus, the algorithm should look like the following:</p>\n\n<pre><code>DivideAndConquer(1,..,n)\n(i1,j1) := DivideAndConquer(1,..,n/2)\n(i2,j2) := DivideAndConquer(n/2 + 1,..,n)\nLet p(i) be minimal in 1,..,n/2\nLet p(j) be maximal in n/2 + 1,..,n\nreturn (i,j)\n</code></pre>\n', 'ViewCount': '31', 'Title': 'Just another Divide-And-Conquer question - but somehow different', 'LastEditorUserId': '12502', 'LastActivityDate': '2014-04-28T19:03:31.463', 'LastEditDate': '2014-04-28T19:03:31.463', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12502', 'Tags': '<algorithms><algorithm-analysis><divide-and-conquer>', 'CreationDate': '2014-04-28T18:15:22.217', 'Id': '24195'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to prove that the Acyclic Subgraph Problem (AS) is NP-hard by showing that the Independent Set Problem (IS) is polynomially reducible to AS. </p>\n\n<p><strong>AS is as follows:</strong> Given a directed graph G = (V, E) and an integer k, does G contain a subset V' of\nk vertices such that the induced subgraph on V'is acyclic?</p>\n\n<p><strong>IS is as follows:</strong> Given an undirected graph G = (V, E) and an integer k, does G contain a subset V' of k vertices such that no two vertices in V'are adjacent to one another?</p>\n\n<p><strong>I have developed the following:</strong>\nGiven an undirected graph, <em>G = (V,E)</em>, we can construct a directed graph, <em>D =(V, E')</em>. We do this by addd the edges <em>(u,v)</em> and <em>(v,u)</em> for every edge in <em>E</em>. If <em>G</em> has an independent set of size <em>k</em>, then the corresponding vertices in <em>D</em> are an acyclic subgraph. Similarly, if <em>D</em> has an acyclic subgraph of size <em>k</em>, then those <em>k</em> vertices must form an independent set in <em>D</em> as if there is an edge between two vertices in <em>D</em>. Then, there is a directed cycle between them, thus those <em>k</em> vertices form an independent set in <em>G</em>. </p>\n\n<p>Can anyone help me further with this proof? I am not sure if what I have is going in the right direction or not. </p>\n\n<p>Any help is appreciated, thanks! </p>\n", 'ViewCount': '23', 'Title': 'Prove that Acyclic Subgraph is NP-Hard by showing Independent Set can be reduced to Acyclic Subgraph', 'LastActivityDate': '2014-04-28T19:11:30.353', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17151', 'Tags': '<algorithms><np-complete><reductions><np-hard>', 'CreationDate': '2014-04-28T19:11:30.353', 'Id': '24197'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading the Paxos notes from yale from the following link:</p>\n\n<p><a href="http://pine.cs.yale.edu/pinewiki/Paxos" rel="nofollow">http://pine.cs.yale.edu/pinewiki/Paxos</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was trying to better understand the revoting mechanism to avoid deadlocks in Paxos. The revoting mechanism is explained as follows in the article: </p>\n\n<blockquote>\n  <p>The revoting mechanism now works like this: before taking a vote, a\n  proposer tests the waters by sending a prepare(n) message to all\n  accepters where n is the proposal number. An accepter responds to this\n  with a promise never to accept any proposal with a number less than n\n  (so that old proposals don\'t suddenly get ratified) <strong>together with\n  the highest-numbered proposal that the accepter has accepted</strong> (so\n  that the proposer can substitute this value for its own, in case the\n  previous value was in fact ratified).</p>\n</blockquote>\n\n<p>The bold section is the one that I was trying to understand better. The author tries to justify it with:</p>\n\n<blockquote>\n  <p>"...so that the proposer can substitute this value for its own, in case the\n  previous value was in fact ratified..."</p>\n</blockquote>\n\n<p>But I didn\'t really understand why the proposer would want to ratify the previous value. By doing this, what crucial safety property is he guaranteeing? Why is he responding with that and not something else? Responding with the highest could be problem, right, since the current proposal would get lost?</p>\n', 'ViewCount': '22', 'Title': 'Why does an acceptor send the highest numbered proposal with number less than n as a response to prepare(n) in paxos?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:32:48.753', 'LastEditDate': '2014-04-30T16:32:48.753', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-04-29T04:00:18.340', 'Id': '24209'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of vertices $V$ and a set of segments $S$. I want to know whether a segment in the set $S$ is <em>Delaunay</em> against the vertices in $V$.</p>\n\n<p>I would like to state my assumed definition of a <em>Delaunay edge</em>: An edge is <em>Delaunay</em>, iff there exists a circumsphere of its endpoints not containing any other vertex <em>inside</em> it. </p>\n\n<p>I would like to know <em>practical</em> approaches/algorithms for such <em>Delaunay-edge</em> test.</p>\n', 'ViewCount': '45', 'Title': 'Practical algorithm for testing whether an edge is Delaunay', 'LastEditorUserId': '2205', 'LastActivityDate': '2014-04-29T10:10:48.570', 'LastEditDate': '2014-04-29T08:04:32.033', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15154', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-04-29T07:23:14.200', 'Id': '24215'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been playing around with a simple probabilistic data structure which is very similar to a Bloom filter. Where a Bloom filter would use $k$ independent hash functions to choose $k$ of the $m$ bits to set, this structure uses $m$ hash functions, and sets each bit with probability $p$.</p>\n\n<p>This structure doesn't produce as low a false-positive rate as Bloom filters, but it seems to be extremely fast to compute, particularly if $m$ is some multiple of the machine word size and $p = 2^{-b}$ for some integer $b$: The hash functions can be computed in parallel by AND-ing $b$ independent $m$-bit hashes, and no dependent indexing or variable bitshifts are required.</p>\n\n<p>I'm certain someone's come up with this idea before me, and done a lot more advanced analysis and comparison of it than I'm qualified to do. Is there a particular name for this type of structure?</p>\n", 'ViewCount': '33', 'Title': 'Bloom filter variant', 'LastEditorUserId': '8410', 'LastActivityDate': '2014-04-29T17:07:58.197', 'LastEditDate': '2014-04-29T13:27:12.427', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24230', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8410', 'Tags': '<reference-request><data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2014-04-29T10:55:19.287', 'Id': '24217'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I just completed a detailed theoretical mathematical study about the PageRank algorithm, I want to make a sample implementation to make some tests and play with it, supposing that I have a local set/folder full of a several webpages, representing a mini Web, I want to make a small software where I can make searches by implementing the PageRank algorithm (with some modifications in the algorithm) and make tests and simulations.</p>\n\n<p>From what I know, I should crawl and index those web pages, then play with my ranking.</p>\n\n<p>Is there a solution that provide me crawling and indexing and spares me the effort of working on them so I can focus on my main problem ?</p>\n', 'ViewCount': '10', 'ClosedDate': '2014-04-29T20:54:41.253', 'Title': 'Steps to implement PageRank Algorithm on a local set of webpages', 'LastActivityDate': '2014-04-29T12:21:55.330', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14915', 'Tags': '<search-algorithms><searching>', 'CreationDate': '2014-04-29T12:21:55.330', 'Id': '24218'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>While going through probabilistic/average analysis of an algorithm, I found written somewhere that average cost and expected cost are same. Can anyone please tell me what does exactly expected cost stands for.I think we take care of likelihood of an event while finding expected cost unlike average cost.</p>\n', 'ViewCount': '49', 'Title': 'What is the difference between expected cost and average cost of an algorithm?', 'LastActivityDate': '2014-04-29T22:28:14.800', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17176', 'Tags': '<algorithms>', 'CreationDate': '2014-04-29T13:27:16.773', 'Id': '24220'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Hello guys I will write a thesis about intelligent System and im just wondering if someone can give me an idea on how can i build my title or what program i can make in intelligent systems.. or Like articles Lectures i can read ETC.</p>\n\n<p>I just need tips and some books you can suggest for me in choosing my title. sorry for questioning it here but im preparing it because im a little nervous so i need some suggestion lectures books article i can read. Tnx for those who will answer I thank you in advance</p>\n\n<p>im planning on using java.</p>\n\n<p>sorry for the bad english.</p>\n', 'ViewCount': '10', 'ClosedDate': '2014-04-29T20:31:22.503', 'Title': 'Intelligent System questions', 'LastActivityDate': '2014-04-29T14:10:29.403', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17179', 'Tags': '<algorithms><algorithm-analysis><artificial-intelligence>', 'CreationDate': '2014-04-29T14:10:29.403', 'Id': '24222'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm using mat lab's implementation of hierarchical clustering algorithm, with pdist, linkage , cophenet, dendrogram, and cluster. But I'm not exactly sure where or how to find the time complexity</p>\n\n<p>It is an agglomerative type of hierarchal clustering, and although they're usually \n$O(n^3)$\nthis one in particular works pretty fast for 177 data points of 3 features (177x3), which got me curious. Is it $n^3$ or an optimal version?</p>\n", 'ViewCount': '15', 'Title': "What is the time complexity of Matlab's Hierarchical Clustering?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T11:39:59.340', 'LastEditDate': '2014-04-30T11:39:59.340', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6980', 'Tags': '<algorithms><runtime-analysis><cluster>', 'CreationDate': '2014-04-29T16:57:40.817', 'Id': '24229'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '26', 'Title': 'Given a complete, weighted and undirected graph $G$, complexity of finding a path with a specific cost', 'LastEditDate': '2014-04-29T18:42:14.217', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17185', 'FavoriteCount': '1', 'Body': '<p>Given a fully connected graph $G$, suppose that we are searching for a simple path $P$ with a specific cost $c$. </p>\n\n<p>Is answering to that problem <em>yes</em> or <em>no</em> equivalent to subset-sum problem?\nWhat would be the complexity of finding such path?</p>\n\n<p>I have made a reduction from subset-sum problem:</p>\n\n<p>If each number in a set $S$ is a vertex of $G$ and weight of $&lt;i,j&gt;$ is $|i-j|$, then answering the question above <em>yes</em> or <em>no</em> is the same as solving the sumbet-sum problem.</p>\n\n<p>P.S. The initial vertex I have visited is added to the cost.</p>\n\n<p><strong>Edit: Edge weights</strong></p>\n', 'Tags': '<algorithms><graph-theory><graphs><decision-problem>', 'LastEditorUserId': '17185', 'LastActivityDate': '2014-04-29T20:37:29.247', 'CommentCount': '4', 'AcceptedAnswerId': '24233', 'CreationDate': '2014-04-29T17:40:06.137', 'Id': '24231'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to implement <code>PageRank</code> algorithm on a set of web pages, for that I need a sample <code>dataset</code> of web pages, and the web graph corresponding to them, this web graph represents the links between the pages that the data set contains.</p>\n\n<p>I need the web graph so I can get the transition matrix and do the calculation needed</p>\n\n<p>Example : </p>\n\n<pre><code>      URL1 -&gt; URL2\n      URL3390-&gt;URL5\n</code></pre>\n\n<p>and the URLxxxx as an id, is mapped somehow to the corresponding web page so I can know how to map.</p>\n\n<p>My question is: how/where can I get this resource (I've tried many links on the internet but nothing really helps), I would also like it to be not of a very large size, (internet connexion limitation), if I can't have this as it is, advice me what should I do ?</p>\n", 'ViewCount': '8', 'ClosedDate': '2014-04-30T09:05:45.803', 'Title': 'Where to get a web graph with corresponding web pages dataset', 'LastActivityDate': '2014-04-29T23:06:59.003', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14915', 'Tags': '<search-algorithms><searching>', 'CreationDate': '2014-04-29T23:06:59.003', 'Id': '24242'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '28', 'Title': "Suzuki Kasami's Broadcast algorithm", 'LastEditDate': '2014-04-30T08:09:21.650', 'AnswerCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17207', 'FavoriteCount': '0', 'Body': "<p>I have referred wikipedia and i have the following doubt...</p>\n\n<p>We want to achieve mutual exclusion in distributed systems through a token based algorithm.</p>\n\n<p>What i would do?\nSuppose there are 4 sites S1,S2,S3,S4 and each site has a queue  of pending req (Q).</p>\n\n<p>Suppose S1 has token and it wants to go to CS it goes directly.Now suppose S2 requests for the token S1 would put that req in Q.After it has exited from CS it would pass the token to the head of the queue...i.e req which arrived 1st.</p>\n\n<p>DOUBT:: But in Suzuki-Kasami's algorithm for what reason have they taken LN[1..N] and RN[1..N]??</p>\n", 'ClosedDate': '2014-05-03T23:50:40.483', 'Tags': '<algorithms><distributed-systems>', 'LastEditorUserId': '17207', 'LastActivityDate': '2014-04-30T08:09:21.650', 'CommentCount': '0', 'CreationDate': '2014-04-30T07:38:31.890', 'Id': '24252'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This question is from a practice exam in my algorithms class. I'm posting the question and the answer listed in that practice exam:</p>\n\n<blockquote>\n  <p>Let $W$ be an $n\\times n$ matrix whose $(i,j)$-th entry is $\\omega_n^{ij}$, where $\\omega_n$ is a principal $n$th root of unity. Let $X=(X_0,\\dots,X_{n-1})$ be an $n$-vector. The product $W \\times X$ can be computed in $O(n\\log n)$ time. Let $FFT(X)$ denote the vector that results by applying the FFT evaluation algorithm to the vector $X$. </p>\n  \n  <p>Describe an $O(n)$ algorithm to compute $FFT(FFT(x))$.</p>\n  \n  <p>Answer: $FFT(FFT(x))$ is $W^2\\times X$<br>\n  $(W^2)_{jk} = 0$ if $j+k$ is not a multiple of $n$, and $n$ otherwise.</p>\n</blockquote>\n\n<p>I'm confused about everything:<br>\n1. How does this run in $O(n)$? To compute $(W^2)_{jk}$ I'm already  iterating through $n^2$ elements....<br>\n2. Why is $FFT(FFT(x)) = W^2\\times X$?<br>\n3. Why is $(W^2)_{jk} = 0$ if $j+k$ is not a multiple of $n$, $n$ otherwise?</p>\n\n<p>I would appreciate an answer that isn't too advanced as my math skills are limited to basic undergrad math classes of an engineering major.</p>\n", 'ViewCount': '40', 'Title': 'An $O(n)$ algorithm to FFT-evaluate an FFT evaluation', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T18:54:56.350', 'LastEditDate': '2014-04-30T18:54:56.350', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24270', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '17225', 'Tags': '<algorithms><algorithm-analysis><fourier-transform>', 'CreationDate': '2014-04-30T18:13:56.290', 'Id': '24269'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Assume that I have a set of coplanar points $P = \\{p_1, p_2, ... , p_n\\}$ \nThe equation of the plane is unknown.\n$\\forall p_i,p_j \\in P$, pairwise euclidian distance $d(p_ip_j)$ is known. </p>\n\n<p>And I have a set of points of whose coordinates are known $Q = \\{q_1, q_2, ..., q_m\\}$</p>\n\n<p>If I know some of the pairwise distances $d(pq)$ $p \\in P$, $q \\in Q$;</p>\n\n<p>At least how many $d(pq)$ distance measurements do I need to find the equation of the plane that points in $P$ lie on?</p>\n', 'ViewCount': '21', 'Title': 'Localizing a plane in 3-D using distance geometry', 'LastEditorUserId': '17185', 'LastActivityDate': '2014-04-30T22:34:52.150', 'LastEditDate': '2014-04-30T22:34:52.150', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17185', 'Tags': '<algorithms><computational-geometry>', 'CreationDate': '2014-04-30T21:46:58.557', 'Id': '24276'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Can anyone provide an example of an ordered graph for which the following algorithm produces the the incorrect longest path?</p>\n\n<pre><code>let A = V[1]\nlongestPath = 0\n\nwhile there is an edge out of A do\n     choose edge (A,V[i]) for which i is as small as possible\n     set A = V[i]\n     LongestPath = LongestPath + 1\nend while\nreturn longestPath\n</code></pre>\n\n<p>Thanks for the comments. I've tried several examples but I dont see why this algorithm would not work</p>\n\n<p>If we start with a small 3 node graph with nodes A,B,C. If node A has an edge that goes to B and and edge that goes to C, And B has one edge that goes to C. Then starting with node A we would take the shortest path per the algorithm and go to node b. From node b we would go to node c and the algorithm works.</p>\n", 'ViewCount': '28', 'ClosedDate': '2014-05-01T02:46:10.497', 'Title': 'Longest path of a directed ordered graph', 'LastEditorUserId': '15850', 'LastActivityDate': '2014-05-01T03:14:35.177', 'LastEditDate': '2014-05-01T03:14:35.177', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15850', 'Tags': '<algorithms><graph-theory><graphs>', 'CreationDate': '2014-05-01T00:36:13.987', 'Id': '24278'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Sutton and Barto\'s reinforcement learning book, in multi-armed bandit problem a phrase has been used. "finding an optimal action" using greedy/$\\epsilon$-greedy algorithm. When it is said that an algorithm "finds the optimal action " ? </p>\n', 'ViewCount': '25', 'Title': 'What does "finding an optimal action" for a bandit mean?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-01T05:10:18.083', 'LastEditDate': '2014-05-01T02:44:31.973', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13091', 'Tags': '<algorithms><terminology><machine-learning><optimization><artificial-intelligence>', 'CreationDate': '2014-05-01T01:17:32.423', 'Id': '24282'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>So suppose we have a functional inequality with polynomial arguments in $2$ variables, $\\sum_i c_i f(p_i(x,y)) \\geq 0$, where $c_i$ are say given integer constants and $p_i$ are given polynomials, say with integer coefficients. We want the inequality to hold for all $x,y \\in \\mathbb{R}$.</p>\n\n<p>Is it a decidable problem to determine whether there are non-constant real-valued functional solutions for $f:{\\mathbb R} \\to {\\mathbb R}$?</p>\n\n<p>A recent problem of this flavor came up on math.stackexchange, which was to determine any non-constant solutions to $f(x) + f(y) \\geq f(x + y) + f(xy)$. I wondered whether it was possible to determine the existence question with an algorithm rather than ad hoc analysis.</p>\n', 'ViewCount': '16', 'Title': 'Is deciding whether there is a non-constant solution to a functional inequality with polynomial arguments decidable, with 2 variables?', 'LastEditorUserId': '755', 'LastActivityDate': '2014-05-01T22:13:34.133', 'LastEditDate': '2014-05-01T22:13:34.133', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><undecidability>', 'CreationDate': '2014-05-01T21:07:09.223', 'Id': '24309'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the problem of finding the string "He is a scary @#$!&amp;$#ing moron" in next Sunday\'s New York Times, which is available to you in electronic form. The Times is available uncompressed, with each letter represented by a byte, using the standard ASCII character codes for numbers 0 to 127. \nYou decide in the first pass use the KMP search algorithm to find all occurrences of "moron" in binary format.</p>\n\n<p>Compute the prefix function for the binary expansion of the word moron. The ASCII character codes for capital letters A-Z start at 65 increasing sequentially to 90 and for lower case letters a-z start at 97 increasing sequentially to 122. Thus, the ASCII codes for the letters are: m = 109, o =111, r = 114, n = 110 in decimal or m = 155, o = 157, r = 162, n = 156 in octal.</p>\n\n<pre><code>#include &lt;stdio.h&gt;  \n#define CHAR_SET 256  \nint main(void)  {  \nint i;  for(i=0; i&lt;CHAR_SET; i++) \nprintf(``%d-th ASIC character, %c, is %o in octal\\n\'\',i,i,i); \nreturn 0;\n}  \n</code></pre>\n', 'ViewCount': '17', 'ClosedDate': '2014-05-02T12:47:01.557', 'Title': 'Use KMP Search Algorithim to find the string', 'LastActivityDate': '2014-05-01T22:11:45.383', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17261', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2014-05-01T22:11:45.383', 'Id': '24311'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to write a genetic algorithm for a program. Most examples for genetic algorithms use something like this as the input:</p>\n\n<pre><code>aaaaaaaaaa\n</code></pre>\n\n<p>and mutate/crossover until they get</p>\n\n<pre><code>helloworld\n</code></pre>\n\n<p>or similar. This requires however that you start with something of length N, but my search space doesn't have a fixed length. How should I alter the mutation/crossover steps to allow for changes in unit size?</p>\n", 'ViewCount': '15', 'Title': 'Programming a genetic algorithm with a non-fixed size', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-02T21:29:08.760', 'LastEditDate': '2014-05-02T21:25:18.507', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17279', 'Tags': '<algorithms><optimization><genetic-algorithms>', 'CreationDate': '2014-05-02T17:10:57.277', 'Id': '24320'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I want to develop one algorithm which can predict shortest time to be taken to go to a destination from a source in a road network based on traffic congestion data. \nConsider that I have a server which gives me real time traffic congestion data for all the path segments in a road network. Now I want to develop such an algorithm which can predict time to be taken based on these data. So in short: </p>\n\n<p>time_to_be_taken_from_source_to_destination = function ( traffic_congestion_data ). And 'time_to_be_taken_from_source_to_destination' is proportional to 'traffic_congestion_data' as time increases when traffic_congestion increases.</p>\n\n<p>Are there any such algorithm alraedy developed? Any idea? reference to any algorithm or theory will be appreciated. </p>\n", 'ViewCount': '10', 'Title': 'shortest time based on traffic congestion data', 'LastActivityDate': '2014-05-03T22:11:25.087', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16563', 'Tags': '<algorithms><graph-theory><shortest-path>', 'CreationDate': '2014-05-03T05:47:16.070', 'Id': '24341'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This <a href="http://en.wikipedia.org/wiki/Max-flow_min-cut_theorem#Example" rel="nofollow">Wikipedia</a> example is very confusing. Its saying the max flow = min cut. But I see the max flow = 9 and the min cut = 7. If not, how does the capacity =min cut here? Which is the max flow min cut theorem. </p>\n\n<p><img src="http://i.stack.imgur.com/HOUJi.png" alt="http://en.wikipedia.org/wiki/Max-flow_min-cut_theorem#Example"></p>\n\n<p>Thanks in advance</p>\n', 'ViewCount': '14', 'Title': 'Max-Flow, minimum cut', 'LastActivityDate': '2014-05-03T08:17:56.363', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7173', 'Tags': '<algorithms><complexity-theory>', 'CreationDate': '2014-05-03T08:15:38.933', 'Id': '24346'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m studying binomial heaps in anticipation for my finals and the <a href="http://en.wikipedia.org/wiki/CLRS" rel="nofollow">CLRS</a> book tells me that insertion in a binomial heap takes $\\Theta(\\log n)$ time. So given an array of numbers it would take $\\Theta(n\\log n)$ time to convert it a a binomial heap. To me that seems a bit pessimistic and like a naive implementation. Does anyone know of a method/implementation that can convert an array of numbers to a binary heap in $\\Theta(n)$ time?</p>\n', 'ViewCount': '46', 'Title': 'efficient binomial heap', 'LastActivityDate': '2014-05-03T15:47:26.787', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24356', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15826', 'Tags': '<algorithms><algorithm-analysis><heaps>', 'CreationDate': '2014-05-03T14:47:09.500', 'Id': '24355'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Most Smalltalk dialects currently implement a naive inexact floating modulus (fmod/remainder).<br>\nI just changed this to improve Squeak/Pharo and eventually other Smalltalk adherence to standards (IEEE 754, ISO/IEC 10967), as I already did for other state of the art floating point operations.</p>\n\n<p>However for adoption of those changes, I anticipate that adhering to standard will not be enough to convince my peers, so explaining in which circumstances this exactness would really matter would help me a lot. I could not find a good example by myself so far.</p>\n\n<p>Does any one here knows why/when/where (IOW in which algorithm) such exactness of modulus would matter?</p>\n', 'ViewCount': '14', 'Title': 'Why does floating point modulus exactness matters?', 'LastActivityDate': '2014-05-03T19:17:55.147', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17303', 'Tags': '<algorithms><floating-point>', 'CreationDate': '2014-05-03T19:17:55.147', 'Id': '24362'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the definition of a "c-competitive" algorithm? For example what does it mean, if we say that there is a 2-competitive algorithm for packet routing?</p>\n', 'ViewCount': '17', 'Title': u'Definition of \u201cc-competitive\u201d algorithm', 'LastActivityDate': '2014-05-03T20:33:48.200', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24365', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2748', 'Tags': '<algorithms><performance>', 'CreationDate': '2014-05-03T19:38:26.040', 'Id': '24364'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know Distributed Graph Coloring algorithm in O(log* n)\nwhich is given at P11: <a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">Vertex Coloring</a></p>\n\n<p>same for Maximal Independent Set [MIS] they gave remark like algorithms exist in O(log* n) time at P70: <a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">Maximal Independetn Set</a></p>\n\n<p>How we can reduce Graph coloring problem to MIS in O(log* n) time?</p>\n\n<p>If you feel difficulty in understanding algorithm then please comment. </p>\n', 'ViewCount': '11', 'Title': 'MIS algorithm for Tree in O(log* n) time', 'LastActivityDate': '2014-05-04T02:29:39.507', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9665', 'Tags': '<algorithms><graph-theory><data-structures><distributed-systems>', 'CreationDate': '2014-05-03T22:14:11.747', 'Id': '24369'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was attempting to implement prim's algorithm, and I noticed the following:</p>\n\n<p>[Assume that there is a vector which holds all vertices that are so far in the minimum spanning tree]</p>\n\n<ol>\n<li>The computer has to go through each element of the vector </li>\n<li>Then it has to go through each possible vertex that is not in the vector</li>\n<li>It has to do this until all vertices of the graph are in the vector</li>\n</ol>\n\n<p>Each of the three steps takes $O(|V|)$ time, so should the time complexity not be $O(|V|^3)$? Please help me find the error in my logic!</p>\n", 'ViewCount': '15', 'Title': "Time Complexity of Prim's Algorithm", 'LastEditorUserId': '11089', 'LastActivityDate': '2014-05-03T22:46:14.660', 'LastEditDate': '2014-05-03T22:46:14.660', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17306', 'Tags': '<algorithms>', 'CreationDate': '2014-05-03T22:31:17.863', 'FavoriteCount': '1', 'Id': '24371'}},