{'Body': '<p>When searching graphs, there are two easy algorithms: <strong>breadth-first</strong> and <strong>depth-first</strong> (Usually done by adding all adjactent graph nodes to a queue (breadth-first) or stack (depth-first)).</p>\n\n<p>Now, are there any advantages of one over another?</p>\n\n<p>The ones I could think of:</p>\n\n<ul>\n<li>If you expect your data to be pretty far down inside the graph, <em>depth-first</em> might find it earlier, as you are going down into the deeper parts of the graph very fast.</li>\n<li>Conversely, if you expect your data to be pretty far up in the graph, <em>breadth-first</em> might give the result earlier.</li>\n</ul>\n\n<p>Is there anything I have missed or does it mostly come down to personal preference?</p>\n', 'ViewCount': '10527', 'Title': 'Graph searching: Breadth-first vs. depth-first', 'LastEditorUserId': '755', 'LastActivityDate': '2013-12-24T23:35:26.743', 'LastEditDate': '2013-09-09T01:44:33.060', 'AnswerCount': '6', 'CommentCount': '1', 'Score': '31', 'PostTypeId': '1', 'OwnerUserId': '101', 'Tags': '<algorithms><graph-theory><search-algorithms><graph-traversal>', 'CreationDate': '2012-03-13T10:05:58.093', 'FavoriteCount': '14', 'Id': '298''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have noticed that different data structures are used when we implement search algorithms. For example, we use queues to implement breadth first search, stacks to implement depth-first search and min-heaps to implement the <a href="https://en.wikipedia.org/wiki/A%2a_algorithm">A* algorithm</a>. In these cases, we do not need to construct the search tree explicitly.</p>\n\n<p>But I can not find a simple data structure to simulate the searching process of the <a href="http://www.cs.cf.ac.uk/Dave/AI2/node26.html">AO* algorithm</a>. I would like to know if constructing the search tree explicitly is the only way to implement AO* algorithm? Can anybody provide me an efficient implementation?</p>\n', 'ViewCount': '994', 'Title': 'How to implement AO* algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T20:12:52.987', 'LastEditDate': '2012-09-22T09:04:58.943', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '931', 'Tags': '<algorithms><graphs><data-structures><search-algorithms>', 'CreationDate': '2012-04-04T03:05:59.537', 'FavoriteCount': '2', 'Id': '1020''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>On Alpha-Beta pruning, <a href="http://en.wikipedia.org/wiki/Negascout" rel="nofollow">NegaScout</a> claims that it can accelerate the process by setting [Alpha,Beta] to [Alpha,Alpha-1].</p>\n\n<p>I do not understand the whole process of NegaScout.</p>\n\n<p>How does it work? What is its recovery mechanism when its guessing failed?</p>\n', 'ViewCount': '714', 'Title': 'How does the NegaScout algorithm work?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-08T21:24:54.280', 'LastEditDate': '2012-04-08T21:24:54.280', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '240', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2012-04-08T16:07:29.703', 'Id': '1134''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m looking for a list of informed search algorithms, also known as heuristic search algorithms. </p>\n\n<p>I\'m aware of: </p>\n\n<ol>\n<li><p><a href="http://en.wikipedia.org/wiki/Best-first_search" rel="nofollow">best-first search</a></p>\n\n<ul>\n<li>Greedy best-first search</li>\n<li><a href="http://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* search</a></li>\n</ul></li>\n</ol>\n\n<p>Are there more best-first algorithm or other informed searches that are not best-first?</p>\n', 'ViewCount': '289', 'Title': 'Survey of informed search algorithms?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-09T08:02:07.933', 'LastEditDate': '2012-04-16T20:08:11.290', 'AnswerCount': '3', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '935', 'Tags': '<algorithms><reference-request><artificial-intelligence><search-algorithms>', 'CreationDate': '2012-04-16T09:53:58.100', 'FavoriteCount': '2', 'Id': '1300''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>So I thought this (though somewhat basic) question belonged here:</p>\n\n<p>Say I have a graph of size 100 nodes arrayed in a 10x10 pattern (think chessboard). The graph is undirected, and unweighted. Moving through the graph involves moving three spaces forward and one space to either right or left (similar to how a chess knight moves across a board).</p>\n\n<p>Given a fixed beginning node, how would one find the shortest path to any other node on the board?</p>\n\n<p>I imagined that there would only be an edge between nodes that are viable moves. So, given this information, I would want to find the shortest path from a starting node to an ending node.</p>\n\n<p>My initial thought was that each edge is weighted with weight 1. However, the graph is undirected, so Djikstras would not be an ideal fit. Therefore, I decided to do it using an altered form of a depth first search.</p>\n\n<p>However, I couldn't for the life of me visualize how to get the shortest path using the search.</p>\n\n<p>Another thing I tried was putting the graph in tree form with the starting node as the root, and then selecting the shallowest (lowest row number) result that gave me the desired end node... this worked, but was incredibly inefficient, and thus would not work for a larger graph.</p>\n\n<p>Does anyone have any ideas that might point me in the right direction on this one?</p>\n\n<p>Thank you very much.</p>\n\n<p>(I tried to put in a visualization of the graph, but was unable to due to my low reputation)</p>\n", 'ViewCount': '2595', 'Title': 'Shortest Path on an Undirected Graph?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-01T22:38:48.113', 'LastEditDate': '2012-04-18T05:56:13.197', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '1330', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1132', 'Tags': '<algorithms><graphs><graph-theory><search-algorithms><shortest-path>', 'CreationDate': '2012-04-18T04:23:36.273', 'Id': '1329''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>[<em>previously appearing in cstheory, it was closed there and introduced here instead</em>]</p>\n\n<p>Given an edge-weighted graph $G=(V,E)$ the problem of finding the shortest path is known to be in P ---and indeed a simple approach would be Dijkstra\'s algorithm which can solve this problem in $O(V^2)$. A similar problem is to find the maximum path in $G$ from a source node to a target node and this can be solved with Integer Programming so that, as far as I know, this is not known to be in P.</p>\n\n<p>Now, the problem of finding a path in $G$ such that it deviates the minimum from a given target value (typically larger than the optimal distance but less than the maximum distance that separates the source and target nodes) has been conjectured to be in EXPTIME (see section "Conventions" of <a href="http://search-conference.org/index.php/Main/SOCS09program" rel="nofollow">A depth-first approach to target-value search</a> in the proceedings of SoCS 2009). In particular, this paper addresses this particular problem for  directed acyclic graphs (DAGs). A previous work is <a href="http://www.uwosh.edu/faculty_staff/furcyd/search_symposium_2008/schedule.html" rel="nofollow">Heuristic Search for Target-Value Path Problem</a>. There is event a US Patent of this algorithm <a href="http://www.google.es/patents?hl=es&amp;lr=&amp;vid=USPATAPP12497353&amp;id=gojwAAAAEBAJ&amp;oi=fnd&amp;dq=%22depth-first+search+for+target+value+problems%22&amp;printsec=abstract#v=onepage&amp;q=%22depth-first%20search%20for%20target%20value%20problems%22&amp;f=false" rel="nofollow">US 2011/0004625</a>.</p>\n\n<p>I\'ve been searching for related problems in other fields of Computer Science and Mathematics and strikingly, I have found none though this problem is clearly relevant in practice ---there are tons of opportunities to look for a specific target value instead of the minimum or the maximum path.</p>\n\n<p>Do you know related problems to this or additional bibliographical references to this problem? Any information on this problem including studies of their complexity would be very welcome</p>\n\n<p><strong>Note</strong>: as already pointed out by Jeffe in cstheory, proving this problem to be in EXPTIME is trivial and the authors probably meant EXPTIME-complete.</p>\n', 'ViewCount': '121', 'Title': 'Target-Value Search (& II)', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T20:16:41.033', 'LastEditDate': '2012-05-09T15:50:10.143', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1337', 'Tags': '<algorithms><complexity-theory><reference-request><search-algorithms>', 'CreationDate': '2012-05-02T12:21:24.380', 'Id': '1634''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1427', 'Title': 'Quicksort to find median?', 'LastEditDate': '2012-05-14T15:04:59.000', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': '<p>Why is the worst scenario $\\mathcal{O}\\left(n^2\\right)$ when using quicksort to find the median of a set of numbers?</p>\n\n<ul>\n<li><p>If your algorithm continually picks a number larger than or smaller than <em>all</em> numbers in the list wouldn\'t your algorithm fail? For example if the list of numbers are:</p>\n\n<p>$S = (12,75,82,34,55,15,51)$</p>\n\n<p>and you keep picking numbers greater than $82$ or less than $12$ to create sublists with, wouldn\'t your set   always remain the same size?</p></li>\n<li><p>If your algorithm continually picks a number that creates sublists of $1$ why is the worst case scenario $\\mathcal{O}\\left(n^2\\right)$? Wouldn\'t efficiency be linear considering that according to the <a href="http://en.wikipedia.org/wiki/Master_theorem" rel="nofollow">Master Theorem</a>,  $d&gt;\\log_b a$?* (and therefore be $\\mathcal{O}\\left(n^d\\right)$ or specifically in this case $\\mathcal{O}\\left(n\\right)$)</p></li>\n</ul>\n\n<p>*Where $d$ is the efficiency exponent (i.e. linear, exponential etc.), $b$ is the factor the size of problem is reduced by at each iteration, $a$ is the number of subproblems and $k$ is the level. Full ratio: $T(n) = \\mathcal{O}\\left(n^d\\right) * (\\frac{a}{b^d})^k$</p>\n', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-14T15:06:04.740', 'CommentCount': '0', 'AcceptedAnswerId': '1791', 'CreationDate': '2012-05-11T01:27:39.883', 'Id': '1789''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am learning the <a href="http://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* search algorithm</a> on an 8-puzzle problem.</p>\n\n<p>I don\'t have questions about A*, but I have some for the heuristic score -  Nilsson\'s sequence score.</p>\n\n<p><a href="http://www.heyes-jones.com/astar.html" rel="nofollow">Justin Heyes-Jones web pages - A* Algorithm</a> explains A* very clearly. It has a picture for Nilsson\'s sequence scores.</p>\n\n<p><img src="http://i.stack.imgur.com/Wbl63.jpg" alt="Nilsson\'s sequence scores"></p>\n\n<p>It explains:</p>\n\n<p><strong>Nilsson\'s sequence score</strong></p>\n\n<blockquote>\n  <p>A tile in the center scores 1 (since it should be empty)</p>\n  \n  <p>For each tile not in the center, if the tile clockwise to it is not the one that should be clockwise to it then score 2. </p>\n  \n  <p>Multiply this sequence by three and finally add the total distance you need to move each tile back to its correct position. </p>\n</blockquote>\n\n<p>I can\'t understand the steps above for calculating the scores.</p>\n\n<p>For example, for the start state, what h = 17?</p>\n\n<blockquote>\n  <p>0 A C </p>\n  \n  <p>H B D </p>\n  \n  <p>G F E</p>\n</blockquote>\n\n<p>So, by following the description, </p>\n\n<p><code>B</code> is in the center, so we have 1</p>\n\n<p>Then <code>for each title not in the center, if the **tile** clockwise to **it** is not the one that should be clockwise to it then score 2.</code> I am not sure what this statement means. </p>\n\n<p>What does the double starred <code>title</code> refer to? </p>\n\n<p>What does the double starred <code>it</code> refer to?</p>\n\n<p>Does the double starred <code>it</code> refer to the center title (B in this example)? Or does it refer to each title not in the center?</p>\n\n<p>Is the next step that we start from <code>A</code>? So <code>C</code> should not be clockwise to <code>A</code>, then we have 2. And then <code>B</code> should be clockwise to <code>A</code>, then we ignore, and so on and so forth?</p>\n', 'ViewCount': '1466', 'Title': "Nilsson's sequence score for 8-puzzle problem in A* algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-18T22:19:54.293', 'LastEditDate': '2012-05-18T07:52:23.553', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'Jackson Tale', 'PostTypeId': '1', 'Tags': '<algorithms><machine-learning><search-algorithms><heuristics>', 'CreationDate': '2012-05-14T15:41:04.650', 'Id': '1904''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a game I\'m building some ai for that has 2 players making simultaneous moves.  In this game there is exactly one move where, if they both make it at the same time, the outcome is different than if they\'d made it separately (all other moves are pretty independent).</p>\n\n<p>Anyway, I\'m trying to find a good algorithm to throw at it.  Minimax with alpha-beta pruning seems like it would be a good candidate if the players were making alternating moves, but not for simultaneous ones.  I found <a href="http://www.lamsade.dauphine.fr/~saffidine/Papers/2012/Alpha-Beta%20Pruning%20for%20Games%20with%20Simultaneous%20Moves.pdf">a paper(pdf)</a> on the topic, but it\'s a little over my head- I\'m having trouble reading the pseduocode.</p>\n\n<p>So, can someone either help clarify that approach, suggest another way to accomplish alpha-beta pruning on such a game, or suggest a better algorithm entirely?</p>\n', 'ViewCount': '188', 'Title': 'Alpha-Beta Pruning with simultaneous moves?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-04T08:42:59.177', 'LastEditDate': '2012-06-04T08:42:59.177', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1737', 'Tags': '<algorithms><artificial-intelligence><search-algorithms><game-theory>', 'CreationDate': '2012-06-04T00:45:22.650', 'Id': '2215''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '753', 'Title': 'Finding interesting anagrams', 'LastEditDate': '2012-06-07T22:08:29.743', 'AnswerCount': '4', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '1786', 'FavoriteCount': '0', 'Body': "<p>Say that $a_1a_2\\ldots a_n$ and $b_1b_2\\ldots b_n$ are two strings of the same length.  An <strong>anagramming</strong> of two strings is a bijective mapping $p:[1\\ldots n]\\to[1\\ldots n]$ such that $a_i = b_{p(i)}$ for each $i$.</p>\n\n<p>There might be more than one anagramming for the same pair of strings.  For example, If $a=$<code>abcab</code> and $b=$<code>cabab</code> we have $p_1[1,2,3,4,5]\\to[4,5,1,2,3]$ and $p_2[1,2,3,4,5] \\to [2,5,1,4,3]$, among others.</p>\n\n<p>We'll say that the <strong>weight</strong> $w(p)$ of an anagramming $p$ is the number of values of $i\\in[1\\ldots n-1]$ for which $p(i)+1\\ne p(i+1)$. That is, it is the number of points at which $p$ does <em>not</em> increase by exactly 1.For example, $w(p_1) = 1$ and $w(p_2) = 4$.</p>\n\n<p>Suppose there exists an anagramming for two strings $a$ and $b$. Then at least one  anagamming must have least weight. Let's say this this one is <strong>lightest</strong>. (There might be multiple lightest anagrammings; I don't care because I am interested only in the weights.)</p>\n\n<h2>Question</h2>\n\n<p>I want an algorithm which, given two strings for which an anagramming exists, efficiently <strong>yields the exact weight of the lightest anagramming</strong> of the two strings. It is all right if the algorithm yields a lightest anagramming, but it need not.</p>\n\n<p>It is a fairly simple matter to generate all anagrammings and weigh them, but there may be many, so I would prefer a method that finds light anagrammings directly.</p>\n\n<hr>\n\n<h2>Motivation</h2>\n\n<p>The reason this problem is of interest is as follows.  It is very easy to make the computer search the dictionary and find anagrams, pairs of words that contain exactly the same letters.  But many of the anagrams produced are uninteresting.  For instance, the longest examples to be found in Webster's Second International Dictionary are:</p>\n\n<blockquote>\n  <p>cholecystoduodenostomy<br>\n  duodenocholecystostomy</p>\n</blockquote>\n\n<p>The problem should be clear: these are uninteresting because they admit a very light anagramming that simply exchanges the <code>cholecysto</code>, <code>duedeno</code>, and <code>stomy</code> sections, for a weight of 2. On the other hand, this much shorter example is much more surprising and interesting:</p>\n\n<blockquote>\n  <p>coastline<br>\n  sectional</p>\n</blockquote>\n\n<p>Here the lightest anagramming has weight 8.</p>\n\n<p>I have a program that uses this method to locate interesting anagrams, namely those for which all anagrammings are of high weight. But it does this by generating and weighing all possible anagrammings, which is slow.</p>\n", 'Tags': '<algorithms><strings><search-algorithms><natural-lang-processing>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-21T14:18:17.910', 'CommentCount': '2', 'AcceptedAnswerId': '2265', 'CreationDate': '2012-06-07T18:31:28.390', 'Id': '2259''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '182', 'Title': 'Determining the particular number in $O(n)$ time and space (worst case)', 'LastEditDate': '2012-07-25T03:17:34.603', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1715', 'FavoriteCount': '1', 'Body': "<p>$\\newcommand\\ldotd{\\mathinner{..}}$Given that $A[1\\ldotd n]$ are integers such that $0\\le A[k]\\le m$ for all $1\\le k\\le n$, and the occurrence of each number except a particular number in $A[1\\ldotd n]$ is an odd number. Try to find the number whose occurrence is an even number.</p>\n\n<p>There is an $\\Theta(n\\log n)$ algorithm: we sort $A[1\\ldotd n]$ into $B[1\\ldotd n]$, and break $B[1\\ldotd n]$ into many pieces, whose elements' value are the same, therefore we can count the occurrence of each element.</p>\n\n<p>I want to find a worst-case-$O(n)$-time-and-$O(n)$-space algorithm.</p>\n\n<p>Supposing that $m=\\Omega(n^{1+\\epsilon})$ and $\\epsilon&gt;0$, therefore radix sort is not acceptable.\n$\\DeclareMathOperator{\\xor}{xor}$\nBinary bitwise operations are acceptable, for example, $A[1]\\xor A[2]$.</p>\n", 'Tags': '<algorithms><search-algorithms>', 'LastEditorUserId': '1715', 'LastActivityDate': '2012-07-25T03:17:34.603', 'CommentCount': '15', 'AcceptedAnswerId': '2867', 'CreationDate': '2012-07-23T02:31:17.933', 'Id': '2863''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '880', 'Title': 'Example using Penetrance & Branching Factor in State-space Heuristic Search', 'LastEditDate': '2012-08-02T06:43:50.807', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2321', 'FavoriteCount': '2', 'Body': u'<p>I need an example for how to calculate penetrance and branching factor of the search tree in in state-space heuristic search. The definitions are as following. <em>Penetrance</em> $P$ is defined by</p>\n\n<p>$\\qquad \\displaystyle P = \\frac{L}{T}$</p>\n\n<p>and <em>branching factor</em> $B$ is defined by</p>\n\n<p>$\\qquad \\displaystyle \\frac{B}{(B-1)} \\cdot (B^L \u2013 1) = T$ </p>\n\n<p>where $L$ is the length of the path from the root to the solution and $T$ the total number of nodes expanded.</p>\n', 'Tags': '<artificial-intelligence><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-26T03:14:19.410', 'CommentCount': '0', 'AcceptedAnswerId': '2964', 'CreationDate': '2012-07-31T12:54:12.277', 'Id': '2961''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>By <a href="http://math.stackexchange.com/questions/112734/in-what-sense-is-uniform-cost-search-uniform">this</a> possibly good attempt to explain this, the "uniformity" in the Uniform cost search <em>is actually the uniformity of the heuristic function</em>.</p>\n\n<p>Is this explanation correct ? If yes, then don\'t all <code>un-informed</code> cost searches (like <code>BFS</code>, <code>DFS</code>, etc) have no heurstic and thus, be called as "<code>uniform cost</code> searches" ?</p>\n', 'ViewCount': '265', 'Title': 'Why is Uniform cost search called "uniform" cost search?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-15T06:30:49.180', 'LastEditDate': '2012-10-14T21:24:18.123', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6084', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3032', 'Tags': '<terminology><artificial-intelligence><search-algorithms><heuristics>', 'CreationDate': '2012-10-14T19:43:54.853', 'Id': '6072''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am studying <a href="http://en.wikipedia.org/wiki/Best-first_search" rel="nofollow">best first search</a> as it compares to BFS (breadth-first search) and DFS (depth-first search), but I don\'t know when BFS is better than best-first search. So, my question is </p>\n\n<blockquote>\n  <p>When would best-first search be worse than breadth-first search?</p>\n</blockquote>\n\n<p>This question is one of the back exercises in <em>Artificial Intelligence</em> by Rich &amp; Knight, and it asks for an answer in terms of time &amp; space complexity and allows you to define any heuristic function.</p>\n', 'ViewCount': '2151', 'Title': 'When would best first search be worse than breadth first search?', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-04T08:12:23.923', 'LastEditDate': '2012-11-04T06:23:27.297', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '6461', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1267', 'Tags': '<graphs><artificial-intelligence><search-algorithms>', 'CreationDate': '2012-10-17T15:52:14.500', 'Id': '6125''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '600', 'Title': 'Iterative binary search analysis', 'LastEditDate': '2012-11-04T15:41:46.187', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Hasan Tahsin', 'PostTypeId': '1', 'OwnerUserId': '4610', 'Body': '<p>I\'m a little bit confused about the analysis of <a href="http://en.wikipedia.org/wiki/Binary_search" rel="nofollow">binary search</a>.\nIn almost every paper, the writer assumes that the array size $n$ is always $2^k$.\nWell I truly understand that the time complexity becomes $\\log(n)$ (worst case) under this assumption. But what if $n \\neq 2^k$?</p>\n\n<p>For example if $n=24$, then we have\n5 iterations for 24<br>\n4 i. for 12<br>\n3 i. for 6<br>\n2 i. for 3<br>\n1 i. for 1</p>\n\n<p>So how do we get the result $k=\\log n$ in this example (I mean of course every similar example whereby $n\\neq2^k$)?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-04T15:41:46.187', 'CommentCount': '4', 'AcceptedAnswerId': '6471', 'CreationDate': '2012-11-03T10:15:40.587', 'Id': '6470''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p><a href="http://academia.stackexchange.com/questions/5079/are-there-hacks-or-smaller-scientific-search-engines-offering-you-context-sensit">I was asking on academia.se</a>, if anyone knows scientific search engines offering a <a href="http://en.wikipedia.org/wiki/Proximity_search_%28text%29" rel="nofollow">proximity operator</a> like Google Web Search does, while Google Scholar Search does not. That\'s sad, because this operator would be most useful for literature research offering you a nearly semantic/context-sensitive search and I\'ve seen requests for this feature in many blogs.</p>\n\n<p>The answer on my question linked above shows that 2 search engines offer something similar, but those operators also only work on titles and abstracts of papers, if I understand correctly. </p>\n\n<p>The wikipedia article doesn\'t explain what exactly limits the implementation of this kind of operator (exponentially rising indexing time, index size,... I\'m no search algorithm expert), but when Google Web Search offers it (the amount of web-text is much bigger), what possibly hinders the scientific search engines from offering it for full article text (cost-benefit ratio? I doubt this, as 99,9% of Google Web Search user don\'t know the AROUND(X) operator and the majority doesn\'t use <a href="http://www.googleguide.com/advanced_operators_reference.html" rel="nofollow">many operators</a> at all)?</p>\n\n<p>PS: If this question better fits SO, move it there, but I\'m more looking for a general explanation, what parameters determine and limit the implementation of an proximity operator.</p>\n', 'ViewCount': '96', 'Title': 'What limits the implementation of proximity operators for text indexing and searching?', 'LastEditorUserId': '298', 'LastActivityDate': '2012-11-10T15:30:49.783', 'LastEditDate': '2012-11-10T15:30:49.783', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '298', 'Tags': '<search-algorithms><data-mining><searching>', 'CreationDate': '2012-11-04T19:35:42.203', 'Id': '6477''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Looking for some tutorials / references that discuss Breadth First Search that takes into consideration the cost of paths, but could not find much information.</p>\n\n<p>Could someone refer a tutorial?</p>\n', 'ViewCount': '292', 'Title': 'Breadth First Search with cost', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-07T08:02:23.767', 'LastEditDate': '2012-11-06T14:28:43.053', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '6523', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4476', 'Tags': '<algorithms><reference-request><graphs><search-algorithms>', 'CreationDate': '2012-11-06T14:22:03.630', 'Id': '6514''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '175', 'Title': 'Smallest string length to contain all types of beads', 'LastEditDate': '2012-12-10T13:39:24.903', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4751', 'FavoriteCount': '1', 'Body': '<p>I read this question somewhere, and could not come up with an efficient answer.</p>\n\n<p>A string of some length has beads fixed on it at some given arbitrary distances from each other. There are $k$ different types of beads and $n$ beads in total on the string, and each bead is present atleast once. We need to find one consecutive section of the string, such that:</p>\n\n<ul>\n<li>that section contains all of the $k$ different types of beads atleast once.</li>\n<li>the length of this section is as small as possible, provided the first condition is met.</li>\n</ul>\n\n<p>We are given the positions of each bead on the string, or alternatively, the distances between each pair of consecutive beads.</p>\n\n<p>Of course, a simple brute force method would be to start from every bead (assume that the section starts from this bead), and go on till atleast on instance of all beads are found while keeping track of the length. Repeat for every starting position, and find the minimum among them. This will give a $O(n^2)$ solution, where $n$ is the number of beads on the string. I think a dynamic programming approach would also probably be $O(n^2)$, but I may be wrong. Is there a faster algorithm? Space complexity has to be sub-quadratic. Thanks!</p>\n\n<p>Edit: $k$ can be $O(n)$.</p>\n', 'Tags': '<algorithms><dynamic-programming><search-algorithms>', 'LastEditorUserId': '4751', 'LastActivityDate': '2012-12-10T15:19:21.183', 'CommentCount': '0', 'AcceptedAnswerId': '7278', 'CreationDate': '2012-12-09T20:04:48.587', 'Id': '7276''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>Consider a directed graph $G$ on which one can dynamically add edges and make some specific queries.</p>\n\n<h3>Example: disjoint-set forest</h3>\n\n<p>Consider the following set of queries:</p>\n\n<pre><code>arrow(u, v)\nequiv(u, v)\nfind(u)\n</code></pre>\n\n<p>the first one adds an arrow $u\u2192v$ to the graph, the second one decides if $u\u2194^*v$, the last one finds a canonical representative of the equivalence class of $\u2194^*$, i.e. a $r(u)$ such that $u\u2194^*v$ implies $r(v)=r(u)$.</p>\n\n<p>There is a <a href="http://en.wikipedia.org/wiki/Disjoint-set_data_structure#Disjoint-set_forests">well-known algorithm using the disjoint-set forest data structure</a> implementing these queries in quasi-constant amortized complexity, namely $O(\u03b1(n))$. Note that in this case <code>equiv</code> is implemented using <code>find</code>.</p>\n\n<h3>More complex variant</h3>\n\n<p>Now I\'m interested in a more complex problem where the directions do matter:</p>\n\n<pre><code>arrow(u, v)\nconfl(u, v)\nfind(u)\n</code></pre>\n\n<p>the first adds an arrow $u\u2192v$, the seconds decides if there is a node $w$ reachable from both $u$ and $v$, i.e. $u\u2192^*\u2190^*v$. The last one should return an object $r(u)$ such that $u\u2192^*\u2190^*v$ implies $r(u) \\bullet r(v)$ where $\\bullet$ should be easily computable. (In order to, say, computes <code>confl</code>). The goal is to find a good data structure such that these operations are fast.</p>\n\n<h3>Cycles</h3>\n\n<p>The graph can contain cycles.</p>\n\n<p>I don\'t know if there is a way to efficiently <em>and incrementally</em> compute the strongly connected components, in order to only consider DAGs for the main problem.</p>\n\n<p>Of course I would appreciate a solution for DAGs, too. It would correspond to an incremental computation of the least common ancestor.</p>\n\n<h3>Naive approach</h3>\n\n<p>The disjoint-set forest data structure is not helpful here, since it disregards the direction of the edges. Note that $r(u)$ cannot be a single node, in the case the graph is not confluent.</p>\n\n<p>One can define $r(u)=\\{v \u2223 u\u2192^*v\\}$ and to define $\\bullet$ as $S_1\\bullet S_2$ when $S_1 \u2229 S_2\u2260\u2205$. But how to compute this incrementally?</p>\n\n<p>Probably that computing such a big set is not useful, a smaller set should be more interesting, as in the usual union-find algorithm.</p>\n', 'ViewCount': '401', 'Title': 'Directed union-find', 'LastEditorUserId': '82', 'LastActivityDate': '2013-01-15T11:49:18.190', 'LastEditDate': '2012-12-15T13:34:20.250', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '82', 'Tags': '<algorithms><graphs><search-algorithms>', 'CreationDate': '2012-12-12T17:52:23.180', 'FavoriteCount': '1', 'Id': '7360''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a tree of foods, which I\'d like to search to find relevant nodes. What\'s the best way to rank the results?</p>\n\n<p>Here\'s an example sub-set after a search for "apple":</p>\n\n<pre><code>|-- apple pie\n|-+ apples\n| |-- cooking\n| |-+ eating\n|   |-- average\n|   |-- Granny Smith\n|   |-- Golden Delicious\n|-- pork and apple casserole\n</code></pre>\n\n<p>Currently, I\'m ranking based on which results come earliest in the full name of each leaf, so a result set would look like:</p>\n\n<pre><code>0 Apple pie\n0 Apples, cooking\n0 Apples, eating, average\n0 Apples, eating, Granny Smith\n0 Apples, eating, Golden Delicious\n2 Pork and apple casserole\n</code></pre>\n\n<p>Where the rank is the index of the first token match, lower the rank the better.</p>\n\n<p>I\'d like to aggregate the matches, so that I don\'t display the full sub-tree on the initial search, for example:</p>\n\n<pre><code>Apples... (4)\nApple pie\nPork and apple casserole\n</code></pre>\n\n<p>The obvious way to rank these is to count the number of matching leaves, then the higher the rank, the better the match.</p>\n\n<p>But, I\'m not sure how to combine these rankings, since one is larger-is-better and one is smaller-is-better. Are there standard ways to combine rankings like this? (I\'m not sure what to search for, so Google is giving me results about search engine optimisation, and searching web pages, which doesn\'t seem to apply.)</p>\n', 'ViewCount': '46', 'Title': 'How to rank search results of short string lists', 'LastActivityDate': '2013-01-09T04:35:05.177', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7845', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5348', 'Tags': '<search-algorithms>', 'CreationDate': '2013-01-08T23:16:46.040', 'Id': '7842''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose that we take an initial search problem and we add $c &gt; 0$ to the costs on all edges. Will <a href="http://en.wikipedia.org/wiki/Uniform-cost_search" rel="nofollow">uniform-cost search</a> return the same answer as in the initial search problem?</p>\n\n<p>Definitions: Uniform-cost search is also known as lowest cost first. Initial search problem can be any graph with a start and a goal state. You just apply the uniform cost search algorithm on the graph. </p>\n', 'ViewCount': '426', 'Title': 'Uniform-cost Search Problem', 'LastEditorUserId': '867', 'LastActivityDate': '2013-11-19T07:11:47.707', 'LastEditDate': '2013-01-29T03:40:50.913', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6601', 'Tags': '<graphs><search-algorithms><search-trees><search-problem>', 'CreationDate': '2013-01-29T02:33:45.660', 'Id': '9265''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Is it true or false that for running a dfs on an undirected graph G with a simple cycle than this cycle will have exactly one back edge?</p>\n\n<p>Looks to me likes its true ,is it?</p>\n', 'ViewCount': '122', 'Title': 'Dfs algorithm and cycles question', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-04T12:13:14.260', 'LastEditDate': '2013-02-04T12:13:14.260', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'Nusha', 'PostTypeId': '1', 'Tags': '<algorithms><graphs><search-algorithms><graph-traversal>', 'CreationDate': '2013-02-03T11:26:50.813', 'Id': '9458''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '641', 'Title': 'The purpose of grey node in graph depth-first search', 'LastEditDate': '2013-02-11T17:33:21.087', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6805', 'FavoriteCount': '1', 'Body': '<p>In many implementations of depth-first search that I saw (for example: <a href="http://ramos.elo.utfsm.cl/~lsb/elo320/aplicaciones/aplicaciones/CS460AlgorithmsandComplexity/lecture9/COMP460%20Algorithms%20and%20Complexity%20Lecture%209.htm" rel="nofollow">here</a>), the code distinguish between a grey vertex (discovered, but not all of its neighbours was visited) and a black vertex (discovered and all its neighbours was visited). What is the purpose of this distinction? It seems that DFS algorithm will never visit a visited vertex regardless of whether it\'s grey or black.</p>\n', 'Tags': '<algorithms><graphs><search-algorithms><graph-traversal>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-12T00:43:33.593', 'CommentCount': '0', 'AcceptedAnswerId': '9681', 'CreationDate': '2013-02-11T13:10:44.110', 'Id': '9676''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Given an array of size n that holds ones and zeros I need to find an index of a  <code>1</code> cell that has <code>0</code> to his right (in then next cell) there could be more than one pair in a given array, any one of them is fine. The array is not sorted, but we do know that the first element is <code>1</code> and the last element is <code>0</code>.</p>\n\n<p>The search should be in $O(\\log n)$ time. I'm thinking that a binary search variation is the answer but I'm not sure how. </p>\n", 'ViewCount': '60', 'Title': "Finding a '1' cell with a '0' to its right in a binary array", 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-20T18:15:12.737', 'LastEditDate': '2013-02-20T18:15:12.737', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9973', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6963', 'Tags': '<algorithms><arrays><search-algorithms><binary-search>', 'CreationDate': '2013-02-20T16:04:03.810', 'Id': '9969''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have some questions regarding tree search and graph search (Uninformed search) as explained in chapter 3 of the book : <a href="http://aima.cs.berkeley.edu/" rel="nofollow">http://aima.cs.berkeley.edu/</a></p>\n\n<p>As I see, the only difference between the two is that the graph search handles loops (avoids them).</p>\n\n<p>First question: Do both graph search and tree search build dynamic trees of the problem at hand?</p>\n\n<p>Second question: I assume graph search was used to solve the map of Romania problem (getting from Arad to Bucharest) with DFS, BFS, UCS as strategies that only sort the frontier queue. Now is there a standard way to change the graph of map of Romania to a tree, and then use tree search? </p>\n\n<p>Third question: What are some of the Criteria that help us choose between graph and tree search for different problems?</p>\n\n<p>Thanks you in advance</p>\n', 'ViewCount': '85', 'Title': 'Using tree search', 'LastActivityDate': '2013-02-20T20:53:51.773', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9990', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6966', 'Tags': '<artificial-intelligence><search-algorithms>', 'CreationDate': '2013-02-20T18:53:46.650', 'Id': '9979''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1157', 'Title': 'Is search a binary heap operation?', 'LastEditDate': '2013-02-24T16:24:27.193', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'Barry Fruitman', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Body': '<p>According to the <a href="http://en.wikipedia.org/wiki/Binary_heap" rel="nofollow">Wikipedia page</a>, search is "not an operation" on binary heaps (see complexity box at top-right).</p>\n\n<p>Why not? Binary heaps may not be sorted, but they are ordered, and a full graph traversal can find any object in $O(n)$ time, no?</p>\n\n<p>Is the page wrong or am I?</p>\n', 'Tags': '<data-structures><search-algorithms><heaps>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:24:27.193', 'CommentCount': '1', 'AcceptedAnswerId': '10052', 'CreationDate': '2013-02-24T07:37:18.370', 'Id': '10049''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>What is Harrison hashing and what are its applications in web searching? \nCan some one give me some relevant information?    </p>\n\n<p><strong>Update:</strong></p>\n\n<p>I found it <a href="http://www.cs.manchester.ac.uk/ugt/COMP26120/" rel="nofollow">here</a> , and is a part of M.Tech syllabus of a friend of mine. I need to explain him this concept and then see how it can be applied in web applications.\nMany thanks for spending your precious time.    </p>\n', 'ViewCount': '207', 'Title': 'What is Harrison hashing, its applications in web search engines?', 'LastEditorUserId': '6466', 'LastActivityDate': '2013-10-15T01:10:32.837', 'LastEditDate': '2013-02-27T06:34:22.677', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10126', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><strings><search-algorithms><hash>', 'CreationDate': '2013-02-26T15:01:05.643', 'Id': '10121''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '233', 'Title': 'Why can the state space of the 15 puzzle be divided into two separate parts?', 'LastEditDate': '2013-02-27T02:30:03.507', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2644', 'FavoriteCount': '1', 'Body': '<p>I am trying to understand the <a href="http://www.cut-the-knot.com/pythagoras/fifteen.shtml">proof here</a> of why the state space in 15 puzzle is divided into two separate parts, but the explanation is complicated for me.</p>\n\n<p>Could someone please explain it in simpler terms? I have been struggling with this for days :(</p>\n', 'Tags': '<artificial-intelligence><search-algorithms>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-27T04:32:32.970', 'CommentCount': '3', 'AcceptedAnswerId': '10133', 'CreationDate': '2013-02-26T23:04:18.613', 'Id': '10130''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose I have a admissible and consistent heuristic.</p>\n\n<p>Is it true, that when I expand a node, I have guaranteed that the path I found to this node is optimal?</p>\n\n<p>Look at this pseudocode from wikipedia:</p>\n\n<pre><code>function A*(start,goal)\n closedset := the empty set    // The set of nodes already evaluated.\n openset := {start}    // The set of tentative nodes to be evaluated, initially containing the start node\n came_from := the empty map    // The map of navigated nodes.\n\n g_score[start] := 0    // Cost from start along best known path.\n // Estimated total cost from start to goal through y.\n f_score[start] := g_score[start] + heuristic_cost_estimate(start, goal)\n\n while openset is not empty\n     current := the node in openset having the lowest f_score[] value\n     if current = goal\n         return reconstruct_path(came_from, goal)\n\n     remove current from openset\n     add current to closedset\n     for each neighbor in neighbor_nodes(current)\n         tentative_g_score := g_score[current] + dist_between(current,neighbor)\n         if neighbor in closedset\n             if tentative_g_score &gt;= g_score[neighbor]\n                 continue\n\n         if neighbor not in openset or tentative_g_score &lt; g_score[neighbor] \n             came_from[neighbor] := current\n             g_score[neighbor] := tentative_g_score\n             f_score[neighbor] := g_score[neighbor] + heuristic_cost_estimate(neighbor, goal)\n             if neighbor not in openset\n                 add neighbor to openset\n\n return failure\n</code></pre>\n\n<p>I suppose it should be true. Because of this:</p>\n\n<pre><code>if current = goal\n     return reconstruct_path(came_from, goal)\n</code></pre>\n\n<p>If it wasn't true then this test would not guarantee me that the solution is optimal right?</p>\n\n<p>What I don't get and the reason I am asking this question is this:</p>\n\n<pre><code>if neighbor in closedset\n         if tentative_g_score &gt;= g_score[neighbor]\n             continue\n</code></pre>\n\n<p>If the neighbor is in closed list, it means that it has already been expanded. Why are they testing the scores then? Why would not the next condition work?</p>\n\n<pre><code>if neighbor in closedset\n         continue\n</code></pre>\n", 'ViewCount': '97', 'Title': 'A* optimality of the expanded node', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-29T23:28:22.880', 'LastEditDate': '2013-04-02T07:37:43.930', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '16559', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7075', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><correctness-proof>', 'CreationDate': '2013-02-28T15:36:33.660', 'Id': '10152''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am reading chapter 32 - String Matching from the book <a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">"Introduction to Algorithms" 3rd edition Cormen et al</a>. The Rabin-Karp Algorithm is not clear to me despite heaving read it several times. Specifically, when the authors start proposing about some modulo operations for solving some problem, I am unable to follow. To be precise, the paragraphs from</p>\n\n<blockquote>\n  <p>We have intentionally overlooked one problem: p and ts may be too\n  large too work with conveniently...</p>\n</blockquote>\n\n<p>on till equation 32.2 (pp 991 in 3rd edition) are <a href="http://i.stack.imgur.com/d5ptz.jpg" rel="nofollow">the paragraphs I am having problems with</a>.</p>\n\n<p>What does "convenient" mean here? Why do they use modulo? I am totally lost.\nIs it possible to explain this in simple english?</p>\n\n<p>I will appreciate if someone can help me in understanding this paragraph. And if there is any other reference (with similar mathematical treatment) where this can be read, please tell me.</p>\n', 'ViewCount': '199', 'Title': "Problem with Cormen's treatment of the Rabin-Karp algorithm", 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:08:04.267', 'LastEditDate': '2013-04-07T12:08:04.267', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><search-algorithms><strings>', 'CreationDate': '2013-03-01T12:51:39.397', 'Id': '10173''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I asked a similar question <a href="http://cs.stackexchange.com/questions/10173/rabin-karp-searching-algorithm">here</a> on Rabin Karp algorithm. My present question is, how do we find the best $q$ (i.e modulus)? What is the criterion? We need to choose a $q$ which will be quick to calculate and also must result in lesser number of spurious hits, right? </p>\n\n<p>Wow do we ensure these things?</p>\n', 'ViewCount': '499', 'Title': 'How do we find the optimal modulus q in Rabin-Karp algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:09:29.210', 'LastEditDate': '2013-04-07T12:09:29.210', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><search-algorithms><strings>', 'CreationDate': '2013-03-01T14:34:49.860', 'Id': '10174''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I asked a question on Rabin-Karp Searching algorithm <a href="http://cs.stackexchange.com/questions/10173/rabin-karp-searching-algorithm">here</a>, which I am reading from the book "<a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">Introduction to Algorithms" 3rd edition Cormen et al.</a>. </p>\n\n<p>After reading few para of the section on Rabin-Karp, I got some more confusions:</p>\n\n<p>In the third paragraph the authors say that the if we could find <strong>p</strong>  (decimal value of pattern P[1....m] )  in  time O(m) <em>and all the <strong>ts</strong> values</em> (i.e decimal value of length-m sub-string T[s+1....s+m], s=0,1,2,,,,n-m) in time O(n-m+1),  then we could determine all valid shifts s in time O(m) + O(n-m+1) by comparing <strong>p</strong> with each of the <strong>ts</strong> values. </p>\n\n<p>How is this possible? O(m) is for finding p, O(n-m+1) is for finding all ts, so total pre-processing time so far is O(m) + O(n-m+1). This is the total pre-processing time; the comparison has yet to start, I have to spend some extra $ for doing comparison of a decimal p with each of the (n-m+1)-ts values. </p>\n\n<p>1-Then why the authors say in the first para that the pre-processing time is O(m)? Why it is not O(m) + O(n-m+1) which include processing time of p and all ts values? </p>\n\n<p>2- Now if we talk about worst case matching time, what should be that? So in the worst my decimal number p (already calculated ) will be compared with <em>each</em> of the another (m-n+1) decimal numbers, which are the values of ts (already calculated, no extra cash needed for doing this job now ). The worst case is when I am most unlucky and I have to compare every value of ts with p. Right? </p>\n\n<p>Based on my understanding,(if I am right) the worst case matching time should be O(m-n+1) and not O((m-n+1)m) as claimed by the authors in the first para. For example let us say my Pattern is P[1...m]=226 and Text is T[1....n]=224225226. so  my p is decimal 226, and ts is decimal value of T[s+1, s+2, s+3], for s=0,1,2...6 as n=9, and m=3. The ts values will be as follows: </p>\n\n<blockquote>\n  <p>s=0 => T[224]=> ts=224     </p>\n  \n  <p>s=1 => T[242]=> ts=242 </p>\n  \n  <p>s=2 => T[422]=> ts=422 </p>\n  \n  <p>s=3 => T[225]=> ts=225 </p>\n  \n  <p>s=4 => T[252]=> ts=252 </p>\n  \n  <p>s=5 => T[522]=> ts=522 </p>\n  \n  <p>s=6 => T[226]=> ts=226</p>\n</blockquote>\n\n<p>Now you will be comparing p=226 with all these values. So are you not making n-m+1=7 comparisons to achieve search for 226 in T, and not (n-m+1)m =7 x3=21? So the worst case time should be O(n-m+1) and not O((n-m+1)m). </p>\n\n<p>In short I understand that:</p>\n\n<blockquote>\n  <p>Total pre-processing time = O(m) + O(n-m+1) (including for both p and\n  all the ts values)</p>\n  \n  <p>Total matching time in worst case = O(n-m+1)</p>\n</blockquote>\n\n<p>Where I am making mistake? </p>\n', 'ViewCount': '877', 'Title': 'Time Complexity of Rabin-Karp matching algorithm', 'LastEditorUserId': '6466', 'LastActivityDate': '2013-03-07T13:53:20.220', 'LastEditDate': '2013-03-04T12:43:50.803', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><complexity-theory><time-complexity><search-algorithms>', 'CreationDate': '2013-03-04T12:37:40.083', 'FavoriteCount': '0', 'Id': '10258''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have some questions regarding local search / optimization as explained in chapter 4 of the book : <a href="http://aima.cs.berkeley.edu/" rel="nofollow">http://aima.cs.berkeley.edu/</a></p>\n\n<p>In classical search (Chap 3), the search starts from an initial node, then the search continues based on strategies of BFS, DFS, etc. What I am unsure of is the process of local search (Chap 4). </p>\n\n<ol>\n<li><p>Does the local search algorithm start from one node in state space? Check if constraints are satisfied? If Yes, this is goal? If not, move to neighbours?</p></li>\n<li><p>Is the entire state space considered goal state? Even nodes that don\'t satisfy the constraints?</p></li>\n<li><p>In optimization, the search is conducted in a part of search space where all constraints are met, but tries to find a better solution. In that case, what if the search algorithm moves to nodes where they don\'t satisfy the constraints?</p></li>\n</ol>\n', 'ViewCount': '178', 'Title': 'Local Search vs Classical Search', 'LastEditorUserId': '472', 'LastActivityDate': '2013-03-17T20:37:57.327', 'LastEditDate': '2013-03-17T20:37:57.327', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '10499', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6966', 'Tags': '<artificial-intelligence><search-algorithms>', 'CreationDate': '2013-03-12T17:09:53.467', 'Id': '10491''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am looking for an example of the "iterative lengthening search". I have searched and I was only able to find definitions like</p>\n\n<blockquote>\n  <p>iterative lengthening search an iterative analog to uniform cost\n  search. The idea is to use increasing limits on path cost. If a node\n  is generated whose path cost exceeds the current limit, it is\n  immediately discarded. For each new iteration, the limit is set to the\n  lowest path cost of any node discarded in the previous iteration</p>\n</blockquote>\n\n<p>I\'d be deeply grateful for an example, that enables me to understand.</p>\n', 'ViewCount': '318', 'Title': 'iterative lengthening search example', 'LastActivityDate': '2014-02-03T08:04:45.670', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5052', 'Tags': '<artificial-intelligence><search-algorithms>', 'CreationDate': '2013-03-13T02:25:48.830', 'Id': '10504''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>E.g.</p>\n\n<p><strong>Matching Problem</strong> The DFA of regex <code>good</code> is like a chain.</p>\n\n<pre><code>match:     "good"\nnot match: "people do not give good comments are not good people"\n</code></pre>\n\n<p><strong>Searching Problem</strong> The DFA used in searching regex <code>good</code> could be:</p>\n\n<p><img src="http://i.stack.imgur.com/BcnoQ.jpg" alt="enter image description here"></p>\n\n<pre><code>1 matching:  "good"\n2 matchings: "people do not give good comments are not good people"\n</code></pre>\n\n<p>Here are the questions:</p>\n\n<ol>\n<li>In the above two problems, it seems the searching problem\'s DFA is the matching problem\'s DFA plus some backedges and an additional state (here state <code>0</code>). Is this the difference in general?</li>\n<li>What is the regex of searching problem in this case?</li>\n</ol>\n', 'ViewCount': '139', 'Title': 'Difference between pattern matching and pattern searching in terms of DFA/Regex', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-20T18:51:02.610', 'LastEditDate': '2013-03-20T11:08:46.557', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'JackWM', 'PostTypeId': '1', 'Tags': '<finite-automata><regular-expressions><search-algorithms>', 'CreationDate': '2013-03-19T00:16:58.863', 'Id': '10632''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I need help with the following problem:</p>\n\n<blockquote>\n  <p><strong>Input:</strong>  An undirected, unweighted graph $G = (V,E)$ and a set of vertices $F \\subseteq V$.</p>\n  \n  <p><strong>Question:</strong>\n  Find a vertex $v$ of $V$ such that the distance from each vertex of $F$ to $v$ is the same and all the distances are minimized?  Return <code>None</code> if there is no such $v$.</p>\n</blockquote>\n\n<p>The runtime should be $O(|V| + |E|)$.</p>\n\n<p>My thoughts were to do a breadth-first search for each vertex in $F$, so for each vertex in $F$, you store all vertices with their distances, then find the intersection of all these.</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '234', 'Title': 'Find a vertex that is equidistant to a set of vertices?', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-03-31T10:13:00.143', 'LastEditDate': '2013-03-31T10:13:00.143', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><graph-theory><search-algorithms><shortest-path>', 'CreationDate': '2013-03-26T05:51:11.470', 'Id': '10794''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In the original paper of A* algorithm, <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=a%20formal%20basis%20for%20the%20heuristic%20determination%20of%20minimum%20cost%20paths&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDYQFjAA&amp;url=http://fai.cs.uni-saarland.de/teaching/winter12-13/heuristic-search-material/Astar.pdf&amp;ei=m-NRUeOUMYa0kAXpgIHIDw&amp;usg=AFQjCNFp9yfCwE0_B_epYI4kPmEmZjOGww&amp;bvm=bv.44342787,d.dGI" rel="nofollow"><em>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</em></a>, the author proved the optimality of A* in <em>Theorem 2</em>, page 105.</p>\n\n<p>However, I cannot understand the proof. The assumption is that we have a node in $G_s$ which is not expanded by algorithm A, but in the proof we change the graph to $G_{n,\\theta}$, isn\'t it a problem?</p>\n', 'ViewCount': '96', 'Title': 'While proving optimality of the A* algorithm, why can we change graphs?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-27T13:07:35.247', 'LastEditDate': '2013-03-27T12:03:09.513', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7432', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><correctness-proof>', 'CreationDate': '2013-03-27T00:57:43.747', 'Id': '10817''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Counting all possible paths, or all possible paths with a given length, between a couple of nodes in a directed or undirected graph is a classical problem. Attention should be given to what <em>all</em> means, due to the possibles cycles.</p>\n\n<p>This question is slightly different, or at least I think.</p>\n\n<p><strong>INPUT:</strong>  Be <strong>G</strong> a directed graph. <strong>G</strong> can have cycles and also selfconnected nodes. Let <strong>A(G)</strong> be the adjacency matrix of <strong>G</strong> (with a 1 in <strong>G</strong><sub>i,j</sub> if there's a link going from i to j and a 0 otherwise). Define <strong>T</strong> and <strong>B</strong> two subset of nodes of <strong>G</strong>, possibly with void intersection.</p>\n\n<p><strong>OUTCOME:</strong> A list of all paths of length <em>at most</em> k going from one node in <strong>T</strong> to one node in <strong>B</strong>. Paths can contain multiple time the same edges, as long as they go from the source node to the target node in strictly less than k+1 steps.</p>\n\n<p><strong>QUESTION:</strong> I would like to know which algorithm perform best in this task. I'm trying to develop a possible answer based on the fact that the n-th power of the adjacency matrix, if computed symbolically (with a different variable for each entry instead of a 1), keep traks of all this paths (and it reduces to the counting of paths if computed numerically with 1 in the entries). But I really don't know if this is the fastest way of doing the task (probably not).</p>\n\n<p><strong>CAVEAT:</strong> I'm not asking for the counting problem, nor for the shortest paths, the length of a path is defined as the number of edges used (counting the repetition). I'm using R, but if you prefer think about it in any other language! I'm really sorry if the question was already posed and solved. Thank you for the kind help!</p>\n\n<p><strong>additional info:</strong> I tried a matrix power series approach (A^3 gives all the 3 long path, ...) and dfs / bfs. I think the latter two are far from optimality as they don' take into account that I'm working on sets of sources and targets, and hence do a lot of redundant work...</p>\n", 'ViewCount': '793', 'Title': 'All paths of less than a given length in a directed graph between couple of nodes', 'LastActivityDate': '2013-04-01T16:54:58.877', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '10954', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7521', 'Tags': '<algorithms><graphs><search-algorithms><shortest-path>', 'CreationDate': '2013-03-31T21:50:11.637', 'FavoriteCount': '1', 'Id': '10949''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was wondering since randomness is embedded in genetic algorithms at almost every level, is there a really fine line between genetic algorithms and pure random search?</p>\n\n<p>Ever since I finished my implementation of a GA , since randomness is present in the mutation function,the initialization part (as well as the reinitialization part) and crossbreeding part as well... other than a encoder which tries to sense of the chromsomes (encoder tailored to make sense of the chromosome in context of the problem) and a fitness function , it feels like genetic algorithms are just random search functions in disguise .</p>\n\n<p>So my question is : are GA implementations just plain old random searches with a shot of memory to make it look like there is some sort of meaningful feedback? </p>\n', 'ViewCount': '108', 'ClosedDate': '2013-04-02T22:15:50.887', 'Title': 'Are genetic algorithms special instances of random search done in an unexpectedly short run-time?', 'LastEditorUserId': '7545', 'LastActivityDate': '2013-04-02T21:07:18.593', 'LastEditDate': '2013-04-02T20:35:30.713', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7545', 'Tags': '<search-algorithms><efficiency><randomness><evolutionary-computing><genetic-algorithms>', 'CreationDate': '2013-04-02T15:57:17.913', 'Id': '10975''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am running some experiments with a maze, and trying different variations of A*. Based on my experiments, I have been able to form some opinion (that at least in those cases, graph checking is better than IDA). </p>\n\n<p>I am looking for online articles that have done similar experiments, comparing variations of A* with respect to expanded nodes, but have not come across anything concrete.</p>\n', 'ViewCount': '81', 'Title': 'Comparing variations of A*', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:52:54.573', 'LastEditDate': '2013-04-08T14:52:54.573', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '11136', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6966', 'Tags': '<algorithms><reference-request><artificial-intelligence><search-algorithms><empirical-research>', 'CreationDate': '2013-04-05T22:21:35.823', 'Id': '11067''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I want an algorithm that calculates which element, among two, appears more often than the other in a sorted array. The array will have only two types of elements. </p>\n\n<p>Example : $aaaaaabbb$ </p>\n\n<p>Here $a&gt;b$. </p>\n\n<p>I have to find an constant time algorithm. Is it possible? The only thing I could come up with was using stack. Push all $a$'s and pop them with $b$. But it takes $O(n)$ operations. Any better approaches? Need a hint (no solution).</p>\n", 'ViewCount': '44', 'Title': 'Finding the element that occurs more often than the other', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-12T23:40:12.290', 'LastEditDate': '2013-04-12T23:40:12.290', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '11268', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithms><data-structures><search-algorithms><arrays>', 'CreationDate': '2013-04-12T15:30:57.237', 'Id': '11266''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>In database query processing, the approximate time for selection operation using primary index when equality is on key is $2(b_s + b_t)$ where $b_s$ is disk seek time and $b_t$ is disk transfer time (assuming one level of indexing), because one seek and transfer time will be needed for finding the index and another one will be for the actual data.  </p>\n\n<p>But what will happen if the equality is on a no- key value? Since now we cannot search in the index, don't we have to do a linear search?</p>\n", 'ViewCount': '52', 'Title': 'Approximate time for selection operation using index when equality is on nonkey', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-15T15:11:13.230', 'LastEditDate': '2013-04-14T11:22:36.033', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11326', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '778', 'Tags': '<runtime-analysis><search-algorithms><databases>', 'CreationDate': '2013-04-14T06:08:19.807', 'Id': '11301''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Been stuck on this for a while, would really appreciate some help:</p>\n\n<blockquote>\n  <p>Suppose you are given an array A[1...n] of sorted integers that has been circularly shifted k positions to the right. For example, [35,42,5,15,27,29] is a sorted array that has been circularly shifted k = 2 positions, while  [27,29,35,42,5,15] has been shifted k = 4 positions. Give an algorithm for finding the maximum element in A that runs in O(log n) time.</p>\n</blockquote>\n\n<p>The elements in A are distinct.</p>\n\n<hr>\n\n<p>I understand that to achieve O(log n) time I'll probably have to search through the list by starting at the middle, and then going left or right, then splitting the list in half over and over, but I'm not sure how to attack it beyond that.</p>\n", 'ViewCount': '1137', 'Title': 'Find maximum element in sorted arrays in logarithmic time', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T19:54:26.320', 'LastEditDate': '2013-04-25T07:38:48.193', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '11547', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7881', 'Tags': '<algorithms><search-algorithms><arrays>', 'CreationDate': '2013-04-25T00:01:42.933', 'Id': '11545''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Starting to use <a href="https://code.google.com/p/nanoflann/" rel="nofollow">nanoflann</a> to do some point cloud nearest neighbor searching and it got me thinking about just how "approximate" ANN methods are.</p>\n\n<p>If I have a (more or less) randomly distributed point cloud what is the likelihood that I get the exact nearest neighbor given a target point within the clouds bounding box?  I know that it is dataset dependent... but does anyone have a good numerical study somewhere that shows trends?</p>\n', 'ViewCount': '94', 'Title': 'How approximate are "approximate" nearest neighbor (ANN) search algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T17:44:30.770', 'LastEditDate': '2013-05-28T06:46:45.047', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8395', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><approximation><heuristics>', 'CreationDate': '2013-05-27T20:13:40.783', 'FavoriteCount': '1', 'Id': '12310''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'ve been working on this graph and just completely botching it. I mean to say that my solution may be the worst possible other than if a monkey had thrown darts at the graph to decide the next path.  Anyhow, I\'m lost and really trying to get a grasp of where my conclusion and the proper conclusion diverged dramatically.</p>\n\n<p>I wanted to perform a DFS, show discovery/finish times, the DF forest, and edge classifications. I assumed that: 1) The vertices are listed in alphabetical order in each adjacency list. 2) The vertices are taken in the alphabetical order in the main loop of the DFS algorithm.</p>\n\n<p><img src="http://i.stack.imgur.com/fgGDu.jpg" alt=""></p>\n\n<p>Should I be treating this as a directed acyclic graph?</p>\n', 'ViewCount': '103', 'Title': 'How to perform alphabetically ordered DFS?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-18T00:02:57.537', 'LastEditDate': '2013-06-17T21:22:38.820', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12729', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8396', 'Tags': '<graph-theory><graphs><search-algorithms>', 'CreationDate': '2013-06-17T20:47:29.400', 'Id': '12728''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a problem that I think should have been studied. I am looking for algorithms for it.</p>\n\n<p>Each item is a set of key-value pairs.\nLet $x$ be an item and $F$ be a set of items.</p>\n\n<p>Each key and each value can appear multiple times.\nThe number of possible keys and possible values can be arbitrary large.</p>\n\n<p>We are given $x$ and $F$. We want to find all those items $y$ in $F$ such that $y.val \\subseteq x$.</p>\n\n<p>For example,</p>\n\n<p>$x = \\{(a,1), (b,2), (c,3), (d,4)\\}$</p>\n\n<p>$F= \\{$<br>\n$(A, \\{(a,1)\\}), $<br>\n$(B, \\{(a,1), (b,2)\\}),$<br>\n$(C, \\{(a,1), (b,3)\\}),$<br>\n$(D, \\{(b,2), (c,3), (d,4)\\}),$<br>\n$(E, \\{(a,1), (b,2), (c,3), (d,4)\\}),$<br>\n$(F, \\{(a,1), (b,2), (c,3), (d,4), (e,5)\\}),$<br>\n$(G, \\{(a,1), (b,2), (c,3), (e,5)\\})$<br>\n$\\}$</p>\n\n<p>The answer is:\n$A$ yes, $B$ yes, $C$ no (right keys, wrong values), $D$ yes, $E$ yes (exact match),\n$F$ no, $G$ no.</p>\n\n<p>Has this problem been studied?</p>\n\n<p>The problem seems similar to finding features from a DNA sequence or detecting plagiarism in a document.</p>\n\n<p>I asked this problem in theoretical CS stack exchange and did not get very helpful answers. <a href="http://cstheory.stackexchange.com/questions/18052/find-all-items-which-are-subsets-of-an-item">http://cstheory.stackexchange.com/questions/18052/find-all-items-which-are-subsets-of-an-item</a></p>\n', 'ViewCount': '54', 'Title': 'Find all items which are subsets of an item', 'LastActivityDate': '2013-06-21T05:46:04.807', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8762', 'Tags': '<algorithms><search-algorithms><sets>', 'CreationDate': '2013-06-20T03:34:14.500', 'Id': '12777''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>I've developed the following backtrack algorithm, and I'm trying to find out it time complexity.</p>\n\n<p>A set of $K$ integers defines a set of modular distances between all pairs of them. In this\nalgorithm, I considered the inverse problem of reconstructing all integer sets which realize a given distance multiset. i.e. :</p>\n\n<p><br>\nInputs: $D=\\{p_i\u2212p_j \\mod N, i\u2260j \\},K $\n<br>\nOutput : $P=\\{p_1,p_2,...,p_K\\},\\qquad p_i \\in \\{0,1,2,...,N-1\\},\\qquad p_i &gt; p_j $ for $i&gt;j$\n<br></p>\n\n<p>Simply saying, the algorithm puts $K$ blanks to be filled. Initially, puts 1 in the first blank. For the second blank it looks for the first integer that if we add to P, it doesn't produce any difference exceeding the existent differences in $D$. Then, it does so, for next blanks. While filling a blank if it checked all possible integers and found no suitable integer for that blank, it turns back to the previous blank and looks for next suitable integer for it. If all blanks are filled, it has finished his job, otherwise it means that there weren't any possible $P$'s for this $D$.</p>\n\n<p>Here's my analysis so far.\nSince the algorithm checks at most all members of $\\{2,...,N\\}$ for each blank (upper bound) there is $N-1$ search for each blank. If each visited blank was filled in visiting time, the complexity would be $O((K-1)(N-1))$ since we have $K-1$ blank (assuming first one is filled with 1). But the algorithm is more complex since for some blanks it goes backward and some blanks may be visited more that once. I'm looking for the worst case complexity i.e. the case that all blanks are visited and no solution is found.</p>\n", 'ViewCount': '1179', 'Title': 'Time complexity of a backtrack algorithm', 'LastEditorUserId': '9098', 'LastActivityDate': '2013-07-13T23:34:21.333', 'LastEditDate': '2013-07-12T07:56:41.700', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9098', 'Tags': '<algorithms><algorithm-analysis><combinatorics><search-algorithms><greedy-algorithms>', 'CreationDate': '2013-07-09T18:22:51.307', 'FavoriteCount': '2', 'Id': '13181''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Searching for a number in an array is said to have a runtime of O(n) because there may be cases where the number doesn't exist in the array. In such cases, you'd have to have gone through the entire array, which is O(n).</p>\n\n<p>But how about in the case where we know the number definately exists in the array? Does the runtime change then?</p>\n\n<p>Also is there a way to find out the average number of searches it would have to do before a number is found in an array based on its size?</p>\n", 'ViewCount': '290', 'Title': 'Runtime of searching for a number in an array?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-18T07:19:24.290', 'LastEditDate': '2013-07-12T14:49:49.223', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'garbage collection', 'PostTypeId': '1', 'OwnerUserId': '9155', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><arrays>', 'CreationDate': '2013-07-12T04:44:49.213', 'Id': '13244''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '231', 'Title': 'Selection in expected linear time: Why am I getting $O(n)$ bound instead of $\\Omega(n \\lg n)$?', 'LastEditDate': '2013-07-22T04:53:45.530', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Body': '<p>The problem is from CLRS 9.3-1:</p>\n\n<blockquote>\n  <p>In the algorithm <code>SELECT</code>, the input elements are divided into groups of\n  $5$. Argue that <code>SELECT</code> does not run in linear time if groups of $3$ are used.</p>\n</blockquote>\n\n<p>If we do the "divide by $3$" technique, we will come up with this recurrence -- </p>\n\n<p>$$T(n) = \\begin{cases}\n\\Theta(1) &amp; \\text{if $n \\le K$} \\\\\nT(\\lceil n/3 \\rceil)+T(2n/3+4) + O(n) &amp; \\text{if $n \\ge K$} \n\\end{cases}$$</p>\n\n<p>I have solved by substituting $T(n) \\le cn$ and  $O(n) = an$ --</p>\n\n<p>$$\\begin{aligned}\nT(n) &amp; \\le \\lceil n/3 \\rceil + c(2n/3 + 4) + an \\\\\n     &amp; \\le cn/3 + c + 2cn/3 + 4c + an \\\\\n     &amp; = cn + 5c + an \\\\\n     &amp; = (c+a)n + 5c \\\\\n     &amp; = c_1n + c_2 \\le c_1n \\approx O(n)\n\\end{aligned}$$</p>\n\n<p>But the solution says it should be $\\Omega(n \\lg n)$. I understand that substitution like $cn \\lg n$ could give $\\Omega(n \\lg n)$ bound, but what is wrong with $O(n)$ formulation above? </p>\n', 'ClosedDate': '2013-07-22T14:13:11.047', 'Tags': '<algorithm-analysis><recurrence-relation><search-algorithms>', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T04:53:45.530', 'CommentCount': '2', 'AcceptedAnswerId': '13382', 'CreationDate': '2013-07-22T04:40:51.103', 'Id': '13381''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I get used to think exhaustive search and combinatorial search are same, but I got confused by reading a paper!</p>\n\n<p>What is the difference between  exhaustive search &amp; combinatorial search ?</p>\n', 'ViewCount': '1059', 'Title': 'What is the difference between exhaustive search & combinatorial search?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-08T11:03:06.587', 'LastEditDate': '2013-08-08T10:49:28.617', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9098', 'Tags': '<algorithms><terminology><search-algorithms>', 'CreationDate': '2013-08-07T18:25:43.057', 'Id': '13663''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>If we have an array $A$ of length $N$, which is partitioned into $\\sqrt{N}$ adjacent subarrays $A(i)$, each of which is monotonically ordered from $\\min(i)$ to $\\max(i)$ (it is known what places have those bounds in A for each subarray), with the following conditions:</p>\n\n<pre><code>min(i) &lt; min(i+1),\nmax(i) &lt; max(i+1),\nmin(i+1) &lt; max(i),\nIf subarrays A(i) and A(i+1) both have the j-th element, then A(i,j) &lt; A(i+1,j),\nThe number of elements in subarray A(i) is strictly increasing with i\n</code></pre>\n\n<p>Would it be possible to search an element in the array $A$, in time $O((\\log N)^k)$, for a positive $k$?  If yes how?</p>\n\n<p>If not, what kind of minimal additional conditions would allow that?</p>\n\n<p>Edit. The subarrays are not necessarily of equal length.\nThey are strictly increasing. The above stated conditions are always guaranteed.</p>\n\n<p>What I have tried so far is to binary search them separately, but that is obviously going to be very inefficient. </p>\n\n<p>[Note that this question has obviously nothing to do with the search of the min of max value in the array. And I am not interested in sorting the array. Just the mere search.]</p>\n', 'ViewCount': '145', 'Title': 'Searching a value in a "piecewise" ordered array', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-23T10:37:11.463', 'LastEditDate': '2013-08-23T10:37:11.463', 'AnswerCount': '1', 'CommentCount': '15', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5147', 'Tags': '<algorithms><search-algorithms><arrays><binary-search>', 'CreationDate': '2013-08-18T12:19:35.963', 'FavoriteCount': '2', 'Id': '13801''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>In the last section of chapter 3 (page 54) in <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em> by Mitzenmacher and Upfal, a randomized algorithm is discussed for finding the median of a set $S$ of distinct elements in $O(n)$ time. As a substep the algorithm requires sorting a multiset $R$ of values sampled from $S$, where $|R| = \\lceil n^{3/4} \\rceil$. I'm sure this is a fairly simple, and straight forward question compared to the others on this site, but how is sorting a multiset of this size bounded by $O(n)$? </p>\n\n<p>I assume the algorithm is using a typical comparison-based, deterministic sorting algorithm that runs in $O(n\\log$ $n)$. So, If I have the expression $O(n^{3/4}\\log(n^{3/4}))$, isn't this just equivalent to $O(\\frac{3}{4}n^{3/4}\\log(n)) = O(n^{3/4}\\log(n))$? How does the reduction down to $O(n)$ proceed?</p>\n", 'ViewCount': '107', 'Title': 'Randomized Median Element Algorithm in Mitzenmacher and Upfal: O(n) sorting step?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:46:22.310', 'LastEditDate': '2013-09-16T07:46:22.310', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10128', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><randomized-algorithms>', 'CreationDate': '2013-09-13T20:07:11.977', 'Id': '14296''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Note: I moved this question from <a href="http://stackoverflow.com/questions/18804045/find-all-intervals-s-e-where-s-e-in-n-0-and-s-e-that-overlap-with-another-gi">stackoverflow.com</a></p>\n\n<p>I have an algorithmic problem where I would like to see if it can be solved in better than $O(n)$:</p>\n\n<p>I have given a table $T$ of $n$ elements where each element is a tuple $(s_i, e_i)$ with $s_i, e_i \\in \\mathbb{N}$ and $s_i &lt; e_i$, i.e. each tuple is some kind of an interval. I have to find all intervals that overlap with a given interval $[t_0, t_1]$ with $t_0, t_1 \\in \\mathbb{N}$ and $t_0 &lt; t_1$. Further, I have available two sorted lists $S$ and $E$, containing the $s$ values, or $e$ values respectively, together with the index $i$ pointing to the respective entry in $T$. The lists are sorted by $s$ values, or $e$ values respectively. (Let\'s assume both, $s$ and $e$ values, are unique.)</p>\n\n<p><strong>Problem:</strong></p>\n\n<p>We have to find each interval/tuple $(s_i, e_i) \\in T$ where $s_i \\leqslant t_1$ and $e_i \\geqslant t_0$.</p>\n\n<p><strong>My thoughts so far:</strong></p>\n\n<p>We can exclude some elements by either applying one of the interval boundaries, i.e. searching $t_1$ in $S$ or $t_0$ in $E$. This gives us a list $L$ of remaining elements:\n$$\n    L \\leftarrow \\{e \\in E \\mid e \\geqslant t_0\\} \\text{ or } L \\leftarrow \\{s \\in S \\mid s \\leqslant t_1\\}\n$$\nHowever, there is no lower bound on the number of elements in $L$, no matter which search we perform. Further, we have to check every element in $L$ if $s \\leqslant t_1$, or $e \\geqslant t_0$ respectively depending on which search we performed before.</p>\n\n<p>The complexity for this solution is $O(n)$.</p>\n\n<p>However, let\'s say that $k$ is the maximum number of elements overlapping with interval $[t_0, t_1]$. <strike>If we assume $k \\ll n$, then the complexity is $O(n/2)$ since we can exclude at least $n/2$ elements by choosing the appropriate search for $L$. Still $O(n/2)$ is in $O(n)$.</strike></p>\n\n<p>Can you think of a better approach to solve this problem?</p>\n\n<p><strong>For record:</strong> </p>\n\n<p>The complexity for finding all intervals overlapping with a certain given interval using an interval tree is $O(\\log n + k)$ where $k$ is the number of overlapping intervals. However, in my practical case I am using a MySQL database which provides index trees for each value, i.e. $s$ and $e$, separately. This way I can not find overlapping intervals in less than $O(n)$. I would need to create an interval tree which is a search tree that stores both interval boundaries, i.e. $s$ and $e$, in a single data structure. The complexity for constructing the interval tree is $O(n \\log n)$. [http://www.dgp.utoronto.ca/people/JamesStewart/378notes/22intervals/]</p>\n', 'ViewCount': '1199', 'Title': 'Find all intervals that overlap with a given interval', 'LastEditorUserId': '10138', 'LastActivityDate': '2013-09-16T12:49:46.127', 'LastEditDate': '2013-09-16T12:49:46.127', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14320', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10138', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-09-14T16:52:27.203', 'Id': '14311''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a $n\\times n$ matrix called $M$, and two integers $k_\\min$ and $k_\\max$. \nEach row and each column of M is sorted in the increasing order. </p>\n\n<p>I would like to know if there is way I can count the number of its elements which are inside $[k_\\min, k_\\max]$, using a $O(n)$ algorithm.</p>\n', 'ViewCount': '95', 'Title': 'Count elements of a sorted matrix that fall into a given interval', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-17T17:14:17.943', 'LastEditDate': '2013-09-17T12:07:52.697', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'user10182', 'PostTypeId': '1', 'Tags': '<algorithms><search-algorithms><matrices><counting>', 'CreationDate': '2013-09-17T09:55:03.567', 'FavoriteCount': '1', 'Id': '14376''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In the <a href="http://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* search algorithm</a>, we use a priority queue with heuristic function to find optimum result with minimum cost. But, how do we get the path after reaching goal?</p>\n', 'ViewCount': '33', 'Title': 'Recover the path to a goal state in A* search algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-20T15:26:46.447', 'LastEditDate': '2013-09-20T15:26:46.447', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14434', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10193', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-09-19T04:15:02.077', 'Id': '14433''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Im studying for the computer science subject GRE test, as an exercise i need to implement the followin algorithm in java, any idea on how to aproach it?.</p>\n\n<p>Given a set $X$ and $z$ not in $X$, indicate between which would be the immediate positions $z$ in $X$</p>\n', 'ViewCount': '18', 'Title': 'Immediate positions algorithm?', 'LastActivityDate': '2013-09-25T04:13:11.850', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2013-09-25T03:45:39.477', 'Id': '14588''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>One naive approach in solving <em>multiple pattern matching</em> problem is to call <em>single pattern matching</em> procedure on each of the pattern.</p>\n\n<p>There <strong>must</strong> be some drawbacks in this approach, given the variety of multiple pattern matching algorithms such as Aho Cornsick algorithm, which prove to be more efficient.</p>\n\n<p>So what are the drawbacks on this straightforward yet naive approach? In what scenario is this algorithm doing unnecessary works?</p>\n', 'ViewCount': '40', 'Title': 'Drawbacks of repeating a single pattern matching procedure for many patterns', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-03T07:09:50.597', 'LastEditDate': '2013-10-04T06:33:51.647', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><search-algorithms><strings><substrings>', 'CreationDate': '2013-10-01T18:24:30.603', 'Id': '14739''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>When using A* or any other best path finding algorithm we say that the heuristic used should be <strong>admissible</strong> i.e. it should never overestimate the actual solution path (moves).</p>\n\n<p>Can someone tell me how this is true? Please avoid equations and all. I need a logical proof.\nIf you  want you can explain using the <strong>Manhattan</strong> distance heuristic of the <strong>8-puzzle.</strong></p>\n', 'ViewCount': '804', 'Title': 'How does an admissible heuristic ensure an optimal solution?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-14T22:14:51.397', 'LastEditDate': '2013-10-14T12:16:36.493', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '16089', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4341', 'Tags': '<algorithms><artificial-intelligence><search-algorithms><heuristics>', 'CreationDate': '2013-10-14T06:34:42.517', 'Id': '16065''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In my world there are nodes and lines, I want to see if there is any path between node A, and node B, that do not cross any line(including the lines of the path itself) and do not go thru the same node twice. What would be the efficiency of such algorithm and where can I find more information about it.</p>\n\n<p>So far I been reading about <a href="https://en.wikipedia.org/wiki/A%2a_search_algorithm" rel="nofollow">A* algorithm</a>, but I do not need the shortest path, just to see if there is any path like this exist at all.</p>\n', 'ViewCount': '86', 'Title': 'Any path detecting between A to B', 'LastActivityDate': '2013-10-20T18:39:42.557', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '16246', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10572', 'Tags': '<search-algorithms>', 'CreationDate': '2013-10-20T05:36:47.547', 'FavoriteCount': '1', 'Id': '16245''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Why is it that Iterative-deepening A* is optimal, even without monotonicity? How can I be sure that the first goal reached is the optimal one?</p>\n', 'ViewCount': '282', 'Title': 'Why is Iterative-deepening A* optimal, even without monotonicity?', 'LastActivityDate': '2013-10-28T20:50:58.197', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '16514', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10575', 'Tags': '<search-algorithms><search-trees>', 'CreationDate': '2013-10-27T17:01:21.627', 'Id': '16477''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>According to the wiki-article, <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="nofollow">http://en.wikipedia.org/wiki/Tf%E2%80%93idf</a> the term frequency of term $t$ in document $d$ is given as follows:\n$$\ntf(t,d) = 0.5 + \\frac{0.5 \\times f(t,d)}{\\max{\\{f(w,d): w~\\epsilon~d\\}}}\n$$</p>\n\n<p>This is the augmented way of computation. But what does $w$ stand for?</p>\n', 'ViewCount': '46', 'Title': 'clarification in tf-idf formula', 'LastActivityDate': '2013-10-29T13:46:21.610', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16544', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2374', 'Tags': '<search-algorithms>', 'CreationDate': '2013-10-29T11:51:18.663', 'Id': '16539''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Given N numbers and M range queries (starting and ending points), I need to \ncompute majority (most frequent element, if it exists) in these ranges.</p>\n\n<p>I'm looking for an algorithm that would answer queries in logarithmic or even constant time.</p>\n", 'ViewCount': '91', 'Title': 'Range majority queries - most freqent element in range', 'LastActivityDate': '2013-11-03T18:09:21.813', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16676', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11149', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-11-03T12:55:59.207', 'Id': '16671''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '150', 'Title': 'Artificial Intelligence: Condition for BFS being optimal', 'LastEditDate': '2013-11-18T19:12:31.383', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6998', 'FavoriteCount': '1', 'Body': '<p>It is said in the book <em>Artificial Intelligence: A Modern Approach</em> for finding a solution on a tree using BFS that: </p>\n\n<blockquote>\n  <p>breadth-first search is optimal if the path cost is a nondecreasing function of the\n  depth of the node. The most common such scenario is that all actions have the same cost.</p>\n</blockquote>\n\n<p>From that I understand that if the path cost is non decreasing function of depth, the BFS algorithm returns an optimal solution, i.e., <strong>the only condition is the cost function being nondecreasing</strong>. But I think the only way for BFS to be optimal is the scenario in which all the path costs are identical, therefore a node found in a certain level is necessarily the optimal solution, as, if they exist, the others are. Therefore I think for BFS to be optimal, cost function should be non decreasing <strong>AND</strong> the costs of nodes should be identical. However, the book says only one of the conditions (former one) makes BFS optimal.</p>\n\n<p>Is there a situation in which the costs are not identical, the cost function is nondecreasing and the solution returned by BFS is guaranteed to be optimal?</p>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms><search-trees><search-problem>', 'LastEditorUserId': '6998', 'LastActivityDate': '2013-11-18T19:12:31.383', 'CommentCount': '0', 'AcceptedAnswerId': '16780', 'CreationDate': '2013-11-06T01:16:28.697', 'Id': '16758''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Assume you have an object moving at a constant velocity(up, down, left, right) in a grid. You have unlimited resources (memory, time). At any given time step in the grid, you can "guess" the location of the object, and you win if it is currently at that position. Without relying on probability techniques, what would be the best way to approach catching this object?</p>\n', 'ViewCount': '30', 'Title': 'finding a an object with constant velocity on an infinite grid in discrete time steps', 'LastActivityDate': '2013-11-26T10:28:16.433', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11632', 'Tags': '<algorithms><search-algorithms><probabilistic-algorithms>', 'CreationDate': '2013-11-26T09:27:30.743', 'Id': '18366''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>This question is inspired by <a href="http://www.spoj.com/problems/HAYBALE/" rel="nofollow">this SPOJ problem</a>. That problem asks median of the final array, that can be done in $O(k+n)$, where <em>k</em> is the number of update queries and <em>n</em> is the size of the array.</p>\n\n<p>The variant : Given an array of length <strong>N</strong> (of the order $10^5$), initially all elements are 0. There are <strong>K</strong> (of the order $10^5$) queries of type <strong>A B</strong>, meaning array elements in range $[A,B]$ must be increased by 1. After <em>every</em> query operation, find the median of the array.<br>\nCan this be solved efficiently?</p>\n', 'ViewCount': '172', 'Title': 'Algorithm : Median of a dynamic array', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T12:35:44.540', 'LastEditDate': '2014-04-10T12:35:44.540', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1742', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2013-12-02T10:58:40.747', 'FavoriteCount': '4', 'Id': '18532''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '111', 'Title': 'Clarification on Tabu Search', 'LastEditDate': '2013-12-03T22:03:19.313', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '170', 'FavoriteCount': '1', 'Body': u'<p>I need some help in understanding the \'<em>Tabu Search</em>\' Algorithm. (<a href="http://en.wikipedia.org/wiki/Tabu_search" rel="nofollow">Wikipedia</a>)</p>\n\n<p>I miss a simple explanation to Tabu Search. Anyway, I\'m trying to refer to available resources and build an understanding. </p>\n\n<p><em><strong>This is what I\'m trying to \'digest\':</em></strong></p>\n\n<ul>\n<li><p><em>Tabu Search</em> is an improvement over the <em>Hill Climbing</em> algorithm (Ref-1).</p></li>\n<li><p>The problem with Hill Climbing is that it does not guarantee about reaching the global optimum, because it only searches on a subset of the whole solution space. It will find the local optimum.</p></li>\n<li>To get rid of this issue, Tabu Search maintains a \'Tabu List\' of previously visited states that cannot be revisited (Ref-2). </li>\n<li>If the tabu list is too large, the oldest candidate solution is removed and it\u2019s no\nlonger tabu to reconsider (Ref-3).</li>\n</ul>\n\n<p><em><strong>My questions are,</em></strong> </p>\n\n<ol>\n<li><p>How does Tabu Search cure the problem of getting stuck in a local\noptimum? Does it increase the search-space?</p></li>\n<li><p>What is the need of maintaining a list (i.e. Tabu List)? Why not\njust remember the optimum solution found so far?</p></li>\n<li><p>When the Tabu List is too large, the oldest candidate will be\nremoved. What if this oldest candidate is the global optimum?</p></li>\n</ol>\n\n<p><em>If anyone could explain Tabu Search algorithm using an example, I\'m sure these questions would be automatically answered.</em></p>\n\n<p>References:</p>\n\n<ul>\n<li><p>(Ref-1) Hill Climbing, Wikipedia Article (<a href="http://en.wikipedia.org/wiki/Hill_climbing" rel="nofollow">link</a>)</p></li>\n<li><p>(Ref-2) Russell, Stuart Jonathan, et al. Artificial intelligence: a modern approach. Vol. 74. Englewood Cliffs: Prentice hall, 1995. (<a href="http://www.worldcat.org/oclc/31288015" rel="nofollow">WorldCat</a>)</p></li>\n<li><p>(Ref-3) Luke, Sean. "Essentials of Metaheuristics.". (<a href="http://cs.gmu.edu/~sean/book/metaheuristics/Essentials.pdf" rel="nofollow">pdf</a>)</p></li>\n</ul>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms>', 'LastEditorUserId': '268', 'LastActivityDate': '2013-12-04T11:19:56.827', 'CommentCount': '0', 'AcceptedAnswerId': '18606', 'CreationDate': '2013-12-03T20:36:30.533', 'Id': '18582''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Consider this tree:</p>\n\n<p><img src="http://i.stack.imgur.com/cMslZ.png" alt="Simple binary tree"></p>\n\n<p>If I traverse it using post-order, I\'d start at <em>B</em> (as it is the leftmost leaf) and that\'s where my misunderstanding begins. I know <em>B</em> is the first and <em>A</em> will be the last node in post order, as the rule is left-right-root. One of my university professors said the correct answer for the post-order traversal of a tree similar to the one above would be <strong><em>B</em>, <em>C</em>, <em>D</em>, <em>E</em>, <em>A</em></strong>, but in my understanding, it should be <strong><em>B</em>, <em>D</em>, <em>E</em>, <em>C</em>, <em>A</em></strong>.  </p>\n\n<p>Am I getting it wrong? Shouldn\'t I evaluate <em>(C,D),(C,E)</em> as a subtree and then go back to the parent tree?</p>\n', 'ViewCount': '40', 'Title': "Doesn't post-order traversal require subtrees to be evaluated separately?", 'LastEditorUserId': '12111', 'LastActivityDate': '2013-12-14T19:07:44.407', 'LastEditDate': '2013-12-14T19:07:44.407', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18988', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12111', 'Tags': '<data-structures><binary-trees><search-algorithms><trees><search-trees>', 'CreationDate': '2013-12-14T17:40:35.087', 'Id': '18987''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>Assume having a graph $G_{variables}=(V,U)$ where $V=\\{v_1,v_2,\u2026,v_n\\}$ is a set of variables; each variable $v_i\\in V$ is associated with a set of possible values (it's domain) $dom(v_i)$. </p>\n\n<p>Let $P$ be a search problem (i.e reachability problem) over graph $G=(O,E)$ where $O$ is the cartesian product of the variables domains. Let $T$ be a junction tree resulted from $G_{variables}$. $P$ can be also solved through searching every clique in $T$. I am looking for keywords/examples of such problems. $G_{variables}$ preferably to be DAG.  </p>\n", 'ViewCount': '30', 'Title': 'Search problems that can also be solved by junction trees and searching cliques', 'LastEditorUserId': '4598', 'LastActivityDate': '2014-01-10T04:15:00.893', 'LastEditDate': '2014-01-10T04:15:00.893', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<graph-theory><reference-request><search-algorithms><search-problem>', 'CreationDate': '2014-01-09T17:48:23.083', 'Id': '19602''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have to generate all possible paths in a directed, acyclic weighted graph with edge costs. I also have to sort them in order of shortest path. </p>\n\n<p>The simplest way that comes to mind is to do a depth-first search (DFS) for all paths, accumulating their edge costs as I traverse the paths, and then doing an NlogN sort on the result. </p>\n\n<p>But I wondering if there is a better way to do this task. Are there any algorithms that could optimize this problem (combining DFS and a shortest path algorithm such as Dijkstra's maybe?). </p>\n", 'ViewCount': '261', 'Title': 'Optimal algorithm to traverse all paths in the order of shortest path', 'LastEditorUserId': '10519', 'LastActivityDate': '2014-01-19T16:10:53.177', 'LastEditDate': '2014-01-16T17:08:00.273', 'AnswerCount': '1', 'CommentCount': '13', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10519', 'Tags': '<algorithms><graphs><shortest-path><search-algorithms>', 'CreationDate': '2014-01-16T01:40:27.343', 'FavoriteCount': '1', 'Id': '19761''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am looking for papers/methods (or at least problem examples) where the original search problem $P$ can be solved by either:</p>\n\n<ol>\n<li>Searching through the original graph. or</li>\n<li>By decomposing it into several subset of problems $P_1,P_2, \\dots,P_n$.</li>\n</ol>\n\n<p>Ideally $sol(P )=sol(P_1)\\cup sol(P_2)\\cup \\ldots \\cup(P_n)$ with no preprocessing (i.e the union of the subproblems correspond directly to the solution of the problem).  </p>\n\n<p>I have no constraint; though prefer the underlying graph to be a DAG and the problem to be a reachability problem. Google seems to fail on finding such papers. </p>\n', 'ViewCount': '74', 'Title': 'Decomposing the search problem into several small problems', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T12:32:29.493', 'LastEditDate': '2014-01-17T21:36:52.663', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '19804', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<graph-theory><reference-request><search-algorithms><search-problem>', 'CreationDate': '2014-01-17T18:59:51.023', 'Id': '19792''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>What is the most efficient algorithm for finding the median of an array when the available operations are limited to Max(), Min(), Multiply, Add, and no conditionals are allowed. Pivots and sorts are not allowed. The algorithm I have come up with so far seems very inefficient and goes like so:</p>\n\n<ol>\n<li>For an array of length n, create a working array of length 2n by\ndoing pairwise Max() comparisons for every pair of elements in the input\narray. This 2n array is guaranteed not to contain the lowest value from the original (or one fewer of the lowest value if there were ties for the lowest value)</li>\n<li>Recursively call (1) n/2 times </li>\n<li>Take min() of final array</li>\n</ol>\n\n<p>There's got to be a faster algorithm!</p>\n", 'ViewCount': '56', 'Title': 'Efficient Median Algorithm With Very Constrained Operators', 'LastEditorUserId': '13052', 'LastActivityDate': '2014-01-24T06:02:32.310', 'LastEditDate': '2014-01-23T06:13:42.257', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19909', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13052', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2014-01-23T05:15:52.513', 'Id': '19908''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose I have a list of distinct integers. I'm looking for a data structure that will support the operations <code>search</code>, <code>insert</code>, <code>delete</code> and <code>closest_pair</code> in $O(\\log n)$ time. </p>\n\n<p>I know that a sorted array supports <code>search</code>, <code>insert</code> and <code>delete</code> in $O(\\log n)$ time. So, all we need to do is maintain information about the closest pair during the <code>insert</code> and <code>delete</code> operations. </p>\n\n<p>The only problem is that the <code>closest_pair</code> would perform in better than $O(\\log n)$ time since information about the closest pair would already be available. So, I'm stuck at this point.</p>\n", 'ViewCount': '227', 'Title': 'Data Structure For Closest Pair Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T21:35:15.070', 'LastEditDate': '2014-01-26T15:56:11.007', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '19998', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<data-structures><search-algorithms>', 'CreationDate': '2014-01-26T05:43:37.223', 'Id': '19984''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Hello I am trying to solve the AlienTiles problem described at <a href="http://www.alientiles.com" rel="nofollow">alientiles.com</a> using the A* algorithm but I cannot find any good heuristic function so far.</p>\n\n<p>In AlienTiles you have a board with $N \\times N$ tiles, all coloured red. By clicking on a tile, all tiles in the same row and column advance to the next color, with the colour order being red $\\rightarrow$ green $\\rightarrow$ blue $\\rightarrow$ purple, resetting to red after purple. A goal state is a state where every tile has the same colour, as long as its not red.</p>\n\n<p>Is there any good point to start? I am completely frustrated about how I am supposed to handle the problem. An easy function that I came up with was the distance of the colour of the current tile with the target tile, but it is very slow.</p>\n', 'ViewCount': '145', 'Title': 'Solving AlienTiles with an A* heuristic', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-29T02:29:56.353', 'LastEditDate': '2014-01-28T14:09:31.010', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '20054', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13168', 'Tags': '<algorithms><search-algorithms><heuristics><board-games>', 'CreationDate': '2014-01-27T21:01:28.317', 'Id': '20017''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '54', 'Title': 'Why is the PageRank vector also the eigenvector of the web adjacency matrix?', 'LastEditDate': '2014-02-04T23:08:39.537', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7706', 'FavoriteCount': '1', 'Body': '<p>From <a href="http://en.wikipedia.org/wiki/PageRank#Damping_factor" rel="nofollow">wikipedia</a>:</p>\n\n<blockquote>\n  <p>The PageRank values are the entries of the dominant eigenvector of the\n  modified adjacency matrix. This makes PageRank a particularly elegant\n  metric</p>\n</blockquote>\n\n<p>Can anyone please elaborate on the connection between the eigenvector and the PR vector? Why are they related?</p>\n', 'Tags': '<graph-theory><search-algorithms>', 'LastEditorUserId': '11946', 'LastActivityDate': '2014-02-04T23:08:39.537', 'CommentCount': '0', 'AcceptedAnswerId': '21291', 'CreationDate': '2014-02-04T11:09:29.440', 'Id': '21283''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have two algorithms which I would like to implement:</p>\n\n<p>First, given a (very long) list $\\{\\mathbf{r}_{i}\\}_{i=1}^{n}\\subset \\mathbb{R}^{3}$, a point $p \\in \\mathbb{R}^{3}$, and a distance $d&gt;0$, find all $i$ such that $|\\mathbf{r}_{i} - \\mathbf{p}| &lt; d$.</p>\n\n<p>Currently, I sort the list over the $x$-coordinate, so that if $x_{i}$ is the $x$-coordinate of $\\mathbf{r}_{i}$, then $x_{1} \\le x_{2} \\le \\cdots \\le x_{n}$. Then I use the condition $(x_{i}-\\mathbf{p}_{x})^{2} &gt; d^2 \\implies |\\mathbf{r}_{i} - \\mathbf{p}|^2 &gt; d^{2}$. This allows me to filter the list by binary search. This leaves me with a subset $\\{\\mathbf{r}_{i}\\}_{i=j_{1}}^{i=j_{2}}$ of the original list where for all $i \\in [j_{1}, j_{2}]$, $(x_{i}-\\mathbf{p}_{x})^{2} &lt;= d^2$. Then I have to bite the bullet and do an explicit test on each one of these to see if $|\\mathbf{r}_{i} - \\mathbf{p}| &lt; d$.</p>\n\n<p>This algorithm strikes me as weird and not optimal; is there a better way?</p>\n\n<p>Next: A related problem: Given a list $\\{\\mathbf{r}_{i}\\}_{i=1}^{n}\\subset \\mathbb{R}^{3}$ and a point $\\mathbf{p} \\in \\mathbf{R}^{3}$, find $j \\in [1,n]$ such that $d_{j}:=|\\mathbf{r}_{j} - \\mathbf{p}| = \\min_{i} |\\mathbf{r}_{i} - \\mathbf{p}|$.  Is there an efficient algorithm to find $j$?</p>\n\n<p>Finally, I should note that any one-time ``sorting'' expense is tolerable, since this operation is repeated many times. </p>\n", 'ViewCount': '65', 'Title': 'Find all neighbors at a certain distance, in 3 dimensions', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-13T22:18:22.163', 'LastEditDate': '2014-02-13T22:18:22.163', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21615', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14616', 'Tags': '<algorithms><computational-geometry><search-algorithms>', 'CreationDate': '2014-02-12T22:55:41.277', 'Id': '21580''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>It seems to me to be incorrect to say that lexicographic DFS is P-complete, since it isn't a decision problem. There is a corresponding decision problem, first DFS ordering, which is known to be P-complete. However, I want to talk about complexity of DFS, not it's decision problem. What complexity class should I say DFS belongs to?</p>\n", 'ViewCount': '36', 'Title': 'lexicographic depth-first search complexity class', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-24T19:24:05.510', 'LastEditDate': '2014-02-24T19:24:05.510', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'OwnerDisplayName': 'Adam Kurkiewicz', 'PostTypeId': '1', 'OwnerUserId': '11718', 'Tags': '<complexity-theory><search-algorithms>', 'CreationDate': '2014-02-13T14:39:10.070', 'Id': '21620''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I am a high school student computationally studying the 3-dimensional structure of chromosomes by 40 kilobase loci. In a nutshell, loci that are close in space tend to express their genes at the same time --- loci are different stops on a 3D-winding DNA chain.</p>\n\n<p>The best way to understand the 3D structure is by gathering what are basically distances between loci.</p>\n\n<p>Now I have an $n\\times n$ ($n$ = number of loci studied) matrix where $(i,j)$ is the distance between locus $i$ and locus $j$. I also have a (somewhat miraculous) 3-dimensional of the same chromosome that maps each locus to a certain point in a 3D $(x,y,z)$ coordinate system.</p>\n\n<p>My task is to find all of the loci within a certain radius of locus $L$. With the matrix, I would have to go to $L$ and traverse many nearby locus-distance chains, possibly for a long time, before being any bit certain that I had everything I wanted (i.e. brute force). With the spatial model, I would only have to conduct a simple search within that radius.</p>\n\n<p>Here is my question. What is the complexity of finding nearby loci in the 3D model and the 2D matrix with respect to loci count and radius size (whichever you think is more complex)? (Compare the two complexities and give both)</p>\n\n<p>I am not very studied in CS, but here is what I guess:</p>\n\n<p>$$\nC_{2D search best-case} = O(n^2)\n$$\n$$\nC_{2D search worst-case} = O(2^n)\n$$</p>\n\n<p>Best-case is what you'd expect, and worst-case would be going through every permutation of the distance.</p>\n\n<p>$$\nC_{3D search any case} = O(n)\n$$</p>\n\n<p>This is just my rather fallible intuition.</p>\n", 'ViewCount': '57', 'Title': 'Time complexity of proximity search in distance matrix', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-17T19:14:00.307', 'LastEditDate': '2014-02-17T09:59:29.453', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21721', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14736', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><ralgorithms>', 'CreationDate': '2014-02-17T00:27:33.090', 'Id': '21711''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '216', 'Title': 'Parallel algorithm for finding the maximum in $\\log n$ time using $n / \\log n$ processors', 'LastEditDate': '2014-02-22T11:43:18.023', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '14907', 'FavoriteCount': '1', 'Body': "<p>We were presented in class with an algorithm for finding the maximum in an array in parallel in $O(1)$ time complexity with $n^2$ computers.</p>\n\n<p>The algorithm was:</p>\n\n<blockquote>\n  <p>Given an array A of length n:</p>\n  \n  <ol>\n  <li>Make a flag array B of length n and initialize it with zeroes with $n$ computers.</li>\n  <li>Compare every 2 elements and write 1 in B at the index of the minimum with $n^2$ computers.</li>\n  <li>find the index with the 0 in A with $n$ computers.</li>\n  </ol>\n</blockquote>\n\n<p>The lecturer teased us it could be done with $\\frac{n}{\\log n}$ computers and with $\\log n$ time complexity.</p>\n\n<p>After alot of thinking I couldn't figure out how to do it.\nAny idea?</p>\n", 'Tags': '<algorithms><search-algorithms><parallel-computing>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-22T11:43:18.023', 'CommentCount': '0', 'AcceptedAnswerId': '21911', 'CreationDate': '2014-02-21T21:02:53.517', 'Id': '21910''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '46', 'Title': 'What are the state-of-the-art algorithms for pathfinding on a continuous map of the Earth?', 'LastEditDate': '2014-02-24T19:22:56.753', 'AnswerCount': '2', 'Score': '12', 'OwnerDisplayName': 'Deer Hunter', 'PostTypeId': '1', 'OwnerUserId': '14799', 'Body': "<p>Suppose I have got a solar-powered autonomous surface vessel somewhere in the fjords of Norway, supplied with a fairly recent set of maps, a GPS receiver, and no means of downlinking detailed commands from me. This vessel has to reach, say, the island of Hainan at the earliest possible moment.</p>\n\n<ul>\n<li>What are the <strong>deterministic</strong> algorithms for finding a maritime route on a globe?</li>\n<li><p>What is their time and memory complexity?</p></li>\n<li><p>Can I, for instance, use A* after transforming the map of the globe into a diagram with connected polygons (i.e. Delaunay triangulation on a sphere/ellipsoid) and what are other feasible approaches?</p></li>\n</ul>\n\n<p>Answers should ideally provide references to papers with discussion of the above-mentioned questions.</p>\n\n<p>As pointed out by <em>Rob Lang</em>, the algorithms must fit the usual criteria: in the absence of time constraints, lead to the shortest path between any two points on Earth's oceans and seas, or indicate pathfinding failure otherwise.</p>\n\n<p>There are interesting sub-topics here (trading pre-computation time/storage for online computations, providing slightly suboptimal routes before a deadline kicks in etc.), but these are ancillary to the main issue.</p>\n", 'Tags': '<artificial-intelligence><search-algorithms>', 'LastEditorUserId': '6980', 'LastActivityDate': '2014-02-24T19:22:56.753', 'CommentCount': '10', 'AcceptedAnswerId': '21994', 'CreationDate': '2014-02-14T05:02:33.373', 'Id': '21993''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>If we were to intuitively construct a lower bound for searching an element in a list $A$ containing $n$ integers, it would be in $\\Omega(n)$.</p>\n\n<p>But with the decision tree model, the number of leafs is $n$, so we conclude that the lower bound is $\\Omega(\\log{n})$.</p>\n\n<p>This is the same as finding the maximum element in a list. Intuitively, it is in $\\Omega(n)$, but with the decision tree model it is $\\Omega(\\log{n})$.</p>\n\n<p>Can someone help me understand this discrepancy ?</p>\n\n<p>Thank you in advance.</p>\n', 'ViewCount': '17', 'Title': 'Doubt in the correctness of decision tree models for constructing a lower bound', 'LastActivityDate': '2014-02-27T19:13:00.927', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22104', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><search-algorithms><decision-problem><trees><lower-bounds>', 'CreationDate': '2014-02-27T17:39:15.747', 'Id': '22099''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have two functions $S$ and $T$ which are interrelated and I want to find the asymptotic worst case runtime. The fact that they are interrelated is stumping me...</p>\n\n<p>How would I find the asymptotic runtime $S(n)$ and $T(n)$?</p>\n\n<p>$$\n\\begin{align*}\nS(n) &amp;= 2S(n/4) +  T(n/4) \\\\\nT(n) &amp;=  S(n/2) + 2T(n/2)\n\\end{align*}\n$$</p>\n', 'ViewCount': '55', 'Title': 'Asymptotic Runtime of Interrelated Functions', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-01T23:51:07.287', 'LastEditDate': '2014-03-01T23:51:07.287', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11204', 'Tags': '<algorithms><algorithm-analysis><asymptotics><search-algorithms><master-theorem>', 'CreationDate': '2014-03-01T02:34:54.417', 'Id': '22149''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I understand the basis of A* as being a derivative of Dijkstra, however, I recently found out about D*. From Wikipedia, I can understand the algorithm. What I do not understand is why I would use D* over Dijkstra. To my understanding, Dijkstra gives a best path and D* works backwards from the end goal, but unlike A* it seems to do many calculations, so it doesn't seem as efficient.</p>\n", 'ViewCount': '116', 'Title': 'Why choose D* over Dijkstra?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-05T09:28:49.120', 'LastEditDate': '2014-03-05T07:08:16.793', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22299', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '15288', 'Tags': '<algorithms><graphs><search-algorithms><efficiency>', 'CreationDate': '2014-03-05T00:30:16.657', 'Id': '22284''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p><a href="http://cs.stackexchange.com/q/11263/8660">This</a> link provides an algorithm for finding the diameter of an undirected tree <strong>using BFS/DFS</strong>. Summarizing:</p>\n\n<blockquote>\n  <p>Run BFS on any node s in the graph, remembering the node u discovered last. Run BFS from u remembering the node v discovered last. d(u,v) is the diameter of the tree. </p>\n</blockquote>\n\n<p>Why does it work ?</p>\n\n<p>Page 2 of <a href="http://courses.csail.mit.edu/6.046/fall01/handouts/ps9sol.pdf" rel="nofollow">this</a> provides a reasoning, but it is confusing. I am quoting the initial portion of the proof:</p>\n\n<blockquote>\n  <p>Run BFS on any node s in the graph, remembering the node u discovered last. Run BFS from u remembering the node v discovered last. d(u,v) is the diameter of the tree.</p>\n  \n  <p>Correctness: Let a and b be any two nodes such that d(a,b) is the diameter of the tree. There is a unique path from a to b. Let t be the first node on that path discovered by BFS. If the paths $p_1$ from s to u and $p_2$ from a to b do not share edges, then the path from t to u includes s. So</p>\n  \n  <p>$d(t,u) \\ge d(s,u)$</p>\n  \n  <p>$d(t,u) \\ge d(s,a)$</p>\n  \n  <p>....(more inequalities follow ..)</p>\n</blockquote>\n\n<p><img src="http://i61.tinypic.com/rji9uq.png" alt=""> </p>\n\n<p>The inequalities do not make sense to me.</p>\n', 'ViewCount': '232', 'Title': 'Algorithm to find diameter of a tree using BFS/DFS. Why does it work?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T23:37:16.420', 'LastEditDate': '2014-03-20T13:30:18.160', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8660', 'Tags': '<algorithms><graphs><search-algorithms><graph-traversal>', 'CreationDate': '2014-03-20T07:09:10.287', 'Id': '22855''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I face this problem a lot while searching phone numbers and bank account numbers, when I do remember it partially. </p>\n\n<p>I save a draft in gmail with the content <code>I am mango</code>. Then I search it, by entering just <code>mango</code> and it gets me to the draft. </p>\n\n<p>But when I save a draft with some number such as <code>123987645</code> and try to search by entering <code>12398764</code> i.e just one character missing I fail to find it. Also I failed when I just typed  <code>87645</code>. </p>\n\n<p>Out of curiosity I am asking are the algorithms for finding numbers and text fundamentally different? Or I am missing something?</p>\n', 'ViewCount': '67', 'Title': 'Are algorithms for searching text vs searching numbers fundamentally different?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-23T12:58:54.220', 'LastEditDate': '2014-03-24T12:30:44.410', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><search-algorithms><strings><matching>', 'CreationDate': '2014-03-24T12:01:50.507', 'Id': '22997''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I came across the following problem in a exam. </p>\n\n<p>We choose a permutation of n elements $[1,n]$ uniformly at random. Now a variable MIN holds the minimum value seen so far at it is defined to $\\infty$ initially. Now during our inspection if we see a smaller value than MIN, then MIN is updated to the new value. </p>\n\n<p>For example, if we consider the permutation, </p>\n\n<p>$$5\\ 9\\ 4\\ 2\\ 6\\ 8\\ 0\\ 3\\ 1\\ 7$$\nthe MIN is updated 4 times as $5,4,2,0$. Then the expected no. of times MIN is updated?</p>\n\n<p>I tried to find the no. of permutations, for which MIN is updated $i$ times, so that I can find the value by $\\sum_{i=1}^{n}iN(i)$, where $N(i)$, is the no. of permutations for which MIN is updated $i$ times.</p>\n\n<p>But for $i\\geq2$, $N(i)$ is getting very complicated and unable to find the total sum.</p>\n', 'ViewCount': '190', 'Title': 'Expected number of updates of minimum', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T22:33:27.977', 'LastEditDate': '2014-03-31T17:47:01.783', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16323', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms>', 'CreationDate': '2014-03-31T15:20:35.840', 'Id': '23295''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Please can anyone explain me the worst ,average and best case running time for the Unifrom binary search .. Also how can the lookup table be explained?</p>\n', 'ViewCount': '19', 'ClosedDate': '2014-04-01T07:40:45.420', 'Title': 'Uniform Binary Search explanation and lookup table', 'LastEditorUserId': '16347', 'LastActivityDate': '2014-04-01T08:37:14.043', 'LastEditDate': '2014-04-01T08:37:14.043', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16347', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><binary-search>', 'CreationDate': '2014-04-01T06:48:15.420', 'Id': '23313''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Is there any algorithm which takes edges (given by its two end points), and determines in which cell (or cells) of grid it is?</p>\n\n<p>Grid has fixed dimensions and number of cells. Grid is represented by its cells with matrix. And every cell has list of edges that intersect that cell.</p>\n\n<p>Input is set of pair points, but I can also transform it in just set of points, or any other needed representation.\nOutput should be the mentioned grid with cells who contain list of edges that intersect that cell.</p>\n\n<p>Algorithm should be fast and robust, and by that I mean it covers special (degenerated) cases and that its time complexity is good.</p>\n\n<p>What I want to be able is to use that grid later for search, for example to answer me question like "Which cells does given edge AB(with end points A and B) intersect?" or "Give me all edges that intersect cell 12"(First row, second column).</p>\n', 'ViewCount': '44', 'Title': 'Algorithm for storing polygon edges into grid', 'LastEditorUserId': '16207', 'LastActivityDate': '2014-04-02T12:08:49.930', 'LastEditDate': '2014-04-02T12:08:49.930', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16207', 'Tags': '<reference-request><computational-geometry><search-algorithms>', 'CreationDate': '2014-04-01T16:03:33.280', 'Id': '23321''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I read in the artificial-intelligence book of Russel and Norvig that The tree-search version of A* is optimal if heuristic function is admissible, while the graph-search version is optimal if heuristic function is consistent(monotone).\nAn admissible heuristic is one that never overestimates the cost to reach the goal.\nA heuristic $h(n)$ is consistent if, for every node $n$ and every successor $n\'$ of $n$ generated by any action $a$, the estimated cost of reaching the goal from $n$ is no greater than the step cost of getting to $n\'$  plus the estimated cost of reaching the goal from $n\'$ :\n$h(n) \u2264 c(n, a, n\') + h(n\')$.</p>\n\n<p>My question is about this graph and heuristic.</p>\n\n<p><img src="http://i.stack.imgur.com/HBX9A.png" alt="enter image description here"></p>\n\n<p>Suppose this graph is a state space of a problem in Artificial intelligence.  $A$ is the start node(initial state),and $D$ is the goal. Numbers on the edges are path costs. Numbers on the nodes are value of heuristic function for this problem.\nI think this heuristic function is consistent. So A* can find the optimal path from start to goal.</p>\n\n<pre><code>step 1: g(A)=0, h(A)=5, so f(A)=5\n        Expand A : B, C\n        add A to close list.\n        add B and C to open list.\n\nstep 2: g(B)=10, h(B)=1, so f(B)=11\n        g(C)=1, h(C)=8, so f(C)=9\n        f(C) &lt; f(B) so:\n            Expand C : D\n            add C to close list\n            add D to open list.\n\nstep 3: g(D)=1+16=17,h(D)=0, so f(D)=17\n        f(B) &lt; f(D) so:\n            Expande B : nothing because D is already in open list.\n\nstep 4: Just D in open list so\n             Expand D : D is goal\n\nResult: path:ACD, cost=17\n</code></pre>\n\n<p>A* found the path ACD but optimal path is ABD.</p>\n', 'ViewCount': '53', 'Title': 'Optimality of A*', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-02T18:54:04.303', 'LastEditDate': '2014-04-02T18:54:04.303', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16375', 'Tags': '<algorithms><artificial-intelligence><search-algorithms>', 'CreationDate': '2014-04-02T16:58:12.097', 'FavoriteCount': '1', 'Id': '23351''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '477', 'Title': 'Best solutions to 6 degrees of separation', 'LastEditDate': '2014-04-06T21:52:34.007', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11306', 'FavoriteCount': '2', 'Body': '<p>From purely my knowledge of computer science a simple breadth first search from root A in search of node B, while keeping track of the depth of the tree, would be the most effective way to check whether A and B have 6 degrees of separation. If I simply wanted to check whether B is within 6 degrees I could also limit my depth to 6. </p>\n\n<p>I have heard however that there are better ways of doing this using bidirectional methods which involve some heuristics. I was wondering if someone could explain the most effective way of doing this and compare space and time complexity between the different approaches. Thanks!</p>\n\n<p>And as a followup, what would be a good algorithm for finding the degree of separation between two arbitrary nodes A and B and the path between them?</p>\n', 'Tags': '<algorithms><graphs><search-algorithms>', 'LastEditorUserId': '11306', 'LastActivityDate': '2014-04-06T21:52:34.007', 'CommentCount': '3', 'AcceptedAnswerId': '23479', 'CreationDate': '2014-04-06T18:03:22.393', 'Id': '23477''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm looking for public algorithm which gives the engine these abilities:</p>\n\n<ul>\n<li>Query by ranked terms</li>\n<li>Limit outcome by date/time range</li>\n</ul>\n\n<p>Basically, i'd like to concentrate articles (generally <code>title|text|timestamp</code>) identify the source and make N-N correlation to terms (is term for datasource same marking as term for dataentry?)</p>\n\n<p>Given the database of such information</p>\n\n<pre><code>entry_data_type:[type_id|title|description]\nentry_data:[entry_id|data_type_id|data_content]\nentry:[id|entry_type(data,source)|parent_entry_id|created|updated]\nterms(keywords):[id|keyword]\nentry2term:[entry_id|term_id|term_weight]\n</code></pre>\n\n<p>Where keywords are both automatically defined (text frequency analysis) and manually assigned (probably abstract terms in context to entry contents)</p>\n\n<p>I should be able to query by keywords like this: <code>kw1:3 kw2:10 kw3:-2 [range:-7 days]</code><br>\nand output shall be entries sorted by given keyword weights (pattern <code>keyword:weight</code>)</p>\n\n<p>I thought about something similar to EdgeRank, but that is social-graph-oriented, and I'm looking for more straight-forward solution (more selfish, meaning input filter is given by personal preferences, not social-graph-near preferences or social-score ranking)</p>\n\n<p>Also TF-IDF would have to be limited by time, so the document base to calculate the entry score is inserted in given date/time range only. Is there any possible break-down of TF-IDF ranking, eg. to pre-calculate raw-data for each day and then, based on query, merge them for given date-range?</p>\n\n<p>This question is independent of any particular programming language, platform, etc. I'm generally looking for keywords to look for, papers to read or ready implementations to study, but accepted are only answers not using paid or closed-source software parts or non-public-domain patents.</p>\n", 'ViewCount': '16', 'Title': 'TF-IDF query engine in context of terms weight', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-07T16:53:08.100', 'LastEditDate': '2014-04-07T16:53:08.100', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16077', 'Tags': '<reference-request><search-algorithms><statistics><search-problem><ranking>', 'CreationDate': '2014-04-07T01:30:29.197', 'Id': '23494''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p><a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a> is often quoted as being used to find the shortest path route however I was surprised to know that there exist A* search which is a extension of Dijkstra\'s algorithm. </p>\n\n<p>How is it that A* search algorithm is able to perform better compared to Dijkstra\'s algorithm , what sort of technique does it used that Dijkstra\'s algorithm did not use ???</p>\n', 'ViewCount': '84', 'Title': "How is A* search superior to Dijkstra's algorithm", 'LastActivityDate': '2014-04-13T01:53:10.240', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2014-04-10T02:18:11.010', 'Id': '23619''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>The <a href="http://en.wikipedia.org/wiki/K-d_tree" rel="nofollow">link</a> in wikipedia about kd-trees store points in the inner nodes. I have to perform NN queries and I <strong>think</strong> (newbie here), I am understanding the concept.</p>\n\n<p>However, I was said to study Kd-trees from Computational Geometry Algorithms and Applications (De Berg, Cheong, Van Kreveld and Overmars), section 5.2, page 99. The main difference I can see is that Overmars places the splitting data in the inner nodes and the actual points of the dataset in the leaves. For example, in 2D, an inner node will hold the splitting line.</p>\n\n<p>Wikipedia on the other hand, seems to store points in inner nodes and leaves (while Overmars only on leaves).</p>\n\n<p>In this case, how do we perform nearest neighbour search? Moreover, why there is this difference?</p>\n', 'ViewCount': '36', 'Title': 'kd-tree stores points in inner nodes? If yes, how to search for NN?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T14:43:00.437', 'LastEditDate': '2014-04-10T14:43:00.437', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16638', 'Tags': '<data-structures><computational-geometry><search-algorithms><search-trees><nearest-neighbour>', 'CreationDate': '2014-04-10T13:20:27.113', 'Id': '23636''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose we have an $N \\times N \\times N$ 3-d sorted array meaning that every row,column, and file is in sorted order. Searching for an element in this structure can be done using $O(N^2)$ comparisons. However are $\\Omega(N^2)$ comparisons needed in the worst-case? For an $N \\times N$ 2-d sorted array I recall a proof that $\\Omega(N)$ comparisons are needed; I'm having trouble seeing how to extend to the 3-d case though</p>\n", 'ViewCount': '24', 'Title': 'Lower bound on number of comparisons needed to search for a number in a sorted 3-d array', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-14T18:45:49.407', 'LastEditDate': '2014-04-14T18:28:24.510', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23793', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><arrays><lower-bounds>', 'CreationDate': '2014-04-14T17:58:35.807', 'Id': '23791''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>How would one search for a string of digits in a large digit sequence? For example, I'd like to search for <code>351814</code> in Euler's number. I'm not too keen on computer science, I'm a pure math major, so I don't really know how to begin. I also wouldn't know how to run said code.   </p>\n\n<p><strong>Any help would be greatly appreciated.</strong></p>\n", 'ViewCount': '85', 'ClosedDate': '2014-04-27T11:41:19.013', 'Title': 'Searching for a string of numbers in a large digit sequence', 'LastEditorUserId': '17106', 'LastActivityDate': '2014-04-27T11:41:04.027', 'LastEditDate': '2014-04-27T05:40:11.397', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16971', 'Tags': '<algorithms><search-algorithms><strings><binary-search>', 'CreationDate': '2014-04-22T00:40:36.540', 'Id': '24009''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Let $G$ be a undirected simple graph with 1000 nodes. My aim to enumerate all minimum cuts in the graph, but we have exponential many minimum cuts. So instead of enumerating all minimum cuts can we do the following task in polynomial time.</p>\n\n<ol>\n<li>Enumerating first $n^2$ mincuts in increasing order of size. </li>\n</ol>\n', 'ViewCount': '18', 'ClosedDate': '2014-04-25T07:05:28.190', 'Title': 'Finding the minimum size of mincut in graph', 'LastEditorUserId': '6522', 'LastActivityDate': '2014-04-27T16:37:41.930', 'LastEditDate': '2014-04-27T16:37:41.930', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<algorithms><graphs><search-algorithms>', 'CreationDate': '2014-04-25T06:13:20.707', 'Id': '24096''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to implement Tabu search, as shown <a href="http://www.cleveralgorithms.com/nature-inspired/stochastic/tabu_search.html" rel="nofollow">here</a>, for my problem. I am now stuck on how to generate the tabu list. In algorithm it mentions that if cost of the candidate is better than the best, then make candidate same as best, and populate tabu list with FeatureDifference(best, candidate). Thus the function gets duplicate copies of argument with different name best and candidate. Also I am not sure how to interpret or implement the function FeatureDifference(best, candidate), because except for name there is no description.</p>\n\n<p>I do understand that this function will differ from problem to problem, but an example would make it clearer. The ruby code on the page does show an implementation of traveling salesman problem in ruby, but what they do is just add edges of the best candidate to the tabu list, and I am not able to make the connection.</p>\n\n<p>If anybody can explain what is FeatureDifference(best, candidate), with an example that would be great. Because I am not able to get given only the current and best solution what goes into the tabu list.</p>\n\n<p>Think found <a href="http://docs.jboss.org/drools/release/latest/optaplanner-docs/html_single/index.html#tabuSearch" rel="nofollow">something</a> that could help, thanks to <a href="http://stackoverflow.com/users/472109/geoffrey-de-smet">Geoffrey De Smet</a> from the <a href="http://stackoverflow.com/questions/20368048/solving-travelling-salesman-with-tabu-search">discussion</a>.</p>\n', 'ViewCount': '32', 'Title': 'How to populate the tabu list in tabu search?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-27T11:38:54.203', 'LastEditDate': '2014-04-27T11:38:54.203', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17094', 'Tags': '<algorithms><search-algorithms><heuristics>', 'CreationDate': '2014-04-26T15:40:00.323', 'Id': '24130''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I just completed a detailed theoretical mathematical study about the PageRank algorithm, I want to make a sample implementation to make some tests and play with it, supposing that I have a local set/folder full of a several webpages, representing a mini Web, I want to make a small software where I can make searches by implementing the PageRank algorithm (with some modifications in the algorithm) and make tests and simulations.</p>\n\n<p>From what I know, I should crawl and index those web pages, then play with my ranking.</p>\n\n<p>Is there a solution that provide me crawling and indexing and spares me the effort of working on them so I can focus on my main problem ?</p>\n', 'ViewCount': '10', 'ClosedDate': '2014-04-29T20:54:41.253', 'Title': 'Steps to implement PageRank Algorithm on a local set of webpages', 'LastActivityDate': '2014-04-29T12:21:55.330', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14915', 'Tags': '<search-algorithms><searching>', 'CreationDate': '2014-04-29T12:21:55.330', 'Id': '24218''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm trying to implement <code>PageRank</code> algorithm on a set of web pages, for that I need a sample <code>dataset</code> of web pages, and the web graph corresponding to them, this web graph represents the links between the pages that the data set contains.</p>\n\n<p>I need the web graph so I can get the transition matrix and do the calculation needed</p>\n\n<p>Example : </p>\n\n<pre><code>      URL1 -&gt; URL2\n      URL3390-&gt;URL5\n</code></pre>\n\n<p>and the URLxxxx as an id, is mapped somehow to the corresponding web page so I can know how to map.</p>\n\n<p>My question is: how/where can I get this resource (I've tried many links on the internet but nothing really helps), I would also like it to be not of a very large size, (internet connexion limitation), if I can't have this as it is, advice me what should I do ?</p>\n", 'ViewCount': '8', 'ClosedDate': '2014-04-30T09:05:45.803', 'Title': 'Where to get a web graph with corresponding web pages dataset', 'LastActivityDate': '2014-04-29T23:06:59.003', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14915', 'Tags': '<search-algorithms><searching>', 'CreationDate': '2014-04-29T23:06:59.003', 'Id': '24242''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Consider the problem of finding the string "He is a scary @#$!&amp;$#ing moron" in next Sunday\'s New York Times, which is available to you in electronic form. The Times is available uncompressed, with each letter represented by a byte, using the standard ASCII character codes for numbers 0 to 127. \nYou decide in the first pass use the KMP search algorithm to find all occurrences of "moron" in binary format.</p>\n\n<p>Compute the prefix function for the binary expansion of the word moron. The ASCII character codes for capital letters A-Z start at 65 increasing sequentially to 90 and for lower case letters a-z start at 97 increasing sequentially to 122. Thus, the ASCII codes for the letters are: m = 109, o =111, r = 114, n = 110 in decimal or m = 155, o = 157, r = 162, n = 156 in octal.</p>\n\n<pre><code>#include &lt;stdio.h&gt;  \n#define CHAR_SET 256  \nint main(void)  {  \nint i;  for(i=0; i&lt;CHAR_SET; i++) \nprintf(``%d-th ASIC character, %c, is %o in octal\\n\'\',i,i,i); \nreturn 0;\n}  \n</code></pre>\n', 'ViewCount': '17', 'ClosedDate': '2014-05-02T12:47:01.557', 'Title': 'Use KMP Search Algorithim to find the string', 'LastActivityDate': '2014-05-01T22:11:45.383', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17261', 'Tags': '<algorithms><search-algorithms>', 'CreationDate': '2014-05-01T22:11:45.383', 'Id': '24311''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}