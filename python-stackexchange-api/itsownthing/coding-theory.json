{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In coding theory, \'how good a code is\' means how many channel errors can be corrected, or better put, the maximal noise level that the code can deal with.</p>\n\n<p>In order to get better codes, the codes are designed using a large alphabet (rather than binary one). And then, the code is good if it can deal with a large rate of erroneous "symbols".</p>\n\n<p><strong>Why isn\'t this consider cheating?</strong> I mean, shouldn\'t we only care about what happens when we "translate" each symbol into a binary string?  The "rate of bit error" is different than the rate of "symbol error". For instance, the rate of bit-error cannot go above 1/2 while (if I understand this correctly), with large enough alphabet, the symbol-error can go up to $1-\\epsilon$. Is this because we <em>artificially</em> restrict the channel to change only "symbols" rather than bits, or is it because the code is actually better?</p>\n', 'ViewCount': '141', 'Title': 'Error-correcting rate is misleading', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-25T14:48:07.313', 'LastEditDate': '2012-03-25T14:48:07.313', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '157', 'Tags': '<information-theory><coding-theory>', 'CreationDate': '2012-03-24T03:54:28.110', 'FavoriteCount': '1', 'Id': '726'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been struggling with several Hamming code/error detection questions because the logic behind it doesn\'t seem to make sense.</p>\n\n<p>eg.1</p>\n\n<p><img src="http://i.stack.imgur.com/v9F4w.png" alt="enter image description here"></p>\n\n<p>eg.2</p>\n\n<p><img src="http://i.stack.imgur.com/tGxIZ.png" alt="enter image description here"></p>\n\n<p>I don\'t really understand the above two examples and the calculations taking place. </p>\n\n<p>How were the conclusions reached concerning the categories of <strong>bin position (dec/bin), parity/data bit and value</strong> in <strong>e.g 1</strong>?</p>\n\n<p>Secondly I don\'t understand the process taking place in <strong>e.g. 2</strong> at all. Does it follow that:</p>\n\n<blockquote>\n  <p>General rule: For any code C,</p>\n  \n  <ul>\n  <li>errors of less than $d(C)$ bits can be detected,</li>\n  <li>errors of less than $d(C)/2$ bits can be corrected.</li>\n  </ul>\n  \n  <p>Definition: A code C with $d(C) \\geq 3$ is called error-correcting.</p>\n</blockquote>\n\n<p>If this is correct then how would I put it into practice. Would really appreciate some assistance!</p>\n', 'ViewCount': '340', 'Title': 'Encoding the sequence 0110 and determining parity, data bit and value', 'LastEditorUserId': '92', 'LastActivityDate': '2012-04-23T17:13:30.037', 'LastEditDate': '2012-04-23T12:54:22.773', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1165', 'Tags': '<coding-theory>', 'CreationDate': '2012-04-23T07:32:14.593', 'FavoriteCount': '2', 'Id': '1458'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $a \\neq b$ be two integers from the interval $[1, 2^n].$ Let $p$ be a random prime with $ 1 \\le p \\le n^c.$ Prove that\n$$\\text{Pr}_{p \\in \\mathsf{Primes}}\\{a \\equiv  b \\pmod{p}\\} \\le c \\ln(n)/(n^{c-1}).$$</p>\n\n<p>Hint: As a consequence of the prime number theorem, exactly $n/ \\ln(n) \\pm o(n/\\ln(n))$ many numbers from $\\{ 1, \\ldots, n \\}$ are prime.</p>\n\n<p>Conclusion: we can compress $n$ bits to $O(\\log(n))$ bits and get a quite small false-positive rate.</p>\n\n<p>My question is how can i proove that $$\\text{Pr}_{p \\in \\mathsf{Primes}}\\{a \\equiv  b \\pmod{p}\\} \\le c \\ln(n)/(n^{c-1})$$?</p>\n', 'ViewCount': '118', 'Title': 'Prove fingerprinting', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-06T23:19:40.733', 'LastEditDate': '2012-05-06T22:16:55.400', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '1704', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<probability-theory><information-theory><coding-theory><number-theory>', 'CreationDate': '2012-05-06T18:34:00.527', 'Id': '1692'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been looking for a way to represent the <a href="http://en.wikipedia.org/wiki/Golden_ratio_base" rel="nofollow">golden ratio ($\\phi$) base</a> more efficiently in binary.  The standard binary golden ratio notation works but is horribly space inefficient.  The  Balanced Ternary Tau System (BTTS) is the best I\'ve found but is quite obscure.  The paper describing it in detail is <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.880" rel="nofollow">A. Stakhov, Brousentsov\'s Ternary Principle, Bergman\'s Number System and Ternary Mirror-symmetrical Arithmetic, 2002</a>.  It is covered in less depth by <a href="http://neuraloutlet.wordpress.com/tag/ternary-tau-system/" rel="nofollow">this blog post</a>.</p>\n\n<p>BTTS is a <a href="http://en.wikipedia.org/wiki/Balanced_ternary" rel="nofollow">balanced ternary representation</a> that uses $\\phi^2 = \\phi + 1$ as a base and 3 values of $\\bar 1$ ($-1$), $0$, and $1$ to represent addition or subtraction of powers of $\\phi^2$.  The table on page 6 of the paper lists integer values from 0 up to 10, and it can represent any $\\phi$-based number as well.</p>\n\n<p>BTTS has some fascinating properties, but being ternary, I didn\'t think I\'d be able to find a compact bit representation for it.</p>\n\n<p>Then I noticed that because of the arithmetic rules, the pattern $\\bar 1 \\bar 1$ never occurs as long as you only allow numbers $\\ge 0$.  This means that the nine possible combinations for each pair of trits ($3^2$) only ever has 8 values, so we can encode 2 trits with 3 bits ($2^3$, a.k.a octal).  Also note that the left-most bit (and also right-most for integers because of the mirror-symmetric property) will only ever be $0$ or $1$ (again for positive numbers only), which lets us encode the left-most trit with only 1 bit.</p>\n\n<p>So a $2^n$-bit number can store $\\lfloor 2^n/3\\rfloor * 2 + 1$ balanced trits, possibly with a bit left over (maybe a good candidate for a sign bit).  For example, we can represent $10 + 1 = 11$ balanced trits with $15 + 1 = 16$  bits, or $20 + 1 = 21$ balanced trits with $30 + 1 = 31$ bits, with 1 left over (32-bit).  This has much better space density than ordinary golden ratio base binary encoding.</p>\n\n<p>So my question is, what would be a good octal (3-bit) encoding of trit pairs such that we can implement the addition and other arithmetic rules of the BTTS with as little difficulty as possible.  One of the tricky aspects of this system is that carries happen in both directions, i.e. <br/>\n$1 + 1 = 1 \\bar 1 .1$ and $\\bar 1 + \\bar 1 = \\bar 1 1.\\bar 1$.</p>\n\n<p>This is my first post here, so please let me know if I need to fix or clarify anything.</p>\n\n<p>--<strong>Edit</strong>--</p>\n\n<p>ex0du5 asked for some clarification of what I need from a binary representation:</p>\n\n<ol>\n<li>I want to be able to represent positive values of both integers and powers of $\\phi$.  The range of representable values need not be as good as binary, but it should be better than phinary per bit.  I want to represent the largest possible set of phinary numbers in the smallest amount of space possible.  Space takes priority over operation count for arithmetic operations.</li>\n<li>I need addition to function such that carries happen in both directions.  Addition will be the most common operation for my application.  Consequently it should require as few operations as possible.  If a shorter sequence of operations are possible using a longer bit representation (conflicting with goal 1), then goal 1 takes priority.  Space is more important than speed.</li>\n<li>Multiplication only needs to handle integers > 0 multiplied to a phinary number, not arbitrary phinary number multiplication, and so can technically be emulated with a series of additions, though a faster algorithm would be helpful.</li>\n<li>I\'m ignoring division and subtraction for now, but having algorithms for them would be a bonus.</li>\n<li>I need to eventually convert a phinary number to a binary floating point approximation of it\'s value, but this will happen only just prior to output.  There will be no converting back and forth.</li>\n</ol>\n', 'ViewCount': '294', 'Title': 'What is a good binary encoding for $\\phi$-based balanced ternary arithmetic algorithms?', 'LastEditorUserId': '2230', 'LastActivityDate': '2012-11-18T06:37:19.827', 'LastEditDate': '2012-07-26T14:22:13.457', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2230', 'Tags': '<algorithms><data-structures><efficiency><coding-theory>', 'CreationDate': '2012-07-20T21:32:29.663', 'Id': '2847'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '313', 'Title': 'No compression algorithm can compress all input messages?', 'LastEditDate': '2012-12-21T09:47:42.073', 'AnswerCount': '3', 'Score': '5', 'OwnerDisplayName': 'Jack M', 'PostTypeId': '1', 'OwnerUserId': '5153', 'Body': '<p>I just started reading a book called Introduction to Data Compression, by Guy E. Blelloch. On page one, he states:</p>\n\n<blockquote>\n  <p>The truth is that if any one message is shortened by an algorithm, then some other message needs to be lengthened. You can verify this in practice by running GZIP on a GIF file. It is, in fact, possible to go further and show that for a set of input messages of fixed length, if one message is compressed, then the average length of the compressed messages over all possible inputs is always going to be longer than the original input messages. </p>\n  \n  <p><strong>Consider, for example, the 8 possible 3 bit messages. If one is compressed to two bits, it is not hard to convince yourself that two messages will have to expand to 4 bits, giving an average of 3 1/8 bits.</strong></p>\n</blockquote>\n\n<p>Really? I find it very hard to convince myself of that. In fact, here\'s a counter example. Consider the algorithm which accepts as input any 3-bit string, and maps to the following outputs:</p>\n\n<pre><code>000 -&gt; 0\n001 -&gt; 001\n010 -&gt; 010\n011 -&gt; 011\n100 -&gt; 100 \n101 -&gt; 101\n110 -&gt; 110\n111 -&gt; 111\n</code></pre>\n\n<p>So there you are - no input is mapped to a longer output. There are certainly no "two messages" that have expanded to 4 bits.</p>\n\n<p>So what exactly is the author talking about? I suspect either there\'s some implicit caveat that just isn\'t obvious to me, or he\'s using language that\'s far too sweeping.</p>\n\n<p><em>Disclaimer:</em> I realize that if my algorithm is applied iteratively, you do indeed lose data. Try applying it twice to the input 110: 110 -> 000 -> 0, and now you don\'t know which of 110 and 000 was the original input. However, if you apply it only once, it seems lossless to me. Is that related to what the author\'s talking about?</p>\n', 'Tags': '<data-compression><coding-theory>', 'LastEditorUserId': '1636', 'LastActivityDate': '2012-12-22T02:24:58.747', 'CommentCount': '2', 'AcceptedAnswerId': '7532', 'CreationDate': '2012-12-21T00:55:54.680', 'Id': '7531'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '91', 'Title': 'Application of Expander Codes', 'LastEditDate': '2013-02-14T18:42:46.433', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'smoes', 'PostTypeId': '1', 'OwnerUserId': '5327', 'Body': '<p>I need to give a talk about <a href="http://en.wikipedia.org/wiki/Expander_code">expander codes</a> at university (I\'m a student of computer science). Since they have been introduced to show a family of codes looking good when thinking of the <a href="http://en.wikipedia.org/wiki/Noisy-channel_coding_theorem">Shannon theorem</a>, I wonder what real world application for expander codes exist.</p>\n\n<p>As far as I know one has a real hard time when encoding but decoding is quite fast. Why aren\'t they used to encode write only media or similar? What is their big disadvantage?</p>\n', 'Tags': '<graph-theory><combinatorics><coding-theory><expanders>', 'LastEditorUserId': '41', 'LastActivityDate': '2013-02-14T22:04:27.807', 'CommentCount': '0', 'AcceptedAnswerId': '9785', 'CreationDate': '2013-01-06T13:47:18.513', 'Id': '7816'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a set of strings.  My goal is to find a minimal set of longest prefixes which will match most of that set.</p>\n\n<p>For instance, if my set is:</p>\n\n<pre><code>abracadabra\nabracado\nabramu\nbanana\nbananasplit\nbananaicecream\nxylophone\nzebra\nzeitgeist\nzello\n</code></pre>\n\n<p>I would want the output to be:</p>\n\n<pre><code>banana (len 6, matches 3)\nabra (len 4, matches 3)\nxylophone (len 9, matches 1)\nze (len 2, matches 3)\n</code></pre>\n\n<p>Now, this question isn't yet properly specified.  That's because I'm ranking my results on two dimensions: maximum matches, and maximum length.  My goal is to find prefixes that cover as much as of the set as possible, which are as long as possible and thus less likely to occur in strings that <em>aren't</em> in the set (all other things being equal, of course).</p>\n\n<p>Ideally, I'd like to find a set of very long strings, ranked by how much of the set they cover.</p>\n\n<p>That's my goal.  Now I'll present my work, and where I need help.</p>\n\n<p><strong>FIRST</strong> Let's specify the problem better.  We want a set of prefixes.  For each prefix, we compute its length, and the number of matches it has in the set, and order the prefixes by their product.  I'm then free to pick the top X prefixes.</p>\n\n<p>I think that's a good specification.</p>\n\n<p><strong>NOW</strong> Comes an efficient implementation.  Brute force is to check every possible prefix against every string, which is complexity n * n * m (n being number of strings, m being average length of strings).</p>\n\n<p>An efficient algorithm?<br>\nSomething like this:   </p>\n\n<ol>\n<li>Build a prefix tree of the set</li>\n<li>Each leaf has value 1</li>\n<li>Work up the tree, with each parent equal to sum of its children , plus 1 if it has an entry</li>\n<li>Now we know each prefix and how many matches it has - I believe complexity is n log n</li>\n<li>Walk through the tree, counting length of each string (complexity n * m)</li>\n<li>And collect all the entries, sort them by length * value (complexity n log n)</li>\n</ol>\n\n<p>That algorithm is roughly n log n, which is efficient enough.  Will it work? How should it be improved? What's a simple way to implement it? </p>\n\n<p>Finally: Since all the data is in a Postgres relational database, I believe it would be simplest to do the algorithm using relational algebra with aggregate functions. Comments on this?</p>\n", 'ViewCount': '216', 'Title': 'Algorithm for determining minimal set of covering prefixes', 'LastEditorUserId': '1623', 'LastActivityDate': '2013-01-11T14:38:16.920', 'LastEditDate': '2013-01-10T16:50:17.210', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5368', 'Tags': '<algorithms><trees><data-compression><sets><coding-theory>', 'CreationDate': '2013-01-10T13:34:14.843', 'Id': '7868'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would I solve the following can anyone help me.I know MIPS is basically how many instruction the processor can do per second but what should I do?</p>\n\n<p>Assume that we are receiving a message across a network using a modem with a rate of 56,000 bits/second. Furthermore assume that we are working on a workstation with an instruction rate of 500 mips. How many instructions can the processor execute between the receipt of each individual bit of the message?</p>\n', 'ViewCount': '413', 'Title': 'Network modem question', 'LastEditorUserId': '157', 'LastActivityDate': '2013-03-18T01:17:39.627', 'LastEditDate': '2013-03-17T21:29:54.510', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '10589', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<computer-architecture><coding-theory><arithmetic>', 'CreationDate': '2013-03-16T20:33:11.283', 'Id': '10561'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $\\mathbb{F}^n_2$ denote the set of $n$-bit 0-1 strings. How to construct an efficiently computable function $f:\\mathbb{F}^n_2\\to \\mathbb{F}^m_2 (m&gt;n)$ satisfying that $\\forall u\\neq v$,$f(u)\\neq f(v)$ and $f(u)$ is not a circular rotation of $f(v)$?</p>\n\n<p>$m$ is expected to be as small as possible.</p>\n\n<p>Thank you very much for your kindness.</p>\n', 'ViewCount': '88', 'Title': 'Efficiently map different codes to rotation-different codes', 'LastActivityDate': '2013-06-23T15:36:40.950', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '12842', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8297', 'Tags': '<coding-theory>', 'CreationDate': '2013-05-23T07:49:27.720', 'FavoriteCount': '0', 'Id': '12224'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here is my problem. I have to compute the amount of information that is possible to encode in a string of bits. This string of bits represent a stream.\nLet us call such stream as $X_1,X_2,X_3,...,X_n$. Important: this succession of Random variable is not a Markov Process, but a process with memory!!\nI suppose that the right measure would be based on the information entropy of the collection of random variables:\n$H(X_1,X_2,X_3,...,X_n)=\\sum_i H(X_i|X_{i-1},...,X_1)$</p>\n\n<p>Am I right?</p>\n\n<p>However I have more doubts:</p>\n\n<ul>\n<li>If $n \\to \\infty$ then a better measure would be the entropy rate, isn't it?.</li>\n<li>What is meaning of $H(X_n)$, i.e. the entropy of only the last random variable?</li>\n<li>In term of information theory, what is the meaning of $H(X_1, X_n)$ or $H(X_1| X_n)$ ?</li>\n</ul>\n\n<p>A point is that I can compute both $H(X_1, X_n)$ and $H(X_1| X_n)$ for my process, but the computation of $H(X_1,X_2,X_3,...,X_n)$ remains very hard, as the process is non markovian.</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '70', 'Title': 'Information of a stream of bits', 'LastEditorUserId': '8353', 'LastActivityDate': '2013-05-28T03:30:45.200', 'LastEditDate': '2013-05-25T18:53:15.680', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-1', 'OwnerDisplayName': 'altroware', 'PostTypeId': '1', 'OwnerUserId': '8353', 'Tags': '<information-theory><coding-theory>', 'CreationDate': '2013-05-25T15:47:53.660', 'Id': '12273'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How do you find the minimum hamming distance of a code?<br>\nA naive way is computing the distance of each pair of codewords in our code.  </p>\n\n<p>It becomes hard when the code is sufficiently large. Is there a formula for minimum hamming distance? </p>\n', 'ViewCount': '53', 'Title': 'Compute minimum hamming distance of a code', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-30T15:19:23.487', 'LastEditDate': '2014-01-30T15:19:23.487', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '20110', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13229', 'Tags': '<string-metrics><coding-theory>', 'CreationDate': '2014-01-30T13:45:36.770', 'Id': '20105'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have a little understanding problem with Appendix A ("Universal Codes") in the paper "Shannon Information and Kolmogorov complexity" by Gr\xfcndwald and Vitanyi (<a href="http://arxiv.org/pdf/cs/0410002.pdf" rel="nofollow">Link</a>).  </p>\n\n<p>At the end of page 50, they say something corresponding to:  </p>\n\n<blockquote>\n  <p>For each Prefix-code, the fraction of sequences of length $n$ that can be compressed by more than $m$ bits is less than $2^{-m}$. </p>\n</blockquote>\n\n<p>Either this is a writing mistake or I should make a break from reading.</p>\n\n<p>It is easy to see, that the number of strings with length $n$ and compression length smaller than $m$ is smaller $2^{m-n}$ for a prefix-code because there are simple not enough "shorter" words.. Did they mean this information?  </p>\n\n<p>It is also easy to understand, that the Kraft-Inequality $\\sum_{x\\in X}2^{-l(x)}\\leq1$ holds for a prefix code  with source word set $X$ and the compression length $l(x)$ for strings $x\\in X$.  </p>\n\n<p>Is that only a writing mistake? Respectively can you tell me an explanation?</p>\n', 'ViewCount': '31', 'Title': 'How many sequences in a prefix code can be compressed by m bits?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T08:18:44.117', 'LastEditDate': '2014-01-31T08:18:44.117', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '20139', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '13249', 'Tags': '<coding-theory>', 'CreationDate': '2014-01-30T15:51:38.603', 'Id': '20114'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We have 2 stations that communicate with each other, but we need to detect (or even correct) when something is wrong.</p>\n\n<p>We use 8 binary words: each consisting of 3 bits <img src="http://i.stack.imgur.com/JbRtH.gif" alt="enter image description here"> and to send it we code it as \n<img src="http://i.stack.imgur.com/u2nA8.gif" alt="enter image description here">  where <img src="http://i.stack.imgur.com/4tjIk.gif" alt="enter image description here"> is the complement of <img src="http://i.stack.imgur.com/mdnEQ.gif" alt="enter image description here"> and <img src="http://i.stack.imgur.com/BMDVh.gif" alt="enter image description here"> is the even parity check bit of <img src="http://i.stack.imgur.com/0msIo.gif" alt="enter image description here"> .</p>\n\n<p>We need to find the capabilities of this code (up to how many can we detect and how many can we correct). BUT, a <strong>proof</strong> is required.</p>\n\n<p>This is how far I\'ve reached so far:</p>\n\n<p>First we find the hamming distance:\nIf <img src="http://i.stack.imgur.com/mdnEQ.gif" alt=""> changes then <img src="http://i.stack.imgur.com/4tjIk.gif" alt=""> changes, also <img src="http://i.stack.imgur.com/BMDVh.gif" alt=""> changes. So we have a hamming distance of 3.</p>\n\n<p>This means that we can detect two bit errors <strong>or</strong> correct a single error. </p>\n\n<p>Can you help me write the proof for that?</p>\n\n<p>(also the tags may need some refinement - comments about the downvote are welcome)</p>\n', 'ViewCount': '53', 'Title': 'Error detection/correction algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-06T15:52:06.677', 'LastEditDate': '2014-02-04T09:49:52.927', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'user2692669', 'PostTypeId': '1', 'Tags': '<coding-theory><error-correcting-codes>', 'CreationDate': '2014-02-03T17:43:37.823', 'Id': '20276'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Knowing the frequencies of each symbol, is it possible to determine the maximum height of the tree without applying the Huffman algorithm? Is there a formula that gives this tree height?</p>\n', 'ViewCount': '94', 'Title': 'Huffman tree and maximum depth', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-10T21:50:18.413', 'LastEditDate': '2014-02-10T21:50:18.413', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8767', 'Tags': '<trees><coding-theory>', 'CreationDate': '2014-02-06T18:16:44.847', 'FavoriteCount': '1', 'Id': '21394'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to understand the Huffman compression algorithm.</p>\n\n<p>Lets assume the word : <strong>YESSSS</strong></p>\n\n<p>According to Huffman tree we will get :</p>\n\n<ul>\n<li>S : 4 times -> Code : 0</li>\n<li>Y : once    -> Code : 01</li>\n<li>E : once    -> Code : 00</li>\n</ul>\n\n<p>at the end YESSSS will become : <strong>01 00 0 0 0 0</strong></p>\n\n<p>So far everything is clear.</p>\n\n<p>Now my problem is in the space between the binary words. How this can be stored in memory ?</p>\n\n<p>In another words ?</p>\n\n<p>How to computer will know that that :</p>\n\n<ul>\n<li>the first character has two bits</li>\n<li>the second character has two bits</li>\n<li>the fourth other characters has only one bit</li>\n</ul>\n\n<p>Because <strong>01 00 0 0 0 0</strong> doesn't have the same meaning than <strong>01 00 00 00</strong></p>\n\n<ul>\n<li><strong>01 00 0 0 0 0</strong> means : YESSSS</li>\n<li><strong>01 00 00 00</strong> means : YEEE</li>\n</ul>\n\n<p>Any ideas please ?</p>\n", 'ViewCount': '43', 'Title': 'How to determine letter boundaries in Huffman encoded strings?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-18T21:33:42.400', 'LastEditDate': '2014-02-18T21:33:42.400', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'Matthiew', 'PostTypeId': '1', 'Tags': '<coding-theory>', 'CreationDate': '2014-02-18T04:12:46.680', 'Id': '21778'}}