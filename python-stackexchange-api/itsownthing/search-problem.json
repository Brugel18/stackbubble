{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is known that each optimization/search problem has an equivalent decision problem. For example the shortest path problem</p>\n\n<blockquote>\n  <ul>\n  <li><strong>optimization/search version:</strong>\n  Given an undirected unweighted graph $G = (V, E)$ and two vertices $v,u\\in V(G)$, find a shortest path between $v$ and $u$.</li>\n  <li><strong>decision version:</strong> \n  Given an undirected unweighted graph $G = (V, E)$, two vertices $v,u\\in V(G)$ and a non-negative integer $k$. Is there a path in $G$ between $u$ and $v$ whose length is at most $k$?</li>\n  </ul>\n</blockquote>\n\n<p>In general, "Find $x^*\\in X$ s.t. $f(x^*) = \\min\\{f(x)\\mid x\\in X\\}$!" becomes "Is there $x\\in X$ s.t. $f(x) \\leq k$?". </p>\n\n<p>But is the reverse also true, i.e. is there an equivalent optimization problem for every decision problem? If not, what is an example of a decision problem that has no equivalent optimization problem?</p>\n', 'ViewCount': '1009', 'Title': 'Optimization version of decision problems', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-21T15:32:42.343', 'LastEditDate': '2012-04-01T08:47:48.317', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '15', 'OwnerDisplayName': 'bek', 'PostTypeId': '1', 'Tags': '<complexity-theory><optimization><search-problem><decision-problem>', 'CreationDate': '2012-03-31T06:30:08.680', 'FavoriteCount': '5', 'Id': '939'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a collection $P \\subseteq \\mathbb{R}^3$ of $N$ particles and there is a function $f : P^2 \\to \\mathbb{R}$. I want to find which configuration of the system minimizes the value of $f$. </p>\n\n<p>Can this problem (or similar ones) be reduced to TSP? Could you point me to literature on the topic?</p>\n\n<p>In my application, $f$ is the <a href="https://en.wikipedia.org/wiki/Van_der_Waals_force" rel="nofollow">atomic van der waals force</a>, which for each pair of particles of atoms is attractive or repulsive depending on some predefined thresholds.</p>\n\n<p>In addition, it would be great to have a list of concrete examples of problems that can be reduced to TSP.</p>\n', 'ViewCount': '135', 'Title': 'Complexity of an optimisation problem in 3D', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T15:44:16.733', 'LastEditDate': '2012-04-22T15:44:16.733', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1167', 'Tags': '<complexity-theory><optimization><search-problem>', 'CreationDate': '2012-04-20T11:26:44.303', 'FavoriteCount': '2', 'Id': '1388'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m reading through the <a href="http://www.cs.rmit.edu.au/AI-Search/Courseware/Slides1/" rel="nofollow">RMIT course notes on state space search</a>.\nConsider a state space $S$, a set of nodes in which we look for an element having a certain property.\nA <a href="http://www.cs.rmit.edu.au/AI-Search/Courseware/Slides1/07ImprovedMethods/07bHeurFunctions/" rel="nofollow">heuristic function</a> $h:S\\to\\mathbb{R}$ measures how promising a node is.</p>\n\n<p>$h_2$ is said to <em>dominate</em> (or to be more informed than) $h_1$ if $h_2(n) \\ge h_1(n)$ for every node $n$. How does this definition imply that using $h_2$ will lead to expanding fewer nodes? - not only fewer but subset of the others.</p>\n\n<p>In Luger \'02 I found the explanation:</p>\n\n<blockquote>\n  <p>This can be verified by assuming the opposite (that there is at least one state expanded by $h_2$ and not by $h_1$). But since $h_2$ is more informed than $h_1$, for all $n$, $h_2(n) \\le h_1(n)$, and both are bounded above by $h^*$, our assumption is contradictory. </p>\n</blockquote>\n\n<p>But I didn\'t quite get it.</p>\n', 'ViewCount': '384', 'Title': 'Is using a more informed heuristic guaranteed to expand fewer nodes of the search space?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-09T10:35:14.547', 'LastEditDate': '2012-05-09T10:35:14.547', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1304', 'Tags': '<artificial-intelligence><heuristics><search-problem>', 'CreationDate': '2012-04-29T19:28:57.290', 'FavoriteCount': '1', 'Id': '1579'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '182', 'Title': 'How do I classify my emulator input optimization problem, and with which algorithm should I approach it?', 'LastEditDate': '2012-05-10T05:41:50.040', 'AnswerCount': '2', 'Score': '9', 'OwnerDisplayName': 'GManNickG', 'PostTypeId': '1', 'OwnerUserId': '1436', 'FavoriteCount': '1', 'Body': '<p>Due to the nature of the question, I have to include lots of background information (because my question is: how do I narrow this down?) That said, it can be summarized (to the best of my knowledge) as:</p>\n\n<p><strong>What methods exist to find local optimums on extremely large combinatorial search spaces?</strong></p>\n\n<h2>Background</h2>\n\n<p>In the tool-assisted superplay community we look to provide specially-crafted (not generated in real-time) input to a video game console or emulator in order to minimize some cost (usually time-to-completion). The way this is currently done is by playing the game frame-by-frame and specifying the input for each frame, often redoing parts of the run many times (for example, the <a href="http://tasvideos.org/2020M.html">recently published</a> run for <em>The Legend of Zelda: Ocarina of Time</em> has a total of 198,590 retries).</p>\n\n<p><strong>Making these runs obtain their goal usually comes down to two main factors: route-planning and traversal.</strong> The former is much more "creative" than the latter.</p>\n\n<p>Route-planning is determining which way the player should navigate overall to complete the game, and is often the most important part of the run. This is analogous to choosing which sorting method to use, for example. The best bubble sort in the world simply isn\'t going to outperform a quick-sort on 1 million elements.</p>\n\n<p>In the desire for perfection, however, traversal (how the route is carried out) is also a huge factor. Continuing the analogy, this is how the sorting algorithm is implemented. Some routes can\'t even be performed without very specific frames of input. This is the most tedious process of tool-assisting and is what makes the production of a completed run takes months or even years. It\'s not a <em>difficult</em> process (to a human) because it comes down to trying different variations of the same idea until one is deemed best, but humans can only try so many variations in their attention-span. The application of machines to this task seems proper here.</p>\n\n<p><strong>My goal now is to try to automate the traversal process in general for the Nintendo 64 system</strong>. The search space for this problem is <em>far</em> too large to attack with a brute-force approach. An n-frame segment of an N64 run has 2<sup>30n</sup> possible inputs, meaning a mere 30 frames of input (a second at 30FPS) has 2<sup>900</sup> possible inputs; it would be impossible to test these potential solutions, let alone those for a full two-hour run.</p>\n\n<p>However, I\'m not interested in attempting (or rather, am not going to even try to attempt) total global optimization of a full run. Rather, <strong>I would like to, given an initial input, approximate the <em>local</em> optimum for a particular <em>segment</em> of a run (or the nearest <em>n</em> local optimums, for a sort of semi-global optimization)</strong>. That is, given a route and an initial traversal of that route: search the neighbors of that traversal to minimize cost, but don\'t degenerate into trying all the cases that could solve the problem.</p>\n\n<p>My program should therefore take a starting state, an input stream, an evaluation function, and output the local optimum by minimizing the result of the evaluation.</p>\n\n<h2>Current State</h2>\n\n<p>Currently I have all the framework taken care of. This includes evaluating an input stream via manipulation of the emulator, setup and teardown, configuration, etc. And as a placeholder of sorts, the optimizer is a very basic genetic algorithm. It simply evaluates a population of input streams, stores/replaces the winner, and generates a new population by mutating the winner stream. This process continues until some arbitrary criteria is met, like time or generation number.</p>\n\n<p><strong>Note that the slowest part of this program will be, by far, the evaluation of an input stream</strong>. This is because this involves emulating the game for <em>n</em> frames. (If I had the time I\'d write my own emulator that provided hooks into this kind of stuff, but for now I\'m left with synthesizing messages and modifying memory for an existing emulator from another process.) On my main computer, which is fairly modern, evaluating 200 frames takes roughly 14 seconds. As such, I\'d prefer an algorithm (given the choice) that minimizes the number of function evaluations.</p>\n\n<p>I\'ve created a system in the framework that manages emulators concurrently. As such <strong>I can evaluate a number of streams at once</strong> with a linear performance scale, but practically speaking the number of running emulators can only be 8 to 32 (and 32 is really pushing it) before system performance deteriorates. This means (given the choice), an algorithm which can do processing while an evaluation is taking place would be highly beneficial, because the optimizer can do some heavy-lifting while it waits on an evaluation.</p>\n\n<p>As a test, my evaluation function (for the game <em>Banjo Kazooie</em>) was to sum, per frame, the distance from the player to a goal point. This meant the optimal solution was to get as close to that point as quickly as possible. Limiting mutation to the analog stick only, it took a day to get an <em>okay</em> solution. (This was before I implemented concurrency.)</p>\n\n<p>After adding concurrency, I enabled mutation of A button presses and did the same evaluation function at an area that required jumping. With 24 emulators running it took roughly 1 hour to reach the goal from an initially blank input stream, but would probably need to run for days to get to anything close to optimal.</p>\n\n<h2>Problem</h2>\n\n<p><strong>The issue I\'m facing is that I don\'t know enough about the mathematical optimization field to know how to properly model my optimization problem</strong>! I can roughly follow the conceptual idea of many algorithms as described on Wikipedia, for example, but I don\'t know how to categorize my problem or select the state-of-the-art algorithm for that category.</p>\n\n<p><strong>From what I can tell, I have a combinatorial problem with an extremely large neighborhood</strong>. On top of that, <strong>the evaluation function is extremely discontinuous, has no gradient, and has many plateaus</strong>. Also, there aren\'t many constraints, though I\'ll gladly add the ability to express them if it helps solve the problem; I would like to allow specifying that the Start button should not be used, for example, but this is not the general case.</p>\n\n<h2>Question</h2>\n\n<p><strong>So my question is: how do I model this? What kind of optimization problem am I trying to solve? Which algorithm am I suppose to use?</strong> I\'m not afraid of reading research papers so let me know what I should read!</p>\n\n<p>Intuitively, a genetic algorithm couldn\'t be the best, because it doesn\'t really seem to learn. For example, if pressing Start seems to <em>always</em> make the evaluation worse (because it pauses the game), there should be some sort of designer or brain that learns: "pressing Start at any point is useless." But even this goal isn\'t as trivial as it sounds, because sometimes pressing start <em>is</em> optimal, such as in so-called "pause backward-long-jumps" in <em>Super Mario 64</em>! Here the brain would have to learn a much more complex pattern: "pressing Start is useless except when the player is in this very specific state <em>and will continue with some combination of button presses</em>." </p>\n\n<p>It seems like I should (or the machine could learn to) represent input in some other fashion more suited to modification. Per-frame input seems too granular, because what\'s really needed are "actions", which may span several frames...yet many discoveries are made on a frame-by-frame basis, so I can\'t totally rule it out (the aforementioned pause backward-long-jump requires frame-level precision). It also seems like the fact that input is processed serially should be something that can be capitalized on, but I\'m not sure how.</p>\n\n<p><strong>Currently I\'m reading about (Reactive) Tabu Search, Very Large-scale Neighborhood Search, Teaching-learning-based Optimization, and Ant Colony Optimization.</strong></p>\n\n<p>Is this problem simply too hard to tackle with anything other than random genetic algorithms? Or is it actually a trivial problem that was solved long ago? Thanks for reading and thanks in advance for any responses.</p>\n', 'Tags': '<reference-request><machine-learning><combinatorics><optimization><search-problem>', 'LastEditorUserId': '1436', 'LastActivityDate': '2014-01-19T16:02:17.470', 'CommentCount': '2', 'AcceptedAnswerId': '2947', 'CreationDate': '2012-05-09T06:34:52.220', 'Id': '1774'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I try to solve the following coverage problem.</p>\n\n<blockquote>\n  <p>There are $n$ transmitters with coverage area of 1km and $n$ receivers. Decide in $O(n\\log n)$ that all receivers  are covered by any transmitter. All reveivers and transmiters are represented by their $x$ and $y$ coordinates.</p>\n</blockquote>\n\n<p>The most advanced solution I can come with takes $O(n^2\\log n)$. For every receiver sort all transmitter by it distance to this current receiver, then take the transmitter with shortest distance and this shortest distance should be within 0.5 km.</p>\n\n<p>But the naive approach looks like much better in time complexity $O(n^2)$. Just compute all distance between all pairs of transmitter and receiver.</p>\n\n<p>I am not sure if I can apply range-search algorithms in this  problem. For example kd-trees allow us to find such ranges, however I never saw an example, and I am not sure if there are kind of range-search for circles. </p>\n\n<p>The given complexity $O(n\\log n)$ assumes that the solution should be somehow similar to sorting.</p>\n', 'ViewCount': '220', 'Title': 'Coverage problem (transmitter and receiver)', 'LastEditorUserId': '157', 'LastActivityDate': '2012-09-14T14:03:11.483', 'LastEditDate': '2012-05-25T06:24:54.113', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '1170', 'Tags': '<algorithms><computational-geometry><search-problem>', 'CreationDate': '2012-05-25T06:06:23.963', 'FavoriteCount': '3', 'Id': '2069'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m a none-computer-science-student and get some knowledge on AI by taking the CS188.1x Course (Artificial Intelligence) on www.edx.org .</p>\n\n<p>Currently, I am working on the "Search in Pacman" Project; the sources can be found online at <a href="http://www-inst.eecs.berkeley.edu/~cs188/pacman/projects/search/search.html" rel="nofollow">Berkley CS188</a> . I have problems finding an good solution for "Finding All the Corners", so I need a good Multiple Goal Heuristic.</p>\n\n<p>I allready tried the simple approach described in <a href="http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html" rel="nofollow">here</a>. I used the minimum of all manhattan distances to all goals. This works, but is considered a rather poor heuistic, because my A-Star Algorithm expands 2606 nodes for the given maze. Using the same with euclidean distance expands even 103081 nodes. A good heuristic should expand 1600 nodes or less. A very good one 1200 nodes, an excellent one even 800 or less.</p>\n\n<p>I got a hint by other students who use minimum spanning trees created with Kruskal\'s Algorithm. I wanted to investigate into that direction, but I am somehow confused how the Kruskal Algorithm can be used to get a Heuristic?\nAs far as I understood, this Algorithm returns a minimum spanning tree (MST) which is a path, right? So it is a solution to the Traveling Salesman Problem (TSP); it returns a sequence of nodes. But I need a heuristic, so a cost function which can be applied to this problem and called by an Algorithm (like A*).</p>\n\n<p>Can anyone of you give me a hint on how to proceed? Every help is highly appreciated!</p>\n', 'ViewCount': '1068', 'Title': 'Heuristic for Finding Multiple Goals in Graph - e.g. using Kruskals Algorithm', 'LastActivityDate': '2013-01-15T21:48:58.973', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'OwnerDisplayName': 'EliteTUM', 'PostTypeId': '1', 'Tags': '<algorithms><graph-theory><heuristics><search-problem>', 'CreationDate': '2012-10-09T13:24:05.047', 'Id': '6208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '121', 'Title': 'BPP search: what does boosting correctness entail?', 'LastEditDate': '2012-11-07T10:22:52.953', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4488', 'FavoriteCount': '2', 'Body': u'<p>It is not really clear to me, how and if I can do boosting for correctness (or error reduction) on a <a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29" rel="nofollow">BPP</a> (bounded-error probabilistic polynomial-time) search problem. Can anyone of you explain me how it works?</p>\n\n<p>With BPP search, I mean a problem that can have false positive-negative, correct solution, and no-solution. Here\'s a definition:</p>\n\n<p>A probabilistic polynomial-time algorithm $A$ solves the search problem of the relation $R$ if</p>\n\n<ul>\n<li>for every $x \u2208 S$, $Pr[A(x) \u2208 R(x)] &gt; 1 - \u03bc(|x|)$</li>\n<li>for every $x \u2209 SR$, $Pr[A(x) = \\text{no-solution}] &gt; 1 - \u03bc(|x|)$</li>\n</ul>\n\n<p>were $R(x)$ is the set of solution for the problem and $\u03bc(|x|)$ is a negligible function (it is rare that it fails).</p>\n\n<p>So now I would like to increase my probability of getting a good answer, how can I do it?</p>\n\n<hr>\n\n<p>~ ".. boosting for correctness.." : a way to increase the probability of the algorithm (generally by multile runs of the probabilistic algorithm), i.e., when the problem have a solution then the algorithm likely return a valid one.</p>\n', 'Tags': '<probabilistic-algorithms><search-problem>', 'LastEditorUserId': '4221', 'LastActivityDate': '2012-11-08T06:49:27.833', 'CommentCount': '3', 'AcceptedAnswerId': '6537', 'CreationDate': '2012-11-06T08:05:34.063', 'Id': '6503'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is the first step in solving a "tough" algorithmic problem always asking whether it\'s hard in the sense that other tough problems can be reduced to it? Not to make the scope of this question tight, what\'s good advice for approaching challenging algorithmic problems?</p>\n', 'ViewCount': '145', 'Title': 'Solving algorithmic problems', 'LastEditorUserId': '2499', 'LastActivityDate': '2013-01-29T16:55:23.057', 'LastEditDate': '2012-12-31T12:11:01.390', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<algorithms><decision-problem><search-problem>', 'CreationDate': '2012-12-31T12:03:23.277', 'Id': '7663'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that we take an initial search problem and we add $c &gt; 0$ to the costs on all edges. Will <a href="http://en.wikipedia.org/wiki/Uniform-cost_search" rel="nofollow">uniform-cost search</a> return the same answer as in the initial search problem?</p>\n\n<p>Definitions: Uniform-cost search is also known as lowest cost first. Initial search problem can be any graph with a start and a goal state. You just apply the uniform cost search algorithm on the graph. </p>\n', 'ViewCount': '426', 'Title': 'Uniform-cost Search Problem', 'LastEditorUserId': '867', 'LastActivityDate': '2013-11-19T07:11:47.707', 'LastEditDate': '2013-01-29T03:40:50.913', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6601', 'Tags': '<graphs><search-algorithms><search-trees><search-problem>', 'CreationDate': '2013-01-29T02:33:45.660', 'Id': '9265'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is NOT HW, this is from Skienas book, and I just couldn't solve it at all.</p>\n\n<p>Please give me a hand here, in understanding and solving it, thanks.</p>\n\n<p>Let G = (V, E) be a binary tree. The distance between two vertices in G is the length of the path connecting these two vertices, and the diameter of G is the maximal distance over all pairs of vertices. Give a linear-time algorithm to find the diameter of a given tree. (*)</p>\n\n<p>I figured I'd do a DFS, and increment on each node in terms of the depth of the tree</p>\n", 'ViewCount': '51', 'ClosedDate': '2013-04-21T23:06:21.807', 'Title': 'LInear time algorithm to find the diameter of a tree', 'LastEditorUserId': '139', 'LastActivityDate': '2013-04-21T23:43:53.940', 'LastEditDate': '2013-04-21T19:20:21.703', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><search-problem>', 'CreationDate': '2013-04-21T17:47:36.077', 'Id': '11470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This question is an extension of <a href="http://cs.stackexchange.com/questions/12305/complexity-of-an-algorithm-for-bounding-a-region-in-2d">a previous question</a> I\'ve asked.</p>\n\n<p>Consider the rectangle $a&lt;x&lt;b , c&lt;y&lt;d$ in the $\\mathbf R^2$ plane. Each point in this rectangle can be of kind #1 or #2 (We have to check each point to know its kind).</p>\n\n<p>Assume that somehow we know that the points of kind 1 (and so the points of kind 2) form a connected region (i.e. , 1s and 2s are not scattered in the plane arbitrarily). Given the condition of being of kind 1 or 2, The goal is to find the region occupied by 1s (a <em>search</em> problem). Consider somehow we know the following attributes of the region occupied by 1s: (one at a time)</p>\n\n<ol>\n<li>The region occupied by 1s forms a convex set (so it is <a href="http://en.wikipedia.org/wiki/1-connected" rel="nofollow">1-connected</a> too).\n<img src="http://i.stack.imgur.com/xKMBb.png" alt="enter image description here"></li>\n</ol>\n\n<p>2.The region occupied by 1s forms a simply connected region , but not necessarily convex.</p>\n\n<p><img src="http://i.stack.imgur.com/7fId9.png" alt="enter image description here"></p>\n\n<p>The simplest algorithm for finding the region of 1s is to simply start from bottom of the rectangle and sweep it and check all of the points in the rectangle to determine their kind and this way find the region.This is not an efficient algorithm, because we can use the known fact of convexity (or simply-connectivity) of the region of 1s to find it more easily without inspecting all of the points.</p>\n\n<p>What more efficient algorithms are there to find the region , as fast as possible? (with an acceptable accuracy, which is about 0.001 in my work). The regions may have sharp edges. But their detection is limited to the mentioned accuracy too. (It is clear that finding the boundary of the region suffices)</p>\n\n<p>Please don\'t forget that the problem is <em>to find an unknown set of points</em>, not <em>bound a known set of points</em>. i.e., <strong>it\'s a search problem , not a convex hull finding problem.</strong></p>\n\n<p><strong>(also, speed is very important for me)</strong></p>\n\n<p><strong>EDIT1:</strong> </p>\n\n<p>After some suggestions (in the comments) I should say that I think we can take advantage of simply-connectivity of the region to write an algorithm that tries to find the boundary of the region instead of checking more points to find the region directly.</p>\n', 'ViewCount': '105', 'Title': 'Efficient algorithms for finding a region in $\\mathbf R^2$', 'LastEditorUserId': '8381', 'LastActivityDate': '2013-05-29T00:24:16.510', 'LastEditDate': '2013-05-28T23:26:13.947', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8381', 'Tags': '<algorithms><computational-geometry><search-problem>', 'CreationDate': '2013-05-28T20:28:05.040', 'Id': '12343'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Well, i have a binary search tree $T$ that is equilibrated by height witch has $2^d+c$ nodes ($c&lt;2^d$). \nWhat is the number of comparisons that will occur in the worst case scenario, if we ask whether $k\\in V(T)$ and why does it arise?</p>\n', 'ViewCount': '274', 'Title': 'Worst case scenario in binary search tree retrieval', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-14T21:57:25.843', 'LastEditDate': '2013-06-14T12:13:40.983', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'Mihai Alin', 'PostTypeId': '1', 'Tags': '<graph-theory><search-trees><search-problem>', 'CreationDate': '2013-06-13T18:46:27.367', 'Id': '12667'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I can't seem to find this stated explicitly anywhere, which makes me wonder if I have it all wrong.</p>\n\n<p>So first, let's say we view problems in NP as degenerate problems in FNP, where the codomain of the binary relation is the set {true, false}.</p>\n\n<p>Second, FSAT is known to be FNP-complete, meaning that everything in FNP can be reduced to it in polynomial time. Also, FSAT is polynomial-time reducible to SAT, which is NP-complete, and then a SAT problem can be changed to anything else in NP-complete in polynomial time. So this shows that everything in FNP-complete can be changed to something in NP-complete in polynomial time.</p>\n\n<p>So the first thing shows that NP-complete $\\subset$ FNP-complete, but then the second thing shows that FNP-complete $\\subset$ NP-complete, which means that NP-complete = FNP-complete.</p>\n\n<p>So given that, it seems like everything in FNP can be reduced to any NP-complete problem in polynomial-time.</p>\n\n<p>Am I going somewhere wrong here, or do I have this all right?</p>\n", 'ViewCount': '67', 'Title': 'Does FNP-complete = NP-complete?', 'LastActivityDate': '2013-10-09T00:41:48.237', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10594', 'Tags': '<complexity-theory><np-complete><decision-problem><search-problem>', 'CreationDate': '2013-10-09T00:41:48.237', 'Id': '14937'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '150', 'Title': 'Artificial Intelligence: Condition for BFS being optimal', 'LastEditDate': '2013-11-18T19:12:31.383', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6998', 'FavoriteCount': '1', 'Body': '<p>It is said in the book <em>Artificial Intelligence: A Modern Approach</em> for finding a solution on a tree using BFS that: </p>\n\n<blockquote>\n  <p>breadth-first search is optimal if the path cost is a nondecreasing function of the\n  depth of the node. The most common such scenario is that all actions have the same cost.</p>\n</blockquote>\n\n<p>From that I understand that if the path cost is non decreasing function of depth, the BFS algorithm returns an optimal solution, i.e., <strong>the only condition is the cost function being nondecreasing</strong>. But I think the only way for BFS to be optimal is the scenario in which all the path costs are identical, therefore a node found in a certain level is necessarily the optimal solution, as, if they exist, the others are. Therefore I think for BFS to be optimal, cost function should be non decreasing <strong>AND</strong> the costs of nodes should be identical. However, the book says only one of the conditions (former one) makes BFS optimal.</p>\n\n<p>Is there a situation in which the costs are not identical, the cost function is nondecreasing and the solution returned by BFS is guaranteed to be optimal?</p>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms><search-trees><search-problem>', 'LastEditorUserId': '6998', 'LastActivityDate': '2013-11-18T19:12:31.383', 'CommentCount': '0', 'AcceptedAnswerId': '16780', 'CreationDate': '2013-11-06T01:16:28.697', 'Id': '16758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>Assume having a graph $G_{variables}=(V,U)$ where $V=\\{v_1,v_2,\u2026,v_n\\}$ is a set of variables; each variable $v_i\\in V$ is associated with a set of possible values (it's domain) $dom(v_i)$. </p>\n\n<p>Let $P$ be a search problem (i.e reachability problem) over graph $G=(O,E)$ where $O$ is the cartesian product of the variables domains. Let $T$ be a junction tree resulted from $G_{variables}$. $P$ can be also solved through searching every clique in $T$. I am looking for keywords/examples of such problems. $G_{variables}$ preferably to be DAG.  </p>\n", 'ViewCount': '30', 'Title': 'Search problems that can also be solved by junction trees and searching cliques', 'LastEditorUserId': '4598', 'LastActivityDate': '2014-01-10T04:15:00.893', 'LastEditDate': '2014-01-10T04:15:00.893', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<graph-theory><reference-request><search-algorithms><search-problem>', 'CreationDate': '2014-01-09T17:48:23.083', 'Id': '19602'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for papers/methods (or at least problem examples) where the original search problem $P$ can be solved by either:</p>\n\n<ol>\n<li>Searching through the original graph. or</li>\n<li>By decomposing it into several subset of problems $P_1,P_2, \\dots,P_n$.</li>\n</ol>\n\n<p>Ideally $sol(P )=sol(P_1)\\cup sol(P_2)\\cup \\ldots \\cup(P_n)$ with no preprocessing (i.e the union of the subproblems correspond directly to the solution of the problem).  </p>\n\n<p>I have no constraint; though prefer the underlying graph to be a DAG and the problem to be a reachability problem. Google seems to fail on finding such papers. </p>\n', 'ViewCount': '74', 'Title': 'Decomposing the search problem into several small problems', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-19T12:32:29.493', 'LastEditDate': '2014-01-17T21:36:52.663', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '19804', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<graph-theory><reference-request><search-algorithms><search-problem>', 'CreationDate': '2014-01-17T18:59:51.023', 'Id': '19792'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '25', 'Title': 'How feasible is it for a non-distributed web crawler running on consumer hardware to search the internet?', 'LastEditDate': '2014-01-22T20:12:45.690', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2605', 'Body': '<p>I am looking for an automated way to answer the question: what are the URLs on the world wide web that contain at least two strings from a set of strings. </p>\n\n<p>So if I have a set of strings {"A", "B" and "C"} -- I want to know, what pages on the world wide web contain "A" and B", "A" and "C", "B" and "C" or "A", "B" and "C."</p>\n\n<p>Obviously, for this simple example: Google it! </p>\n\n<p>But I want a scaleable, automated, and free solution. Google does <a href="https://support.google.com/webmasters/answer/66357?hl=en" rel="nofollow">not permit</a> automated queries. Yahoo makes you pay.</p>\n\n<p>One idea I have is (1) start with a URL, (2) check the text at that URL for the search strings (3) parse out the links from the text (4) record that you have checked the page and if it contains the strings then (5) search the links from the initial URL. Repeat until you have searched the tree. </p>\n\n<p>How feasible is this in terms of time and space on a single commodity machine -- given the size the internet? The internet is really, really big -- but only a comparatively few pages will contain these strings (they are proper names). </p>\n\n<p>I don\'t want to index the whole web as if my laptop were google!</p>\n\n<p>Most of the crawler\'s time will be spent confirming that the pages don\'t contain the strings. </p>\n\n<p>I\'m trying to get a rough ballpark to understand if this is even remotely feasible. </p>\n', 'ClosedDate': '2014-01-22T21:35:12.443', 'Tags': '<algorithm-analysis><search-problem><searching>', 'LastEditorUserId': '2605', 'LastActivityDate': '2014-01-22T20:47:27.130', 'CommentCount': '3', 'AcceptedAnswerId': '19902', 'CreationDate': '2014-01-22T19:00:09.453', 'Id': '19898'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '64', 'Title': 'Time complexity of 8-queen, by placeing one by one without attack', 'LastEditDate': '2014-02-22T09:23:17.930', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14922', 'FavoriteCount': '1', 'Body': '<p>I am new to artificial intelligence. I have been trying to analyse the time complexity of 8-queen, by placing one by one without attack.</p>\n\n<p>One approach to achieve goal state is to "add a queen to any square in the leftmost empty column such that it is not attacked by any other queen". And this approach will have a state space of 2057 (also wondering: How to compute this?)</p>\n\n<p>What is the time complexity if I am using Depth First search algorithm (which I think is the most suitable one)? How about the space complexity?</p>\n\n<p>I am puzzled because the brunching of the search tree is reducing greatly when goes deep. $O(8^8)$ looks too much for time complexity, even for worst case.</p>\n', 'Tags': '<time-complexity><search-problem><board-games>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-22T09:23:17.930', 'CommentCount': '2', 'AcceptedAnswerId': '21908', 'CreationDate': '2014-02-21T20:05:40.270', 'Id': '21905'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for public algorithm which gives the engine these abilities:</p>\n\n<ul>\n<li>Query by ranked terms</li>\n<li>Limit outcome by date/time range</li>\n</ul>\n\n<p>Basically, i'd like to concentrate articles (generally <code>title|text|timestamp</code>) identify the source and make N-N correlation to terms (is term for datasource same marking as term for dataentry?)</p>\n\n<p>Given the database of such information</p>\n\n<pre><code>entry_data_type:[type_id|title|description]\nentry_data:[entry_id|data_type_id|data_content]\nentry:[id|entry_type(data,source)|parent_entry_id|created|updated]\nterms(keywords):[id|keyword]\nentry2term:[entry_id|term_id|term_weight]\n</code></pre>\n\n<p>Where keywords are both automatically defined (text frequency analysis) and manually assigned (probably abstract terms in context to entry contents)</p>\n\n<p>I should be able to query by keywords like this: <code>kw1:3 kw2:10 kw3:-2 [range:-7 days]</code><br>\nand output shall be entries sorted by given keyword weights (pattern <code>keyword:weight</code>)</p>\n\n<p>I thought about something similar to EdgeRank, but that is social-graph-oriented, and I'm looking for more straight-forward solution (more selfish, meaning input filter is given by personal preferences, not social-graph-near preferences or social-score ranking)</p>\n\n<p>Also TF-IDF would have to be limited by time, so the document base to calculate the entry score is inserted in given date/time range only. Is there any possible break-down of TF-IDF ranking, eg. to pre-calculate raw-data for each day and then, based on query, merge them for given date-range?</p>\n\n<p>This question is independent of any particular programming language, platform, etc. I'm generally looking for keywords to look for, papers to read or ready implementations to study, but accepted are only answers not using paid or closed-source software parts or non-public-domain patents.</p>\n", 'ViewCount': '16', 'Title': 'TF-IDF query engine in context of terms weight', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-04-07T16:53:08.100', 'LastEditDate': '2014-04-07T16:53:08.100', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16077', 'Tags': '<reference-request><search-algorithms><statistics><search-problem><ranking>', 'CreationDate': '2014-04-07T01:30:29.197', 'Id': '23494'}},