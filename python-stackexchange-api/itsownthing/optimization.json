{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Initially, <a href="http://en.wikipedia.org/wiki/Matroid">Matroids</a> were introduced to generalize the notions of linear independence of a collection of subsets $E$ over some ground set $I$. Certain problems that contain this structure permit greedy algorithms to find optimal solutions. The concept of <a href="http://en.wikipedia.org/wiki/Greedoid">Greedoids</a> was later introduced to generalize this structure to capture more problems that allow for optimal solutions to be found by greedy methods.</p>\n\n<p>How often do these structures arise in algorithm design? </p>\n\n<p>Furthermore, more often than not a greedy algorithm will not be able to fully capture what is necessary to find optimal solutions, but may still find very good approximate solutions (Bin Packing for example). Given that, is there a way to measure how "close" a problem is to a greedoid/matroid?</p>\n', 'ViewCount': '366', 'Title': 'How fundamental are matroids and greedoids in algorithm design?', 'LastActivityDate': '2012-03-08T03:56:04.670', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '124', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '19', 'Tags': '<algorithms><combinatorics><optimization>', 'CreationDate': '2012-03-08T01:48:58.297', 'FavoriteCount': '5', 'Id': '119'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '410', 'Title': 'Analyzing a modified version of the card-game "War"', 'LastEditDate': '2012-03-25T20:54:36.950', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '1', 'Body': '<p>A simple game usually played by children, the game of War is played by two people using a standard deck of 52 playing cards. Initially, the deck is shuffled and all cards are dealt two the two players, so that each have 26 random cards in a random order. We will assume that players are allowed to examine (but not change) both decks, so that each player knows the cards and orders of cards in both decks. This is typically note done in practice, but would not change anything about how the game is played, and helps keep this version of the question completely deterministic.</p>\n\n<p>Then, players reveal the top-most cards from their respective decks. The player who reveals the larger card (according to the usual order: 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace) wins the round, placing first his card (the high card) at the bottom of his deck, and then his opponent\'s card (the low card) at the bottom of the deck (typically, the order of this isn\'t enforced, but to keep the first version of this question deterministic, such an ordering will be enforced).</p>\n\n<p>In the event of a tie, each player reveals four additional cards from the top of their decks. If the fourth card shown by one player is higher than the fourth card shown by another player, the player with the higher fourth card wins all cards played during the tie-breaker, in which case the winner\'s cards are first placed at the bottom of the winner\'s deck (in first-in, first-out order; in other words, older cards are placed at the bottom first), followed by the loser\'s cards (in the same order).</p>\n\n<p>In the event of subsequent ties, the process is repeated until a winner of the tie is determined. If one player runs out of cards and cannot continue breaking the tie, the player who still has cards is declared the winner. If both players run out cards to play at the same time the game is declared a tie.</p>\n\n<p>Rounds are played until one player runs out of cards (i.e., has no more cards in his deck), at which point the player who still has cards is declared the winner.</p>\n\n<p>As the game has been described so far, neither skill nor luck is involved in determining the outcome. Since there are a finite number of permutations of 52 cards, there are a finite number of ways in which the decks may be initially dealt, and it follows that (since the only state information in the game is the current state of both players\' decks) the outcome of each game configuration can be decided a priori. Certainly, it is possibly to win the game of War, and by the same token, to lose it. We also leave open the possibility that a game of War might result in a Tie or in an infinite loop; for the completely deterministic version described above, such may or may not be the case.</p>\n\n<p>Several variations of the game which attempt to make it more interesting (and no, not all involve making it into a drinking game). One way which I have thought of to make the game more interesting is to allow players to declare automatic "trumps" at certain rounds. At each round, either player (or both players) may declare "trump". If one player declares "trump", that player wins the round regardless of the cards being played. If both players declare "trump", then the round is treated as a tie, and play continues accordingly.</p>\n\n<p>One can imagine a variety of rules limiting players\' ability to trump (unlimited trumping would always result in a Tie game, as players would trump every turn). I propose two versions (just off the top of my head; more interesting versions along these lines are probably possible) of War based on this idea but using different trump limiting mechanisms:</p>\n\n<ol>\n<li>Frequency-War: Players may only trump if they have not trumped in the previous $k$ rounds.</li>\n<li>Revenge-War: Players may only trump if they have not won a round in the previous $k$ rounds.</li>\n</ol>\n\n<p>Now for the questions, which apply to each of the versions described above:</p>\n\n<blockquote>\n  <ol>\n  <li>Is there a strategy such that, for some set of possible initial game configurations, the player using it always wins (strongly winning strategy)? If so, what is this strategy? If not, why not?</li>\n  <li>Is there a strategy such that, for some set of possible initial game configurations, the player using it can always win or force a tie (winning strategy)? If so, what is this strategy? If not, why not?</li>\n  <li>Are their initial game configurations such that there is no winning strategy (i.e., a player using any fixed strategy $S$ can always be defeated by a player using fixed strategy $S\'$)? If so, what are they, and explain?</li>\n  </ol>\n</blockquote>\n\n<p>To be clear, I am thinking of a "strategy" as a fixed algorithm which determines at what rounds the player using the strategy should trump. For instance, the algorithm "trump whenever you can" is a strategy, and an algorithm (a heuristic algorithm). Another way of what I\'m asking is this:</p>\n\n<blockquote>\n  <p>Are there any good (or provably optimal) heuristics for playing these games?</p>\n</blockquote>\n\n<p>References to analyses of such games are appreciated (I am unaware of any analysis of this version of War, or of essentially equivalent games). Results for any $k$ are interesting and appreciated (note that, in both cases, $k=0$ leads to unlimited trumping, which I have already discussed).</p>\n', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-29T16:31:04.390', 'CommentCount': '2', 'AcceptedAnswerId': '872', 'CreationDate': '2012-03-12T18:58:22.227', 'Id': '248'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The simplex algorithm walks greedily on the corners of a polytope to find the optimal solution to the linear programming problem. As a result, the answer is always a corner of the polytope. Interior point methods walk the inside of the polytope. As a result, when a whole plane of the polytope is optimal (if the objective function is exactly parallel to the plane), we can get a solution in the middle of this plane.</p>\n\n<p>Suppose that we want to find a corner of the polytope instead. For example if we want to do maximum matching by reducing it to linear programming, we don\'t want to get an answer consisting of "the matching contains 0.34% of the edge XY and 0.89% of the edge AB and ...". We want to get an answer with 0\'s and 1\'s (which simplex would give us since all corners consist of 0\'s and 1\'s). Is there a way to do this with an interior point method that guarantees to find exact corner solutions in polynomial time? (for example perhaps we can modify the objective function to favor corners)</p>\n', 'ViewCount': '414', 'Title': 'Finding exact corner solutions to linear programming using interior point methods', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-05T12:33:33.760', 'LastEditDate': '2012-03-23T22:39:23.917', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '771', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '700', 'Tags': '<algorithms><optimization><linear-programming>', 'CreationDate': '2012-03-23T20:54:01.227', 'Id': '706'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is known that each optimization/search problem has an equivalent decision problem. For example the shortest path problem</p>\n\n<blockquote>\n  <ul>\n  <li><strong>optimization/search version:</strong>\n  Given an undirected unweighted graph $G = (V, E)$ and two vertices $v,u\\in V(G)$, find a shortest path between $v$ and $u$.</li>\n  <li><strong>decision version:</strong> \n  Given an undirected unweighted graph $G = (V, E)$, two vertices $v,u\\in V(G)$ and a non-negative integer $k$. Is there a path in $G$ between $u$ and $v$ whose length is at most $k$?</li>\n  </ul>\n</blockquote>\n\n<p>In general, "Find $x^*\\in X$ s.t. $f(x^*) = \\min\\{f(x)\\mid x\\in X\\}$!" becomes "Is there $x\\in X$ s.t. $f(x) \\leq k$?". </p>\n\n<p>But is the reverse also true, i.e. is there an equivalent optimization problem for every decision problem? If not, what is an example of a decision problem that has no equivalent optimization problem?</p>\n', 'ViewCount': '1009', 'Title': 'Optimization version of decision problems', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-21T15:32:42.343', 'LastEditDate': '2012-04-01T08:47:48.317', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '15', 'OwnerDisplayName': 'bek', 'PostTypeId': '1', 'Tags': '<complexity-theory><optimization><search-problem><decision-problem>', 'CreationDate': '2012-03-31T06:30:08.680', 'FavoriteCount': '5', 'Id': '939'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a test about the <a href="http://en.wikipedia.org/wiki/Branch_and_bound">branch and bound</a> algorithm. I understand theoretically how this algorithm works but I couldn\'t find examples that illustrates how this algorithm can be implemented practically. </p>\n\n<p>I found some examples such as <a href="http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html">this one</a>\nbut I\'m still confused about it. I also looked for travelling salesman problem and I couldn\'t understand it.</p>\n\n<p>What I need is some problems and how can these problems solved by using branch and bound.</p>\n', 'ViewCount': '2222', 'Title': 'Branch and Bound explanation', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-25T17:00:01.013', 'LastEditDate': '2012-10-14T09:28:44.170', 'AnswerCount': '2', 'CommentCount': '11', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '701', 'Tags': '<algorithms><optimization><branch-and-bound>', 'CreationDate': '2012-04-04T00:31:20.227', 'FavoriteCount': '4', 'Id': '1016'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am currently working on a solution to a problem for which (after a bit of research) the use of a hill climbing, and more specificly a <em>shotgun</em> (or <em>random-restart</em>) <a href="http://en.wikipedia.org/wiki/Hill_climbing" rel="nofollow">hill climbing</a> algorithmic idea seems to be the best fit, as I have no clue how the best start value can be found.</p>\n\n<p>But there is not a lot of information about this type of algorithm except the <a href="http://en.wikipedia.org/wiki/Hill_climbing#Variants" rel="nofollow">rudimentary idea</a> behind it:</p>\n\n<blockquote>\n  <p>[Shotgun] hill climbing is a meta-algorithm built on top of the hill climbing algorithm. It iteratively does hill-climbing, each time with a random initial condition $x_0$. The best $x_m$ is kept: if a new run of hill climbing produces a better $x_m$ than the stored state, it replaces the stored state.</p>\n</blockquote>\n\n<p>If I understand this correctly, this means something like this (assuming maximisation):</p>\n\n<pre><code>x = -infinity;\nfor ( i = 1 .. N ) {\n  x = max(x, hill_climbing(random_solution()));\n}\nreturn x;\n</code></pre>\n\n<p>But how can you make this really effective, that is better than normal hill climbing? It is hard to believe that using random start values helps a lot, especially for huge search spaces. More precisely, I wonder:</p>\n\n<ul>\n<li>Is there a good strategy for choosing the $x_0$ (that is implementing <code>random_solution</code>), in particular knowing (intermediate) results of former iterations?</li>\n<li>How to choose $N$, that is how many iterations are needed to be quite certain that the perfect solution is not missed (by much)?</li>\n</ul>\n', 'ViewCount': '396', 'Title': 'How to implement the details of shotgun hill climbing to make it effective?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-02T20:52:19.537', 'LastEditDate': '2012-09-02T15:43:00.100', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1089', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '78', 'Tags': '<algorithms><optimization><heuristics>', 'CreationDate': '2012-04-06T19:13:39.907', 'Id': '1084'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '299', 'Title': 'A continuous optimization problem that reduces to TSP', 'LastEditDate': '2012-04-09T03:13:54.610', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '898', 'FavoriteCount': '0', 'Body': '<p>Suppose I am given a  finite set of points $p_1,p_2,..p_n$ in the plane, and asked to draw a twice-differentiable curve $C(P)$ through the $p_i$\'s, such that its perimeter is as small as possible. Assuming $p_i=(x_i,y_i)$ and $x_i&lt;x_{i+1}$, I can formalize this problem as:</p>\n\n<p><i> Problem 1 (edited in response to Suresh\'s comments) </i>Determine  $C^2$ functions $x(t),y(t)$ of a parameter $t$ such that the arclength $ L = \\int_{[t \\in 0,1]} \\sqrt{x&#39;^2+y&#39;^2}dt$  is minimized, with $x(0) = x_1, x(1) = x_n$ and for all $t_i: x(t_i) = x_i$, we have $y(t_i)=y_i)$. </p>\n\n<blockquote>\n  <p>How do I prove (or perhaps refute) that Problem 1 is NP-hard?</p>\n</blockquote>\n\n<p><i> Why I suspect NP-hardness </i>   Suppose the $C^2$ assumption is relaxed. Evidently, the function of minimal arclength is the Travelling Salesman tour of the $p_i$\'s.  Perhaps the $C^2$ constraint only makes the problem much harder?</p>\n\n<p><i> Context </i> A variant of this problem was posted on <a href="http://math.stackexchange.com/questions/23181/extremal-curve-passing-through-a-set-of-points">MSE</a>. It didn\'t receive an answer both there and on <a href="http://mathoverflow.net/questions/58885/extremal-curves-with-a-should-pass-through-constraint" rel="nofollow">MO</a>. Given that it\'s nontrivial to solve the problem, I want to establish how hard it is. </p>\n', 'Tags': '<complexity-theory><np-hard><optimization><computable-analysis>', 'LastEditorUserId': '898', 'LastActivityDate': '2012-04-10T00:58:18.347', 'CommentCount': '19', 'AcceptedAnswerId': '1185', 'CreationDate': '2012-04-08T20:08:11.753', 'Id': '1142'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '214', 'Title': "Ordering elements so that some elements don't come between others", 'LastEditDate': '2012-05-30T07:24:52.507', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '5', 'Body': '<p>Given an integer $n$ and set of triplets of distinct integers\n$$S \\subseteq \\{(i, j, k) \\mid 1\\le i,j,k \\le n, i \\neq j, j \\neq k, i \\neq k\\},$$\nfind an algorithm which either finds a permutation $\\pi$ of the set $\\{1, 2, \\dots, n\\}$ such that\n$$(i,j,k) \\in S \\implies (\\pi(j)&lt;\\pi(i)&lt;\\pi(k)) ~\\lor~ (\\pi(i)&lt;\\pi(k)&lt;\\pi(j))$$\nor correctly determines that no such permutation exists.  Less formally, we want to reorder the numbers 1 through $n$; each triple $(i,j,k)$ in $S$ indicates that $i$ must appear before $k$ in the new order, but $j$ must not appear between $i$ and $k$.</p>\n\n<p><strong>Example 1</strong></p>\n\n<p>Suppose $n=5$ and $S = \\{(1,2,3), (2,3,4)\\}$.  Then</p>\n\n<ul>\n<li><p>$\\pi = (5, 4, 3, 2, 1)$ is <em>not</em> a valid permutation, because $(1, 2, 3)\\in S$, but $\\pi(1) &gt; \\pi(3)$.</p></li>\n<li><p>$\\pi = (1, 2, 4, 5, 3)$ is <em>not</em> a valid permutation, because $(1, 2, 3) \\in S$ but $\\pi(1) &lt; \\pi(3) &lt; \\pi(5)$.</p></li>\n<li><p>$(2, 4, 1, 3, 5)$ is a valid permutation.</p></li>\n</ul>\n\n<p><strong>Example 2</strong></p>\n\n<p>If $n=5$ and $S = \\{(1, 2, 3), (2, 1, 3)\\}$, there is no valid permutation.  Similarly, there is no valid permutation if $n=5$ and $S = \\{(1,2,3), (3,4,5), (2,5,3), (2,1,4)\\}$  (I think; may have made a mistake here).</p>\n\n<p><em>Bonus: What properties of $S$ determine whether a feasible solution exists?</em></p>\n', 'Tags': '<algorithms><optimization><scheduling>', 'LastEditorUserId': '72', 'LastActivityDate': '2012-05-30T07:24:52.507', 'CommentCount': '5', 'AcceptedAnswerId': '1275', 'CreationDate': '2012-04-13T19:26:19.010', 'Id': '1255'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for algorithms to optimize a strictly monotonic function $f$ such that $f(x) &lt; y$ </p>\n\n<p>$f : [a,b] \\longrightarrow [c,d]\r\n\\qquad \\text{where } [a,b] \\subset {\\mathbb N},  [c,d] \\subset {\\mathbb N}$<br>\nsuch that $\\arg\\max{_x} f(x) &lt; y$</p>\n\n<p>My first idea was to use a variant of binary search, pick a point $x$ in $[a,b]$ at random; if $f(x) &gt; y$ then we eliminate $[x, b]$, and if $f(x) &lt; y$ we eliminate $[a, x]$. We repeat this procedure until the solution is found.</p>\n\n<p>Do you have any other ideas to maximize the function $f$ ?</p>\n', 'ViewCount': '236', 'Title': 'Optimizing a strictly monotone function', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-29T16:10:36.877', 'LastEditDate': '2012-04-19T21:53:36.550', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '6888', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '652', 'Tags': '<algorithms><optimization>', 'CreationDate': '2012-04-19T10:27:02.390', 'Id': '1353'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a collection $P \\subseteq \\mathbb{R}^3$ of $N$ particles and there is a function $f : P^2 \\to \\mathbb{R}$. I want to find which configuration of the system minimizes the value of $f$. </p>\n\n<p>Can this problem (or similar ones) be reduced to TSP? Could you point me to literature on the topic?</p>\n\n<p>In my application, $f$ is the <a href="https://en.wikipedia.org/wiki/Van_der_Waals_force" rel="nofollow">atomic van der waals force</a>, which for each pair of particles of atoms is attractive or repulsive depending on some predefined thresholds.</p>\n\n<p>In addition, it would be great to have a list of concrete examples of problems that can be reduced to TSP.</p>\n', 'ViewCount': '135', 'Title': 'Complexity of an optimisation problem in 3D', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T15:44:16.733', 'LastEditDate': '2012-04-22T15:44:16.733', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1167', 'Tags': '<complexity-theory><optimization><search-problem>', 'CreationDate': '2012-04-20T11:26:44.303', 'FavoriteCount': '2', 'Id': '1388'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to use a color camera to track multiple objects in space. Each object will have a different color and in order to be able to distinguish well between each objects I'm trying to make sure that each color assigned to an object is as different from any color on any other object as possible.</p>\n\n<p>In RGB space, we have three planes, all with values between 0 and 255. In this cube $(0,0,0) / (255,255,255)$, I would like to distribute the $n$ colors so that there is as much distance between themselves and others as possible. An additional restriction is that $(0, 0, 0)$ and $(255, 255, 255)$ (or as close to them as possible) should be included in the $n$ colors, because I want to make sure that none of my $(n-2)$ objects takes either color because the background will probably be one of these colors.</p>\n\n<p>Probably, $n$ (including black and while) will not be more than around 14.</p>\n\n<p>Thanks in advance for any pointers on how to get these colors.  </p>\n", 'ViewCount': '253', 'Title': 'Distribute objects in a cube so that they have maximum distance between each other', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T11:55:42.867', 'LastEditDate': '2012-04-22T11:55:42.867', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '1411', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1176', 'Tags': '<algorithms><optimization><computational-geometry>', 'CreationDate': '2012-04-20T20:56:07.157', 'Id': '1399'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '201', 'Title': 'What is the name of this logistic variant of TSP?', 'LastEditDate': '2012-04-23T14:25:32.307', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '1', 'Body': '<p>I have a logistic problem that can be seen as a variant of $\\text{TSP}$. It is so natural, I\'m sure it has been studied in Operations research or something similar. Here\'s one way of looking at the problem.</p>\n\n<p>I have $P$ warehouses on the Cartesian plane. There\'s a path from a warehouse to every other warehouse and the distance metric used is the Euclidean distance. In addition, there are $n$ different items. Each item $1 \\leq i \\leq n$ can be present in any number of warehouses. We have a collector and we are given a starting point $s$ for it, say the origin $(0,0)$. The collector is given an order, so a list of items. Here, we can assume that the list only contains distinct items and only one of each. We must determine the shortest tour starting at $s$ visiting some number of warehouses so that the we pick up every item on the order.</p>\n\n<p>Here\'s a visualization of a randomly generated instance with $P = 35$. Warehouses are represented with circles. Red ones contain item $1$, blue ones item $2$ and green ones item $3$. Given some starting point $s$ and the order ($1,2,3$), we must pick one red, one blue and one green warehouse so the order can be completed. By accident, there are no multi-colored warehouses in this example so they all contain exactly one item. This particular instance is a case of <a href="http://en.wikipedia.org/wiki/Set_TSP_problem" rel="nofollow">set-TSP</a>.</p>\n\n<p><img src="http://i.stack.imgur.com/5kKsj.png" alt="An instance of the problem."></p>\n\n<p>I can show that the problem is indeed $\\mathcal{NP}$-hard. Consider an instance where each item $i$ is located in a different warehouse $P_i$. The order is such that it contains every item. Now we must visit every warehouse $P_i$ and find the shortest tour doing so. This is equivalent of solving an instance of $\\text{TSP}$.</p>\n\n<p>Being so obviously useful at least in the context of logistic, routing and planning, I\'m sure this has been studied before. I have two questions:</p>\n\n<ol>\n<li>What is the name of the problem?</li>\n<li>How well can one hope to approximate the problem (assuming $\\mathcal{P} \\neq \\mathcal{NP}$)? </li>\n</ol>\n\n<p>I\'m quite happy with the name and/or reference(s) to the problem. Maybe the answer to the second point follows easily or I can find out that myself.</p>\n', 'Tags': '<algorithms><optimization><reference-request><approximation>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-23T18:22:16.660', 'CommentCount': '4', 'AcceptedAnswerId': '1464', 'CreationDate': '2012-04-22T15:35:56.930', 'Id': '1440'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Having extracted the data-flow in some rather large programs as directed, acyclic graphs, I'd now like to optimize the order of evaluation to minimze the maximum amount of memory used.</p>\n\n<p>That is, given a graph {1 -> 3, 2 -> 3, 4 -> 5, 3 -> 5}, I'm looking for an algorithm that will decide the order of graph reduction to minimize the number of 'in-progress' nodes, in this particular case to decide that it should be reduced in the order 1-2-3-4-5; avoiding the alternative ordering, in this case 4-1-2-3-5, which would leave the output from node 4 hanging until 3 is also complete.</p>\n\n<p>Naturally, if there are two nodes using the output from a third, then it only counts once; data is not copied unnecessarily, though it does hang around until both of those nodes are reduced.</p>\n\n<p>I would also quite like to know what this problem is called, if it has a name. It looks similar to the graph bandwidth problem, only not quite; the problem statement may be defined in terms of path/treewidth, but I can't quite tell, and am unsure if I should prioritize learning that branch of graph theory right now.</p>\n", 'ViewCount': '126', 'Title': 'Optimizing order of graph reduction to minimize memory usage', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-10T03:23:26.373', 'LastEditDate': '2012-05-10T03:23:26.373', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1425', 'Tags': '<algorithms><graphs><optimization><software-engineering><program-optimization>', 'CreationDate': '2012-05-09T12:07:24.633', 'FavoriteCount': '1', 'Id': '1752'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '182', 'Title': 'How do I classify my emulator input optimization problem, and with which algorithm should I approach it?', 'LastEditDate': '2012-05-10T05:41:50.040', 'AnswerCount': '2', 'Score': '9', 'OwnerDisplayName': 'GManNickG', 'PostTypeId': '1', 'OwnerUserId': '1436', 'FavoriteCount': '1', 'Body': '<p>Due to the nature of the question, I have to include lots of background information (because my question is: how do I narrow this down?) That said, it can be summarized (to the best of my knowledge) as:</p>\n\n<p><strong>What methods exist to find local optimums on extremely large combinatorial search spaces?</strong></p>\n\n<h2>Background</h2>\n\n<p>In the tool-assisted superplay community we look to provide specially-crafted (not generated in real-time) input to a video game console or emulator in order to minimize some cost (usually time-to-completion). The way this is currently done is by playing the game frame-by-frame and specifying the input for each frame, often redoing parts of the run many times (for example, the <a href="http://tasvideos.org/2020M.html">recently published</a> run for <em>The Legend of Zelda: Ocarina of Time</em> has a total of 198,590 retries).</p>\n\n<p><strong>Making these runs obtain their goal usually comes down to two main factors: route-planning and traversal.</strong> The former is much more "creative" than the latter.</p>\n\n<p>Route-planning is determining which way the player should navigate overall to complete the game, and is often the most important part of the run. This is analogous to choosing which sorting method to use, for example. The best bubble sort in the world simply isn\'t going to outperform a quick-sort on 1 million elements.</p>\n\n<p>In the desire for perfection, however, traversal (how the route is carried out) is also a huge factor. Continuing the analogy, this is how the sorting algorithm is implemented. Some routes can\'t even be performed without very specific frames of input. This is the most tedious process of tool-assisting and is what makes the production of a completed run takes months or even years. It\'s not a <em>difficult</em> process (to a human) because it comes down to trying different variations of the same idea until one is deemed best, but humans can only try so many variations in their attention-span. The application of machines to this task seems proper here.</p>\n\n<p><strong>My goal now is to try to automate the traversal process in general for the Nintendo 64 system</strong>. The search space for this problem is <em>far</em> too large to attack with a brute-force approach. An n-frame segment of an N64 run has 2<sup>30n</sup> possible inputs, meaning a mere 30 frames of input (a second at 30FPS) has 2<sup>900</sup> possible inputs; it would be impossible to test these potential solutions, let alone those for a full two-hour run.</p>\n\n<p>However, I\'m not interested in attempting (or rather, am not going to even try to attempt) total global optimization of a full run. Rather, <strong>I would like to, given an initial input, approximate the <em>local</em> optimum for a particular <em>segment</em> of a run (or the nearest <em>n</em> local optimums, for a sort of semi-global optimization)</strong>. That is, given a route and an initial traversal of that route: search the neighbors of that traversal to minimize cost, but don\'t degenerate into trying all the cases that could solve the problem.</p>\n\n<p>My program should therefore take a starting state, an input stream, an evaluation function, and output the local optimum by minimizing the result of the evaluation.</p>\n\n<h2>Current State</h2>\n\n<p>Currently I have all the framework taken care of. This includes evaluating an input stream via manipulation of the emulator, setup and teardown, configuration, etc. And as a placeholder of sorts, the optimizer is a very basic genetic algorithm. It simply evaluates a population of input streams, stores/replaces the winner, and generates a new population by mutating the winner stream. This process continues until some arbitrary criteria is met, like time or generation number.</p>\n\n<p><strong>Note that the slowest part of this program will be, by far, the evaluation of an input stream</strong>. This is because this involves emulating the game for <em>n</em> frames. (If I had the time I\'d write my own emulator that provided hooks into this kind of stuff, but for now I\'m left with synthesizing messages and modifying memory for an existing emulator from another process.) On my main computer, which is fairly modern, evaluating 200 frames takes roughly 14 seconds. As such, I\'d prefer an algorithm (given the choice) that minimizes the number of function evaluations.</p>\n\n<p>I\'ve created a system in the framework that manages emulators concurrently. As such <strong>I can evaluate a number of streams at once</strong> with a linear performance scale, but practically speaking the number of running emulators can only be 8 to 32 (and 32 is really pushing it) before system performance deteriorates. This means (given the choice), an algorithm which can do processing while an evaluation is taking place would be highly beneficial, because the optimizer can do some heavy-lifting while it waits on an evaluation.</p>\n\n<p>As a test, my evaluation function (for the game <em>Banjo Kazooie</em>) was to sum, per frame, the distance from the player to a goal point. This meant the optimal solution was to get as close to that point as quickly as possible. Limiting mutation to the analog stick only, it took a day to get an <em>okay</em> solution. (This was before I implemented concurrency.)</p>\n\n<p>After adding concurrency, I enabled mutation of A button presses and did the same evaluation function at an area that required jumping. With 24 emulators running it took roughly 1 hour to reach the goal from an initially blank input stream, but would probably need to run for days to get to anything close to optimal.</p>\n\n<h2>Problem</h2>\n\n<p><strong>The issue I\'m facing is that I don\'t know enough about the mathematical optimization field to know how to properly model my optimization problem</strong>! I can roughly follow the conceptual idea of many algorithms as described on Wikipedia, for example, but I don\'t know how to categorize my problem or select the state-of-the-art algorithm for that category.</p>\n\n<p><strong>From what I can tell, I have a combinatorial problem with an extremely large neighborhood</strong>. On top of that, <strong>the evaluation function is extremely discontinuous, has no gradient, and has many plateaus</strong>. Also, there aren\'t many constraints, though I\'ll gladly add the ability to express them if it helps solve the problem; I would like to allow specifying that the Start button should not be used, for example, but this is not the general case.</p>\n\n<h2>Question</h2>\n\n<p><strong>So my question is: how do I model this? What kind of optimization problem am I trying to solve? Which algorithm am I suppose to use?</strong> I\'m not afraid of reading research papers so let me know what I should read!</p>\n\n<p>Intuitively, a genetic algorithm couldn\'t be the best, because it doesn\'t really seem to learn. For example, if pressing Start seems to <em>always</em> make the evaluation worse (because it pauses the game), there should be some sort of designer or brain that learns: "pressing Start at any point is useless." But even this goal isn\'t as trivial as it sounds, because sometimes pressing start <em>is</em> optimal, such as in so-called "pause backward-long-jumps" in <em>Super Mario 64</em>! Here the brain would have to learn a much more complex pattern: "pressing Start is useless except when the player is in this very specific state <em>and will continue with some combination of button presses</em>." </p>\n\n<p>It seems like I should (or the machine could learn to) represent input in some other fashion more suited to modification. Per-frame input seems too granular, because what\'s really needed are "actions", which may span several frames...yet many discoveries are made on a frame-by-frame basis, so I can\'t totally rule it out (the aforementioned pause backward-long-jump requires frame-level precision). It also seems like the fact that input is processed serially should be something that can be capitalized on, but I\'m not sure how.</p>\n\n<p><strong>Currently I\'m reading about (Reactive) Tabu Search, Very Large-scale Neighborhood Search, Teaching-learning-based Optimization, and Ant Colony Optimization.</strong></p>\n\n<p>Is this problem simply too hard to tackle with anything other than random genetic algorithms? Or is it actually a trivial problem that was solved long ago? Thanks for reading and thanks in advance for any responses.</p>\n', 'Tags': '<reference-request><machine-learning><combinatorics><optimization><search-problem>', 'LastEditorUserId': '1436', 'LastActivityDate': '2014-01-19T16:02:17.470', 'CommentCount': '2', 'AcceptedAnswerId': '2947', 'CreationDate': '2012-05-09T06:34:52.220', 'Id': '1774'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have trouble understanding the cyclic coordinate method. How does it differ with the <a href="http://en.wikipedia.org/wiki/Pattern_search_%28optimization%29" rel="nofollow">Hook and Jeeves method</a> and the <a href="http://en.wikipedia.org/wiki/Rosenbrock_methods" rel="nofollow">Rosenbrock method</a>?</p>\n\n<p>From a past exam text:</p>\n\n<blockquote>\n  <p>Describe the cyclic coordinate method and outline the similarities and the \n  differences between the Cyclic Coordinate method, the Hooke and Jeeves \n  method, and the Rosenbrock method.</p>\n</blockquote>\n\n<p>I would appreciate a good reference, I\'m having trouble finding any.</p>\n', 'ViewCount': '258', 'Title': 'Cyclic coordinate method: how does it differ from Hook & Jeeves and Rosenbrock?', 'LastEditorUserId': '4304', 'LastActivityDate': '2012-11-27T17:35:21.360', 'LastEditDate': '2012-11-27T17:35:21.360', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'qurty', 'PostTypeId': '1', 'Tags': '<algorithms><reference-request><optimization><numerical-analysis>', 'CreationDate': '2012-05-06T17:32:23.577', 'Id': '1792'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder if somebody could quickly and briefly outline some of the similarities and differences between the line search methods <a href="http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Golden_section_search" rel="nofollow">Golden Section Search</a>, <a href="http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php/Fibonacci_search" rel="nofollow">Fibonacci Search</a> and <a href="https://en.wikipedia.org/wiki/Dichotomic_search" rel="nofollow">Dichotomic Search</a>.</p>\n\n<p>I know Dichotomous has two functional evaluations per iteration whereas the other two only one, and that the Fibonacci search tends to the Golden Section as the number of functional evaluates tends to infinity. I know also that you have to predetermine the number of functional evaluates for Fibonacci. Are there other, similar techniques?</p>\n', 'ViewCount': '371', 'Title': 'Golden Section, Fibonacci and Dichotomic Searches', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T09:22:55.040', 'LastEditDate': '2012-05-15T09:22:55.040', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1495', 'Tags': '<algorithms><optimization>', 'CreationDate': '2012-05-14T20:03:20.293', 'Id': '1843'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following query:</p>\n\n<pre><code>SELECT Customer.Name FROM Customer\nINNER JOIN Order on Order.CustomerId = Customer.Id\nWHERE Customer.Preferred = True AND\n      Order.Complete = False\n</code></pre>\n\n<p>Let\'s suppose all of the relevant attributes (Customer.Preferred, Order.Complete, Order.CustomerId and Customer.Id) are indexed. How can I evaluate this as quickly as possible?</p>\n\n<p>Standard optimization advice would say that I should do the select on each table first, then the join using sort-merge or whatever the cardinality would imply. But this involves two passes through the data - I\'m wondering if there\'s a better way.</p>\n\n<hr>\n\n<p><strong>EDIT</strong>: I think asking if there was a "better way" was too ill-defined. Suppose we are trying to find $\\sigma_a(A)\\bowtie_j\\sigma_b(B)$. Observe that we can find this in $O(\\alpha)$ (where $\\alpha$ is the cardinality of $\\sigma_a(A)$) with the following pseudocode:</p>\n\n<pre><code>for each a in A:\n   find foreign tuple in B  // constant-time, if using hash table\n   check if foreign tuple meets foreign constraint  // again, constant time\n</code></pre>\n\n<p>As mentioned by some answerers, there are various minor permutations (do the for loop over B instead, etc.). But they all seem to be $O(\\alpha)$ or $O(\\beta)$. Is there a better way?</p>\n\n<p>Note that if it the query were a self join, we could just do the merge part of a sort-merge join, (since our indexes would already be sorted) which would run in time proportional to the number of results. So I ask if a similar thing can be done here.</p>\n\n<p>I am more than happy to accept a proof that there is no better method as an answer. I believe that there is no faster algorithm, but I\'m unable to prove it.</p>\n', 'ViewCount': '86', 'Title': 'Optimizing a join where each table has a selection', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-06-11T21:35:22.203', 'LastEditDate': '2012-06-11T21:35:22.203', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1590', 'Tags': '<optimization><database-theory><relational-algebra><databases>', 'CreationDate': '2012-05-21T20:03:25.400', 'FavoriteCount': '0', 'Id': '1980'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Solving the <a href="https://en.wikipedia.org/wiki/Maximum_flow_problem" rel="nofollow">maximum flow problem</a> yields one qualified minimal cut. But I want several (maybe hundreds) small cuts as candidates. The cuts don\'t have to be minimum cuts, as long as they are small (in weight). How do I do that?</p>\n', 'ViewCount': '154', 'Title': 'In s-t directed graph, how to find many small cuts?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-02T17:30:29.300', 'LastEditDate': '2012-05-25T11:18:35.083', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1620', 'Tags': '<algorithms><graphs><graph-theory><optimization><approximation>', 'CreationDate': '2012-05-24T20:19:25.250', 'FavoriteCount': '2', 'Id': '2052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Has anyone seen this problem before? It\'s suppose to be NP-complete.</p>\n\n<blockquote>\n  <p>We are given vertices $V_1,\\dots ,V_n$ and possible parent sets for each vertex. Each parent set has an associated cost. Let $O$ be an ordering (a permutation) of the vertices. We say that a parent set of a vertex $V_i$ is consistent with an ordering $O$ if all of the parents come before the vertex in the ordering. Let $mcc(V_i, O)$ be the minimum cost of the parent sets of vertex $V_i$ that are consistent with ordering $O$. I need to find an ordering $O$ that minimizes the total cost: $mcc(V_1, O), \\dots ,mcc(V_n, O)$.</p>\n</blockquote>\n\n<p>I don\'t quite understand the part "...if all of the parents come before the vertex in the ordering." What does it mean?</p>\n', 'ViewCount': '94', 'Title': 'Need help understanding this optimization problem on graphs', 'LastEditorUserId': '1123', 'LastActivityDate': '2012-05-27T18:05:16.293', 'LastEditDate': '2012-05-27T18:05:16.293', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2104', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1556', 'Tags': '<algorithms><graph-theory><terminology><optimization>', 'CreationDate': '2012-05-27T07:00:31.647', 'Id': '2100'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '761', 'Title': 'How to use a greedy algorithm to find the non-decreasing sequence closest to the given one?', 'LastEditDate': '2012-10-11T21:23:25.513', 'AnswerCount': '3', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '1718', 'FavoriteCount': '1', 'Body': "<p>You are given n integers $a_1, \\ldots, a_n$ all between $0$ and $l$. Under each integer $a_i$ you should write an integer $b_i$ between $0$ and $l$ with the requirement that the $b_i$'s form a non-decreasing sequence. Define the deviation of such a sequence to be $\\max(|a_1-b_1|, \\ldots, |a_n-b_n|)$. Design an algorithm that finds the $b_i$'s with the minimum deviation in runtime $O(n\\sqrt[4]{l})$.</p>\n\n<p>I honestly have no clue whatsoever how to even begin to solve this question. It looks like a dynamic programming question to me, but the professor said that this should be solved using a greedy algorithm. It would be much appreciated if someone can point me in the right direction by giving a small hint.</p>\n", 'Tags': '<algorithms><optimization><greedy-algorithms><subsequences>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-10-11T21:23:25.513', 'CommentCount': '6', 'AcceptedAnswerId': '2242', 'CreationDate': '2012-06-01T15:43:28.810', 'Id': '2188'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '303', 'Title': 'Weighted subset sum problem', 'LastEditDate': '2012-06-03T12:19:58.383', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '851', 'FavoriteCount': '1', 'Body': '<p>Given an integer sequence $\\{ a_1, a_2, \\ldots, a_N  \\}$ that has length $N$ and a fixed integer $M\\leq N$, the <a href="http://opc.iarcs.org.in/public/WEIGHTED-SUM.pdf" rel="nofollow">problem</a> is to find a subset $A =\\{i_1, \\dots, i_M\\} \\subseteq [N]$ with $1 \\leq i_1 \\lt i_1 \\lt \\dots \\lt i_M \\leq N$ such that</p>\n\n<p>$\\qquad \\displaystyle \\sum_{j=1}^M j \\cdot a_{i_j}$ </p>\n\n<p>is maximized.</p>\n\n<hr>\n\n<p>For instance, if the given sequence is $-50; 100; -20; 40; 30$ and $M = 2$, the best weighted sum arises when we choose positions 2 and 4. </p>\n\n<p>So that we get a value $1 \\cdot 100 + 2 \\cdot 40 = 180$.</p>\n\n<p>On the other hand, if the given sequence is $10; 50; 20$ and $M$ is again 2, the best option is to choose positions 1 and 2 that we get a value $1 \\cdot 10 + 2 \\cdot 50 = 110$.</p>\n\n<hr>\n\n<p>To me it looks similar to the <a href="http://en.wikipedia.org/wiki/Maximum_subarray_problem" rel="nofollow">maximum subarray problem</a>, but I can think of many examples in which the maximum subarray is not the best solution.</p>\n\n<p>Is this problem an instance of a well studied problem? What is the best algorithm to solve it?</p>\n\n<p>This question was inspired by <a href="http://stackoverflow.com/questions/10861642/find-maximum-weighted-sum-over-all-m-subsequences">this StackOverflow question</a>.</p>\n', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-03T12:19:58.383', 'CommentCount': '9', 'AcceptedAnswerId': '2202', 'CreationDate': '2012-06-02T15:07:15.513', 'Id': '2200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a graph G. How can we find a spanning tree that minimizes the maximum weight of all the edges in the tree? I am convinced that by simply finding an MST of G would suffice, but I am having a lot of trouble proving that my idea is actually correct. Can anyone show me a proof sketch or give me some hints as to how to construct the proof? Thanks!</p>\n', 'ViewCount': '991', 'Title': 'How to find spanning tree of a graph that minimizes the maximum edge weight?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-10-08T20:18:19.143', 'LastEditDate': '2012-06-04T23:30:42.360', 'AnswerCount': '1', 'CommentCount': '12', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1718', 'Tags': '<algorithms><graph-theory><optimization>', 'CreationDate': '2012-06-04T16:24:30.567', 'Id': '2226'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '27', 'Title': 'How to use greedy algorithm to solve this?', 'LastEditDate': '2012-06-09T07:28:40.890', 'AnswerCount': '0', 'Score': '1', 'OwnerDisplayName': 'Aden Dong', 'PostTypeId': '1', 'OwnerUserId': '1718', 'Body': u'<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/2188/how-to-use-greedy-algorithm-to-solve-this">How to use greedy algorithm to solve this?</a>  </p>\n</blockquote>\n\n\n\n<p>You are given $n$ integers $a_1, \\ldots, a_n$ all between $0$ and $l$. Under each integer $a_i$ you should write an integer $b_i$ between $0$ and $l$ with the requirement that the $b_i$\'s form a non-decreasing sequence (i.e. $b_i \\le b_{i+1}$ for all $i$). Define the deviation of such a sequence to be $\\max(|a_1\u2212b_1|,\\ldots,|a_n\u2212b_n|)$. Design an algorithm that finds the $b_i$\'s with the minimum deviation in runtime $O(n\\sqrt[4]{l})$.</p>\n\n<p>There were also two hints, one is to first find an algorithm in $O(nl)$ time, the other is that the runtime of the optimal algorithm is actually must less than $\\Theta(n\\sqrt[4]{l})$.</p>\n\n<p>I was able to find a solution that runs in $O(n^2)$ (without using any of the hints), but I have no idea how to find an algorithm that runs in $O(n\\sqrt[4]{l})$. Can anyone offer some insight into this? Maybe give a rough sketch of your algorithm? Thanks!</p>\n', 'ClosedDate': '2012-06-09T08:32:15.807', 'Tags': '<algorithms><optimization><greedy-algorithms>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-09T07:28:40.890', 'CommentCount': '0', 'CreationDate': '2012-06-06T04:06:54.050', 'Id': '2296'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '131', 'Title': 'Efficient bandwidth algorithm', 'LastEditDate': '2012-06-25T13:00:38.987', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1289', 'FavoriteCount': '0', 'Body': "<p>Recently I sort of stumbled on a problem of finding an efficient topology given a weighted directed graph. Consider the following scenario:</p>\n\n<ol>\n<li><p>Node 1 is connected to 2,3,4 at 50 Mbps. Node 1 has 100 Mbps network card.</p></li>\n<li><p>Node 3 is connected to 5 at 50 Mbps. Node 3 has 100 Mbps card.</p></li>\n<li><p>Node 4 is connected to Node 3 at 40 Mbps. Node 4 has 100 Mbps card.</p></li>\n</ol>\n\n<p>(Sorry about not having a picture)</p>\n\n<p>Problem: If Node 1 starts sending data to its immediate nodes (2 and 3), we can clearly see it's network card capacity will be drained out after Node 3. Whereas if it were to <em>skip</em> node 3 and start sending to node 4, the data will eventually reach to node 3 via 4 and hence, node 5 will be getting data via node 3.\nThe problem becomes more complicated if all the links were of 50 Mbps and we can clearly see that node 2 and node 4 are the only way to reach all nodes.</p>\n\n<p>Question: Is there an algorithm which gives the optimal path to ALL nodes keeping the network (card) capacity in mind? </p>\n\n<p>I read the shortest path algorithm,max flow algorithms but none of them seem to address my problems. perhaps,im missing something. I'll appreciate if someone can help me out.</p>\n", 'Tags': '<algorithms><graph-theory><optimization><linear-programming>', 'LastEditorUserId': '29', 'LastActivityDate': '2012-06-25T13:00:38.987', 'CommentCount': '2', 'AcceptedAnswerId': '2480', 'CreationDate': '2012-06-24T15:25:59.813', 'Id': '2470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I cannot seem to find an answer to this question with Google, so I am  going to ask here: is it required for a good neighbourhood function that it in principle (i. e. by recursively considering all neighbours of a certain solution - which is not practical) can reach all possible solutions?</p>\n\n<p>My question is whether there are references in literature that explicitely state it's a requirement - I can see that it is a good property of a neighbourhood.</p>\n", 'ViewCount': '145', 'Title': 'Neighbourhood in local search metaheuristic', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-06T13:10:29.363', 'LastEditDate': '2014-02-06T13:10:29.363', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2502', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1979', 'Tags': '<optimization><heuristics>', 'CreationDate': '2012-06-26T19:24:35.870', 'Id': '2501'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Here is the question: suppose we are given x cents, the amount we want to pay, and a 6-tuple (p, n, d, q, l, t) that represents respectively the number of pennies, nickels, dimes, quarters, loonies and toonies you have. Assume that you have enough coins to pay x cents. You do not have to pay exactly x cents; you can pay more. The cashier is assumed to be smart enough to give you back the optimal number of coins as change. We want to minimize the number of coins that changes hands, that is the number of coins you give to the cashier plus the number of coins the cashier gives back to you.</p>\n\n<p>For example, if we want to pay 99 cents and we have 99 pennies and 1 loonie, then the optimal solution would be to give the cashier the loonie and take back 1 penny.</p>\n\n<p>A particularly easy solution that occurs to me is to create a six-dimensional array. But in practice this is not feasible. So I am wondering if anyone can give me a small hint as to how to use dynamic programming to solve this (as this question looks intuitively to me like a DP problem). Once I have a hint, I can perhaps work out the remaining details myself. Thanks.</p>\n', 'ViewCount': '467', 'Title': 'How to use dynamic programming to solve this?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-27T13:23:56.797', 'LastEditDate': '2012-06-27T13:13:22.973', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1718', 'Tags': '<algorithms><optimization><dynamic-programming>', 'CreationDate': '2012-06-26T23:26:55.670', 'Id': '2507'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My setup is something like this: I have a sequence of sets of integers $C_i (1\\leq i\\leq n)$, with $|C_i|$ relatively small - on the order of four or five items for all $i$.  I want to choose a sequence $x_i (1\\leq i\\leq n)$ with each $x_i\\in C_i$ such that the total variation (either $\\ell_1$ or $\\ell_2$, i.e. $\\sum_{i=1}^{n-1} |x_i-x_{i+1}|$ or  $\\sum_{i=1}^{n-1} \\left(x_i-x_{i+1}\\right)^2$) is minimized.  While it seems like the choice for each $x_i$ is 'local', the problem is that choices can propagate and have non-local effects and so the problem seems inherently global in nature.</p>\n\n<p>My primary concern is in a practical algorithm for the problem; right now I'm using annealing methods based on mutating short subsequences, and while they should be all right it seems like I ought to be able to do better.  But I'm also interested in the abstract complexity &mdash; my hunch would be that the standard query version ('is there a solution of total variation $\\leq k$?') would be NP-complete via a reduction from some constraint problem like 3-SAT but I can't quite see the reduction.  Any pointers to previous study would be welcome &mdash; it seems like such a natural problem that I can't believe it hasn't been looked at before, but my searches so far haven't turned up anything quite like it.</p>\n", 'ViewCount': '133', 'Title': 'Minimizing the total variation of a sequence of discrete choices', 'LastEditorUserId': '242', 'LastActivityDate': '2012-06-29T20:59:15.583', 'LastEditDate': '2012-06-28T22:07:25.153', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '2542', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '242', 'Tags': '<algorithms><complexity-theory><optimization>', 'CreationDate': '2012-06-28T21:57:56.210', 'Id': '2539'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '541', 'Title': 'Collectively pay the bill problem', 'LastEditDate': '2013-07-11T19:54:15.473', 'AnswerCount': '2', 'Score': '20', 'PostTypeId': '1', 'OwnerUserId': '220', 'FavoriteCount': '10', 'Body': "<p>There are $n$ people at a table. The $i$th person has to pay $p_i$ dollars. </p>\n\n<p>Some people don't have the right bills to pay exactly $p_i$, so they come up with the following algorithm.</p>\n\n<blockquote>\n  <p>First, everyone puts some of their money on the table. Then each individual takes back the money they overpaid. </p>\n</blockquote>\n\n<p>The bills have a fixed set of denominations (not part of the input).</p>\n\n<p>An example:\nSuppose there are two people, Alice and Bob.  Alice owes \\$5 and has five \\$1 bills.  Bob owes \\$2 and has one \\$5 bill.  After Alice and Bob put all their money on the table, Bob takes back \\$3, and everyone is happy.</p>\n\n<p>Of course, there are times where one doesn't have to put <em>all</em> his money on the table. For example, if Alice had a thousand \\$1 bills, it's not necessary for her to put them all on the table and then take most of them back.</p>\n\n<p>I want to find an algorithm with the following properties: </p>\n\n<ol>\n<li><p>The input specifies the number of people, how much each person owes and how many bills of each denomination each person has.</p></li>\n<li><p>The algorithm tells each person which bills to put on the table in the first round.</p></li>\n<li><p>The algorithm tells each person which bills to remove from the table in the second round.</p></li>\n<li><p>The number of bills put on the table + the number of bills removed from the table is minimized. </p></li>\n</ol>\n\n<p>If there is no feasible solution, the algorithm just return an error.</p>\n", 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '220', 'LastActivityDate': '2013-07-11T19:54:15.473', 'CommentCount': '8', 'AcceptedAnswerId': '4808', 'CreationDate': '2012-07-08T20:31:24.413', 'Id': '2648'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a bivariate function like</p>\n\n<p>$\nf(x,y) = \\frac{1}{x^3 \\sqrt{\\pi}}. e^{\\frac{2-x}{x^2}} . y^3 . e^{3.y \\over 3-y}\n$</p>\n\n<p>and I want to find its global maximum over a range of \n$\nx \\in [0, 200] \\text{, and } y \\in [300,50000]\n$</p>\n\n<p>What kind of algorithms I can use to find the global maximum. I want to have keywords for searching and finding materials. </p>\n\n<p>Are there any java library which I can use to solve these kind of problems? </p>\n', 'ViewCount': '406', 'Title': 'Function Maximization in Java', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T09:36:15.783', 'LastEditDate': '2012-07-18T01:36:31.670', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2156', 'Tags': '<algorithms><optimization><mathematical-analysis><mathematical-software>', 'CreationDate': '2012-07-13T15:18:16.297', 'Id': '2727'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Mark lives in a tiny country populated by people who tend to over-think things. One day, the king of the country decides to redesign the country's currency to make giving change more efficient. The king wants to minimize the expected number of coins it takes to exactly pay any amount up to (but not including) the amount of the smallest paper bill.</p>\n\n<p>Suppose that the smallest unit of currency is the Coin. The smallest paper bill in the kingdom is worth $n$ Coins. The king decides that there should not be more than $m$ different coin denominations in circulation. The problem, then, is to find a $m$-set $\\{d_1, d_2, ..., d_m\\}$ of integers from $\\{1, 2, ..., n - 1\\}$ which minimizes $\\frac{1}{n-1}\\sum_{i = 1}^{n-1}{c_1(i) + c_2(i) + ... + c_m(i)}$ subject to $c_1(i)d_1 + c_2(i)d_2 + ... c_m(i)d_m = i$.</p>\n\n<p>For instance, take the standard USD and its coin denominations of $\\{1, 5, 10, 25, 50\\}$. Here, the smallest paper bill is worth 100 of the smallest coin. It takes 4 coins to make 46 cents using this currency; we have $c_1(46) = 1, c_2(46) = 0, c_3(46) = 2, c_4(46) = 1, c_5(46) = 0$. However, if we had coin denominations of $\\{1, 15, 30\\}$, it would take only 3 coins: $c_1(46) = 1, c_2(46) = 1, c_3(46) = 1$. Which of these denomination sets minimizes the average number of coins to make any sum up to and including 99 cents?</p>\n\n<p>More generally, given $n$ and $m$, how might one algorithmically determine the optimal set? Clearly, one might enumerate all viable $m$-subsets and compute the average number of coins it takes to make sums from 1 to $n - 1$, keeping track of the optimal one along the way. Since there are around $C(n - 1, m)$ $m$-subsets (not all of which are viable, but still), this would not be terribly efficient. Can you do better than that?</p>\n", 'ViewCount': '1098', 'Title': 'Algorithm to find optimal currency denominations', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:16:54.940', 'LastEditDate': '2012-07-18T01:01:59.327', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '2806', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '69', 'Tags': '<algorithms><optimization><combinatorics><integers>', 'CreationDate': '2012-07-13T18:00:47.967', 'Id': '2734'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '314', 'Title': 'Minimize the maximum component of a sum of vectors', 'LastEditDate': '2012-07-21T21:36:25.613', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1689', 'FavoriteCount': '1', 'Body': '<p>I\'d like to learn something about this optimization problem: For given non-negative whole numbers $a_{i,j,k}$,\nfind a function $f$ minimizing the expression</p>\n\n<p>$$\\max_k \\sum_i a_{i,f(i),k}$$</p>\n\n<p>An example using a different formulation might make it clearer:\nYou\'re given a set of sets of vectors like</p>\n\n<pre><code>{\n    {(3, 0, 0, 0, 0), (1, 0, 2, 0, 0)},\n    {(0, 1, 0, 0, 0), (0, 0, 0, 1, 0)},\n    {(0, 0, 0, 2, 0), (0, 1, 0, 1, 0)}\n}\n</code></pre>\n\n<p>Choose one vector from each set, so that the maximum component of their sum is minimal.\nFor example, you may choose</p>\n\n<pre><code>(1, 0, 2, 0, 0) + (0, 1, 0, 0, 0) + (0, 1, 0, 1, 0) = (1, 1, 2, 1, 0)\n</code></pre>\n\n<p>with the maximum component equal to 2, which is clearly optimal here.</p>\n\n<p>I\'m curious if this is a well-known problem and what problem-specific approximate solution methods are available. It should be fast and easy to program (no <a href="http://en.wikipedia.org/wiki/Linear_programming#Integer_unknowns" rel="nofollow">ILP</a> solver, etc.). No exact solution is needed as it\'s only an approximation of the real problem.</p>\n\n<hr>\n\n<p>I see that I should have added some details about the problem instances I\'m interested in:</p>\n\n<ul>\n<li>$i \\in \\{0, 1, \\ldots, 63\\}$, i.e., there\'re always 64 rows (when written as in the above example).</li>\n<li>$j \\in \\{0, 1\\}$, i.e., there\'re only 2 vectors per row.</li>\n<li>$k \\in \\{0, 1, \\ldots, N-1\\}$ where $N$ (the vector length) is between 10 and 1000.</li>\n</ul>\n\n<p>Moreover, on each row the sum of the elements of all vectors is the same, i.e.,</p>\n\n<p>$$\\forall i, j, j\':\\quad \\sum_k a_{i,j,k} = \\sum_k a_{i,j\',k}$$</p>\n\n<p>and the sum of the elements of the sum vector is less than its length, i.e.,</p>\n\n<p>$$\\sum_k \\sum_i a_{i,f(i),k} &lt; N$$</p>\n', 'Tags': '<algorithms><optimization><linear-programming>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-21T21:36:25.613', 'CommentCount': '5', 'AcceptedAnswerId': '2758', 'CreationDate': '2012-07-14T03:22:03.487', 'Id': '2741'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '130', 'Title': 'Assign m agents to N points by minimizing the total distance', 'LastEditDate': '2012-07-16T20:18:03.193', 'AnswerCount': '2', 'Score': '4', 'OwnerDisplayName': 'd. th. man', 'PostTypeId': '1', 'OwnerUserId': '2192', 'Body': '<p>Suppose we have $N$ fixed points (set $S$ with $|S|=N$) on the plane and $m$ agents with fixed, known initial positions ($m&lt;N$) outside $S$. We should transfer the agents so that in our final configuration they are all positioned to different points of $S$. How could we achieve it by minimizing the total distance covered by the agents? </p>\n', 'Tags': '<algorithms><graphs><optimization>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-18T01:55:49.667', 'CommentCount': '3', 'AcceptedAnswerId': '2772', 'CreationDate': '2012-07-15T16:18:37.857', 'Id': '2771'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Assume we have an optimization problem with function $f$ to maximize.</p>\n\n<p>Then, the corresponding decision problem 'Does there exist a solution with $f\\ge k$ for a given $k$?' can easily be reduced to the optimization problem: calculate the optimal solution and check if it is $\\ge k$.</p>\n\n<p>Now, I was wondering, is it always possible to do the reduction (in polynomial time) the other way around?</p>\n\n<p>For an example consider MAX-SAT: to reduce the optimization problem to the decision problem we can do a binary search in the integer range from 0 to the number of clauses. At each stoppage $k$ we check, with the decision problem solver, if there is a solution with $\\ge k$.</p>\n", 'ViewCount': '514', 'Title': 'Optimization problem vs decision problem - reduction', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-01T17:03:21.837', 'LastEditDate': '2012-08-02T06:18:37.633', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2313', 'Tags': '<complexity-theory><optimization><reductions><decision-problem>', 'CreationDate': '2012-08-01T13:08:35.477', 'FavoriteCount': '1', 'Id': '2983'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On <a href="https://en.wikipedia.org/wiki/Levenshtein_distance#Computing_Levenshtein_distance">Wikipedia</a>, an implementation for the bottom-up dynamic programming scheme for the edit distance is given. It does not follow the definition completely; inner cells are computed thus:</p>\n\n<pre><code>if s[i] = t[j] then  \n  d[i, j] := d[i-1, j-1]       // no operation required\nelse\n  d[i, j] := minimum\n             (\n               d[i-1, j] + 1,  // a deletion\n               d[i, j-1] + 1,  // an insertion\n               d[i-1, j-1] + 1 // a substitution\n             )\n}\n</code></pre>\n\n<p>As you can see, the algorithm <em>always</em> chooses the value from the upper-left neighbour if there is a match, saving some memory accesses, ALU operations and comparisons. </p>\n\n<p>However, deletion (or insertion) may result in a <em>smaller</em> value, thus the algorithm is locally incorrect, i.e. it breaks with the optimality criterion. But maybe the mistake does not change the end result -- it might be cancelled out.</p>\n\n<p>Is this micro-optimisation valid, and why (not)?</p>\n', 'ViewCount': '306', 'Title': 'Micro-optimisation for edit distance computation: is it valid?', 'LastActivityDate': '2012-08-02T07:32:35.867', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2997', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><dynamic-programming><string-metrics><correctness-proof><program-optimization>', 'CreationDate': '2012-08-01T15:41:33.670', 'Id': '2985'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Chapter 9 of the Dragon Book, the authors describe the dataflow framework for global analysis (described also in <a href="http://dragonbook.stanford.edu/lecture-notes/Stanford-CS243/l3-handout.pdf">these slides</a>).  In this framework, an analysis is defined by a set of transfer functions, along with a <a href="http://en.wikipedia.org/wiki/Semilattice">meet semilattice</a>.</p>\n\n<p>At each step of the iteration, the algorithm works by maintaining two values for each basic block: an IN set representing information known to be true on input to the basic block, and an OUT set representing information known to be true on output from the basic block.  The algorithm works as follows:</p>\n\n<ol>\n<li>Compute the meet of the OUT sets for all predecessors of the current basic block, and set that value as the IN set to the current basic block.</li>\n<li>Compute $f(IN)$ for the current basic block, where $f$ is a transfer function representing the effects of the basic block.  Then set OUT for this block equal to this value.</li>\n</ol>\n\n<p>I am confused about why this algorithm works by taking the meet of all the input blocks before applying the transfer function.  In some cases (non-distributive analyses), this causes a loss of precision.  Wouldn\'t it make more sense to apply the transfer function to each of the OUT values of the predecessors of the given block, then to meet all of those values together?  Or is this not sound?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '84', 'Title': 'Dataflow framework for global analysis: Why meet, then apply?', 'LastActivityDate': '2012-08-12T05:26:27.267', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3133', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<compilers><program-optimization>', 'CreationDate': '2012-08-08T17:40:34.587', 'Id': '3095'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm interested in a discrete max-convolution problem, which is to compute\n$$r(c) =  \\max_{x | x \\ge 0, \\sum_k x_k = c} \\left[ \\sum_{k=1} f_k(x_k) \\right] $$\nfor all values $c=0, \\ldots, C$, where $x=(x_1, \\ldots, x_k)$ is a vector of non-negative integers.</p>\n\n<p>If we assume that $f_k$ are all concave functions i.e., $f(i+1) - f(i) \\le f(i) - f(i-1)$, how efficiently can $r = (r(0), \\ldots, r(C))$ be computed?</p>\n\n<p>Follow-up: what if $J$ of the $f_k$ functions are not concave?</p>\n", 'ViewCount': '161', 'Title': 'Fast algorithm for max-convolution with concave functions?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-13T09:17:15.410', 'LastEditDate': '2012-08-13T09:17:15.410', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2475', 'Tags': '<algorithms><optimization><discrete-mathematics>', 'CreationDate': '2012-08-12T21:16:06.727', 'FavoriteCount': '2', 'Id': '3140'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>An optimisation problem requires minimising some function $f(x)$, where $x$ is a\nvector of integers. What is the corresponding decision version of the problem?</p>\n', 'ViewCount': '31', 'Title': 'Produce decision version of the problem', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-22T20:13:03.270', 'LastEditDate': '2012-08-22T20:12:13.753', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '3294', 'Score': '1', 'OwnerDisplayName': 'Princeps Tairu', 'PostTypeId': '1', 'Tags': '<complexity-theory><optimization><reductions>', 'CreationDate': '2012-08-22T15:23:59.557', 'Id': '3293'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For the branch-and-cut method, it is essential to know many facets of the polytopes generated by the problem. However, it is currently one of the hardest problems to actually calculate all facets of such polytopes as they rapidly grow in size.</p>\n\n<p>For an arbitrary optimization problem, the polytope used by branch-and-cut or also by cutting-plane-methods is the convex hull of all feasible vertices. A vertex is an assignment of all variables of the model. As a (very simple) example: if one would maximize $2\\cdot x+y$ s.t. $x+y \\leq 1$ and $0\\leq x,y\\leq 1.5$ then the vertices $(0,0)$, $(0,1)$ and $(1,0)$ are feasible vertices. $(1,1)$ violates the inequality $x+y\\leq 1.5$ and is therefore not feasible. The (combinatorical) optimization problem would be to choose among the feasible vertices. (In this case, obviously $(1,0)$ is the optimum). The convex hull of these vertices is the triangle with exactly these three vertices. The facets of this simple polytope are $x\\geq0$, $y\\geq 0$ and $x+y\\leq 1$. Note that the description through facets is more accurate than the model. In most hard problems - such as the TSP - the number of facets exceeds the number of model inequalities by several orders of magnitude.</p>\n\n<p>Considering the Travelling Salesman Problem, for which number of nodes is the polytope fully known and how much facets are there. if it is not complete, what are lower bounds on the number of facets?</p>\n\n<p>I'm particularly interested in the so-called hamiltonian path formulation of the TSP:</p>\n\n<p>$$min \\sum_{i=0}^{n-1}(\\sum_{j=0}^{i-1}c_{i,j}\\cdot x_{i,j}+\\sum_{j=i+1}^{n-1}c_{i,j}\\cdot x_{i,j})$$ s.t.</p>\n\n<p>$$\\forall i \\neq j:\\ \\ 0 \\leq x_{i,j}\\leq 1$$\n$$\\forall i \\neq j\\ \\ \\ x_{i,j}+x_{j,i}\\leq 1$$\n$$\\forall j \\ \\ \\sum_{i=0}^{j-1}x_{i,j}+\\sum_{i=j+1}^{n-1}x_{i,j}\\leq 1$$\n$$\\forall j \\ \\ \\sum_{i=0}^{j-1}x_{j,i}+\\sum_{i=j+1}^{n-1}x_{j,i}\\leq 1$$\n$$\\sum_{i=0}^{n-1}(\\sum_{j=0}^{i-1}x_{i,j}+\\sum_{j=i+1}^{n-1}x_{i,j})=n-1$$</p>\n\n<p>If you have any information about polytopes of other formulations of the TSP, feel free to share that too.</p>\n", 'ViewCount': '458', 'Title': 'Known facets of the Travelling Salesman Problem polytope', 'LastEditorUserId': '39', 'LastActivityDate': '2014-01-10T11:09:53.913', 'LastEditDate': '2013-06-06T15:04:53.917', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1227', 'Tags': '<algorithms><optimization><linear-programming><mathematical-programming><traveling-salesman>', 'CreationDate': '2012-08-29T21:07:54.547', 'FavoriteCount': '2', 'Id': '3367'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it possible to design a compiler which optimizes a loop in which arrays are accessed in alternate fashion? For example like this:</p>\n\n<pre><code>// int[] a,b\nint sum = 0;\nfor(int i = 0; i &lt; n; i++)\n{\n  sum += a[i] + b[i];\n}\n</code></pre>\n\n<p>With the usual sequential array storage, <code>a[i]</code> and <code>b[i]</code> may be far away from each other in memory. Therefore, I think a good compiler optimization would detect that <code>a[i]</code> and <code>b[i]</code> are always accesses at the "same" time, and store the arrays interleaved, that is <code>a[0] b[0] a[1] b[1] ...</code> so that one memory access may retrieve both <code>a[i]</code> and <code>b[i]</code>.</p>\n', 'ViewCount': '101', 'Title': 'Are compilers able to detect alternating accesses to arrays and interleave them in memory?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-05T19:48:04.607', 'LastEditDate': '2012-09-05T19:48:04.607', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '3434', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2741', 'Tags': '<compilers><arrays><program-optimization><memory-management>', 'CreationDate': '2012-09-05T13:27:15.477', 'Id': '3433'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We collect most of the information about possible compiler optimizations during forward pass. Is it possible to utilize the information collected in forward pass in a backward pass so as to perform better optimizations ?</p>\n\n<p>Note: I have been going through the patent <a href="http://www.google.com/patents/US7765534" rel="nofollow">Compiler with cache utilization optimizations</a> by Roch G. Archambault et al. (2004) and was wondering what kind of information might have been utilized in their backward pass.</p>\n', 'ViewCount': '71', 'Title': 'Benefit of Backward Pass at compile time', 'LastEditorUserId': '2741', 'LastActivityDate': '2012-09-06T23:36:56.983', 'LastEditDate': '2012-09-06T23:36:56.983', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3452', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2741', 'Tags': '<compilers><program-optimization>', 'CreationDate': '2012-09-06T13:29:18.803', 'Id': '3449'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How exactly <a href="http://en.wikipedia.org/wiki/Loop_dependence_analysis" rel="nofollow">Loop Dependence Analysis</a> helps in <a href="http://en.wikipedia.org/wiki/Vectorization_%28parallel_computing%29" rel="nofollow">vectorization</a> ? Are there any standard rules of safety criterias for parallizing such loops ?</p>\n', 'ViewCount': '71', 'Title': 'Using Loop Dependence analysis for vectorization', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-08T09:27:02.593', 'LastEditDate': '2012-09-07T07:27:34.463', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '3465', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2741', 'Tags': '<compilers><parallel-computing><program-optimization>', 'CreationDate': '2012-09-07T00:25:42.440', 'Id': '3454'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a directed graph $G=(V,E)$ and a node $r\\in V$, I need to grow a tree $T$ rooted at $r$ that has a minimum weight and spans all reachable nodes in $G$.</p>\n\n<p>The weight function assigns a non-negative weight to each node, which depends on the node's ancestors in $T$.  Specifically, for some fixed sets of nodes $S_1, S_2, \\dots, S_k \\subseteq V$, the weight of node $v$ is the number of sets $S_i$ that contain $v$ and all its ancestors in $T$.</p>\n\n<p>Any suggestion how to approach this problem?</p>\n", 'ViewCount': '141', 'Title': 'Minimum vertex-weight directed spanning tree where the weight function depends on the tree', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-13T06:27:26.233', 'LastEditDate': '2012-09-13T06:27:26.233', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2774', 'Tags': '<algorithms><graph-theory><optimization><spanning-trees>', 'CreationDate': '2012-09-10T07:57:07.253', 'FavoriteCount': '1', 'Id': '3486'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '152', 'Title': 'Recommended Reading for non-CS undergraduate student doing a research Project on Travelling Salesman Problem', 'LastEditDate': '2013-06-06T15:05:33.373', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'rrampage', 'PostTypeId': '1', 'OwnerUserId': '2642', 'FavoriteCount': '1', 'Body': '<p>I am an undergraduate student in Industrial Engineering. I have taken the topic of Travelling Salesman Problem as a Research Project for my final year. More specifically, I am focusing on <strong>Convex Hulls for solving the Euclidean TSP</strong>.</p>\n\n<p>I have gone through the published literature and have found that there are no approximation bounds for solving TSP using this method. I have just a little background in TheoCS (Started reading for fun since the last 3 months).</p>\n\n<p><strong>My Question:</strong> </p>\n\n<p>Additional Books / Papers needed for achieving a good understanding of the mathematical rigor of the problem.</p>\n\n<p>Here\'s what I am currently going through:</p>\n\n<ul>\n<li>Approximation Algorithms - Vazirani</li>\n<li>Introduction to Algorithms - CLRS</li>\n<li>Introduction to Graph Theory - Douglas West</li>\n<li>Randomized Algorithms - Motwani and Raghavan</li>\n</ul>\n\n<p>I have also completed an introductory online course on Algorithms by Udacity.</p>\n\n<p>I think I may need some background reading in Computational Geometry (Currently I do not know more than what I read in the chapter from Cormen)</p>\n\n<p>This is my first question here, so sincere apologies in case my question does not conform to the Community Standards.</p>\n\n<p>EDIT:</p>\n\n<p>A resource I stumbled across today: <a href="http://press.princeton.edu/titles/9531.html" rel="nofollow">In Pursuit of the Traveling Salesman - Mathematics at the Limits of Computation</a>.\nThis is a good book for popular reading, providing a survey of the problem and various methods that have been used to solve it.</p>\n', 'Tags': '<reference-request><optimization><books><traveling-salesman>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-06T15:05:33.373', 'CommentCount': '0', 'CreationDate': '2012-09-13T21:59:28.047', 'Id': '3539'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it possible to reduce MaxUNSAT to <a href="http://en.wikipedia.org/wiki/Maximum_satisfiability_problem" rel="nofollow">MaxSAT</a> in a polynomial way ?</p>\n\n<p>When considering the MaxSAT problem, one often considers also the <code>MinUNSAT</code> problem, which is almost the same. And for a propositional formula <code>f</code> in <a href="http://en.wikipedia.org/wiki/Conjunctive_normal_form" rel="nofollow">CNF</a> it holds:</p>\n\n<pre><code>|f| = MaxSAT(f) + MinUNSAT(f)\n</code></pre>\n\n<p>where <code>|f|</code> is the number of clauses of f.</p>\n\n<p>When considering <code>MaxUNSAT</code> and the corresponding <code>MinSAT</code> problem, the same relationship holds:</p>\n\n<pre><code>|f| = MaxUNSAT(f) + MinSAT(f)\n</code></pre>\n\n<p>Now, I was wondering if there is also a relationship between those two pairs, e.g. to reduce <code>MaxSAT</code> to <code>MaxUNSAT</code> or <code>MinSAT</code> (or the other way round) ?</p>\n\n<p>Unfortunately, I could not figure out one by myself. And maybe there is none ?</p>\n\n<p><strong>Update 1:</strong> Inspired by Yuval Filmus\'s answer, I will give a reduction for my question.</p>\n\n<p><strong>Reduction from MaxUNSAT to its corresponding decision problem:</strong></p>\n\n<p>Let $\\phi = {C_1, ..., C_m}$ a set of clauses over the variables $x_1, ..., x_n$, then it holds:\n$$MaxUNSAT(\\phi) = BinarySearch(0, |\\phi|, MaxUNSAT(\\phi, k) )$$\nwith \n$$\nBinarySearch(start, end, CompareProcedure(k))\n:= \\\\\\text{Searches for the element $e$ between $start$ and $end$ with help of the $CompareProcedure(k)$, so that holds $CompareProcedure(e) = true$ and $CompareProcedures(e+1) = false$ }$$\nand\n$$MaxUNSAT(\\phi, k) := \\exists v\\in\\{0, 1\\}^n:\\sum_{i=1}^m 1 - I_v(C_i) \\geq k$$\nwhere $I_v$ is the interpretation of a propositional formula under assignment $v$.</p>\n\n<p><strong>Reduction from decision problem $MaxUNSAT(\\phi, k)$ to SAT:</strong></p>\n\n<p>One can reduce the devision problem $MaxUNSAT(\\phi, k)$ to the SAT problem by adding blocking variables to each clause and adding a cardinality constraint as propositional formula to limit the number of used clauses with help of the blocking variables.</p>\n\n<p>I can describe this in more detail, if needed.</p>\n\n<p><strong>Conculsion:</strong></p>\n\n<p>One can reduce the MaxUNSAT problem to the SAT problem and then solve the SAT problem with the MaxSAT problem. This is a reduction that works in polynomial time.</p>\n', 'ViewCount': '127', 'Title': 'How to reduce MaxUNSAT to MaxSAT?', 'LastEditorUserId': '2313', 'LastActivityDate': '2012-09-24T21:55:55.063', 'LastEditDate': '2012-09-24T21:55:55.063', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2313', 'Tags': '<complexity-theory><optimization><reductions><satisfiability>', 'CreationDate': '2012-09-20T08:42:23.210', 'Id': '4625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In question <a href="http://cs.stackexchange.com/questions/4625/how-to-reduce-maxunsat-to-maxsat">How to reduce MaxUNSAT to MaxSAT?</a> I was asking, how to reduce the MaxUNSAT problem to MaxSAT. With help of the given answer I could give a polynomial reduction : $MaxUNSAT \\leq decisionProblemMaxUNSAT \\leq SAT \\leq MaxSAT$.</p>\n\n<p><strong>Question:</strong> Is there a more direct reduction without much overhead from MaxUNSAT to MaxSAT (or to minUNSAT)?</p>\n\n<p>Because the two problems are very similiar, it seems there is a more direct reduction, but I could not figure out one.</p>\n\n<p>Something like $MaxUNSAT(\\phi) = MaxSAT(\\neg\\phi)$ (which does not work).</p>\n', 'ViewCount': '61', 'Title': 'How to reduce MaxUNSAT to MaxSAT in a (almost) direct way?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-03T21:41:01.693', 'LastEditDate': '2012-09-24T22:21:34.283', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2313', 'Tags': '<complexity-theory><optimization><reductions><satisfiability>', 'CreationDate': '2012-09-24T21:59:46.447', 'Id': '4724'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am interested in <a href="http://en.wikipedia.org/wiki/Oriented_matroid" rel="nofollow">oriented matroids</a> in the context of directed graphs and optimization. Unfortunately, I know very little of the topic. Is there a book, article or a resource that serves as a good introduction to oriented matroids, especially in the context of directed graphs? It\'s a bonus if the resource is suitable for an (under)graduate level course and is preferably even free.</p>\n', 'ViewCount': '51', 'Title': 'What is a good resource to learn about oriented matroids in the context of digraphs and optimization?', 'LastActivityDate': '2012-10-13T20:53:20.050', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6047', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<reference-request><optimization><discrete-mathematics>', 'CreationDate': '2012-10-13T20:19:48.250', 'Id': '6046'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If there are many solutions to a linear program s.t. the objective function is minimized/maximized (= optimal solutions are on an edge of the polytope), how can I force an LP solver to find only an extreme point solution?</p>\n', 'ViewCount': '129', 'Title': 'Formulating a linear program s.t. only extreme point solutions are found', 'LastEditorUserId': '19', 'LastActivityDate': '2013-09-12T20:11:20.917', 'LastEditDate': '2013-09-12T20:11:20.917', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4208', 'Tags': '<optimization><linear-programming>', 'CreationDate': '2012-10-15T14:07:50.050', 'Id': '6093'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In my variant of the assignment problem I have a set $A$ of agents and a set (of possibly different cardinality) $T$ of tasks. Each agent needs to be assigned exactly $n$ or $n+1$ tasks, and each task needs to be assigned to exactly $m$ or $m+1$ agents.</p>\n\n<p>It is guaranteed this this is possible: the segment $\\left[ |A|n, |A|(n+1) \\right]$ intersects the segment $\\left[ |B|m, |B|(m+1) \\right]$.</p>\n\n<p>Each agent-task combination yields a profit, and I want to maximize the profit.</p>\n\n<p>Is this a special case of one of the known problems? How can this be solved? If not practical for n=100,000, what are good approximations and what is their complexity?</p>\n\n<p><sub> Not a comp.scientist, but have done the basic research. Please excuse me if I've overlooked anything obvious. </sub></p>\n", 'ViewCount': '289', 'Title': 'A variant of the Assignment Problem', 'LastEditorUserId': '4283', 'LastActivityDate': '2012-10-21T11:52:13.743', 'LastEditDate': '2012-10-21T08:07:04.320', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4283', 'Tags': '<optimization><linear-programming><assignment-problem>', 'CreationDate': '2012-10-20T06:30:51.020', 'Id': '6186'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '889', 'Title': "Using Amdahl's law how do you determine execution time after an improvement?", 'LastEditDate': '2012-10-22T21:23:53.303', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1480', 'FavoriteCount': '1', 'Body': "<p>Speeding up a new floating-point unit by 2 slows down data cache accesses by a factor of 2/3 (or a 1.5 slowdown for data caches).  If old FP unit took 20% of program's execution time and data cache accesses took 10% of program's execution time, what is the overall speed up? </p>\n\n<p>I solved this problem using amdahl's law: </p>\n\n<p>FeFP = floating point enhanced fraction = .2</p>\n\n<p>FeDC = data cache access enhanced fraction = .1</p>\n\n<p>SeFP = floating point enhanced speedup = 2</p>\n\n<p>SeDC = data cache access enhanced speedup = 2/3</p>\n\n<p>Speedup overall = 1 / (   (1 - FeFP - FeDC)   +   FeFP/SeFP   +    FeDC * SeDC    )</p>\n\n<p>= 1 /  (   (   1 - .2 - .1  ) + .2/2 + (.1) * (2/3)   )\n = 1.154. </p>\n\n<p>I hope I did this correctly, but I'm confused about the next part asking what percentage of execution time is spent on floating point operations after implementing the new FP unit? </p>\n\n<p>I know that T[improved ] = T[affected] / improvement factor   + T[unaffected]</p>\n\n<p>But I'm unclear how to use it in the context of this problem.  Would appreciate all / any advice. </p>\n", 'Tags': '<computer-architecture><program-optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-22T21:23:53.303', 'CommentCount': '1', 'AcceptedAnswerId': '6204', 'CreationDate': '2012-10-20T22:07:45.577', 'Id': '6200'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a bipartite graph with $|E|=O(|V|^2)$, a super-source and a super-sink. I am looking for the min-cost max-flow (the max-flow of all possible max-flows that has the minimum cost).</p>\n\n<p>For the sake of my question, denote $n=|E|$.</p>\n\n<p>Are there algorithms that will run in $O(n^2)$, or even $O(n\\log(n))$, or for that matter anything less than $O(n^3)$? In my case, $n$ is ~100,000, so $O(n^3)$ is impractical for me.</p>\n\n<p>\'Extra\' questions (should these go under a separate question?):</p>\n\n<ol>\n<li>Do any of these supper multi-graphs (I have 2 edges from my super-source node to each of the "blue" nodes in my bipartite graph, and 2 edges from each of the "red" nodes to my super-sink node)?</li>\n<li>Are there efficient implementations of such algorithms (that run in less than $O(n^3)$, and support multi-graphs) in C++, C, or Python?</li>\n<li>If the answer is \'no\', what are popular approximation algorithms and their associated run times?</li>\n</ol>\n', 'ViewCount': '500', 'Title': 'Min cost max flow in bipartite run time', 'LastEditorUserId': '4283', 'LastActivityDate': '2014-03-18T21:51:17.517', 'LastEditDate': '2012-10-23T05:03:44.203', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4283', 'Tags': '<graph-theory><graphs><optimization>', 'CreationDate': '2012-10-22T17:25:16.093', 'Id': '6234'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can all <a href="http://en.wikipedia.org/wiki/Packing_problem">packing problems</a> be rephrased as <a href="http://en.wikipedia.org/wiki/Set_packing">set packing problems</a>?</p>\n\n<p>Can all <a href="http://en.wikipedia.org/wiki/Covering_problem">covering problems</a> be rephrased as <a href="http://en.wikipedia.org/wiki/Set_cover_problem">set covering problems</a>?</p>\n\n<p>In other words, I was wondering if set packing/covering problems are the most general forms of packing/covering problems?</p>\n\n<p>BTW, do these problems belong to combinatorial optimization, integer optimization, or/and other types of optimization problems?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '92', 'Title': 'Can all packing/covering problems be rephrased as set packing/covering problems?', 'LastActivityDate': '2012-10-28T22:52:46.767', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<optimization>', 'CreationDate': '2012-10-28T22:52:46.767', 'FavoriteCount': '1', 'Id': '6362'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Take a Turing machine, with a terminating program, convert it to some representation of the machine which captures, in a lossless manner, its state as it performs the computation.</p>\n\n<p>So you have a complete representation of the machine, the program, and its internal organisation as it performs the computation.</p>\n\n<p>I am going to suggest a graphical form, nodes and edges, names for nodes.</p>\n\n<p>Take a second Turing machine with a slightly different program. This program is identical save that it performs a single unit of function from the first program in a non optimal way, say it performs the single unit 3 times, changing some value to the correct output the first time, taking it to some second result the second time and then finally returning again to the correct first result. Like a reflection.</p>\n\n<p>Would it not be possible for some statistical technique to analyse the graph of the two machines including their process and find a compression of the graph of the second machine, which is smaller in terms of the size of the graph of its process and yet consistent with the mode of operation of the machine.</p>\n\n<p>For instance a graph matching algorithm could find that there is a subgraph match between one portion of the the process graph of the first machine and one part of the process graph of the second machine and replace the subgraph of the second machine with the subgraph of the process of the first machine.</p>\n\n<p>How it would then alter the program of the second machine to generate that altered graph I am unsure of.</p>\n\n<p>Do such techniques exist? Where would I find them, or is the analysis flawed or incomplete in some way which prevents its operation? What should I learn to understand its implementation or the truth of its deficiency?</p>\n', 'ViewCount': '104', 'Title': 'Is it possible to analyse computation?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-02T21:06:44.393', 'LastEditDate': '2012-11-02T21:06:44.393', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4340', 'Tags': '<graphs><optimization><runtime-analysis><compilers><artificial-intelligence>', 'CreationDate': '2012-11-02T12:58:51.063', 'Id': '6452'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>A film producer is seeking actors and investors for his new movie. There are $n$ available actors; actor $i$ charges $s_i$ dollars. For funding, there are $m$ available investors. Investor $j$ will provide $p_j$ dollars, but only on the condition that certain actors $L_j \\subseteq \\{1,2,...,n\\}$, are included in the cast (all actors $i \\in L_j$ must be chosen in order to receive funding from investor $j$). The producer's profit is the sum of the payments from investors minus the payments to actors. The goal is to maximize this profit.</p>\n\n<ol>\n<li>Express this problem as an integer linear program in which the variables take on values on [0,1] </li>\n<li>Show that there must in fact be an integral optimal solution (as is the case, for example, with maximum flow and bipartite matching).</li>\n</ol>\n\n<p>I am lost on both parts for this problem.</p>\n", 'ViewCount': '633', 'Title': 'Integer LP formulation and the existence of a solution', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-20T17:29:59.943', 'LastEditDate': '2012-11-20T17:29:59.943', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4523', 'Tags': '<optimization><linear-programming>', 'CreationDate': '2012-11-09T13:25:42.710', 'Id': '6582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For directed graph $(G=(V, E),s,t,{Ce})$ in which we want to maximize max flow. All edge capacities are at least one. Define the capacity of an $s \\to t$ path to be the smallest capacities of constituent edges. The fastest path from $s$ to $t$ is the path with the most capcity.</p>\n\n<p>b) Show that the fastest path from $s$ to $t$ in a graph can be computed by Dijkstra's algorithm.</p>\n\n<p>c) Show that the maximum flow in $G$ is the sum of individual flows along at most $|E|$ paths from $s$ to $t$.</p>\n\n<p>It's one of the questions from my algorithms assignment, and I figured out (a), but can't get these two above.</p>\n", 'ViewCount': '418', 'Title': "Dijskstra's algorithm, maximum flow", 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-11T17:50:53.390', 'LastEditDate': '2012-11-09T22:10:28.233', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4523', 'Tags': '<algorithms><graphs><optimization><shortest-path><network-flow>', 'CreationDate': '2012-11-09T19:31:05.533', 'Id': '6586'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a complex query $Q$ used to search a dataset $S$ to find $H_\\text{exact} = \\{s \\in S \\mid \\text{where $Q(s)$ is True}\\}$. Each query takes on average time $t$ so the overall time in the linear search is $t\\cdot |S|$. I can break a query down into simpler sub-queries q_i and find $H_\\text{approx} = \\{s\\in S \\mid \\forall q_j(s) \\text {is True}\\}$  and where $H_\\text{exact}\\subseteq H_\\text{approx}$. Each subquery $q_i$ is much faster to compute, so overall it is faster to find $H_\\text{approx}$ and then use $Q$ to find $H_\\text{exact}$.</p>\n\n<p>Each $Q$ has many $q_i$. The overlap between different $Q$ is high. I\'m looking for a way to determine a decision-tree-like set of fixed questions $q_j$ which minimize the average time to find a H_exact, based on a large sample of search queries.</p>\n\n<p>To make this more concrete, suppose the data set contains the 7 billion people in the world, and the complex queries are things like "the woman who lives in the red house on the corner of 5th and Lexington in a city starting with B."</p>\n\n<p>The obvious solution is to check every person in world and see who matches the query. There may be more than one such person. This method takes a long time. </p>\n\n<p>I could pre-compute this query exactly, in which case it would be very fast .. but only for this question. However, I know that other queries are for the woman who lives on the blue house on the same corner, the man who lives on the same corner, the same question but in a city starting with C, or something totally different, like \'the king of Sweden.\'</p>\n\n<p>Instead, I can break the complex question down into a set of easier but more general sets. For example, all of the above questions have a gender-role based query, so I can precompute the set of all people in the world who consider themselves a \'woman.\' This sub-query takes essentially no time, so the overall search time decreases by roughly 1/2. (Assuming that by other knowledge we know that a Swedish "king" cannot be a "woman." Hatshepsut was an Egyptian woman who was king.)</p>\n\n<p>However, there are sometimes queries which aren\'t gender-based, like "the person who lives on 8th street in a red house in a city starting with A." I can see that the subquery "lives in a red house" is common, and pre-compute a list of all those people who live in a red house.</p>\n\n<p>This gives me a decision tree. In the usual case, each branch of the decision tree contains different questions, and the methods to select the optimal terms for the decision tree are well known. However, I\'m building on an existing system which requires that all branches must ask the same questions.</p>\n\n<p>Here\'s an example of a possible final decision set: question 1 is \'is the person a woman?\', question 2 is \'does the person live in a red house?\', question 3 is \'does the person live in a city starting with A or does the person live in a city starting with B?\', and question 4 is \'does the person live on a numbered street?\'.</p>\n\n<p>When a query $Q$ comes in, I see if its $q_i$ match any of the pre-computed questions $q_j$ I\'ve determined. If so, then I get the intersection of those answers, and ask the question $Q$ on that intersection subset. Eg, if the question is "people who live in a red house on an island" then find that "person lives in a red house" is already precomputed, so it\'s only matter of finding the subset of those who also live on an island.</p>\n\n<p>I can get a cost model by looking at a set of many $Q$ and check to see the size of the corresponding $H_\\text{approx}$. I want to minimize the average size of $H_\\text{approx}$.</p>\n\n<p>The question is, how do I optimize the selection of possible $q_j$ to make this fixed decision tree? I tried a GA but it was slow to converge. Probably because my feature space has a few million possible $q_j$. I\'ve come up with a greedy method, but I\'m not happy with the result. It too is very slow, and I think I\'m optimizing the wrong thing.</p>\n\n<p>What existing research should I be looking at for ideas?</p>\n', 'ViewCount': '98', 'Title': 'Fixed-length decision-tree-like feature selection to minimize average search performance', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-19T09:26:37.540', 'LastEditDate': '2012-11-19T09:26:37.540', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4650', 'Tags': '<algorithms><optimization><machine-learning><greedy-algorithms>', 'CreationDate': '2012-11-19T09:17:01.957', 'Id': '6763'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a <a href="http://en.wikipedia.org/wiki/Binary_expression_tree" rel="nofollow">binary expresion tree</a>, with addition and multiplication operations, how can we optimize it\'s evaluation?</p>\n\n<p>Can we learn from <a href="http://en.wikipedia.org/wiki/Matrix_chain_multiplication" rel="nofollow">matrix chain multiplication</a>? A <a href="http://en.wikipedia.org/wiki/Matrix_chain_multiplication#Generalizations" rel="nofollow">generalization</a> of matrix chain multiplication is defined as:</p>\n\n<blockquote>\n  <p>Given a linear sequence of objects, an associative binary operation on those objects, and a way to compute the cost of performing that operation on any two given objects (as well as all partial results), compute the minimum cost way to group the objects to apply the operation over the sequence.</p>\n</blockquote>\n\n<p>What happens if we put <em>two</em> binary operators? <strong>Can the algorithm for <em>Matrix chain multiplication</em> be further generalized (or how can we otherwise solve this problem) to <em>two</em> binary operators in a <em>binary expresion tree</em>, given the cost functions of these operations?</strong> In particular, <strong>multiplication and addition, which complicates things further by allowing distribution</strong>. Also, does it matter that mind that some of the numbers can be negative, allowing reduction in size of intermediate results (see <a href="http://cs.stackexchange.com/q/1424/2755">Overflow safe summation</a>)?</p>\n\n<p><strong>Also, how does this relate to  <a href="http://en.wikipedia.org/wiki/Graph_reduction" rel="nofollow">Graph Reduction</a>?</strong></p>\n\n<p>I also remember learning about database <a href="http://en.wikipedia.org/wiki/Query_optimization" rel="nofollow">query optimization</a> which seemed to do something similar to determine how early to execute particular joins to keep the intermediate values smaller.</p>\n', 'ViewCount': '422', 'Title': 'Chained operations on sequences with two operators', 'LastEditorUserId': '2755', 'LastActivityDate': '2012-11-28T22:31:40.290', 'LastEditDate': '2012-11-28T22:31:40.290', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<optimization><binary-trees><dynamic-programming><efficiency><arithmetic>', 'CreationDate': '2012-11-20T18:58:03.297', 'FavoriteCount': '1', 'Id': '6790'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have ordered a few leather sheets from which I would like to build juggling balls by sewing edges together. I\'m using the Platonic solids for the shape of the balls.</p>\n\n<p>I can scan the leather sheets and generate a polygon that approximates the shape of the leather sheet (as you know, it\'s animal skin, and it doesn\'t come in rectangles).</p>\n\n<p>So now, I would like to maximize the size of my juggling ball.</p>\n\n<p>In my example, the polygons are regular ones, but I\'m looking for a solution with simple polygons.</p>\n\n<p>What is the largest scale factor that I can apply to my polygons so that they all fit inside the sheet ?</p>\n\n<p>I am trying to minimize the waste by using as much as material as possible.</p>\n\n<p>Obviously, cutting the polyhedron net into individual polygon will increase the space of possible combination, but also decrease the quality of the final geometry, because there is more sewing involved and accumulated errors. But this question is not about enumerating the different ways of unfolding a polyhedron. They can be considered independently. So the polygons are simple polygons.</p>\n\n<p><em>Formally:</em></p>\n\n<p>Input:</p>\n\n<ul>\n<li>$P$ : a simple polygon (the target)</li>\n<li>$S$ : the set of polygons I want to place</li>\n<li>$G$ : a graph of $n$ simple polygons - each node represents a simple polygon in $S$, and there is one edge edge between each pair of polygons that share a common edge   </li>\n<li>$\\alpha &gt;= 0, \\beta &gt;= 0$ (usage of material and connectivity)</li>\n</ul>\n\n<p>Output:</p>\n\n<ul>\n<li>a scale factor $f$</li>\n<li>$H$, a subgraph of $G$ </li>\n<li>$Loc$: a location and an angle for each polygon in $V(G)$</li>\n<li>a measure of the quality $m$ of the solution: $ m = \\alpha.f + \\beta. {|E(H)|\\over|E(G)|} $</li>\n</ul>\n\n<p>Maximize $m$ subject to these conditions:</p>\n\n<ul>\n<li>$ | V(H) | =  |V(G)| $  (1)</li>\n<li>$ | E(H) | &lt;= |E(G)| $  (2)</li>\n<li>for every polygon $S_i$ in $S$, $S_i$ scaled by a factor $f$ at location $Loc(S_i)$ is inside $P$ (3)</li>\n<li>polygons in $V(H)$ don\'t overlap (4)</li>\n</ul>\n\n<p>( V(G) are the vertices in the graph, and S is the set of polygons, but they describe the same set of objects. Maybe there is a more compact way to do this.) </p>\n\n<p>Explanation of the conditions:</p>\n\n<ul>\n<li>(1) I want all the polygons to be in the final layout</li>\n<li>(2) Some connections may be broken if necessary</li>\n<li>(3) (4) the ball is made of leather</li>\n</ul>\n\n<p>Here is the target polygon\n<img src="http://i.stack.imgur.com/mopxJ.jpg" alt="Leather sheet"></p>\n\n<p>Here is the set of polygons I want to pack:\n<img src="http://i.stack.imgur.com/H4LEi.png" alt="Polyhedron net"></p>\n', 'ViewCount': '337', 'Title': 'How to pack polygons inside another polygon?', 'LastEditorUserId': '2151', 'LastActivityDate': '2012-11-22T11:14:54.447', 'LastEditDate': '2012-11-22T11:14:54.447', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '2151', 'Tags': '<optimization><computational-geometry><packing>', 'CreationDate': '2012-11-21T06:14:34.403', 'FavoriteCount': '1', 'Id': '6800'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '35', 'Title': 'A variant of the assignment problem (?)', 'LastEditDate': '2012-11-21T17:30:35.567', 'AnswerCount': '0', 'Score': '3', 'OwnerDisplayName': 'Nitzan Shaked', 'PostTypeId': '1', 'OwnerUserId': '4283', 'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/6186/a-variant-of-the-assignment-problem">A variant of the Assignment Problem</a>  </p>\n</blockquote>\n\n\n\n<p>(Not a comp.scientist, but have the basic research. Please excuse me if I\'ve overlooked anything obvious.)</p>\n\n<p>In my variant of the problem I have a set $A$ of agents and a set (of possibly different cardinality) $T$ of tasks. Each agent needs to be assigned exactly $n$ or $n+1$ tasks, and each task needs to be assigned to exactly $m$ or $m+1$ agents.</p>\n\n<p>It is guaranteed that this is possible: the segment $[ |A|n, |A|(n+1) ]$ intersects the segment $[ |B|m, |B|(m+1) ]$.</p>\n\n<p>Each agent-task combination yields a profit, and I want to maximize the profit.</p>\n\n<p>Is this a special case of one of the known problems? How can this be solved? If not practical for $n=100000$, what are good approximations and what is their complexity?</p>\n\n<p>Cheers!</p>\n', 'ClosedDate': '2012-11-21T20:10:47.473', 'Tags': '<optimization><linear-programming>', 'LastEditorUserId': '140', 'LastActivityDate': '2012-11-21T17:30:35.567', 'CommentCount': '6', 'CreationDate': '2012-10-15T20:43:46.737', 'Id': '6819'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Many common operations are <a href="http://en.wikipedia.org/wiki/Monoid">monoids</a>. Haskell has leveraged this observation to make many higher-order functions more generic (<code>Foldable</code> being one example).</p>\n\n<p>There is one obvious way in which using monoids can be used to improve performance: the programmers is asserting the operation\'s associativity, and so operations can be parallelized. </p>\n\n<p>I\'m curious if there are any other ways a compiler could optimize the code, knowing that we\'re dealing with a monoid. </p>\n', 'ViewCount': '129', 'Title': 'Are monoids useful in optimization?', 'LastActivityDate': '2012-11-24T08:10:13.990', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '6860', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1590', 'Tags': '<optimization><compilers><category-theory>', 'CreationDate': '2012-11-23T15:34:10.673', 'FavoriteCount': '2', 'Id': '6858'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Optimization factoring:<br>\nInput: $N\\in \\mathbb{N}$<br>\nOutput: All prime factors of $N$</p>\n\n<p>Decision factoring:<br>\nInput: $N, k\\in \\mathbb{N}$<br>\nOutput: True iff $N$ has a prime factor of at most $k$</p>\n\n<p>How can I solve the optimization problem in polynomial time if the decision problem is polynomially solvable?</p>\n', 'ViewCount': '57', 'Title': 'Optimization-factoring $\\le_p$ Decision-factoring', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-26T01:10:28.467', 'LastEditDate': '2012-11-25T20:58:32.787', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '6900', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4730', 'Tags': '<optimization><reductions><factoring>', 'CreationDate': '2012-11-25T20:54:34.053', 'Id': '6897'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '164', 'Title': 'Randomized Rounding of Solutions to Linear Programs', 'LastEditDate': '2012-11-30T04:10:17.983', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/Linear_programming#Integral_linear_programs" rel="nofollow">Integer linear programming</a> (ILP) is an incredibly powerful tool in combinatorial optimization. If we can formulate some problem as an instance of an ILP then solvers are guaranteed to find the global optimum. However, enforcing integral solutions has runtime that is exponential in the worst case. To cope with this barrier, several approximation methods related to ILPs can be used,</p>\n\n<ul>\n<li>Primal-Dual Schema</li>\n<li>Randomized Rounding</li>\n</ul>\n\n<p>The Primal-Dual Schema is a versatile method that gives us a "packaged" way to come up with a greedy algorithm and prove its approximation bounds using the relaxed dual LP. Resulting combinatorial algorithms tend to be very fast and perform quite well in practice. However its relation to linear programming is closer tied to the analysis. Further because of this analysis, we can easily show that constraints are not violated.</p>\n\n<p>Randomized rounding takes a different approach and solves the relaxed LP (using interior-point or ellipsoid methods) and rounds variables according to some probability distribution. If approximation bounds can be proven this method, like the Primal-Dual schema, is quite useful. However, one portion is not quite clear to me:</p>\n\n<blockquote>\n  <p>How do randomized rounding schemes show that constraints are not violated?</p>\n</blockquote>\n\n<p>It would appear that naively flipping a coin, while resulting in a 0-1 solution, could violate constraints! Any help illuminating this issue would be appreciated. Thank you.</p>\n', 'Tags': '<optimization><randomized-algorithms><linear-programming><approximation>', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-30T04:10:17.983', 'CommentCount': '4', 'AcceptedAnswerId': '6949', 'CreationDate': '2012-11-27T05:05:34.050', 'Id': '6941'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am building a program that uses branching-and-bounding to find an optimal path in a complete graph (the heuristic, faster algorithm is the second part). I have to begin and end at node 0. I was given a large hint that eliminating "hopeless" branches early in the algorithm will lead to an overall much faster runtime. I have tried several things: greedily selecting the smallest edge as either the first or last edge in the tour, branching into two subsets of solutions with one branch being solutions including the smallest available edge and the other being solutions that exclude the largest available edge. Neither have proved to be worth any salt, and I was wondering if there were any clever optimizations to branch-and-bound I could implement to help speed up my program. Any help will be appreciated!</p>\n\n<p>Edit: any advice on computing better bounds and how to use them effectively is also welcomed.</p>\n', 'ViewCount': '27', 'Title': 'Inferences about Branching in TSP algorithm', 'LastEditorUserId': '4771', 'LastActivityDate': '2012-11-30T04:06:20.010', 'LastEditDate': '2012-11-30T04:06:20.010', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4771', 'Tags': '<optimization><graph-traversal>', 'CreationDate': '2012-11-30T03:37:03.467', 'Id': '7033'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How can you perform the clique decision algorithm fewer than $ O(n) $ times to solve clique optimization?</p>\n\n<p>I\'m not sure if my approach is right but this is my thought process: you would pick vertices in a graph and see if they form a clique, then keep picking more vertices until you have the max possible clique.</p>\n\n<p>I\'m not sure how it can be done less than $ O(n) $ times.</p>\n\n<p>I can imagine an undirected graph such as:</p>\n\n<p><img src="http://i.stack.imgur.com/GptO1.png" alt="undirected graph"></p>\n\n<p>where $ \\{A, B, C\\} $ and $ \\{B, C, D\\} $ would be cliques. The number of vertices is 4, and the number of vertices in the cliques is 3, which is $ n - 1 $. Would this count as being done in less than $ O(n) $ times, or is this the wrong approach to this problem?</p>\n', 'ViewCount': '100', 'Title': 'Using Clique decision to solve Clique optimization', 'LastEditorUserId': '4689', 'LastActivityDate': '2012-12-02T03:39:34.423', 'LastEditDate': '2012-12-02T03:39:34.423', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '7053', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4689', 'Tags': '<algorithms><graph-theory><optimization>', 'CreationDate': '2012-11-30T20:10:46.630', 'Id': '7052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I had the choice of choosing one out of the following two optimization problems which I could use to solve my problem. Which choice is the fastest? How much of a trade-off would it be?  Is the improvement in speed by many factors!?</p>\n\n<ol>\n<li><p>Minimizing a convex function $L(X)$ in one matrix variable with orthogonality constraints over the matrix-essentially in my case this ends up to solving an eigen-decomposition.</p></li>\n<li><p>Minimizing the same convex function $L(X)$ with linear constraints in $X$.</p></li>\n</ol>\n\n<p>I know that 2.) should be faster. But what is the direction of work I need to do- to compare the improvement in speed-especially in terms of using the fastest available eigen solver for 1.)-what would be the corresponding fastest approach to solve 2.)?</p>\n', 'ViewCount': '61', 'Title': 'Time - Complexity Convex Optimization and Eigen Decomposition', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T00:32:56.540', 'LastEditDate': '2012-12-06T10:17:22.360', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'VSPC', 'PostTypeId': '1', 'Tags': '<algorithms><time-complexity><optimization><efficiency><linear-algebra>', 'CreationDate': '2012-10-16T18:17:29.447', 'Id': '7206'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '258', 'Title': 'Find maximum distance between elements given constraints on some', 'LastEditDate': '2013-01-14T22:06:36.387', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4896', 'FavoriteCount': '1', 'Body': '<p>I have a list of numbered elements 1 to N that fit into positions on a number line starting with 1. I also have constraints for these elements:</p>\n\n<ul>\n<li>The element 1 is in position 1, and element N must be at a position >= the position of element N-1. (i.e. element 2 could be at position 1, element 3 at position 7, and element 4 at position 8 (but not position 5))</li>\n<li>Some elements must be within a certain distance from each other on the line.</li>\n<li>Some elements must be at least a certain distance from other on the line.</li>\n</ul>\n\n<p>My objective is to return an integer that represents the maximum span between element 1 and element N. If no lineup is possible, return -1, and if the elements can be any distance apart, return -2. </p>\n\n<p>I am given:</p>\n\n<ul>\n<li>The number of elements</li>\n<li>A withinArray[][] where withinArray[x][y] = the distance elements x and y must be within on the line. Any zero values represent no constraints.</li>\n<li>An atLeastArray[][] where atLeastArray[x][y] = the distance elements x and y must be apart on the line. Any zero values represent no constraints.</li>\n</ul>\n\n<p>An example input would be: 4 elements, withinArray<a href="http://stackoverflow.com/questions/13714903/find-maximum-distance-between-elements-given-constraints-on-some">1</a>[3] = 10, withinArray[2][4] = 20, and atLeastArray[2][3] = 3. (all other array values are zero).</p>\n\n<p>The return value for this input would be 27. (element 1 at position 1, element 2 at position 8, element 3 at position 11, and element 4 at position 28)</p>\n\n<p>The problem was first posted <a href="http://stackoverflow.com/questions/13714903/find-maximum-distance-between-elements-given-constraints-on-some">here</a> by someone else. I\'d like to figure out an elegant solution to it programmatically. Though I\'ve been working on it for a whole day, I still have no luck coming up with a good solution. I feel that I need to use dynamic programming techniques, but have a hard time finding a good substructure. </p>\n\n<p>I am not able to locate the same example on the web. Any pointer to such materials online would be appreciated. It\'s even better if you are an expert for such kind of question and can outline the solution in detail. Executable code is a plus.</p>\n', 'Tags': '<algorithms><optimization><dynamic-programming><linear-programming>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-14T22:06:36.387', 'CommentCount': '0', 'AcceptedAnswerId': '7213', 'CreationDate': '2012-12-06T10:22:20.080', 'Id': '7208'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The problem is\n$$\\max  f(\\mathbf{x}) \\text{ subject to } \\mathbf{Ax} = \\mathbf{b}$$</p>\n\n<p>where $f(\\mathbf{x}) = \\sum_{i=1}^N\\sqrt{1+\\frac{x_i^4}{(\\sum_{i=1}^{N}x_i^2)^2}}$, <br/>\n$\\mathbf{x} = [x_1,x_2,...,x_N]^T \\in \\mathbb{R}^{N\\times 1}$, and  <br/>\n$\\mathbf{A} \\in \\mathbb{R}^{M\\times N} $ <br/></p>\n\n<p>We can see that $f(.)$ is in the form of $\\sqrt{1+y^2}$ and is a convex function.\n<br/> \nIt can be also shown that f(.) is bounded in $[\\sqrt{2}, 2]$. <br/> </p>\n\n<p>I know that a convex maximization problem is NP-hard, in general.</p>\n\n<p>But using the specific nature of the problem, is it possible to solve it using any standard convex optimization software/package?</p>\n', 'ViewCount': '176', 'Title': 'Maximizing a convex function with a linear constraint', 'LastEditorUserId': '165', 'LastActivityDate': '2012-12-27T05:53:20.510', 'LastEditDate': '2012-12-25T19:54:53.643', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '5183', 'Tags': '<optimization><linear-programming>', 'CreationDate': '2012-12-25T12:03:00.633', 'Id': '7594'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '632', 'Title': 'Scheduling algorithm to minimize maximum deadline overshoot in pre-emptive scheduler', 'LastEditDate': '2013-01-12T18:59:53.610', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4751', 'FavoriteCount': '1', 'Body': '<p>Suppose there are $n$ tasks, which need to be scheduled by a <em>pre-emptive</em> scheduler. Each task $T_i$ has a deadline $d_i$ and a total processing time $t_i$ associated with it. Now, all $n$ tasks are given a priory to the scheduler. The scheduler can run a task for 1 unit of time in one go. After each unit of time, it can schedule any process (including the current one) for the next 1 unit of time.</p>\n\n<p>The goal of the scheduler is to <em>minimize</em> the <em>maximum</em> overshoot of its deadline by any process. For example, for tasks $T_1: (2, 2)$, $T_2: (1, 1)$ and $T_3: (4, 3)$ are the 3 tasks with their respective $(d_i, t_i)$, then a schedule of $T_2, T_1, T_3, T_1, T_3, T_3$ gives a maximum overshoot of 2. No other schedule can reduce the maximum overshoot.</p>\n\n<p>My solution is to use "Earliest Deadline First Scheduling", with tie-breaks based on most time/work remaining. Further ties are broken arbitrarily. Basically, after each unit of time, the task with the earliest deadline is scheduled first. Any ties are decided on which task has the most work remaining. Further ties are broken arbitrarily. This seems to work on a few small hand-constructed cases. But I could not prove or disprove it.</p>\n\n<p>To make the question more than a yes/no question, I would really appreciate it if someone could prove if this is correct, or provide a correct and efficient (sub-quadratic time) algorithm for this. This is not homework. It was presented to me by someone, and I suspect it might be a popular interview question or a question on a programming forum.</p>\n', 'Tags': '<algorithms><optimization><scheduling>', 'LastEditorUserId': '2100', 'LastActivityDate': '2013-08-14T07:27:28.117', 'CommentCount': '0', 'AcceptedAnswerId': '7906', 'CreationDate': '2013-01-12T06:01:51.120', 'Id': '7900'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I want to implement efficiently "streaming Knapsack" problem in java.</p>\n\n<p>The problem is I have a stream input of integer data coming continuously for example -1, 2, 9, 5, 5, 11, 1 -3,...</p>\n\n<p>The question is to find the first "k" elements in which their sum is "n>0". for example k=3 and n=12, \nthen the solution is: ...,2,...,5, 5.</p>\n\n<p>I found an answer in <a href="http://programmingpraxis.com/2012/05/15/streaming-knapsack/2/" rel="nofollow">http://programmingpraxis.com/2012/05/15/streaming-knapsack/2/</a> \nas: (It is mainly for positive Integer Values)</p>\n\n<p>But looking for simpler one! Any Ideas?</p>\n', 'ViewCount': '200', 'Title': 'Streaming Knapsack Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-07T11:30:04.040', 'LastEditDate': '2013-01-26T17:50:18.787', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><optimization><streaming-algorithm><online-algorithms>', 'CreationDate': '2013-01-25T18:37:58.193', 'FavoriteCount': '1', 'Id': '9155'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for algorithms to prioritize equipment renewals.</p>\n\n<p>Input: (years since last renewal, cost of renewal, importance of renewal).</p>\n\n<p>Output: An ordering of the equipment according to which it will be renewed.</p>\n\n<p>I do not know if there are any algorithms for this particular problem. If you have any idea how to fit this problem into a more general context, that would be useful too.</p>\n\n<p>A way to rephrase the problem:</p>\n\n<p>You have $n$ pieces of equipment $E_1,\\ldots,E_n$. For each piece $E_i$ you have a triple $(\\text{age}_i,\\text{cost}_i,\\text{importance}_i)$. At the beginning of the year you have $X$ amount of money. You want to spend these money in order to <em>minimize</em> the function $\\sum_i \\text{age}_i\\cdot \\text{importance}_i$ at the end of the year. So, during the year you have to select a subset $S$ of $\\{1,\\ldots,n\\}$ such that:\n$$\\sum_{i\\in S} \\text{cost}_i\\le X \\text{ (cost constraint)}$$ and the sum $$\\sum_{i\\in S} \\text{age}_i\\cdot \\text{importance}_i$$ is <em>maximal</em> among all subsets of $\\{1,\\ldots,n\\}$ that satisfy the cost constraint.</p>\n\n<p>Any help?</p>\n', 'ViewCount': '67', 'Title': 'Algorithms to prioritize equipment renewals', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-30T17:19:11.647', 'LastEditDate': '2013-01-30T17:19:11.647', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '9286', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6610', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-01-29T17:36:29.230', 'Id': '9282'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '161', 'Title': 'Convergence of Simulated Annealing Based Algorithms', 'LastEditDate': '2013-01-31T14:01:23.310', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6550', 'FavoriteCount': '2', 'Body': "<p>I designed a simulated annealing-based optimization algorithm. My simulation shows that it converge fast. I am looking for some sort of proof to show that simulation annealing-based algorithm converge fast (based on satisfying some properties) to global/local optimal point and doesn't oscillate in the optimal points (or any related fast about its stability). Are there any useful literature about it?</p>\n", 'Tags': '<algorithms><reference-request><proof-techniques><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T04:03:50.740', 'CommentCount': '7', 'AcceptedAnswerId': '9400', 'CreationDate': '2013-01-31T00:36:58.170', 'Id': '9340'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I don\'t know the right name for this problem, or if there is a name, but it is inspired by my initial interpretation of the title of <a href="http://cs.stackexchange.com/questions/9155/streaming-knapsack-problem">this</a> question (my question is very different, so the link may be misleading). Anyways, my question is this:</p>\n\n<p>We are initially given a list of "items" to be filled in a knapsack of fixed size. Each item has a weight (bounded, integral) and value, and we need to maximize the total value of items in the knapsack. So far, this is identical to the 0/1 Knapsack problem. Now, at each step, we perform one of the following:</p>\n\n<ul>\n<li>Remove the first item in the list (first means encountered earliest)</li>\n<li>Add a new item to the list at the end.</li>\n</ul>\n\n<p>To keep the solution space small, we can assume that the maximum size of the list is fixed, so that it will behave like a fixed size buffer overflow - oldest item is removed before new item is added.</p>\n\n<p>Now, the list is smallish, so the initial instance of the knapsack on the original list can be performed to obtain the first solution. Now, <strong>after <em>every</em> operation</strong> on the list (addition or removal of items), we again want to find out the best way to fill a <strong>new (empty) knapsack with the items in the new list</strong>. And we want to do it <strong>without repeating a full knapsack algorithm</strong> on this slightly modified list (since there will be many such operations).</p>\n\n<p>Is there some way the results of the previous state can be utilized to speed up the process? Is there some information from the previous state that can <em>usually</em> speed up the process? Is there any research on this or some related problem? </p>\n\n<p>The pseudo-polynomial time DP algorithm can be adapted for the case where an item is added (since the table depends on the previous items), but I could not figure out how to deal with it in case the first item is removed from the list. Similarly, a branch-and-bound approach seems pointless. Any ideas or references?</p>\n', 'ViewCount': '189', 'Title': 'Dynamic Knapsack Problem - Algorithms and References', 'LastEditorUserId': '4751', 'LastActivityDate': '2013-02-01T19:15:14.520', 'LastEditDate': '2013-02-01T19:15:14.520', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4751', 'Tags': '<algorithms><reference-request><optimization><combinatorics><knapsack-problems>', 'CreationDate': '2013-02-01T11:46:06.723', 'Id': '9384'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Theoretically speaking, is it possible to have a Lisp/Scheme compiler that can produce code that can compete with compiled C, let\'s say within 15-25% margin?</p>\n\n<p>In my testing, I\'ve found that the current crop of compilers (Bigloo, SBCL, Gambit, Chicken, etc) are <strong>20-50 times slower than equivalent C code</strong>.</p>\n\n<p>The <strong>only outlier is the Stalin compiler</strong>. For simple programs, it produces binaries that are equivalent to C. However, what I find suspicious is that none of the other projects (Bigloo, Chicken, Clozure, etc) have attempted to implement whatever tricks Stalin uses ("whole program optimization", etc).</p>\n\n<p>I\'m a huge fan of LISP since the mid 90s and would love to bring it on board so my team can crank out projects in half the time in normally takes using C/C++/.NET/etc, but...the performance issues are a huge roadblock.</p>\n\n<p>I wonder if the lack of quality LISP compilers are due to the fact that no serious time and money has been invested into the subject OR if this simply isn\'t a feasible task given the current state of compiler technology??</p>\n', 'ViewCount': '259', 'Title': 'Quality LISP/Scheme compilers to compete with C/C++', 'LastActivityDate': '2013-02-04T18:53:37.353', 'AnswerCount': '0', 'CommentCount': '8', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6703', 'Tags': '<optimization><compilers>', 'CreationDate': '2013-02-04T18:53:37.353', 'Id': '9483'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I molded my problem as the following game (it is a <a href="http://en.wikipedia.org/wiki/Congestion_game" rel="nofollow">congestion game</a> with varying price):</p>\n\n<p>$N$ players share resources $E$,</p>\n\n<p>$S_i$ is the strategy space of player $i$ which is in $2^E$ (where $2^E$ is the power set of resources).</p>\n\n<p>$P_e^i$ is the price of resource $e \\in E$ considering player $i$. <strong>The price of resource $e$ is different for different users.</strong></p>\n\n<p>The goal of each player is to select a strategy $S_i$ which minimize its price $\\sum_{e\\in S_i}P_e^i$ .</p>\n\n<p>My questions are:</p>\n\n<ol>\n<li>Does this game have any <a href="http://en.wikipedia.org/wiki/Nash_equilibrium" rel="nofollow">Nash Equilibrium</a> (NE)? If so under which conditions? </li>\n<li>If it has any NE, what is a sample algorithm for achieving it?</li>\n</ol>\n\n<p>I searched literature but could not find any appropriate information! Any solution is appreciated!</p>\n', 'ViewCount': '52', 'Title': 'Congestion Game with Varying Price', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-09-13T13:49:18.273', 'LastEditDate': '2013-02-10T14:36:23.537', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><optimization><game-theory>', 'CreationDate': '2013-02-05T16:41:45.657', 'FavoriteCount': '2', 'Id': '9512'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What are the fastest known algorithms for general convex optimisation under linear inequality constraints?</p>\n', 'ViewCount': '67', 'Title': 'Convex optimisation under linear inequality constraints', 'LastActivityDate': '2013-02-11T13:48:41.673', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '192', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-02-11T13:48:41.673', 'Id': '9679'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is it possible to have the following condition in a semidefinite programming as a constraint?</p>\n\n<p>$\n         M=\n            \\left[ {\\begin{array}{cc}\n             a &amp; \\sqrt{u} \\\\\n             \\sqrt{u} &amp; b \\\\\n                \\end{array} } \\right]\n        \\geq 0$</p>\n\n<p>where $\\geq 0$ means positive semidefinite.</p>\n', 'ViewCount': '71', 'Title': 'Can one have a condition like this in semidefinite programming?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-14T19:11:54.900', 'LastEditDate': '2013-02-14T19:11:54.900', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'OwnerDisplayName': 'Star', 'PostTypeId': '1', 'Tags': '<optimization>', 'CreationDate': '2013-02-10T18:24:57.150', 'Id': '9724'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for a good survey/study of experimental results of heuristics for Knapsack problem (or implemented libraries in java/c++). Any help is appreciated!</p>\n', 'ViewCount': '92', 'Title': 'Experimental Survey on Different Heuristics for Knapsack Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-14T06:38:43.647', 'LastEditDate': '2013-02-14T06:38:43.647', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<reference-request><np-complete><optimization><heuristics>', 'CreationDate': '2013-02-13T16:57:46.143', 'Id': '9747'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m developing software to run variations on a base process flow (see #1, below). A user specifies in a text file what steps in the process to modify. Because each step takes a long time to run, I\'d like to minimize the amount of duplicate processing required. For example, if variations occur at step B, I could run step A one for all results before "branching" at step B (see #2 below). Similarly, I could branch again at step D if additional variations on step D are indicated (see #3 below).</p>\n\n<p><strong>1) Base Process:</strong></p>\n\n<pre><code>input --&gt; A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; result\n</code></pre>\n\n<p><strong>2) Modification of Step B:</strong></p>\n\n<pre><code>input --&gt; A --&gt; B1 --&gt; C --&gt; D --&gt; E --&gt; result1\n                B2 --&gt; C --&gt; D --&gt; E --&gt; result2\n</code></pre>\n\n<p><strong>3) Modification of Steps B and D:</strong></p>\n\n<pre><code>input --&gt; A --&gt; B1 --&gt; C --&gt; D1 --&gt; E --&gt; result1\n                             D2 --&gt; E --&gt; result2\n                B2 --&gt; C --&gt; D1 --&gt; E --&gt; result3\n                             D2 --&gt; E --&gt; result4\n</code></pre>\n\n<p>Is there a simple algorithm to determine the the common process steps and the branch points as in #3 given a base flow as in #1 and a list of steps to change, e.g.</p>\n\n<pre><code>Variation  StepB  StepD\n   1         1      1\n   2         1      2\n   3         2      1\n   4         2      2\n</code></pre>\n\n<p>The above example is simple but there could be hundreds of variations modifying dozens of different steps in actual usage. </p>\n', 'ViewCount': '56', 'Title': 'Algorithm for finding optimal branch points', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T14:08:38.050', 'LastEditDate': '2013-04-21T14:08:38.050', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6936', 'Tags': '<algorithms><optimization><scheduling>', 'CreationDate': '2013-02-18T19:13:35.097', 'Id': '9901'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a set $S = \\{(a_1,b_1),...,(a_n,b_n)\\}$ where $a_i &lt; m$, $b_i = m-a_i$, $m \\in \\mathbb{Z}^{+}$, $m&gt;2$ and $n$ is an even number greater than $3$. What is the most efficient algorithm to determine if it is possible to partition $S$ into two distinct subsets, $C$ and $D$, of equal size such that</p>\n\n<p>$\\sum_{a \\in C} a &gt; \\sum_{b \\in C} b$ &nbsp;&nbsp;and&nbsp;&nbsp; $\\sum_{a \\in D} a &gt; \\sum_{b \\in D} b$ </p>\n\n<p>or&nbsp; $\\sum_{b \\in C} b &gt; \\sum_{a \\in C} a$ &nbsp;and&nbsp; $\\sum_{b \\in D} b &gt; \\sum_{a \\in D} a$ &nbsp;?</p>\n\n<p>For example, if $S = \\{(56,44),(48,52),(43,57),(60,40)\\}$, $C = \\{(56,44),(48,52)\\}$, and $D = \\{(43,57),(60,40)\\}$. </p>\n\n<p>I am considering iteratively matching a pair with the best value of $a$ with a pair with the worst value of $a$. Is there another algorithm? </p>\n', 'ViewCount': '184', 'Title': 'Optimal partition of a set of pairs', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:10:06.683', 'LastEditDate': '2013-02-24T16:10:06.683', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '6723', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-02-21T08:21:51.740', 'Id': '10004'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the <a href="http://en.wikipedia.org/wiki/Interval_scheduling" rel="nofollow">interval scheduling problem</a>, see also <a href="http://www.phailed.me/2012/08/interval-scheduling-problem/" rel="nofollow">here</a>. </p>\n\n<p>In order to schedule the $n$ job requests over one resource, you sort the requests in order of finish time, choose the request with earliest finish time, choose the next compatible one, and so on. </p>\n\n<p>Now, let us say we have two resources instead of one. How do I schedule my jobs now? As my idea goes, again you start by sorting the requests in order of finish time. My problem is, how do I proceed after that? Do I choose the resources in sequential or in an alternate fashion?</p>\n\n<p>If I go for sequential manner, I schedule all the possible jobs in the first resource and then do the same for second resource with the jobs yet to be scheduled. </p>\n\n<p>If I go for alternate fashion, I choose the first possible job in the first resource, then second possible job in the second resource and so on.</p>\n\n<p>In each case we will have take in to account the chosen jobs being compatible, needless to say.</p>\n\n<p>I can not decide which of these is going to be optimal.</p>\n\n<p>Any input will be appreciated.</p>\n', 'ViewCount': '168', 'Title': 'Interval Scheduling Problem with more than One Resource', 'LastEditorUserId': '7200', 'LastActivityDate': '2013-03-18T00:07:18.963', 'LastEditDate': '2013-03-18T00:07:18.963', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7200', 'Tags': '<algorithms><optimization><scheduling>', 'CreationDate': '2013-03-12T00:49:56.563', 'Id': '10460'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It seems that (in a broad sense) two approaches can be utilized to produce an algorithm for solving various optimization problems: </p>\n\n<ol>\n<li>Start with a feasible solution and expand search until constraints are tight and solution is maximal (or minimal).  </li>\n<li>Begin with violated constraints and search for maximal (or minimal) feasible approach.</li>\n</ol>\n\n<p>For the Max-Flow problem Ford-Fulkerson satisfies condition (1), while Push-Relabel satisfies condition (2). An interesting point is that Push-Relabel is a more efficient algorithm than Ford-Fulkerson. My question is this:</p>\n\n<blockquote>\n  <p>What other examples are there where (2)-based approaches outperform their (1)-based counterparts?</p>\n</blockquote>\n\n<p>A follow up is:</p>\n\n<blockquote>\n  <p>Do there exist meta-theorems regarding approaches based on condition (2)? </p>\n</blockquote>\n', 'ViewCount': '106', 'Title': 'Constraint violation and efficiency in search', 'LastEditorUserId': '19', 'LastActivityDate': '2013-03-19T00:26:53.147', 'LastEditDate': '2013-03-15T19:22:46.253', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '10530', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '19', 'Tags': '<algorithms><optimization><lower-bounds>', 'CreationDate': '2013-03-15T02:46:32.423', 'Id': '10528'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '266', 'Title': 'What algorithm would compute the maximum choices from two sets?', 'LastEditDate': '2013-03-17T18:03:31.260', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7284', 'FavoriteCount': '1', 'Body': '<p>Given two vectors of integers of possibly unequal lengths, how can I determine the maximum result possible from accumulating choosing the maximum between corresponding pairs of numbers between the two vectors with extra zeros inserted into the shorter vector to make up for the size difference?</p>\n\n<p>For example, consider the following two vectors as inputs:</p>\n\n<pre><code>[8 1 4 5]\n[7 3 6]\n</code></pre>\n\n<p>The choices for inserting the zero and the resulting sum are:</p>\n\n<pre><code>[0 7 3 6]  =&gt; Maximums: [8 7 4 6]  =&gt;  Sum is: 25\n[7 0 3 6]  =&gt; Maximums: [8 1 4 6]  =&gt;  Sum is: 19\n[7 3 0 6]  =&gt; Maximums: [8 3 4 6]  =&gt;  Sum is: 21\n[7 3 6 0]  =&gt; Maximums: [8 3 6 5]  =&gt;  Sum is: 22\n</code></pre>\n\n<p>Therefore, in this case, the algorithm should return 25.</p>\n\n<p>I could do this by brute force by calculating for all permutations of placing zeros into the smaller vector (as just done above) but this would be computationally expensive, and worst in the case when one vector is exactly half the size of the other.</p>\n\n<p>Is there a way to compute the answer in linear time proportional to the length of the longer vector even when the vectors differ in length?  If not, can we do better than the number of factorial permutations being chosen?</p>\n', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-17T18:03:31.260', 'CommentCount': '3', 'AcceptedAnswerId': '10544', 'CreationDate': '2013-03-15T20:44:46.160', 'Id': '10543'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '163', 'Title': 'Fastest Algo to separate the 0s and 1s', 'LastEditDate': '2013-03-18T12:23:35.627', 'AnswerCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2042', 'Body': '<p>Suppose, I have an array of length 100.\nI have 0s at some positions and 1s at some other positions.\nWhat is the <strong><em>fastest method</em></strong> by which I can separate the <strong>0s</strong> and <strong>1s</strong>, so that we get all the 0s at the beginning and all the 1s at the remaining positions / vice-versa?</p>\n', 'ClosedDate': '2013-03-20T13:18:10.800', 'Tags': '<program-optimization>', 'LastEditorUserId': '2042', 'LastActivityDate': '2013-03-19T19:20:56.090', 'CommentCount': '9', 'AcceptedAnswerId': '10623', 'CreationDate': '2013-03-18T10:49:18.463', 'Id': '10598'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need an efficient algorithm that takes input a collection of intervals and outputs the largest subset of non-intersecting intervals. </p>\n\n<p>i.e. Given a set of intervals $I =  \\{I_1, I_2, \\ldots, I_n\\}$ of the real line, we need to output a set of intervals $O = \\{O_1, O_2, \\ldots, O_k\\}$ such that</p>\n\n<ul>\n<li>$O$ is a subset of $I$.</li>\n<li>For any $i \\neq j$, $O_i$ and $O_j$ are non-intersecting.</li>\n<li>$k$ is the maximum possible.</li>\n</ul>\n\n<p>Example: if the intervals are $[1,100], [2,3], [4,5], [6,7], [3,20]$ we should return $\\{[2,3], [4,5], [6,7]\\}$.</p>\n', 'ViewCount': '503', 'Title': 'Algorithm to return largest subset of non-intersecting intervals', 'LastEditorUserId': '7391', 'LastActivityDate': '2013-03-24T21:29:01.710', 'LastEditDate': '2013-03-24T21:29:01.710', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'OwnerDisplayName': 'user2112791', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><integers><intervals><set-cover>', 'CreationDate': '2013-03-23T02:48:15.283', 'FavoriteCount': '0', 'Id': '10713'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There are many NP-complete decision problems that ask the question whether it holds for the optimal value that OPT=m (say bin packing asking whether all items of given sizes can fit into m bins of a given size).\nNow, I am interested in the problem whether OPT>m. Is this a decision problem or an optimization problem? It seems to be that it lies in NP (a NTM can guess a solution and it can be verified in polynomial time that the bound is met). Is it also NP-complete?</p>\n\n<p>I would have said yes, because having a polynomial algorithm, we could find a solution in polynomial time for the original problem (asking whether OPT=m) by using binary search and repeatedly using the polynomial algorithm to test if OPT larger than some bound.</p>\n\n<p>However when I try to construct a proper solution, I always see the complication that the oracle (that asks whether OPT>m') would need to be queried more than once, and this is forbidden in the polynomial time Karp reduction.</p>\n\n<p>Any solutions or remarks?\nWould it make a difference if I ask whether OPT>=m?</p>\n\n<p>Thanks in advance</p>\n", 'ViewCount': '136', 'Title': 'Polynomial time reductions using binary search', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-26T11:07:26.827', 'LastEditDate': '2013-03-25T15:20:47.793', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7309', 'Tags': '<complexity-theory><terminology><np-complete><optimization><decision-problem>', 'CreationDate': '2013-03-25T14:08:42.207', 'Id': '10774'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I have to implement a limitation algorithm in order to avoid to reach a throughput limit imposed by the service I'm interacting with.</p>\n\n<p>The limit is specified as \xabN request over 1 day\xbb where N is of the order of magnitude of 10^6.</p>\n\n<p>I have a distributed system of clients interacting with the service so they should share the measure.</p>\n\n<p>An exact solution should involve to record all the events and than computing the limit \xabwhen\xbb the event of calling the service occur: of course this approach is too expensive and so I'm looking for an approximate solution.</p>\n\n<p>The first one I devised imply to discretize the detection of the events: for example maintaing 24 counters at most and recording the number of requests occurred within an hour.</p>\n\n<p>Acceptable.</p>\n\n<p>But I feel that a more elegant, even if leaded by different \xabforces\xbb, is to declinate the approach to the continuum.</p>\n\n<p>Let's say recording the last N events I could easily infer the \xabcurrent\xbb throughput. Of course this algorithm suffer for missing consideration of the past events occurred the hours before. I could improve with with an aging algorithm but\u2026 and here follow my question:</p>\n\n<p>Q: \xabThere's an elegant approximate solution to the problem of estimating the throughput of a service?\xbb</p>\n", 'ViewCount': '26', 'Title': 'Throughput measure', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T11:58:18.983', 'LastEditDate': '2013-03-28T11:58:18.983', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7177', 'Tags': '<optimization><distributed-systems><approximation>', 'CreationDate': '2013-03-28T10:33:22.480', 'FavoriteCount': '1', 'Id': '10864'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem similar to the interval scheduling algorithm. The differences are:</p>\n\n<ul>\n<li>The jobs have the same length.</li>\n<li>There are several categories of jobs and only one job from each category can be chosen.</li>\n<li>There can be no overlap in time between different categories.</li>\n</ul>\n\n<p>I have illustrated the problem with a picture:</p>\n\n<p>Each line is a job. Different color means that they belong to a different category. So I need to choose one of each colored line to try to cover as large an area as possible without any lines overlapping. Remember that lines with the same color should not be chosen twice. And yes, for the illustration the problem is trivial.</p>\n\n<p><img src="http://i.stack.imgur.com/fLk1Z.png" alt="enter image description here"></p>\n\n<p>Does my problem have a name? Does it require a DP algorithm?</p>\n', 'ViewCount': '98', 'Title': 'Variation of interval scheduling algorithm with several job categories, only one from each can be used', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T15:13:15.947', 'LastEditDate': '2013-04-02T15:13:15.947', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2826', 'Tags': '<algorithms><optimization><scheduling>', 'CreationDate': '2013-04-02T13:16:11.903', 'FavoriteCount': '2', 'Id': '10970'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '29', 'Title': 'Why are optimization problems always NP-hard and not NP-complete and what does this mean for other levels of the polynomial time hierarchy?', 'LastEditDate': '2013-04-03T20:22:50.270', 'AnswerCount': '0', 'Score': '2', 'OwnerDisplayName': 'user2145167', 'PostTypeId': '1', 'OwnerUserId': '7309', 'Body': '<p>I have read that optimization problems cannot be $\\mathcal{NP}$-complete, but are always classified as $\\mathcal{NP}$-hard. When a problem is NP-complete, I know it is contained in $\\mathcal{NP}$P. This implies in particular that it is not hard for the second level of the polynomial time hierarchy, e.g. for $\\Sigma_2^P$ or $\\Pi_2^P$. But since optimization problems are only NP-hard, I have no such knowledge. Or are optimization problems usually also $\\Sigma_2^P$-hard or $\\Pi_2^P$-hard, or just some of them?</p>\n\n<p>Are there any interesting problems from combinatorial optimization that are harder than $\\mathcal{NP}$-hard, e.g. hard for the second level of the polynomial time hierarchy?</p>\n\n<p>I am in particular interested in problems from combinatorial optimization, e.g. BP (bin packing), TSP and CVRP (capacitated vehicle routing problem). They are all classified as $\\mathcal{NP}$-hard, but CVRP is a generalization of both TSP and BP, so it should be harder? Bin packing should be easier, are there any results showing this?\nDoes anyone know, if there are hardness results for any of these problems that imply more difficult than $\\mathcal{NP}$-hard?</p>\n\n<p>I know there are many versions of CVRP and TSP and unfortunately I know not a lot about them.</p>\n', 'ClosedDate': '2013-04-05T08:15:21.540', 'Tags': '<complexity-theory><terminology><optimization><integer-programming>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-03T20:22:50.270', 'CommentCount': '3', 'CreationDate': '2013-03-21T09:30:40.023', 'Id': '11001'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '85', 'Title': 'Prize collecting steiner tree', 'LastEditDate': '2013-04-07T11:49:54.243', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Armin Meisterhirn', 'PostTypeId': '1', 'OwnerUserId': '7866', 'Body': '<p>I\'m reading about the <strong>prize collecting steiner tree</strong> problem and an approximation algorithm that uses randomization to set a lower bound on the optimal solution (see Chapter 5.7 in <a href="http://www.designofapproxalgs.com/book.pdf" rel="nofollow"> The Design of Approximation Algorithms </a> by Williamson and Shmoys). I don\'t understand the second line in the proof for Lemma 5.16: <img src="http://i.stack.imgur.com/9uldl.png" alt="Lemma 5.16">.</p>\n\n<p>It seems to me that $V-V(T)$ is a much larger set than $U$. So, how can the total penalty for this set be upper bounded by the total penalty of a set that is much smaller?</p>\n', 'Tags': '<algorithm-analysis><optimization><approximation><trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T11:49:54.243', 'CommentCount': '0', 'AcceptedAnswerId': '11070', 'CreationDate': '2013-04-04T00:13:44.200', 'Id': '11069'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've done some testing of different initial temperatures in my simulating annealing algorithm and noticed the starting temperature has an affect on the performance of the algorithm.</p>\n\n<p>Is there any way of calculating a good initial temperature?</p>\n", 'ViewCount': '1201', 'Title': 'Initial temperature in simulated annealing algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-09T23:24:51.137', 'LastEditDate': '2013-04-08T14:45:40.150', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7636', 'Tags': '<optimization><artificial-intelligence><heuristics>', 'CreationDate': '2013-04-08T01:23:34.130', 'Id': '11126'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Traditionally, linear programming is used to find the one optimal solution to a set of constraints, variables and a goal (all described as linear relationships). Sometimes, when the objective is parallel to a constraint, there are infinite or many equally good optimal solutions. I am not asking about this latter case.</p>\n\n<p>I am more interested in finding many solutions that are in the feasible region generated by my set of constraints. But I would like the solutions I find to be \'scattered\' around the feasible region in the sense that they are maximally far from one another. Is there a known way to, without running a solver multiple times, generate multiple solutions and use the objective function to enforce that the solutions should be separated?</p>\n\n<p>For example, any linear program with decisions a and b and constraints w &lt;= a &lt;= x and y &lt;= b &lt;= z can be \'duplicated\' to find two solutions. Our new linear program has variables a1, a2, b1, and b2 and the constraints w &lt;= a1 &lt;= x and w &lt;= a2 &lt;= x and similar for b1, b2. However, when it comes to forming an objective function we run into trouble as we can not use norms other than the L1-norm without discarding linearity and we can not truly even use the L1 norm because it is not possible (so far as I know) to encode absolute values.</p>\n\n<p>Perhaps I should look into convex optimization or semidefinite programming or something?</p>\n\n<p>Is there a known way to generate a set of solutions to a linear program, and using an objective that enforces "distance" between the solutions?</p>\n', 'ViewCount': '95', 'Title': 'Finding a set of maximally different solutions using linear programming or other optimization technique', 'LastActivityDate': '2013-11-08T11:14:17.417', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7687', 'Tags': '<optimization><linear-programming>', 'CreationDate': '2013-04-11T20:52:42.737', 'Id': '11241'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In an exercise I have to show that minimizing a multivariate polynomial with $n$ variables over the hyper-cube $H = \\{ (x_1, \\ldots, x_n) : 0 \\leq x_i \\leq 1 \\}$ is NP-Hard. Formally, given $p(x_1, \\ldots, x_n)$ and $\\alpha$, does $\\min_{0 \\leq x_i \\leq 1} p(x_1, \\ldots, x_n) \\leq \\alpha$?</p>\n\n<p>My idea is to reduce it to MAX-SAT as follows. Suppose I am given the formula:</p>\n\n<p>$(x_1 \\vee \\overline{x_2} \\vee x_3) \\wedge (\\overline{x_1} \\vee \\overline{x_3}) \\wedge (\\overline{x_1} \\vee x_2 \\vee \\overline{x_3})$</p>\n\n<p>Then I consider:</p>\n\n<p>$p(y_1, y_2, y_3) = y_1 (1 - y_2) y_3 + (1 - y_1) (1 - y_3) + (1 - y_1) y_2 (1 - y_3)$</p>\n\n<p>If $p$ reaches a minimum at a corner of $H$ then the assignment:\n$$x_i = \\textit{true} \\ \\text{if} \\ y_i = 0 \\ \\text{and} \\ x_i = \\textit{false} \\ \\text{if} \\ y_i = 1$$\nis a solution for MAX-SAT value for the corresponding formula and since MAX-SAT is NP-Hard we are done. However, how do I proceed if $p$ reaches its minimum at an interior point? Or is it the case that it will always be a corner?</p>\n', 'ViewCount': '118', 'Title': 'Minimizing a multivariate polynomial over the hyper-cube is NP-Hard', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-12T03:01:45.210', 'LastEditDate': '2013-04-11T23:39:17.320', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'OwnerDisplayName': 'fran.aubry', 'PostTypeId': '1', 'OwnerUserId': '7697', 'Tags': '<complexity-theory><reductions><optimization><np-hard>', 'CreationDate': '2013-04-11T15:42:38.420', 'Id': '11246'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a compiler optimization which should be quite common, but I can not find a name for it, nor a reference that describes it. </p>\n\n<p>Given an integer x, not known at optimization time, a known constant c and the following program</p>\n\n<pre><code>x_1 = x * c\nx_2 = x_1 / c\nprint(x_2)\n</code></pre>\n\n<p>It's pretty clear that the code can be optimized to</p>\n\n<pre><code>print(x)\n</code></pre>\n\n<p>What is the name of this optimization? Is there a paper/book describing it?</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '38', 'Title': 'What is the name of the optimization that removes self eliminating multiplication-division statements?', 'LastActivityDate': '2013-04-16T16:19:39.100', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7752', 'Tags': '<optimization><compilers>', 'CreationDate': '2013-04-16T14:41:40.720', 'Id': '11356'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider this problem: </p>\n\n<blockquote>\n  <p>Given an undirected graph $G = (V, E)$, find $G' = (V', E')$ such that:</p>\n  \n  <ol>\n  <li>$G'$ is an induced subgraph of $G$</li>\n  <li>$G'$ has no 3-cliques</li>\n  <li>$|V'|$ is maximal</li>\n  </ol>\n</blockquote>\n\n<p>So the least number of vertices must be eliminated from $G$ so that 3-cliques are eliminated.</p>\n\n<p>An equivalent problem would be to find a 2-coloring for $G$ such that if $(v_1, v_2, v_3) \\in V$ and $((v_1, v_2), (v_2, v_3), (v_3, v_1)) \\in V$, </p>\n\n<ol>\n<li><p>$(v_1.color == v_2.color \\wedge v_2.color == v_3.color \\wedge v_3.color == v_1.color) = False$</p></li>\n<li><p>The (absolute) difference between the number of nodes with color 1 and the number of nodes with color 2 is maximal.</p></li>\n</ol>\n\n<p>Can anyone think of a polynomial-time algorithm to solve one of these problems?</p>\n", 'ViewCount': '155', 'Title': 'Finding the largest 3-clique-free induced subgraph', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-24T10:36:57.313', 'LastEditDate': '2013-04-24T06:13:48.183', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '11534', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '3101', 'Tags': '<algorithms><graph-theory><graphs><optimization>', 'CreationDate': '2013-04-23T15:30:56.217', 'Id': '11518'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I have an optimization problem called $k$-foo which asks for a solution of size $k$ minimizing some quality criterion.</p>\n\n<p>Now the corresponding decision problem $foo(M)$ would be: <br> Is there a solution to foo with quality at least $M$ of size $k$.</p>\n\n<p>For problems on one parameter (for example vertex cover) it is obvious that solving the optimization problem sovles the decision problem.</p>\n\n<p>But here I do not see such a correspondance between the $k$-foo optimization problem and the $foo(M)$ decision problem. How does for example showing that $foo(M)$ is NP-hard implies that $k$-foo is NP-hard?</p>\n\n<p>The $k$-center problem is an example of such a problem where the decision version takes the radius as input and asks wether a solution of size $k$ exists.  </p>\n", 'ViewCount': '80', 'Title': 'Decision vs Optimization version for Problems of two Parameters', 'LastEditorUserId': '7921', 'LastActivityDate': '2013-04-26T23:12:46.047', 'LastEditDate': '2013-04-26T23:12:46.047', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7921', 'Tags': '<reductions><optimization><np-hard><decision-problem>', 'CreationDate': '2013-04-26T22:04:14.953', 'FavoriteCount': '1', 'Id': '11589'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have seen some problems that are NP-hard but polynomially solvable in fixed dimension.</p>\n\n<p>Examples, I think, are Knapsack that is polynomial time solvable if the number of items is fixed and Integer Linear Programming with fixed number of variables or constraints by Lenstras result.</p>\n\n<p>Questions:</p>\n\n<p>What are other examples of NP-hard problems that become polynomial time solvable if the dimension is fixed?</p>\n\n<p>Are there problems for which this is not the case?</p>\n\n<p>Is this always the case for problems that admit an FPTAS/pseudo-polynomial time algorithm such as Knapsack?</p>\n', 'ViewCount': '441', 'Title': 'NP complete problems that are solvable in polynomial time if the input (e.g. number of variables) is fixed?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-05-10T12:17:04.700', 'LastEditDate': '2013-05-10T12:17:04.700', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11762', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7309', 'Tags': '<np-complete><optimization><decision-problem><linear-programming><parametrized-complexity>', 'CreationDate': '2013-05-03T15:30:53.200', 'Id': '11761'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We wish to manufacture n distinct hardware items. Each item needs to go through 3 stages of processing. The first stage called design can only be performed by our master designer who works by starting work on an item, designing it through to the end, and only then starting on another item. The remaining two phases, called assembly and testing, are outsourced and for each of them there is an infinite supply of people who can perform the corresponding task as soon as it is assigned to them. Naturally, each item first needs to be designed, then assembled, and then tested.</p>\n\n<p>Each item a requires d\u2090 hours of design, a\u2090 hours of assembly, and t\u2090 hours of testing. We are interested in determining the order in which we should design the items so that we minimize the time by which all items will be ready. For example, if we only had two pieces and we first designed item 1 and then designed item 2, the time by which both items would be finished is</p>\n\n<p>max{d\u2081 + a\u2081 + t\u2081, d\u2081 + d\u2082 + a\u2082 + t\u2082}.</p>\n\n<p>If, alternatively, we first design item 2 and then item 1, the time by which both items would</p>\n\n<p>be finished is</p>\n\n<p>max{d\u2082 + a\u2082 + t\u2082, d\u2082 + d\u2081 + a\u2081 + t\u2081}.</p>\n\n<p>Give a O(n log n) algorithm which takes as input n triples (d, a, t) and determines the optimal design order.</p>\n\n<p>Thanks for your help guys!</p>\n', 'ViewCount': '80', 'Title': 'Interval Scheduling Optimization type of Problem, optimal order of manufacture', 'LastActivityDate': '2013-05-07T06:43:02.403', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8075', 'Tags': '<algorithms><asymptotics><optimization><scheduling>', 'CreationDate': '2013-05-07T06:43:02.403', 'Id': '11847'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an expression $$Ax+By+Cz.$$ where $A$, $B$ and $C$ are positive constants $\\ge1$. The variables $x$, $y$ and $z$ are non-negative integers. I am also given a number $T$. </p>\n\n<p>I want to find the largest integer value such that it is less than $T$ and not satisfied by $Ax+By+Cz$, how can I do it without using brute force. </p>\n', 'ViewCount': '82', 'Title': 'Finding the required value of an algebric expression', 'LastEditorUserId': '8110', 'LastActivityDate': '2013-05-14T02:02:03.263', 'LastEditDate': '2013-05-12T21:43:10.770', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><optimization><linear-programming><linear-algebra>', 'CreationDate': '2013-05-10T17:11:35.057', 'Id': '11940'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have an expression $$Ax+By+Cz.$$ where $A$, $B$ and $C$ are positive constants $\\ge1$. The variables $x$, $y$ and $z$ are non-negative integers. I am also given a number $T$. </p>\n\n<p>I want to find the largest integer value such that it is less than $T$ and not satisfied by $Ax+By+Cz$, how can I do it without using brute force. </p>\n\n<p>I can use LinearProgramming but that will give me the value that is satisfied by $Ax+By+Cz$ such that it is less than T. But I want to find the largest value which is less than T, but doesn't belong to $Ax+By+Cz$ for any value of x, y and z</p>\n", 'ViewCount': '67', 'Title': 'Issues with an optimization problem', 'LastEditorUserId': '8110', 'LastActivityDate': '2013-05-12T22:24:06.363', 'LastEditDate': '2013-05-12T22:24:06.363', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><optimization><linear-programming>', 'CreationDate': '2013-05-12T21:48:08.947', 'FavoriteCount': '1', 'Id': '11977'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a function which takes 22 real-valued parameters as input that returns a real value. The function is reasonably fast for low return values (ms/seconds/minutes), but takes much longer (minutes/hours) to compute for high/optimal values. The noise will also increase with higher values.</p>\n\n<p>I expect the function to contain many local maxima, and I cannot use vanilla gradient descent.</p>\n\n<p>I want to find the parameters that maximize the expected return value.</p>\n\n<p>As this is going to run on a desktop computer, I don't mind storing all inputs and results in memory, as the limitation is probably going to be the run-time anyway.</p>\n\n<p>Do you have any suggestions for an appropriate optimization algorithm?</p>\n", 'ViewCount': '36', 'Title': 'What is an appropriate global optimization technique for a noisy and expensive function?', 'LastActivityDate': '2013-05-15T02:11:37.833', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1797', 'Tags': '<optimization>', 'CreationDate': '2013-05-15T02:11:37.833', 'Id': '12029'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is something I've been wondering for years. Software like Mathematica is great at manipulating expressions into simplified, factorized, and other forms. I'm wondering if there's a way, theoretically and/or practically, to find the form that has the fewest operations. The next step would be to prefer operations that are faster (ie. multiply instead of divide). Lastly, to find a form that maximizes extraction of repetitive subexpressions, so that the subexpressions can be evaluated once and substituted for potentially significant performance gains. Has any research been done in this area? Thanks.</p>\n", 'ViewCount': '95', 'Title': 'Using a computer algebra system to optimize mathematical expressions', 'LastEditorUserId': '8233', 'LastActivityDate': '2013-09-07T11:32:43.477', 'LastEditDate': '2013-05-17T18:58:26.217', 'AnswerCount': '4', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8233', 'Tags': '<optimization><computer-algebra>', 'CreationDate': '2013-05-17T18:24:29.617', 'Id': '12092'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can someone suggest an algorithm to solve job assignment problem with condition?</p>\n\n<p>With condition means that some jobs cannot be done by some workers. For example table as shown below:</p>\n\n<p><img src="http://i.stack.imgur.com/48Tqv.png" alt="enter image description here"></p>\n\n<p>In this table x - means that it is impossible to do. For example, worker 1 cannot do jobs 1,3 and 5.</p>\n\n<p>I encountered such situation and there may be cases as shown above when usual Hungarian algorithm seems cannot solve such task because there is no way to complete all tasks by distributing one task per worker. </p>\n\n<p>However, my main case it is allowed that one worker wil do several tasks (tasks, which worker can do). Main task is to complete all jobs using existing workers, but it is desirable that, all workers do roughly same number of tasks.</p>\n\n<p>So is there some solution of such problem? May be any algorithms do exist?</p>\n', 'ViewCount': '137', 'Title': 'Algorithm to solve job assignment problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:51:50.040', 'LastEditDate': '2013-05-19T14:51:50.040', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5207', 'Tags': '<algorithms><optimization><linear-programming><scheduling><assignment-problem>', 'CreationDate': '2013-05-18T19:39:04.773', 'Id': '12122'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Here's the problem:</p>\n\n<p>I have a collection of collections, $C$, where each $c\\in C$ is a collection of sets $X\\subset U$.  Denote $c_i$ as the i-th $X$ in $c$.  Informally, I want to map all the sets in each collection to bins, where no two sets in a single collection can occupy the same bin, such that the sum of the sizes of the unions of all sets in each bin is minimized.  More formally:</p>\n\n<p>Let $N = \\max_{c \\in C} |c|$, and let $P_N$ be the set of all permutations of all non-empty subsets of the set $\\{1,2,...,N\\}$.  I wish to define a mapping:</p>\n\n<p>$$F : C \\rightarrow P_{N},\\ s.t.\\ \\forall c \\in C\\ (|F(c)| = |c|)$$</p>\n\n<p>with bin sets\n$$B(k) = \\{X \\subset U : \\exists c \\in C\\ (\\exists i \\in \\{1,2,...,|c|\\}\\ s.t.\\ c_i = X \\wedge (F(c))_i = k)\\}$$</p>\n\n<p>Such that the quantity</p>\n\n<p>$$\\sum_{k=1}^{N} { \\Biggl|\\bigcup_{X \\in B(k)}\\Biggr| } $$</p>\n\n<p>is minimized.</p>\n\n<hr>\n\n<p>Off the bat, I'd guess that this is an NP-hard problem - a reduction from Set Cover seems to be just within reach.  </p>\n\n<p>Even a greedy algorithm that iteratively processes each collection $c \\in C$, producing minimal results each time, requires $O(2^N \\cdot |C|)$ time using dynamic programming, where $|U|$ is assumed to be a constant factor.</p>\n\n<p>I'm having trouble proving whether or not the Greedy algorithm is even optimal - or if a more efficient solution exists.  Anyone have any thoughts?</p>\n\n<hr>\n\n<p>Alternatively, minimizing the quantity:</p>\n\n<p>$$\\max_{1 \\leq k \\leq N} {\\Biggl| \\bigcup_{X \\in B(k)} \\Biggr| }$$</p>\n\n<p>Is also of interest.  It's definitely a different problem, as demonstrated by a simple case where $C$ has 2 collections, one of the form $\\{\\{1\\}, \\{3, 4\\}\\}$, and the other $\\{\\{2\\}, \\{3, 4\\}\\}$.  I am not sure this problem is any easier though</p>\n", 'ViewCount': '74', 'Title': 'Overlap Maximization problem', 'LastEditorUserId': '7614', 'LastActivityDate': '2013-05-22T20:36:21.743', 'LastEditDate': '2013-05-22T20:36:21.743', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7614', 'Tags': '<algorithms><time-complexity><optimization><sets>', 'CreationDate': '2013-05-22T17:58:46.787', 'FavoriteCount': '1', 'Id': '12219'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m writing deduplication program that implements content-defined chunking. Just now i know 3 algorithms to do it (<a href="http://mattmahoney.net/dc/dce.html#Section_527" rel="nofollow">hashing a fixed-size window and selecting hashes &lt; maxuint/N</a>, <a href="http://encode.ru/threads/456-zpaq-updates?p=29800&amp;viewfull=1#post29800" rel="nofollow">hashing an order-1 bounded window</a>, and <a href="http://research.microsoft.com/~gurevich/Opera/183.pdf" rel="nofollow">local maxima chunking</a>). Are you know other algorithms or ideas to improve these ones? My goal is to get best compression for a given average chunk size.</p>\n\n<p>A few deduplication-related links:<br>\n<a href="http://en.wikipedia.org/wiki/Data_deduplication" rel="nofollow">http://en.wikipedia.org/wiki/Data_deduplication</a><br>\n<a href="http://encode.ru/threads/1726-Deduplication-X-Files" rel="nofollow">http://encode.ru/threads/1726-Deduplication-X-Files</a>  </p>\n', 'ViewCount': '200', 'Title': 'Deduplication: how to implement content-defined chunking?', 'LastEditorUserId': '8825', 'LastActivityDate': '2013-06-26T20:16:24.343', 'LastEditDate': '2013-06-26T20:16:24.343', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8825', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-06-26T19:40:17.520', 'FavoriteCount': '1', 'Id': '12919'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>On the <a href="http://en.wikipedia.org/wiki/Circulation_problem" rel="nofollow">circulation problem page on wikipedia</a>, the multicommodity circulation problem formulation seems to be insufficient, since we can just set all but one flow to $0$, and reduce it to a circulation problem.</p>\n\n<p>I can\'t find any concrete description of multicommodity circulation problem to verify the correctness. There is only <a href="http://transci.journal.informs.org/content/8/4/355.full.pdf" rel="nofollow">one paper</a> I can find and it\'s behind a pay wall.</p>\n\n<p>Here is a formulation I thought to make more sense:</p>\n\n<p>Let $G=(V,E)$. $l_i,u,c_i:E\\to \\mathbb{R}$ for $1 \\leq i\\leq n$. We want to find a sequence of flow function $f_i$, such that:</p>\n\n<p>$$\n\\begin{align}\n\\text{min} &amp; \\sum_{(v,w) \\in E} c_i(v,w)f_i(v,w) \\\\\n\\text{s.t.} &amp; \\sum_{(v,w) \\in E} f_i(v,w) = 0 \\text{ for } 1 \\le i \\le n, v \\in V, \\\\\n&amp; l_i(v,w) \\leq f_i(v,w) \\text{ for } 1 \\le i \\le n, (v,w)\\in E\\\\\n&amp; \\sum_{i=1}^n f_i(v,w) \\leq u(v,w) \\text{ for } (v,w)\\in E\\\\\n\\end{align}\n$$</p>\n\n<p>Is this a formulation of multicommodity circulation problem?</p>\n', 'ViewCount': '71', 'Title': 'Multicommodity circulation formulation', 'LastEditorUserId': '220', 'LastActivityDate': '2013-07-06T13:07:35.350', 'LastEditDate': '2013-07-05T18:43:56.970', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12965', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<reference-request><optimization><network-flow>', 'CreationDate': '2013-06-29T09:56:58.443', 'Id': '12963'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The problem is a scheduling problem with n jobs and k machines. Each job i can be started at any time, but its duration is not exactly known except a time span interval. For example, a job may take anything from 5 mins to 10 mins. The list of jobs and their duration interval is given to the scheduler and the aim is to minimize the time all jobs would be finished. \nIn interval scheduling, the job start and end time is given and the length is fixed. Here, the job can start and end anytime (should be done in whole though) but its duration (length) is not exactly known.</p>\n\n<p>I searched the literature for this problem but couldn't find it. Is there any keywords I can use or any reference for this problem? Is it well-defined?</p>\n", 'ViewCount': '159', 'Title': 'A variant of job assignment (scheduling) problem with variable time span', 'LastActivityDate': '2013-08-02T19:50:55.893', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8995', 'Tags': '<optimization><scheduling><process-scheduling><assignment-problem>', 'CreationDate': '2013-07-03T18:47:45.597', 'Id': '13066'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider we have a finite set $S$ with $n$ distinct elements. We want to find a subset $\\{a_1, a_2, \\dotsc, a_k\\}\\subseteq S$ ($k\\ll n$) such that a function $f(a_1,a_2,\\dotsc,a_k)$ is maximized. Consider $f$ to be a <a href="http://en.wikipedia.org/wiki/Symmetric_function" rel="nofollow">symmetric function</a> that takes $k$ arguments.</p>\n\n<hr>\n\n<p>More specifically, we are given $n = 120$ items, each item being associated with three positive numbers $(A_i, B_i,C_i)$, and we want to choose $k=12$ items within this set such that</p>\n\n<p>$$ \\frac{\\sum_{k=1}^{12} A_{i_k} \\times \\left\\lceil\\frac{\\sum_{k=1}^{12} B_{i_k}}{10000}\\right\\rceil}{\\sum_{k=1}^{12} C_{i_k}} $$</p>\n\n<p>is maximal.</p>\n\n<hr>\n\n<p>If we solve it by exhaustive search it requires $\\binom{120}{12} \\approx 10^{16}$ operations. Is there faster method to this problem? Approximate solution is also fine.</p>\n', 'ViewCount': '107', 'Title': 'Subset optimization problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-07T07:40:27.017', 'LastEditDate': '2013-08-07T07:40:27.017', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9021', 'Tags': '<algorithms><optimization><approximation>', 'CreationDate': '2013-07-04T15:56:53.990', 'Id': '13088'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a complete $n$-partite graph, where each partite set has $n$ vertices (yes it\'s also $n$), so the graph has $n^2$ vertices in total.  My problem is to find a minimum weight $n$-clique in the graph.  I would like to know whether the problem can be solved in polynomial time. </p>\n\n<p>More details of the terms: </p>\n\n<p><strong>Complete $n$-partite graph</strong>: a graph in which vertices are adjacent if and only if they belong to different partitions (<a href="http://en.wikipedia.org/wiki/Glossary_of_graph_theory" rel="nofollow">wikipedia</a>).  There are $n$ partitions in the graph.  (In my case, each partition contains exactly $n$ vertices.)</p>\n\n<p><strong>Minimum weight clique</strong>:  Every edge in the graph has a weight.  The weight of a clique is the sum of the weights of all edges in the clique.  The goal is to find a clique with the minimum weight.</p>\n\n<p>Note that the size of the required clique is $n$, which is the largest clique size in a complete $n$-partite graph, and it is always attainable. </p>\n\n<p>I have searched for hours and there seems no research tackling the exact problem.  Any suggestions?</p>\n', 'ViewCount': '107', 'Title': 'Is this NP-hard: min-weight n-clique in a complete n-partite graph', 'LastEditorUserId': '755', 'LastActivityDate': '2013-07-08T04:47:58.273', 'LastEditDate': '2013-07-08T00:19:34.617', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13154', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8732', 'Tags': '<complexity-theory><optimization><np-hard>', 'CreationDate': '2013-07-07T16:38:18.347', 'Id': '13135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm writing a program (using genetic algorithms) that finds sort-of-optimal scheduling plan for a factory.</p>\n\n<ul>\n<li>The factory has several types of machines (say, <code>locksmith, miller, welding</code>)</li>\n<li>There are few machines of each type. (say, <code>3 locksmiths, 2 millers, 3 welders</code>)</li>\n<li>There are several types of operations (some machines do more than one operation on the job, say, <code>locksmith does soldering and assembling</code>).</li>\n<li>The jobs on the machines have different times, all known beforehand.</li>\n<li>The jobs have dependencies on jobs done before (say, <code>a product's made of 10 screws and 4 subparts, each of which needs 4 screws</code>).</li>\n</ul>\n\n<p>From what I searched, this looks sort of like a Flow Shop problem. The difference is in the dependencies and in the same machine doing different operations with different times on a job.</p>\n\n<hr>\n\n<h2>My main question is:</h2>\n\n<p><strong>Is there some kind of a classification of these problems?</strong> A summary telling the differences?</p>\n\n<p>For example, I don't understand much of how do these differ: Open Shop, Job Shop, Flow Shop, Permutation Flow Shop. And whether or not I missed something similar that could fit better to my problem.</p>\n\n<hr>\n\n<p>As a side question, what approach do you think could help me best with the unusual requirements I've posted above? I'm writing my current approach below.</p>\n\n<p>So far I've been able to work with the tree of dependencies without regard to the makespan times: just making a plan - a list of IDs, really - of what comes after what, from looking at the tree of what's been done so far and what are the leaves (nodes having done all their dependencies).</p>\n\n<p>This allows for fast creation of meaningful individuals in the Genetic Algorithm population, but there seems to be no computationally cheap way to learn the individual's makespan time (which I have as the fitness function).</p>\n\n<p>For that I have to create a calendar, or Gantt chart, if you will, to which I put the operations on the jobs in the earliest place possible, in the machine queue that's free at that moment, etc. The whole plan has to materialize and that seems the most costly computation of the whole problem.</p>\n", 'ViewCount': '234', 'Title': 'Classification of job shop scheduling problems', 'LastActivityDate': '2013-07-23T01:50:49.417', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13366', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9122', 'Tags': '<optimization><scheduling><heuristics><genetic-algorithms>', 'CreationDate': '2013-07-11T03:03:06.187', 'Id': '13219'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given an array of integers. Find two disjoint contiguous sub-arrays such that the absolute difference between the sum of two sub-array is maximum.</p>\n\n<p>The sub-arrays should not overlap and it can not be empty.</p>\n\n<p>eg [2 -1 -2 1 -4 2 8]</p>\n\n<p>ans (-1 -2 1 -4) (2 8)</p>\n\n<p>diff = 16</p>\n\n<p>Better than $\\mathcal O(n^2)$ solution was expected.</p>\n', 'ViewCount': '445', 'Title': 'Find disjoint contiguous sub-arrays in better than $\\mathcal O(n^2)$', 'LastEditorUserId': '713', 'LastActivityDate': '2013-07-12T14:19:09.267', 'LastEditDate': '2013-07-12T07:55:55.977', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8596', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-07-12T06:42:51.217', 'Id': '13239'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Andrew W. Appel\'s book, <em>Modern Compiler Implementation in ML</em>, he says under chapter 17 that <em>Computability theory shows that it will always be possible to invent new optimizing transformations</em> and proceeds to prove that a <em>fully optimizing compiler</em> will solve the halting problem: A program <em>Q</em> that produces no output and never halts can easily be replaced by its optimal representation, <em>Opt(Q)</em>, being "L: goto L". So a fully optimizing compiler can solve the halting problem.</p>\n\n<p>So my question is this: <strong>Does a fully optimizing compiler exist for terminating programs?</strong> My only thoughts are the following: Even though a program is guaranteed to terminate, it can still be arbitrarily complex, and for any concrete optimizing compiler, C, one could perhaps construct a program that takes C as input and somehow produces a worse program as some kind of corner case.</p>\n\n<p>Also, <strong>What are the implications of restricting ourselves to terminating programs?</strong></p>\n', 'ViewCount': '185', 'Title': 'Do fully optimizing compilers for terminating programs exist?', 'LastEditorUserId': '9213', 'LastActivityDate': '2013-07-18T16:25:27.483', 'LastEditDate': '2013-07-18T16:25:27.483', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '13315', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '9213', 'Tags': '<computability><compilers><program-optimization>', 'CreationDate': '2013-07-17T12:14:57.423', 'Id': '13313'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Now I am confused about symbolic execution (SE) and reachability analysis (RA). As I know, SE uses symbols to execute some code to reach each branch with branch conditions. And RA can be used to find the reachability of each branch, right? When RA is used, we can extract the branch condition for each branch. If so, what's the difference between them? Can they be swift? Are they all static analysis?</p>\n", 'ViewCount': '54', 'Title': 'Difference between symbolic execution and reachability analysis', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-30T10:30:32.577', 'LastEditDate': '2013-07-30T10:30:32.577', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13497', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9406', 'Tags': '<algorithm-analysis><compilers><program-optimization>', 'CreationDate': '2013-07-29T16:10:30.270', 'Id': '13492'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a constrainted optimization problem in the (Shannon) matrix entropy $\\mathtt{(sum(entr(eig(A))))}$. The matrix $A$ can be written as the sum of rank 1 matrices of the form $[v_i\\,v_i^T]$ where $v_i$ is a given normalized vector. The coefficients of the rank one matrices are the unknowns in which we optimize and they have to be larger than zero and sum up to 1.</p>\n\n<p>In a CVX-like syntax the problem goes as follows:\ngiven variable $\\mathtt{c(n)}$ </p>\n\n<p>$$\\text{minimize} \\qquad \\mathtt{sum(entr(eig(A)))}$$ </p>\n\n<p>$$\\begin{align} \\text{subject to} \\qquad A &amp;= \\sum c_i v_i v_i^T\\\\ \n\\sum c_i &amp;= 1\\\\ c_i &amp;\\ge 0\\end{align}$$.</p>\n\n<p>Does any have an idea how to solve this efficiently? I already know it probably can't be cast as a semi-definite programming (SDP) problem.</p>\n", 'ViewCount': '274', 'Title': 'Constrainted Optimization Problem in Matrix Entropy', 'LastEditorUserId': '903', 'LastActivityDate': '2013-09-28T08:34:27.347', 'LastEditDate': '2013-08-01T17:56:02.527', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '14575', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '9430', 'Tags': '<optimization><entropy>', 'CreationDate': '2013-07-30T16:05:17.310', 'Id': '13522'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say I have a finite set S with, say, 1000 elements, and a function f on some subset of S. Suppose that f has no useful mathematical properties.</p>\n\n<p>What algorithm is most relevant in finding the subset of S that maximizes f?</p>\n", 'ViewCount': '34', 'Title': 'Algorithm to maximize function of subsets', 'LastActivityDate': '2013-08-01T00:04:07.833', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9224', 'Tags': '<optimization>', 'CreationDate': '2013-07-31T23:44:58.407', 'Id': '13552'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a $D$-dimensional grid with the size $(N_1, \\ldots, N_D)$, where $N_i$ are natural numbers, and a "flat block size" $M$, also a natural number. I want to find a decomposition $(m_1, \\ldots, m_D)$ such that:</p>\n\n<ol>\n<li><p>$\\prod_{i=1}^D m_i = M$,</p></li>\n<li><p>$R = \\prod_{i=1}^D f(N_i, m_i) m_i - \\prod_{i=1}^D N_i$ is as low as possible. Here $f(N,m)$ is the minimal number of blocks of length $m$ necessary to cover a 1D grid with size $N$ (or, formally, $f(N, m) = N / m$ if $N$ is a multiple of $m$, and $f(N,m) = N\\,\\mathrm{div}\\,m + 1$ otherwise).</p></li>\n<li><p>The number of $m_i$ equal to 1 is as high as possible (but this is low priority, the condition 2 is more important).</p></li>\n</ol>\n\n<p>How should I approach this? Is there some standard algorithm this can be reduced to?</p>\n\n<p>In my case $M$ is not very big (of the order of 1000). Also, an absolute minimum in all cases is not strictly required; if there is an approximate algorithm, it will do to.</p>\n\n<p>(In case anyone is interested in the application, I want to use it to find work group dimensions for an OpenCL kernel with a known global size and total number of work items).</p>\n', 'ViewCount': '23', 'Title': 'Optimal coverage of a $D$-dimensional grid with small blocks', 'LastEditorUserId': '9455', 'LastActivityDate': '2013-08-01T06:13:22.680', 'LastEditDate': '2013-08-01T05:44:12.093', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13556', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9455', 'Tags': '<algorithms><optimization><dynamic-programming>', 'CreationDate': '2013-08-01T02:03:46.657', 'Id': '13555'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '218', 'Title': 'Finding maximal factorization of regular languages', 'LastEditDate': '2013-08-10T18:28:15.740', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1382', 'FavoriteCount': '3', 'Body': u"<p>Let language $\\mathcal{L} \\subseteq \\Sigma^*$ be regular.</p>\n\n<p>A factorization of $\\mathcal{L}$ is a maximal pair $(X,Y)$ of sets of words with</p>\n\n<ul>\n<li>$X \\cdot Y \\subseteq \\mathcal{L}$</li>\n<li>$X  \\neq \\emptyset \\neq Y$,</li>\n</ul>\n\n<p>where $X \\cdot Y = \\{xy$ | $x \\in X, y \\in Y\\}$.</p>\n\n<p>$(X,Y)$ is maximal if for each pair $(X',Y') \\neq (X,Y)$ with $X'\\cdot Y' \\subseteq \\mathcal{L} $ either $X \\not \\subseteq X'$ or $Y \\not \\subseteq Y'$.</p>\n\n<p>Is there a simple procedure to find out which pairs are maximal?</p>\n\n<p>Example:</p>\n\n<p>Let $\\mathcal{L} = \\Sigma^\u2217ab \\Sigma^\u2217$. The set $F = \\{u, v, w\\}$ is computed: </p>\n\n<ul>\n<li><p>$u =(\\Sigma^\u2217, \\Sigma^\u2217ab\\Sigma^\u2217)$</p></li>\n<li><p>$v = (\\Sigma^\u2217a\\Sigma^\u2217, \\Sigma^\u2217b\\Sigma^\u2217)$</p></li>\n<li><p>$w = (\\Sigma^\u2217ab\\Sigma^\u2217, \\Sigma^\u2217) $</p></li>\n</ul>\n\n<p>where $\\Sigma = \\{a,b\\}$.</p>\n\n<p>Another example:</p>\n\n<p>$\\Sigma = \\{a, b\\}$ and $\\mathcal{L} = \\Sigma^*a\\Sigma$\nFactorization set $F = \\{q, r, s, t\\}$ with</p>\n\n<ul>\n<li><p>$q = (\\Sigma^*, \\mathcal{L})$</p></li>\n<li><p>$r = (\\Sigma^*a, \\Sigma + \\mathcal{L})$</p></li>\n<li><p>$s = (\\Sigma^*aa, \\epsilon + \\Sigma + \\mathcal{L})$</p></li>\n<li><p>$t = (\\mathcal{L}, \\epsilon + \\mathcal{L}) $</p></li>\n</ul>\n", 'Tags': '<algorithms><regular-languages><optimization>', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-12T07:32:56.617', 'CommentCount': '9', 'AcceptedAnswerId': '13713', 'CreationDate': '2013-08-05T16:12:02.770', 'Id': '13617'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'m trying to find an algorithm for a motion planning problem. I have $N$ points, $P_1$ to $P_N$, in $k$-dimensional cartesian space, defining $N-1$ segments. The problem is about constructing the fastest motion plan, that is, a function $p(t)$, <em>approximating</em> the path along the segments, bound by some constraints:</p>\n\n<ul>\n<li><p>For each segment defined by the points $P_i$ and $P_{i+1}$, let $M_i$ be some point on this segment ($M_i$ is part of the solution, not the input). For now, let\'s assume it\'s strictly within the segment. Now, the constraint is that our sought plan $p(t)$ starts with an initial segment from $P_1$ to $M_1$, is followed by $N-2$ second-order B\xe9zier curves, each defined by the control points $(M_i, P_{i+1}, M_{i+1})$, and finally followed by a segment from $M_{n-1}$ to $P_n$.</p></li>\n<li><p>Velocity is preserved on boundaries between the curves and the start/end segments. Note that the <em>direction</em> of velocity is implicitly preserved by the above constraint. (because the vector from $P_i$ to $M_i$ is parallel to the one from $M_i$ to $P_{i+1}$)</p></li>\n<li><p>Along each of the B\xe9zier curves, as well as on the initial and final segment, the acceleration of the plan $p(t)$ is constant. This means that, if the plan specifies that we arrive to $M_i$ at time $T_i$, and it takes $t_i$ time to traverse the curve that begins there, this part of the motion plan is equal to $p(t)=B_i((t-T_i)/t_i)$, where $B_i$ is standard B\xe9zier formula for this curve. Informally, we cannot manually accelerate along the B\xe9zier curves, all we can do is scale the speed/acceleration across an entire curve.</p></li>\n<li><p>The absolute value of acceleration along each dimension $d$ must be no more than $A_d$. The maximum velocity limit may also be defined for each dimension, and I\'m not sure whether or not that would make the problem significantly harder.</p></li>\n<li><p>The initial and final velocity is zero (or possibly a constant).</p></li>\n</ul>\n\n<p>Here\'s a picture of such a path composed of B\xe9zier curves.<img src="http://i.stack.imgur.com/uSjZr.jpg" alt="enter image description here"></p>\n\n<p>Notice that the shape of the path is defined by $N-1$ real numbers $m_i \\in(0, 1)$, which define the position of the points $M_i$ as a convex combination of $P_i$ and $P_{i+1}$. The difficulty of this problem is in determining these numbers. The constraints imply that, given $m_i$, the velocities on every point of the plan are determined up to a common factor. Therefore, knowing $m_i$ of the optimal solution, it is not hard to finish the plan by finding the smallest total plan time which does not violate the acceleration constraints on any of the individual components of the plan.</p>\n\n<p>Ideally, the algorithm would work incrementally - given an optimal solution for the first $n$ points, and the next point, it would fix this solution into an optimal plan for the first $n+1$ points.</p>\n\n<p><strong>Some initial work</strong></p>\n\n<p>Let\'s define $D_i=P_{i+1}-P_i$. Also define $V_i$ to be the velocity of the plan at point $M_i$. But since $V_i$ is parallel to $D_i$, we write $V_i=v_i D_i$, for some $v_i \\in \\mathbb{R}$. Let\'s call $v_i$ the <em>relative speeds</em>.</p>\n\n<p>We will now use some knowledge about B\xe9zier curves to express a relationship between the relative velocities on the ends of a single curve, resulting in an equation involving $v_i$ and $v_{i+1}$. Let\'s forget about our points for a moment and assume we have a quadratic B\xe9zier curve with the control points $(A, B, C)$. If we start with the standard formula for B\xe9zier curves, and factor by $t$, we arrive at:</p>\n\n<p>$$ B(t) = A + 2(B-A)t + (A-2B+C)t^2 $$</p>\n\n<p>The parameter in this curve is $t \\in [0, 1]$, and it is easy to see that $B(0)=A$ and $B(1)=C$. We can also take the derivative of this function and compute it at the ends of the curve:</p>\n\n<p>$$ B\'(0) = 2(B-A) $$\n$$ B\'(1) = 2(C-B) $$</p>\n\n<p>Now return to the problem of finding a relationship between $v_i$ and $v_{i+1}$. The above formula tells us how to compute the velocities at the start and end of a quadratic B\xe9zier curve, however we have to take into account that the B\xe9zier curves in our plan are scaled proportionally in time. So, assume the plan takes $t_i$ time to traverse the i-th B\xe9zier curve. Then, the initial and final velocities along this curve are given by:</p>\n\n<p>$$ V_i = \\frac{2(1-m_i)D_i}{t_i} $$\n$$ V_{i+1} = \\frac{2m_{i+1}D_{i+1}}{t_i} $$</p>\n\n<p>since $(1-m_i)D_i$ corresponds to $B-A$ above, and $m_{i+1}D_{i+1}$ corresponds to $C-B$. Then we substitute $V_i=v_i D_i$ and $V_{i+1}=v_{i+1} D_{i+1}$, which turns the vector equations into real equations. Eradicating $t_i$ gives the following:</p>\n\n<p>$$ v_{i+1} = \\frac{m_{i+1}}{1-m_i}v_i $$</p>\n\n<p>This equation captures many of the constraints of the problem, in terms of real numbers $m_i$ (the relative positions of the splice points) and $v_i$ (the relative velocities at the splice points).\nIn fact, given all $m_i$, this effectively defines the ratios between all $v_i$. Therefore, if after choosing $m_i$ we also choose a value for $v_1$, the plan is completely defined.</p>\n\n<p>Though I don\'t know yet how this helps with finding the optimal $m_i$, taking into account the other constraints (maximum acceleration, starting and ending velocity). While the last equation looks nice, its non-linearity in terms of $m_i$ is not encouraging.</p>\n', 'ViewCount': '156', 'Title': u'Motion planning using second order B\xe9zier curves', 'LastEditorUserId': '4213', 'LastActivityDate': '2013-08-08T01:04:59.303', 'LastEditDate': '2013-08-08T01:04:59.303', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4213', 'Tags': '<optimization><computational-geometry><online-algorithms>', 'CreationDate': '2013-08-07T19:41:56.430', 'Id': '13667'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Given a list of pairs $(a_1,b_1),\\ldots,(a_n,b_n)$, where all $a_i \\geq 0$ and all $b_i &gt; 0$, my general problem is when we can use linear subset scan (described below) to solve the optimization problem of finding the optimal combination of pairs,</p>\n\n<p>$$I^* = \\hbox{argmax}_I F(\\sum_{i \\in I} a_i) / G(\\sum_{i \\in I} b_i)$$</p>\n\n<p>where $F,G$ are given increasing positive functions for positive inputs.</p>\n\n<p>I have found a class of functions where there is a fast solution, namely $F(x) = x + A$ and $G(x) = (x + B)^\\beta$ where $A,B \\geq 0$ and $0 \\leq \\beta \\leq 1$.  In this case, the optimal solution can be found by sorting all pairs $(a_i,b_i)$ in decreasing order according to $a_i/b_i$, and then trying the first $k$ pairs in sorted order for all $k$ and choosing the best solution, and this gives the optimal solution.  (This is an example of linear subset scan optimization.)  </p>\n\n<p>Now I want to know, if there a general class of functions $F,G$, ideally defined by abstract properties, where this linear subset scan approach works, where pairs are sorted either according to $a_i/b_i$ or perhaps sorted according to $H(a_i,b_i)$ where $H$ depends on $F,G$?  This could be a question where someone has a new insight and proof, or simply someone has seen something like this in the literature.  At any rate, I feel like the class of $F,G$ I stated where I have a proof is perhaps not the most general possible, and I'm missing some key abstract property that makes the linear subset scan work.   </p>\n", 'ViewCount': '98', 'Title': 'Generalizing the linear subset scan algorithm to a wider class of objective functions, maybe by finding a paper', 'LastActivityDate': '2014-04-06T23:59:48.870', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithms><optimization><greedy-algorithms>', 'CreationDate': '2013-08-08T23:01:49.873', 'Id': '13680'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need help to understand Chaitin\'s elegant program proof. An elegant program is the shortest program that produces a given output.</p>\n\n<p>Here is the proof:</p>\n\n<blockquote>\n  <p>Construct a program $B$ that takes as input a number $N$ and enumerates all possible programs $P_k$ longer than $N$. $B$ runs the elegance tester $\\mathrm{ET}$ on each enumerated program $P_k$ in turn until it finds some $P_k$ which $\\mathrm{ET}$ claims is elegant. $B$ then runs that $P_k$, thus producing the same output as that $P_k$.</p>\n  \n  <p><strong>Lemma:</strong> B must produce some output.</p>\n  \n  <p><strong>Proof:</strong> There are an infinite number of elegant programs, as noted earlier. So if $\\mathrm{ET}$ works as assumed, $B$ must eventually find one of those elegant programs whereupon it will produce that program\'s output.\n  Now run $B$ with $N$ set to $|B| + 1$ (See note 1). (This is the "threshold size" mentioned in the theorem.) $B$ now will produce the same output as some program $P_k$ which $\\mathrm{ET}$ claimed was elegant. But $P_k$ is longer than $B$, so $P_k$ cannot be elegant because $B$, which is shorter, produced the same output. Therefore, $\\mathrm{ET}$ was wrong when it claimed $P_k$ was elegant. QED.</p>\n</blockquote>\n\n<p>My question is: The proof begins with a program $B$ that is a program "that takes as input a number $N$ and enumerates all possible programs $P_k$ longer than $N$" But because of the halting problem such a program is not possible, so the proof starts dead? There is something I\'m not understanding here.</p>\n', 'ViewCount': '91', 'Title': "Chaitin's elegant programs", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-13T10:15:58.190', 'LastEditDate': '2013-08-13T08:44:01.653', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9631', 'Tags': '<computability><program-optimization>', 'CreationDate': '2013-08-13T07:22:40.833', 'Id': '13728'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm trying to find an algorithm to solve the seating chart problem. The goal is to place pepole at one (or multiple) tables such that the overall happiness is maximized.</p>\n\n<p>Each seat has neighbors. A person on a seat can talk to persons on neighboring seats (note that these neighbors normally sit next to each other or on the opposite side of the table).</p>\n\n<p>Example of a table with 4 seats on each side:</p>\n\n<pre><code>1 2 3 4\n8 7 6 5\n</code></pre>\n\n<p>6 has neighbors 2, 3, 4, 5 and 7, 8 has neighbors 1, 2, 7</p>\n\n<p>There's a matrix describing how well two persons get along with each other. This is a value between 0 and 9 (the relation value). Relations are symmetrical.</p>\n\n<p>Example of a relation matrix with four people:</p>\n\n<pre><code>  A B C D\nA 0 2 0 5\nB 2 0 3 6\nC 0 3 0 1\nD 5 6 1 0\n</code></pre>\n\n<p>The happiness of a single person is the sum of the relation values of all neighboring persons.</p>\n\n<p>How would you find a solution that maximizes the overall happiness?</p>\n", 'ViewCount': '233', 'Title': 'Seating Chart Optimization', 'LastActivityDate': '2013-08-23T09:00:23.500', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13884', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9785', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-08-22T20:00:13.087', 'Id': '13873'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose I have two polynomials $f(x)$ and $g(x)$ and I somehow represent their coefficients.\nI have a couple of ways to hold a polynomial depending on how many significant coefficients the polynomial has.\nI want to determine the amount of significant coefficients in the results of\n$f(x) + g(x)$ ,$f(x) \\cdot g(x)$ , $f(x) - g(x)$ etc. .</p>\n\n<p>But I'd like to do it before I create the object that holds them, is there some efficient way of doing this without calculating the result twice?</p>\n\n<p>I can assume that I know the current rank and number of elements in $f(x)$ and $g(x)$</p>\n\n<p>If this is not possible knowing that the new polynomial's non-trivial coefficients will be at least half of the rank will suffice, but I'm unsure how to do it as well.</p>\n\n<p>I did try to apply various heuristics but didn't come up with something consistent and fast. </p>\n", 'ViewCount': '67', 'Title': 'Calculate the number of elements after multiplying/adding two polynomials', 'LastActivityDate': '2013-08-28T21:13:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14006', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8709', 'Tags': '<algorithms><time-complexity><optimization><efficiency><arithmetic>', 'CreationDate': '2013-08-28T19:13:48.453', 'Id': '13997'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The problem can be formulated as:</p>\n\n<p>$\\min f(\\textbf{x})=\\sum_{i=1}^{n} \\prod_{j \\in N(i)}(1-F(x_i))$ s.t. $\\sum_{i=1}^n x_i \\leq B$ </p>\n\n<p>$N(i)$ is a set of i. And $F_{x_i}$ can be any function with range between [0,1].</p>\n\n<p>if $F(x_i)$ is a convex function and $\\prod_{j \\in N(i)}(1-x_i)$ is concave, the objective function is a concave function.</p>\n\n<p>Is this an NP-hard problem? Is there any possible ways so that I can get the approximation?</p>\n', 'ViewCount': '70', 'Title': 'Minimizing concave function with a linear constraint', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T09:46:45.970', 'LastEditDate': '2013-09-02T09:46:45.970', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9898', 'Tags': '<complexity-theory><optimization>', 'CreationDate': '2013-08-30T10:14:48.120', 'FavoriteCount': '1', 'Id': '14035'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am looking for advice in the following optimization problem. I have a website (system) that receives database updates later returned in queries made to the same system. In order to speed up the system the obvious solution is to cache data in different places. I have identified the most likely places where caching may be useful, but caching in every case is clearly not practical. I have begun to decompose the times it takes to place data in different caches. I will end up with.</p>\n\n<pre><code>Cu = uth computation time\nSu = storage required to store result of uth computation\nSCn = storage cost using nth storage resource\nWn = storage cache write time for nth storage resource\nRn = storage cache read time for nth storage resource\n</code></pre>\n\n<p>I understand that there are multiple caches inside today's microprocessors, but I want to ignore those for now. I actually have several options for storage. I can buy plenty of SSD, Spinning drives and/or RAM. But given the amount of possible caching I want to buy what makes most sense.</p>\n\n<p>I am trying to come up with a cost function, storage seems to be pretty straight forward since there will be an allocated space that will cost X dollars. I am struggling a bit more with timing. Clearly I could minimize the time it takes to query and update. But that does not seem correct. I need responsiveness, we need servers to be responsive with every request not to be able to process every single update and query at once in a couple of days, instead of three. I am constructing a $$ to seconds function. This function would imply, we are willing to pay $X for a computer that responds in Y seconds.</p>\n\n<p>I will also need an optimization algorithm, there are a lot out there and I am not sure which could be a good fit for this problem.</p>\n\n<p>I am having a lot of difficulty finding research similar to this description. And I do know I will need to redo this optimization a number of times, so I will end up writing code so I don't have to do this every time.</p>\n", 'ViewCount': '63', 'Title': 'How to compute an optimally cost-effective cache strategy?', 'LastActivityDate': '2013-09-03T18:39:54.083', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9964', 'Tags': '<optimization><program-optimization>', 'CreationDate': '2013-09-03T18:39:54.083', 'Id': '14109'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let's say we have an inequality, $p \\le {a \\choose b}$ where $p$ is a fixed constant and $a, b$ are variables. The problem is that, we are trying to find the minimum $a$ with respect to the inequality $p \\le {a \\choose b}$. Is there a closed form solution (can be approximate as well/doesn't have to be exact) for that combinatorial optimization problem? </p>\n", 'ViewCount': '62', 'Title': 'Solution for a combinatorial minimization problem', 'LastActivityDate': '2013-09-05T14:11:02.100', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '14138', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2956', 'Tags': '<optimization><combinatorics>', 'CreationDate': '2013-09-04T23:03:35.267', 'Id': '14135'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been studying about <a href="http://en.wikipedia.org/wiki/Activity_selection_problem" rel="nofollow">activity-selection-problem</a> and the solution of greedy choice I came across is to select the activity that finishes in the earliest among the present activities.</p>\n\n<p>But surely there are other greedy choices to solve the problem. The one I have been able to figure out is to select the activity that starts last.</p>\n\n<p>My question is: are there any other greedy choices that can lead to solve activity selection problem? Any formal proof is also appreciated.</p>\n', 'ViewCount': '118', 'Title': 'Other greedy choices to solve activity selection problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:59:27.903', 'LastEditDate': '2013-09-16T07:59:27.903', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6699', 'Tags': '<algorithms><optimization><greedy-algorithms>', 'CreationDate': '2013-09-15T03:55:05.647', 'Id': '14316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to modify the Floyd-Warshall algorithm to find all-pairs minimax paths in a graph. (That is, the shortest length paths such that the maximum edge weight along a path is minimized.)</p>\n\n<p>Floyd-Warshall algorithm contains the following loop to enhance the distance (<code>ds</code>) and the next vertex (<code>ns</code>) matrices at each iteration.</p>\n\n<pre><code>for (int k = 0; k &lt; n; k++)\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            if (ds[i][k] != inf &amp;&amp; ds[k][j] != inf) {\n                final int d = ds[i][k] + ds[k][j];\n                if (d &lt; ds[i][j]) {\n                    ds[i][j] = d;\n                    ns[i][j] = k;\n                }\n            }\n</code></pre>\n\n<p>I replaced <code>ds</code> with two new matrices: <code>ws</code> (weights) and <code>ls</code> (lengths). Further, updated the iteration step as follows:</p>\n\n<pre><code>for (int k = 0; k &lt; n; k++)\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            if (ws[i][k] != inf &amp;&amp; ws[k][j] != inf) {\n                final int w = Math.max(ws[i][k], ws[k][j]);\n                final int l = ls[i][k] + ls[k][j];\n                if (w &lt; ws[i][j] || (w == ws[i][j] &amp;&amp; l &lt; ls[i][j])) {\n                    ws[i][j] = w;\n                    ls[i][j] = l;\n                    ns[i][j] = k;\n                }\n            }\n</code></pre>\n\n<p>However, the modified algorithm finds paths with loops, that is, paths such as 1-<strong>3-2-3</strong>-4. While the maximum edge weight of the paths 1-3-2-3-4 and 1-3-4 are identical, the latter has a shorter path length and supposed to be returned by the enhanced Floyd-Warshall. Any ideas?</p>\n\n<p>A working Java version of both algorithms and a test case which produces a path with loop can be found <a href="https://gist.github.com/vy/6580214" rel="nofollow">here</a>.</p>\n\n<p><strong>Edit:</strong> Since no solutions were presented yet, I implemented my own shortest minimax path algorithm using incremental link removal method. Java sources to the solutions can be accessed from the <a href="https://gist.github.com/vy/6580214" rel="nofollow">above link</a>. </p>\n', 'ViewCount': '683', 'LastEditorDisplayName': 'user742', 'Title': 'Shortest Minimax Path via Floyd-Warshall', 'LastActivityDate': '2013-10-20T10:59:54.067', 'LastEditDate': '2013-09-20T09:26:04.407', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10167', 'Tags': '<algorithms><graph-theory><optimization><shortest-path>', 'CreationDate': '2013-09-16T12:50:56.260', 'Id': '14353'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In real word problems, the influence of multiple not perfectly known factors results in using heuristics instead of mathemacial solutions that calculates a perfect value from only precisly defined input data. Consequently, any method that does not supply the mathematical maximum or minimum is not an optimisation but an improvement.</p>\n\n<p>Somehow my opinion on this topic differs from the use of the term <code>optimisation</code> in many papers. Are the people just not precise in their language or is my understanding of the term wrong?</p>\n\n<p><code>Improvement</code> doesn't sound as facy as <code>optimisation</code>, but is there maybe some facy word that allows people to still be precise?</p>\n", 'ViewCount': '40', 'Title': 'Is a non-perfect improvement and optimisation?', 'LastActivityDate': '2013-10-14T13:53:07.487', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10634', 'Tags': '<terminology><optimization><approximation><applied-theory><approximation-algorithms>', 'CreationDate': '2013-10-14T13:28:29.267', 'Id': '16071'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What is the complexity of MIN-2-XOR-SAT and MAX_2-XOR-SAT?  Are they in P?  Are they NP-hard?</p>\n\n<p>To formalize this more precisely, let</p>\n\n<p>$$\\Phi\\left(\\mathbf x\\right)={\\huge\\wedge}_{i}^{n}C_i,$$</p>\n\n<p>where $\\mathbf{x} = (x_1,\\dots,x_m)$ and each clause $C_i$ is of the form $(x_i \\oplus x_j)$ or $(x_i \\oplus \\neg x_j)$.</p>\n\n<p>The $\\text{2-XOR-SAT}$ problem is to find an assignment to $\\mathbf{x}$ that satisfies $\\Phi$.  This problem is in $P$, as it corresponds to a system of linear equations mod $2$.</p>\n\n<p>The $\\text{MAX-2-XOR-SAT}$ problem is to find an assignment to $\\mathbf{x}$ that maximizes the number of clauses that are satisfied.  The $\\text{MIN-2-XOR-SAT}$ problem is to find an assignment to $\\mathbf{x}$ that minimizes the number of clauses that are satisfied.  What are the complexities of these problems?</p>\n\n<p>Inspired by <a href="http://cs.stackexchange.com/q/16682/755">Is MIN or MAX-True-2-XOR-SAT NP-hard?</a></p>\n', 'ViewCount': '108', 'Title': 'MIN-2-XOR-SAT and MAX-2-XOR-SAT: are they NP-hard?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-04T19:19:55.910', 'LastEditDate': '2013-11-04T19:19:55.910', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<complexity-theory><optimization><np-hard><satisfiability>', 'CreationDate': '2013-11-04T03:28:01.333', 'Id': '16691'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '150', 'Title': 'Artificial Intelligence: Condition for BFS being optimal', 'LastEditDate': '2013-11-18T19:12:31.383', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6998', 'FavoriteCount': '1', 'Body': '<p>It is said in the book <em>Artificial Intelligence: A Modern Approach</em> for finding a solution on a tree using BFS that: </p>\n\n<blockquote>\n  <p>breadth-first search is optimal if the path cost is a nondecreasing function of the\n  depth of the node. The most common such scenario is that all actions have the same cost.</p>\n</blockquote>\n\n<p>From that I understand that if the path cost is non decreasing function of depth, the BFS algorithm returns an optimal solution, i.e., <strong>the only condition is the cost function being nondecreasing</strong>. But I think the only way for BFS to be optimal is the scenario in which all the path costs are identical, therefore a node found in a certain level is necessarily the optimal solution, as, if they exist, the others are. Therefore I think for BFS to be optimal, cost function should be non decreasing <strong>AND</strong> the costs of nodes should be identical. However, the book says only one of the conditions (former one) makes BFS optimal.</p>\n\n<p>Is there a situation in which the costs are not identical, the cost function is nondecreasing and the solution returned by BFS is guaranteed to be optimal?</p>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms><search-trees><search-problem>', 'LastEditorUserId': '6998', 'LastActivityDate': '2013-11-18T19:12:31.383', 'CommentCount': '0', 'AcceptedAnswerId': '16780', 'CreationDate': '2013-11-06T01:16:28.697', 'Id': '16758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have the following problem, which seems to be similar to Set Cover.</p>\n\n<p>We are given a set $U$ of elements (the universe, e.g., $U=\\{1,2,3,4,5\\}$).\nWe're also given a set $S$ of subsets (e.g., $S=\\{\\{1\\},\\{2\\},\\{3\\},\\{4\\},\\{5\\},\\{1,2,5\\},\\{1,3,4\\},\\{2,3,4,5\\}\\}$).</p>\n\n<p>The standard set cover problem asks for the minimum subset of $S$ that covers the whole universe $U$, i.e., the smallest set-cover.  In our case this would be $\\{\\{1\\},\\{2,3,4,5\\}\\}$.</p>\n\n<p>However, I'm interested in finding a collection of set-covers where each subset from $S$ is used at most once.  I want to find the largest such collection possible, i.e., to find as many different, disjoint set-covers of $U$ as possible.  Considering our example, the three set-covers $\\{\\{1\\},\\{2,3,4,5\\}\\}$ and $\\{\\{3\\},\\{4\\},\\{1,2,5\\}\\}$ and $\\{\\{2\\},\\{5\\},\\{1,3,4\\}\\}$ would be the output, since each one is a set-cover of $U$ and they are pairwise disjoint (no subset from $S$ is used in more than one set-cover).</p>\n\n<p>Just testing all possible combinations of set-covers is definitely not an option. I have tried using an algorithm for set cover (ILP implementation) repeatedly, eliminating all used subsets in between runs. However, I'm certain that this strategy does not actually maximize the number of all possible covers.</p>\n\n<p><strong>Edit 1:</strong> I will try to describe the size of my problem a bit better. The typical size of $U$ is about 4000. $S$ consists of about 50 subsets. Each element of $S$ covers on average about 70% of $U$. Please keep in mind that I don't want to find <em>all</em> subset combinations out of $S$ that cover $U$. I just want to determine the maximum number of set covers using each subset from $S$ <em>only once</em> (or not at all). As of now I'm guessing the maximum number of possible covers is approximately 5.</p>\n", 'ViewCount': '297', 'Title': 'Variation of Set Cover Problem: Finding a maximum-sized collection of disjoint set-covers', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-13T17:28:54.193', 'LastEditDate': '2013-11-13T17:28:54.193', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11255', 'Tags': '<algorithms><optimization><set-cover>', 'CreationDate': '2013-11-08T09:43:48.263', 'FavoriteCount': '1', 'Id': '16816'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '238', 'Title': 'Maximum degree of concurrency in task dependency graphs', 'LastEditDate': '2013-11-09T15:10:52.437', 'AnswerCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11256', 'FavoriteCount': '1', 'Body': "<p>I've been researching ways of modeling and executing tasks which are dependent on each other (but in an acyclic way) and came up with task graphs. But the question that's bugging me is how can I find out the maximum degree of concurrency in a given task graph.</p>\n\n<p>In my case, I'm talking of a relatively small graph, around 100 nodes, but nodes, representing tasks, are long running tasks. So the occuracy, more then complexity of such an algorithm would matter.</p>\n\n<p>Assuming I came up of such a degree, the second problem, is how should I distrubute tasks? I've read about topological sort, and transforming the result in a list of sets, with each set being run in parallel. But again, I suspect if this is the best approach.</p>\n", 'Tags': '<optimization><distributed-systems><concurrency>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-07T15:51:08.327', 'CommentCount': '1', 'AcceptedAnswerId': '16829', 'CreationDate': '2013-11-08T13:56:22.220', 'Id': '16823'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My Problem is like this:</p>\n\n<ol>\n<li><p>I have a physical layout represented as a graph. The Nodes represents hooks/ducts where a wire can anchor and Edges are the possible connection between 2 nodes from where wire can go.</p></li>\n<li><p>There are some special Nodes, called splitters, from where a single wire can be splitted to  2 or more up to k. The k can be taken constant for now but it varies from node to node. Not all nodes are splitters. </p></li>\n<li><p>There is one source of power from where a wire will emerge. It is the source. The wire has to be taken to n sinks. </p></li>\n<li><p>An edge can take any number of wires traversing through it in either direction.</p></li>\n<li><p>The the total wire length has to be minimized. </p></li>\n<li><p>The nature of graph, planar or euclidean is not known. </p></li>\n</ol>\n\n<p><strong>Example</strong>: Below is a sample network. Nodes are named as numbers and edges are provided with equal weights of 1. Source is Node1 and Sinks are Node5, Node9 and Node13. In case 1 Node6 is Splitter node. In case 2 Node6 and Node4 are splitter nodes. The splitter node\'s k=3, i.e., it can take in one wire and split it out to 3 wires.</p>\n\n<p><strong>Case 1</strong>. Only one splitter Node. It makes sense to split at Node6.\n<img src="http://i.stack.imgur.com/IVvkw.jpg" alt="enter image description here"></p>\n\n<p><strong>Case 2</strong>. Two splitter Node. It makes sense to split at Node4 instead of Node6.\n<img src="http://i.stack.imgur.com/6ZKJD.jpg" alt="enter image description here"></p>\n\n<p>I am looking for different strategies to find out a generic solution for this problem. The graph presented here is of a smaller scale as compared to the problem in hand. The graph is static and can not be changed (i mean the solution should not suggest any new edge or propose new splitter location ). \nAny reference to research paper published on this kind of problem is also welcomed.</p>\n\n<p><strong>Case 3</strong>. Two splitter Node. It makes sense to split at Node4 and Node14. Note that this case has edge weights changed for Edge 8-12, 6-10 and 10-11. The important thing in this case is retracing of a wire after getting splitted from Node14.</p>\n\n<p><img src="http://i.stack.imgur.com/aYuPN.jpg" alt="enter image description here"></p>\n', 'ViewCount': '128', 'Title': 'Wiring Length Minimization', 'LastEditorUserId': '7976', 'LastActivityDate': '2014-01-20T06:19:48.023', 'LastEditDate': '2013-11-15T09:44:52.997', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '7976', 'Tags': '<algorithms><graph-theory><optimization>', 'CreationDate': '2013-11-15T08:43:01.883', 'FavoriteCount': '3', 'Id': '18041'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider a standard feasibility problem. The goal is to examine the state of feasible solutions for $Ax=b$ to find an $x$ that satisfies some property. Does the dual of this problem tell us anything about the solution? Since there is no objective function that we're examining I think the dual looks like $A^Tx \\leq 0$. Is this right?</p>\n", 'ViewCount': '24', 'Title': 'Does it make sense to examine the dual of a feasbility problem?', 'LastActivityDate': '2013-11-15T22:32:41.220', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11410', 'Tags': '<algorithms><optimization><linear-programming>', 'CreationDate': '2013-11-15T22:32:41.220', 'FavoriteCount': '1', 'Id': '18056'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given N finite subsets of the finite universe set E, it is necessary to find the intersection which contains maxumum number of subsets. Let call this problem MSI (Maximum Subset Intersetion).</p>\n\n<p>Firstly I had thought that it is variation of well-known Maximum Clique Problem where vertices are subsets and edges are relations of intersection, so the classic Bron-Kerbosch algorithm or Tomita et al. algorithm can be easily used. But the k-size clique is certainly not the same that the mutual intesection of k subsets (cause subset A can intersect B, B can intersect C, A can intersect C, but at the same time A, B and C may not have common points of intersection).</p>\n\n<p>Actually the multiple intersections form edges in <strong>hypergraph</strong>, so the main task is to find edge of maximal cardinality.</p>\n\n<p>I found an article by Eduardo C. Xavier <a href="http://www.ic.unicamp.br/~eduardo/publications/ipl12.pdf" rel="nofollow">A Note on a Maximum k-Subset Intersection Problem\n</a> where he proves that the MSI-problem is a variation of Maximum Edge Biclique (MEB) problem which is in turn NP-hard.</p>\n\n<p>My naive solution is to implement branch and bound greedy algorithm (in other words, to adopt Bron-Kerbosch algorithm to this domain).</p>\n\n<p>May be there are already any implementations or acadimic paper on that problem?</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Actually the exact problem I met in my work is the following:</p>\n\n<p>Suppose that I can run some function which returns the cardinality of one subset or N subsets intersection. The function is time-consuming. I am interested in finding optimal or approximately optimal argorithm to find the intersection of maximum number of subsets (next thing is to find the whole set of such intersections in cardinality decreasing order).\nWorking on element-level is even more costly from the time consumption point of view.</p>\n\n<p>I thought firstly to take cardinalities of all subsets (N operations) and then run Bron-Kerbosh kind of search for subsets sorted by cardinality decreasing order. So I will start with the largest subset, check if it intersects with the 2nd largest subset and so on.</p>\n', 'ViewCount': '134', 'Title': 'Maximum subset intersection problem', 'LastEditorUserId': '10736', 'LastActivityDate': '2013-11-21T23:49:34.870', 'LastEditDate': '2013-11-21T23:49:34.870', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10736', 'Tags': '<graph-theory><graphs><optimization>', 'CreationDate': '2013-11-21T13:58:09.430', 'Id': '18221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been told it is possible to find a solution to this optimization problem in $\\Theta(n)$ but I still don't know how I could do it. I did find easily a solution in $n\\lg (n)$ though. I only need to have a VALID solution, not the optimal.</p>\n\n<p>This is the problem :</p>\n\n<p>Let say you have n tasks that have a start time and a max time. Each task takes 1 unit of time to complete. </p>\n\n<p>So for example :</p>\n\n<pre><code>T1 = 2,3\nT2 = 1,4\nT3 = 4,5\nT4 = 1,5\nT5 = 3,4\n</code></pre>\n\n<p>This means that T1 CAN be started at time 2 and must be done by time 3. So a correct solution could be : </p>\n\n<blockquote>\n  <p>T2, T1, T5, T3, T4</p>\n</blockquote>\n\n<p>Any idea on how I could create an algorithm in $\\Theta(n)$? For now I thought of using a sorting algorithm that does not use comparison but it only works with integer so I can't really get a reference to an object or something.</p>\n", 'ViewCount': '71', 'Title': 'Scheduling optimization problem in theta(n)', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-11-22T20:47:50.657', 'LastEditDate': '2013-11-22T08:44:07.007', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11536', 'Tags': '<optimization><scheduling>', 'CreationDate': '2013-11-21T19:01:37.490', 'Id': '18233'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Some guy on the internet <a href="http://networkengineering.stackexchange.com/a/5133/296">recommends</a> using the same ntp server when it is required to troubleshoot asymmetric routes through ICMP, and it\'s somewhat important to have synchronised time between the two machines doing ICMP.</p>\n\n<p>Granularity of timestamps in ICMP is 1ms (unique per 24h period), assume packet roundtrip between the source and destination of at least 100ms, each way of at least 50ms, plus jitter.</p>\n\n<p>I find the recommendation of using the same ntp server unreasonable; for one, because it would seem that the likelihood of any given reliable ntp server, anywhere in the world, carrying correct time is much higher than the likelihood of transmitting said time through the internet over longer distances (plus with potential jitter and packet loss), e.g. a good collection of local servers is already the best you could do for the task at stake.</p>\n\n<p>Basically, my conjecture is that, should a single ntp server be shared, it won\'t necessarily be a good server for both hosts doing ICMP, and would not contribute to the clock between the two (and only two) machines being the most synchronised, compared to a good collection of local servers instead.</p>\n\n<p>What\'s the mathematical take on this?</p>\n', 'ViewCount': '45', 'Title': 'NTP: synchronisation of time between two machines for ICMP timestamping', 'LastActivityDate': '2013-11-24T20:59:38.447', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11551', 'Tags': '<algorithm-analysis><reference-request><optimization><computer-networks><synchronization>', 'CreationDate': '2013-11-24T20:59:38.447', 'Id': '18310'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Considering two sets $A, B$ containing some $p$-dimensional points $x \\in \\mathbb{R}^p$. Let $d_x^S = \\min_{x' \\in S \\setminus \\{x\\}} \\lVert \\mathbf{x} - \\mathbf{x'} \\rVert$ denote the Euclidean distance from $x$ to its nearest point in $S$. We have a very simple algorithm:</p>\n\n<ol>\n<li>$\\forall x \\in A$, if $d_x^A &gt; d_x^B$ then move $x$ from $A$ to\n$B$.</li>\n<li>$\\forall x \\in B$, if $d_x^A &lt; d_x^B$ then move $x$ from $B$ to\n$A$.</li>\n<li>Repeat (1) and (2) until convergence</li>\n</ol>\n\n<p>Convergence is when there is no more $x \\in A$ such that $d_x^A &gt; d_x^B$, and there is no more $x \\in B$ such that $d_x^A &lt; d_x^B$.</p>\n\n<p>How could I figure out which function does this algorithm minimize or maximize at each iteration ? The function $\\Phi(A)+\\Phi(B) = \\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^B$ does not seem to decrease at each iteration.</p>\n\n<p>Note: another version of this algorithm is when we define $d_x^S$ as the mean distance from $x$ to its $k$ nearest points in $S$, instead of the distance to its nearest point in $S$. I don't know if $k &gt; 1$ would make the proof more complicated or not.</p>\n", 'ViewCount': '87', 'Title': 'Which potential function does this algorithm minimize or maximize?', 'LastEditorUserId': '2895', 'LastActivityDate': '2013-11-26T16:29:35.140', 'LastEditDate': '2013-11-26T16:29:35.140', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2895', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><optimization>', 'CreationDate': '2013-11-25T20:40:26.780', 'FavoriteCount': '2', 'Id': '18333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '230', 'Title': 'Easy way to prove that this algorithm eventually terminates', 'LastEditDate': '2013-11-26T19:24:53.680', 'AnswerCount': '1', 'Score': '10', 'OwnerDisplayName': 'user995434', 'PostTypeId': '1', 'OwnerUserId': '2895', 'FavoriteCount': '3', 'Body': '<h2>Introduction and notations:</h2>\n\n<p>Here is a new and simple version of my algorithm which seems to terminates (according to my experiments), and now I would like to prove that.</p>\n\n<p>Let the notation $x_i \\in \\mathbb{R}^p$ refer to a $p$ dimensional data point (a vector). I have three sets A, B and C, such that $|A| = n$, $|B| = m$, $|C| = l$:\n$$A = \\{x_i | i = 1, .., n\\}$$\n$$B = \\{x_j | j = n+1, .., n+m\\}$$\n$$C = \\{x_u | u = n+m+1, .., n+m+l\\}$$</p>\n\n<p>Given $k \\in \\mathbb{N^*}$, let $d_{x_i}^A$ denote the mean Euclidean distance from $x_i$ to its $k$ nearest points in $A$; and $d_{x_i}^C$ denote the mean Euclidean distance from $x_i$ to its $k$ nearest points in $C$.</p>\n\n<h2>Algorithm:</h2>\n\n<p>I have the following algorithm which iteratively modifies the sets A and B by moving some selected elements from A to B and vis versa, and C remains always the same (do not change). To make it simple: the purpose of the algorithm is to better separate the sets $A$ and $B$ such that "the points of $B$ are more similar to those of a known fixed set $C$" and "the points of $A$ are finally self-similar and farther from those of $C$ and the final set $B$":</p>\n\n<ul>\n<li>$A\' = \\{ x_i \\in A \\mid d_{x_i}^A &gt; d_{x_i}^C \\}$ ... (1)</li>\n<li>$A = A \\setminus A\'$; $B = B \\cup A\'$ ... (2)</li>\n<li>$B\' = \\{ x_i \\in B \\mid d_{x_i}^A &lt; d_{x_i}^C$ } ... (3)</li>\n<li>$B = B \\setminus B\'$; $A = A \\cup B\'$ ... (4)</li>\n<li>Repeat (1), (2), (3), and (4) until: (no element moves from $A$ to $B$ or from $B$ to $A$, that is A\' and B\' become empty) or ($|A| \\leq k$ or $|B| \\leq k$)</li>\n</ul>\n\n<p>The algorithm terminates in two cases:</p>\n\n<ul>\n<li>when $|A|$ or $|B|$ becomes less than or equals to $k$</li>\n<li>or the most standard case, when $A\' = B\' = \\emptyset$, which means that no more elements moves between A and B.</li>\n</ul>\n\n<h2>Question:</h2>\n\n<p>How to prove that this algorithm eventually terminates ? I didn\'t found a convenient potential function which can be strictly minimized or maximized by the algorithm.\nI have unsuccessfully tried some functions: the function $\\sum_{x \\in A} d_x^C + \\sum_{x \\in B} d_x^A$ but it is not increasing at each iteration. The function $\\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^C$ but it is not decreasing at each iteration. The function $\\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^B$ seems not to be decreasing at each iteration. The function $\\sum_{x \\in A} d_x^B + \\sum_{x \\in B} d_x^A$ seems not to be increasing at each iteration. So what is the convenient potential function which can be show to either increase or decrease at each iteration ? Or should we show that the function decreases but not at each iteration (after some iterations rather) ? How ?</p>\n\n<h2>Notes:</h2>\n\n<ul>\n<li>The $k$ nearest points to $x$ in a set $S$, means: the $k$ points\n(others than $x$) in $S$, having the smallest Euclidean distance to\n$x$. You can just take $k = 1$ to simplify the analysis.</li>\n<li>I don\'t know if this may help or not, but I have the following\nproperty for my initial sets $A, B, C$: initially $\\forall x_i \\in B,\n   x_j \\in A$, if $x_b \\in C$ is the nearest point to $x_i$ and $x_a \\in\n   C$ is the nearest point to $x_j$ then always $distance(x_i, x_b) &lt;\n   distance(x_j, x_a)$. This intuitively means that points in $B$ are\ncloser to $C$ than points in $A$.</li>\n<li>If that makes the analysis easier: it is totally possible to consider a slightly different version of the Algorithm where as soon as a point from $A$ should be moved to $B$, it is moved from $A$ to $B$ (without passing by $A\'$), and vis versa for $B$.</li>\n</ul>\n', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><optimization>', 'LastEditorUserId': '2895', 'LastActivityDate': '2013-11-27T22:36:33.060', 'CommentCount': '11', 'CreationDate': '2013-11-22T10:53:45.843', 'Id': '18393'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>My question is about numerical methods for inverting integral transforms;</p>\n\n<p>I'm trying to numerically invert the following integral transform:</p>\n\n<p>$$F(y) = \\int_{0}^{\\infty} y\\exp{\\left[-\\frac{1}{2}(y^2 + x^2)\\right]} I_0\\left(xy\\right)f(x)\\;\\mathrm{d}x$$</p>\n\n<p>So for a given $F(y)$ I need to approximate $f(x)$\nwhere:</p>\n\n<ul>\n<li><strong>$f(x)$ and $F(y)$ are real and positive</strong> (they are continuous probability distributions)</li>\n<li><strong>$x,y$ are real and positive</strong> (they are magnitudes)</li>\n</ul>\n\n<p>I have a very messy and brute force method for doing this at the minute: </p>\n\n<p>I define $f(x)$ and the spline over a series of points, the values of the splined points are 'guessed' by random sampling, which yields a predicted $F(y)$. A basic genetic algorithm I wrote up minimises the difference between the predicted and measured $F(y)$ array. I then take the $f(x)$ which the algorithm converges to as my answer for the inversion.</p>\n\n<p>This approach works fairly well for some simple cases, but it feels messy to me and not particularly robust.</p>\n\n<p><strong>Can anyone give me guidance on better ways of solving this problem?</strong></p>\n\n<p>Thanks for your time &amp; help!</p>\n", 'ViewCount': '40', 'Title': 'Numerically approximating an inverse integral transform?', 'LastActivityDate': '2013-11-27T05:40:37.663', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user11649', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><numerical-analysis><numerical-algorithms>', 'CreationDate': '2013-11-27T05:40:37.663', 'Id': '18406'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How would you approach the knapsack problem in a dynamic programming situation if you now have to limit the number of item in the knapsack by a constant $p$ ? This is the same problem (max weight of $W$, every item have a value $v$ and weight $w$) but you can only add $p$ item(s) to the knapsack and obviously need to optimize the value of the knapsack.</p>\n\n<p>Do we need a 3rd dimension or we could find an other approach without it. I tried to simply add the number of item in the knapsack in the cell and taking the max value at the end with the number of item &lt;= $p$ but it is not the BEST solution. </p>\n', 'ViewCount': '238', 'Title': 'Variant of the knapsack problem', 'LastActivityDate': '2013-12-02T08:43:27.143', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18529', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11536', 'Tags': '<algorithms><optimization><dynamic-programming><knapsack-problems>', 'CreationDate': '2013-11-30T19:46:10.817', 'FavoriteCount': '1', 'Id': '18492'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have come across many sorting algorithms during my high school studies. However, I never know which is the fastest (for a random array of integers). So my questions are:</p>\n\n<ul>\n<li>Which is the fastest currently known sorting algorithm?</li>\n<li>Theoretically, is it possible that there are even faster ones? So, what's the least complexity for sorting?</li>\n</ul>\n", 'ViewCount': '2193', 'Title': 'What is a the fastest sorting algorithm for an array of integers?', 'LastActivityDate': '2013-12-03T19:50:58.720', 'AnswerCount': '4', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8870', 'Tags': '<algorithms><time-complexity><optimization><sorting>', 'CreationDate': '2013-12-02T16:15:25.337', 'Id': '18536'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have asked this question on programmers.stackexchange but nobody was able to answer this question.I have asked for help on other forums but did not get much help.Since this is a part of my research I figured I should ask on this site.</p>\n\n<p>I am working on a problem similar to the assembly line scheduling by dynamic programming.The issue is that unlike the classic problem where we have predefined stations now I only have information which task should run before which other(could be more than one) tasks.</p>\n\n<p>I have to find out which tasks to put on which line to minimize the total time taken by the production.So if the tasks are on a single line then they are executing in serial fashion and hence are slower.However,unlike the original problem the tasks are also communicating and communicating time is only added if tasks are on a different line.</p>\n\n<p>I have to determine whether this communication cost is worth moving the task to a separate line(from its communicating task)</p>\n\n<p>I have to decide which tasks to put on the same line and which tasks to put on the different lines (given the communication time when tasks are on different lines) to minimize the production time.</p>\n\n<p>So if I try to minimize the communication time then I am increasing the execution time as now all tasks are on a single line and are executing serially.</p>\n\n<p><strong>Problem:</strong></p>\n\n<p>If I try to move tasks to different lines now all tasks are executing in parallel.How do I determine the time saved due to this?Different tasks would be executing in different lines with the different starting and ending times.How do I calculate this metric?</p>\n', 'ViewCount': '32', 'Title': 'Help in developing a dynamic programming solution to this problem', 'LastActivityDate': '2013-12-03T07:41:32.070', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7181', 'Tags': '<optimization><dynamic-programming><np>', 'CreationDate': '2013-12-03T07:41:32.070', 'Id': '18561'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '92', 'Title': '"Unusual" coupling between a decision problem and a corresponding optimization problem', 'LastEditDate': '2013-12-04T18:21:05.133', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '2', 'Body': '<p>There seems to usually be a tight connection between <a href="http://en.wikipedia.org/wiki/Decision_problem" rel="nofollow">decision problems</a> and (corresponding) <a href="http://en.wikipedia.org/wiki/Optimization_problem" rel="nofollow">optimization problems</a> in general. However, is this always the case? </p>\n\n<blockquote>\n  <p>Are there examples where the typical "tight coupling" between a decision problem and the correponding optimization problem breaks down or behaves in an unusual way, e.g. have significantly different complexity?</p>\n</blockquote>\n\n<p>Or, maybe there is a case where there is a cluster of problems that are all closely related, but the "best" or "definitive" version is not obvious or apparent? Also, I am looking for any survey or broad overview or discussion of this apparent basic connection between decision and optimization problems.</p>\n\n<p>A similar question was asked <a href="http://cs.stackexchange.com/questions/939/optimization-version-of-decision-problems">here</a>, but the answers were highly theoretical and it did not seem to yield any specific or tangible examples.</p>\n', 'Tags': '<complexity-theory><reference-request><optimization><decision-problem>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-04T18:21:05.133', 'CommentCount': '5', 'AcceptedAnswerId': '18608', 'CreationDate': '2013-12-03T17:10:07.033', 'Id': '18575'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '111', 'Title': 'Clarification on Tabu Search', 'LastEditDate': '2013-12-03T22:03:19.313', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '170', 'FavoriteCount': '1', 'Body': u'<p>I need some help in understanding the \'<em>Tabu Search</em>\' Algorithm. (<a href="http://en.wikipedia.org/wiki/Tabu_search" rel="nofollow">Wikipedia</a>)</p>\n\n<p>I miss a simple explanation to Tabu Search. Anyway, I\'m trying to refer to available resources and build an understanding. </p>\n\n<p><em><strong>This is what I\'m trying to \'digest\':</em></strong></p>\n\n<ul>\n<li><p><em>Tabu Search</em> is an improvement over the <em>Hill Climbing</em> algorithm (Ref-1).</p></li>\n<li><p>The problem with Hill Climbing is that it does not guarantee about reaching the global optimum, because it only searches on a subset of the whole solution space. It will find the local optimum.</p></li>\n<li>To get rid of this issue, Tabu Search maintains a \'Tabu List\' of previously visited states that cannot be revisited (Ref-2). </li>\n<li>If the tabu list is too large, the oldest candidate solution is removed and it\u2019s no\nlonger tabu to reconsider (Ref-3).</li>\n</ul>\n\n<p><em><strong>My questions are,</em></strong> </p>\n\n<ol>\n<li><p>How does Tabu Search cure the problem of getting stuck in a local\noptimum? Does it increase the search-space?</p></li>\n<li><p>What is the need of maintaining a list (i.e. Tabu List)? Why not\njust remember the optimum solution found so far?</p></li>\n<li><p>When the Tabu List is too large, the oldest candidate will be\nremoved. What if this oldest candidate is the global optimum?</p></li>\n</ol>\n\n<p><em>If anyone could explain Tabu Search algorithm using an example, I\'m sure these questions would be automatically answered.</em></p>\n\n<p>References:</p>\n\n<ul>\n<li><p>(Ref-1) Hill Climbing, Wikipedia Article (<a href="http://en.wikipedia.org/wiki/Hill_climbing" rel="nofollow">link</a>)</p></li>\n<li><p>(Ref-2) Russell, Stuart Jonathan, et al. Artificial intelligence: a modern approach. Vol. 74. Englewood Cliffs: Prentice hall, 1995. (<a href="http://www.worldcat.org/oclc/31288015" rel="nofollow">WorldCat</a>)</p></li>\n<li><p>(Ref-3) Luke, Sean. "Essentials of Metaheuristics.". (<a href="http://cs.gmu.edu/~sean/book/metaheuristics/Essentials.pdf" rel="nofollow">pdf</a>)</p></li>\n</ul>\n', 'Tags': '<optimization><artificial-intelligence><search-algorithms>', 'LastEditorUserId': '268', 'LastActivityDate': '2013-12-04T11:19:56.827', 'CommentCount': '0', 'AcceptedAnswerId': '18606', 'CreationDate': '2013-12-03T20:36:30.533', 'Id': '18582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When I was studying Comp Sci, we had <a href="http://rads.stackoverflow.com/amzn/click/0716710455" rel="nofollow">Garey &amp; Johnson</a> as a course textbook, with a large collection of NP-Complete problems. But by that time you could also have a look at the <a href="http://www.ensta-paristech.fr/~diam/ro/online/viggo_wwwcompendium/wwwcompendium.html" rel="nofollow">Compendium of NP Optimization Problems</a>, online.</p>\n\n<p>However, it seems the \'Compendium\' site has not seen any updates in several years. Is that indeed the case? Is there a more up-to-date compendium (perhaps in print) which accounts for further research and contains more problems in more domains?</p>\n', 'ViewCount': '45', 'Title': "Is there a more up-to-date / wider-scope version of the 'Compendium of NP Optimization Problems'", 'LastEditorUserId': '11796', 'LastActivityDate': '2013-12-14T12:47:16.403', 'LastEditDate': '2013-12-14T12:47:16.403', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11796', 'Tags': '<complexity-theory><time-complexity><optimization><np>', 'CreationDate': '2013-12-03T22:00:16.900', 'FavoriteCount': '1', 'Id': '18584'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Let $M$ be a finite set of even cardinality. Define $C=\\{\\{a,b\\}:a,b \\in M, a \\neq b\\}$ the set of all pairs over $M$. Let $w:C \\rightarrow \\mathbb{R}^+_0$ be a function.</p>\n\n<p>Now find $C' \\subset C$ with the following constraints:</p>\n\n<p>$$\n\\bigcup C' = M \\\\\n\\forall x,y \\in C': x \\cap y = \\emptyset \\\\\n\\sum_{x \\in C'}w(x) \\text{ minimal}\n$$</p>\n\n<p>In words: Find a subset $C'$ of pair-wise disjunctive pairs over $M$ that covers $M$, with the sum of these pairs being minimal. Any element of $M$ must appear in exactly one pair.</p>\n\n<p>I could not find an efficient algorithmic solution, and I also fail to relate this to any other known (optimization) problem. I was thinking of the subset sum problem, but I don't see any relation.</p>\n\n<p>So the questions is: Can you find an efficient algorithm to find $C'$? A good approximation might also be sufficient. If not, can you reduce this to any other known computer science problem? </p>\n", 'ViewCount': '87', 'Title': 'Find subset with minimal sum under constraints', 'LastActivityDate': '2013-12-10T00:52:40.407', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18801', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11970', 'Tags': '<algorithms><reductions><optimization>', 'CreationDate': '2013-12-10T00:01:00.097', 'Id': '18799'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have got some equipment of standard lengths, say: </p>\n\n<blockquote>\n  <p>equipment_lengths = {60, 48, 36, 29}</p>\n</blockquote>\n\n<p>that I have to place on a given length of, say 100. I have to place this equipment so as to minimize material waste while covering the given length. I am allowed to exceed the given length of 100 but keep the excess minimized.  I am not allowed to drop short of the given length. Also I have an unlimited number of each equipment.</p>\n\n<p>So, one solution to the above example would be:</p>\n\n<blockquote>\n  <p>{60, 48} (excess = 60 + 48 - 100 = 8)</p>\n</blockquote>\n\n<p>Another could be </p>\n\n<blockquote>\n  <p>{60, 60} (excess = 60 + 60 - 100 = 20)</p>\n</blockquote>\n\n<p>The first solution is preferable to the second one as it minimizes the excess. </p>\n\n<p>My first approach has been to implement a greedy algorithm that takes the longest equipment and keeps placing those till I exceed the given length. Once that happens I remove the last equipment I placed and place the next smaller ones till I exceed again. In the end I select the combination with the least amount of excess. For example the algorithm will successively do the following:</p>\n\n<ul>\n<li>60, 60 (excess = 20)</li>\n<li>60, 48 (excess = 8)</li>\n<li>60, 36, 36 (excess = 32)</li>\n<li>60, 36, 29 (excess = 25)</li>\n</ul>\n\n<p>So, now it will select {60, 48} as the optimal solution when you remove last one equipment on excess. The algorithm then repeats the exercise by removing the last two equipments and finds another optimal solution. Finally it chooses the best one among the two optimal solutions it found.</p>\n\n<p>Now, I have been studying bin packing and knapsack problem variations and while these problems look similar, I haven't been able to apply them to my problem. Does my problem have a well known solution? If not, how should I go about it to efficiently find the globally optimum solution?</p>\n\n<p>Additional information: I think giving preference to longer equipments should be a good idea as the equipments need to be joined with screws or bolts so joining two longer equipments would be less work than three shorter ones, but I don't think it's more important than material wastage.</p>\n", 'ViewCount': '26', 'Title': 'optimal placement of fixed length items on a given length', 'LastActivityDate': '2013-12-25T19:18:09.683', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19285', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12367', 'Tags': '<algorithms><optimization>', 'CreationDate': '2013-12-25T15:34:12.247', 'Id': '19277'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Does anyone know a good algorithm for quickly finding an approximate solution to the following problem?</p>\n\n<p>Given two square matrices $A$ and $B$, minimize $\\| P A P^\\top - B \\|$ over all permutation matrices $P$.</p>\n\n<p>I have heard that there are several types of algorithms for these kinds of problems, like iterative improvement, simulated annealing, tabu search, genetic algorithms, evolution strategies, ant algorithms, and scatter search. I am looking for existing software.</p>\n', 'ViewCount': '34', 'Title': 'Quadratic programming problem involving permutation matrices', 'LastActivityDate': '2013-12-26T14:22:58.533', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12383', 'Tags': '<algorithms><optimization><permutations><approximation-algorithms>', 'CreationDate': '2013-12-26T14:22:58.533', 'Id': '19303'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Many problems in computer science come in two flavors:</p>\n\n<ul>\n<li>Optimization problem: "Find an object with the largest size".</li>\n<li>Threshold problem: "Given $n$, find an object with a size of at least $n$, or reply that such an object does not exist".</li>\n</ul>\n\n<p>Given a solution for the optimization problem, we can solve the threshold problem simply by running the optimization solution and checking if the result has a size of at least $n$. But this doesn\'t help us if the optimization problem is NP-complete.</p>\n\n<p>MY QUESTION IS: If we have a constant-factor approximation algorithm for the optimization problem, how can we use it for the threshold problem?</p>\n\n<p>An obvious answer is: Given $n$, run the approximation algorithm. If the result has a size of at least $n$, return it. If the result has a size of less than $n/c$ (where c is the approximation constant), return that an object of size $n$ does not exist. Otherwise, return "I don\'t know".</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '48', 'Title': 'Using approximations to optimization problems for threshold problems', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-01-31T16:18:31.590', 'LastEditDate': '2014-01-01T14:23:41.853', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<np-complete><reductions><optimization><approximation>', 'CreationDate': '2014-01-01T13:25:36.750', 'Id': '19433'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a problem whose solution can be written as a binary string with a given length $N$, where $N$ is a given parameter. Standard GA works well on this problem. From runs of small values $N$, I found that optimal solutions are binary strings that contain only 01 or 011. For example, the optimal solution for $N=16$ is <code>0101011010101101</code>. So I think it would be a good idea to only search the solutions space where all solutions/chromosomes only contain 01 or 011. But clearly the standard GA cannot restrict the search in the subspace I desire. One mutation or one crossover will make the new solution go into the larger solution space.</p>\n\n<p>My question is: is there a way to adapt the standard GA to restrict its solution space to one where the chromosomes contain only 01 or 011?</p>\n', 'ViewCount': '99', 'Title': 'A genetic algorithm modified for a specific problem', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-30T15:55:12.573', 'LastEditDate': '2014-01-30T15:55:12.573', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12564', 'Tags': '<algorithms><optimization><genetic-algorithms>', 'CreationDate': '2014-01-03T12:57:36.797', 'Id': '19484'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of elements U = {1, 2, .... , n} and a set S of k sets whose union form the whole universe. Each of these sets is associated with a cost.</p>\n\n<p>I have a fixed number of colors, C = {1 , 2, ... , m}. Some of the sets mentioned above interfere with each other. I cannot assign same color to both those sets together.</p>\n\n<p>I want to pick the sets and color them from my available color list in the following way:</p>\n\n<p>**Objective: Minimize the total cost of the selected sets</p>\n\n<p>Constraints:</p>\n\n<ol>\n<li><p>All elements of the universe are covered</p></li>\n<li><p>No two sets that interfere with each other is assigned the same color**</p></li>\n</ol>\n\n<p>If the second constraint, i.e., coloring constraint, is taken out, the problem reduces to standard weighted set covering problem. I can solve that using a greedy manner. For example, greedy unweighted set covering will work in the following way: -- 1. pick the set with the highest number of elements at first, 2. Remove that set and the associated elements from the universe, 3. Repeat step 1 until all elements of the universe are covered.</p>\n\n<p>But the coloring constraint in the presence of interference among sets and a fixed number of colors complicates the issue.</p>\n\n<p>For example, let\'s assume,</p>\n\n<p>U = {1, 2, 3, 4, 5}.</p>\n\n<p>There are three sets, {1, 2, 3}; {2, 3, 4, 5} ; {4, 5}.</p>\n\n<p>Assume {2, 3, 4, 5} interferes with both {1, 2, 3} and {4, 5}. {1, 2, 3} and {4, 5} do not interfere with each other. Assume that there is only one color in the system.</p>\n\n<p>A standard greedy unweighted set coloring solution will pick {2, 3, 4, 5} at first and {1, 2, 3} in the second round. But that is an infeasible solution to my problem since they interfere with each other and I have only one color in my system. A feasible solution will be the selection of {1, 2, 3} and {4, 5}.</p>\n\n<p>I wonder how I can minimize the total cost while meeting the color constraint. Any hint on the unweighted version of the problem (where all sets have equal cost) will be very helpful, too.</p>\n\n<p>Thanks,</p>\n\n<p>Nazmul</p>\n\n<p>Additional information: The coloring of the sets and the interference can be understood by my application scenario. </p>\n\n<p>I am looking at a wireless cell. I have a set of frequencies, possible base station locations and their associated users. Each set is associated with a station and it shows the set of users that the base station can serve. </p>\n\n<p>"Interference" from one set to the other means that the users\' signals of one set reaches the other set\'s base station. In a wireless setting, this interference is not symmetric. But assumption of symmetry is OK in the algorithm because if set A interferes with set B, A &amp; B both should get different colors if they are selected. </p>\n\n<p>The interference among sets is not transitive. Set A may interfere with set B (A\'s users may interfere with B\'s base station) and set B may interfere with set C (B\'s users may interfere with C\' base station) but that does not mean that set A will interfere with set C.</p>\n\n<p>The current example is showing that two sets interfere if they intersect. This is just a coincidence. I will have a pre-generated look up table that shows which set interferes with which set before the algorithm starts. This pre-generated table will be an input to the optimization problem</p>\n', 'ViewCount': '76', 'Title': 'Weighted Set covering problem with a fixed number of colors', 'LastEditorUserId': '12596', 'LastActivityDate': '2014-03-06T18:52:06.767', 'LastEditDate': '2014-01-05T07:54:04.203', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '19516', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12596', 'Tags': '<algorithms><graphs><optimization>', 'CreationDate': '2014-01-05T01:30:35.303', 'Id': '19503'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have question on understanding the following neighborhood relation within a local-search approximation scheme. \nLet $M$ be a legal matching on any bipartite graph. \nLet $U_k$ be the neighborhood defined as follows:\n$$U_k := \\{M' : |(M' \\backslash M) \\cup (M \\backslash M')| \\leq k\\}$$</p>\n\n<p>Can somebody give me an example or explain this to me? </p>\n\n<p>If i choose a small k-value, the cardinality of $M'$ will be small as well, but how does an algorithm decide which matching pair of nodes to take?</p>\n\n<p>If we define node-values and make it a weighted matching,let say we define a weight function $w_e \\in \\mathbb{R}$ for any edge e in our graph, now the algorithm may use greedy method and take the best possible pair of nodes (with greatest weight). </p>\n\n<p>But I still don't understand the exact set definition of our neighborhood.</p>\n\n<p>I would be grateful for an example, because I'm stumped on this one. </p>\n", 'ViewCount': '59', 'Title': 'Local search: Problem with neighborhood definition', 'LastActivityDate': '2014-01-10T12:36:41.790', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19626', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12731', 'Tags': '<optimization><approximation><heuristics><bipartite-matching>', 'CreationDate': '2014-01-10T11:48:35.183', 'Id': '19625'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the following problem.</p>\n\n<p>maximize $\\sum\\limits_{k=1}^Lx_k$</p>\n\n<p>subject to: $\\mathbf{x}^T\\mathbf{A}~ \\tilde{\\mathbf{x}_i} \\geq 0,~~ \\forall~ i\\in\\{1, 2, \\cdots, L\\}.$</p>\n\n<p>where, $~\\mathbf{x}^T = (x_1, x_2, \\cdots, x_L)\\in \\{0, 1\\}^{1\\times L}$, $\\tilde{\\mathbf{x}_i}=(0, \\cdots, 0, x_i, 0, \\cdots, 0)^T\\in \\{0, 1\\}^{L\\times 1}$, and \n$\\mathbf{A}\\in\\mathbb{R}^{L\\times L}.$</p>\n\n<p>Please tell me how can I specify the kind of this optimization problem?\nIs this an easy integer programming problem?</p>\n', 'ViewCount': '83', 'ClosedDate': '2014-01-15T08:21:38.273', 'Title': 'Is this NP-Hard problem?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-15T08:20:38.660', 'LastEditDate': '2014-01-15T08:20:38.660', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12683', 'Tags': '<optimization><integer-programming>', 'CreationDate': '2014-01-14T22:06:13.497', 'Id': '19727'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '508', 'Title': 'How does the 3-opt algorithm for TSP work?', 'LastEditDate': '2014-01-19T16:11:51.803', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12945', 'FavoriteCount': '1', 'Body': u'<p>I understand that the 3-Opt Heuristic for solving the Traveling Salesman problem involves removing three edges from a graph and adding three more to recomplete the tour. However, I\'ve seen many papers that mention that when three edges are removed, there remain only 2 possible ways to recombine the tour - this doesn\'t make sense to me.</p>\n\n<p>For example, I found a paper [1] that says:</p>\n\n<blockquote>\n  <p>The 3-opt algorithm works in a similar fashion, but instead of removing two edges we remove three. This means that we have two ways of reconnecting the three paths into a valid tour1 (\ufb01gure 2 and \ufb01gure 3). A 3-opt move can actually be seen as two or three 2-opt moves.</p>\n</blockquote>\n\n<p>However, I count 8 different ways to reconnect the tour (7 if not counting the permutation before removing the edges). What am I missing here? <strong>Edit: 3 different ways, not 8</strong></p>\n\n<p>Also, can someone link me to an algorithm for 3-opt if possible? I\'m just trying to understand it but I haven\'t come across any clear algorithms yet; all resources I find simply say "remove three edges, reconnect them". That\'s it, which is sort of vague.</p>\n\n<p>Here are the 3 tours that seem to me to be 3-opt moves after removing three edges.</p>\n\n<p><img src="http://i.stack.imgur.com/KynPB.png" alt="enter image description here"></p>\n\n<hr>\n\n<ol>\n<li><a href="http://web.tuke.sk/fei-cit/butka/hop/htsp.pdf" rel="nofollow">Heuristics for the Traveling Salesman Problem</a> by C. Nilsson</li>\n</ol>\n', 'Tags': '<algorithms><optimization><heuristics><traveling-salesman>', 'LastEditorUserId': '12945', 'LastActivityDate': '2014-03-03T18:36:23.367', 'CommentCount': '6', 'AcceptedAnswerId': '19810', 'CreationDate': '2014-01-18T16:42:44.363', 'Id': '19808'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the following problem.</p>\n\n<blockquote>\n  <p>Maximize $\\sum\\limits_{m=1}^M\\sum\\limits_{n=1}^N x_{mn}$</p>\n  \n  <p>subject to: $\\sum\\limits_{\\substack{m^\\prime=1\\\\ m^\\prime \\neq m}}^M\\sum\\limits_{\\substack{n^\\prime=1\\\\ n^\\prime \\neq n}}^N \\alpha_{mn^\\prime}x_{m^\\prime n^\\prime} \\leq \\alpha_{mn},~~ \\forall~ m\\in\\{1, 2, \\cdots, M\\}, \\forall~ n\\in\\{1, 2, \\cdots, N\\} .$</p>\n  \n  <p>where, $x_{mn} \\in \\{0, 1\\}$, and $\\alpha_{mn} \\in \\mathbb{R} ~\\forall~ m\\in\\{1, 2, \\cdots, M\\}, \\forall~ n\\in\\{1, 2, \\cdots, N\\}$ </p>\n</blockquote>\n\n<p>Please can I say that this is a knapsack problem? \nIs there a way to find a reduction from knapsack problem? In the <a href="http://en.wikipedia.org/wiki/List_of_knapsack_problems" rel="nofollow">most basic form of knapsack problem</a>, if the weights are all equal 1 the optimal solution is easy to solve.</p>\n', 'ViewCount': '61', 'Title': 'Is this problem a knapsack problem?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-22T21:40:49.310', 'LastEditDate': '2014-01-22T21:40:49.310', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12683', 'Tags': '<complexity-theory><optimization><np-hard><knapsack-problems><integer-programming>', 'CreationDate': '2014-01-22T18:58:47.070', 'Id': '19897'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say I have variables $w_1, \\dots w_n, h_1, \\dots h_m \\in \\mathbb R$, constants $W, H$, functions $f_1, \\dots f_k : \\mathbb R\\times\\mathbb R\\to\\mathbb R$ from some family $F$ and for each function $f_i$, a pair of intervals $x_i \\subseteq [1, n]$, $y_i \\subseteq [1, m]$. All quantities $\\geq 0$.</p>\n\n<p>I want to find the $w_i$ and $h_i$ to minimize $\\sum f_i \\Big(\\sum _{j\\in x_i} w_j, \\sum _{j\\in y_i} h_j \\Big)$ with constraints $\\sum w_i = W$ and $\\sum h_i = H$. Approximations are perfectly fine.</p>\n\n<p>Informally, this is a grid with column widths $w_i$ and row heights $h_i$ and cells that may span multiple rows and columns, that have costs $f_i$.</p>\n\n<p>My question is, for what families $F$ does this problem have reasonably efficient solutions? The set of affine functions should work. What about step functions? Piecewise linear? Smooth monotonic? Smooth functions in general?</p>\n', 'ViewCount': '41', 'Title': 'What functions are easy to optimize?', 'LastActivityDate': '2014-01-26T12:56:04.577', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5167', 'Tags': '<optimization><approximation>', 'CreationDate': '2014-01-26T12:56:04.577', 'FavoriteCount': '1', 'Id': '19994'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '80', 'Title': 'Genetic algorithm: What is the expected number of strings that are explored?', 'LastEditDate': '2014-01-31T12:10:35.870', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'Remi.b', 'PostTypeId': '1', 'OwnerUserId': '13275', 'Body': "<p>My question concerns genetic algorithm searching along bit strings.</p>\n\n<p>Given:</p>\n\n<ul>\n<li>$N$ = population size</li>\n<li>$l$ = length of bit strings</li>\n<li>$p_c$ = probability that a single crossover occur (double crossover never occur)</li>\n<li>$p_m$ = probability for a given bit that a mutation occur</li>\n</ul>\n\n<p>$w(x)$, the fitness function is equal to the number of 1 in the strings. Therefore, the fitness can take any integer value between 0 and $l$ (the length of the strings).</p>\n\n<p>My question is (three ways of formulating the same question):</p>\n\n<ul>\n<li>What is the expected total number of possibilities explored in $G$ generations?</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>What is the expected proportion of the total possibility space (which equals $2^l$) that is explored in $G$ generations?</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>What is the expected size of the subset of strings that have ever existed in the population during a simulation that last $G$ generations?</li>\n</ul>\n\n<p>Secondary questions:</p>\n\n<ul>\n<li>How does the frequency distribution - of the total number of possibilities explored in $G$ generation - looks like?\n<ul>\n<li>is it a normal (Gauss) distribution?</li>\n<li>Is it skewed?</li>\n<li>...</li>\n</ul></li>\n</ul>\n\n<hr>\n\n<p>I don't quite know how complex is my question. Here are two assumptions that one would like to consider in order to ease the problem.</p>\n\n<ul>\n<li><p>one might want to assume that the population at start is not randomly drawn from the possibility space. He could assume that the whole population is made of identical strings (only one instance). For example, the string <code>000000000</code> (which length equals $l$).</p></li>\n<li><p>one might want to assume that $p_c = 0$</p></li>\n</ul>\n", 'Tags': '<algorithms><optimization><average-case><genetic-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T17:51:05.043', 'CommentCount': '3', 'AcceptedAnswerId': '20153', 'CreationDate': '2014-01-31T00:26:34.640', 'Id': '20152'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>There is a new <a href="http://quant.stackexchange.com/">Quantitative Finance SE site</a>. However, I am interested in asking the "CS crowd": </p>\n\n<blockquote>\n  <p>What are some interesting key references or surveys on applying algorithms to stock trading analysis?</p>\n</blockquote>\n\n<p>There are many such references, however, I am particularly interested in those that would appeal to those with a CS background. Further, I am especially interested in those that find surprising applications of TCS or mathematics theory.</p>\n', 'ViewCount': '41', 'Title': 'Applications of algorithms to stock trading analysis', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-31T23:16:57.623', 'LastEditDate': '2014-01-31T23:16:57.623', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<reference-request><machine-learning><optimization><statistics>', 'CreationDate': '2014-01-31T22:55:00.353', 'Id': '20173'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>The optimization version of TSP asks for the length of the shortest tour. Unlike the decision version of TSP, there's no obvious way to verify a proposed solution of the optimization problem in polynomial time. But is there a proof of whether or not it can be verified in polynomial time assuming P \u2260 NP?</p>\n", 'ViewCount': '56', 'Title': u'Has it been proven that the optimization TSP is (or is not) polynomial-time verifiable if P \u2260 NP?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-02T13:42:30.540', 'LastEditDate': '2014-02-02T13:42:30.540', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '20209', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<complexity-theory><optimization><np><traveling-salesman>', 'CreationDate': '2014-02-02T05:46:32.140', 'Id': '20204'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given a set of line segments, how do we identify a subset of maximal cardinality where all line segments are pairwise non-intersecting?</p>\n\n<p>Brute force we would get $2^n$ sets to check where $n$ is the number of line segments, so that isn\'t viable. Anyone got a bright idea how the do this efficiently? I tried doing it this way: remove a line segment that intersects with the most other line segments, iterate until no line segments intersect anymore; but that didn\'t work.</p>\n\n<hr>\n\n<p><a href="http://jsfiddle.net/afaucogney/RwNXL/" rel="nofollow">Here</a> is a "ready to help me" place, where you can test your solution; it visualizes the set of line segments.</p>\n\n<p>To try it out, please implement your attempt in the following function on the linked site:</p>\n\n<pre><code>function showAnalysis() {\n    debug("Just do it");\n}\n</code></pre>\n\n<p>and then click on the top canvas. The fiddle generates randoms segments in the top canvas, and the bottom canvas is the place where an optimal subset will be displayed.</p>\n', 'ViewCount': '117', 'Title': 'Efficiently pick a largest set of non-intersecting line segments', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-07T07:22:28.420', 'LastEditDate': '2014-02-07T07:22:28.420', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '1', 'OwnerDisplayName': 'Anthony', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><np-hard><efficiency>', 'CreationDate': '2014-01-22T14:04:07.477', 'Id': '20263'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The following is Figure 1 in Gene M. Amdahl\'s "Validity of the single processor approach to achieving large scale computing capabilities" (1967) (<a href="http://www-inst.eecs.berkeley.edu/~n252/paper/Amdahl.pdf" rel="nofollow">PDF</a>):</p>\n\n<p><img src="http://i.stack.imgur.com/KkY3p.png" alt="Figure 1 from Amdahl\'s &quot;Validity of the single processor approach&quot;"></p>\n\n<p>The text describes this as representing the performance (presumably vertical) as a function of the fraction of instructions that can be executed in parallel for three machines. Machine A has 32 arithmetic execution units, machine B has 3-deep pipelined execution units working on 8-element vectors, machine C has 3-deep pipelined execution units with 8-wide scalar execution (if I understood the descriptions correctly). With a note that "The probable region of operation is centered around a point corresponding to 25% data management overhead and 10% of the problem operations forced to be sequential."</p>\n\n<p>How did Amdahl derive this graph and how is the graph meant to be interpreted?</p>\n', 'ViewCount': '41', 'Title': u'Amdahl \u201cvalidity of the single processor approach\u201d', 'LastEditorUserId': '4577', 'LastActivityDate': '2014-02-12T20:21:39.273', 'LastEditDate': '2014-02-09T02:09:36.137', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14488', 'Tags': '<program-optimization>', 'CreationDate': '2014-02-07T21:48:45.900', 'Id': '21434'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I would like to find a algorithm that will do the following:</p>\n\n<blockquote>\n  <p>Given two sets $A, B \\subseteq \\mathbb{R}$, where $|B| &gt; |A|$, find the largest subset $C \\subseteq B$, such that:</p>\n  \n  <p>$\\qquad |\\operatorname{mean}(A) - \\operatorname{mean}(C) |&lt; \\delta$ and</p>\n  \n  <p>$\\qquad |\\operatorname{std}(A) - \\operatorname{std}(C) | &lt; \\epsilon$ </p>\n</blockquote>\n\n<p>I think this problem is NP-hard, but I would like a good approximation that does reasonably well. An even harder version of this is to find the larget subset $C$ that matches not just the moments, but the histogram of $A$ to some quality metric. If you have a solution, or can point to same papers that would great!</p>\n\n<p>I am a neuroscientist, and this is for my research.</p>\n', 'ViewCount': '61', 'Title': 'Finding largest subset that matches moments', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T23:46:11.150', 'LastEditDate': '2014-02-12T08:37:57.857', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14589', 'Tags': '<algorithms><reference-request><optimization>', 'CreationDate': '2014-02-11T23:42:08.537', 'Id': '21545'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve been reading about the No Free Lunch Theorem, but I can\'t quite understand what it is about.  I\'ve heard this theorem described elsewhere as the claim that "no general purpose universal optimiser exists". On the other hand, the <a href="https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization" rel="nofollow">Wikipedia article</a> talks about \'candidate solutions" that are "evaluated one by one" - if we only consider algorithms of a particular form, then that is a much more limited claim.</p>\n\n<p>Can anyone explain what this theorem actually claims?</p>\n', 'ViewCount': '459', 'Title': 'What is the no free lunch theorem?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-18T13:02:10.723', 'LastEditDate': '2014-02-18T12:58:43.690', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '644', 'Tags': '<algorithms><terminology><optimization><heuristics>', 'CreationDate': '2014-02-18T10:48:00.223', 'FavoriteCount': '3', 'Id': '21758'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we had 2 arrays of the same size with positive numbers and we wanted to pair up the elements of each array such that the total difference between the pairs is minimized.</p>\n\n<p>The first thought would be to choose pairs with the minimum difference and so on. But it turns out the correct algorithm is to sort them and them pair accordingly.</p>\n\n<p>Any ideas on how to prove that the latter algorithm correctly minimizes the sum of differences?</p>\n', 'ViewCount': '85', 'Title': 'How can we minimize the total distance of cross pairs in an array', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-21T16:31:05.690', 'LastEditDate': '2014-02-21T16:31:05.690', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21815', 'Score': '2', 'OwnerDisplayName': 'user14805', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><optimization><correctness-proof><permutations>', 'CreationDate': '2014-02-18T14:51:18.950', 'Id': '21767'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to write some little code (POC for the selection/mutation operators) that uses a genetic algorithm to solve a global maximum for a function.</p>\n\n<pre><code>f(x_1...x_n) = M - (x_1 - a_1)^2 - (x_2 - a_2)^2 - ... - (x_n - a_n)^2\n</code></pre>\n\n<p>M a_i are constants. I have to find x_i such that f(x_i) = max(f) = M</p>\n\n<p>My selection method is truncation (I select the top 100 fittest of a population of 500).\nMy crossover method is average. there is a 80% chance for crossover, other wise one of the parents is passed on.\nMy elite count is 5 (1% of the population)\nThere is a 3% chance for a mutation for an individual, the range of the mutation is [-0.3, 0.3]</p>\n\n<p>My fitness function is f it self and my stopping condition is ABS(previous best fitness - current best fitness) &lt;= 10^(-21)</p>\n\n<p>You can find the code I wrote <a href="https://bitbucket.org/nocgod/ga-testing" rel="nofollow">here</a>.</p>\n\n<p>The problem is that it converges before it reaches even an approximate solution.</p>\n\n<p>What can I change in the solution approach so that the algorithm would converge on the maximum(f)?\n(This is not my algorithm, it\'s a reduction of a problem I have at work.)</p>\n', 'ViewCount': '49', 'ClosedDate': '2014-02-18T21:38:38.293', 'Title': 'Genetic algorithm fitness function', 'LastEditorUserId': '14811', 'LastActivityDate': '2014-02-19T12:52:37.320', 'LastEditDate': '2014-02-19T12:52:37.320', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14811', 'Tags': '<algorithms><optimization><heuristics><genetic-algorithms>', 'CreationDate': '2014-02-18T16:16:47.953', 'Id': '21776'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been looking into diploid genetic algorithms for a while.</p>\n\n<p>Although, it seems like an implementation which includes diploid (dominant/recessive) genes is closer to the implementation that has been running successfully for billions of years in nature, <strong>does it offer any <em>real</em> advantages in GA?</strong></p>\n\n<p>In nature, 'Diploidness' of genes ensures that recessive genes are preserved, even when they are not expressed for generations: Just in case a scenario occurs where organisms having the genes would be more successful than the organisms with the dominant gene.</p>\n\n<p>In GA, by implementing elitism and bringing back organisms from past generations to the current population at random (perhaps based on a '<em>resurrection factor</em> ') should replicate the effects of diploidness, Right?</p>\n\n<p>Or is there some other advantage that diploid nature of genes offers?</p>\n\n<p>Please cite your sources, if possible.</p>\n", 'ViewCount': '26', 'ClosedDate': '2014-04-01T21:57:53.547', 'Title': 'Does using diploid (dominant/recessive) genes in genetic algorithm offer any advantage?', 'LastEditorUserId': '9479', 'LastActivityDate': '2014-02-25T03:14:55.333', 'LastEditDate': '2014-02-25T03:14:55.333', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<optimization><artificial-intelligence><genetic-algorithms>', 'CreationDate': '2014-02-19T14:41:52.503', 'Id': '21814'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '86', 'Title': 'minimizing the summed cardinality of set unions', 'LastEditDate': '2014-02-24T17:46:51.283', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14883', 'FavoriteCount': '1', 'Body': '<p>this optimization problem, I am working on, is kind of making me crazy. ;)</p>\n\n<p>Given is a list <code>o</code> of sets (with finite cardinality) of strictly positive integer values (Z>0), e.g.:</p>\n\n<pre><code>o_without_sizes =\n[ {1, 2, 3, 4}\n, {5, 6}\n, {2, 3, 4, 5}\n, {5, 6, 7}\n, {7, 8}\n. {9} ]\n</code></pre>\n\n<p>Every set has a name <code>n</code> (also in Z>0, but only for identification) and a fixed independent size value <code>s</code> (also in Z>0), e.g.:</p>\n\n<pre><code>type O = [(Name, Size, Values)]\no =\n[ (1, 2, {1, 2, 3, 4})\n, (2, 1, {5, 6})\n, (3, 2, {2, 3, 4, 5})\n, (4, 3, {5, 6, 7})\n, (5, 2, {7, 8})\n. (6, 1, {9}) ]\n</code></pre>\n\n<p>These sets are to be combined to unions <code>b</code> of a maximum size value sum <code>h (&gt;= max s, that means that no set has a size making it too big to fit into a single union)</code>, e.g. 4.</p>\n\n<p>The goal is to find the <code>b</code> so that the sum of cadinalities of the unions in it is as small as possible.\nhere is a bad <code>b</code>:</p>\n\n<pre><code>size:   3,  cardinality:   6,   sets: [1,2]  ,  values: [1,2,3,4,5,6]\nsize:   2,  cardinality:   4,   sets: [3]    ,  values: [2,3,4,5]\nsize:   3,  cardinality:   3,   sets: [4]    ,  values: [5,6,7]\nsize:   3,  cardinality:   3,   sets: [5,6]  ,  values: [7,8,9]\ncardinality sum:  16\n</code></pre>\n\n<p>and the optimum <code>b</code> for this example:</p>\n\n<pre><code>size:   4,  cardinality:   5,   sets: [3,1]  ,  values: [1,2,3,4,5]\nsize:   4,  cardinality:   3,   sets: [2,4]  ,  values: [5,6,7]\nsize:   3,  cardinality:   3,   sets: [5,6]  ,  values: [7,8,9]\ncardinality sum:  11\n</code></pre>\n\n<p>Until now I only implemented a naive brute force solution (Haskell code): <a href="http://lpaste.net/7204008959806537728" rel="nofollow">http://lpaste.net/7204008959806537728</a></p>\n\n<p>I was hoping to find a dynamic programming solution like it exists for the (Z>0) 0-1 knapsack problem, but did not yet succeed.\nIs my problem perhaps NP-hard? If so, is it many-one-reducible to SAT or something? Or is there a good approximation?</p>\n\n<p>Of course, if there exists a known efficient optimal algorithm, it would be awesome if you could enlighten me. :)</p>\n', 'Tags': '<algorithms><np-complete><optimization><dynamic-programming><np-hard>', 'LastEditorUserId': '14883', 'LastActivityDate': '2014-02-24T17:46:51.283', 'CommentCount': '2', 'AcceptedAnswerId': '21867', 'CreationDate': '2014-02-20T19:25:23.093', 'Id': '21857'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I was proposed (in school) to develop an approach to solve optimally the balanced partition problem. I tried the pseudo-linear algorithms but SUM is very large (~1M) and so O(S*N) cant run under available time (1000ms). I talked to the teacher and with N =&lt; 60 he recommended the brute force (we are learning recursion). I've tried some approaches but I cant find a recursive algorithm that runs under 1 second. Its possible as some students got an solution that runs in less than 100ms.</p>\n\n<p>What is an efficient recursive approach to this problem given data with this magnitude?</p>\n", 'ViewCount': '27', 'LastEditorDisplayName': 'user14946', 'Title': 'Balanced partition problem for N =< 60 and very large sums', 'LastActivityDate': '2014-02-22T22:18:40.147', 'LastEditDate': '2014-02-22T22:18:40.147', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user14946', 'PostTypeId': '1', 'Tags': '<np-complete><optimization><partition-problem>', 'CreationDate': '2014-02-22T22:09:20.530', 'Id': '21932'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $X_1,\\dots,X_n$ be $n$ boolean variables.  I have an unknown predicate $P(X_1,\\dots,X_n)$ on these boolean variables.  Of course, I can view the predicate as a function $f_P : \\{0,1\\}^n \\to \\{0,1\\}$ that maps a vector of $n$ boolean values to the truth value of this predicate on those inputs.</p>\n\n<p>Now I have a truth table of pairs $(x_1,y_1), \\dots, (x_m,y_m)$, and I want to find a predicate $P$ that is consistent with these pairs and that is as "simple" as possible.  In particular, I have two variants of the problem:</p>\n\n<p><strong>Problem 1.</strong> Given $(x_1,y_1), \\dots, (x_m,y_m)$, find a predicate $P$ such that (1) it agrees with the entire truth table (i.e., for all $i$, $f_P(x_i)=y_i$), and (2) out of all such predicates, the complexity of $P$ is minimized.</p>\n\n<p><strong>Problem 2.</strong> Given $(x_1,y_1), \\dots, (x_m,y_m)$ and a threshold $t$, find a predicate $P$ such that (1) $P$ agrees with at least a $t/m$ fraction of the truth table (i.e., there are at least $t$ values of $i$ such that $f_P(x_i)=y_i$), and (2) out of all such predicates, the complexity of $P$ is minimized.</p>\n\n<p>Are there any algorithms for solving either of these problems, in a way that is more efficient than enumerating all predicates?</p>\n\n<p>Of course, to make the problem well-posed, we must agree on a definition of the complexity of a predicate.  Here I can see any number of realistic complexity metrics.  One metric might be that, when we express $P$ as a formula in boolean logic, the length of that formula.  Another might be the number of operators in that formula, or the nesting depth of the formula.  I am interested in any and all algorithms for any plausible notion of complexity.</p>\n\n<p>This can be viewed as a kind of learning problem, where Occam\'s razor suggests that low-complexity predicates are a priori more likely than high-complexity predicates.</p>\n', 'ViewCount': '30', 'Title': 'Boolean formula that agrees with most truth assignments', 'LastActivityDate': '2014-02-28T03:52:00.653', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<optimization><logic><formal-methods><learning-theory><boolean-algebra>', 'CreationDate': '2014-02-28T03:52:00.653', 'FavoriteCount': '1', 'Id': '22122'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '97', 'Title': 'Bin packing problem or not?', 'LastEditDate': '2014-03-01T16:16:32.943', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12683', 'FavoriteCount': '1', 'Body': '<p>Suppose I have $N$ bins and $M$ items as depicted in the figure below (3 bins and 3 items):</p>\n\n<p>Suppose that every bin has unit capacity and the weights of the items depend on the bins used. I want to maximize the number of items in the bins subject to:</p>\n\n<ul>\n<li>One bin contains at most one item.</li>\n<li>If item $i$ is on bin $j$ then $g_{ij}\\geq1$ must hold now if all other bins are empty.</li>\n<li>If item $i$ is on bin $j$ (so $g_{ij}\\geq1$ must hold now) and item $i^\\prime$ is on bin $j^\\prime$, then $g_{ij}\\geq g_{ij^\\prime}$ and $g_{i^\\prime j^\\prime}\\geq g_{i^\\prime j}$ must both hold now.</li>\n<li>If item $i$ is on bin $j$ (so $g_{ij}\\geq1$ must hold now) and item $i^\\prime$ is on bin $j^\\prime$ (so $g_{ij}\\geq g_{ij^\\prime}$ and $g_{i^\\prime j^\\prime}\\geq g_{i^\\prime j}$ must both hold now) and item $i^{\\prime\\prime}$ is on bin $j^{\\prime\\prime}$, then $g_{ij}\\geq g_{ij^\\prime}+g_{ij^{\\prime\\prime}}$ and $g_{i^\\prime j^\\prime}\\geq g_{i^\\prime j}+g_{i^\\prime j^{\\prime\\prime}}$ and $g_{i^{\\prime\\prime} j^{\\prime\\prime}}\\geq g_{i^{\\prime\\prime} j}+g_{i^{\\prime\\prime} j^{\\prime}}$ must all hold now.</li>\n<li>And so on and so forth.</li>\n<li>In general I will have the following constraint: $g_{ij}x_{ij}\\geq\\sum\\limits_{i^\\prime=1,\\;i^\\prime \\neq i}^{M}\\sum\\limits_{j^\\prime=1,\\;j^\\prime \\neq j}^{N}g_{ij^\\prime}x_{i^\\prime j^\\prime}$, where $x_{ij}$ equals $1$ if item $i$ is in bin $j$ and equals $0$ otherwise.</li>\n</ul>\n\n<p>Finally, I have the following problem:</p>\n\n<p>Maximize $\\sum\\limits_{i=1}^{M}\\sum\\limits_{j=1}^{N}x_{ij}$</p>\n\n<p>subject to</p>\n\n<ul>\n<li><p>$\\frac{g_{ij}x_{ij}}{\\sum\\limits_{i^\\prime=1,\\;i^\\prime \\neq i}^{M}\\sum\\limits_{j^\\prime=1,\\;j^\\prime \\neq j}^{N}g_{ij^\\prime}x_{i^\\prime j^\\prime}}\\geq x_{ij},\\; \\forall i, j,$ (C1)</p></li>\n<li><p>$\\sum\\limits_{j=1}^{N}x_{ij}\\leq1,\\; \\forall i,$ (C2)</p></li>\n<li><p>$\\sum\\limits_{i=1}^{M}x_{ij}\\leq1,\\; \\forall j,$ (C3)</p></li>\n</ul>\n\n<p>and $x_{ij}\\in\\{0, 1\\},\\; \\forall i, j,$ (C4)</p>\n\n<p>The input of the problem is $M$, $N$, and $g_{ij},\\;\\forall i,j$. The right hand side of constraint (C1) is to say that when item $i$ is not in bin $j$ (i.e., $x_{ij}=0$) then (C1) is not violated. (C2) and (C3) say that one item goes to one bin and one bin contains one item, respectively. Finally, (C4) is the variable of the problem which is a binary variable.</p>\n\n<p>My question is: Can I say that this problem is a bin packing problem and it is therefore NP-hard? If not, Can you suggest a reduction idea from an NP-complete problem?</p>\n\n<p>Thank you for your help.</p>\n\n<p><img src="http://i.stack.imgur.com/xL94M.jpg" alt="enter image description here"></p>\n', 'Tags': '<complexity-theory><np-complete><optimization><np-hard>', 'LastEditorUserId': '12683', 'LastActivityDate': '2014-03-01T18:23:05.530', 'CommentCount': '5', 'AcceptedAnswerId': '22157', 'CreationDate': '2014-02-28T18:42:52.333', 'Id': '22136'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've written a genetic algorithm (GA) that solves a 7-dimensional optimisation problem.  All seven variables are floating point numbers.  The problem is that the entire population seems to converge to very nearly the same point in the solution space within about 20 generations, even if I increase the population size by 10x.</p>\n\n<p><strong>Attempted solutions</strong></p>\n\n<p>Starting with a parent population $\\vec{x}^{(j)}$, where $j$ is the individual's index in the population, I take the best 10% (i.e. highest fitness scores) to reproduce.  First I perform cross-over producing 2 children for every randomly selected pair of parents.  The parents can be re-used (is that a problem?).  About 5% of the children get mutated randomly.</p>\n\n<p>I've tried two variations of cross-over but both have the same problem:</p>\n\n<ol>\n<li><p>Calculating a 7 element weights vector $[w_i]$ and calculating the children's elements as $c_i^{(1)}=w_i x_i^{(1)} + (1-w_i)x_i^{(2)}$ and $c_i^{(2)}=(1-w_i)x_i^{(1)} + w_i x_i^{(2)}$.  The parents ith elements are $x_i^{(1)}$ for parent 1 and $x_i^{(2)}$ for parent 2.  Each weight is a sample from a uniform random variable in $[0.0;1.0)$.</p></li>\n<li><p>Confining the weights $w_i$ to be either $0.0$ or $1.0$, i.e. randomly exchanging elements of the two parents genomes to create the children.</p></li>\n</ol>\n\n<p>The mutation operation consists of randomly choosing one of the child's 7 elements and adding some Gaussian noise to it.  The standard deviation of that noise differs for each element since the elements have different physical units.</p>\n\n<p>Afterwards, I evaluate the fitness of the children and keep the best few hundred or thousand (setting that I choose at the start of the algorithm) out of the combined population of parents and children to give the next generation.  Note that the parents do not get mutated, especially since I want to preserve the best from the parent generation in case none of the children improve on it.</p>\n\n<p>I've tried population sizes from 1024 to 20480 and have also tried increasing the probability of mutating a child from 5% to 50% but I still have the problem that all the individuals in the population become very similar within the first 20 or so iterations.  Please advise on what I'm doing wrong.</p>\n\n<p>I should point out that the algorithm, despite this problem, does get fairly close to the optimal solution.  I know this because the quantity to maximise is the correlation between between two things (no more details, sorry) and I can get to about 0.94 (the maximum physically possible is always 1.0).  However, I am concerned that the GA is not covering enough of the solution space, causing it to miss the global maximum.</p>\n\n<p><strong>My questions</strong></p>\n\n<ol>\n<li>Are either of the above cross-over methods correct?</li>\n<li>Is it ok to re-use parents in cross-over?  Stated another way, is polygamy a good idea in this algorithm or should I change that part to ensure that no parent gets used more than once?</li>\n<li>Should I mutate the parents as well?</li>\n<li>What should I do with the 90% of parents that did not get used in the reproduction?</li>\n</ol>\n", 'ViewCount': '72', 'Title': 'How to stop genetic algorithm population converging to a single value', 'LastActivityDate': '2014-03-03T21:17:23.897', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22238', 'Score': '2', 'OwnerDisplayName': 'chippies', 'PostTypeId': '1', 'OwnerUserId': '15228', 'Tags': '<optimization><genetic-algorithms>', 'CreationDate': '2014-02-19T15:05:28.010', 'Id': '22216'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Hello I study computer science and I just digged into combinatorial optimization and ILP. </p>\n\n<p>I remember a riddle abotu a bunch of people on a river bank, and a boat with limited capacity (lets say the boat can transport 2 people at a time). There are various relations between the people like: person $x_m$ and $x_n$ cannot sail together and cannot even stay on the same bank of the river or $x_m$ and $x_n$ always have to be together.</p>\n\n<p>When I remembered this riddle, I thought whether it could be represented as an optimization problem. The criterial function would be $min(numberOfSails)$ and the conditions will be my limitations, but I am not sure whether I can model my problem like this (and I am not sure how to model all my conditions).</p>\n\n<p>Could you tell me or give me a hint, how such an approach for modelling this program would look like?</p>\n\n<p>At first I thought it could look something like this:</p>\n\n<pre><code>min i // i = number of sails\nx_i1 + ... + x_i2 &lt;= 2 // i-th sail can contain max 2 people\nx_im + x_in &lt;= 1 // x_m and x_n cant sail together\netc.\n</code></pre>\n\n<p>But there is so much wrong with this model (like considering the people left on the first bank or the people already transported) and so on, so I am not usre how to tackle this problem.</p>\n\n<p>Thanks you very much for any tips!</p>\n', 'ViewCount': '34', 'Title': 'Boat riddle as a Combinatorial optimization problem?', 'LastActivityDate': '2014-03-03T14:29:41.137', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12756', 'Tags': '<optimization><linear-programming>', 'CreationDate': '2014-03-03T14:29:41.137', 'Id': '22221'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Lets say you have a data model that consists of a 2D grid of integer points. This grid is sparsely populated and boundless in x and y (up to the max of a 32-bit integer).</p>\n\n<p>What is the best way to index these points in order to have an optimised lookup on an arbitrary (x,y) coordinate? Is an O(1) lookup solution possible?</p>\n', 'ViewCount': '105', 'Title': 'What is the best way to index lookups on a 2D array of integers that is boundless in x and y?', 'LastActivityDate': '2014-03-11T19:28:28.083', 'AnswerCount': '5', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15254', 'Tags': '<algorithms><optimization><databases><data-sets>', 'CreationDate': '2014-03-04T02:39:54.287', 'Id': '22251'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a cost function $f(X)=\\|\\hat{X}-X\\|_2$ to minimize which depends on a $s\\times s$ matrix $X$ where $\\hat{X}$ is given and $\\|X\\|_2=\\big(\\sum_{i,j}x_{ij}^2\\big)^{1/2} $. This matrix $X$ is generated by selecting only $s$ different rows from a matrix $B$ of dimension $n\\times s$. At the end, we are going to choose one matrix $X$ that generates the least cost $f(X)$ within all possible $n\\choose s$ submatrices of B. And so, this is a combinatorial problem that becomes complicated mostly when $n$ is big. </p>\n\n<p>So my question is can we find a suboptimal solution without going through all possible $n\\choose s$ submatrices and what kind of algorithm that I can apply to find such solution.</p>\n\n<p>My second question is can we apply a feature selection algorithm to find a suboptimal solution for a combinatorial problem.</p>\n', 'ViewCount': '156', 'Title': 'Suboptimal Solution for a combinatorial problem', 'LastEditorUserId': '7487', 'LastActivityDate': '2014-03-06T15:28:34.257', 'LastEditDate': '2014-03-06T01:52:17.043', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '22323', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7487', 'Tags': '<optimization><combinatorics><parallel-computing>', 'CreationDate': '2014-03-05T20:54:09.867', 'Id': '22316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have an unknown $n$-dimensional vector $x$ whose analytical expression depends on the following sum $x = z + Ba$ where the vector $z$ and the matrix $B\\in \\mathbb{R}^{n\\times s}$ are given. So the $s$-dimensional vector $a$ is to be computed to find $x$.</p>\n\n<p>The only assumption that we have is $x=0$ when we project $x$ onto the space spanned by $s$ different rows (that we don\u2019t know their indices) of the matrix $B$ which has $n$ rows. To do this projection we can use $P_s\\in \\mathbb{R}^{n\\times n}$  which is $1$ on the diagonal entries that correspond to the $s$ selected rows of $B$ and $0$ elsewhere. Hence, $P_s x= P_s z + P_s Ba=0 \\implies a=-(P_sB)^{-1}P_sz$.</p>\n\n<p>The main issue is that we don\u2019t know the positions of these $s$ rows, so the problem is combinatorial and we need to go through all possible $n\\choose s$ projections to find the exact $x$ which corresponds to the least cost $f(x)=\\|y-Ax\\|_2$ where $\\|v\\|_2=\\big(\\sum_iv_i^2\\big)^{1/2}$, $y\\in \\mathbb{R}^{m\\times 1}$ and the matrix $A\\in \\mathbb{R}^{m\\times n}$ are given. </p>\n\n<p>So my question is how I can reformulate my problem as a mixed-integer quadratic programming to go through all possible $n\\choose s$ submatrices of $B$ formed by the $s$ selected rows and finally find the set of rows which corresponds to the least $f(x)$.</p>\n', 'ViewCount': '42', 'Title': 'How to reformulate my problem as a mixed-integer quadratic problem', 'LastEditorUserId': '7487', 'LastActivityDate': '2014-03-06T17:56:47.790', 'LastEditDate': '2014-03-06T16:54:52.230', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22348', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7487', 'Tags': '<optimization><combinatorics><parallel-computing><integer-programming>', 'CreationDate': '2014-03-06T09:30:18.030', 'Id': '22333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Efficiency must be maximised and accuracy must be at least 97% for a algorithm doing image recognition in a database.</p>\n\n<p>In question (as stated in the question) is the dimension of the objectivness, also see:</p>\n\n<p>"Multiobjective optimization deals with solving problems having not only one, but multiple, often conflicting, criteria. Such problems can arise in practically every field of science, engineering and business, and the need for efficient and reliable solution methods is increasing. The task is challenging due to the fact that, instead of a single optimal solution, multiobjective optimization results in a number of solutions with different trade-offs among criteria, also known as Pareto optimal or efficient solutions. Hence, a decision maker is needed to provide additional preference information and to identify the most satisfactory solution. Depending on the paradigm used, such information may be introduced before, during, or after the optimization process. Clearly, research and application in multiobjective optimization involve expertise in optimization as well as in decision support."</p>\n\n<p>-- Multiobjective Optimization, Springer. Interactive and Evolutionary Approaches, Series: Lecture Notes in Computer Science, Vol. 5252</p>\n', 'ViewCount': '9', 'ClosedDate': '2014-04-01T22:09:33.723', 'Title': 'Is this problem a multi-objective optimisation problem?', 'LastEditorUserId': '13282', 'LastActivityDate': '2014-03-12T22:27:47.977', 'LastEditDate': '2014-03-12T22:27:47.977', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '-4', 'PostTypeId': '1', 'OwnerUserId': '13282', 'Tags': '<algorithms><optimization>', 'CreationDate': '2014-03-12T21:25:09.297', 'Id': '22555'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I need to use discrete differential evolutionary algorithm for assigning discrete values from set size $L$ to vectors of size $D$ where $L$ could be smaller, equal or larger than $D$. Elements of vector $X$ could take the same values of other elements.  My question is if we have a population of size $NP$ with each vector $X$ in the population of size $D$. How do we actually apply the mutation operand:</p>\n\n<p>$$V_{j,i}^{G+1} = X_{j, r_1}^{G} + F\\cdot (X_{j, r_2}^{G}-X_{j, r_3}^{G})$$</p>\n\n<p>where $i$, $r_1$, $r_2$, $r_3$ are references to vectors in $NP$ and none is equal to the other, $J$ is an index in vector $X$, and $F$ is a random number between $0$ and $1.2$.</p>\n\n<p>Suppose $X_{r_1}^{G}$ is equal to $\\{4, 1, 3, 2, 2, 0\\}$ and  $X_{r_2}^{G}$ is equal to $\\{2, 2, 3, 0, 4, 2\\}$ and $X_{r_3}^{G}$ is equal to $\\{1, 2, 3, 3, 0, 1\\}$\nCould anyone explain in detail the steps (through example if possible) on how to get the mutant vector $V_{j,i}^{G+1}$</p>\n', 'ViewCount': '25', 'Title': 'Mutation and crossover operations in discrete differential evolutionary operations?', 'LastEditorUserId': '15742', 'LastActivityDate': '2014-03-17T01:45:19.773', 'LastEditDate': '2014-03-17T01:45:19.773', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15742', 'Tags': '<algorithms><optimization><heuristics><evolutionary-computing>', 'CreationDate': '2014-03-16T07:22:44.630', 'Id': '22668'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have the memory reference string for a multithreaded application and want to run this through a simulator which implements/approximates Belady\'s OPT page replacement algorithm.</p>\n\n<p>But what is the best way to do this?</p>\n\n<p>There is no problem with a single threaded approach - just look for the page in memory with longest reuse distance and get rid of it. But with multithreaded this becomes much more complex - we know what the reuse distances for each thread is, but we don\'t know the "combined" reuse distance.</p>\n\n<p>Practically, it\'s easy to get into the position where a page for one thread has a very long reuse distance and so gets chucked out when memory space is needed, only for it to be quickly faulted back in by another thread.</p>\n\n<p>I cannot find any scientific literature on this - but cannot believe it hasn\'t been studied before: does anyone know of any papers that consider this issue?</p>\n', 'ViewCount': '25', 'Title': "Implementing/approximating Belady's OPT for multithreaded environments (papers?)", 'LastActivityDate': '2014-03-16T22:39:32.250', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6712', 'Tags': '<memory-management><paging><program-optimization><threads>', 'CreationDate': '2014-03-16T22:39:32.250', 'Id': '22687'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a set of points $C$, and I have the distance between each point $D(P_i,P_j)$. These distances are euclidean but the points are actually in a feature space.</p>\n\n<p>From the $C$ points I want to choose a subset of $n$ points. Call this subset $s$. I want to choose this subset so as to maximize the minimum distance between all the points in the new set $s$.</p>\n\n<p>$$\n\\max_{\\substack{s \\subset C \\\\ |s| = n}} \\left( \\min_{\\substack{i,j \\in s \\\\ i \\neq j} }   D \\left( P_i, P_j \\right) \\right)\n$$</p>\n\n<p>Right now I am using hill climbing to solve this problem. I understand that simulated annealing may give a better solution.</p>\n\n<p>Is there a known solution to this type of problem? Or can this problem be reformulated into another problem that is easily solved?</p>\n', 'ViewCount': '45', 'Title': 'Choosing a subset to maximize the minimum distance between points', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-19T00:06:01.163', 'LastEditDate': '2014-03-18T23:45:21.593', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22783', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15833', 'Tags': '<optimization>', 'CreationDate': '2014-03-18T19:27:14.100', 'Id': '22767'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Imagine we have N houses, on a standard euclidean 2D plane. We also have N "packages", each of which contains several "objects" of different types, let\'s call them A, B, C, etc. We know the content of all boxes beforehand, but can\'t change them (they\'re randomly generated).</p>\n\n<p>We need to send one package to every house, and we know that the person who lives in the house will need to use one or more of the objects, but we can\'t know which of them they will need. To use the object, they will travel to the nearest house that has it (ideally their own), use it there, and go back to their house. We need to assign a package to each house to minimize the potential distance they will have to travel.</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Package 1 contains B, C, D, E  </li>\n<li>Package 2 contains A, B, D, F  </li>\n<li>Package 3 contains A, E, F, G  </li>\n<li>House 1 is at (0,0)  </li>\n<li>House 2 is at (0, 5)</li>\n<li>House 3 is at (12, 0)</li>\n</ul>\n\n<p>We send packages 1, 2 and 3 to houses 1, 2 and 3: </p>\n\n<ul>\n<li>Owner of house 1 needs A and B: travels 5 to house 2 to get A (and back, but we\'re not counting that).</li>\n<li>Owner of house 2 needs C and G: travels 5 to house 1 to get C, goes back, then 13 to house 3 to get G (total = 18)</li>\n<li>Owner of house 3 needs A and G: travels 0 to their own house.</li>\n</ul>\n\n<p>Total distance = 23</p>\n\n<p>Since we can\'t know what they will want, we assume everyone needs everything. This means I need to minimize the sum of the shortest distance between every location and every type of object. Is this a known problem? How can I make an approximation algorithm for it? It sounds simple enough but I\'m stumped, I have no idea how to search for it.</p>\n\n<p>I was thinking along the lines of calculating a "differentness" of every package to every other one, then trying to place the "most different" packages closer to each other and the more similar ones further, but I don\'t really know how to do that either. </p>\n', 'ViewCount': '22', 'Title': 'Assigning packages to different points by minimizing distance: is this a known problem?', 'LastActivityDate': '2014-03-20T20:39:36.420', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15940', 'Tags': '<algorithms><optimization><combinatorics>', 'CreationDate': '2014-03-20T20:39:36.420', 'Id': '22879'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://en.wikipedia.org/wiki/Maximum_subarray_problem" rel="nofollow">Kadane Algorithm</a> is used to solve the maximum subarray problem which in simpler terms is to find the highest possible value of a continuous sub array in an array. </p>\n\n<p>One often cited application of Kadane Algorithm is that given the prices of the stock over X days , on which days should we buy and sell the stock in order to maximize given profit. These example is not practical at all as it is impossible to predict future prices of stocks and hence impossible to apply it . </p>\n\n<p>What are some real world application of Kadane algorithm?</p>\n', 'ViewCount': '30', 'Title': 'Practical Application of Kadane algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T08:20:17.887', 'LastEditDate': '2014-03-26T08:20:17.887', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><reference-request><optimization>', 'CreationDate': '2014-03-26T01:38:44.523', 'Id': '23055'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For a class project we're developing a software that solves a common optimisation problem. After some research we've found out that our problem is called QAP (Quadratic Asssignment Problem) and the algorithm that is commonly used is Branch and Bound. I understand the basics of the problem and I see the need of a lower bound to compute the sollution. I came up with a trivial bound example but our teacher told us lower bounds were no trivial matter and we should do some research. After a while we've found out that the Gilmore-Lawler bound is a good one to solve our problem (or at least good enough for learning purposes). </p>\n\n<p>Although I have read a couple of papers I can't get the grasp of it. The idea seems to be to convert the QAP into an LAP combining the two matrices of the original problem. I've got completely lost after that. How is the number I'm supposed to find as the lower bound calculated? </p>\n\n<p>Also, I'm aware that the lower bound has to be calculated for partial solutions, but how do I do that? The lower bound, as I understood, it's calculated from the program's matrices, which are a parameter for the branch and bound algorithm and are fixed from the beginning aren't they? I'd also need an explanation for that.</p>\n", 'ViewCount': '110', 'Title': 'Trying to understand the Gilmore-Lawler lower bound', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-28T12:34:17.467', 'LastEditDate': '2014-03-28T12:34:17.467', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12801', 'Tags': '<algorithms><optimization><heuristics><lower-bounds><branch-and-bound>', 'CreationDate': '2014-03-27T17:56:59.077', 'Id': '23146'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In the multi criteria decision making context, let $\\mathcal{A}$ be a set of alternatives or choices. Each alternative $\\alpha\\in \\mathcal{A}$ is a vector of $k$ criteria $\\alpha=(v_1,v_2,\\dots,v_k)$. Let $\\geq_1,\\dots,\\geq_k$ be a set of $k$ partial orders over $\\mathcal{A}$. \nFor any two alternative $\\alpha,\\beta\\in \\mathcal{A}$,  $\\alpha$ <em>dominates</em> $\\beta$ $(\\alpha\\geq \\beta)$ if and only if $\\alpha\\geq_i \\beta$ for all $i\\in\\{1,2,\\dots,k\\}$. </p>\n\n<p>An alternative $\\alpha$ is said to be Pareto optimal if there is no alternative dominates it. That is, $\\beta \\not\\geq \\alpha$ for all $\\beta\\in \\mathcal{A}$. \nThe Pareto set $S_{pareto}$ is the set of all Pareto optimal alternatives. </p>\n\n<p>I am looking for different notions with which one can have a subset of $S_{pareto}$. \nAre there other notions in the literature define a set $S$ which is guaranteed to be a subset of $S_{pareto}$? </p>\n', 'ViewCount': '15', 'Title': 'In multi criteria decision making, what notions are there to get a subset of the Pareto set?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-05T15:50:16.913', 'LastEditDate': '2014-04-05T15:50:16.913', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<terminology><optimization>', 'CreationDate': '2014-04-04T18:44:32.453', 'Id': '23430'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '62', 'Title': 'Automatic seat assignment algorithm', 'LastEditDate': '2014-04-05T20:43:50.023', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16478', 'Body': '<p>I am looking for articles relating to algorithms that deal with automatic selection of seating assignment.\nI need an algorithm (preferably more than one) that can automatically select a seating place while enforcing certain constraints that are predefined.\nOriginally I was planning on having the seats selected on the fly, meaning whenever a new person comes, the system selects the optimal seat for him based on the seats which were already taken, but I guess it is not a must.\nif there is a more general algorithm that can also present an approach fit for my problem that is also great.</p>\n\n<p>Lets call the seated people "players" , and our seating domain lets picture as a 2d matrix. lets say we have several groups among our "players" and you can set your "players" anywhere within the matrix as long as they are not seated next to other "players" from their own group . I am not claiming there is a perfect solution, I am looking for articles that are dealing with some approach for giving a solution - if you can direct me to an article or even give me a name for that kind of problem it is also good for me.</p>\n\n<p>Thanks,\nOlaf</p>\n', 'ClosedDate': '2014-04-07T07:05:22.693', 'Tags': '<algorithms><optimization>', 'LastEditorUserId': '16478', 'LastActivityDate': '2014-04-06T07:37:45.630', 'CommentCount': '6', 'AcceptedAnswerId': '23467', 'CreationDate': '2014-04-05T17:48:27.070', 'Id': '23458'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m trying to understand the material in "A Dual Coordinate Descent Method for Large-scale Linear SVM" by Hsieh et. al. (<a href="http://ntu.csie.org/~cjlin/papers/cddual.pdf" rel="nofollow">link to paper</a>) There is an equation for the Dual form of an unconstrained optimisation problem,</p>\n\n<p>$$\nf(\\mathbf{\\alpha})=\\dfrac{1}{2}\\mathbf{\\alpha}^T\\bar{Q}\\alpha-e^T\\alpha\n$$</p>\n\n<p>I don\'t understand what the $\\mathbf{e}^T$ means, it\'s not explained in the surronding text, so I assume it\'s just some common notation. Later in the paper $\\mathbf{e}_i$ is defined as $\\mathbf{e}_i=[0,\\ldots,1,0,\\ldots,0]^T$, so maybe it\'s some sort of selector term? Not sure if this second mention is even related.</p>\n\n<p>Please may someone explain to be what the $\\mathbf{e}^T$ bit is doing in this formula? Thank you for your time.</p>\n', 'ViewCount': '93', 'Title': 'Unknown notation "$e^T$" in a machine learning paper', 'LastEditorUserId': '472', 'LastActivityDate': '2014-04-08T12:08:07.270', 'LastEditDate': '2014-04-08T12:08:07.270', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23517', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16012', 'Tags': '<terminology><machine-learning><optimization>', 'CreationDate': '2014-04-07T14:04:03.300', 'Id': '23515'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let me preface this question by giving some helpful <a href="http://lcm.csa.iisc.ernet.in/dsa/node187.html" rel="nofollow">background material</a>. </p>\n\n<p>I\'m trying to solve the traveling salesman problem using branch and bound. Concretely, for a partial solution, I\'m using the solution the algorithm would attain by making greedy choices as an upper bound, and comparing this bound to the current best solution. If the bound attained using this heuristic is better than the cost of the best known solution, we continue exploring the partial solution. Otherwise, we prune and discard. Note that the greedy solution attained is always valid.</p>\n\n<p>The above link mentions a method for finding a global lower bound. This is fairly intuitive: for each node, we add to the bound the sum of the two minimum cost edges to other nodes (representing the cheapest in/out edges, if you will) and divide by two to avoid double counting. The solution implied by this global bound is almost never a valid solution but instead gives a hard limit on the lowest cost we could ever hope to attain. </p>\n\n<p>Here\'s how I\'m currently trying to calculate a local lower bound (a lower bound for a partial solution) with cost $C(S)$.</p>\n\n<ul>\n<li>Set <code>bound</code> to the current accumulated cost, $C(S)$</li>\n<li>For each unvisited node, add to <code>bound</code> the sum of the two least cost edges, making sure that the edges selected don\'t lead to a node already visited</li>\n<li>return <code>bound/2</code></li>\n</ul>\n\n<p>Here\'s my question: How is the above solution different from the greedy bound? Maybe a better lower bound would be to add to <code>bound</code> the sum of the two least cost edges, paying <strong>no attention to whether these edges lead to nodes already visited</strong>. Also, how do I actually use this bound to prune the search space? Do I compare the local lower bound computed for a partial solution with the global lower bound, pruning if $L(S) &lt; GL$ or if $L(S) &gt; C(B)$, or if $L(S) &gt; L(B)$? (here $GL$ corresponds to the global lower bound; $L(S)$ computes the local lower bound on a partial solution; $C(B)$ represents the trip cost on the best known solution; $L(B)$ is the lower bound on the best solution). Also, how will $L(S)$ ever be less than the global lower bound (i.e. an infeasible solution) if we only ever use (valid) edges to cities we haven\'t yet visited? </p>\n\n<p>Comments and suggestions on using this lower bound as well as its efficacy would be very much appreciated. Thank you. </p>\n', 'ViewCount': '70', 'Title': 'Traveling Salesman: how to use a lower bound?', 'LastEditorUserId': '16389', 'LastActivityDate': '2014-04-08T08:07:23.937', 'LastEditDate': '2014-04-08T03:01:07.293', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16389', 'Tags': '<complexity-theory><optimization><lower-bounds><traveling-salesman><branch-and-bound>', 'CreationDate': '2014-04-07T17:34:38.440', 'Id': '23520'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Students will identify certain students they want to work with. I have therefore decided to split them into two groups where I want to minimize the number of people in Group 1 who want to work with students from Group 2. </p>\n\n<p>I was thinking about creating a source node <em>s</em>, and creating a node for each person ($p_i$) - followed by hooking up the <em>s</em> to each $p_i$. Then I would create, another series of nodes for each person ($q_i$) and hook up each $p_i$ to each $q_i$ if $p_i$ <strong>doesn\'t</strong> want to work with $q_i$. Then, I would hook up each $q_i$ to a terminal node <em>t</em>. Each of the edges would have weight 1.</p>\n\n<p>I was thinking about running Edmonds\u2013Karp on it. Now, the solution would yield the maximum bipartite matching of the group (see e.g. <a href="https://www.youtube.com/watch?v=c9uLwB6aUVQ" rel="nofollow">here</a>). For each active arc from $p_i$  to $q_i$ in the final diagram, I would separate those two students.</p>\n\n<p>However, I have a bad taste in my mouth after running this algorithm; the bad taste stems from modeling the instance with respect to my intention: If I maximize the complement (the desire not to work with someone), do I really minimize the desire of students to work with each other across the two groups?</p>\n\n<p>If my hunch is correct (in that I\'m wrong), please point me in the right direction.</p>\n', 'ViewCount': '53', 'Title': 'How to optimally seperate a student body?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T09:54:48.057', 'LastEditDate': '2014-04-09T09:53:59.487', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23586', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<algorithms><optimization><polynomial-time><bipartite-matching>', 'CreationDate': '2014-04-09T06:15:38.270', 'Id': '23582'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading up on <a href="http://en.wikipedia.org/wiki/Dynamic_programming">Dynamic Programming</a> when I came across the following quote</p>\n\n<blockquote>\n  <p>A dynamic programming algorithm will examine all possible ways to\n  solve the problem and will pick the best solution. Therefore, we can\n  roughly think of dynamic programming as an <strong>intelligent, brute-force\n  method that enables us to go through all possible solutions to pick\n  the best one</strong>. If the scope of the problem is such that going through\n  all possible solutions is possible and fast enough, dynamic\n  programming guarantees finding the optimal solution</p>\n</blockquote>\n\n<p>The following example was given </p>\n\n<blockquote>\n  <p>For example, let\'s say that you have to get from point A to point B as\n  fast as possible, in a given city, during rush hour. A dynamic\n  programming algorithm will look into the entire traffic report,\n  looking into all possible combinations of roads you might take, and\n  will only then tell you which way is the fastest. Of course, you might\n  have to wait for a while until the algorithm finishes, and only then\n  can you start driving. The path you will take will be the fastest one\n  (assuming that nothing changed in the external environment)</p>\n</blockquote>\n\n<p><a href="http://en.wikipedia.org/wiki/Brute-force_search">Brute Force</a> is trying every possible solution before deciding on the best solution . </p>\n\n<p>How is Dynamic Programming different from Brute Force if it also <strong>goes through all possible solutions before picking the best one</strong> , the only difference i see is that Dynamic Programming takes into account the additional factors ( traffic conditions in this case).</p>\n\n<p>Am  i correct to say that Dynamic Programming is a subset of Brute Force method ??</p>\n', 'ViewCount': '1511', 'Title': 'How is Dynamic programming different from Brute force', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T13:32:34.613', 'LastEditDate': '2014-04-09T22:36:34.710', 'AnswerCount': '6', 'CommentCount': '5', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><terminology><optimization><dynamic-programming>', 'CreationDate': '2014-04-09T15:58:08.683', 'FavoriteCount': '1', 'Id': '23599'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Has the following problem been studied before? If yes, what approaches/algorithms were developed to solve it?</p>\n\n<blockquote>\n  <p><strong>Problem ("Maximum Stacking Height Problem")</strong></p>\n  \n  <p>Given $n$ polygons, find their stable, non-overlapping arrangement\n  that <strong>maximizes their stacking height</strong> on a fixed floor under the\n  influence of gravity.</p>\n</blockquote>\n\n<p><br></p>\n\n<h2>Example</h2>\n\n<p>Three polygons:</p>\n\n<p><img src="http://i.stack.imgur.com/SbCt3.png" alt="enter image description here"></p>\n\n<p>and three of their infinitely many stable, non-overlapping arrangements, with different stacking heights:</p>\n\n<p><img src="http://i.stack.imgur.com/h938i.png" alt="enter image description here"></p>\n\n<p><br></p>\n\n<h2>Clarifications</h2>\n\n<ul>\n<li>All polygons have uniform mass and equal density</li>\n<li>Friction is zero</li>\n<li>Gravity is acting on every point into the downwards direction (i.e. the force vectors are all parallel)</li>\n<li>A configuration is not considered stable if it rests on an unstable equilibrium point (for example, the green triangle in the pictures can not balance on any of its vertices, even if the mass to the left and the right of the balance point is equal)</li>\n<li>To further clarify the above point: A polygon is considered unstable ("toppling") <em>unless</em> it rests on at least one point <em>strictly to the left</em> <strong>and</strong> at least one point <em>strictly to the right</em> of its center of gravity (this definition greatly simplifies simulation and in particular makes position integration etc. unnecessary for the purpose of evaluating whether or not an arrangement is stable.</li>\n<li>The problem in its "physical" form is a continuous problem that can only be solved approximately for most cases. <strong>To obtain a discrete problem that can be tackled algorithmically, constrain both the polygon vertices and their placement in the arrangement to suitable lattices.</strong></li>\n</ul>\n\n<p><br></p>\n\n<h2>Notes</h2>\n\n<ul>\n<li>Brute force approaches of any kind are clearly infeasible. Even with strict constraints on the placement of polygons inside the lattice (such as providing a limited region "lattice space") the complexity simply explodes for more than a few polygons.</li>\n<li>Iterative algorithms must bring some very clever heuristics since it is easy to construct arrangements where removing any single polygon results in the configuration becoming unstable and such arrangements are unreachable by algorithms relying on every intermediate step being stable.</li>\n<li>Since the problem smells at least NP- but more likely EXPTIME-complete in the total number of vertices, even heuristics would be of considerable interest. <strong>One thing that gives hope is the fact that most humans will recognize that the third arrangement in the example is optimal.</strong></li>\n</ul>\n', 'ViewCount': '46', 'Title': 'Maximum Stacking Height Problem', 'LastActivityDate': '2014-04-10T18:20:35.550', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16652', 'Tags': '<algorithms><time-complexity><optimization><computational-geometry><heuristics>', 'CreationDate': '2014-04-10T17:14:32.580', 'Id': '23651'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '52', 'Title': 'Application of Combinatorics, Logic and computability theory in physical science: Tiling of Wang Tile with proportionality', 'LastEditDate': '2014-04-11T12:23:29.707', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14553', 'FavoriteCount': '1', 'Body': '<p>The original problem of Domino Tiling and Wang Tile has great theoretical interest on computability theory... However, the great emerging problem on application of Wang Tile in material science and physics requires the tiling to satisfy one more condition:</p>\n\n<p>The tiling should satisfy some proportionality, say, Tile 1 should appear with frequency 1/16, Tile 2 with frequency 9/16, Tile 3 with 6/16, Tile 4 with frequency 0...</p>\n\n<p>The most important decision problem is the following:\nCould a given set of Tile tile a grid of size NxN satisfying the frequency constraint within a error of +-epsilon.</p>\n\n<p>For example: could the set {Tile 1, Tile 2, Tile 3, Tile 4} tile the NxN grid with frequency 1/16+-0.01, 9/16+-0.01, 6/16+-0.01, 0+-0.01 respectively....</p>\n\n<p>From one of my previous post:</p>\n\n<p><a href="http://mathoverflow.net/questions/161731/practical-algorithms-for-np-complete-problems">Algorithms for NP complete problem</a></p>\n\n<p>I realize the decision problem of tiling without such constraint could be modeled by SAT... With this constraint the problem becomes ridiculously difficult and I eagerly seek for solutions towards this finite decidable problem.... (we could forget epsilon for a moment if the problem with epsilon is too hard)...</p>\n\n<p>So here is the question: how do we model this problem in MIP or SAT or any other optimization algorithm?</p>\n\n<p>For more detail why this problem is practical in material science and physics, see my previous post:</p>\n\n<p><a href="http://mathoverflow.net/questions/147374/coloring-in-lattice">coloring in lattice</a></p>\n\n<p><a href="http://mathoverflow.net/questions/149565/reference-for-wang-tile">reference for wang tile</a></p>\n\n<p><a href="http://mathoverflow.net/questions/157239/computational-approach-deciding-whether-a-set-of-wang-tile-could-tile-the-space">Computational approach deciding whether a set of Wang Tile could tile the space up to some size</a></p>\n\n<p>P.S. this is a bounty question from mathoverflow without yet a applicable solution...</p>\n\n<p><a href="http://mathoverflow.net/questions/162248/application-of-combinatorics-logic-and-computability-theory-in-physical-science">Application of Combinatorics, Logic and computability theory in physical science: Tiling of Wang Tile with proportionality</a></p>\n', 'ClosedDate': '2014-04-11T06:41:13.197', 'Tags': '<complexity-theory><optimization><logic><satisfiability>', 'LastEditorUserId': '14553', 'LastActivityDate': '2014-04-11T12:23:29.707', 'CommentCount': '6', 'AcceptedAnswerId': '23664', 'CreationDate': '2014-04-10T23:23:56.653', 'Id': '23662'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Using gradient descent in <em>d</em> dimensions to find a local minimum requires computing gradients, which is computationally much faster than Newton's method, because Newton's method requires computing both gradients and Hessians.</p>\n\n<p>However, gradient descent generally requires many more iterations than Newton's method to converge within the same accuracy.</p>\n\n<p>My question, then, is:</p>\n\n<p>Assuming they both converge, in terms of the <em>number of elementary floating-point operations</em>, which is usually faster:  Newton's method or gradient descent?  Why?</p>\n", 'ViewCount': '32', 'Title': "Gradient descent vs. Newton's method: which is more efficient?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T13:38:17.120', 'LastEditDate': '2014-04-12T13:38:17.120', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<algorithms><algorithm-analysis><optimization><numerical-algorithms>', 'CreationDate': '2014-04-12T11:27:02.383', 'Id': '23701'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I will try to give the motivation behind this problem and later the math formality.<br>\nGiven a grayscale image (1 Channel - M by N Matrix).<br>\nSomeone marks some pixels as anchors.<br>\nNow, you need to interpolate the other pixels (Which are not anchors) by minimizing a given cost function s.t. the end result is an image which has the original image values at the anchors and interpolated values else were s.t. it minimizes the cost function.</p>\n\n<p>Given an $M$ by $N$ matrix (A 1 channel image for that matter) $ I $.</p>\n\n<p>Subset of the elements (Pixels) in the matrix are marked as reference and their location is a group marked as $ S $.</p>\n\n<p>The optimization cost function is given by:</p>\n\n<p>$$ \\sum_{\\mathbf{r}} \\left( E(\\mathbf{r}) - \\sum_{\\mathbf{s} \\in N(\\mathbf{r})} {w}_{\\mathbf{rs}} E(\\mathbf{s}) \\right)^2, \\text{ s.t. }  \\forall p \\in S \\; E(\\mathbf{p}) = I(\\mathbf{p})\\,, $$</p>\n\n<p>where a bold letter $ \\mathbf{p}, \\mathbf{r}, \\mathbf{s} $ means an element (Pixel) location.</p>\n\n<p>The group $ N(r) $ is the neighborhood of $ \\mathbf{r} $, which is size $ k $ namely, a $ k  $ by $ k $ rectangle where $ \\mathbf{r} $ is in the middle.</p>\n\n<p>The weights $ {w}_{\\mathbf{rs}} $ are defined as following:</p>\n\n<p>$$ {w}_{\\mathbf{rs}} \\propto \\exp\\left(-\\frac{(I(\\mathbf{r}) - I(\\mathbf{s}))^2}{2\\sigma^2_r} \\right )\\ \\ \\text{ s.t. } \\sum_{\\mathbf{s} \\in N(\\mathbf{r})} w_{\\mathbf{rs}} = 1\\,. $$</p>\n\n<p>Namely, the weights are normalized to 1 within the neighborhood. The variance is calculated on the matrix $ I $ in the neighborhood (You can assume it is given).</p>\n\n<p>So the problem is to find a matrix $ E $ which is equal to $ I $ on all reference points and interpolates other places by bringing the cost function to minimum.</p>\n\n<p>It looks like a weighted least squares per neighborhood (The inner brackets).  </p>\n\n<p>I couldn't formalize it (For the whole matrix) a way that can be easily calculated and solved in e.g. MATLAB.\nIs there a way to formalize it as classic Weighted LS problem?<br>\nOr any other form which using the classic tools will bring solution (Get the interpolated matrix)?</p>\n\n<p>Could any one help with that? </p>\n\n<p>P.S.\nI tried using the Weighted LS per neighborhood disregarding the rest didn't yield the expected results (Obviously).<br>\nTried it just to see how far the real solution is from this naive solution.</p>\n", 'ViewCount': '69', 'Title': 'Interpolation Optimization Problem', 'LastEditorUserId': '1645', 'LastActivityDate': '2014-04-21T22:36:15.533', 'LastEditDate': '2014-04-14T23:52:12.547', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1645', 'Tags': '<optimization>', 'CreationDate': '2014-04-14T18:48:09.033', 'Id': '23794'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The query is given as follows:</p>\n\n<pre><code>Select T1.A \nFROM T1,T2,T3\nWHERE T1.A = T2.A\nAND T1.B=T3.B\nAND T1.C=T3.C\n</code></pre>\n\n<p>TABLE T1,T2 AND T3 ARE HAVING 4,8, AND 16 PARTITIONS.\n(USE nested loops,sort merge and hash join as available methods of join)</p>\n\n<p>4 processors means, this query should run in parallel.\nso the table blocks will be accessed in parallel.</p>\n', 'ViewCount': '9', 'ClosedDate': '2014-04-26T12:05:58.080', 'Title': 'How to prepare execution plan for given sql query on a server having 4 processors', 'LastEditorUserId': '17091', 'LastActivityDate': '2014-04-26T13:52:21.703', 'LastEditDate': '2014-04-26T13:52:21.703', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17091', 'Tags': '<optimization><databases>', 'CreationDate': '2014-04-26T08:59:05.430', 'Id': '24124'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am working on a solving a graph partitioning problem and have found a way to formulate it as a trace minimization. I am hoping this will allow me to relax the problem to a continuous one. I am wondering if someone is familiar with any similar trace minimization problem or can help reformulate my constraints such that they fit a trace minimization problem.</p>\n\n<p>The initial problem is as follows. Given an undirected connected graph $G=(V,E)$ partition the nodes in the graph into 2 connected components denoted $S$ and $\\bar S$ to maximize $\\frac{(\\sum\\limits_{i \\in S} w_i)^2}{|S|}+\\frac{(\\sum\\limits_{i \\in \\bar S} w_i)^2}{ |\\bar S|}$</p>\n\n<p>I have mapped this problem into the following trace optimization problem.</p>\n\n<p>Let $x,y \\in\\{0,1\\}^n$,$x=\\mathbf{1}-y$. Here the set $S=\\{i:x_i=1\\}$ . We define  $X=[\\alpha x \\ \\beta y]$ for $\\alpha=\\frac{1}{\\sqrt{x'x}}$ and $\\beta=\\frac{1}{\\sqrt{y'y}}$ . $A$ is the adjacency matrix of the undirected graph $G$. Finally $W=ww'$ where $w_i$ is the weight of vertex $i$. We seek</p>\n\n<p>$\\max \\limits_{X'X=I, (A-I)X \\geq 0} trace(X'WX)$</p>\n\n<p>For the case of a complete graph I have been able to show that this maps to an NP-hard problem, but for sparse graphs I am not sure if it is NP-hard, although I suspect that it is and hope a continuous relaxation might give an approximation. Is anyone familiar with any similar optimization problems? Given the 2 constraints if I can rewrite them in the form of $X'BX=I$ then this would be a well known trace minimization. Any ideas would be helpful.</p>\n", 'ViewCount': '76', 'Title': 'Graph partitioning problem', 'LastEditorUserId': '16985', 'LastActivityDate': '2014-04-28T16:00:54.943', 'LastEditDate': '2014-04-28T16:00:54.943', 'AnswerCount': '0', 'CommentCount': '13', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16985', 'Tags': '<graph-theory><optimization><adjacency-matrix>', 'CreationDate': '2014-04-26T12:01:33.800', 'Id': '24127'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Assume I have some positive numbers $a_1,\\ldots,a_n$ and a number $k \\in \\mathbb{N}$. </p>\n\n<p>I want to partition these numbers into exactly $k$ sets $A_1,\\ldots,A_k$ such that the weighted arithmetic mean</p>\n\n<p>$$\\text{cost}(A_i,\\ldots,A_k)=\\sum_{i=1}^{k}\\frac{|A_i|}{n}c(A_i)$$</p>\n\n<p>is minimal, where $c(A_i)=\\sum_{a \\in A_i}a$ is simply the sum of all numbers in $A_i$.</p>\n\n<p>Is there actually a (polynomial) algorithm to do this or is this a (<strong>NP</strong>) hard problem? </p>\n\n<p>I tried to reduce it to some NP-hard problems but didn't get anywhere, especially because the numbers are nonnegative and thus in an optimal partition big sets need to have smaller weight which seems to be some kind of balancing problem instead of a packing problem (which I am more familiar with).</p>\n", 'ViewCount': '23', 'Title': 'Minimum weighted arithmetic mean partion?', 'LastEditorUserId': '6970', 'LastActivityDate': '2014-04-29T20:54:41.280', 'LastEditDate': '2014-04-29T20:19:59.883', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24234', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6970', 'Tags': '<optimization><np-hard><np><partitions>', 'CreationDate': '2014-04-29T20:10:23.417', 'Id': '24232'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How to know the size of page frame used by my OS ?</p>\n\n<p>This could be useful for some optimizations when I code. (Allocate big buffer that fit in a page frame for example).</p>\n\n<p>Page frame is determined by the operating system ? Mine is Windows 7 (but impossible to find information about it on Google. So, may be I wrong...)</p>\n', 'ViewCount': '10', 'ClosedDate': '2014-04-30T11:39:20.670', 'Title': 'How to know the size of page frame used by my OS?', 'LastActivityDate': '2014-04-30T09:04:45.557', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17208', 'Tags': '<optimization><operating-systems><paging><virtual-memory><memory-allocation>', 'CreationDate': '2014-04-30T09:04:45.557', 'Id': '24253'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In Sutton and Barto\'s reinforcement learning book, in multi-armed bandit problem a phrase has been used. "finding an optimal action" using greedy/$\\epsilon$-greedy algorithm. When it is said that an algorithm "finds the optimal action " ? </p>\n', 'ViewCount': '25', 'Title': 'What does "finding an optimal action" for a bandit mean?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-01T05:10:18.083', 'LastEditDate': '2014-05-01T02:44:31.973', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '13091', 'Tags': '<algorithms><terminology><machine-learning><optimization><artificial-intelligence>', 'CreationDate': '2014-05-01T01:17:32.423', 'Id': '24282'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to write a genetic algorithm for a program. Most examples for genetic algorithms use something like this as the input:</p>\n\n<pre><code>aaaaaaaaaa\n</code></pre>\n\n<p>and mutate/crossover until they get</p>\n\n<pre><code>helloworld\n</code></pre>\n\n<p>or similar. This requires however that you start with something of length N, but my search space doesn't have a fixed length. How should I alter the mutation/crossover steps to allow for changes in unit size?</p>\n", 'ViewCount': '15', 'Title': 'Programming a genetic algorithm with a non-fixed size', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-02T21:29:08.760', 'LastEditDate': '2014-05-02T21:25:18.507', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17279', 'Tags': '<algorithms><optimization><genetic-algorithms>', 'CreationDate': '2014-05-02T17:10:57.277', 'Id': '24320'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '11', 'Title': 'Is it possible to do reductions with non-decision problems?', 'LastEditDate': '2014-05-02T21:48:04.360', 'AnswerCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16780', 'FavoriteCount': '1', 'Body': "<p>I've recently begun studying reductions in my algorithms class. All the reductions I've seen have been from decision problem $\\to$ decision problem.</p>\n\n<p>Is it possible to do reductions with non-decision problems? \nCan NP-hardness be shown in that way?</p>\n", 'ClosedDate': '2014-05-02T21:57:12.900', 'Tags': '<complexity-theory><reductions><optimization>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-02T21:48:04.360', 'CommentCount': '3', 'CreationDate': '2014-05-02T20:58:15.810', 'Id': '24327'}},