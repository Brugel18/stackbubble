{'ViewCount': '227', 'Title': 'Is Smoothed Analysis used outside academia?', 'LastEditDate': '2012-05-16T23:22:33.777', 'AnswerCount': '2', 'Score': '15', 'OwnerDisplayName': 'user20', 'PostTypeId': '1', 'FavoriteCount': '2', 'Body': '<p>Did the <a href="http://en.wikipedia.org/wiki/Smoothed_analysis">smoothed analysis</a> find its way into main stream analysis of algorithms? Is it common for algorithm designers to apply smoothed analysis to their algorithms?</p>\n', 'Tags': '<algorithms><complexity-theory><algorithm-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-05-17T23:41:36.817', 'CommentCount': '6', 'AcceptedAnswerId': '100', 'CreationDate': '2012-03-07T06:57:12.917', 'Id': '74''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've not gone much deep into CS. So, please forgive me if the question is not good or out of scope for this site.</p>\n\n<p>I've seen in many sites and books, the big-O notations like $O(n)$ which tell the time taken by an algorithm. I've read a few articles about it, but I'm still not able to understand how do you calculate it for a given algorithm.</p>\n", 'ViewCount': '7841', 'Title': 'How to come up with the runtime of algorithms?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-06T16:07:12.323', 'LastEditDate': '2013-06-06T16:07:12.323', 'AnswerCount': '5', 'CommentCount': '5', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '132', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><reference-question>', 'CreationDate': '2012-03-10T12:03:19.397', 'FavoriteCount': '11', 'Id': '192''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '893', 'Title': 'Logarithmic vs double logarithmic time complexity', 'LastEditDate': '2012-04-12T05:54:48.283', 'AnswerCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '652', 'FavoriteCount': '2', 'Body': '<p>In real world applications is there a concrete benefit when using $\\mathcal{O}(\\log(\\log(n))$ instead of $\\mathcal{O}(\\log(n))$ algorithms ?</p>\n\n<p>This is the case when one use for instance van Emde Boas trees instead of more conventional binary search tree implementations. \nBut for example, if we take $n &lt; 10^6$ then in the best case the double logarithmic algorithm outperforms the logarithmic one by (approximately) a factor of $5$. And also in general the implementation is more tricky and complex. </p>\n\n<p>Given that I personally prefer BST over VEB-trees, what do you think ?</p>\n\n<p><em>One could easily demonstrate that :</em></p>\n\n<p>$\\qquad \\displaystyle \\forall n &lt; 10^6.\\ \\frac{\\log n}{\\log(\\log(n))} &lt; 5.26146$</p>\n', 'Tags': '<algorithms><complexity-theory><binary-trees><algorithm-analysis><search-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T03:54:23.937', 'CommentCount': '3', 'AcceptedAnswerId': '661', 'CreationDate': '2012-03-22T14:23:03.533', 'Id': '654''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In algorithm analysis you often have to solve recurrences. In addition to Master Theorem, substitution and iteration methods, there is one using <em>characteristic polynomials</em>.</p>\n\n<p>Say I have concluded that a characteristic polynomial $x^2 - 2x + 2$ has <em>imaginary</em> roots, namely $x_1 = 1+i$ and $x_2 =1-i$. Then I cannot use</p>\n\n<p>$\\qquad c_1\\cdot x_1^n + c_2\\cdot x_2^n$</p>\n\n<p>to obtain the solution, right? How should I proceed in this case?</p>\n', 'ViewCount': '438', 'Title': 'Solving Recurrences via Characteristic Polynomial with Imaginary Roots', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-19T00:28:07.237', 'LastEditDate': '2012-04-01T07:20:09.083', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '852', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation>', 'CreationDate': '2012-03-31T10:13:58.077', 'FavoriteCount': '1', 'Id': '915''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>On Alpha-Beta pruning, <a href="http://en.wikipedia.org/wiki/Negascout" rel="nofollow">NegaScout</a> claims that it can accelerate the process by setting [Alpha,Beta] to [Alpha,Alpha-1].</p>\n\n<p>I do not understand the whole process of NegaScout.</p>\n\n<p>How does it work? What is its recovery mechanism when its guessing failed?</p>\n', 'ViewCount': '714', 'Title': 'How does the NegaScout algorithm work?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-04-08T21:24:54.280', 'LastEditDate': '2012-04-08T21:24:54.280', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '240', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2012-04-08T16:07:29.703', 'Id': '1134''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have the following algorithmic problem:</p>\n\n<blockquote>\n  <p>Determine the space Turing complexity of recognizing DNA strings that are Watson-Crick palindromes. </p>\n</blockquote>\n\n<p>Watson-Crick palindromes are strings whose reversed complement is the original string. The <em>complement</em> is defined letter-wise, inspired by DNA: A is the complement of T and C is the complement of G. A simple example for a WC-palindrome is ACGT.</p>\n\n<p>I've come up with two ways of solving this.</p>\n\n<p><strong>One requires $\\mathcal{O}(n)$ space.</strong></p>\n\n<ul>\n<li>Once the machine is done reading the input. The input tape must be copied to the work tape in reverse order. </li>\n<li>The machine will then read the input and work tapes from the left and compare each entry to verify the cell in the work tape is the compliment of the cell in the input. This requires $\\mathcal{O}(n)$ space. </li>\n</ul>\n\n<p><strong>The other requires $\\mathcal{O}(\\log n)$ space.</strong></p>\n\n<ul>\n<li>While reading the input. Count the number of entries on the input tape.</li>\n<li>When the input tape is done reading\n<ul>\n<li>copy the complement of the letter onto the work tape</li>\n<li>copy the letter L to the end of the work tape</li>\n</ul></li>\n<li>(Loop point)If the counter = 0, clear the worktape and write yes, then halt</li>\n<li>If the input tape reads L\n<ul>\n<li>Move the input head to the left by the number of times indicated by the counter  (requires a second counter)</li>\n</ul></li>\n<li>If the input tape reads R \n<ul>\n<li>Move the input head to the right by the number of times indicated by the counter (requires a second counter)</li>\n</ul></li>\n<li>If the cell that holds the value on the worktape matches the current cell on the input tape\n<ul>\n<li>decrement the counter by two</li>\n<li>Move one to the left or right depending if R or L is on the worktape respectively</li>\n<li>copy the Complement of L or R to the worktape in place of the current L or R</li>\n<li>continue the loop</li>\n</ul></li>\n<li>If values dont match, clear the worktape and write no, then halt</li>\n</ul>\n\n<p>This comes out to about $2\\log n+2$ space for storing both counters, the current complement, and the value L or R.</p>\n\n<p><strong>My issue</strong></p>\n\n<p>The first one requires both linear time and space. The second one requires $\\frac{n^2}{2}$ time and $\\log n$ space. I was given the problem from the quote and came up with these two approaches, but I don't know which one to go with. I just need to give the space complexity of the problem. </p>\n\n<p><strong>The reason I'm confused</strong></p>\n\n<p>I would tend to say the second one is the best option since it's better in terms of time, but that answer only comes from me getting lucky and coming up with an algorithm. It seems like if I want to give the space complexity of something, it wouldn't require luck in coming up with the right algorithm. Am I missing something? Should I even be coming up with a solution to the problem to answer the space complexity?</p>\n", 'ViewCount': '406', 'Title': 'The space complexity of recognising Watson-Crick palindromes', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-28T07:35:10.873', 'LastEditDate': '2012-04-11T20:15:46.800', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '1224', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '596', 'Tags': '<algorithms><algorithm-analysis><turing-machines><space-complexity>', 'CreationDate': '2012-04-11T15:21:09.507', 'Id': '1223''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>This is the recursive formula for which I\'m trying to find an asymptotic closed form by the <a href="http://en.wikipedia.org/wiki/Master_theorem" rel="nofollow">master theorem</a>:\n$$T(n)=9T(n/27)+(n \\cdot \\lg(n))^{1/2}$$</p>\n\n<p>I started with $a=9,b=27$ and $f(n)=(n\\cdot \\lg n)^{1/2}$  for using the master theorem by $n^{\\log_b(a)}$, and if so $n^{\\log_{27}(9)}=n^{2/3}$ but I don\'t understand how to play with the $(n\\cdot \\lg n)^{1/2}$. </p>\n\n<p>I think that the $(n\\cdot \\lg n)^{1/2}$ is bigger than $n^{2/3}$ but I\'m sure I skip here on something. </p>\n\n<p>I think it fits to the third case of the master theorem.</p>\n', 'ViewCount': '545', 'Title': 'Solve a recurrence using the master theorem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T13:23:52.493', 'LastEditDate': '2013-02-02T13:23:52.493', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '1297', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '747', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2012-04-16T01:19:14.927', 'Id': '1296''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1359', 'Title': 'Randomized Selection', 'LastEditDate': '2012-04-18T19:51:56.180', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1134', 'FavoriteCount': '2', 'Body': u'<p>The randomized selection algorithm is the following:</p>\n\n<p>Input: An array $A$ of $n$ (distinct, for simplicity) numbers and a number $k\\in [n]$</p>\n\n<p>Output: The the "rank $k$ element" of $A$ (i.e., the one in position $k$ if $A$ was sorted)</p>\n\n<p>Method:</p>\n\n<ul>\n<li>If there is one element in $A$, return it</li>\n<li>Select an element $p$ (the "pivot") uniformly at random</li>\n<li>Compute the sets $L = \\{a\\in A : a &lt; p\\}$ and $R = \\{a\\in A : a &gt; p\\}$</li>\n<li>If $|L| \\ge k$, return the rank $k$ element of $L$.</li>\n<li>Otherwise, return the rank $k - |L|$ element of $R$</li>\n</ul>\n\n<p>I was asked the following question:</p>\n\n<blockquote>\n  <p>Suppose that $k=n/2$, so you are looking for the median, and let $\\alpha\\in (1/2,1)$\n  be a constant.  What is the probability that, at the first recursive call, the \n  set containing the median has size at most $\\alpha n$?</p>\n</blockquote>\n\n<p>I was told that the answer is $2\\alpha - 1$, with the justification "The pivot selected should lie between $1\u2212\\alpha$ and $\\alpha$ times the original array"</p>\n\n<p>Why? As $\\alpha \\in (0.5, 1)$, whatever element is chosen as pivot is either larger or smaller than more than half the original elements. The median always lies in the larger subarray, because the elements in the partitioned subarray are always less than the pivot. </p>\n\n<p>If the pivot lies in the first half of the original array (less than half of them), the median will surely be in the second larger half, because once the median is found, it must be in the middle position of the array, and everything before the pivot is smaller as stated above. </p>\n\n<p>If the pivot lies in the second half of the original array (more than half of the elements), the median will surely first larger half, for the same reason, everything before the pivot is considered smaller. </p>\n\n<p>Example:</p>\n\n<p>3 4 5 8 7 9 2 1 6 10</p>\n\n<p>The median is 5.</p>\n\n<p>Supposed the chosen pivot is 2. So after the first iteration, it becomes:</p>\n\n<p>1 2 ....bigger part....</p>\n\n<p>Only <code>1</code> and <code>2</code> are swapped after the first iteration. Number 5 (the median) is still in the first greater half (accroding to the pivot 2). The point is, median always lies on greater half, how can it have a chance to stay in a smaller subarray?</p>\n', 'Tags': '<algorithms><algorithm-analysis><probability-theory><randomized-algorithms>', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-18T19:51:56.180', 'CommentCount': '5', 'AcceptedAnswerId': '1343', 'CreationDate': '2012-04-18T08:21:27.190', 'Id': '1334''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>The usual simple algorithm for finding the median element in an array $A$ of $n$ numbers is:</p>\n\n<ul>\n<li>Sample $n^{3/4}$ elements from $A$ with replacement into $B$</li>\n<li>Sort $B$ and find the rank $|B|\\pm \\sqrt{n}$ elements $l$ and $r$ of $B$</li>\n<li>Check that $l$ and $r$ are on opposite sides of the median of $A$ and that there are at most $C\\sqrt{n}$ elements in $A$ between $l$ and $r$ for some appropriate constant $C &gt; 0$.  Fail if this doesn\'t happen.</li>\n<li>Otherwise, find the median by sorting the elements of $A$ between $l$ and $r$</li>\n</ul>\n\n<p>It\'s not hard to see that this runs in linear time and that it succeeds with high probability. (All the bad events are large deviations away from the expectation of a binomial.)</p>\n\n<p>An alternate algorithm for the same problem, which is more natural to teach to students who have seen quick sort is the one described here: <a href="http://cs.stackexchange.com/questions/1334/randomized-selection/1343">Randomized Selection</a></p>\n\n<p>It is also easy to see that this one has linear expected running time: say that a "round" is a sequence of recursive calls that ends when one gives a 1/4-3/4 split, and then observe that the expected length of a round is at most 2.  (In the first draw of a round, the probability of getting a good split is 1/2 and then after actually increases, as the algorithm was described so round length is dominated by a  geometric random variable.)</p>\n\n<p>So now the question: </p>\n\n<blockquote>\n  <p>Is it possible to show that randomized selection runs in linear time with high probability?</p>\n</blockquote>\n\n<p>We have $O(\\log n)$ rounds, and each round has length at least $k$ with probability at most $2^{-k+1}$, so a union bound gives that the running time is $O(n\\log\\log n)$ with probability $1-1/O(\\log n)$.</p>\n\n<p>This is kind of unsatisfying, but is it actually the truth?</p>\n', 'ViewCount': '98', 'Title': 'Sharp concentration for selection via random partitioning?', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-19T19:52:47.013', 'LastEditDate': '2012-04-19T19:52:47.013', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1350', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '657', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms>', 'CreationDate': '2012-04-18T20:22:01.597', 'Id': '1346''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have written a program to sort Linked Lists and I noticed that my insertion sort works much better than my quicksort algorithm. \nDoes anyone have any idea why this is?\nInsertion sort has a complexity of $\\Theta(n^2)$ and quicksort $O(n\\log n)$ so therefore quicksort should be faster. I tried for random input size and it shows me the contrary. Strange...</p>\n\n<p>Here the code in Java:</p>\n\n\n\n<pre><code>public static LinkedList qSort(LinkedList list) {\n\n    LinkedList x, y;\n    Node currentNode;\n    int size = list.getSize();\n\n    //Create new lists x smaller equal and y greater\n    x = new LinkedList();\n    y = new LinkedList();\n\n    if (size &lt;= 1)\n        return list;\n    else {\n\n        Node pivot = getPivot(list);\n        // System.out.println("Pivot: " + pivot.value);     \n        //We start from the head\n        currentNode = list.head;\n\n        for (int i = 0; i &lt;= size - 1; i++) {\n            //Check that the currentNode is not our pivot\n            if (currentNode != pivot) {\n                //Nodes with values smaller equal than the pivot goes in x\n                if (currentNode.value &lt;= pivot.value) {\n                    {\n                        x.addNode(currentNode.value);\n                        // System.out.print("Elements in x:");\n                        // x.printList();\n                    }\n\n                } \n                //Nodes with values greater than the pivot goes in y\n                else if (currentNode.value &gt; pivot.value) {\n                    if (currentNode != pivot) {\n                        y.addNode(currentNode.value);\n                        // System.out.print("Elements in y:");\n                        // y.printList();\n                    }\n                }\n            }\n            //Set the pointer to the next node\n            currentNode = currentNode.next;\n        }\n\n        //Recursive calls and concatenation of the Lists and pivot\n        return concatenateList(qSort(x), pivot, qSort(y));\n\n    }\n}\n</code></pre>\n', 'ViewCount': '3274', 'Title': 'Quicksort vs. insertion sort on linked list: performance', 'LastEditorUserId': '1011', 'LastActivityDate': '2013-02-01T13:07:24.713', 'LastEditDate': '2012-04-20T07:08:17.467', 'AnswerCount': '3', 'CommentCount': '6', 'AcceptedAnswerId': '1386', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><algorithm-analysis><sorting><lists>', 'CreationDate': '2012-04-19T12:05:13.983', 'Id': '1354''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '4509', 'Title': 'Quicksort explained to kids', 'LastEditDate': '2012-04-19T21:11:51.423', 'AnswerCount': '4', 'Score': '10', 'OwnerDisplayName': 'd555', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '3', 'Body': u'<p>Last year, I was reading a fantastic <a href="http://arxiv.org/abs/quant-ph/0510032">paper on \u201cQuantum Mechanics for Kindergarden\u201d</a>. It was not easy paper.</p>\n\n<p>Now, I wonder how to explain quicksort in the simplest words possible. How can I prove (or at least handwave) that the average complexity is $O(n \\log n)$, and what the best and the worst cases are, to a kindergarden class? Or at least in primary school?</p>\n', 'Tags': '<algorithms><education><algorithm-analysis><didactics><sorting>', 'LastEditorUserId': '5', 'LastActivityDate': '2012-04-20T14:53:59.763', 'CommentCount': '8', 'AcceptedAnswerId': '1369', 'CreationDate': '2012-04-19T20:23:14.023', 'Id': '1367''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In many texts a lower bound for finding $k$th smallest element is derived making use of arguments using medians. How can I find one using an adversary argument?</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Selection_algorithm">Wikipedia</a> says that tournament algorithm runs in $O(n+k\\log n)$, and $n - k + \\sum_{j = n+2-k}^{n} \\lceil{\\operatorname{lg}\\, j}\\rceil$ is <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Lower_bounds">given</a> as lower bound.</p>\n', 'ViewCount': '489', 'Title': 'Lower bound for finding kth smallest element using adversary arguments', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-20T17:33:33.450', 'LastEditDate': '2012-04-20T07:47:54.503', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '1378', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '947', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2012-04-20T04:11:34.473', 'Id': '1377''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '660', 'Title': 'How to use adversary arguments for selection and insertion sort?', 'LastEditDate': '2012-04-23T06:41:39.370', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '947', 'FavoriteCount': '1', 'Body': '<p>I was asked to find the adversary arguments necessary for finding the lower bounds for selection and insertion sort. I could not find a reference to it anywhere.</p>\n\n<p>I have some doubts regarding this. I understand that adversary arguments are usually used for finding lower bounds for certain "problems" rather than "algorithms".</p>\n\n<p>I understand the merging problem. But how could I write one for selection and insertion sort?</p>\n', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-23T17:27:14.653', 'CommentCount': '4', 'AcceptedAnswerId': '1465', 'CreationDate': '2012-04-23T03:40:40.260', 'Id': '1455''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Is there any algorithm that works better than $\\Theta(n^2)$ to verify whether a square matrix is a magic one? (E.g. such as sum of all the rows, cols and diagonally are equal to each other). \nI did see someone mention a $O(n)$ time on a website a few days ago but could not figure out how.</p>\n', 'ViewCount': '380', 'Title': 'Magic Square Check for NxN Matrix - with Minimum Complexity?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-23T17:16:30.050', 'LastEditDate': '2012-04-23T17:15:09.943', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '852', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2012-04-23T09:51:52.267', 'Id': '1460''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I\'m working on problem H in the <a href="http://neerc.ifmo.ru/past/2004/problems/problems.pdf" rel="nofollow">ACM ICPC 2004\u20132005 Northeastern European contest</a>.</p>\n\n<p>The problem is basically to find the worst case that produces a maximal number of exchanges in the algorithm (sift down) to build the heap.</p>\n\n<ul>\n<li>Input: Input \ufb01le contains $n$ ($1 \\le n \\le 50{,}000$).</li>\n<li>Output:  Output the array containing $n$ different integer numbers from $1$ to $n$, such that it is a heap, and when converting it to a sorted array, the total number of exchanges in sifting operations is maximal possible.</li>\n</ul>\n\n<p>Sample input: <code>6</code><br>\nCorresponding output: <code>6 5 3 2 4 1</code></p>\n\n<p>And the basics outputs:</p>\n\n<pre><code>[2, 1]   \n[3, 2, 1]   \n[4, 3, 1, 2] \n[5, 4, 3, 2, 1] \n[6, 5, 3, 4, 1, 2]\n</code></pre>\n', 'ViewCount': '2275', 'Title': 'Finding a worst case of heap sort', 'LastEditorUserId': '1152', 'LastActivityDate': '2013-10-28T16:39:52.673', 'LastEditDate': '2012-04-28T13:55:03.753', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><data-structures><algorithm-analysis><sorting>', 'CreationDate': '2012-04-27T21:28:44.810', 'FavoriteCount': '1', 'Id': '1540''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I have developed two algorithms and now they are asking me to find their running time.\nThe problem is to develop a singly linked list version for manipulating polynomials. The two main operations are <em>addition</em> and <em>multiplication</em>.</p>\n\n<p>In general for lists the running for these two operations are ($x,y$ are the lists lengths):</p>\n\n<ul>\n<li>Addition: Time $O(x+y)$, space $O(x+y)$</li>\n<li>Multiplication: Time $O(xy \\log(xy))$, space $O(xy)$</li>\n</ul>\n\n<p>Can someone help me to find the running times of my algorithms?\nI think for the first algorithm it is like stated above $O(x+y)$, for the second one I have two nested loops and two lists so it should be $O(xy)$, but why the $O(xy \\log(xy))$ above?</p>\n\n<p>These are the algorithms I developed (in Pseudocode):</p>\n\n<pre><code> PolynomialAdd(Poly1, Poly2):\n Degree := MaxDegree(Poly1.head, Poly2.head);\n while (Degree &gt;=0) do:\n      Node1 := Poly1.head;\n      while (Node1 IS NOT NIL) do:\n         if(Node1.Deg = Degree) then break;\n         else Node1 = Node1.next;\n      Node2 := Poly2.head;\n      while (Node2 IS NOT NIL) do:\n         if(Node2.Deg = Degree) then break;\n         else Node2 = Node2.next;\n      if (Node1 IS NOT NIL AND Node2 IS NOT NIL) then\n         PolyResult.insertTerm( Node1.Coeff + Node2.Coeff, Node1.Deg);\n      else if (Node1 IS NOT NIL) then\n         PolyResult.insertTerm(Node1.Coeff, Node1.Deg);\n      else if (Node2 IS NOT NIL) then\n         PolyResult.insertTerm(Node2.Coeff, Node2.Deg);\n      Degree := Degree \u2013 1;\n return PolyResult; \n\n PolynomialMul(Poly1, Poly2): \n Node1 := Poly1.head;\n while (Node1 IS NOT NIL) do:\n      Node2 = Poly2.head;\n      while (Node2 IS NOT NIL) do:\n           PolyResult.insertTerm(Node1.Coeff * Node2.Coeff, \n                              Node1.Deg + Node1.Deg);\n           Node2 = Node2.next;                 \n      Node1 = Node1.next;\n return PolyResult;\n</code></pre>\n\n<p><code>InsertTerm</code> inserts the term in the correct place depending on the degree of the term. </p>\n\n<pre><code> InsertTerm(Coeff, Deg):\n NewNode.Coeff := Coeff;\n NewNode.Deg := Deg;\n if List.head = NIL then\n    List.head := NewNode;\n else if NewNode.Deg &gt; List.head.Deg then\n    NewNode.next := List.head;\n    List.head := NewNode;\n else if NewNode.Deg = List.head.Deg then \n    AddCoeff(NewNode, List.head);\n else\n    Go through the List till find the same Degree and summing up the coefficient OR\n    adding a new Term in the right position if Degree not present;\n</code></pre>\n', 'ViewCount': '1224', 'Title': 'Running time - Linked Lists Polynomial', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-09T09:08:38.773', 'LastEditDate': '2012-05-09T09:04:04.817', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '1584', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-04-29T10:01:54.643', 'Id': '1567''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I applied the Master theorem to a recurrence for a running time I encountered (this is a simplified version):</p>\n\n<p>$$T(n)=4T(n/2)+O(r)$$</p>\n\n<p>$r$ is independent of $n$. Case 1 of the Master theorem applies and tells us that $T(n)=O(n^2)$.</p>\n\n<p>However, this hides a constant dependent on $r$ in the big-oh notation: our recurrence has depth $O(\\log_2 n)$ so at the final level we have $O(4^{\\log_2 n})=O(n^2)$ subproblems, each of which takes $O(r)$ time to be handled. This means the actual running time is $O(n^2 r)$ (or worse: this analysis only talks about the lowest level).</p>\n\n<p>This is my actual recursion:</p>\n\n<p>$$T(n)=r^2T(n/r)+O(nr^2)$$</p>\n\n<p>Is there a method similar to the Master theorem for these kinds of recursions?</p>\n', 'ViewCount': '227', 'Title': 'Master theorem and constants independent of $n$', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-02T13:24:14.520', 'LastEditDate': '2013-02-02T13:24:14.520', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation><mathematical-analysis><master-theorem>', 'CreationDate': '2012-04-29T17:39:46.100', 'FavoriteCount': '2', 'Id': '1576''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1389', 'Title': 'How can we assume that basic operations on numbers take constant time?', 'LastEditDate': '2013-09-10T22:18:05.507', 'AnswerCount': '6', 'Score': '31', 'OwnerDisplayName': 'user742', 'PostTypeId': '1', 'FavoriteCount': '15', 'Body': "<p>Normally in algorithms we do not care about comparison, addition, or subtraction of numbers -- we assume they run in time $O(1)$.  For example, we assume this when we say that comparison-based sorting is $O(n\\log n)$, but when numbers are too big to fit into registers, we normally represent them as arrays so basic operations require extra calculations per element.</p>\n\n<p>Is there a proof showing that comparison of two numbers (or other primitive arithmetic functions) can be done in $O(1)$? If not why are we saying that comparison based sorting is $O(n\\log n)$?</p>\n\n<hr>\n\n<p><em>I encountered this problem when I answered a SO question and I realized that my algorithm is not $O(n)$ because sooner or later I should deal with big-int, also it wasn't pseudo polynomial time algorithm, it was $P$.</em></p>\n", 'Tags': '<algorithms><complexity-theory><algorithm-analysis><time-complexity><reference-question>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T22:18:05.507', 'CommentCount': '2', 'AcceptedAnswerId': '1661', 'CreationDate': '2012-05-03T00:06:31.453', 'Id': '1643''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '234', 'Title': 'Recursion for runtime of divide and conquer algorithms', 'LastEditDate': '2012-05-10T16:31:21.383', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': "<p>A divide and conquer algorithm's work at a specific level can be simplified into the equation:</p>\n\n<p>$\\qquad \\displaystyle O\\left(n^d\\right) \\cdot \\left(\\frac{a}{b^d}\\right)^k$</p>\n\n<p>where $n$ is the size of the problem, $a$ is the number of sub problems, $b$ is the factor the size of the problem is broken down by at each recursion, $k$ is the level, and $d$ is the exponent for Big O notation (linear, exponential etc.).</p>\n\n<p>The book claims  if the ratio is greater than one the sum of work is given by the last term on the last level, but if it is less than one the sum of work is given by the first term of the first level. Could someone explain why this is true?</p>\n", 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><recursion><mathematical-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T16:31:21.383', 'CommentCount': '0', 'AcceptedAnswerId': '1746', 'CreationDate': '2012-05-09T03:13:56.910', 'Id': '1745''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1427', 'Title': 'Quicksort to find median?', 'LastEditDate': '2012-05-14T15:04:59.000', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': '<p>Why is the worst scenario $\\mathcal{O}\\left(n^2\\right)$ when using quicksort to find the median of a set of numbers?</p>\n\n<ul>\n<li><p>If your algorithm continually picks a number larger than or smaller than <em>all</em> numbers in the list wouldn\'t your algorithm fail? For example if the list of numbers are:</p>\n\n<p>$S = (12,75,82,34,55,15,51)$</p>\n\n<p>and you keep picking numbers greater than $82$ or less than $12$ to create sublists with, wouldn\'t your set   always remain the same size?</p></li>\n<li><p>If your algorithm continually picks a number that creates sublists of $1$ why is the worst case scenario $\\mathcal{O}\\left(n^2\\right)$? Wouldn\'t efficiency be linear considering that according to the <a href="http://en.wikipedia.org/wiki/Master_theorem" rel="nofollow">Master Theorem</a>,  $d&gt;\\log_b a$?* (and therefore be $\\mathcal{O}\\left(n^d\\right)$ or specifically in this case $\\mathcal{O}\\left(n\\right)$)</p></li>\n</ul>\n\n<p>*Where $d$ is the efficiency exponent (i.e. linear, exponential etc.), $b$ is the factor the size of problem is reduced by at each iteration, $a$ is the number of subproblems and $k$ is the level. Full ratio: $T(n) = \\mathcal{O}\\left(n^d\\right) * (\\frac{a}{b^d})^k$</p>\n', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-14T15:06:04.740', 'CommentCount': '0', 'AcceptedAnswerId': '1791', 'CreationDate': '2012-05-11T01:27:39.883', 'Id': '1789''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '677', 'Title': 'Recurrences and Generating Functions in Algorithms', 'LastEditDate': '2012-09-22T18:14:36.253', 'AnswerCount': '4', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '5', 'Body': '<p>Combinatorics plays an important role in computer science. We frequently utilize combinatorial methods in both analysis as well as design in algorithms. For example one method for finding a $k$-vertex cover set in a graph might just inspect all $\\binom{n}{k}$ possible subsets. While the binomial functions grows exponentially, if $k$ is some fixed constant we end up with a polynomial time algorithm by asymptotic analysis.</p>\n\n<p>Often times real-life problems require more complex combinatorial mechanisms which we may define in terms of recurrences. One famous example is the <a href="http://en.wikipedia.org/wiki/Fibonacci_number" rel="nofollow">fibonacci sequence</a> (naively) defined as:</p>\n\n<p>$f(n) = \\begin{cases}\n   1 &amp; \\text{if } n = 1 \\\\\n   0 &amp; \\text{if } n = 0 \\\\\n   f(n-1) + f(n-2) &amp; \\text{otherwise}\n  \\end{cases}\n$</p>\n\n<p>Now computing the value of the $n$th term grows exponentially using this recurrence, but thanks to dynamic programming, we may compute it in linear time. Now, not all recurrences lend themselves to DP (off hand, the factorial function), but it is a potentially exploitable property when defining some count as a recurrence rather than a generating function.</p>\n\n<p>Generating functions are an elegant way to formalize some count for a given structure. Perhaps the most famous is the binomial generating function defined as:</p>\n\n<p>$(x + y)^\\alpha = \\sum_{k=0}^\\infty \\binom{\\alpha}{k}x^{\\alpha - k}y^k$</p>\n\n<p>Luckily this has a closed form solution. Not all generating functions permit such a compact description. </p>\n\n<blockquote>\n  <p>Now my question is this: how often are generating functions used in <em>design</em> of algorithms? It is easy to see how they may be exploited to understand the rate of growth required by an algorithm via analysis, but what can they tell us about a problem when creating a method to solve some problem?</p>\n</blockquote>\n\n<p>If many times the same count may be reformulated as a recurrence it may lend itself to dynamic programming, but again perhaps the same generating function has a closed form. So it is not so evenly cut.</p>\n', 'Tags': '<algorithms><algorithm-analysis><combinatorics>', 'LastEditorUserId': '699', 'LastActivityDate': '2013-01-24T03:59:38.067', 'CommentCount': '9', 'AcceptedAnswerId': '1937', 'CreationDate': '2012-05-18T17:13:56.063', 'Id': '1913''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '370', 'Title': 'How to go from a recurrence relation to a final complexity', 'LastEditDate': '2012-05-22T17:52:22.850', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1562', 'FavoriteCount': '1', 'Body': u'<p>I have an algorithm, shown below, that I need to analyze. Because it\'s recursive in nature I set up a recurrence relation.  </p>\n\n<pre><code>//Input: Adjacency matrix A[1..n, 1..n]) of an undirected graph G  \n//Output: 1 (true) if G is complete and 0 (false) otherwise  \nGraphComplete(A[1..n, 1..n]) {\n  if ( n = 1 )\n    return 1 //one-vertex graph is complete by definition  \n  else  \n    if not GraphComplete(A[0..n \u2212 1, 0..n \u2212 1]) \n      return 0  \n    else \n      for ( j \u2190 1 to n \u2212 1 ) do  \n        if ( A[n, j] = 0 ) \n          return 0  \n      end\n      return 1\n}\n</code></pre>\n\n<p>Here is what I believe is a valid and correct recurrence relation:  </p>\n\n<p>$\\qquad \\begin{align}\r\n  T(1) &amp;= 0 \\\\\r\n  T(n) &amp;= T(n-1) + n - 1 \\quad \\text{for } n \\geq 2\r\n\\end{align}$</p>\n\n<p>The "$n - 1$" is how many times the body of the for loop, specifically the "if A[n,j]=0" check, is executed.</p>\n\n<p>The problem is, where do I go from here? How do I convert the above into something that actually shows what the resulting complexity is?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-22T17:52:22.850', 'CommentCount': '1', 'AcceptedAnswerId': '1960', 'CreationDate': '2012-05-20T21:24:29.637', 'Id': '1959''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>For a given undirected graph $G$, a <a href="http://en.wikipedia.org/wiki/Gomory%E2%80%93Hu_tree" rel="nofollow">Gomory-Hu tree</a> is a graph which has the same nodes as $G$, but its edges represent the minimal cut between each pair of nodes in $G$. The Gomory-Hu algorithm finds such a tree for a graph. A representant pair of nodes is defined as follows: if $R$ and $S$ are two components of the Gomory-Hu tree, and there is an edge $e$ between them, then the nodes $r \\in R$ and $s \\in S$ are representants if the weight of the edge $(r,s)$ is the same as the weight of $e$. </p>\n\n<p>I have to learn not only the algorithm, but also all the lemmas needed to prove that it works. For this specific lemma, there is a proof given in my learning materials, but I am afraid I don\'t understand how it works. </p>\n\n<p>It starts by picking two components of the Gomory-Hu tree, $A$ and $B$, with an edge $h$ between them, $a \\in A$ and $b \\in B$ being the representants. In the next iteration, nodes $x$ and $y$ in $A$ are picked, and a new minimal $(x,y)$-cut is calculated (dividing $A$ into the subsets $X$ and $Y$), such that now $h$ connects $X$ and $B$. If $a \\in X$, then $a$ and $b$ are still representants. But if $a \\in Y$, the proof claims that $x$ and $b$ are the new representants of $h$. </p>\n\n<p>For this, it states that </p>\n\n<blockquote>\n  <p>The cut which created $h$ divides $x$ and $b$. From that, it follows that $f(x,b) \\le f(a,b)$. </p>\n</blockquote>\n\n<p>[It uses $f(a,b)$ to denote the flow in the minimal cut between nodes $a$ and $b$.] Then it goes on to prove that also $f(x,b) \\ge f(a,b)$. And then the two flows must be equal, so the flow between $x$ and $b$ is the same as the flow in the minimal $(a,b)$-cut, so $x$ and $b$ are representants. </p>\n\n<p>But as I understand the algorithm, the cut which created $h$ was a minimal cut between the nodes $a$ and $b$. The node $x$ wasn\'t even a special node at the time the graph was divided into components $A$ and B$.$ Yes, this cut happens to divide $x$ and $b$ too, but there is no guarantee that it is the minimal cut between $x$ and $b$ (this is exactly what we are trying to prove here). So I think that we can follow that $f(x,b) \\ge f(a,b)$, but not that $f(x,b) \\le f(a,b)$. I suspect that there is an error in my reasoning and not in the reasoning of the prof who wrote the learning materials, but where is it? </p>\n\n<p>And if there actually is an error in this proof, what is the correct proof? </p>\n', 'ViewCount': '79', 'Title': 'What is the proof for the lemma "For every iteration of the Gomory-Hu algorithm, there is a representant pair for each edge"?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-02T04:53:12.310', 'LastEditDate': '2012-06-02T04:53:12.310', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1717', 'Tags': '<algorithms><graph-theory><algorithm-analysis>', 'CreationDate': '2012-06-01T15:45:55.923', 'Id': '2189''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>We want to solve a minimal-cost-flow problem with a generic negative-cycle cancelling algorithm. That is, we start with a random valid flow, and then we do not pick any "good" negative cycles such as minimal average cost cycles, but use Bellman-Ford to discover a minimal cycle and augment along the discovered cycle. Let $V$ be the number of nodes in the graph, $A$ the number of edges, $U$ the maximal capacity of an edge in the graph, and $W$ the maximal costs of an edge in the graph. Then, my learning materials claim: </p>\n\n<ul>\n<li>The maximal costs at the beginning can be no more than $AUW$ </li>\n<li>The augmentation along one negative cycle reduces the costs by at least one unit </li>\n<li>The lower bound for the minimal costs is 0, because we don\'t allow negative costs </li>\n<li>Each negative cycle can be found in $O(VA)$ </li>\n</ul>\n\n<p>And they follow from it that the algorithm\'s complexity is $O(V\xb2AUW)$. I understand the logic behind each of the claims, but think that the complexity is different. Specifically, the maximal number of augmentations is given by one unit of flow per augmentation, taking the costs from $AUW$ to zero, giving us a maximum of $AUW$ augmentations. We need to discover a negative cycle for each, so we multiply the maximal number of augmentations by the time needed to discover a cycle ($VA$) and arrive at $O(A\xb2VUW)$ for the algorithm. </p>\n\n<p>Could this be an error in the learning materials (this is a text provided by the professor, not a student\'s notes from the course), or is my logic wrong? </p>\n', 'ViewCount': '258', 'Title': u'Why is the complexity of negative-cycle-cancelling $O(V\xb2AUW)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-06T00:47:27.197', 'LastEditDate': '2012-06-10T11:42:45.650', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1717', 'Tags': '<algorithms><graph-theory><algorithm-analysis><runtime-analysis><network-flow>', 'CreationDate': '2012-06-08T13:26:38.437', 'FavoriteCount': '1', 'Id': '2283''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Let $f$ and $g$ be two functions and $p$ a number. Consider the following program:</p>\n\n<pre><code>Recurs(v,p) :\n  find s &lt; v such that f(s,v) &lt; v/2 and g(s,v-s) &lt; p\n\n  if no such s exists then\n    return v\n  else if s &lt;= v/4 then \n    return v-s U Recurs(s,p)\n  else if s &gt; v/4 then \n    return Recurs(s,p) U Recurs(v-s,p)\nend\n</code></pre>\n\n<p>Can the recurrence for the running time of this recursion be $T(v)=T\\left(\\frac{v}{4}\\right)+T\\left(\\frac{3v}{4}\\right)+1$?</p>\n', 'ViewCount': '154', 'Title': "Is the following recurrence for this program's runtime correct?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-11T14:11:29.740', 'LastEditDate': '2012-06-11T10:38:39.823', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '3', 'OwnerDisplayName': 'raarava', 'PostTypeId': '1', 'OwnerUserId': '1818', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2012-06-04T03:17:56.457', 'Id': '2295''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In the book <a href="http://www.cs.uu.nl/geobook/">"Computational Geometry: Algorithms and Applications"</a> by Mark de Berg et al., there is a very simple brute force algorithm for computing Delaunay triangulations. The algorithm uses the notion of <em>illegal edges</em> -- edges that may not appear in a valid Delaunay triangulation and have to be replaced by some other edges. On each step, the algorithm just finds these illegal edges and performs required displacements (called <em>edge flips</em>) till there are no illegal edges.</p>\n\n<blockquote>\n  <p>Algorithm <strong>LegalTriangulation</strong>($T$)</p>\n  \n  <p><em>Input</em>. Some triangulation $T$ of a point set $P$.<br>\n  <em>Output</em>. A legal triangulation of $P$.</p>\n  \n  <p><strong>while</strong> $T$ contains an illegal edge $p_ip_j$<br>\n  <strong>do</strong><br>\n  $\\quad$ Let $p_i p_j p_k$ and $p_i p_j p_l$ be the two triangles adjacent to $p_ip_j$.<br>\n  $\\quad$ Remove $p_ip_j$ from $T$, and add $p_kp_l$ instead.<br/>\n  <strong>return</strong> $T$.</p>\n</blockquote>\n\n<p>I\'ve heard that this algorithm runs in $O(n^2)$ time in worst case; however, it is not clear to me whether this statement is correct or not. If yes, how can one prove this upper bound?</p>\n', 'ViewCount': '769', 'Title': 'Brute force Delaunay triangulation algorithm complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-21T13:44:11.817', 'LastEditDate': '2012-06-17T13:28:32.350', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><time-complexity><algorithm-analysis><computational-geometry><runtime-analysis>', 'CreationDate': '2012-06-16T22:01:33.543', 'FavoriteCount': '2', 'Id': '2400''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>This is an example in my lecture notes.\nIs this function with time complexity $O(n \\log n)$?.\nBecause the worst case is the funtion goes into <code>else</code> branch, and 2 nested loops with time complexity of $\\log n$ and $n$, so it is $O(n \\log n)$. Am I right?</p>\n\n<pre><code>int j = 3;\nint k = j * n / 345;\nif(k &gt; 100){\n    System.out.println("k: " + k);\n}else{\n    for(int i=1; i&lt;n; i*=2){\n        for(int j=0; j&lt;i; j++){\n            k++;\n        }\n    }\n}\n</code></pre>\n', 'ViewCount': '557', 'Title': 'What is the time complexity of this function?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-26T23:13:31.913', 'LastEditDate': '2012-06-25T17:21:54.770', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '2497', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Tags': '<complexity-theory><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-06-25T14:48:54.900', 'Id': '2487''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Given an array $A$ of $N$ integers, each element in the array can be increased by a fixed number $b$ with some probability $p[i]$, $0 \\leq i &lt; n$. I have to find the expected number of swaps that will take place to sort the array using <a href="http://en.wikipedia.org/wiki/Bubble_sort" rel="nofollow">bubble sort</a>.</p>\n\n<p>I\'ve tried the following:</p>\n\n<ol>\n<li><p>The probability for an element $A[i] &gt; A[j]$ for $i &lt; j$ can be calculated easily from the given probabilities.</p></li>\n<li><p>Using the above, I have calculated the expected number of swaps as:</p>\n\n<pre><code>double ans = 0.0;\nfor ( int i = 0; i &lt; N-1; i++ ){\n    for ( int j = i+1; j &lt; N; j++ ) {\n        ans += get_prob(A[i], A[j]); // Computes the probability of A[i]&gt;A[j] for i &lt; j.\n</code></pre></li>\n</ol>\n\n<p>Basically I came to this idea because the expected number of swaps can be calculated by the number of inversions of the array. So by making use of given probability I am calculating whether a number $A[i]$ will be swapped with a number $A[j]$.</p>\n\n<p>Note that the initial array elements can be in any order, sorted or unsorted. Then each number can change with some probability. After this I have to calculate the expected number of swaps.</p>\n\n<p>I have posted <a href="http://stackoverflow.com/questions/11331314/number-of-swaps-in-bubble-sort">a similar question</a> before but it did not had all the constraints.</p>\n\n<p>I did not get any good hints on whether I am even on the right track or not, so I listed all the constraints here. Please give me some hints if I am thinking of the problem in an incorrect way.</p>\n', 'ViewCount': '2139', 'Title': 'Expected number of swaps in bubble sort', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:48:38.960', 'LastEditDate': '2012-07-09T08:40:49.420', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '9', 'OwnerDisplayName': 'TheRock', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><sorting><average-case>', 'CreationDate': '2012-07-05T08:44:10.770', 'FavoriteCount': '3', 'Id': '2630''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to understand the approach for this problem:</p>\n\n<blockquote>\n  <p>"If all streets are one way, there is still a legal way to drive from\n  one intersection to another"</p>\n</blockquote>\n\n<p>The question is to prove that it can be done in linear time. I am not looking for direct answers but the approach to this problem.</p>\n\n<p>How can I think about this problem in terms of graph theory? AFAI understand, this will result in a DAG. But then should I choose BFS or DFS and why to prove it? (both are liner time algos)</p>\n', 'ViewCount': '122', 'Title': 'Existence of a route following one-way streets', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-23T23:19:37.897', 'LastEditDate': '2012-07-17T05:56:03.863', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '271', 'Tags': '<algorithms><graphs><algorithm-analysis>', 'CreationDate': '2012-07-10T17:53:32.663', 'Id': '2674''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p><a href="http://en.wikipedia.org/wiki/Bor%C5%AFvka%27s_algorithm">Bor\u016fvka\'s algorithm</a> is one of the standard algorithms for calculating the minimum spanning tree for a graph $G = (V,E)$, with $|V| = n, |E| = m$.</p>\n\n<p>The pseudo-code is:</p>\n\n<pre><code>MST T = empty tree\nBegin with each vertex as a component\nWhile number of components &gt; 1\n    For each component c\n       let e = minimum edge out of component c\n       if e is not in T\n           add e to T  //merging the two components connected by e\n</code></pre>\n\n<p>We call each iteration of the outer loop a round. In each round, the inner loop cuts the number of components at least in half. Therefore there are at most $O(\\log n)$ rounds. In each round, the inner loop looks at each edge at most twice (once from each component). Therefore the running time is at most $O(m \\log n)$.</p>\n\n<p>Now suppose after each round, we remove all the edges which only connect vertices within the same component and also remove duplicate edges between components, so that the inner loop only looks at some number of edges m\' &lt; m which are the minimum weight edges which connect two previously disconnected components. </p>\n\n<p><strong>How does this optimization affect the running time?</strong></p>\n\n<p>If we somehow knew that in each round, it would cut the number of edges in half, then the running time would be significantly improved:\n$T(m) = T(m /2) + O(m) = O(m)$.</p>\n\n<p>However, while the optimization will dramatically reduce the number of edges examined, (only 1 edge by the final round, and at most # of components choose 2 in general), it\'s not clear how/if we can use this fact to tighten the analysis of the run-time. </p>\n', 'ViewCount': '374', 'Title': u"Tighter analysis of modified Bor\u016fvka's algorithm", 'LastActivityDate': '2012-07-20T08:15:16.420', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '2829', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<algorithms><algorithm-analysis><spanning-trees>', 'CreationDate': '2012-07-18T18:53:31.420', 'FavoriteCount': '3', 'Id': '2816''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '175', 'Title': 'Bound on space for selection algorithm?', 'LastEditDate': '2012-07-24T08:41:54.167', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '67', 'FavoriteCount': '2', 'Body': '<p>There is a well known worst case $O(n)$ <a href="http://en.wikipedia.org/wiki/Selection_algorithm" rel="nofollow">selection algorithm</a> to find the $k$\'th largest element in an array of integers.  It uses a <a href="http://en.wikipedia.org/wiki/Selection_algorithm#Properties_of_pivot" rel="nofollow">median-of-medians</a> approach to find a good enough pivot, partitions the input array in place and then recursively continues in it\'s search for the $k$\'th largest element.</p>\n\n<p>What if we weren\'t allowed to touch the input array, how much extra space would be needed in order to find the $k$\'th largest element in $O(n)$ time?  Could we find the $k$\'th largest element in $O(1)$ extra space and still keep the runtime $O(n)$?  For example, finding the maximum or minimum element takes $O(n)$ time and $O(1)$ space.  </p>\n\n<p>Intuitively, I cannot imagine that we could do better than $O(n)$ space but is there a proof of this?</p>\n\n<p>Can someone point to a reference or come up with an argument why the $\\lfloor n/2 \\rfloor$\'th element would require $O(n)$ space to be found in $O(n)$ time?</p>\n', 'Tags': '<algorithms><algorithm-analysis><space-complexity><lower-bounds>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-26T18:36:48.300', 'CommentCount': '1', 'AcceptedAnswerId': '2896', 'CreationDate': '2012-07-24T03:19:59.790', 'Id': '2893''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<h3>Background</h3>\n\n<p>$\\newcommand\\ldotd{\\mathinner{..}}$Last month, I heard about a new linear-time algorithm to determine the <a href="https://en.wikipedia.org/wiki/Longest_palindromic_substring" rel="nofollow">longest palindromic substring</a> called Jeuring\'s algorithm. It seemed interesting, therefore I had a try to analyze the algorithm. It\'s not too difficult to show that it\'s an $\\Theta(n)$-time-algorithm, but I want to observe more closely.</p>\n\n<p>I wrote a C++ implementation, and an essay to prove and analyze the algorithm. It\'s somewhat long, therefore I will only post the critical part of the code, where you can learn the algorithm by heart, see the succeeding section. You can get all my previous work from <a href="https://github.com/FrankEular/pldrm" rel="nofollow">github</a>. pldrm.nw is the literate programming source file, which produces the pldrm.cc and pldrm.pdf. pldrm.pdf is the essay I\'ve written.</p>\n\n<h3>Problem</h3>\n\n<p>Let $A=$ the number of times <em>goto</em> statement is executed in <em>move loop</em>, and $B=$ the number of times <em>move loop</em> is executed where <em>goto</em> statement is <strong>NOT</strong> executed, $C=$ the number of times a[++j]=min(a[p],l) is executed in <em>move loop</em>. You can see the code in the following section.\nI found that $A+B+C=2n$ and $B=\\sum_{k=2}^{n+1}[b_k=1]$, where $b_k$ is the length of longest tail palindrome of $s[0\\ldotd k]$, and $[P]$ is <a href="http://en.wikipedia.org/wiki/Iverson_bracket" rel="nofollow">Iverson bracket</a>. For details, you can read my pdf from <a href="https://github.com/FrankEular/pldrm" rel="nofollow">github</a>.\nI\'m looking for somebody to help me analyze the quantity $A,C$. Any help? Thanks!</p>\n\n<h3>The critical part of the algorithm</h3>\n\n<p>Given that $s[1\\ldotd n]$ is the string inputted, and $n$ is the length of $s$, where $s[0]=1,s[n+1]=0$.\nWe say $l+r$ is the center of substring $s[l\\ldotd r]$. For example, $4$ is the center of $s[2\\ldotd2]$ or $s[1\\ldotd3]$.\n$a_k$ is the length of the longest palindrome whose center is $k$, and $a[0\\ldotd2n]$ is the array to save $\\langle a_k\\rangle$.\n<strong>The algorithm is used to determine $a_k$.</strong>\nWe call some string $A$ is a tail palindrome of the other string $B$ if and only if $A$ is a palindromic tail substring of $B$. For example, $A=aba$ and $B=aaaababa$, where $A$ is palindromic and $A$ is a tail substring of $B$.</p>\n\n<p>Here\'s the critical part of the code:</p>\n\n<pre><code>&lt;&lt;main loop&gt;&gt;=\nj = 1;\nl = 1;\na[0] = 1;\na[1] = 0;\nfor (int k=2; k&lt;=n+1; k++) {\n  &lt;&lt;process&gt;&gt;\n  advance:\n  ;\n}\n@\n</code></pre>\n\n<p>Process is made up of an infinite loop, which is used to find the longest tail palindrome of $s[0\\ldotd k]$. There are two exits of it. One is in extension subroutine, while the other one is in move loop. The way of exit is <em>goto advance;</em>.</p>\n\n<pre><code>&lt;&lt;process&gt;&gt;=\nfor (;;) {\n  &lt;&lt;check&gt;&gt;\n  &lt;&lt;move loop&gt;&gt;\n}\n@\n</code></pre>\n\n<p>The check subroutine checks whether a tail palindrome of $s[0\\ldotd k-1]$ could be extended to that of $s[0\\ldotd k]$. If so, exit from the process loop and advance $k$, otherwise start the move loop.</p>\n\n<pre><code>&lt;&lt;check&gt;&gt;=\nif (s[k] == s[k-l-1]) {\n  l += 2;\n  goto advance;\n}\n@\n</code></pre>\n\n<p>Here\'s the move loop, which looks short and easy. It\'s used to find a shorter tail palindrome of $s[0\\ldotd k-1]$.</p>\n\n<pre><code>&lt;&lt;move loop&gt;&gt;=\na[++j] = l;\nfor (p=j-1; --l&gt;=0&amp;&amp;l!=a[p]; p--) {\n  a[++j] = min(a[p], l);\n}\nif (l &lt; 0) {\n  l = 1;\n  goto advance;\n}\n@\n</code></pre>\n', 'ViewCount': '510', 'Title': 'Analysis of a linear-time algorithm for longest palindromic substring', 'LastEditorUserId': '29', 'LastActivityDate': '2012-08-12T13:37:49.487', 'LastEditDate': '2012-08-12T13:37:49.487', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1715', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-07-28T04:29:45.943', 'Id': '2936''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have trouble understanding how to calculate the depth of a sorting network on $n$ inputs.</p>\n\n<p>For example, in case of selection sort, we have:</p>\n\n<p>$\\qquad \\displaystyle D(n)=D(n-1)+2\\\\\\qquad D(2)=1$</p>\n\n<p>which leads to</p>\n\n<p>$\\qquad \\displaystyle D(n)=2n-3=\\Theta(n)$</p>\n\n<p>I have confirmed that the depth of selection sort is equal to $2n-3$ by hand, but I can't understand how the recurrence $D(n)=D(n-1)+2$ is derived.</p>\n", 'ViewCount': '292', 'Title': 'How to calculate the depth of sorting networks?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-03T04:32:08.207', 'LastEditDate': '2012-07-30T07:08:14.057', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'Steven', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation><sorting>', 'CreationDate': '2012-07-29T11:18:33.773', 'FavoriteCount': '1', 'Id': '2950''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'ve just begun this stage 2 Compsci paper on algorithms, and stuff like this is not my strong point. I\'ve come across this in my lecture slides.</p>\n\n<pre><code>int length = input.length();\nfor (int i = 0; i &lt; length - 1; i++) {\n    for (int j = i + 1; j &lt; length; j++) {\n        System.out.println(input.substring(i,j));\n    }\n}\n</code></pre>\n\n<p>"In each iteration, the outer loop executes $\\frac{n^{2}-(2i-1)n-i+i^{2}}{2}$ operations from the inner loop for $i = 0, \\ldots, n-1$."</p>\n\n<p>Can someone please explain this to me step by step?</p>\n\n<p>I believe the formula above was obtained by using Gauss\' formula for adding numbers... I think...</p>\n', 'ViewCount': '1969', 'Title': 'Time complexity formula of nested loops', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-02T08:36:54.223', 'LastEditDate': '2012-08-02T08:36:54.223', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '2996', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-02T04:56:03.523', 'Id': '2994''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>Of large sparse biparite graphs (say degree 4) with N verticies, roughly speaking, which of them cause the worst case running time of the <a href="https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm" rel="nofollow">Hopcroft-Karp algorithm</a>? What is their general structure and architecture, and why does it cause a problem?</p>\n\n<p>Further, in many implementations the DFS part is implemented using recursion, eg from Wikipedia:</p>\n\n<pre><code>function DFS (v)\n    if v != NIL\n        for each u in Adj[v]\n            if Dist[ Pair_G2[u] ] == Dist[v] + 1\n                if DFS(Pair_G2[u]) == true\n                    Pair_G2[u] = v\n                    Pair_G1[v] = u\n                    return true\n        Dist[v] = \u221e\n        return false\n    return true\n</code></pre>\n\n<p>What is the approximate maximum depth of the recursion in the worst case?</p>\n', 'ViewCount': '227', 'Title': 'Worst-case sparse graphs for Hopcroft-Karp Algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-08T06:54:35.197', 'LastEditDate': '2012-08-08T06:54:35.197', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1577', 'Tags': '<algorithms><graph-theory><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-07T05:31:16.787', 'Id': '3064''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I notice that in a few CS research papers, to compare the efficiency of two algorithms, the total number of key comparison in the algorithms is used rather than the real computing times themselves. Why can't we compare which one is better by running both programs and counting the total time needed to run the algorithms? </p>\n", 'ViewCount': '310', 'Title': 'Why use comparisons instead of runtime for comparing two algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-12T01:53:58.833', 'LastEditDate': '2012-08-12T00:14:15.607', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2460', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-11T20:41:56.243', 'FavoriteCount': '3', 'Id': '3126''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '233', 'Title': 'Good text on algorithm complexity', 'LastEditDate': '2012-08-16T15:01:05.350', 'AnswerCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2530', 'FavoriteCount': '1', 'Body': '<p>Where should I look for a good introductory text in algorithm complexity? So far, I have had an Algorithms class, and several language classes, but nothing with a theoretical backbone. I get the whole complexity, but sometimes it\'s hard for me to differentiate between O(1) and O(n) plus there\'s the whole theta notation and all that, basic explanation of P=NP and simple algorithms, tractability. I want a text that covers all that, and that doesn\'t require a heavy mathematical background, or something that can be read through.</p>\n\n<p>LE: I\'m still in highschool, not in University, and by heavy mathematical background I mean something perhaps not very high above Calculus and Linear Algebra (it\'s not that I can\'t understand it, it\'s the fact that for example learning Taylor series without having done Calculus I is a bit of a stretch; that\'s what I meant by not mathematically heavy. Something in which the math, with a normal amount of effort can be understood). And, do pardon if I\'m wrong, but theoretically speaking, a class at which they teach algorithm design methods and actual algorithms should be called an "Algorithms" class, don\'t you think?\nIn terms of my current understanding, infinite series, limits and integrals I know (most of the complexity books I\'ve glanced at seemed to use those concepts), but you\'ve lost me at the Fast Fourier Transform.</p>\n', 'Tags': '<complexity-theory><reference-request><algorithm-analysis><education><books>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-08-16T15:37:01.093', 'CommentCount': '1', 'AcceptedAnswerId': '3222', 'CreationDate': '2012-08-15T18:15:23.667', 'Id': '3201''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p><strong>Updated Algorithm:</strong> There was a major flaw in my original presentation of the algorithm which could have impacted the results. I apologize for the same. The correction has been posted underneath.\n<hr></p>\n\n<p>The original algorithm posted had a major flaw in its working. I tried my best but could not get the desired accuracy in presenting the algorithm in pseudo code and/or Set Theory notation. I am thus posting python code, which has been tested and produces the desired results.</p>\n\n<p>Note that my question, however, remains the same: What is the time complexity of the algorithm (assuming that powersets are already generated)?</p>\n\n<pre><code># mps is a set of powersets; below is a sample (test case)\nmps = [\n        [       [], [1], [2], [1,2]     ],\n        [       [], [3], [4], [3,4]     ],\n        [       [], [5], [6], [5,6]     ]\n      ]\n\n\n# Core algorithm\n# enumerate(mps) may not be required in languages like C which support indexed loops\nlen = mps.__len__()\nfor idx, ps in enumerate(mps):\n    if idx &gt; len - 2:\n            break;\n    mps[idx + 1] = merge(mps[idx], mps[idx+1])   # merge is defined below\n\n\n# Takes two powersets and merges them\ndef merge (psa, psb):\n    fs = []\n    for a in psa:\n            for b in psb:\n                    fs.append(list(set(a) | set(b)))\n    return fs\n</code></pre>\n\n<p><strong>Output</strong>: <code>mps[-1]   #Last item of the list</code></p>\n\n<p>Running the above example will result in listing out the powerset of $\\{1,2,3,4,5,6\\}$.</p>\n', 'ViewCount': '154', 'Title': 'What is the complexity of this subset merge algorithm?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-23T09:30:36.730', 'LastEditDate': '2012-08-23T09:30:36.730', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2597', 'Tags': '<algorithms><time-complexity><algorithm-analysis><check-my-algorithm>', 'CreationDate': '2012-08-21T15:00:26.797', 'Id': '3273''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Please consider the following triple-nested loop:</p>\n\n<pre><code>for (int i = 1; i &lt;= n; ++i)\n    for (int j = i; j &lt;= n; ++j)\n        for (int k = j; k &lt;= n; ++k)\n            // statement\n</code></pre>\n\n<p>The statement here is executed exactly $n(n+1)(n+2)\\over6$ times. Could someone please explain how this formula was obtained? Thank you.</p>\n', 'ViewCount': '3201', 'Title': 'Time complexity of a triple-nested loop', 'LastEditorUserId': '41', 'LastActivityDate': '2012-08-24T22:01:32.450', 'LastEditDate': '2012-08-24T14:24:26.183', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2620', 'Tags': '<algorithms><time-complexity><algorithm-analysis>', 'CreationDate': '2012-08-24T02:42:49.667', 'FavoriteCount': '5', 'Id': '3306''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>This is a problem from CLRS 23-2 that I'm trying to solve. The problem assumes that given graph G is very sparse connected. It wants to improve further over Prim's algorithm $O(E + V \\lg V)$. The idea is to contract the graph, i.e. collapse two or more nodes into one node. So each reduction will reduce the graph by at least half nodes. The question is to come up with implementation such that time complexity of MST-REDUCE is $O(E)$. This uses set operations. MakeSet, Union and Find-Set. I've annotated my analysis in the picture along with algorithm. \nI'm thinking to implement the set as linked list here. So my make-set and find-set are $O(1)$. But Union sucks: $O(V)$. Since we are doing union for all the elements, we have total $O(V^2)$ time spent in union. Which gives amortized $O(V)$. Now the problem isn't clear whether it is expecting amortized time complexity or not. So I'm wondering if any better approach is possible. Note the algorithm is running for all nodes and all edges. Hence I think amortized makes sense.</p>\n\n<p>Here is my analysis (line, complexity)</p>\n\n<p>1-3 $V$ </p>\n\n<p>4-9 $\\frac{V}{2} \\cdot union = \\frac{V}{2} \\cdot \\frac{V}{2} = V^2 = V$ (amortized)</p>\n\n<p>10 $V \\cdot findset = V$</p>\n\n<p>12-21 $E \\cdot findset = E$</p>\n\n<p>Since $E &gt;= V - 1$, we have overall time complexity of $O(E)$. </p>\n\n<pre><code>0   MST-REDUCE(G, orig, c T)\n1   for each v in V[G]\n2       mark[v] &lt;- FALSE\n3       MAKE-SET(v)\n4   for each u in V[G]\n5       if mark[u] = FALSE\n6           choose v in Adj[u] such that c[u,v] is minimized.\n7           UNION(u,v)\n8           T &lt;- T union { orig(u,v) }\n9           mark[u] &lt;- mark[v] &lt;- TRUE\n10  V[G'] &lt;- { FIND-SET(v) : v in V[G] }\n11  E[G'] &lt;- { }\n12  for each (x,y) in E[G]\n13      u &lt;- FIND-SET(x)\n14      v &lt;- FIND-SET(y)\n15      if (u,v) doesn't belong E[G']\n16          E[G'] &lt;- E[G'] union {(u,v)}\n17          orig'[u,v] &lt;- orig[x,y]\n18          c'[u,v] &lt;- c[x,y]\n19      else if c[x,y] &lt; c'[u,v]\n20          orig'[u,v] &lt;- orig[x,y]\n21          c'[u,v] &lt;- c[x,y]\n22  construct adjacency list Adj for G'\n23  return G', orig', c', T\n</code></pre>\n", 'ViewCount': '671', 'Title': 'Show that the Minimum spanning tree Reduce Algorithm runs in O(E) on sparse graphs', 'LastEditorUserId': '2375', 'LastActivityDate': '2012-08-31T08:37:11.837', 'LastEditDate': '2012-08-31T08:37:11.837', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2375', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-30T22:19:05.153', 'Id': '3375''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>If you have a quick-sort algorithm, and you always select the smallest (or largest) element as your pivot; am I right in assuming that if you provide an already sorted data set, you will always get worst-case performance regardless of whether your 'already sorted' list is in ascending or descending order? </p>\n\n<p>My thinking is that, if you always choose the smallest element for your pivot, then whether your 'already-sorted' input is sorted by ascending or descending doesn't matter because the subset chosen to be sorted relative to your pivot will always be the same size?</p>\n", 'ViewCount': '433', 'Title': 'Does Quicksort always have quadratic runtime if you choose a maximum element as pivot?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-27T12:58:30.307', 'LastEditDate': '2012-08-31T07:28:12.087', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '3379', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-08-31T04:24:50.107', 'Id': '3377''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I saw a RAM model diagram that displayed an input tape, output tape, the program (read-only), the instruction pointer, and the memory registers. However, when I look at questions of time complexity, it is relevant how much time the program needs to spend on one action. Say you want to read one integer symbol from the read tape, add it to an integer from a memory register, and then squish the result into the write tape cell, and then move the read head one to the right and the write head one to the left. How much time or how many moves did I just waste?</p>\n', 'ViewCount': '82', 'Title': 'What constitutes one operation/cycle/move in the RAM model?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-16T21:23:57.300', 'LastEditDate': '2012-09-15T13:06:08.960', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '4574', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2835', 'Tags': '<algorithms><time-complexity><algorithm-analysis><machine-models>', 'CreationDate': '2012-09-15T04:38:07.633', 'Id': '3557''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1040', 'Title': 'Why does heapsort run in $\\Theta(n \\log n)$ instead of $\\Theta(n^2 \\log n)$ time?', 'LastEditDate': '2012-09-16T22:11:13.370', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1758', 'FavoriteCount': '1', 'Body': '<p>I am reading section 6.4 on Heapsort algorithm in CLRS, page 160.</p>\n\n<pre><code>HEAPSORT(A)  \n1 BUILD-MAX-HEAP(A)  \n2 for i to A.length downto 2  \n3   exchange A[i] with A[i]\n4   A.heap-size = A.heap-size-1  \n5   MAX-HEAPIFY(A,1)\n</code></pre>\n\n<p>Why is the running time, according to the book is $\\Theta (n\\lg{n})$ rather than $\\Theta (n^2\\lg{n})$ ? <code>BUILD-MAX-HEAP(A)</code> takes $\\Theta(n)$, <code>MAX-HEAPIFY(A,1)</code> takes $\\Theta(\\lg{n})$ and repeated $n-1$ times (line 3).</p>\n', 'Tags': '<algorithms><algorithm-analysis><landau-notation><sorting>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-09-16T22:11:13.370', 'CommentCount': '0', 'AcceptedAnswerId': '4579', 'CreationDate': '2012-09-16T20:47:58.343', 'Id': '4578''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '66', 'Title': 'Computing the running time of a divide-by-4-and-conquer algorithm', 'LastEditDate': '2012-09-18T22:12:40.627', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '1', 'Body': "<p>I write this code in python:</p>\n\n<pre><code>def sub(ma):\n    n = len(ma); m = len(ma[0])\n    if n != m : return\n    n2 = int(ceil(n/2))\n    a = []; b = []; c = []; d = [] \n    for i in range(n2):\n        a.append(ma[i][0:n2])\n        b.append(ma[i][n2:n])\n        c.append(ma[n2+i][0:n2])\n        d.append(ma[n2+i][n2:n])\n    return [a,b,c,d] \n\ndef sum(ma):\n        if len(ma) == 1 : return ma[0][0]\n        div = sub(ma)       \n        return sum(div[0])+sum(div[1])+sum(div[2])+sum(div[3]) \n</code></pre>\n\n<p>Do you know what is a possibly recurrence equation $T(n)$ to the 'sum' method? \nI suppose that is like that\n$$T(n) = 4T(n/2) + f(n)$$\nwhat it is $f(n)$ ?\nThanks, </p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-18T22:12:40.627', 'CommentCount': '0', 'AcceptedAnswerId': '4586', 'CreationDate': '2012-09-17T06:29:26.353', 'Id': '4584''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I was given a homework assignment with Big O. I'm stuck with nested for loops that are dependent on the previous loop. Here is a changed up version of my homework question, since I really do want to understand it:</p>\n\n<pre><code>sum = 0;\nfor (i = 0; i &lt; n; i++ \n    for (j = 0; j &lt; i; j++) \n        sum++;\n</code></pre>\n\n<p>The part that's throwing me off is the <code>j &lt; i</code> part. It seems like it would execute almost like a factorial, but with addition. Any hints would be really appreciated.</p>\n", 'ViewCount': '2880', 'Title': 'Big O: Nested For Loop With Dependence', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-18T20:24:32.557', 'LastEditDate': '2012-09-17T17:55:35.750', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1000229', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><landau-notation>', 'CreationDate': '2012-09-07T23:38:26.383', 'Id': '4590''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>The source of this question comes from an undergraduate course I am taking, which covers an introduction to the analysis of algorithms. This is not for homework, but rather a question asked in CLRS.</p>\n\n<p>You have a slow machine running at $x$ MIPS, and a fast machine running at $y$ MIPS. You also have two algorithms of the same class, but different running time complexities: one "slow" algorithm runs at $T(n) = c_1n^2$ whereas a "fast" algorithm runs at $T(n) = c_2n \\log n$.</p>\n\n<p>You execute the slow algorithm on the fast machine, and the fast algorithm on the slow machine. What is the largest value of n such that the fast machine running the slow algorithm beats the slow machine running the fast algorithm?</p>\n\n<p><strong>My solution so far:</strong></p>\n\n<p>Find the set of all $n$ such that $$\\frac{c_2n\\log n}{x} &gt; \\frac{c_1n^2}{y}$$ where $n$ is a natural number.</p>\n\n<p>This is my work thus far:</p>\n\n<p>$$\\{n : \\frac{c_2 n \\log_2 n}{x} &gt; \\frac{c_1 n^2}{y}, n \\in \\mathbb{N}\\} = \\{n : n &lt; \\frac{c_2 y}{c_1 x} \\log_2 n, n \\in \\mathbb{N}\\}$$</p>\n\n<p>The only solution that comes to mind now is to plug-n-chug all values of $n$ until I find the first n where </p>\n\n<p>$$n &lt; \\frac{c_2y}{c_1x}\\log(n)$$ </p>\n\n<p>no longer holds.</p>\n', 'ViewCount': '288', 'Title': 'Given a fast and a slow computer, at what sizes does the fast computer running a slow algorithm beat the slow computer running a fast algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-19T10:41:35.640', 'LastEditDate': '2012-09-19T10:07:43.623', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2867', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><mathematical-analysis>', 'CreationDate': '2012-09-18T04:09:39.707', 'Id': '4600''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was watching the <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-16-greedy-algorithms-minimum-spanning-trees/" rel="nofollow">video lecture from MIT on Prim\'s algorithm for minimum spanning trees</a>.\nWhy do we need to do the swap step for proving the theorem that if we choose a set of vertices  in minimum spanning tree of $G(V,E)$and let us call that $A$ such  $A\\subset B$,  the edge with the least weight connecting $A$ to $V-A$ will always be in the minimum spanning tree ?  The professor has done the swap step at point 59:07 seconds in the video.</p>\n', 'ViewCount': '179', 'Title': "Why do the swap step in Prim's algorithm for minimum spanning trees?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-20T06:52:26.807', 'LastEditDate': '2012-09-19T21:19:36.247', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '4624', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><graph-theory><algorithm-analysis><greedy-algorithms><spanning-trees>', 'CreationDate': '2012-09-19T13:11:11.090', 'Id': '4614''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m going through the MIT Online Course Videos on Intro. to Algorithms at <a href="http://video.google.com/videoplay?docid=-3860843648161712896" rel="nofollow">here</a> at around 38:00.</p>\n\n<p>So we have a recursion formula</p>\n\n<p>$\\qquad T(n) = T(n/10) + T(9n/10) + O(n)$</p>\n\n<p>If we build a recursion tree it looks like </p>\n\n<pre><code>                   T(n)                     -- Level 1       = c*n\n             /               \\\n       T(n/10)             T(9n/10)         -- Level 2       = c*n\n        /   \\             /         \\\n T(n/100)  T(9n/100) T(9n/100)  T(81n/100)  -- Level 3       = c*n\n\n   /                                  \\                      &lt;= c*n\n    .                                .\n    .                                .\n 0(1)                                 0(1)\n</code></pre>\n\n<p>where $c$ is a constant larger than $0$.</p>\n\n<p>Shortest path from the root to the leaf is $\\log_{10}(n)$.</p>\n\n<p>Longest path from the root to the leaf is $\\log_{10/9}(n)$</p>\n\n<p>Therefore, the cost could be calculated as Cost = Cost of each level * number of levels.</p>\n\n<p>With the shortest path cost, we get a lower bound of $cn\\log_{10}(n)$, and with the  longest path cost an upper bound of  $cn\\log_{10/9}(n)$.</p>\n\n<p>And now I have to add the costs of leaf nodes, which leads to my problem. In the video it says the total number of leaves is in $\\Theta(n)$. I have trouble figuring out how he got to $\\Theta(n)$.</p>\n\n<p>The video further says $T(n)$ is bounded by</p>\n\n<p>$\\qquad cn\\log_{10}(n) + O(n) \\leq T(n) \\leq cn\\log_{10/9}(n) + O(n)$</p>\n\n<p>Wouldn\'t it make more sense to say it\'s </p>\n\n<p>$\\qquad cn\\log_{10}(n) + O(n^{\\log_{10}(2)}) \\leq T(n) \\leq cn\\log_{10/9}(n) + O(n^{\\log_{10/9}(2)})$</p>\n\n<p>where $\\Theta(n^{log_{10}(2)})$ represents the leaves on the left and $\\Theta(n^{\\log_{10/9}(2)})$ represents the leaves on the right.</p>\n\n<p>Or is there a way to simplify these terms to $\\Theta(n)$? </p>\n', 'ViewCount': '381', 'Title': 'Finding the number of leaves in a imbalanced recursion tree', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-21T21:08:01.437', 'LastEditDate': '2012-09-21T21:05:43.703', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '4657', 'Score': '1', 'OwnerDisplayName': 'user1671022', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation>', 'CreationDate': '2012-09-14T10:17:19.653', 'Id': '4656''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I recently read an article <a href="http://www.scottaaronson.com/writings/bignumbers.html" rel="nofollow">Scott Aaronson - Big Numbers</a> . That has made me think about the effective upper-bound for sorting.</p>\n\n<p>According to the article, some of the big numbers like the number of particles in the universe and age of universe in milliseconds are less than $10^{100}$.</p>\n\n<p>In any realistic computational device, the data to be sorted will have to be lesser than these numbers. (As otherwise, it would be impossible to store the numbers physically).</p>\n\n<p>$\\log_2(10^{100}) \\approx 333$</p>\n\n<p>Hence, if we take a number $C &gt; 333$, we can show that number of steps required for sorting an input of size $n$ will always be lesser than $Cn$</p>\n\n<p>This makes sorting an $O(n)$ time operation using algorithms like QuickSort or HeapSort.</p>\n\n<p>Is there a point I\'ve wrongly considered while making this assumption? </p>\n\n<p><strong>Should we consider physical constraints while analyzing algorithms? If not, why?</strong></p>\n', 'ViewCount': '65', 'Title': 'Upper-bounding the number of comparisons for Sorting to $\\Theta(n)$ using a physically big number like Number of Particles in the Universe', 'LastActivityDate': '2012-09-24T04:08:05.383', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4701', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2642', 'Tags': '<algorithm-analysis><sorting>', 'CreationDate': '2012-09-24T03:34:39.680', 'FavoriteCount': '1', 'Id': '4700''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I see <strong>structural induction</strong> the usual way for proving an algorithm's <strong>termination</strong> property, but it's not that easy to prove by means of induction on a <strong>tree</strong> algorithm. Now I am struggling on proving that the pre-order tree traversal algorithm is terminable:</p>\n\n<pre><code>preorder(node)\n  if node == null then return\n  visit(node)\n  preorder(node.left) \n  preorder(node.right)\n</code></pre>\n\n<p>How should I prove?</p>\n", 'ViewCount': '220', 'Title': 'How to prove that the pre-order tree traversal algorithm terminates?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-26T20:14:24.327', 'LastEditDate': '2012-09-26T20:14:24.327', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4726', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2954', 'Tags': '<algorithms><data-structures><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2012-09-24T15:52:44.860', 'Id': '4719''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '326', 'Title': 'How to prove that BFS directed-graph traversal algorithm terminates?', 'LastEditDate': '2012-09-27T00:17:49.203', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2954', 'FavoriteCount': '0', 'Body': u'<p>How to prove that BFS directed-graph traversal algorithm terminates?\n(I copy the pseudocode from <a href="http://en.wikipedia.org/wiki/Breadth-first_search" rel="nofollow">here</a>) Input: A graph G and a root v of G.</p>\n\n<pre><code>  procedure BFS(G,v):\n      create a queue Q\n      enqueue v onto Q\n      mark v\n      while Q is not empty:\n          t \u2190 Q.dequeue()\n          if t is what we are looking for:\n              return t\n          for all edges e in G.incidentEdges(t) do\n             o \u2190 G.opposite(t,e)\n             if o is not marked:\n                  mark o\n                  enqueue o onto Q\n</code></pre>\n', 'Tags': '<algorithms><algorithm-analysis><correctness-proof>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-09-27T00:17:49.203', 'CommentCount': '1', 'AcceptedAnswerId': '4749', 'CreationDate': '2012-09-26T18:21:07.543', 'Id': '4746''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>What would the order of growth for this loop be:</p>\n\n<pre><code>int sum = 0;\nfor (int n = N; n &gt; 0; n /= 2)\n          for(int i = 0; i &lt; n; i++)\n             sum++;\n</code></pre>\n\n<p>The first loop seems to run for $\\log N + 1$ times and the second loop runs $n$ times.\nSo is the correct answer $O(n \\log n)$?</p>\n', 'ViewCount': '803', 'Title': 'The order of growth analysis of simple for loop', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-03T10:40:26.673', 'LastEditDate': '2012-10-02T17:33:25.097', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'Rick', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><asymptotics>', 'CreationDate': '2012-09-28T18:11:59.537', 'Id': '4800''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>What is the runtime complexity of the following implementation of Bubblesort (for integers)?</p>\n\n<pre><code>    #define SWAP(a,b)   { int t; t=a; a=b; b=t; }\n\n    void bubble( int a[], int n )\n    /* Pre-condition: a contains n items to be sorted */\n    {\n       int i, j;\n      /* Make n passes through the array */\n      for(i=0;i&lt;n-1;i++)\n      {\n     /* From the first element to the end\n       of the unsorted section */\n        for(j=1;j&lt;(n-i);j++)\n        {\n        /* If adjacent items are out of order, swap them */\n       if( a[j-1]&gt;a[j] ) SWAP(a[j-1],a[j]);\n       }\n    }\n}  \n</code></pre>\n', 'ViewCount': '282', 'Title': 'Complexity of optimized bubblesort', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-07T15:49:43.613', 'LastEditDate': '2012-10-07T15:49:43.613', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3087', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-10-07T09:52:54.087', 'Id': '4915''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>This is a similar algorithm to one I used in a previous question, but I\'m trying to illustrate a different problem here.</p>\n\n<pre><code>for (int i = 0; i &lt; numbers.length - 1; i++) {\n    for (int j = i + 1; j &lt; numbers.length; j++) {\n        if (numbers[i] + numbers[j] == 10) {\n            System.out.println(i+" and "+j+" add up to 10!");\n            return;\n        }\n    }\n}\nSystem.out.println("None of these numbers add up to 10!");\nreturn;\n</code></pre>\n\n<p>Basically, I realised, that I have a decent understanding of how to work out the best case run time here. I.e. the first two numbers which are fed in, add up to 10, and therefore our best-case runtime is constant time. Also, I understand that in the worst-case, none of the numbers we provide add up to 10, so we must then iterate through all loops, giving us a quadratic runtime. However, in the slides I was going through (for this particular course) I noticed that I had missed this:</p>\n\n<p>Average case</p>\n\n<p>\u2022 Presume that we\'ve just randomly given\nintegers between 1 and 9 as input in\nnumbers to our previous example</p>\n\n<p>\u2022 Exercise: Work out the average running time</p>\n\n<p>I realise I don\'t have the first clue as to how to go about calculating average case run-time. I\'m not asking for the answer to: "what is the average-case runtime for this algorithm?", what I need to know is, how do you work something like this out? </p>\n', 'ViewCount': '253', 'Title': 'How to go about working the average case run time of this trivial algorithm (and other algorithms)?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T16:02:27.433', 'LastEditDate': '2012-10-28T16:02:27.433', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2012-10-10T10:06:09.990', 'Id': '4993''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '56', 'Title': 'Compute relational composition in $O(|E||V|)$', 'LastEditDate': '2012-10-14T09:37:36.983', 'AnswerCount': '2', 'Score': '7', 'OwnerDisplayName': 'Johannes', 'PostTypeId': '1', 'OwnerUserId': '3181', 'Body': '<p>Definitions: Let $G=(V,E)$ be a DAG without self-loops, and $X \\subseteq G$ and $Y \\subseteq G$ be graphs.</p>\n\n<p>Input: $X,Y$. Output: The relational composition <a href="http://en.wikipedia.org/wiki/Relation_composition">relational composition</a> $X \\circ Y$ in $\\mathcal{O}(|E||V|)$.</p>\n\n<ul>\n<li>Case 1: $|E| \\le |V|$. Two for loops over $E(X)$ and $E(Y)$: Runtime $ \\le \\mathcal{O}(|E|^2) \\le \\mathcal{O}(|E||V|)$.</li>\n<li>Case 2: $|V| \\le |E|$\n<ol>\n<li>Draw the graph $(V(G),E(X) \\cup E(Y))$: $(O(|V|)+\\mathcal{O}(|2E|)))$. We call edges from $E(X)$ black and from $E(Y)$ red.</li>\n<li>Topologically sort it (Kahn: $\\mathcal{O}(|V|) + \\mathcal{O}(|E|)$). Let the first level be $0$, and edges go from a level to a higher level.</li>\n<li>Draw this graph twice.</li>\n<li>In the first copy, remove every red edge beginning at even level, and every black edge beginning at odd level: $\\mathcal{O}(E)$.</li>\n<li>In the second copy, remove every "black even" and "red odd": $\\mathcal{O}(E)$.</li>\n<li>For the first copy:\n<ul>\n<li>for all nodes $u$ on level $2i$</li>\n<li>for all nodes $v$ on level $2i+1$</li>\n<li>report edge $(u,v)$ (Runtime $\\mathcal{O}(V^2) \\le \\mathcal{O}(EV)$).</li>\n</ul></li>\n<li>For the second copy: The same for "$2i+1$".</li>\n<li>Union the reported nodes, throw out duplicates  $\\mathcal{O}(V^2) &lt;= \\mathcal{O}(EV)$ (I hope the graph representation allows this).</li>\n</ol></li>\n</ul>\n\n<p>Could some of you please look over my algorithm and check whether </p>\n\n<ul>\n<li>it is correct</li>\n<li>it is in $\\mathcal{O}(|E||V|)$</li>\n</ul>\n\n<p>If it\'s correct, does my algorithm already "exist"? If not, could you provide an alternative? I\'ll accept the first answer, but upvote if some more people are so kind to check.</p>\n\n<p>EDIT: Step 6 Seems to be in $O(E^2)$ sometimes. I wish this would not be true. Has anyone a working algorithm?</p>\n', 'Tags': '<algorithms><graph-theory><algorithm-analysis><check-my-algorithm>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T09:41:40.783', 'CommentCount': '0', 'AcceptedAnswerId': '6057', 'CreationDate': '2012-10-13T08:23:35.550', 'Id': '6055''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have some difficulties performing the worst case analysis on this algorithm.<br>\nThe outermost loop is executed $2N$ times.<br>\nThe while loop, in the worst case, will increase by $2$ each time, so it performs $i/2$ basic operations ($*2$ because double call)</p>\n\n<pre><code>for (i=1; i&lt;=2*N; i++) {\n        j = 0;\n        while (j &lt;= i) {\n            a[i] = function (function (a[i]));\n            if (c[i][j] != 0)\n                j = j + 6;\n            else\n                j = j + 2;\n        }\n    }\n</code></pre>\n\n<p><code>function</code> is the basic operation.<br>\nAm I going the right way? </p>\n', 'ViewCount': '525', 'Title': 'Runtime analysis of a nested loop', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-17T09:19:56.000', 'LastEditDate': '2012-11-17T09:19:56.000', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'eouti', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2012-10-17T16:35:50.160', 'Id': '6129''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>From <a href="http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/lect12.pdf" rel="nofollow">Van Emde Boas trees lecture</a>:</p>\n\n<blockquote>\n  <p>We will use the idea of superimposing a tree of degree ${u^{1/2}}$ on top of\n  a bit vector, but <strong>shrink the universe size recursively</strong> by a square\n  root at each tree level. The ${u^{1/2}}$ items on the \ufb01rst level each hold\n  structures of ${u^{1/4}}$ items, which hold structures of ${u^{1/8}}$ items, and\n  so on, down to size 2.\n  I have a  question regarding van Emde Boas trees :</p>\n</blockquote>\n\n<ol>\n<li>How is the universe size getting reduced ? Aren\'t we just spreading the universe keys which is always constant at $u$ to different levels ? I can not understand the idea of "<strong>shriniking</strong>" the universe size .  I find similar language is used in defining the recursive structure for Van Emde Boas tree in <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/" rel="nofollow">Introduction to Algorithms</a> by CLRS also .</li>\n</ol>\n', 'ViewCount': '563', 'Title': 'Explanation of recursive structure of Van Emde Boas Tree', 'LastEditorUserId': '2223', 'LastActivityDate': '2013-01-18T15:27:02.663', 'LastEditDate': '2012-10-22T09:04:38.213', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6197', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><data-structures><algorithm-analysis><search-trees><trees>', 'CreationDate': '2012-10-20T14:15:05.167', 'Id': '6192''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In order to achieve the time complexity of $O(\\log \\log u)$ for van Emde Boas trees I read in <a href="http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/lect12.pdf" rel="nofollow">this lecture</a> that the the universe size  $u$  is chosen as $u = 2^{2^k}$ for some integer $k$ for van Emde Boas trees. Why choose $u$ to be of this specific form ?</p>\n', 'ViewCount': '116', 'Title': 'Size of the universe for van Emde Boas Trees', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-10-20T14:34:19.040', 'LastEditDate': '2012-10-20T14:34:19.040', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6195', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><data-structures><algorithm-analysis><binary-trees><trees>', 'CreationDate': '2012-10-20T14:22:39.857', 'Id': '6193''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm looking for a $\\Theta$ approximation of\n$$T(n) = T(n-1) + cn^{2}$$</p>\n\n<p>This is what I have so far:</p>\n\n<p>$$\n\\begin{align*}\nT(n-1)&amp; = T(n-2) + c(n-1)^2\\\\\nT(n) &amp;= T(n-2) + c(n-1) + cn^2\\\\[1ex]\nT(n-2) &amp;= T(n-3) + c(n-2)^2\\\\\nT(n) &amp; = T(n-3) + c(n-2)^2 + c(n-1)^2 + cn^2 \\\\[1ex]\nT(n-3) &amp;= T(n-4) + c(n-3)^2 \\\\\nT(n) &amp;= T(n-4) + c(n-3)^2 + c(n-2)^2 + c(n-1)^2 + cn^2\n\\end{align*}\n$$</p>\n\n<p>So, at this point I was going to generalize and substitute $k$ into the equation.</p>\n\n<p>$$T(n)= T(n-k) + (n-(k-1))^2 + c(k-1)^2$$</p>\n\n<p>Now, I start to bring the base case of 1 into the picture. On a couple of previous, more simple problems, I was able to set my generalized k equation equal to 1 and then solve for $k$. Then put $k$ back into the equation to get my ultimate answer.</p>\n\n<p>But I am totally stuck on the $(n-k+1)^2$ part. I mean, should I actually foil all this out? I did it and got $k^2-2kn-2k+n^2 +2n +1 = 1$. At this point I'm thinking I totally must have done something wrong since I've never see this in previous problems.</p>\n\n<p>Could anyone offer me some help with how to solve this one? I would greatly appreciate it. I also tried another approach where I tried to set $n-k = 0$ from the last part of the equation and got that $k = n$. I plugged n back into the equation towards the end and ultimately got $n^2$ as an answer. I have no clue if this is right or not.</p>\n\n<p>I am in an algorithms analysis class and we started doing recurrence relations and I'm not 100% sure if I am doing this problem correct. I get to a point where I am just stuck and don't know what to do. Maybe I'm doing this wrong, who knows. The question doesn't care about upper or lower bounds, it just wants a theta.</p>\n", 'ViewCount': '1572', 'Title': 'Recurrence relation for time complexity $T(n) = T(n-1) + n^2$', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-28T00:27:27.063', 'LastEditDate': '2012-10-25T22:53:46.943', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '6275', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4336', 'Tags': '<time-complexity><algorithm-analysis><proof-techniques><recurrence-relation>', 'CreationDate': '2012-10-24T00:56:53.863', 'Id': '6274''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm in a first year discrete math course and we started algorithms. I created a recursive algorithm to multiply two numbers together:</p>\n\n<pre><code>function multiply($n, $r) {\n    if($n == 1)\n return $r;\n    else if($r == 1)\n return $n;\n    else\n        return $r + multiply($n - 1, $r);\n}\n</code></pre>\n\n<p>How do I prove my algorithm is correct?</p>\n\n<p>A quick google search tells me I have to prove that it works for $n + 1$ and I have to prove that it terminates. Unfortunately I'm still incredibly new to this and haven't the faintest clue as to where to start for proving my algorithm correct, so I would really appreciate some help here. I think maybe I have to do some sort of proof by induction but as this is an algorithm, I wouldn't know where to start.</p>\n", 'ViewCount': '643', 'Title': 'Prove correctness of recursive multiplication algorithm', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-31T05:56:55.600', 'LastEditDate': '2013-08-31T05:56:55.600', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4359', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2012-10-25T15:37:15.703', 'Id': '6311''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1759', 'Title': 'Proof that a randomly built binary search tree has logarithmic height', 'LastEditDate': '2012-10-28T11:16:30.370', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '1', 'Body': '<p>How do you prove that the expected height of a randomly built <a href="http://en.wikipedia.org/wiki/Binary_search_tree" rel="nofollow">binary search tree</a> with $n$ nodes is $O(\\log n)$? There is a proof in CLRS <em>Introduction to Algorithms</em> (chapter 12.4), but I don\'t understand it.</p>\n', 'Tags': '<data-structures><algorithm-analysis><binary-trees><search-trees><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T12:56:45.620', 'CommentCount': '4', 'AcceptedAnswerId': '6356', 'CreationDate': '2012-10-27T19:37:43.787', 'Id': '6342''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have the following pseudo code:    </p>\n\n<pre><code>Multiply(y,z)    \n1. if (z==0) return 0     \n2. else if (z is odd)    \n3.    then return (Multiply(2y, floor(z/2)) + y )    \n4. else return (Multiply(2y, floor(z/2)))    \n</code></pre>\n\n<p>Towards analysing this procedure's runtime, this recurrence relation is given as answer:</p>\n\n<p>$\\qquad \\displaystyle T(z) = \\begin{cases} 0 &amp; z=0 \\\\ T(z/2)+1 &amp; z&gt;0\\end{cases}$ </p>\n\n<p>Why is $T(z)=0$ when $z=0$? Shouldn't it be $1$ for this case?        </p>\n\n<p>And, the $+1$ in $T(z/2)\\mathbf{+1}$ is because the worst case is \n<code>(multiply(2y, floor(z/2)) + y</code> (note the <code>+ y</code>). Am I correct?     </p>\n", 'ViewCount': '128', 'Title': 'How does this recurrence relation fit the algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-29T00:23:43.783', 'LastEditDate': '2012-10-28T11:02:22.307', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4379', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><recurrence-relation>', 'CreationDate': '2012-10-28T01:06:28.050', 'Id': '6345''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>One can compress data with straight-line grammars. An algorithm that employs this technique is called <em>Sequitur</em>. If I understood correctly, Sequitur basically starts with one rule representing the input and then does these three steps till the grammar does not change anymore:</p>\n\n<ol>\n<li>For each rule, try to find any sequences of symbols in any other rule that match the rule's right hand side and replace these sequences by the rules left hand side.</li>\n<li>For each pair of adjacent symbols in any right hand side, find all non-overlapping other pairs of adjacent symbols that are equal to the original pair. If there are any other pairs, add a new nonterminal, replace all occurrences of these pairs by the new nonterminal and add a new rule that defines the nonterminal.</li>\n<li>For each nonterminal that appears exactly once on all right-hand sides of all rules, replace its occurrence by its definition, remove the nonterminal and the rule that defines it.</li>\n</ol>\n\n<p>For each (non-empty) input, can one guarantee that the above algorithm terminates?</p>\n", 'ViewCount': '251', 'Title': 'Will this algorithm terminate on any input?', 'LastEditorUserId': '2280', 'LastActivityDate': '2013-01-28T22:04:37.830', 'LastEditDate': '2012-10-29T06:14:04.147', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2280', 'Tags': '<algorithms><algorithm-analysis><formal-grammars><data-compression><correctness-proof>', 'CreationDate': '2012-10-28T21:15:53.433', 'FavoriteCount': '1', 'Id': '6360''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '985', 'Title': "Base of logarithm in runtime of Prim's and Kruskal's algorithms", 'LastEditDate': '2012-11-01T22:48:50.390', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'CodeKingPlusPlus', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Body': "<p>For Prim's and Kruskal's Algorithm there are many implementations which will give different running times. However suppose our implementation of Prim's algorithm has runtime $O(|E| + |V|\\cdot \\log(|V|))$ and Kruskals's algorithm has runtime $O(|E|\\cdot \\log(|V|))$.</p>\n\n<p>What is the base of the $\\log$?</p>\n", 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T22:48:50.390', 'CommentCount': '3', 'AcceptedAnswerId': '6436', 'CreationDate': '2012-10-27T20:48:47.810', 'Id': '6435''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am currently working on a project where I\'m using an implementation of Hoffman and Pavley\'s "<a href="http://dl.acm.org/citation.cfm?doid=320998.321004" rel="nofollow">Method for the Solution of the Nth Best Path Problem</a>" to find n-th best path through a directed graph. The implementation is based on <a href="http://quickgraph.codeplex.com/wikipage?title=Ranked%20Shortest%20Path" rel="nofollow">QuickGraph\'s Ranked Shortest Path implementation</a>.</p>\n\n<p>I have been trying to determine the complexity of Hoffman and Pavley\'s algorithm as well as QuickGraph\'s implementation, but without any luck -- so basically my question is if someone knows the complexity of the original method proposed by Hoffman and Pavley as well as the complexity of QuickGraph\'s implementation?</p>\n', 'ViewCount': '175', 'Title': "What is the complexity of Hoffman and Pavley's Nth best path algorithm?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-02T10:09:43.120', 'LastEditDate': '2012-11-02T10:09:43.120', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4433', 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><shortest-path>', 'CreationDate': '2012-11-02T08:43:34.007', 'FavoriteCount': '1', 'Id': '6444''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>How would I solve these problems involving time complexity:</p>\n\n<ol>\n<li><p>Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$, insertion sort runs in $8n^2$ steps, while merge sort runs in $64n \\log_2 n$ steps. For which values of $n$ does insertion sort beat merge sort?</p></li>\n<li><p>What is the smallest value of n such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine?</p></li>\n<li><p>For each function $f(n)$ and time $t$ , determine the largest size $n$ of a problem that can be solved in time $t$, assuming that the algorithm to solve the problem takes $f(n)$ microseconds. </p>\n\n<p>(a) $n! = 1$ second </p>\n\n<p>(b) $n \\log_2 n$ = 1 second</p></li>\n</ol>\n', 'ViewCount': '492', 'Title': 'Comparing Time complexity?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-04T06:34:16.437', 'LastEditDate': '2012-11-03T10:04:20.713', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-5', 'PostTypeId': '1', 'OwnerUserId': '4442', 'Tags': '<time-complexity><algorithm-analysis>', 'CreationDate': '2012-11-03T03:42:34.377', 'Id': '6459''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '600', 'Title': 'Iterative binary search analysis', 'LastEditDate': '2012-11-04T15:41:46.187', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Hasan Tahsin', 'PostTypeId': '1', 'OwnerUserId': '4610', 'Body': '<p>I\'m a little bit confused about the analysis of <a href="http://en.wikipedia.org/wiki/Binary_search" rel="nofollow">binary search</a>.\nIn almost every paper, the writer assumes that the array size $n$ is always $2^k$.\nWell I truly understand that the time complexity becomes $\\log(n)$ (worst case) under this assumption. But what if $n \\neq 2^k$?</p>\n\n<p>For example if $n=24$, then we have\n5 iterations for 24<br>\n4 i. for 12<br>\n3 i. for 6<br>\n2 i. for 3<br>\n1 i. for 1</p>\n\n<p>So how do we get the result $k=\\log n$ in this example (I mean of course every similar example whereby $n\\neq2^k$)?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-04T15:41:46.187', 'CommentCount': '4', 'AcceptedAnswerId': '6471', 'CreationDate': '2012-11-03T10:15:40.587', 'Id': '6470''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to understand summation for amortization analysis of a hash-table from a <a href="http://videolectures.net/mit6046jf05_leiserson_lec13/" rel="nofollow">MIT lecture video</a> (at time 16:09). </p>\n\n<p>Although you guys don\'t have to go and look at the video, I feel that the summation he does is wrong so I will attach the screenshot of the slide.</p>\n\n<p><img src="http://i.stack.imgur.com/EBRfs.jpg" alt="MIT Lecture Slide"></p>\n', 'ViewCount': '138', 'Title': 'Why is $\\sum_{j=0}^{\\lfloor\\log (n-1)\\rfloor}2^j$ in $\\Theta (n)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-05T17:15:47.033', 'LastEditDate': '2012-11-05T17:15:47.033', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '6474', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<algorithms><data-structures><algorithm-analysis><mathematical-analysis><discrete-mathematics>', 'CreationDate': '2012-11-04T16:32:11.537', 'Id': '6473''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '439', 'Title': 'How asymptotically bad is naive shuffling?', 'LastEditDate': '2012-11-06T20:49:40.330', 'AnswerCount': '2', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '242', 'FavoriteCount': '3', 'Body': "<p>It's well-known that this 'naive' algorithm for shuffling an array by swapping each item with another randomly-chosen one doesn't work correctly:</p>\n\n<pre><code>for (i=0..n-1)\n  swap(A[i], A[random(n)]);\n</code></pre>\n\n<p>Specifically, since at each of $n$ iterations, one of $n$ choices is made (with uniform probability), there are $n^n$ possible 'paths' through the computation; because the number of possible permutations $n!$ doesn't divide evenly into the number of paths $n^n$, it's impossible for this algorithm to produce each of the $n!$ permutations with equal probability.  (Instead, one should use the so-called <em>Fischer-Yates</em> shuffle, which essentially changes out the call to choose a random number from [0..n) with a call to choose a random number from [i..n); that's moot to my question, though.)</p>\n\n<p>What I'm wondering is, how 'bad' can the naive shuffle be?  More specifically, letting $P(n)$ be the set of all permutations and $C(\\rho)$ be the number of paths through the naive algorithm that produce the resulting permutation $\\rho\\in P(n)$, what is the asymptotic behavior of the functions </p>\n\n<p>$\\qquad \\displaystyle M(n) = \\frac{n!}{n^n}\\max_{\\rho\\in P(n)} C(\\rho)$ </p>\n\n<p>and </p>\n\n<p>$\\qquad \\displaystyle m(n) = \\frac{n!}{n^n}\\min_{\\rho\\in P(n)} C(\\rho)$?  </p>\n\n<p>The leading factor is to 'normalize' these values: if the naive shuffle is 'asymptotically good' then </p>\n\n<p>$\\qquad \\displaystyle \\lim_{n\\to\\infty}M(n) = \\lim_{n\\to\\infty}m(n) = 1$.  </p>\n\n<p>I suspect (based on some computer simulations I've seen) that the actual values are bounded away from 1, but is it even known if $\\lim M(n)$ is finite, or if $\\lim m(n)$ is bounded away from 0?  What's known about the behavior of these quantities?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics><probability-theory><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-10T04:07:42.843', 'CommentCount': '13', 'AcceptedAnswerId': '6596', 'CreationDate': '2012-11-06T19:22:56.410', 'Id': '6519''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<pre><code>Insertion-Sort (A) [where A is an array of numbers to be sorted]\n\n(1) for j = 2 to A.length\n(2)       key = A[j]\n(3)       i = j -1\n(4)       while i &gt; 0 and A[i] &gt; key\n(5)              A[i+1] = A[i]\n(6)              i = i - 1\n(7)       A[i + 1] = key\n</code></pre>\n\n<p>CLRS proves the correction of the above algorithm by using a loop invariant:</p>\n\n<blockquote>\n  <p><strong>Loop Invariant:</strong> At the start of each iteration of the for loop of lines 1\u20138, the subarray A[1... j - 1] consists of the elements originally in A[1... j - 1]  but in sorted order.</p>\n  \n  <p>We use loop invariants to help us understand why an algorithm is correct. We must show three things about a loop invariant:</p>\n  \n  <p>Initialization: It is true prior to the first iteration of the loop.</p>\n  \n  <p>Maintenance: If it is true before an iteration of the loop, it remains true before the</p>\n  \n  <p>Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.</p>\n</blockquote>\n\n<p>In the explanation of the maintenance aspect of the loop invariant, the following is mentioned:</p>\n\n<blockquote>\n  <p>Maintenance: A more formal treatment of the this property would require us to state and show a loop invariant for the while loop of lines 5\u20137. At this point, however, we prefer not to get bogged down in such formalism, and so we rely on our informal analysis to show that the second property holds for the outer loop.</p>\n</blockquote>\n\n<p>Why would a "formal treatment" require a loop invariant for the while loop? Having one invariant for the outer for loop is sufficient to prove the correctness of the algorithm- why would a "formal treatment" require a loop invariant?</p>\n', 'ViewCount': '153', 'Title': 'Loop invariants?', 'LastActivityDate': '2012-11-09T07:23:45.327', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4519', 'Tags': '<algorithms><algorithm-analysis><correctness-proof>', 'CreationDate': '2012-11-09T03:03:51.197', 'Id': '6567''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Could anyone point me to simple tutorial on greedy algorithm  for Minimum Spanning tree - Kruskal's and Prims' Method</p>\n\n<p>I am looking for a tutorial which </p>\n\n<ul>\n<li>does not include all the mathematical notation  </li>\n<li>explains algorithm along with the analysis of the running time.</li>\n</ul>\n", 'ViewCount': '614', 'Title': 'Greedy algorithms tutorial', 'LastEditorUserId': '3004', 'LastActivityDate': '2012-11-13T07:33:33.163', 'LastEditDate': '2012-11-12T18:32:49.273', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6635', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<algorithm-analysis><greedy-algorithms>', 'CreationDate': '2012-11-12T15:02:47.490', 'Id': '6634''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to find the maximum palindrome sub-sequence and after going through some tutorials, I came up with a memoized version.But I am not sure about the runtime.I want to know if the following algorithm will work.Could also someone explain what the runtime will be?</p>\n\n<pre><code>Memoized-Palindrome(A,n)\ninitialize longest [i][j] =0 for all i and j\nthen return Memoized-Palindrome1(A,1,n,longest)\n\nMemoized-Palindrome1(A,i,j,longest)\nif longest[i][j]&gt;0 return longest [i][j]\nif (j-i) &lt;=1 return j-i\nif A[i]==A[j] \n      then longest[i][j] = 2 + Memoized-Palindrome1(A,i+1,j-1,longest)\n    else \n      longest[i][j]= max(Memoized-Palindrome1(A,i+1,j,longest),Memoized-Palindrome1(A,i,j+1,longest)\nreturn longest[i][j]\n</code></pre>\n', 'ViewCount': '342', 'Title': 'Memoized Palindrome Subsequence', 'LastEditorUserId': '3004', 'LastActivityDate': '2012-11-12T19:43:36.053', 'LastEditDate': '2012-11-12T19:24:14.483', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6639', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<algorithm-analysis><dynamic-programming>', 'CreationDate': '2012-11-12T16:12:19.757', 'Id': '6637''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>For what size alphabet does it take longer to construct <a href="http://en.wikipedia.org/wiki/Suffix_tree" rel="nofollow">a suffix tree</a> - for a really small alphabet size (because it has to go deep into the tree) or for a large alphabet size? Or is it dependent on the algorithm you use? If it is dependent, how does the alphabet size affect <a href="http://en.wikipedia.org/wiki/Ukkonen%27s_algorithm" rel="nofollow">Ukkonen\'s algorithm</a>?</p>\n', 'ViewCount': '88', 'Title': 'What are the effects of the alphabet size on construct algorithms for suffix trees?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-23T09:43:36.243', 'LastEditDate': '2012-11-23T09:30:49.477', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4696', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings><efficiency>', 'CreationDate': '2012-11-22T22:14:10.630', 'Id': '6842''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I understood what amortized analysis does, but can anyone tell me what is the main purpose of this kind of analysis?</p>\n\n<p>What I understood:</p>\n\n<p>Let say we have 3 three operations a,b,c used 1,2 and 3 times to achieve d. Based on aggregate analysis a,b and c are used 2 times each. Is this correct?</p>\n\n<p>I am trying to understand the advantages of this in CLRS but I am completely lost. For example in dynamic programming we save the answers to sub problems in tables which helps us reduce the running time(lets say from exponential to polynomial). But I am unable to get a complete picture of amortized analysis.</p>\n', 'ViewCount': '356', 'Title': 'Advantages of amortized analysis', 'LastEditorUserId': '2755', 'LastActivityDate': '2014-05-03T23:17:14.713', 'LastEditDate': '2012-11-25T07:34:07.733', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '6873', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<time-complexity><algorithm-analysis><amortized-analysis>', 'CreationDate': '2012-11-24T07:49:27.967', 'Id': '6865''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '325', 'Title': 'What is the time complexity of the following program?', 'LastEditDate': '2012-12-23T09:30:04.240', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4732', 'FavoriteCount': '2', 'Body': '<p>Please help me calculate the time complexity of the following program.</p>\n\n<pre><code>int fun (int n) {\n  if (n &lt;= 2)\n   return 1;\n  else\n   return fun(sqrt(n)) + n;\n}\n</code></pre>\n\n<p>Please explain.</p>\n\n<p>There were four choices given.</p>\n\n<ol>\n<li>$\\Theta(n^2)$</li>\n<li>$\\Theta(n \\log n)$</li>\n<li>$\\Theta(\\log n)$</li>\n<li>$\\Theta(\\log \\log n)$</li>\n</ol>\n', 'Tags': '<algorithm-analysis><runtime-analysis><landau-notation>', 'LastEditorUserId': '3016', 'LastActivityDate': '2012-12-23T09:30:04.240', 'CommentCount': '0', 'AcceptedAnswerId': '6904', 'CreationDate': '2012-11-26T02:06:18.790', 'Id': '6901''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '123', 'Title': 'How to guess the value of $j$ at the end of the loop?', 'LastEditDate': '2012-11-27T15:10:08.253', 'AnswerCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4732', 'FavoriteCount': '1', 'Body': '<pre><code>for ( i = n , j = 0 ; i &gt; 0 ; i = i / 2 , j = j + i ) ;\n</code></pre>\n\n<p>All variables are integers.(i.e. if decimal values occur, consider their floor value)</p>\n\n<p>Let $\\text{val}(j)$ denote the value of $j$, after the termination of the loop. Which of the following is true?</p>\n\n<p>(A)$\\quad \\text{val(j)} = \\Theta(\\log(n)) $ <br>\n(B)$\\quad \\text{val(j)} = \\Theta(\\sqrt n) $ <br>\n(C)$\\quad \\text{val(j)} = \\Theta(n) $ <br>\n(D)$\\quad \\text{val(j)} = \\Theta(\\log\\log n) $</p>\n\n<p>Please explain, is there any easy way to guess the value of $j$?</p>\n', 'Tags': '<algorithm-analysis><asymptotics><integers>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-28T00:56:50.520', 'CommentCount': '13', 'AcceptedAnswerId': '6963', 'CreationDate': '2012-11-27T11:02:16.417', 'Id': '6952''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '196', 'Title': 'A puzzle related to nested loops', 'LastEditDate': '2012-11-29T09:37:21.453', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4732', 'FavoriteCount': '1', 'Body': '<p>For a given input $N$, how many times does the enclosed statement executes?</p>\n\n<blockquote>\n  <p>for $i$ in $1\\ldots N$ loop <br>\n  $\\quad$for $j$ in $1\\ldots i$ loop <br>\n  $\\quad$$\\quad$for $k$ in $i\\ldots j$ loop <br>\n  $\\quad$$\\quad$$\\quad$$sum = sum + i$ ; <br>\n  $\\quad$$\\quad$end loop; <br>\n  $\\quad$end loop; <br>\n  end loop; <br></p>\n</blockquote>\n\n<p>Can anyone figure out an easy way or a formula to do this in general. Please explain.</p>\n', 'Tags': '<algorithm-analysis><loops>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-29T09:52:14.510', 'CommentCount': '0', 'AcceptedAnswerId': '7009', 'CreationDate': '2012-11-29T08:20:58.547', 'Id': '7008''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm trying to understand a proof regarding radix-sort but to no avail.</p>\n\n<p>I'll first write down a brief summary of the proof and then assign some questions which I hope will be clarified enough.</p>\n\n<blockquote>\n  <p>Suppose you have an array of integer numbers in the range $\\{0,1,2,\\ldots n \\log n\\}$.\n  Show and explain an effective algorithm in the worst case which finds out if there are two elements with the same value.\n  note - You can use an extra memory in order of magnitude equals to $O(n)$.</p>\n</blockquote>\n\n<p>I don't really care about how the algorithm will look like. The idea to this proof is using radix-sort in $O(n)$ and then looking for two elements with the same value as the array is sorted.</p>\n\n<p><strong>The proof outline:</strong></p>\n\n<p>Let's suppose we are examining a larger domain which is $\\{0,1,2 ,\\ldots n^2 - 1\\}$.\nNow we'll treat each number according to its binary representation using radix-sort bit by bit.</p>\n\n<p>Right after this, comes the part I can't understand at all....</p>\n\n<p>As we know the order of magnitude of radix-sort is $\\Theta(d(n+k))$ and therefore all we have to decide is which a to choose to have order of magnitude equals to $O(n)$.</p>\n\n<p>using the formula $\\frac{2\\log n}{a}  (n + 2^a)$.</p>\n\n<p>After this step, you just choose $a = \\log n$ and you are done.</p>\n\n<p><strong>My questions are:</strong></p>\n\n<ol>\n<li><p>How did the writer concluded the following formula: \n$\\frac{2\\log n}{a}  (n + 2^a)$?</p></li>\n<li><p>Also, why do the writer prefer to work on the domain of $\\{0,1,2 ,\\ldots n^2 - 1\\}$ instead the one given in the question?</p></li>\n</ol>\n", 'ViewCount': '310', 'Title': 'Radix sort exercise', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-02T08:16:39.277', 'LastEditDate': '2012-12-01T09:36:18.030', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '7100', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4514', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2012-11-30T18:38:03.813', 'Id': '7051''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am doing research on control flow analysis on aspect oriented programs and I found in some papers an interprocedural approach for doing control flow analysis on others call graph or control flow graph. Is there a real difference between control flow graphs and interprocedural control flow graphs?</p>\n', 'ViewCount': '363', 'Title': 'What is the difference between control flow graph & interprocedural control flow graph?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-01T16:49:00.093', 'LastEditDate': '2012-12-01T14:24:14.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7085', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4817', 'Tags': '<terminology><algorithm-analysis><programming-languages>', 'CreationDate': '2012-11-30T22:48:51.100', 'Id': '7054''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>According to a book I am reading, the unary representation of a number exponentially larger than a base k representation of it. I, however, feel that the unary representation should scale linearly with the input.</p>\n\n<p>After all, 1 is 1, 2 is 11, 3 is 111, and so on, right? Wouldn't that be linear?</p>\n", 'ViewCount': '159', 'Title': 'Why is the unary representation of a number exponentially larger than a base k representation of it?', 'LastActivityDate': '2012-12-01T00:55:51.167', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7063', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3003', 'Tags': '<algorithm-analysis>', 'CreationDate': '2012-12-01T00:49:42.677', 'Id': '7062''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>What is the best known approximation for the computational complexity of the clique problem? Is it accurate to consider it $O(2^n)$?</p>\n', 'ViewCount': '116', 'Title': 'Computational complexity of the clique problem', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-03T08:10:55.423', 'LastEditDate': '2012-12-03T07:57:07.407', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7113', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4846', 'Tags': '<complexity-theory><time-complexity><algorithm-analysis>', 'CreationDate': '2012-12-03T00:18:28.903', 'Id': '7112''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/6597/proof-of-linear-search">Proof of linear search?</a>  </p>\n</blockquote>\n\n\n\n<p>I\'m reading the MIT Press, Introduction to Algorithms textbook 3rd edition, and I am a bit confused by an exercise. </p>\n\n<p>2.1-3\nConsider the searching problem:\nInput: A sequence of n numbers A =  and a value v.\nOutput: An index i such that v = A[i] or the special value NIL if v does not appear in A.</p>\n\n<p>Write pseudocode for linear search, which scans through the sequence looking for v. Using a loop invariant, prove that your algorithm is correct. Make sure your loop invariant fulfills the three necessary properties.</p>\n\n<p>Earlier in the book I read that "We must show three things about a loop invariant:</p>\n\n<p>Initialization: It is true prior to the first iteration of the loop.</p>\n\n<p>Maintenance: If it is true before an iteration of the loop, it remains true before the\nnext iteration.</p>\n\n<p>Termination: When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.\n"</p>\n\n<p>I wrote this as my pseudocode</p>\n\n<pre><code>index = nil\nj = 1 \nwhile j &lt;= A.length and index == nil\n  if A[j] == v\n    index = j\n  j = j + 1\n</code></pre>\n\n<p>But when it comes to proving using a loop invariant I have no idea what to do. What do I even use as a loop invariant? To be honest I\'m not really clear about the entire loop invariant concept. Can someone please help me understand this? </p>\n\n<p>Thanks.</p>\n', 'ViewCount': '38', 'ClosedDate': '2012-12-04T17:03:05.747', 'Title': 'loop invariant proof', 'LastEditorUserId': '4648', 'LastActivityDate': '2012-12-04T03:56:29.240', 'LastEditDate': '2012-12-04T03:56:29.240', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'the.alch3m1st', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><loop-invariants>', 'CreationDate': '2012-12-03T20:11:11.017', 'Id': '7138''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Given a graph $G = (V,E)$, where $|V| = n$. What is a fast algorithm for generating the collection of all 2-hop neighborhood lists of all nodes in $V$. </p>\n\n<p>Naively, you can do that in $O(n^3)$. With power of matrices, you can do that with $O(n^{2.8})$ using Strassen algorithm. You can do better than this using another matrix multiplication algorithm. Any better method ? Any Las Vegas algorithm ? </p>\n', 'ViewCount': '422', 'Title': 'Algorithm to find all 2-hop neighbors lists in a graph', 'LastActivityDate': '2012-12-06T15:48:01.947', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '7183', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '867', 'Tags': '<algorithms><algorithm-analysis><graphs><randomized-algorithms>', 'CreationDate': '2012-12-05T04:50:09.537', 'Id': '7169''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Sorry if this question is very simplistic; I'm just starting out and I'm trying to wrap my head around all this asymptotic bound stuff. When trying to find the upper bound for the worst case of a function does it need to take into account what the meat of the code actually does? I have some code that would (in the worst case) iterate through a while loop n times, but when you consider what that code actually does, it would always make it so that the condition for the while loop becomes false on the next iteration.</p>\n\n<p>Some people say that it doesn't matter what is actually happening within the code; just that if it has the ability to iterate n times (even though it's virtually impossible because of the body of the loop) then that would be the worst case vs. however many steps the code ACTUALLY runs. </p>\n\n<p>If anyone has any insight it would be greatly appreciated! Thanks!</p>\n", 'ViewCount': '144', 'Title': 'Input to make worst case on big O not possible?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-06T06:06:04.333', 'LastEditDate': '2012-12-06T06:06:04.333', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4888', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2012-12-05T21:47:04.640', 'Id': '7198''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>When in a Parallel algorithm we say:</p>\n\n<blockquote>\n  <p>"This algorithm is done in $O(1)$ time using $O(n\\log n)$ work, with $n$-exponential probability, or alternatively, in $O(\\log n)$ time using $O(n)$ work, with $n$-exponential probability."</p>\n</blockquote>\n\n<p>Then Can we Implement this algorithm for a Quad-Core Computer (and just 4 threads) with $n=100,000$?</p>\n\n<p>The other question is what is the "$n$-exponential probability" in this sentence?</p>\n\n<p>Thanks.</p>\n', 'ViewCount': '152', 'Title': 'A question about parallel algorithm complexity', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-13T14:09:52.017', 'LastEditDate': '2012-12-13T14:09:52.017', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5058', 'Tags': '<algorithm-analysis><runtime-analysis><parallel-computing>', 'CreationDate': '2012-12-13T08:38:02.557', 'FavoriteCount': '0', 'Id': '7371''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In their book <a href="http://www.amazon.co.uk/Randomized-Algorithms-Cambridge-International-Computation/dp/0521474655/ref=sr_1_1?ie=UTF8&amp;qid=1356380410&amp;sr=8-1"><em>Randomized Algorithms</em>,</a> Motwani and Raghavan open the introduction with a description of their RandQS function -- Randomized quicksort -- where the pivot, used for partitioning the set into two parts, is chosen at random.</p>\n\n<p>I have been racking my (admittedly somewhat underpowered) brains over this for some time, but I haven\'t been able to see what advantage this algorithm has over simply picking, say, the middle element (in index, not size) each time.</p>\n\n<p>I suppose what I can\'t see is this: if the initial set is in a random order, what is the difference between picking an element at a random location in the set and picking an element at a fixed position?</p>\n\n<p>Can someone enlighten me, in fairly simple terms? </p>\n', 'ViewCount': '1069', 'Title': 'What is the advantage of Randomized Quicksort?', 'LastActivityDate': '2014-05-03T22:52:13.613', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7583', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '5178', 'Tags': '<algorithm-analysis><sorting><randomized-algorithms>', 'CreationDate': '2012-12-24T20:26:07.713', 'FavoriteCount': '1', 'Id': '7582''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '356', 'Title': 'Finding maximum and minimum of consecutive XOR values', 'LastEditDate': '2012-12-28T21:15:49.023', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5218', 'FavoriteCount': '1', 'Body': '<p>Given an integer array (maximum size 50000), I have to find the minimum and maximum $X$ such that $X = a_p \\oplus a_{p+1} \\oplus \\dots \\oplus a_q$ for some $p$, $q$ with $p \\leq q$.</p>\n\n<p>I have tried this process: $\\text{sum}_i = a_0 \\oplus a_1 \\oplus  \\dots \\oplus a_i$ for all $i$. I pre-calculated it in $O(n)$ and then the value of $X$ for some $p$, $q$ such that $(p\\leq q)$ is: $X = \\text{sum}_q \\oplus \\text{sum}_{p-1}$. Thus:</p>\n\n<p>$$\n\\mathrm{MinAns} = \\min_{(p,q) \\text{ s.t. } p\\le q}{\\text{sum}_q \\oplus \\text{sum}_{p-1}} \\\\\n\\mathrm{MaxAns} = \\max_{(p,q) \\text{ s.t. } p\\le q}{\\text{sum}_q \\oplus \\text{sum}_{p-1}} \\\\\n$$</p>\n\n<p>But this process is of $O(n^2)$. How can I do that more efficiently?</p>\n', 'Tags': '<algorithms><algorithm-analysis><performance><binary-arithmetic><arithmetic>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-29T05:03:58.663', 'CommentCount': '1', 'AcceptedAnswerId': '7640', 'CreationDate': '2012-12-28T03:12:04.687', 'Id': '7622''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I saw in <a href="http://www.youtube.com/watch?v=7ayAEHS1-F8" rel="nofollow">this</a> video that computing clustering coefficient of central node  of a star graph using the following algorithm is $\\Theta(n^2)$ and for a clique it is $\\Theta(n^3)$. is that correct?</p>\n\n<pre><code>def clustering_coefficient(G,v):\n    neighbors = G[v].keys()\n    if len(neighbors) == 1: return 0.0\n    links = 0.0\n    for w in neighbors:\n        for u in neighbors:\n            if u in G[w]: links += 0.5\n    return 2.0*links/(len(neighbors)*(len(neighbors)-1))\n</code></pre>\n', 'ViewCount': '255', 'Title': 'Computing the clustering coefficient', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-28T17:39:00.513', 'LastEditDate': '2012-12-28T12:03:27.460', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7626', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5202', 'Tags': '<graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-12-28T10:30:04.533', 'Id': '7624''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>This is a snippet from some pseudocode for a sorting algorithm. In it, the symbol \u2190 is used to denote assignment, for example for the variable <code>done</code>. However, in the <code>while</code> loop the statement <code>done:= false</code> is written. I would assume it is also an assignment statement but I suspect it means somethings else, or perhaps extra, since if not, the \u2190 would have simple be used again.</p>\n\n<pre><code>Algorithm MyAlgorithm(A, n) \n    Input: Array of integer containing n elements \n    Output: Possibly modified Array A\n\ndone \u2190 true \nj \u2190 0\nwhile j \u2264 n - 2 do\n     if A[j] &gt; A[j + 1] then\n        swap(A[j], A[j + 1])\n        done:= false\n     j \u2190 j + 1\n</code></pre>\n', 'ViewCount': '123', 'Title': u'Difference between := and \u2190 in pseudocode', 'LastActivityDate': '2013-01-17T17:20:36.793', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '8996', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-01-17T17:08:05.570', 'Id': '8995''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>From what I've read, Big O is the absolute worst ever amount of complexity an algorithm will be given an input. On the side, Big Omega is the best possible efficiency, i.e. lowest complexity.</p>\n\n<p>Can it be said then that every algorithm has a complexity of  $O(\\infty)$ since infinite complexity is the worst ever possible?\nBy the same token, can it be said that every algorithm is $\\Omega(1)$ since the absolute lowest complexity an algorithm can be is a constant?</p>\n", 'ViewCount': '208', 'Title': "Is every algorithm's complexity $\\Omega(1)$ and $O(\\infty)$?", 'LastEditorUserId': '55', 'LastActivityDate': '2013-01-18T05:27:12.653', 'LastEditDate': '2013-01-18T05:27:12.653', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '9000', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics>', 'CreationDate': '2013-01-17T18:16:04.023', 'Id': '8998''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Finding Big-O is pretty straightforward for an algorithm where $f(n)$ is</p>\n\n<p>$$f(n) = 3n^4 + 6n^3 + 10n^2 + 5n + 4$$</p>\n\n<p>The lower powers of $n$ simply fall off because in the long run $3n^4$ outpaces all of them. And with $g(n) = 3n^4$ we say $f(n)$ is $O(n^4)$.</p>\n\n<p>But what would Big-O be if instead of 3 we were given a really small constant, for example \n$$f(n) = 0.0000000001n^4 + 6n^3 + 10n^2 + 5n + 4$$</p>\n\n<p>Would we still say $f(n)$ is $O(n^4)$?</p>\n', 'ViewCount': '115', 'Title': 'Big-O complexity when c is a tiny fraction', 'LastActivityDate': '2013-01-18T06:39:20.660', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '9003', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<complexity-theory><algorithm-analysis>', 'CreationDate': '2013-01-17T21:17:33.200', 'Id': '9002''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '267', 'Title': 'Questions about amortised analysis', 'LastEditDate': '2013-01-19T18:04:21.067', 'AnswerCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '5053', 'FavoriteCount': '3', 'Body': '<p>As a preperation of an exam about algorithms and complexity, I am currently solving old exercises. One concept I have already been struggling with when I encountered it for the first time is the concept of amortised analysis. What is amortised analysis and how to do it? In our lecture notes, it is stated that "amortised analysis gives bounds for the "average time" needed for certain operation and it can also give a bound for the worst case". That sounds really useful but when it comes to examples, I have no idea what I have to do and even after having read the sample solution, I have no idea what they are doing.</p>\n\n<blockquote>\n  <p>Let\'s add up 1 in base 2, i.e. 0, 1, 10, 11, 100, 101, 110, 111, 1000, ... Using amortised analysis, show that in each step only amortised constantly many bits need to be changed.</p>\n</blockquote>\n\n<p>(the exercise originally is in German, so I apologise for my maybe not perfectly accurate translation)</p>\n\n<p>Now the standard solution first defines $\\phi(i) := c \\cdot \\# \\{\\text{1-bits in the binary representation}\\}$ for some constant $c &gt; 0$. I think this is what is called the potential function which somehow corresponds to the excessive units of time (but I have no idea why I would come up with this particular definition). Assuming that we have to change $m$ bits in the $i$-th step. Such a step always is of the form</p>\n\n<p>$$\\dots \\underbrace{0 1 \\dots 1}_m \\to \\dots \\underbrace{1 0 \\dots 0}_m.$$</p>\n\n<p>This statement is understandable to me, however again I fail to see the motivation behind it. Then, out of nowhere, they come up with what they call an "estimate"</p>\n\n<p>$$a(i) = m + c(\\phi(i) - \\phi(i-1)) = m + c(-m + 2)$$</p>\n\n<p>and they state that for $c=1$, we get $a(i)=2$ which is what we had to show.</p>\n\n<p>What just happened? What is $a(i)$? Why can we choose $c=1$? In general, if I have to show that in each step only amortised constantly many "units of time" are needed, does that mean that I have to show that $a(i)$ is constant?</p>\n\n<p>There are a few other exercises regarding amortised analysis and I don\'t understand them either. I thought if someone could help me out with this one, I could give the other exercises another try and maybe that\'ll help me really grasp the concept.</p>\n\n<p>Thanks a lot in advance for any help.</p>\n', 'Tags': '<algorithm-analysis><proof-techniques><amortized-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-21T09:05:29.557', 'CommentCount': '2', 'AcceptedAnswerId': '9023', 'CreationDate': '2013-01-18T10:20:25.633', 'Id': '9021''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Based on the definition of a <a href="http://mathworld.wolfram.com/Multiset.html" rel="nofollow">multiset</a> and the information in this <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.145.8728" rel="nofollow">paper</a>, why do we use multisets in proving the termination of a program?\nIs not the well-founded order enough?</p>\n', 'ViewCount': '126', 'Title': 'The use of multiset ordering in proving termination', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-21T10:55:18.597', 'LastEditDate': '2013-01-21T10:55:18.597', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1584', 'Tags': '<algorithm-analysis><proof-techniques><correctness-proof><sets>', 'CreationDate': '2013-01-19T23:46:45.707', 'Id': '9046''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Concerning the Master Theorem. I have found the following equation as the base of analysis:  </p>\n\n<p>$\\quad T(n) = aT(n/b) + \\Theta(n^k)$  </p>\n\n<p>but I also found the following:  </p>\n\n<p>$\\quad T(n) = aT(n/b) + \\Theta(n^k\\cdot\\log_p n)$  </p>\n\n<p>where the base $p$ is a real number.  </p>\n\n<p>Can anyone explain the second equation? I understand the proof with the first equation but can not understand the second formula.</p>\n', 'ViewCount': '130', 'Title': 'Explanation of a specific recurrence with respect to Master Theorem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-01T03:32:14.113', 'LastEditDate': '2013-01-20T15:26:39.403', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6487', 'Tags': '<algorithm-analysis><asymptotics><recurrence-relation><master-theorem>', 'CreationDate': '2013-01-20T10:42:04.463', 'Id': '9055''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '138', 'Title': 'Example for the analysis of a recursive function', 'LastEditDate': '2013-01-26T18:10:30.180', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5222', 'FavoriteCount': '1', 'Body': "<pre><code>l is a matrix of size [1...n, 1...n]\n\nfunction: rec(i,j)\n  if (i*j == 0)\n    return 1\n  else\n    if (l[i,j] == 0)\n      l[i,j] = 1 * rec(i-1,j) + 2 * rec(i,j-1) + 3 * rec(i-1,j-1)\n    return l[i,j]\nend_function\n\nfor i=1 to n\n  for j=1 to n\n    l[i,j] = 0\n\nrec(n,n)\n</code></pre>\n\n<p>The nested for's are O(n<sup>2</sup>). But i have difficulties to analyse the recursive part. There is another variation of this example with l as 3d. And the essential part of 3drec function is defined as:</p>\n\n<pre><code>if (l[i,j,k] == 0)\n  l[i,j,k] = 2 * rec(i-1,j,k) + 2 * rec(i,j-1,k) + 2 * rec(i,j,k-1)\n</code></pre>\n\n<p>Anyway let's think about the 2d version again. I thought something like that (that's the running time for the whole code including the nested loops):</p>\n\n<p>T(n) = T(n-1, n<sup>2</sup>) + T(n, n-1<sup>2</sup>) + T(n-1<sup>2</sup>, n-1<sup>2</sup>)</p>\n\n<p>And i'm stuck here. Besides i don't know if i did right till this point.</p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-26T22:54:57.413', 'CommentCount': '4', 'AcceptedAnswerId': '9172', 'CreationDate': '2013-01-26T01:11:04.723', 'Id': '9162''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>When I am reading a paper I found a notation $\\operatorname{poly}( |F|,n,$$\\frac{1}{\\epsilon}) $.\nIts not clear to me that what this notation represents. Can you please help me out?</p>\n', 'ViewCount': '97', 'Title': 'What is meant by $\\operatorname{poly}(|F|, n, e)$?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-30T17:20:24.550', 'LastEditDate': '2013-01-30T17:20:24.550', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '9298', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<terminology><algorithm-analysis>', 'CreationDate': '2013-01-30T04:51:59.990', 'Id': '9295''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<blockquote>\n  <p><strong>Problem:</strong> Consider a graph $G = (V, E)$ on $n$ vertices and $m &gt; n$ edges, $u$ and $v$ are two vertices of $G$.</p>\n  \n  <p>What is the asymptotic complexity to calculate the shortest path from $u$ to $v$ with Dijkstra\'s algorithm using <a href="http://en.wikipedia.org/wiki/Binary_heap" rel="nofollow">Binary Heap</a> ?</p>\n</blockquote>\n\n<p>To clarify, Dijkstra\'s algorithm is run from the source and allowed to terminate when it reaches the target. Knowing that the target is a neighbor of the source, what is the time complexity of the algorithm?</p>\n\n<p><strong>My idea:</strong></p>\n\n<p>Dijkstra\'s algorithm in this case makes $O(n)$ <strong>inserts</strong> ( $n$ if the graph is complete) and 1 <strong>extract min</strong> in the binary heap, before calculate the shortest path from $u$ to $v$.</p>\n\n<p>In a binary heap insert costs $O(\\log n)$ and extract min $O(\\log n)$ too.</p>\n\n<p>So the cost in my opinion is $O(n \\cdot \\log n + \\log n) = O(n \\log n)$</p>\n\n<p>But the answer is $\\Theta(n)$, so there is something wrong in my thinking.</p>\n\n<p>Where is my mistake?</p>\n', 'ViewCount': '793', 'Title': "What's the complexity of calculating the shortest path from $u$ to $v$ with Dijkstra's algorithm using binary heap?", 'LastEditorUserId': '3011', 'LastActivityDate': '2013-01-30T17:46:35.080', 'LastEditDate': '2013-01-30T17:41:32.873', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '9319', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<algorithms><graphs><algorithm-analysis><runtime-analysis><shortest-path>', 'CreationDate': '2013-01-30T16:17:56.463', 'Id': '9314''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>An assignment question asks me to find the complexity of a [tail] recursive algorithm, copied below. While I understand all the complexity specifics, for example that the while loop's complexity is $n-1$ and the complexity of setting $j$ to $0$ is 1, I don't understand how I could trace the code recursively, that is within itsel - it's too hard to keep track of. </p>\n\n<p>What I tried doing, is turning the algorithm into an iterative one, by simply putting all the code into a big while loop and thus avoiding the recursive call. But I'm not sure if this affects the complexity of the original algorithm.</p>\n\n<pre><code>Algorithm MyAlgorithm(A, n) \n   Input: Array of integer containing n elements \n   Output: Possibly modified Array A\n     done \u2190 true \n     j \u2190 0\n     while j \u2264 n - 2 do {\n       if A[j] &gt; A[j + 1] then {\n       swap(A[j], A[j + 1])\n       done:= false\n       }\n     j \u2190 j + 1\n     end while\n     j \u2190 n - 1\n     while j \u2265 1 do\n       if A[j] &lt; A[j - 1] then\n       swap(A[j - 1], A[j])\n       done:= false\n    j \u2190 j - 1\n    end while\n    if \xac done\n       MyAlgorithm(A, n)\n    else\n      return A\n</code></pre>\n", 'ViewCount': '278', 'Title': 'Finding the complexity of a recursive method', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-01T10:51:52.300', 'LastEditDate': '2013-02-01T10:51:52.300', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-01-31T16:59:09.883', 'Id': '9360''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>If they were all linked to make a condition such as ($1 &lt; i &lt; j &lt; k &lt; n$), I know how to solve, but the last loop is disconnected so I have no clue on how to do these...</p>\n\n<p>the ones like</p>\n\n<pre><code>for(i = 1 to n);\n   for(j = i to n);\n      x++;\n</code></pre>\n\n<p>I can use $1 \\leq i \\leq j \\leq n$ and use $x_1 + x_2 + x_3 = n-1$ and find out the general solution which is $\\frac{n(n+1)}{2}$. But disconnected loops I have no idea. Consider the following for loops.</p>\n\n<pre><code>    for(i = 1 to n);\n       for(j = i to n);\n          for(k = 1 to i*n);\n              x++; (constant time)\n\n\n    for(i = 1 to n-1);\n        for(j = i+1 to n);\n            for(k = 1 to j);\n                x++; (constant time)\n</code></pre>\n\n<p>I need to find the general solution.</p>\n', 'ViewCount': '228', 'Title': 'Analyzing programs with multiple for-loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-08T14:56:44.287', 'LastEditDate': '2013-02-04T12:24:30.640', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '9606', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6695', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2013-02-04T02:45:11.513', 'Id': '9461''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p><strong>Input:</strong>\nA set of $\\ell$ arrays $A_i$ (of numbers). \n<br>\nThe elements within each array are in sorted order, but the set of arrays is not necessarily sorted. The arrays are not necessarily the same size. The total number of elements is $n$.</p>\n\n<p><strong>Output:</strong>\nThe $k$th smallest element out of all elements in the input.</p>\n\n<p>What's the most efficient algorithm for this problem?</p>\n\n<p>Is it possible, for example to achieve a running time of $O(\\ell + \\log n)$?</p>\n", 'ViewCount': '278', 'Title': 'Find the median of a list of lists', 'LastEditorUserId': '71', 'LastActivityDate': '2013-02-13T00:06:56.700', 'LastEditDate': '2013-02-06T01:21:59.107', 'AnswerCount': '4', 'CommentCount': '8', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-02-04T23:55:34.930', 'FavoriteCount': '1', 'Id': '9497''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm an autodidact who works best with a mix of theory and implementation examples. I'm having a hard time finding resources on implementing algorithms and data structures in Python (2.7-3.X  preferably). I found a text ...mastering basic algorithms in the python language but I find it to be far too introductory and thus cluttered with unnecessary prose, which makes it for me difficult to move efficiently over it. What I have in mind is a text or github gist or whatever really, that has some coded python imps of what I've determined to be foundational data-structures in algorithmic's(not to sure on the usage/conjugation of that). Specifically I'm looking for trees, traversal, dynamic and greedy algorithms and the like. If this question is a repeat, or adequately answered elsewhere, links are adequate replies.  </p>\n", 'ViewCount': '49', 'Title': 'Looking for non-entry level implementation of foundational algorithms and data structures in python. where to look?', 'LastEditorUserId': '6830', 'LastActivityDate': '2013-02-12T00:03:28.820', 'LastEditDate': '2013-02-11T23:23:51.723', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9697', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6830', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-02-11T20:44:52.050', 'Id': '9691''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>So I have the following pseudo-code:</p>\n\n<pre><code>Function(n):\nfor (i = 4 to n^2):\n    for (j = 5 to floor(3ilog(i))):\n        // Some math that executes in constant time\n</code></pre>\n\n<p>So far, I know that to find this i will be calculating</p>\n\n<p>$\\sum_{i=4}^{n^2}\\sum_{j=5}^{3i \\log_{2}i}C$ where $C$ is a constant, but I am completely lost as to how to proceed past the first summation which, if I'm not mistaken, will give me $3C \\cdot (\\sum_{i=4}^{n^2}(i \\log_{2}i) - 5(n^2 - 4))$, but from here I'm lost.  I don't need exact running time, but the asymptotic complexity.</p>\n\n<p>All help is greatly appreciated!  I realize that this might be a duplicate, but I haven't been able to find a nested for loop problem of this nature anywhere...</p>\n", 'ViewCount': '468', 'Title': 'Running time of a nested loop with $\\sum i \\log i$ term', 'LastEditorUserId': '4751', 'LastActivityDate': '2013-02-12T22:39:49.373', 'LastEditDate': '2013-02-12T22:39:49.373', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '9716', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6847', 'Tags': '<time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-02-12T19:55:39.130', 'Id': '9714''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am concerned with the question of <strong>the asymptotic running time of the Ukkonen\'s algorithm</strong>, perhaps the most popular algorithm for constructing <strong>suffix trees</strong> in linear (?) time.</p>\n\n<p>Here is a citation from the book "Algorithms on strings, trees and sequences" by Dan Gusfield (section 6.5.1):</p>\n\n<blockquote>\n  <p>"... the Aho-Corasick, Weiner, <strong>Ukkonen</strong> and McCreight algorithms all either require $\\Theta(m|\\Sigma|)$ space, or the $O(m)$ time bound should be replaced with the minimum of $O(m \\log m)$ and $O(m \\log|\\Sigma|)$".</p>\n  \n  <p><em>[$m$ is the string length and $\\Sigma$ is the size of the alphabet]</em></p>\n</blockquote>\n\n<p>I don\'t understand why that is true.</p>\n\n<ul>\n<li><strong>Space:</strong> well, in case we represent branches out of the nodes using arrays of size $\\Theta(|\\Sigma|)$, then, indeed, we end up with $\\Theta(m|\\Sigma|)$ space usage. However, as far as I can see, it is also possible to store the branches using hash tables (say, dictionaries in Python). We would then have only $\\Theta(m)$ pointers stored in all hash tables altogether (since there are $\\Theta(m)$ edges in the tree), while still being able to access the children nodes in $O(1)$ time, as fast as when using arrays.</li>\n<li><strong>Time</strong>: as mentioned above, using hash tables allows us to access the outgoing branches of any node in $O(1)$ time. Since the Ukkonen\'s algorithm requires $O(m)$ operations (including accessing children nodes), the overall running time then would be also $O(m)$.</li>\n</ul>\n\n<p>I would be very grateful to you for any hints on why I am wrong in my conclusions and why Gusfield is right about the dependence of the Ukkonen\'s algorithm on the alphabet.</p>\n', 'ViewCount': '238', 'Title': "How does the runtime of the Ukkonen's algorithm depend on the alphabet size?", 'LastEditorUserId': '162', 'LastActivityDate': '2013-02-18T14:59:52.107', 'LastEditDate': '2013-02-16T08:05:47.417', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings>', 'CreationDate': '2013-02-15T22:05:13.680', 'FavoriteCount': '1', 'Id': '9820''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In Corman, Introduction To Algorithms, 3rd edition, question 2-4 it asks to count the number of inversions in a list of numbers in $\\theta( n \\lg n )$ time.  He uses a modified Merge Sort to accomplish this.  However, there is something in his algorithm which seems redundant / unnecessary to me:</p>\n\n<pre><code>MERGE-INVERSIONS(A, p, q, r)\nn1 = q - p + 1\nn2 = r - q\nlet L[1 ... n1 + 1] and R[1 ... n2 + 1] be new arrays\nfor i = 1 to n1\n    L[i] = A[p + i - 1]\nfor j = 1 to n2\n    R[j] = A[q + j] \nL[n1 + 1] = infinity\nR[n2 + 1] = infinity\ni = 1\nj = 1\ninversions = 0\ncounted = FALSE\nfor k = p to r\n    if counted == FALSE and R[j]  &lt; L[i]\n        inversions = inversions + n1 - i + 1\n        counted = TRUE\n    if L[i] &lt;= R[j] \n        A[k] = L[i]\n        i++\n    else A[k] = R[j] \n        j++\n        counted = FALSE\nreturn inversions\n</code></pre>\n\n<p>The <code>counted</code> variable seems redundant to me and I would have written the last for loop as follows:</p>\n\n<pre><code>inversions = 0\nfor k = p to r\n    if L[i] &lt;= R[j] \n        A[k] = L[i]\n        i++\n    else A[k] = R[j] \n        inversions = inversions + n1 - i + 1\n        j++\nreturn inversions\n</code></pre>\n\n<p>What am I missing, or is <code>counted</code> really unnecessary?</p>\n', 'ViewCount': '959', 'Title': 'Counting Inversions Using Merge Sort', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-17T11:56:28.703', 'LastEditDate': '2013-02-17T11:38:20.870', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9861', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><algorithm-analysis><sorting><program-correctness>', 'CreationDate': '2013-02-17T10:23:24.240', 'Id': '9858''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '1038', 'Title': 'Worst case analysis of bucket sort using insertion sort for the buckets', 'LastEditDate': '2013-02-18T22:03:28.823', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6815', 'FavoriteCount': '1', 'Body': '<p>Suppose I am using the <a href="http://en.wikipedia.org/wiki/Bucket_sort#Pseudocode" rel="nofollow">Bucket-Sort</a> algorithm, and on each bucket/list I sort with insertion sort (replace nextSort with insertion sort in the wikipedia pseudocode).</p>\n\n<p>In the worst case, this would imply that we would have $O(n^2)$ performance, because if every element was in one bucket, then we would have to use insertion sort on $n$ elements which is $O(n^2)$. </p>\n\n<p>So the first thing that comes to mind to fix the worst case running time is to not use insertion-sort, because it is $O(n^2)$. Instead we could use merge-sort or heap-sort m, because the worst case running time for both of those algorithms is $O(n\\log n)$. However, if we use merge-sort and heap-sort, do they preserve the expected linear running-time of bucket-sort?</p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-18T22:03:28.823', 'CommentCount': '1', 'AcceptedAnswerId': '9882', 'CreationDate': '2013-02-18T01:16:55.900', 'Id': '9876''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '933', 'Title': 'Algorithm to find the mode in a unimodal array', 'LastEditDate': '2013-02-18T06:27:00.790', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4492', 'FavoriteCount': '1', 'Body': u'<p>I am given the following problem in an Algorithms class:</p>\n\n<blockquote>\n  <p>Assume that you are given an array A[1 . . . n] of distinct numbers.\n  You are told that the sequence of numbers in the array is unimodal, in\n  other words, there is an index i such that the sequence A[1 . . . i]\n  is increasing (A[j] &lt; A[j + 1] for 1 \u2264 j &lt; i), and the sequence A[i .\n  . . n] is decreasing. The index i is called the mode of A. Give an\n  O(log n) algorithm that find the mode of A</p>\n</blockquote>\n\n<p>I have written this draft solution as my solution but I want to make sure that this is an acceptable CORRECT solution.</p>\n\n<p>My Algorithm:</p>\n\n<pre><code>FIND_MODE(A)\nn = A.length\nif n == 1\n    return 1\n\nmid = floor(n/2)\nif A[mid] &lt; A[mid+ 1]\n    return FIND_MODE(A[1 \u2026 mid])\nelse\n    return mid + FIND_MODE(A[mid+1 \u2026 n])\n</code></pre>\n\n<p>Is it this acceptable and correct pseudocode algorithm?</p>\n\n<p>Is it correct that this is a Big-O(log n) algorithm?</p>\n', 'Tags': '<algorithms><algorithm-analysis><arrays>', 'LastEditorUserId': '4492', 'LastActivityDate': '2013-02-18T21:58:16.793', 'CommentCount': '0', 'AcceptedAnswerId': '9890', 'CreationDate': '2013-02-18T06:14:07.823', 'Id': '9888''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to analyze the running time of a bad implementation of generating the $n$th member of the fibonacci sequence (which requires generating the previous 2 values from the bottom up).</p>\n\n<p>Why does this algorithm have a time complexity of $\\Omega(2^{\\frac{n}{2}})$? Where does the exponent come from?</p>\n', 'ViewCount': '852', 'Title': 'Why does a recurrence of $T(n - 1) + T(n - 2)$ yield something in $\\Omega(2^{\\frac{n}{2}})$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-19T23:48:57.897', 'LastEditDate': '2013-02-19T06:22:05.190', 'AnswerCount': '4', 'CommentCount': '5', 'AcceptedAnswerId': '9908', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2013-02-18T19:02:47.990', 'Id': '9899''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have this confusion related to the time complexity of FFT. I was reading this book related to Design and Analysis of Algorithms and I came across FFT.</p>\n\n<p>It says that lets say I have a polynomial of degree n-1. I want to evaluate the polynomial at $2n^{th}$ roots of unity. For that I can use divide and conquer rule</p>\n\n<p>I will divide my polynomial into evens and odds i.e,</p>\n\n<p>$A(x) = A_{even}(x^2) + xA_{odd}(x^2)$</p>\n\n<p>Now if I want to evaluate A at one of the  $2n^{th}$ roots of unity. I can break it into evaluating the $n^{th}$ root of unity at two polynomials $A_{even}$ and $A_{odd}$ and then add the results with complexity O(n).</p>\n\n<p>They have shown the results to be O(nlogn). However, I think this is for evaluating the value of the polynomial at one of the roots not all the $2n^{th}$.  But the book seems to say it is the total complexity. I am a bit confused.</p>\n\n<p>Can anyone please explain this to me? I am confused</p>\n', 'ViewCount': '33', 'Title': 'Confusion related to time complexity of fast fourier transform', 'LastEditorUserId': '157', 'LastActivityDate': '2013-02-23T22:36:17.030', 'LastEditDate': '2013-02-23T21:43:22.033', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6999', 'Tags': '<algorithm-analysis><fourier-transform>', 'CreationDate': '2013-02-23T21:22:04.400', 'Id': '10037''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>I have some confusion related to a divide and conquer problem. Here is the problem</p>\n\n<p>You\u2019re consulting for a small computation-intensive investment company, and they have the following type of problem that they want to solve over and over. A typical instance of the problem is the following. They\u2019re doing a simulation in which they look at n consecutive days of a given stock, at some point in the past. Let\u2019s number the days i=1,2,...,n; for each day i, they have a price p(i) per share for the stock on that day. (We\u2019ll assume for simplicity that the price was fixed during each day.) Suppose during this time period, they wanted to buy 1,000 shares on some day and sell all these shares on some (later) day. They want to know: When should they have bought and when should they have sold in order to have made as much money as possible? (If\nthere was no way to make money during the n days, you should report this instead.)</p>\n\n<p>For example, suppose n = 3, p(1) = 9, p(2) = 1, p(3) = 5. Then you should return \u201cbuy on 2, sell on 3\u201d (buying on day 2 and selling on day 3 means they would have made $4 per share, the maximum possible for that period).</p>\n\n<p>Clearly, there\u2019s a simple algorithm that takes time $O(n^2)$: try all possible pairs of buy/sell days and see which makes them the most money. Your investment friends were hoping for something a little better.</p>\n\n<p>Show how to find the correct numbers i and j in time O(n log n).</p>\n\n<p>Solution We\u2019ve seen a number of instances in this chapter where a brute- force search over pairs of elements can be reduced to O(n log n) by divide and conquer. Since we\u2019re faced with a similar issue here, let\u2019s think about how we might apply a divide-and-conquer strategy.</p>\n\n<p>A natural approach would be to consider the first n/2 days and the final\nn/2 days separately, solving the problem recursively on each of these two\nsets, and then figure out how to get an overall solution from this in O(n) time.\nThis would give us the usual recurrence T (n) \u2264 2T(n/2) + O(n), and hence \n\ufffcO(n log n).</p>\n\n<p>Also, to make things easier, we\u2019ll make the usual assumption that n is a power of 2. This is no loss of generality: if n\u2032 is the next power of 2 greater than n, we can set p(i) = p(n) for all i between n and n\u2032. In this way, we do not change the answer, and we at most double the size of the input (which will not affect the O() notation).</p>\n\n<p>Now, let S be the set of days 1,...,n/2, and S\u2032 be the set of days n/2+ 1, . . . , n. Our divide-and-conquer algorithm will be based on the following observation: either there is an optimal solution in which the investors are holding the stock at the end of day n/2, or there isn\u2019t. Now, if there isn\u2019t, then the optimal solution is the better of the optimal solutions on the sets S and S\u2032. If there is an optimal solution in which they hold the stock at the end of day n/2, then the value of this solution is p(j) \u2212 p(i) where i \u2208 S and j \u2208 S\u2032. But this value is maximized by simply choosing i \u2208 S which minimizes p(i), and choosing j \u2208 S\u2032 which maximizes p(j).</p>\n\n<p>Thus our algorithm is to take the best of the following three possible solutions.</p>\n\n<pre><code>. The optimal solution on S.\n. The optimal solution on S\u2032.\n. The maximum of p(j)\u2212p(i), over i\u2208S and j\u2208S\u2032.\n</code></pre>\n\n<p>The first two alternatives are computed in time T(n/2), each by recursion, and the third alternative is computed by finding the minimum in S and the\nmaximum in S\u2032, which takes time O(n). Thus the running time T(n) satisfies\nT(n) \u2264 2T(n/2) + O(n),\nas desired.</p>\n\n<p>Can anyone explain whats going on here? I didn't get how this algorithm actually works. I know they are dividing the days into first half and second half. But I didn't get this part specially </p>\n\n<p><strong>Our divide-and-conquer algorithm will be based on the following observation: either there is an optimal solution in which the investors are holding the stock at the end of day n/2, or there isn\u2019t. Now, if there isn\u2019t, then the optimal solution is the better of the optimal solutions on the sets S and S\u2032. If there is an optimal solution in which they hold the stock at the end of day n/2, then the value of this solution is p(j) \u2212 p(i) where i \u2208 S and j \u2208 S\u2032. But this value is maximized by simply choosing i \u2208 S which minimizes p(i), and choosing j \u2208 S\u2032 which maximizes p(j).</strong></p>\n", 'ViewCount': '228', 'Title': 'Confusion related to a divide and conquer problem', 'LastActivityDate': '2013-02-24T11:25:31.807', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6999', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-02-24T09:27:01.003', 'Id': '10050''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a quick question on the bubble sort algorithm. Why does it perform $\\Theta(n^2)$ comparisons on an $n$ element list?</p>\n\n<p>I looked at the Wikipedia page and it does not seem to tell me. I know that because of its magnitude it takes a lot of work with large numbers.</p>\n', 'ViewCount': '541', 'Title': 'Why does bubble sort do $\\Theta(n^2)$ comparisons on an $n$ element list?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-27T12:14:10.343', 'LastEditDate': '2013-02-25T07:26:15.987', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '10061', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2013-02-24T18:04:04.793', 'Id': '10058''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '852', 'Title': 'What is the complexity of this matrix transposition?', 'LastEditDate': '2013-02-26T02:44:32.997', 'AnswerCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '5391', 'FavoriteCount': '1', 'Body': "<p>I'm working on some exercises regarding graph theory and complexity. \nNow I'm asked to give an algorithm that computes a transposed graph of $G$, $G^T$ given the adjacency matrix of $G$. So basically I just have to give an algorithm to transpose an $N \\times N$ matrix.</p>\n\n<p>My first thought was to loop through all rows and columns and simply swapping values in each of the $M[i,j]$ place. Giving a complexity of $O(n^2)$ But I immediately realized there's no need to swap more than once, so I can skip a column every time e.g. when I've iterated over row i, there's no need to start iteration of the next row at column i, but rather at column i + 1.</p>\n\n<p>This is all well and good, but how do I determine the complexity of this. When I think about a concrete example, for instance a 6x6 matrix this leads to 6 + 5 + 4 + 3 + 2 + 1 swaps (disregarding the fact that position [i,i] is always in the right position if you want to transpose a $N \\times N$ matrix, so we could skip that as well).\nThis looks alot like the well-known arithmetic series which simplifies to $n^2$, which leads me to think this is also $O(n^2)$. There are actually $n^2/2$ swaps needed, but by convention the leading constants may be ignored, so this still leads to $O(n^2)$. Skipping the i,i swaps leads to $n^2/2 - n$ swaps, which still is $O(n^2)$, but with less work still..</p>\n\n<p>Some clarification would be awesome :)</p>\n", 'Tags': '<graph-theory><time-complexity><algorithm-analysis><linear-algebra><adjacency-matrix>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-25T22:31:32.400', 'CommentCount': '3', 'AcceptedAnswerId': '10082', 'CreationDate': '2013-02-25T13:54:53.913', 'Id': '10081''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '2134', 'Title': 'What are the characteristics of an $O(n \\log n)$ time complexity algorithm?', 'LastEditDate': '2013-02-26T07:32:50.337', 'AnswerCount': '6', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '7007', 'FavoriteCount': '3', 'Body': '<p>Sometimes it\'s easy to identify the time complexity of an algorithm my examining it carefully. Algorithms with two nested loops of $N$ are obviously $N^2$. Algorithms that explore all the possible combinations of $N$ groups of two values are obviously $2^N$.</p>\n\n<p>However I don\'t know how to "spot" an algorithm with $O(N \\log N)$ complexity. A recursive mergesort implementation, for example, is one. What are the common characteristics of mergesort or other $O(N \\log N)$ algorithms that would give me a clue if I was analyzing one?</p>\n\n<p>I\'m sure there is more than one way an algorithm can be of $O(N \\log N)$ complexity, so any and all answers are appreciated. BTW I\'m seeking general characteristics and tips, not rigorous proofs.</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><intuition>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-26T19:08:10.450', 'CommentCount': '5', 'AcceptedAnswerId': '10102', 'CreationDate': '2013-02-25T21:02:00.667', 'Id': '10091''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In a recitation video for <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=P7frcB_-g4w">MIT OCW 6.006</a> at 43:30, </p>\n\n<p>Given an $m \\times n$ matrix $A$ with $m$ columns and $n$ rows, the 2-D peak finding algorithm, where a peak is any value greater than or equal to it\'s adjacent neighbors, was described as:</p>\n\n<p><em>Note: If there is confusion in describing columns via $n$, I apologize, but this is how the recitation video describes it and I tried to be consistent with the video.  It confused me very much.</em></p>\n\n<blockquote>\n  <ol>\n  <li><p>Pick the middle column $n/2$ // <em>Has complexity $\\Theta(1)$</em></p></li>\n  <li><p>Find the max value of column $n/2$ //<em>Has complexity  $\\Theta(m)$ because there are $m$ rows in a column</em></p></li>\n  <li><p>Check horiz. row neighbors of max value, if it is greater then a peak has been found, otherwise recurse with $T(n/2, m)$ //<em>Has complexity $T(n/2,m)$</em></p></li>\n  </ol>\n</blockquote>\n\n<p>Then to evaluate the recursion, the recitation instructor says</p>\n\n<blockquote>\n  <p>$T(1,m) =  \\Theta(m)$ because it finds the max value</p>\n  \n  <p>$$ T(n,m) =  \\Theta(1) +  \\Theta(m) + T(n/2, m) \\tag{E1}$$</p>\n</blockquote>\n\n<p>I understand the next part, at 52:09 in the video, where he says to treat $m$ like a constant, since the number of rows never changes.  But I don\'t understand how that leads to the following product:</p>\n\n<p>$$ T(n,m) = \\Theta(m) \\cdot \\Theta(\\log n) \\tag{E2}$$</p>\n\n<p>I think that, since $m$ is treated like a constant, it is thus treated like $\\Theta(1)$ and eliminated in $(E1)$ above.  But I\'m having a hard time making the jump to $(E2)$.  Is this because we are now considering the case of $T(n/2)$ with a constant $m$?</p>\n\n<p>I think can "see" the overall idea is that a $\\Theta(\\log n)$ operation is performed, at worst, for m number of rows.  What I\'m trying to figure out is how to describe the jump from $(E1)$ to $(E2)$ to someone else, i.e. gain real understanding.</p>\n', 'ViewCount': '168', 'Title': '2-D peak finding complexity (MIT OCW 6.006)', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-28T14:11:09.913', 'LastEditDate': '2013-02-27T22:09:59.223', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '7062', 'Tags': '<algorithms><algorithm-analysis><asymptotics><matrices>', 'CreationDate': '2013-02-27T18:22:56.113', 'FavoriteCount': '0', 'Id': '10141''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose I have a admissible and consistent heuristic.</p>\n\n<p>Is it true, that when I expand a node, I have guaranteed that the path I found to this node is optimal?</p>\n\n<p>Look at this pseudocode from wikipedia:</p>\n\n<pre><code>function A*(start,goal)\n closedset := the empty set    // The set of nodes already evaluated.\n openset := {start}    // The set of tentative nodes to be evaluated, initially containing the start node\n came_from := the empty map    // The map of navigated nodes.\n\n g_score[start] := 0    // Cost from start along best known path.\n // Estimated total cost from start to goal through y.\n f_score[start] := g_score[start] + heuristic_cost_estimate(start, goal)\n\n while openset is not empty\n     current := the node in openset having the lowest f_score[] value\n     if current = goal\n         return reconstruct_path(came_from, goal)\n\n     remove current from openset\n     add current to closedset\n     for each neighbor in neighbor_nodes(current)\n         tentative_g_score := g_score[current] + dist_between(current,neighbor)\n         if neighbor in closedset\n             if tentative_g_score &gt;= g_score[neighbor]\n                 continue\n\n         if neighbor not in openset or tentative_g_score &lt; g_score[neighbor] \n             came_from[neighbor] := current\n             g_score[neighbor] := tentative_g_score\n             f_score[neighbor] := g_score[neighbor] + heuristic_cost_estimate(neighbor, goal)\n             if neighbor not in openset\n                 add neighbor to openset\n\n return failure\n</code></pre>\n\n<p>I suppose it should be true. Because of this:</p>\n\n<pre><code>if current = goal\n     return reconstruct_path(came_from, goal)\n</code></pre>\n\n<p>If it wasn't true then this test would not guarantee me that the solution is optimal right?</p>\n\n<p>What I don't get and the reason I am asking this question is this:</p>\n\n<pre><code>if neighbor in closedset\n         if tentative_g_score &gt;= g_score[neighbor]\n             continue\n</code></pre>\n\n<p>If the neighbor is in closed list, it means that it has already been expanded. Why are they testing the scores then? Why would not the next condition work?</p>\n\n<pre><code>if neighbor in closedset\n         continue\n</code></pre>\n", 'ViewCount': '97', 'Title': 'A* optimality of the expanded node', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-29T23:28:22.880', 'LastEditDate': '2013-04-02T07:37:43.930', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '16559', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7075', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><correctness-proof>', 'CreationDate': '2013-02-28T15:36:33.660', 'Id': '10152''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have an algorithm which, basically given an array of $n$ numbers, checks if there is any repeated numbers in the array, and returns true if there is and false otherwise.</p>\n\n<p>It uses a direct access table (hashing function $h(x)=x$), which makes the running time linear. So it creates a new array, initializes all values to false, then iterates through the given array, and since each value is an index to the hash table, it accesses that array location and changes it to $1$. If it is already a $1$, then you know that it is a repeated number.</p>\n\n<p>But when getting the expected running time, you need to first define a probability space. This is the part I am confused about. How can you define a probability of a number being repeated in A, if you are just given an array with arbitrary numbers? </p>\n', 'ViewCount': '93', 'Title': 'How to get the expected running time of an algorithm', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-06T17:51:22.880', 'LastEditDate': '2013-03-06T17:51:22.880', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-03-06T15:35:43.183', 'FavoriteCount': '1', 'Id': '10320''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a recurrence relation which is like the following:<p></p>\n\n<p>$T(n) = 2T(\\frac{n}{2}) + \\log_{2}n$</p>\n\n<p>I am using recursion tree method to solve this. And at the end, i came up with the following equation:<p></p>\n\n<p>$T(n)=(2\\log_{2}n)(n-1)-(1\\times 2 + 2\\times 2^{2} + \\ldots + k\\times2^{k})$ where $k=\\log_{2}n$</p>\n\n<p>I am trying to find a theta notation for this equation. But i cannot find a closed formula for the sum $(1\\times 2 + 2\\times 2^{2} + \\ldots + k\\times2^{k})$. How can I find a big theta notation for $T(n)$? </p>\n', 'ViewCount': '250', 'Title': 'Need help about solving a recurrence relation', 'LastEditorUserId': '1636', 'LastActivityDate': '2013-03-07T20:27:23.937', 'LastEditDate': '2013-03-07T00:27:04.883', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7175', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation><recursion>', 'CreationDate': '2013-03-07T00:10:55.800', 'FavoriteCount': '2', 'Id': '10346''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I can think of functions such as $n^2 \\sin^2 n$ that don't have asymptotically tight bounds,  but are there actually common algorithms in computer science that don't have asymptotically tight bounds on their worst case running times?</p>\n", 'ViewCount': '205', 'Title': 'Common Algorithms without Asymptotically Tight Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T11:27:03.257', 'LastEditDate': '2013-03-07T11:27:03.257', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '10355', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-07T08:39:31.110', 'Id': '10354''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've learned that a while loop such as </p>\n\n<pre><code>int i = 100;\nwhile (i &gt;= 1){\n     ...\n     ///Stuff\n     i = i/2\n}\n</code></pre>\n\n<p>will run in logarithmic time, specifically, <code>O(logn)</code>, since it keeps dividing in half each time (like a binary search).</p>\n\n<p>However, what if my while loop looks like this</p>\n\n<pre><code> int i = 100;\n    while (i &gt;= 1){\n         ...\n         ///Stuff\n         i = i/3\n    }\n</code></pre>\n\n<p>Is the complexity still <code>O(logn)</code>?</p>\n\n<p>Can someone explain yes/no and why?</p>\n", 'ViewCount': '1248', 'Title': 'Complexity of a while loop that divides by parameter by three each iteration', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-08T20:48:01.123', 'LastEditDate': '2013-03-08T20:48:01.123', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '-3', 'OwnerDisplayName': 'Imray', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-08T19:15:47.000', 'Id': '10389''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm trying to determine the complexity of this for loop:</p>\n\n<pre><code>for (int j =3; j &lt;= n-2; j+=2) {\n   ....\n}\n</code></pre>\n\n<p>By trying out lots of examples, I came up with $\\frac{n-4}{2} + 1$. This seems to work with every number now.</p>\n\n<p>However, I am looking for a systematic and quick way to find these sorts of complexities. For example, can you show me the proper way to find the complexity for the above loop?</p>\n", 'ViewCount': '135', 'Title': 'Complexity of slightly tricky for loop', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-10T17:54:06.147', 'LastEditDate': '2013-03-08T20:48:09.873', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-03-08T20:07:39.920', 'Id': '10391''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I am trying to understand how to do the amortized cost for a dynamic table. Suppose we are using the accounting method.</p>\n\n<p>Let A of size m be an array of n elements. When $n = m$, then we create a new array of size $4m$, and then copy the elements of A to the new array. When $n = \\frac{m}{4}$, then you create a new array of size $\\frac{m}{4}$, and copy the elements to that array.</p>\n\n<p>What I am confused about is how to calculate the costs.\nFrom what I know so far:\nBefore the first expansion, you pay two dollars to insert. <code>1$</code> for the insert, and <code>1$</code> you just store with the element, so that you can use that later for a copy operation.</p>\n\n<p>Then when you expand it, you use that stored <code>$</code> to move the element to the new array.\nNow in the new array the elements won't have any <code>$</code> with them. But now as you insert a new element, you use <code>3$</code>. <code>1$</code> for the insert, then one more for itself (for a future copy), and one more for the previous element that was just copied.</p>\n\n<p>The problem here is, what if you have an array like this:</p>\n\n<p><code>1$ 2$</code></p>\n\n<p>Then insert an element</p>\n\n<p><code>1$ 2 3$ _ _ _ _ _</code></p>\n\n<p>Now how do you handle a delete operation?</p>\n", 'ViewCount': '207', 'Title': 'How to compute amoritized cost for a dynamic array?', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-05T18:24:05.000', 'LastEditDate': '2013-05-05T18:24:05.000', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<data-structures><algorithm-analysis><amortized-analysis>', 'CreationDate': '2013-03-09T01:47:07.637', 'Id': '10399''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to calculate the amortized cost of a dynamic array, that\'s size becomes 4 times the size when the array is filled. (when you re-size, you create a new one and copy the elements there).</p>\n\n<p><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-13-amortized-algorithms-table-doubling-potential-method/lec13.pdf" rel="nofollow">Here</a> is what I am reading from. (starts at pg. 30)\nThis example has the array doubling when it is filled.</p>\n\n<p>This is my potential function analysis so far:\nBut in the end I am getting 7-2i, I don\'t think it can be like that. I think the i\'s should cancel out.</p>\n\n<p>Does anyone know whats wrong?</p>\n\n<p>Potential of the array after the $i^{th}$ insertion is $\\Phi(D_i) = 4i - 4^{\\left\\lceil \\log_{4}i\\right\\rceil}$.</p>\n\n<p>Assume $4^{\\left\\lceil \\log_{4}0\\right\\rceil} = 0$.</p>\n\n<p>The amortized cost of the $i^{th}$ insertion is:</p>\n\n<p>$\\^c_i = c_i + \\Phi(D_i)-\\Phi(D_{i-1})$</p>\n\n<p>$~~~= \\{ i$ if $i-1$ is an exact power of $4$</p>\n\n<p>$~~~~~~~\\{ 1$ otherwise</p>\n\n<p>$~~~~~~~+ (4i-4^{\\left\\lceil \\log_{4}i\\right\\rceil})-(4(i-1)-4^{\\left\\lceil \\log_{4}\n(i-1)\\right\\rceil})$</p>\n\n<p>$~~~= \\{ i$ if $i-1$ is an exact power of $4$</p>\n\n<p>$~~~~~~~\\{ 1$ otherwise</p>\n\n<p>$~~~~~~~+ 4-4^{\\left\\lceil \\log_{4}i\\right\\rceil}+4^{\\left\\lceil \\log_{4}(i-1)\\right\\rceil}$</p>\n\n<p>Case 1 ($i-1$ is an exact power of $4$):</p>\n\n<p>$\\^c_i = i + 4-4^{\\left\\lceil \\log_{4}i\\right\\rceil}+4^{\\left\\lceil \\log_{4}(i-1)\\right\\rceil}$</p>\n\n<p>$~~~= i + 4-4(i-1)+(i-1)$</p>\n\n<p>$~~~= i + 4-4i+4+i-1$</p>\n\n<p>$~~~= 7-2i$</p>\n', 'ViewCount': '25', 'ClosedDate': '2013-03-12T10:25:30.603', 'Title': 'Potential function in amortized analysis', 'LastActivityDate': '2013-03-12T02:25:07.893', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><amortized-analysis>', 'CreationDate': '2013-03-12T02:25:07.893', 'Id': '10463''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Would it be correct to characterize loop invariants as a type of tautology?  I ask since the invariant must basically always be true, before the loop starts, before each iteration and after the loop terminates.  I realize that there is the possibility that the invariant could become false during the body of the loop.  But since inside the loop "doesn\'t count" is it fair to characterize the invariant as a tautology?</p>\n', 'ViewCount': '92', 'Title': 'Loop Invariants as Tautologies', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T21:25:07.547', 'LastEditDate': '2013-03-12T14:08:56.163', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '10471', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithm-analysis><logic><correctness-proof><loop-invariants><program-correctness>', 'CreationDate': '2013-03-12T07:57:56.437', 'Id': '10469''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much for an expected running time. Am I making a mistake here?</p>\n", 'ViewCount': '321', 'Title': 'How to get expected running time of hash table?', 'LastActivityDate': '2013-03-12T23:38:43.887', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '10500', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><hash-tables><hash><probabilistic-algorithms>', 'CreationDate': '2013-03-12T23:20:41.050', 'Id': '10498''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p><strong>Definition:</strong> A preserved invariant of a state machine is a predicate, $P$, on\nstates, such that whenever $P(q)$ is true of a state, $q$, and $q  \\rightarrow r$ for some state, $r$,\nthen $P(r)$ holds.</p>\n\n<p><strong>Definition:</strong> A line graph is a graph whose edges are all on one path.</p>\n\n<p><strong>Definition:</strong> Formally, a state machine is nothing more than a binary relation on a set, except\nthat the elements of the set are called \u201cstates,\u201d the relation is called the transition\nrelation, and an arrow in the graph of the transition relation is called a transition.\nA transition from state $q$ to state $r$ will be written $q \\rightarrow r$.</p>\n\n<p><strong>DAG</strong>: Directed Acylic Graph</p>\n\n<blockquote>\n  <p>The following procedure can be applied to any directed graph, $G$:</p>\n  \n  <ol>\n  <li>Delete an edge that is in a cycle.</li>\n  <li>Delete edge $&lt;u \\rightarrow v&gt;$ if there is a path from vertex $u$ to vertex $v$ that does not\n  include $&lt;u \\rightarrow v&gt;$.</li>\n  <li>Add edge $&lt;u \\rightarrow v&gt;$ if there is no path in either direction between vertex $u$ and\n  vertex $v$.</li>\n  </ol>\n  \n  <p>Repeat these operations until none of them are applicable.</p>\n</blockquote>\n\n<p>This procedure can be modeled as a state machine. The start state is $G$, and the\nstates are all possible digraphs with the same vertices as $G$.</p>\n\n<p><strong>(b)</strong> Prove that if the procedure terminates with a digraph, $H$, then $H$ is a line\ngraph with the same vertices as $G$.</p>\n\n<p>Hint: Show that if $H$ is not a line graph, then some operation must be applicable.</p>\n\n<p><strong>(c)</strong> Prove that being a DAG is a preserved invariant of the procedure.</p>\n\n<p><strong>(d)</strong> Prove that if $G$ is a DAG and the procedure terminates, then the walk relation\nof the final line graph is a topological sort of $G$.</p>\n\n<p>Hint: Verify that the predicate\n$P(u,v)$:: there is a directed path from $u$ to $v$\nis a preserved invariant of the procedure, for any two vertices $u, \\ v$ of a DAG.</p>\n\n<p><strong>(e)</strong> Prove that if $G$ is finite, then the procedure terminates.</p>\n\n<p>Hint: Let $s$ be the number of cycles, $e$ be the number of edges, and $p$ be the number\nof pairs of vertices with a directed path (in either direction) between them. Note\nthat $p \\leq n^2$ where $n$ is the number of vertices of $G$. Find coefficients $a,b,c$ such\nthat as+bp+e+c is nonnegative integer valued and decreases at each transition.</p>\n\n<blockquote>\n  <p><em><strong>My Problems:</em></strong></p>\n  \n  <p>I got stuck with problems $d$ and $e$ but solutions to other problems are welcome too.</p>\n  \n  <p>At problem  $d$, <strong>I could not understand the hint and why it is given, how it helps</strong>. </p>\n</blockquote>\n\n<p>In my way for proving $d$, I am trying to show that given procedure always preserves the order of vertices, which are associated with edges, on the start graph $G$. So a line graph is automatically a topological sort since the "precedence order" of the vertices are preserved. </p>\n\n<blockquote>\n  <p>But procedure number $3$ is problematic, <strong>how to show it preserves precedence ?</strong></p>\n</blockquote>\n', 'ViewCount': '486', 'Title': 'A procedure for Topological sort, proof for its correctness', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-18T19:45:06.263', 'LastEditDate': '2013-09-20T15:21:45.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7337', 'Tags': '<algorithms><algorithm-analysis><sorting><correctness-proof>', 'CreationDate': '2013-03-23T20:42:41.853', 'Id': '10720''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>The worst case running time of insertion sort is $\\Theta(n^2)$, we don\u2019t write it as $O(n^2)$.</p>\n\n<p>$O$-notation is used to give upper bound on function. If we use it to bound a worst case running time of insertion sort, it implies that $O(n^2)$ is upper bound of algorithm no matter what type of input is, means it doesn\u2019t matter whether input is sorted, unsorted, reverse sorted, have same values, etc the upper bound will be same $O(n^2)$. But this is not the case of insertion sort. Insertion sort running time depends on type of input used. So when the input is already sorted, it runs in linear time and doesn\u2019t take more that $O(n)$ time.</p>\n\n<p>Therefore to write insertion sort running time as $O(n^2)$ is technically not good.</p>\n\n<p><strong>We use $\\Theta$-notation to write worst case running time of insertion sort. But I\u2019m not able to relate properties of $\\Theta$-notation with insertion sort, why $\\Theta$-notation is suitable to insertion sort.If $f(n)$ belong to $\\Theta(g(n))$ we write it as $f(n)= \\Theta(g(n))$, then $f(n)$ must satisfies the properties. And properties state that there exits constants $c_1$, $c_2$ and $n_0$ such that $0$$\\leq$$c_1\\cdot g(n)$$\\leq$$f(n)$$\\leq$$c_2\\cdot g(n)$ For all $n&gt;n_0$. How does the insertion sort function lies between the $c_1\\cdot n^2$ and $c_2\\cdot n^2$ for all $n&gt;n_0$.</strong></p>\n\n<p>Running time of insertion sort as $\\Theta(n^2)$ implies that it has upper bound $O(n^2)$ and lower bound $\\Omega(n^2)$. I\u2019m confused as to whether the lower bound on insertion-sort is $\\Omega(n^2)$ or $\\Omega(n)$.</p>\n', 'ViewCount': '477', 'Title': 'Why is $\\Theta$ notation suitable to insertion sort to describe its worst case running time?', 'LastEditorUserId': '7384', 'LastActivityDate': '2013-04-29T21:19:54.897', 'LastEditDate': '2013-04-29T16:22:00.450', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'siddstuff', 'PostTypeId': '1', 'OwnerUserId': '7384', 'Tags': '<algorithms><time-complexity><algorithm-analysis><asymptotics>', 'CreationDate': '2013-03-25T06:59:09.077', 'Id': '10763''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In the original paper of A* algorithm, <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=a%20formal%20basis%20for%20the%20heuristic%20determination%20of%20minimum%20cost%20paths&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDYQFjAA&amp;url=http://fai.cs.uni-saarland.de/teaching/winter12-13/heuristic-search-material/Astar.pdf&amp;ei=m-NRUeOUMYa0kAXpgIHIDw&amp;usg=AFQjCNFp9yfCwE0_B_epYI4kPmEmZjOGww&amp;bvm=bv.44342787,d.dGI" rel="nofollow"><em>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</em></a>, the author proved the optimality of A* in <em>Theorem 2</em>, page 105.</p>\n\n<p>However, I cannot understand the proof. The assumption is that we have a node in $G_s$ which is not expanded by algorithm A, but in the proof we change the graph to $G_{n,\\theta}$, isn\'t it a problem?</p>\n', 'ViewCount': '96', 'Title': 'While proving optimality of the A* algorithm, why can we change graphs?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-27T13:07:35.247', 'LastEditDate': '2013-03-27T12:03:09.513', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7432', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><correctness-proof>', 'CreationDate': '2013-03-27T00:57:43.747', 'Id': '10817''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p><a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a> has a good cheat sheet, but however it does not involve no. of comparisons or swaps. (though no. of swaps is usually decides its complexity). So I created the following. Is the following info is correct ? Please let me know if there is any error, I will correct it.</p>\n\n<p><strong>Insertion Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case : $\\Theta(n^2)$ ; happens when input is\nalready sorted in descending order</li>\n<li>Best Case : $\\Theta(n)$ ; when input is already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(n)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in Best case</li>\n</ul>\n\n<p><strong>Selection Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best Case: $\\Theta(n^2)$ </li>\n<li>No. of comparisons : $\\Theta(n^2)$</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Merge Sort :</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best case : $\\Theta(nlgn)$ ; doesn\'t matter at all whether the input is sorted or not</li>\n<li>No. of comparisons : $\\Theta(n+m)$ in worst case &amp; $\\Theta(n)$ in best case ; assuming we are merging two array of size n &amp; m where $n&lt;m$</li>\n<li>No. of swaps : No swaps ! [but requires extra memory, not in-place sort]</li>\n</ul>\n\n<p><strong>Quick Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$ ; happens input is already sorted</li>\n<li>Best Case : $\\Theta(nlogn)$ ; when pivot divides array in exactly half</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(nlogn)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Bubble Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$</li>\n<li>Best Case : $\\Theta(n)$ ; on already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Linear Search:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n)$ ; search key not present or last element</li>\n<li>Best Case : $\\Theta(1)$ ; first element</li>\n<li>No. of comparisons : $\\Theta(n)$ in worst case &amp; $1$ in best case</li>\n</ul>\n\n<p><strong>Binary Search:</strong></p>\n\n<ul>\n<li>Worst case/Average case : $\\Theta(logn)$</li>\n<li>Best Case : $\\Theta(1)$ ; when key is middle element</li>\n<li>No. of comparisons : $\\Theta(logn)$ in worst/average case &amp; $1$ in best case</li>\n</ul>\n\n<hr>\n\n<ol>\n<li>I have considered only basic searching &amp; sorting algorithms. </li>\n<li>It is assumed above that sorting algorithms produce output in ascending order</li>\n<li>Sources : The awesome <a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">CLRS</a> and this <a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a></li>\n</ol>\n', 'ViewCount': '9952', 'ClosedDate': '2014-02-09T15:34:28.957', 'Title': 'Complexities of basic operations of searching and sorting algorithms', 'LastActivityDate': '2014-02-09T07:07:24.577', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><sorting><searching>', 'CreationDate': '2013-04-03T13:09:39.333', 'FavoriteCount': '1', 'Id': '10991''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>When clustering a set of data points, what exactly are the differences between <a href="http://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_c-means_clustering" rel="nofollow">Fuzzy C-Means</a> (aka Soft K-Means) and <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="nofollow">Expectation Maximization</a>?</p>\n\n<p>In slide 30 and 32 of <a href="http://eniac.cs.qc.cuny.edu/andrew/gcml-11/lecture10c.pptx" rel="nofollow">this lecture</a> I found, it says that Soft K-Means is a special case of EM in Soft K-Means only the means are re-estimated and not the covariance matrix, why\'s that and what are the advantages / disadvantages? How does covariance matrix affect the outcomes of EM?</p>\n\n<p>Another question about these two algorithms: When they converge, all the data points will be hard-assigned to a particular cluster if the probability of it being in the said cluster is highest, right?</p>\n', 'ViewCount': '444', 'Title': 'Differences between Fuzzy C-Means and EM', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-29T14:41:12.010', 'LastEditDate': '2013-08-29T14:41:12.010', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14017', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7329', 'Tags': '<algorithms><terminology><algorithm-analysis><machine-learning><statistics>', 'CreationDate': '2013-04-04T17:14:03.000', 'Id': '11022''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '85', 'Title': 'Prize collecting steiner tree', 'LastEditDate': '2013-04-07T11:49:54.243', 'AnswerCount': '1', 'Score': '4', 'OwnerDisplayName': 'Armin Meisterhirn', 'PostTypeId': '1', 'OwnerUserId': '7866', 'Body': '<p>I\'m reading about the <strong>prize collecting steiner tree</strong> problem and an approximation algorithm that uses randomization to set a lower bound on the optimal solution (see Chapter 5.7 in <a href="http://www.designofapproxalgs.com/book.pdf" rel="nofollow"> The Design of Approximation Algorithms </a> by Williamson and Shmoys). I don\'t understand the second line in the proof for Lemma 5.16: <img src="http://i.stack.imgur.com/9uldl.png" alt="Lemma 5.16">.</p>\n\n<p>It seems to me that $V-V(T)$ is a much larger set than $U$. So, how can the total penalty for this set be upper bounded by the total penalty of a set that is much smaller?</p>\n', 'Tags': '<algorithm-analysis><optimization><approximation><trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T11:49:54.243', 'CommentCount': '0', 'AcceptedAnswerId': '11070', 'CreationDate': '2013-04-04T00:13:44.200', 'Id': '11069''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '210', 'Title': 'Proving correctness of the algorithm for convex polygon minimum cost triangulation', 'LastEditDate': '2013-04-07T12:38:35.643', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7336', 'FavoriteCount': '1', 'Body': '<p>I have read many solutions for the minimum cost of triangulation problem and intuitively get the idea , however I am struggling to figure out how to prove it formally. I kind of feel that it has to be proven by induction but I struggle at choosing the right quantity to look at and also at the inductive step portion of the proof.</p>\n\n<p>For example, can you provide a formal proof for the algorithm described <a href="http://users.eecs.northwestern.edu/~dda902/336/hw6-sol.pdf" rel="nofollow">here</a> (page 5 problem 6.12).</p>\n', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming><recursion><correctness-proof>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:53:48.043', 'CommentCount': '2', 'AcceptedAnswerId': '11098', 'CreationDate': '2013-04-07T01:15:34.200', 'Id': '11085''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I\'ve been reading about hypercube connection template for parallel algorithms. The general scheme is explained in <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node124.html#SECTION04310000000000000000" rel="nofollow"><em>Designing and Building Parallel Programs</em> by Ian Foster</a> and it\'s pretty clear.</p>\n\n<p>What I don\'t understand is how it\'s applied on the merge sort <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node127.html" rel="nofollow">in \xa711.4</a> The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<p>The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<pre><code>procedure parallel_mergesort(myid, d, data, newdata)\nbegin\n  data = sequential_mergesort(data)\n  for dim = 1 to d\n    data = parallel_merge(myid, dim, data)\n  endfor\n  newdata = data\nend\n</code></pre>\n\n<p>Please, explain to me step by step, assuming we have an array of twelve elements $(3,1,5,7,4,2,8,9,4,2,7,5)$ and we\'ve broken this data to four processors like this: </p>\n\n<p>$\\qquad ((3,1,5),(7,4,2),(8,9,4),(2,7,5))$. </p>\n\n<p>What data will have each process after each iteration? I understand why we use the hybercube template in this algorithm, but why do we have exactly $i$ compare-exchanges at the $i$-th level? I mean, when $i=1$, we compare-exchange data from processes $1-2, 3-4, .. P-1, P$. That\'s not $1$, that\'s $P/2$? Do I misunderstand something?</p>\n', 'ViewCount': '282', 'Title': 'Parallel merge sort using hypercube connection template', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T20:43:03.117', 'LastEditDate': '2013-04-10T20:43:03.117', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'OwnerDisplayName': u'\u0418\u0433\u043e\u0440\u044c \u041c\u043e\u0440\u043e\u0437\u043e\u0432', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><parallel-computing><machine-models>', 'CreationDate': '2013-04-10T18:14:08.597', 'Id': '11205''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>A program takes as input a balanced binary search tree with $n$ leaf nodes and computes the value of a function $g(x)$ for each node $x$. If the cost of computing $g(x)$ is </p>\n\n<p>$\\qquad \\min(\\#\\text{leaves in } L(x), \\#\\text{leaves in } R(x))$</p>\n\n<p>for $L(x), R(x)$ the left resp. right subtree of $x$, then the worst-case time complexity of the program is</p>\n\n<ol>\n<li>$\\Theta(n)$</li>\n<li>$\\Theta(n \\log n)$</li>\n<li>$\\Theta(n^2)$ </li>\n<li>$\\Theta(n^2 \\log n)$</li>\n</ol>\n\n<p>I am actually looking for a subtle hint. </p>\n', 'ViewCount': '492', 'Title': 'Finding no. of leaf nodes for each node in a BST', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-22T14:55:46.203', 'LastEditDate': '2013-04-12T10:03:59.117', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '11254', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><binary-trees><search-trees>', 'CreationDate': '2013-04-12T08:03:52.183', 'Id': '11252''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '300', 'Title': "What's harder: Shuffling a sorted deck or sorting a shuffled one?", 'LastEditDate': '2013-04-14T15:00:30.950', 'AnswerCount': '2', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '4197', 'FavoriteCount': '1', 'Body': '<p>You have an array of $n$ distinct elements. You have access to a comparator (a black box function taking two elements $a$ and $b$ and returning true iff $a &lt; b$) and a truly random source of bits (a black box function taking no arguments and returning an independently uniformly random bit). Consider the following two tasks:</p>\n\n<ol>\n<li>The array is currently sorted. Produce a uniformly (or approximately uniformly) randomly selected permutation.</li>\n<li>The array consists of some permutation selected uniformly at random by nature. Produce a sorted array.</li>\n</ol>\n\n<p>My question is</p>\n\n<blockquote>\n  <p>Which task requires more energy asymptotically?</p>\n</blockquote>\n\n<p>I am unable to define the question more precisely because I don\'t know enough about the connection between information theory, thermodynamics, or whatever else is needed to answer this question. However, I think the question can be made well-defined (and am hoping someone helps me with this in an answer!).</p>\n\n<p>Now, algorithmically, my intuition is that they are equal. Notice that every sort is a shuffle in reverse, and vice versa. Sorting requires $\\log n! \\approx n \\log n$ comparisons, while shuffling, since it picks a random permutation from $n!$ choices, requires $\\log n! \\approx n \\log n$ random bits. Both shuffling and sorting require about $n$ swaps.</p>\n\n<p>However, I feel like there should be an answer applying <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer\'s principle</a>, which says that it requires energy to "erase" a bit. Intuitively, I think this means that sorting the array is more difficult, because it requires "erasing" $n \\log n$ bits of information, going from a low-energy, high-entropy ground state of disorder to a highly ordered one. But on the other hand, for any given computation, sorting just transforms one permutation to another one. Since I\'m a complete non-expert here, I was hoping someone with a knowledge of the connection to physics could help "sort" this out!</p>\n\n<p>(The question didn\'t get any answers on <a href="http://math.stackexchange.com/questions/359911/which-takes-more-energy-shuffling-a-sorted-deck-or-sorting-a-shuffled-one">math.se</a>, so I\'m reposting it here. Hope that is ok.)</p>\n', 'Tags': '<algorithms><algorithm-analysis><information-theory><entropy>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-21T14:19:51.400', 'CommentCount': '15', 'AcceptedAnswerId': '11452', 'CreationDate': '2013-04-14T03:49:03.497', 'Id': '11299''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Given a positively weighted DAG (directed acyclic graph) $D = (V,E)$, can you create a new non-weighted DAG $D'$ by converting each edge with weight $w(e) = x$ into x non-weighted edges and vertices? I believe this would take $O(|E|+W)$ time where $|E|$ is the number of edges and $W$ is the total weight of all edges. My concern is whether I can include this weight variable and still consider this algorithm to be in polynomial time.</p>\n\n<p>(NOTE: This algorithm may apply to all positively weighted graphs, not just DAGs.)</p>\n", 'ViewCount': '37', 'Title': 'Can you convert a positively weighted DAG into a non-weighted DAG in polynomial time?', 'LastActivityDate': '2013-04-15T22:05:19.823', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11344', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7715', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><polynomial-time>', 'CreationDate': '2013-04-15T20:19:10.183', 'Id': '11343''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Prove that if G is an undirected connected graph, then each of its edges is either in the depth-first search tree or is a back edge.</p>\n\n<p>Now, from intuition and in class lectures by Steven Skiena, I know that the above holds true, since it dives all the way down, and then throw a rope back to a previous vertex. I also know that DFS is great in finding cycles.</p>\n\n<p>However, my problem here is that I don't know how to <em>prove</em> that the edge is either a tree edge or a back edge.</p>\n", 'ViewCount': '1467', 'Title': 'Why does DFS only yield tree and back edges on undirected, connected graphs?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-25T09:39:45.153', 'LastEditDate': '2013-04-21T14:32:57.923', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><graphs><algorithm-analysis><graph-traversal>', 'CreationDate': '2013-04-20T19:03:21.160', 'FavoriteCount': '1', 'Id': '11438''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>This is NOT HW, this is from Skienas book, and I just couldn't solve it at all.</p>\n\n<p>Please give me a hand here, in understanding and solving it, thanks.</p>\n\n<p>Let G = (V, E) be a binary tree. The distance between two vertices in G is the length of the path connecting these two vertices, and the diameter of G is the maximal distance over all pairs of vertices. Give a linear-time algorithm to find the diameter of a given tree. (*)</p>\n\n<p>I figured I'd do a DFS, and increment on each node in terms of the depth of the tree</p>\n", 'ViewCount': '51', 'ClosedDate': '2013-04-21T23:06:21.807', 'Title': 'LInear time algorithm to find the diameter of a tree', 'LastEditorUserId': '139', 'LastActivityDate': '2013-04-21T23:43:53.940', 'LastEditDate': '2013-04-21T19:20:21.703', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><search-problem>', 'CreationDate': '2013-04-21T17:47:36.077', 'Id': '11470''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Can someone give me a hand here, I am new to backtracking, and preparing for an interview. I couldn't even attempt this question, please help.</p>\n\n<p>Describe a back tracking algorithm for efficiently listing all k-element subsets of <code>n</code> items.</p>\n\n<p>For <code>n = 5</code> the 3 element subsets are <code>(1,2,3), (1,2,4), (1,2,5), (1,3,4), (1,3,5), (1,4,5), (2,3,4), (2,3,5), (2,4,5), (3,4,5)</code></p>\n\n<p>In particular, I am interesting in first describing the solution vector representation to use, and then how I would partition the work among construct-candidates, is-a-solution and process-solution functions.</p>\n", 'ViewCount': '386', 'Title': 'Backtracking for listing k elements', 'LastActivityDate': '2013-04-23T03:54:03.443', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7807', 'Tags': '<algorithms><algorithm-analysis><backtracking>', 'CreationDate': '2013-04-23T02:18:34.563', 'Id': '11507''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>How to solve fractional knapsack in linear time? I found this on <a href="https://www.google.com.sg/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CC0QFjAA&amp;url=http://algo2.iti.kit.edu/sanders/courses/algdat03/sol12.pdf&amp;ei=UJt8UZ_tGMHVrQfk1ICQDg&amp;usg=AFQjCNFKeWMLX_Gr2Pu-wS4zzjCT-ESCkg&amp;bvm=bv.45645796,d.bmk&amp;cad=rja" rel="nofollow">Google</a> but don\'t really understand it. </p>\n\n<ol>\n<li>Choose element $r$ at random from $R$ (set of profit/weight ratios)</li>\n<li>Determine\n<ul>\n<li>$R_1 = \\{ p_i / w_i | p_i / w_i &gt; r, for 1 \\leq i \\leq n \\}, W_1 = \\sum_{i \\in R_1} w_i$</li>\n<li>$R_2 = \\{ p_i / w_i | p_i / w_i = r, for 1 \\leq i \\leq n \\}, W_2 = \\sum_{i \\in R_3} w_i$</li>\n<li>$R_3 = \\{ p_i / w_i | p_i / w_i &lt; r, for 1 \\leq i \\leq n \\}, W_3 = \\sum_{i \\in R_3} w_i$</li>\n</ul></li>\n<li>if $W_1 &gt; W$\n<ul>\n<li>recurse $R_1$ and return computed solution</li>\n</ul></li>\n<li>else\n<ul>\n<li>while (there\'s space in knapsack and $R_2$ is not empty)\n<ul>\n<li>add items from $R_2$</li>\n</ul></li>\n<li>if (knapsack gets full)\n<ul>\n<li>return items in $R_1$ and items just added from $R_2$</li>\n</ul></li>\n<li>else \n<ul>\n<li>reduce knapsack capacity by $W_1 + W_2$</li>\n<li>recurse on $R_3$ and return items in $R_1 \\cup R_2$</li>\n<li>add items returned from recursive call </li>\n</ul></li>\n</ul></li>\n</ol>\n\n<p>I don\'t get how it works, what $R$ and $W$ are supposed to represent ... can someone explain? Or maybe if you have another algorithm to propose? </p>\n', 'ViewCount': '605', 'Title': 'Fractional Knapsack in linear time', 'LastEditorUserId': '683', 'LastActivityDate': '2013-04-28T04:43:06.850', 'LastEditDate': '2013-04-28T04:43:06.850', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3099', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms>', 'CreationDate': '2013-04-28T04:03:32.447', 'Id': '11620''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>The exercises in a textbook I studied asks about the best case for shell sort. I have scribbled a derivation for the same along the margins almost two years ago. Basically I don't know if this was my own derivation or one copied from an authoritative source. </p>\n\n<p>I have elaborated upon the same below. Could you let me know if the reasoning is right here?</p>\n\n<ul>\n<li>The least number of comparisons occur when the data is completely sorted.</li>\n<li>For a particular value of the increment, say, $h_i$, each of the $h_i$ sub-sequences require at most one less comparison than the number of elements in the sub-sequence(as insertion sort is used) which is,${N \\over h_i} - 1$ ,where N is the total number of data items.</li>\n<li>For the given data in this situation $h_i \\times \\left (N \\over h_i - 1 \\right ) = N - h_i$ number of comparisons are needed as there are $h_i$ sub-sequences.</li>\n<li>If the increment sequence selected is has $k$ increments(such that $h_k = 1$), the total number of comparisons required would be $C(N) \\ge (N - h_i) + (N - h_2) + ... + (N - h_k) = kN - \\sum h_i = O(N)$</li>\n</ul>\n", 'ViewCount': '369', 'Title': 'Best case analysis for shell sort', 'LastActivityDate': '2013-05-03T02:56:18.547', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2980', 'Tags': '<algorithms><data-structures><algorithm-analysis><sorting>', 'CreationDate': '2013-05-03T02:56:18.547', 'FavoriteCount': '1', 'Id': '11749''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>How would I solve the following. </p>\n\n<p>An algorithm that is O(Lg_2 n) takes 10 seconds to execute on a particular computer when n=100, how long would you expect to take it when n=500?</p>\n\n<p>An algorithm that is O(n lg_2 n) takes 10 seconds to execute on a particular computer when n=100, how long would you expect to take it when n=500?</p>\n\n<p>For the first one would I do </p>\n\n<p>10/ln(100)/ln(2) times ln(500)/(ln2)?</p>\n', 'ViewCount': '32', 'ClosedDate': '2013-05-05T08:39:34.667', 'Title': 'Algorithmic analysis using log', 'LastActivityDate': '2013-05-05T03:30:53.273', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7002', 'Tags': '<algorithm-analysis>', 'CreationDate': '2013-05-04T22:27:12.923', 'Id': '11784''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am struggling to calculate the lower bounds of an algorithm. What is the right way to proceed.</p>\n\n<p>For eg, I have the following algorithm</p>\n\n<pre><code>For i=1, 2,...,n\n    For j=i+1, i+2,...,n\n        Add up array entries A[i] through A[j]\n    Store the result in B[i, j] Endfor\nEndfor\n</code></pre>\n\n<p>How do I calculate the lower bounds of this algorithm</p>\n', 'ViewCount': '56', 'Title': 'Finding the lower bounds of an algorithm', 'LastActivityDate': '2013-05-08T00:11:49.063', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8086', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-05-07T22:57:21.287', 'Id': '11868''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>I have this confusion related to solving this problem</p>\n\n<p>You\u2019vebeenworkingwithsomephysicistswhoneedtostudy,aspartof their experimental design, the interactions among large numbers of very small charged particles. Basically, their setup works as follows. They have an inert lattice structure, and they use this for placing charged particles at regular spacing along a straight line. Thus we can model their structure as consisting of the points $ \\{1, 2, 3, \\cdots , n\\}$ on the real line; and at each of these points $j$, they have a particle with charge $q_j$. (Each charge can be either positive or negative.)</p>\n\n<p>They want to study the total force on each particle, by measuring it and then comparing it to a computational prediction. This computational part is where they need your help. The total net force on particle $j$, by Coulomb\u2019s Law, is equal to</p>\n\n<p>$F_j = \\sum_{i&lt;j}\\frac{Cq_iq_j}{(j-i)^2} - \\sum_{i&gt;j}\\frac{Cq_iq_j}{(j-i)^2}$</p>\n\n<p>They\u2019ve written the following simple program to compute $F_j$ for all $j$:</p>\n\n<pre><code>For j = 1, 2,...,n \n   Initialize Fj to 0 \n   For i = 1, 2, ..., n\n\ufffc     If i &lt; jthen\n       Add Cqiqj/(j-i)^2 to Fj\n     Elseif i &gt; j then\n       \ufffcAdd \u2212Cqiqj/(j\u2212i)^2 to Fj\n     Endif \n   Endfor\n   Output Fj \nEndfor\n</code></pre>\n\n<p>It\u2019s not hard to analyze the running time of this program: each invocation of the inner loop, over i, takes $O(n)$ time, and this inner loop is invoked $O(n)$ times total, so the overall running time is $O(n^2)$.</p>\n\n<p>The trouble is, for the large values of n they\u2019re working with, the pro- gram takes several minutes to run. On the other hand, their experimental setup is optimized so that they can throw down n particles, perform the measurements, and be ready to handle n more particles within a few sec- onds. So they\u2019d really like it if there were a way to compute all the forces $F_j$ much more quickly, so as to keep up with the rate of the experiment.</p>\n\n<p>Help them out by designing an algorithm that computes all the forces $Fj$ in $O(n \\log n)$ time.</p>\n\n<p>I am sure that this problem is solved by convolution which takes time $O(n \\log n)$. However, I am not being able to proceed and see how it's converted to a problem related to convolution. Any suggestions?</p>\n", 'ViewCount': '429', 'Title': 'Solving a problem related to convolution', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-06-07T04:33:38.667', 'LastEditDate': '2013-05-08T02:45:27.943', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8086', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-05-08T02:20:07.617', 'FavoriteCount': '1', 'Id': '11875''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Given $n$ points in $\\mathbf{R}^2$, define the optimal Euclidean Steiner tree to be a minimum (Euclidean) length tree containing all $n$ points and any other subset of points from $\\mathbf{R}^2$.\nProve that each of the additional points must have degree 3, with all three angles being $120^\\circ$.</p>\n', 'ViewCount': '113', 'Title': 'Euclidean Steiner Tree Question in Approximation Algorithms', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-05-08T07:18:28.980', 'LastEditDate': '2013-05-08T07:18:28.980', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11881', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7300', 'Tags': '<algorithms><algorithm-analysis><computational-geometry><approximation><trees>', 'CreationDate': '2013-05-08T06:34:08.107', 'Id': '11880''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I want to know which is the best way to find the longest common subsequence of two strings</p>\n', 'ViewCount': '147', 'Title': 'Find the longest subsequence of two strings', 'LastEditorUserId': '8105', 'LastActivityDate': '2013-05-10T12:19:08.163', 'LastEditDate': '2013-05-10T12:19:08.163', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8105', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming><subsequences>', 'CreationDate': '2013-05-10T01:58:55.797', 'FavoriteCount': '2', 'Id': '11924''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have some confusion related to calculating the time complexity of this algorithm</p>\n\n<pre><code>opt(i)\n   for j=i:n\n      a = f(i,j) + opt(j+1)   \n   end\n</code></pre>\n\n<p>How is the running time of this algorithm $O(n^2)$?</p>\n', 'ViewCount': '81', 'Title': 'Confusion related to calculating time complexity', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-05-14T07:22:49.993', 'LastEditDate': '2013-05-14T07:01:15.480', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithm-analysis><loops>', 'CreationDate': '2013-05-14T04:55:26.697', 'Id': '12002''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I am trying to solve a problem of finding compatible jobs set using greedy algorithm. However, I am not sure if greedy algorithm can solve this problem or I need to perform another approach.</p>\n\n<p>I have a set of jobs with start and finish time and I want to find the smallest subset of this jobs such that all the jobs are incompatible with at least one job of this subset. And all the jobs in this subset are compatible</p>\n\n<p>Suppose</p>\n\n<pre><code>job  start   end\n1    1       3\n2    2       11\n3    4       6\n4    12       14\n</code></pre>\n\n<p>My required job set J is {2,4} since  all the jobs are incompatible with at least one job of the job set J. And all the jobs in the job set J are compatible. I tried using earliest deadline first and schedule but it doesn't work. Any suggestions?</p>\n\n<p>Am I going the right way?</p>\n", 'ViewCount': '171', 'Title': 'Solving a variant of interval scheduling problem', 'LastActivityDate': '2013-05-14T12:34:59.047', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8110', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms>', 'CreationDate': '2013-05-14T12:34:59.047', 'Id': '12018''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Hi I've read this lemma in my book:</p>\n\n<blockquote>\n  <p><strong>Lemma 2.1.</strong> Let $p(n) = \\sum_{i=0}^{k} a_in^i$ denote any polynomial and assume $a_k &gt; 0$. Then $p(n) \\in \\Theta(n^k)$</p>\n  \n  <p><strong>Proof.</strong> It suffices to show that $p(n) \\in O(n^k)$  and $p(n) \\in \\Omega(n^k)$. First observe that for $n &gt; 0$,\n  $$p(n) \\leq \\sum_{i=0}^{k} |a_i|n^i \\leq n^k \\sum_{i=0}^{k}|a_i|,$$\n  and hence $p(n) \\leq (\\sum_{i=0}^{k}|a_i|)n^k$ for all positive $n$. Thus $p(n) \\in O(n^k)$.</p>\n  \n  <p>Let $A = \\sum_{i=0}^{k-1}|a_i|$. For positive $n$ we have\n  $$p(n) \\geq a_kn^k -An^{k-1} = \\frac{a_k}{2}n^k + n^{k-1}(\\frac{a_k}{2}n - A)$$\n  and hence $p(n) \\geq (a_k/2)n^k$ for $n &gt; \\frac{2A}{a^k}$. We choose $c=a_k/2$ and $n_0 = 2A/a^k$ in the definition of $\\Omega(n^k)$, and obtain $p(n) \\in \\Omega(n^k)$.</p>\n</blockquote>\n\n<p>Can anyone explain me the part $p(n)\\in \\Omega(n^k)$ of the proof? Why should we divide $a_k\\cdot n^k$ by 2? Why can't we take $a_kn^k$ as coefficient of $n^k$? And how do we obtain that $n&gt;2A/a_k$?</p>\n", 'ViewCount': '118', 'Title': 'Understanding why the polynomial $p(n) = \\sum_{i=0}^{k} a_in^i$ is in $\\Theta(n^k)$', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-14T23:41:17.610', 'LastEditDate': '2013-05-14T21:26:38.743', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8173', 'Tags': '<algorithms><algorithm-analysis><proof-techniques>', 'CreationDate': '2013-05-14T17:34:14.640', 'Id': '12022''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>There is a famous <a href="http://en.wikipedia.org/wiki/Consensus_%28computer_science%29" rel="nofollow">Consensus Problem</a> in Distributed Computing.</p>\n\n<p>Let\'s consider and try to find the best possible algorithm for a simplified version of the consensus problem. </p>\n\n<p><strong>Assumptions:</strong> a process may undergo only crash failure (process abruptly stops and does not resume), process represent a complete graph.</p>\n\n<p><strong>Simplification</strong>: a crash failure may occur only between round, so there no a case when a process succeeds in sending some message and fails to send other message during the same round.</p>\n\n<p>The algorithm for the general case <strong><em>FloodSet</em></strong> when a process may crash during the round is described in <a href="http://books.google.co.il/books?id=2wsrLg-xBGgC&amp;lpg=PP1&amp;pg=PA103#v=onepage&amp;q&amp;f=false" rel="nofollow">Distributed algorithms - Nancy Ann Lynch</a>. By the analysis it was shown that $f+1$ rounds is enough to reach a consensus.</p>\n\n<p>Intuitively, it looks like that the simplification completely changes the approach to the solution. It is may be enough just one round, every processes send an input message to every other processes and agree on the minimal value. </p>\n\n<p>What is the simplest possible algorithms for simplified problem ? </p>\n\n<p>What can we do in the case of general graph?</p>\n', 'ViewCount': '83', 'Title': 'Is this simplified consensus problem easier than the original?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-16T06:17:45.843', 'LastEditDate': '2013-05-16T06:17:45.843', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12059', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2013-05-15T16:55:04.127', 'Id': '12043''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Given three strings $x$, $y$, and $z$ over an arbitrary finite alphabet, I need to determine their longest common subsequence (LCS).</p>\n\n<p><strong>Example</strong>: A longest common subsequence of <code>bandana</code>, <code>cabana</code>, and <code>magazine</code> is <code>aan</code>.</p>\n\n<p>I'm trying to find an algorithm which uses $O(|x|\\cdot |y| \\cdot |z|)$ space where $|s|$ denotes the length of the string $s$.</p>\n", 'ViewCount': '163', 'Title': 'Find longest common subsequence in limited space', 'LastEditorUserId': '2205', 'LastActivityDate': '2014-03-25T19:29:35.657', 'LastEditDate': '2013-05-15T18:05:44.963', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '12046', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3083', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming><strings>', 'CreationDate': '2013-05-15T17:37:25.197', 'Id': '12045''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've ran some tests and found that Shellsort runs much faster on ordered and reversed lists compared to random lists and almost ordered lists.</p>\n\n<pre><code>Results:\n        Random Reverse Order AlmostOrder\n  time    24      5      4        29\n</code></pre>\n\n<p>The problem that is confusing me is that Shellsort performs insertion sorts on lists separated by gaps, and insertion sort only runs very fast on ordered lists, not reversed lists.</p>\n\n<p>So my question is why does Shellsort work well on ordered and reversed lists when it uses insertion sort and insertion sort doesn't work well on reversed lists?</p>\n", 'ViewCount': '160', 'Title': 'Why does Shellsort work well on Sorted and Reverse ordered lists?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:52:46.560', 'LastEditDate': '2013-05-19T14:52:46.560', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'clay', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-05-18T02:16:41.193', 'FavoriteCount': '0', 'Id': '12124''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've seen two definitions of a universal hash family, and my questions is if those are equivalent, i think they are and will explain why but i'm not sure if it is.</p>\n\n<p>Definition 1:</p>\n\n<p>$H$ is a universal hash family if and only if $$\\mathbb{Pr_{h \\in H}}[h(k) = h(l)] = 1/m$$</p>\n\n<p>where $k \\neq l$ and $k,l\\in Key$ and $m$ is the Size of the Hashtable</p>\n\n<p>Definition 2:</p>\n\n<p>The same as above just substitute the equals sign with a less or equal sign.</p>\n\n<p>Well, i think those definitions are equivalent, because i think the probability for a collision can't possibly be strictly less than 1/m or am i missing something?</p>\n\n<p>P.S.\nI'm assuming that the cardinality of Key is bigger than the size of the hashtable m</p>\n", 'ViewCount': '82', 'Title': 'Are those definitions of universal hash family equivalent?', 'LastEditorUserId': '8282', 'LastActivityDate': '2013-05-22T00:16:10.807', 'LastEditDate': '2013-05-21T17:02:54.330', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8282', 'Tags': '<algorithms><algorithm-analysis><hash><hash-tables>', 'CreationDate': '2013-05-21T16:00:59.023', 'Id': '12193''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>First I apologize if the title is unclear, but I didn\'t find anything better.</p>\n\n<p>I\'m solving a differential equation that has two parameters , here denoted by points of a plane.These parameters are real numbers. For some points of the plane (or, equivalently, for some parameters of the differential equation) , the solution of the equation satisfies some condition and we denote such points of the plane with 1. This points make a simply-connected region in the plane, and I know that this region lies somewhere in $0&lt;x&lt;1.6, -.7&lt;y&lt;.3$ rectangle. My goal is to find this region.Remember that this region is on the real plane.($\\mathbf R^2$)</p>\n\n<p><strong>My main question is this:</strong> I already know these two things about the wanted region:</p>\n\n<ol>\n<li><p>The set of points on the plane that satisfy the conditions (the wanted region) , form a simply-connected region</p></li>\n<li><p>This region lies somewhere in $0&lt;x&lt;1.6, -.7&lt;y&lt;.3$ rectangle</p></li>\n</ol>\n\n<p>for example it may be something like this:</p>\n\n<p><img src="http://i.stack.imgur.com/Q8lvC.png" alt="enter image description here"></p>\n\n<p>I want to use this two facts to find the region with less computations; i.e. instead of checking the condition on all points on the rectangle, actually , on a very high-resolution grid (this the first approach below), use an algorithm (below : Variant 2) that more quickly converges to <strong>the boundary</strong> (and so determines the region without inspecting all points).  </p>\n\n<p>(If you know a better approach ,I\'ll be happy to hear) </p>\n\n<h3>Variant 1 (the naive approach, noted above)</h3>\n\n<p>Divide each axis to identical steps (of length $\\Delta$ ) and check the condition on each node to find the region.$\\Delta$ must be as small as possible to find the region with an acceptable accuracy ($\\Delta0.001$ suffices for my purpose) . (in the picture : nodes = intersections). This method needs a huge number of check operations , but can be used to find all kinds of regions; I mean if I didn\'t know that the wanted region is connected or it had sharp edges, etc. ,this method was the only way.</p>\n\n<p><img src="http://i.stack.imgur.com/P9RoC.png" alt="http://i.stack.imgur.com/MvhRH.png"></p>\n\n<h3>Variant 2</h3>\n\n<p>(It may be a famous method, but I haven\'t seen it before) </p>\n\n<p>Because the region will be simply connected, it suffices to find its boundary .We use a <strong>recursive</strong> approach. We start from a grid (like the first step, but with much larger distance between nodes, say, $100\\Delta$) and check the condition on this grid.For the next step, I assume the interval between two adjacent 1s is 1 everywhere and between two adjacent 0s is 0 everywhere. <em>If two adjacent nodes gave different results (1 on one them and 0 on the other) , I put a point between them and check the condition on that point (red points in the picture) check this point. If it was 1, I put a new point between this point and the adjacent 0 and check that point; and if it was 0, I put a new point between this zero and adjacent 1 and check that point</em>. I continue till I arrive at a distance of $\\Delta$ between points.So I\'ve found the boundary. (This method is like bisection method  for finding the roots of a function)</p>\n\n<p>In the picture, the first iteration is shown.Black and yellow points are the points of the initial grid (that are distributed on the whole rectangle) and red points are those that are added after checking the initial grid nodes. Black points are points that are determined to satisfy the condition (are 1) and so are certainly inside the region. Yellow points are those that did not satisfy the condition and so are outside, and red points are those added in the 2nd iteration , between adjacent nodes with different results (between a 1 and a 0) ,according to the  above paragraph.</p>\n\n<p><img src="http://i.stack.imgur.com/CMgqT.png" alt="http://i.stack.imgur.com/ZqtFN.png"></p>\n\n<p>So , using this method I\'ve found the region with the same accuracy as in the first method, and saved a lot of time too.</p>\n\n<p>I want to know <em>how much this method is faster.</em> A qualitative answer that shows if it is better to implement this method , suffices. My problem is so computational intensive that I can\'t use the first approach.</p>\n', 'ViewCount': '124', 'Title': 'Complexity of an algorithm for bounding a region in 2D', 'LastEditorUserId': '8381', 'LastActivityDate': '2013-05-28T18:01:59.797', 'LastEditDate': '2013-05-28T18:01:59.797', 'AnswerCount': '2', 'CommentCount': '13', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8381', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><computational-geometry>', 'CreationDate': '2013-05-27T10:46:57.837', 'Id': '12305''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Starting to use <a href="https://code.google.com/p/nanoflann/" rel="nofollow">nanoflann</a> to do some point cloud nearest neighbor searching and it got me thinking about just how "approximate" ANN methods are.</p>\n\n<p>If I have a (more or less) randomly distributed point cloud what is the likelihood that I get the exact nearest neighbor given a target point within the clouds bounding box?  I know that it is dataset dependent... but does anyone have a good numerical study somewhere that shows trends?</p>\n', 'ViewCount': '94', 'Title': 'How approximate are "approximate" nearest neighbor (ANN) search algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T17:44:30.770', 'LastEditDate': '2013-05-28T06:46:45.047', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8395', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><approximation><heuristics>', 'CreationDate': '2013-05-27T20:13:40.783', 'FavoriteCount': '1', 'Id': '12310''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I am reading Algorithm design manual by Skiena. It gives proof of Insertion sort by Induction. I am giving the proof described in the below.</p>\n\n<blockquote>\n  <p>Consider the correctness of insertion sort, which we introduced at the beginning of this chapter. The reason it is correct can be shown inductively:</p>\n  \n  <ol>\n  <li>The basis case consists of a single element, and by definition a\n  one-element array is completely sorted.</li>\n  <li>In general, we can assume that the first n \u2212 1 elements of array A\n  are completely sorted after n \u2212 1 iterations of insertion sort.</li>\n  <li>To insert one last element x to A, we find where it goes, namely the\n  unique spot between the biggest element less than or equal to x and\n  the smallest element greater than x. This is done by moving all the\n  greater elements back by one position, creating room for x in the\n  desired location.</li>\n  </ol>\n</blockquote>\n\n<p>I do not understand paragraph #3. Could someone please explain it to me with an example?</p>\n', 'ViewCount': '368', 'Title': 'Insertion sort Proof by Induction', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-02T22:48:42.980', 'LastEditDate': '2013-06-02T22:48:42.980', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8496', 'Tags': '<algorithms><algorithm-analysis><sorting><correctness-proof><induction>', 'CreationDate': '2013-06-02T16:03:58.043', 'Id': '12434''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm studying for an exam on software testing and in my notes I found this:</p>\n\n<pre><code>Many more paths than branches. A pragmatic compromise will be needed\n</code></pre>\n\n<p>I cannot understand the difference between paths and branches; each time I work out the paths and branches of a graph I end up with the same number. Thanks!</p>\n", 'ViewCount': '97', 'Title': 'In control flow graphs, what is the difference between a path and a branch?', 'LastActivityDate': '2013-06-03T16:52:38.610', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12437', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8498', 'Tags': '<algorithm-analysis><software-testing>', 'CreationDate': '2013-06-02T19:11:09.537', 'FavoriteCount': '0', 'Id': '12436''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Let $a$ and $b$ be integers, and let $\\text{RANDOM}(a,b)$ be a method returning an integer from the range $[a,b]$ uniformly at random. Now consider the following program, that takes as input an array $A$ of integers.</p>\n\n<pre><code>PERMUTE-BY-SORTING(A)\n    1. n = A.length\n    2. let P[1..n] be a new array\n    3. for i = 1 to n \n    4.     P[i] = RANDOM(1, n^3)\n    5. sort A, using P as sort keys\n</code></pre>\n\n<p>I\'m solving the problem 5.3-6 in <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">CLRS</a>, which is asking me to explain how to implement the algorithm PERMUTE-BY-SORTING to handle the case in which two or more priorities are identical. In other words, the algorithm should produce a uniform random permutation, even if two or more priorities are identical. </p>\n\n<p>Because priorities are repeated in $P$, we will not get a uniform random permutation. I thought of adding <code>i</code> to step 4 but that doesn\'t produce the uniform random permutation. More specifically, the problem is that if two or more priorities are identical we will not get a uniform random permutation since the probability is not same for all the numbers. Ex 1,2,2,3 the probability of 2 in the example is 1/2 and the probability of 1 is 1/4 and 3 is 3/4. </p>\n', 'ViewCount': '319', 'Title': 'PERMUTE-BY-SORTING with similar priorities', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-02T12:14:26.043', 'LastEditDate': '2013-06-09T21:52:16.823', 'AnswerCount': '2', 'CommentCount': '11', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8263', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-06-09T00:49:55.697', 'FavoriteCount': '2', 'Id': '12551''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Find the number of topological sorts in a tree that has nodes that hold the size of their sub-tree including itself.</p>\n\n<p>I've tried thinking what would be the best for m to define it but couldn't get anything specific. </p>\n\n<p>Maybe $\\mbox{Number of sorts =}\\prod\\limits_{x\\in \\mbox{children}}\\mbox{Number of sorts}(x)$\nMeaning that starting at the root I call the the method recursively multilying each result by the previous children's result. When we reach a node with size 1 we assume that there's just 1 topological sort;</p>\n\n<p>If this is correct I'd really appreciate some help with proving correctness and if not a explanation why and a clue could be nice :)</p>\n", 'ViewCount': '50', 'Title': 'Find the number of topological sorts in a tree', 'LastEditorUserId': '8709', 'LastActivityDate': '2013-06-17T16:06:57.213', 'LastEditDate': '2013-06-17T15:42:53.633', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12716', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8709', 'Tags': '<graph-theory><graphs><algorithm-analysis><trees>', 'CreationDate': '2013-06-17T15:29:23.190', 'Id': '12713''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>When analysing treaps (or, equivalently, BSTs or Quicksort), it is not too hard to show that</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[d(k)] \\in O(\\log n)$</p>\n\n<p>where $d(k)$ is the depth of the element with rank $k$ in the set of $n$ keys.\nIntuitively, this seems to imply that also</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\in O(\\log n)$</p>\n\n<p>where $h(T)$ is the height of treap $T$, since</p>\n\n<p>$\\qquad\\displaystyle h(T) = \\max_{k \\in [1..n]} d(k)$.</p>\n\n<p>Formally, however, there does not seem to be an (immediate) relationship. We even have</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\geq \\max_{k \\in [1..n]} \\mathbb{E}[d(k)]$</p>\n\n<p>by Jensen\'s inequality. Now, one can show expected logarithmic height via tail bounds, using more insight into the distribution of $d(k)$.</p>\n\n<p>It is easy to construct examples of distributions that skrew with above intuition, namely extremely asymmetric, heavy-tailed distributions. The question is, can/do such occur in the analysis of algorithms and data structures?</p>\n\n<p>Are there example for data structures $D$ (or algorithms) for which</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(D)] \\in \\omega(\\max_{e \\in D} \\mathbb{E}[d(e)])$?</p>\n\n<p>Nota bene:</p>\n\n<ul>\n<li><p>Of course, we have to interpret "depth" and "height" liberally if we consider structures that are not trees. Based on the posts Wandering Logic links to, "Expected average search time" (for $1/n \\cdot \\sum_{e \\in D} \\mathbb{E}[d(e)]$) and "expected maximum search time" (for $\\mathbb{E}[h(D)]$) seem to be used.</p></li>\n<li><p>A <a href="http://math.stackexchange.com/q/426998/3330">related question</a> on math.SE has yielded an interesting answer that may allow deriving useful bounds on $\\mathbb{E}[h(D)]$ given suitable bounds on $\\mathbb{E}[d(e)]$ and $\\mathbb{V}[d(e)]$.</p></li>\n</ul>\n', 'ViewCount': '93', 'Title': 'Can expected "depth" of an element and expected "height" differ significantly?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-23T20:06:53.820', 'LastEditDate': '2013-06-23T16:04:31.497', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12833', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><data-structures><algorithm-analysis><probability-theory><average-case>', 'CreationDate': '2013-06-22T16:23:33.990', 'Id': '12830''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '3687', 'Title': 'Heap - Give an $O(n \\lg k)$ time algorithm to merge $k$ sorted lists into one sorted list', 'LastEditDate': '2013-06-24T19:13:31.113', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7678', 'FavoriteCount': '1', 'Body': "<p>Most probably, this question is asked before. It's from CLRS (2nd Ed) problem 6.5-8 -- </p>\n\n<blockquote>\n  <p>Give an $O(n \\lg k)$ time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. (Hint: Use a min-heap for $k$-way merging.)</p>\n</blockquote>\n\n<p>As there are $k$ sorted lists and total of $n$ values, let us assume each list contains $\\frac{n}{k}$ numbers, moreover each of the lists are sorted in strictly ascending order, and the results will also be stored in the ascending order. </p>\n\n<p>My pseudo-code looks like this --</p>\n\n<pre><code>    list[k]   ; k sorted lists\n    heap[k]   ; an auxiliary array to hold the min-heap\n    result[n] ; array to store the sorted list\n    for i := 1 to k                 ; O(k)\n    do\n        heap[i] := GET-MIN(list[i]) ; pick the first element \n                                    ; and keeps track of the current index - O(1)\n    done\n    BUILD-MIN-HEAP(heap) ; build the min-heap - O(k)\n    for i := 1 to n\n    do\n        array[i] := EXTRACT-MIN(heap)   ; store the min - O(logk)\n        nextMin := GET-MIN(list[1])     ; get the next element from the list 1 - O(1)\n        ; find the minimum value from the top of k lists - O(k)\n        for j := 2 to k                 \n        do\n            if GET-MIN(list[j]) &lt; nextMin\n                nextMin := GET-MIN(list[j]) \n        done\n        ; insert the next minimum into the heap - O(logk)\n        MIN-HEAP-INSERT(heap, nextMin)\n    done\n</code></pre>\n\n<p>My overall complexity becomes $O(k) + O(k) + O(n(k + 2 \\lg k)) \\approx O(nk+n \\lg k) \\approx O(nk)$. I could not find any way to avoid the $O(k)$ loop inside the $O(n)$ loop to find the next minimum element from k lists. Is there any other way around? How to get an $O(n \\lg k)$ algorithm?</p>\n", 'Tags': '<algorithms><algorithm-analysis><heaps><priority-queues>', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-06-24T20:38:10.317', 'CommentCount': '0', 'AcceptedAnswerId': '12854', 'CreationDate': '2013-06-24T06:20:18.200', 'Id': '12853''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I am a bit confused about calculating complexities.</p>\n\n<p>Above is a C++ program converting a char array into an int, incrementing the value, parsing it back to char array.</p>\n\n<pre><code>#include &lt;iostream&gt;\n\nint main() {\n    char number[] = {'4', '3', '1'};    \n    int num = 0;\n    //char to int conversion\n    for (int i = 0; i &lt; (int)sizeof(number); i++) {\n        num += number[i] - '0';\n        num*=10;\n    }\n    num/=10;\n\n    //incrementation\n    num++;\n\n    //int to char conversion\n    for (int i = (int)sizeof(number) -1; i &gt;= 0; i--) {\n        number[i] = '0' + num % 10;\n        num/=10;\n    }\n\n    //printing the result\n    std::cout &lt;&lt; number &lt;&lt; endl;\n    return 0;\n}\n</code></pre>\n\n<p>Now let's say array size(3) is n. In that case I would say that the complexity is O(n+n) which is O(2n). However I've heard that O(2n) is actually O(n) for some reason but I could not find any actual source about it. What is the time complexity of this program?</p>\n", 'ViewCount': '72', 'Title': 'Complexity of a particular algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-25T08:26:42.210', 'LastEditDate': '2013-06-25T08:26:42.210', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-06-24T18:53:48.697', 'Id': '12873''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>The main question is, how exactly is the big O analysis calculated on routines? Is there a specific formula that relates what each function in a program does to a big O calculation?</p>\n\n<p>Also, what about more complex iterations, such as colour conversions etc?</p>\n\n<p>I would like to point out that this is not a homework question, rather, it is a question from my own research/programming learning curve. I have code that I am working on, but would like to know how this analysis is carried out.</p>\n', 'ViewCount': '251', 'Title': "Analysis of algorithms, 'big O' question", 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-17T15:40:51.770', 'LastEditDate': '2013-07-17T10:26:48.837', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '12901', 'Score': '3', 'OwnerDisplayName': 'user8872', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><runtime-analysis>', 'CreationDate': '2013-06-25T19:08:19.800', 'Id': '12899''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>A bit of background, the work that I currently and will be doing involves sorting very large amounts of data (in this case, grayscale pixels in descending order), sometimes up to 4 million.  </p>\n\n<p>Which are the most effective and efficient sorting algorithms that could handle multiple large datasets (such as descried in the 1st paragraph)?  Is there an algorithm that could simultaneously sort through 2 or more sets of pixels?</p>\n', 'ViewCount': '243', 'LastEditorDisplayName': 'user8872', 'Title': 'Which are the most effective sorting algorithms for a large dataset?', 'LastActivityDate': '2013-06-30T23:45:55.733', 'LastEditDate': '2013-06-30T23:45:55.733', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '12991', 'Score': '0', 'OwnerDisplayName': 'user8872', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2013-06-30T08:50:22.210', 'Id': '12983''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have two for loops, one nested within the other. I have two int variables n and U, where <code>U &gt; n</code>. I know that the outer loop runs exactly U times (pretty much <code>for(int i = 0; i &lt; U -1; i++)</code>), and the inner loop runs exactly n times in total <strong>for the duration of the entire execution</strong>. It can run n times once and never run again, or it can run once in each iteration of the outer loop n times, or anything in between.</p>\n\n<p>It's obvious that this runs in O(nU) time. However, I think I can also state it runs in <code>theta(U + n)</code> time because the inner loop runs independently of the outer one. But still, they are nested, and not multiplying the runtimes of nested loops feels awkward. </p>\n\n<p>Intuitively I know I am right, but I don't really know how to show it. I'm not looking for a formal proof, but any kind of insight is welcome.   </p>\n", 'ViewCount': '137', 'Title': 'Theta Runtime of Nested for Loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-31T08:16:53.290', 'LastEditDate': '2013-07-31T08:16:53.290', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8943', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2013-06-30T17:08:51.800', 'Id': '12995''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '2139', 'Title': 'Recurrence for recursive insertion sort', 'LastEditDate': '2013-07-09T11:26:11.003', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9051', 'FavoriteCount': '0', 'Body': '<p>I tried this problem from CLRS (Page 39, 2.3-4)</p>\n\n<blockquote>\n  <p>We can express insertion sort as a recursive procedure as follows. In order to sort <code>A[1... n]</code>, we recursively sort <code>A[1... n-1]</code> and then insert <code>A[n]</code> into the sorted array <code>A[1... n-1]</code>. Write a recurrence for the running time of this recursive version of insertion sort.</p>\n</blockquote>\n\n<p>The recurrence I formed was</p>\n\n<p>$$\nT(n) = \\begin{cases}\\Theta(1) &amp; \\textrm{if } n = 1,\\\\\n       T(n-1) + \\Theta(n) &amp; \\textrm{if } n &gt; 1.\n\\end{cases}\n$$</p>\n\n<p><strong>My reasoning</strong></p>\n\n<ul>\n<li>the base case of $n = 1$ the list is sorted so there is no work hence constant time.</li>\n<li>For all other cases the time depends on sorting the sequence <code>A[1...n-1]</code> and then insertion into that sequence. Hence it should be their sum, i.e., $T(n-1) + \\Theta(n)$.</li>\n</ul>\n\n<p>I wanted to know whether the recurrence relation is correct. If not what are the mistakes and how to correctly formulate a recurrence relation?</p>\n', 'Tags': '<algorithm-analysis><recurrence-relation><sorting>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-09T14:06:04.290', 'CommentCount': '2', 'AcceptedAnswerId': '13174', 'CreationDate': '2013-07-09T06:51:21.537', 'Id': '13168''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>I've developed the following backtrack algorithm, and I'm trying to find out it time complexity.</p>\n\n<p>A set of $K$ integers defines a set of modular distances between all pairs of them. In this\nalgorithm, I considered the inverse problem of reconstructing all integer sets which realize a given distance multiset. i.e. :</p>\n\n<p><br>\nInputs: $D=\\{p_i\u2212p_j \\mod N, i\u2260j \\},K $\n<br>\nOutput : $P=\\{p_1,p_2,...,p_K\\},\\qquad p_i \\in \\{0,1,2,...,N-1\\},\\qquad p_i &gt; p_j $ for $i&gt;j$\n<br></p>\n\n<p>Simply saying, the algorithm puts $K$ blanks to be filled. Initially, puts 1 in the first blank. For the second blank it looks for the first integer that if we add to P, it doesn't produce any difference exceeding the existent differences in $D$. Then, it does so, for next blanks. While filling a blank if it checked all possible integers and found no suitable integer for that blank, it turns back to the previous blank and looks for next suitable integer for it. If all blanks are filled, it has finished his job, otherwise it means that there weren't any possible $P$'s for this $D$.</p>\n\n<p>Here's my analysis so far.\nSince the algorithm checks at most all members of $\\{2,...,N\\}$ for each blank (upper bound) there is $N-1$ search for each blank. If each visited blank was filled in visiting time, the complexity would be $O((K-1)(N-1))$ since we have $K-1$ blank (assuming first one is filled with 1). But the algorithm is more complex since for some blanks it goes backward and some blanks may be visited more that once. I'm looking for the worst case complexity i.e. the case that all blanks are visited and no solution is found.</p>\n", 'ViewCount': '1179', 'Title': 'Time complexity of a backtrack algorithm', 'LastEditorUserId': '9098', 'LastActivityDate': '2013-07-13T23:34:21.333', 'LastEditDate': '2013-07-12T07:56:41.700', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9098', 'Tags': '<algorithms><algorithm-analysis><combinatorics><search-algorithms><greedy-algorithms>', 'CreationDate': '2013-07-09T18:22:51.307', 'FavoriteCount': '2', 'Id': '13181''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Below is a well-known equation for generalized recurrence relation in a divide and conquer paradigm (as described in CLRS) --</p>\n\n<p>$$T(n) = aT(n/b) + f(n), \\quad \\text{where} \\quad a \\gt 1 \\text{ , } b \\geq 1$$</p>\n\n<p>If we consider a case for merge-sort, the relation will look like this -- </p>\n\n<p>$$T(n) = 2T(n/2) + \\Theta(n) \\qquad \\qquad (i)$$</p>\n\n<p>which is quite straight-forward, i.e. we have $2$ sub-problems of size $n/2$ each, ($1/2$ of the original sub-problem), and $\\Theta(n)$ operations to merge them. </p>\n\n<p>Now if we have a relation like --</p>\n\n<p>$$T(n) = 3T(n/2) + \\Theta(n) \\qquad \\qquad (ii)$$</p>\n\n<p>then we can assume that there are $3$ "overlapping" sub-problems, as each of them with size $n/2$. Let\'s consider again -- </p>\n\n<p>$$T(n) = 2T(n/4) + \\Theta(n) \\qquad \\qquad (iii)$$</p>\n\n<p>now, what does it mean? Are there $2$ sub-problems with size $n/4$? How is it possible? If we divide the whole problem into $4$ equal sizes then we should need something like $4T(n/4)$ instead of $2T(n/4)$ to balance the recurrence tree (each node with 4 leaves), right? Is this relation realistic?</p>\n\n<p>If this is the case, then why there is no constraint like $b &lt; a$ ? Moreover, is there any algorithm that follows the recurrence as in $(ii)$ and $(iii)$?</p>\n', 'ViewCount': '92', 'Title': 'Relation between the number of sub-problems ($a$) and the size of sub-problems ($b$) in a recurrence', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-08-04T05:39:58.010', 'LastEditDate': '2013-08-04T05:39:58.010', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13238', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation>', 'CreationDate': '2013-07-12T04:38:44.167', 'Id': '13234''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Searching for a number in an array is said to have a runtime of O(n) because there may be cases where the number doesn't exist in the array. In such cases, you'd have to have gone through the entire array, which is O(n).</p>\n\n<p>But how about in the case where we know the number definately exists in the array? Does the runtime change then?</p>\n\n<p>Also is there a way to find out the average number of searches it would have to do before a number is found in an array based on its size?</p>\n", 'ViewCount': '290', 'Title': 'Runtime of searching for a number in an array?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-18T07:19:24.290', 'LastEditDate': '2013-07-12T14:49:49.223', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'garbage collection', 'PostTypeId': '1', 'OwnerUserId': '9155', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><arrays>', 'CreationDate': '2013-07-12T04:44:49.213', 'Id': '13244''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>When calculating runtime dependence on the input, what calculations are considered? For instance, I think I learned that array indexing as well as assignment statements don't get counted, why is that?</p>\n", 'ViewCount': '86', 'Title': 'What constitutes one unit of time in runtime analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-13T22:31:00.860', 'LastEditDate': '2013-07-13T09:43:51.293', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8722', 'Tags': '<terminology><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-07-13T04:30:59.783', 'Id': '13254''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Recently I have had this question in one of my interviews.</p>\n\n<p>You have 1 Million sorted integer, you have a value of $x$, compare each pair in this array and if the addition of two pair is less or equal to $x$ then increment the paircounter.</p>\n\n<p>I have implemented this solution in C++ however I will write pseudocode here(I am sorry I am not very good at pseudo-code)</p>\n\n<pre><code>Initialise ARR_SIZE to 1000000\n\nInitialise index_i to 0\n\nInitialise pairCount to 0\n\nInitialise x to 54321\n\nwhile index_i is less than ARR_SIZE\n\n  Initialise index_j to index_i+1\n\n    while index_j is less than ARR_SIZE\n\n      if array[index_i]+array[index_j] is less or equal to x\n\n        Increment pairCount\n\n      increment index_j\n\n    endof while\n\n    increment index_x\n\nendof while\n</code></pre>\n\n<p>At first I said it is $O(n \\log n)$ but then with the hint the second loop itself average complexity is O(n/2) so overall I said it would be $O(n\\cdot n/2)$ but in Big $O$ notation it would be $O(n)$ because $n/2$ is a constant(although I was not too sure). So what is the average complexity of this overall algorithm?</p>\n\n<p>PS: I know that I could have decreased the complexity by adding an extra else, index_j = ARR_SIZE, which would be $O(N)$ complexity, but I couldn't think of it during the interview.</p>\n", 'ViewCount': '818', 'Title': 'Average Time Complexity of Two Nested Loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-20T18:26:01.993', 'LastEditDate': '2013-07-16T16:38:41.020', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '13303', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-07-16T14:06:15.760', 'Id': '13302''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've constructed an algorithm that solves the 3SUM problem in $O(n\\lg n) + \\frac{n^2}{4}$ time. I'm new to algorithms and was wondering how good is my running time? Googling didn't help.\nthanks..</p>\n", 'ViewCount': '158', 'Title': 'Computing 3SUM problem in $O(n\\lg n) + \\frac{n^2}{4}$ time', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T05:17:15.700', 'LastEditDate': '2013-07-22T05:17:15.700', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9273', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-07-21T02:47:31.517', 'Id': '13372''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>Let there is a binary-string, $B$, of length $N$. The probability of occurrence of 0 and 1 in this binary-word is $p$  and $q$ , respectively. Each bit in the string is independent of any other bit.</p>\n\n<p>There is an algorithm (divide and conquer) which finds the location of 1\u2019s in a given binary-string, in Q # of steps (cost).</p>\n\n<p>I am looking for some close-form solution of the expected # of steps,$E[Q]$, with given probabilities $p$ and $q$ for a string of length $N$.</p>\n\n<p>For instance, for $N=4$ the cost ,${{Q}_{i}}$,  for each possible word is:<br>\n[\\begin{matrix}\n   {{B}_{i}} &amp; {{Q}_{i}} &amp; {{P}_{i}}  \\\\\n   0000 &amp; 1 &amp; {{p}^{4}}  \\\\\n   0001 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0010 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0011 &amp; 5 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0100 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0101 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0110 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0111 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1000 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   1001 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1010 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1011 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1100 &amp; 5 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1101 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1110 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1111 &amp; 7 &amp; {{q}^{4}}  \\\\\n\\end{matrix}]\nFrom the discrete probability theory, we can evaluate the expected cost of the above case ,$N=4$,as follow $\\begin{align}\n  &amp; \\therefore E[Q]=\\sum\\limits_{i=0}^{{{2}^{N}}-1}{{{Q}_{i}}({{p}^{N-i}}}{{q}^{i}}) \\\\ \n &amp; \\Rightarrow E[Q]=\\sum\\limits_{i=0}^{15}{{{Q}_{i}}({{p}^{N-i}}}{{q}^{i}}) \\\\ \n &amp; \\Rightarrow E[Q]={{p}^{4}}+4\\times 5\\times {{p}^{3}}q+2\\times 5\\times {{p}^{2}}{{q}^{2}}+4\\times 7\\times {{p}^{2}}{{q}^{2}}+4\\times 7\\times p{{q}^{3}}+1\\times 7\\times {{q}^{4}} \\\\ \n &amp; \\Rightarrow E[Q]={{p}^{4}}+20{{p}^{3}}q+10{{p}^{2}}{{q}^{2}}+28{{p}^{2}}{{q}^{2}}+28p{{q}^{3}}+7{{q}^{4}} \\\\ \n\\end{align}$ </p>\n\n<p>However, for very large values of N, say 1024, it is computationally not possible to evaluate the cost of each possible binary string (i-e ${{2}^{1024}}=\\text{1}\\text{.79}\\times \\text{1}{{\\text{0}}^{308}}$ binary words). So, this is the problem I am stuck in. </p>\n\n<p>Is it possible to deduce some analytical/ close-form expression for evaluating the expected value of the cost of this algorithm for a given length N and probabilities p and q (instead of brute-force method defined above)?</p>\n\n<p>I will be very thankful, if someone could help in this regard.</p>\n\n<p>ADDITIONAL INFORMATION:</p>\n\n<p>Divide &amp; Conquer Algorithm:\n    Lets take 0000 0001, for instance:</p>\n\n<ol>\n<li><p>First question: Is it (i-e 0000 0001) equal to ZERO (i-e 0000 0000) ? The answer is No, for our case. </p></li>\n<li><p>Then divide the original 8 bit word into two 4 bit segments, and again ask the same question for each of the two 4 bit words. So for our case it would be YES for the first segment (0000) and NO for the other segment (0001)?</p></li>\n<li><p>Now, this time I will be questioning only the segment where I got NO. In our case it was 0001. Then, I will now again divide this 4-bit segment into two segments and pose the same question. Hence, is 00 equal to ZERO? answer : YES. For the other segment , 01, the answer is NO.</p></li>\n<li><p>This is the final step. I will again divide the 2-bit word into two 1-bits, i-e 0 and 1. So, my first question: is 0 equal to 0? answer is YES. And for the other remaining bit, is 1 equal to 0? Answer is NO.</p></li>\n</ol>\n\n<p>So, I asked a total of 7 questions to find the location of 1 in a binary word of 0000 0001. Similarly, we will go through other binary words.</p>\n\n<p>Efficient Way for evaluating the cost of Above algorithm:\n(courtesy to Yuval Filmus)</p>\n\n<p>Here is how to calculate the cost of the algorithm. Start with the bit vector $x$, and consider the following operation $O$, which divides $x$ into pairs of bits and computes their ORs. Thus $|O(x)| = |x|/2$. Compute a sequence $O(x),O(O(x)),O(O(O(x))),\\ldots$ until you get a vector of width $1$. Count the total number of 1s in the sequence. If you got $N$, then the cost is $2N+1$.</p>\n\n<p>For example, suppose $x=0101$. Then the sequence is\n$$ 11,1, $$\nand so $N = 3$ and the cost is $7$.</p>\n', 'ViewCount': '155', 'Title': 'Closed-form Expression of the Expected value of the Cost of D&C Algorithm?', 'LastEditorUserId': '9000', 'LastActivityDate': '2013-08-02T16:50:37.103', 'LastEditDate': '2013-08-02T16:50:37.103', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '13377', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9000', 'Tags': '<algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2013-07-21T19:03:42.417', 'Id': '13376''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>The paragraph on V-opt heuristic TSP algorithms at <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem#Heuristic_and_approximation_algorithms" rel="nofollow">this site</a> mentions a very effective algorithm due to Lin-Kernigham-Johnson. That page says: </p>\n\n<blockquote>\n  <p>For many years Lin\u2013Kernighan\u2013Johnson had identified optimal solutions for all TSPs where an optimal solution was known and had identified the best known solutions for all other TSPs on which the method had been tried.</p>\n</blockquote>\n\n<p>Impressive, so what is the complexity of that algorithm? Does the algorithm often work faster than predicted  based on theoretical complexity (if yes, how much)? Is that algorithm used most often in software that solves the TSP?</p>\n', 'ViewCount': '138', 'Title': 'Complexity of the V-opt heuristic Traveling Salesman algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-23T22:21:29.497', 'LastEditDate': '2013-07-22T14:15:56.187', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13404', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6641', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><traveling-salesman>', 'CreationDate': '2013-07-22T01:01:32.150', 'Id': '13380''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '231', 'Title': 'Selection in expected linear time: Why am I getting $O(n)$ bound instead of $\\Omega(n \\lg n)$?', 'LastEditDate': '2013-07-22T04:53:45.530', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Body': '<p>The problem is from CLRS 9.3-1:</p>\n\n<blockquote>\n  <p>In the algorithm <code>SELECT</code>, the input elements are divided into groups of\n  $5$. Argue that <code>SELECT</code> does not run in linear time if groups of $3$ are used.</p>\n</blockquote>\n\n<p>If we do the "divide by $3$" technique, we will come up with this recurrence -- </p>\n\n<p>$$T(n) = \\begin{cases}\n\\Theta(1) &amp; \\text{if $n \\le K$} \\\\\nT(\\lceil n/3 \\rceil)+T(2n/3+4) + O(n) &amp; \\text{if $n \\ge K$} \n\\end{cases}$$</p>\n\n<p>I have solved by substituting $T(n) \\le cn$ and  $O(n) = an$ --</p>\n\n<p>$$\\begin{aligned}\nT(n) &amp; \\le \\lceil n/3 \\rceil + c(2n/3 + 4) + an \\\\\n     &amp; \\le cn/3 + c + 2cn/3 + 4c + an \\\\\n     &amp; = cn + 5c + an \\\\\n     &amp; = (c+a)n + 5c \\\\\n     &amp; = c_1n + c_2 \\le c_1n \\approx O(n)\n\\end{aligned}$$</p>\n\n<p>But the solution says it should be $\\Omega(n \\lg n)$. I understand that substitution like $cn \\lg n$ could give $\\Omega(n \\lg n)$ bound, but what is wrong with $O(n)$ formulation above? </p>\n', 'ClosedDate': '2013-07-22T14:13:11.047', 'Tags': '<algorithm-analysis><recurrence-relation><search-algorithms>', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-22T04:53:45.530', 'CommentCount': '2', 'AcceptedAnswerId': '13382', 'CreationDate': '2013-07-22T04:40:51.103', 'Id': '13381''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Now I am confused about symbolic execution (SE) and reachability analysis (RA). As I know, SE uses symbols to execute some code to reach each branch with branch conditions. And RA can be used to find the reachability of each branch, right? When RA is used, we can extract the branch condition for each branch. If so, what's the difference between them? Can they be swift? Are they all static analysis?</p>\n", 'ViewCount': '54', 'Title': 'Difference between symbolic execution and reachability analysis', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-30T10:30:32.577', 'LastEditDate': '2013-07-30T10:30:32.577', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13497', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9406', 'Tags': '<algorithm-analysis><compilers><program-optimization>', 'CreationDate': '2013-07-29T16:10:30.270', 'Id': '13492''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose, we have an array of numbers $x_j$ and their corresponding weights $w_j$ where $\\sum_j w_j \\gt 1$. Now we need to find $x_m$ such that </p>\n\n<p>$$\\sum_{j=1}^{m-1} w_j \\lt 1/2 \\quad \\text{and} \\quad \\sum_{j=m+1}^{n} w_j \\ge 1/2$$</p>\n\n<p>Moreover, $x_m &gt; x_j$, $x_m &lt; x_k$ where $j \\ne k$. i.e. a solution should be like this -- </p>\n\n<p>$$\\underbrace{x_1, x_2, \\ldots, x_{m-1}}_{\\lt \\, x_m}, x_m, \\underbrace{x_{m+1}, \\ldots, x_{n-1}, x_n}_{\\ge \\, x_m} \\\\\n\\underbrace{w_1, w_2, \\ldots, w_{m-1}}_{\\lt \\, 1/2}, w_m, \\underbrace{w_{m+1}, \\ldots, w_{n-1}, w_n}_{\\ge \\, 1/2}$$</p>\n\n<p>Moreover, it was also mentioned that I may use Dynamic Programming that could be bounded by $O(n\\lg n)$.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>$\\{x_j, w_j\\}: \\quad x_j \\text{ is the value and } w_j \\text{ is the weight.}$</p>\n\n<p>Example Input: $\\{10, 0.4\\}, \\, \\{5, 0.1\\}, \\,  \\{6, 0.9\\}, \\, \\{2, 0.3\\}, \\, \\{3, 0.1\\}$</p>\n\n<p>Example Output: $\\{2, 0.3\\}, \\, \\{3, 0.1\\}, \\,  \\underbrace{\\{5, 0.1\\}}_{x_m}, \\, \\{6, 0.9\\}, \\, \\{10, 0.4\\}$ </p>\n\n<p><strong>How I tried</strong></p>\n\n<p>Step 1: First sort the list according to $w_j$. -- $O(n \\lg n)$</p>\n\n<p>Step 2: Start from the first element from the left, add the weights $w_j$ until \n$\\sum_j w_j \\ge \\, 1/2$. The current $x_j$ is the $x_m$. -- $O(n)$</p>\n\n<p>Step 3: Stop, now we have two lists. One is on the left $L=\\{x_1, x_2, \\ldots, x_{m-1}\\}$ and the other is on the right $R = \\{x_m, x_{m+1}, \\ldots, x_n\\}$.</p>\n\n<p>Step 4: Go through the list $L$, if there is any value $x_k &gt; x_m$, move $x_k$ into $R$ at an appropriate position. Do this until all elements in $L$ is smaller than $x_m$. -- $O(n^2)$</p>\n\n<p>Step 5: if $L \\ne \\emptyset$, $x_m$ is the answer, otherwise $x_1$ is the answer.</p>\n\n<p>The overall complexity will be $O(n \\lg n) + O(n) + O(n^2) \\approx O(n^2)$. I got confused about the DP stuff at the end of the question, so I was wondering if there is really any way to do it in $O(n \\lg n)$ (or better), how do I build the optimal substructure in the case of DP?</p>\n', 'ViewCount': '174', 'Title': 'A complicated variant of Weighted Median problem', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-08-03T00:16:40.533', 'LastEditDate': '2013-08-01T23:17:14.153', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><algorithm-analysis><dynamic-programming>', 'CreationDate': '2013-07-31T05:35:21.857', 'Id': '13535''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>A typical way of proving the greedy choice property of the fractional knapsack problem is as follows:</p>\n\n<hr>\n\n<p>From Slide 5 of <a href="http://www.cs.kzoo.edu/cs215/lectures/f4-knapsack.pdf" rel="nofollow">this link</a>:</p>\n\n<p>Given: A set of items $I = \\{I_1,I_2..I_n\\}$ with weights $\\{w_1,w_2 ... w_n\\}$ and values $\\{v_1,v_2 ...v_n\\}$. Let $P$ be the problem of selecting items from $I$, with the weight limit $K$ such that the resulting value is maximum.</p>\n\n<p>Let $O = \\{o_1,o_2 ... o_j\\} \\subseteq I$ be the <strong>optimum</strong> solution of problem $P$. </p>\n\n<p>Let\xa0$G = \\{g_1,g_2 ... g_k\\} \\subseteq I$ be\xa0the\xa0greedy\xa0solution,\xa0where\xa0the\xa0 items\xa0are\xa0ordered\xa0according\xa0to the\xa0greedy\xa0choices.\xa0</p>\n\n<p>We\xa0need to\xa0show\xa0that\xa0there\xa0exists\xa0some\xa0optimal\xa0solution\xa0$O\'$\xa0that\xa0includes\xa0the\xa0choice $g_1$\n.</p>\n\n<p>CASE\xa01:\xa0$g_1$\xa0is\xa0non-\xadfractional.</p>\n\n<ol>\n<li>If\xa0$g_1$\xa0is\xa0included\xa0in $O$,\xa0then\xa0we\xa0are\xa0done.</li>\n<li>If\xa0$g_1$\xa0is\xa0not\xa0included\xa0in\xa0$O$,\xa0then\xa0we\xa0arbitrarily\xa0remove\xa0$w_{g_1}$\xa0worth\xa0of\xa0stuff from\xa0$O$\xa0and\xa0replace\xa0it\xa0with\xa0$g_1$\xa0to\xa0produce\xa0$O\'$.</li>\n<li>$O\'$ is\xa0a\xa0solution, and\xa0it\xa0is at\xa0least\xa0as\xa0good as\xa0$O$.</li>\n</ol>\n\n<hr>\n\n<p>In the above proof, step $3$ for CASE 1 merely shows that weight criteria is satisfied. How does it show that $O\'$ is also an optimal solution(i.e. in terms of value achieved), more so when we are "arbitrarily removing $w_{g_1}$ worth of stuff" without paying attention to corresponding change in value ?</p>\n\n<p><strong>UPDATE</strong>: I found the answer in terms of change in value <a href="http://oucsace.cs.ohiou.edu/~razvan/courses/cs4040/lecture15.pdf" rel="nofollow">here</a>. I am not sure if this should go into the answer part. Mods, please suggest.</p>\n', 'ViewCount': '1236', 'Title': 'Proving greedy choice property of fractional knapsack', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-05T20:37:19.433', 'LastEditDate': '2013-08-05T20:37:19.433', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'curryage', 'PostTypeId': '1', 'OwnerUserId': '8660', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><greedy-algorithms>', 'CreationDate': '2013-07-03T00:09:45.020', 'Id': '13575''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>According to CLRS, the Prim's algorithms is implemented as below -- </p>\n\n<blockquote>\n  <p>$\\mathtt{\\text{MST-PRIM}}(G,w,r)$  </p>\n  \n  <ul>\n  <li>for each $u \\in V[G]$ do<br>\n  <ul>\n  <li>$\\mathtt{\\text{key}}[u] \\leftarrow \\infty$  </li>\n  <li>$\\pi[u] \\leftarrow \\mathtt{\\text{NIL}}$ </li>\n  </ul></li>\n  <li>$\\mathtt{\\text{key}}[r] \\leftarrow 0$  </li>\n  <li>$Q \\leftarrow V[G]$  </li>\n  <li>while $Q \\ne \\emptyset$ do // ... $O(V)$\n  <ul>\n  <li>$u$ $\\leftarrow$ $\\mathtt{\\text{EXTRACT-MIN}}(u)$ // ... $O(\\lg V)$<br>\n  <ul>\n  <li>for each $v \\in \\mathtt{\\text{adj}}[u]$ do // ... $O(E)$<br>\n  <ul>\n  <li>if $v \\in Q$ and $w(u,v) \\gt \\mathtt{\\text{key}}[v]$\n  <ul>\n  <li>then $\\pi[v] \\leftarrow u$\n  <ul>\n  <li>$\\mathtt{\\text{key}} \\leftarrow w(u,v)$ // $\\mathtt{\\text{DECREASE-KEY}}$ ... $O(\\lg V)$</li>\n  </ul></li>\n  </ul></li>\n  </ul></li>\n  </ul></li>\n  </ul></li>\n  </ul>\n</blockquote>\n\n<p>The book says the total complexity is $O(V \\lg V + E \\lg V) \\approx O(E \\lg V)$. However, what I understood is that the inner <code>for</code> loop with the <code>DECREASE-KEY</code> operation will cost $O(E \\lg V)$, and the outer <code>while</code> loop encloses both the <code>EXTRACT-MIN</code> and the inner <code>for</code> loop, so the total complexity should be $O(V (\\lg V + E \\lg V)) = O(V \\lg V + EV \\lg V) \\approx O(EV \\lg V)$. </p>\n\n<p>Why the complexity analysis is not performed as such? and What is wrong with my formulation?</p>\n", 'ViewCount': '534', 'Title': "MST: Prim's algorithm complexity, why not $O(EV \\lg V)$?", 'LastActivityDate': '2013-08-05T06:45:45.717', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13609', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7678', 'Tags': '<algorithms><algorithm-analysis><spanning-trees>', 'CreationDate': '2013-08-05T05:59:33.073', 'FavoriteCount': '1', 'Id': '13608''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Currently, I'm delving into Analysis of Algorithms and I've discovered that I would need to improve my knowledge of Probability Theory. Any recommendation? Where do I start?\nThanks in advance!</p>\n", 'ViewCount': '195', 'Title': 'Recommended readings for Probability theory applied to algorithms', 'LastActivityDate': '2013-08-09T19:23:00.830', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '13690', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9085', 'Tags': '<algorithms><algorithm-analysis><probability-theory>', 'CreationDate': '2013-08-09T11:23:03.367', 'FavoriteCount': '4', 'Id': '13687''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I would appreciate an intuitive way to find the time complexity of dynamic programming problems. Can anyone explain me \u201c#subproblems * time/subproblem\u201d? I am not able to grok it.</p>\n\n<p>Code for LCS - </p>\n\n<pre><code>public static String findLCS(String str1, String str2 ) {\n    // If either string is empty, return the empty string\n    if(null == str1 || null == str2)\n        return "";\n    if("".equals(str1) || "".equals(str2)) {\n        return "";\n    }\n    // are the last characters identical?\n    if(str1.charAt(str1.length()-1) == str2.charAt(str2.length()-1)) {\n        // yes, so strip off the last character and recurse\n        return findLCS(str1.substring(0, str1.length() -1), str2.substring(0, str2.length()-1)) + str1.substring(str1.length()-1, str1.length());\n    } else {\n       // no, so recurse independently on (str1_without_last_character, str2)\n       // and (str1, str2_without_last_character)\n       String opt1 = findLCS(str1.substring(0, str1.length() -1), str2); \n       String opt2 = findLCS(str1, str2.substring(0, str2.length()-1));\n       // return the longest LCS found\n       if(opt1.length() &gt;= opt2.length())\n           return opt1;\n       else\n           return opt2;\n    }\n}\n</code></pre>\n\n<p>I am just providing the actual code instead of pseudo code (i hope pseudo code or the algo is pretty self explanatory from above)</p>\n', 'ViewCount': '623', 'Title': 'Understand the time complexity for this LCS (longest common subsequence) solution', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-11T14:45:43.030', 'LastEditDate': '2013-08-11T11:39:51.457', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13706', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9166', 'Tags': '<algorithm-analysis><time-complexity><dynamic-programming>', 'CreationDate': '2013-08-11T10:28:35.407', 'Id': '13704''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In the following function, let $n \\geq m$.  </p>\n\n<pre><code>int gcd(n,m)  \n{  \n if (n % m == 0) return m;  \n n = n % m;  \n return gcd(m, n);  \n}\n</code></pre>\n\n<p>How many recursive calls are made by this function?</p>\n\n<ul>\n<li>$\\Theta (\\log_2 n)$</li>\n<li>$\\Omega (n)$</li>\n<li>$\\Theta (\\log_2(\\log_2 n))$</li>\n<li>$\\Theta ( \\sqrt{n} )$  </li>\n</ul>\n\n<p>I think the answer is $\\Theta (\\log_2(\\log_2n))$, but my book is saying $\\Theta (\\log_2 n)$.    </p>\n\n<p>My reasoning is as follows. Here we are not dividing the number. If there was a division then it would be $\\log n$. But here operation is $\\bmod$. So we will get a very small number after the first call. So it must be $\\log \\log n$. Am I thinking correctly?</p>\n', 'ViewCount': '259', 'Title': 'How many recursive calls are made by this gcd function?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-19T12:42:07.087', 'LastEditDate': '2013-08-17T18:18:39.033', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<algorithms><algorithm-analysis><arithmetic>', 'CreationDate': '2013-08-15T17:26:36.207', 'Id': '13761''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>Imagine that an algorithm A runs in worst-case time $f(n)$ and that algorithm B runs in worst-case time $g(n)$. Answer either yes, no, or can\u2019t tell and could you explain me why?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(h)=\u03a9(f(n)\\log n)$?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(n)=O(f(n)\\log n)$?</p>\n\n<p>Is A more faster than B, for all $n&gt;n_0$ if $g(n)=\u0398(f(n)\\log n)$?</p>\n', 'ViewCount': '133', 'Title': 'Worst-case time algorithm?...which one is faster?', 'LastEditorUserId': '683', 'LastActivityDate': '2013-08-20T17:22:07.640', 'LastEditDate': '2013-08-20T17:22:07.640', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-08-20T03:11:35.790', 'FavoriteCount': '2', 'Id': '13827''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Let $f(n)$ and $g(n)$ be complexity functions. Are the following statements true?</p>\n\n<p>$$\n\\begin{equation}\n\\frac {f(n)}{g(n)}= O\\left(\\frac{f(n)}{ g(n)}\\right) \\\\\nf(n) \\times g(n) = O(\\max(f^2(n),g^2(n)))\n\\end{equation}\n$$</p>\n', 'ViewCount': '50', 'Title': 'Maximum complexity of a product and of a quotient', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-21T07:41:40.623', 'LastEditDate': '2013-08-21T07:41:40.623', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'CreationDate': '2013-08-21T03:24:32.137', 'Id': '13850''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '838', 'Title': "What's Big O of $\\log(n+7)^{\\log(n)}$?", 'LastEditDate': '2013-08-25T11:38:43.547', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9815', 'FavoriteCount': '1', 'Body': "<p>As part of my continual improvement plans, I'm reviewing algorithm analysis and I've hit a roadblock. I cannot figure out how to determine the Big O complexity of $\\log(n+7)^{\\log(n)}$. I've spent the last few days rolling this over in my head and I still haven't made any progress in solving it. Could someone tell me what the Big O of this algorithm is, and explain how you solved it?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-03T17:16:56.050', 'CommentCount': '2', 'AcceptedAnswerId': '13913', 'CreationDate': '2013-08-24T17:35:13.647', 'Id': '13912''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>A shuffling algorithm is supposed to generate a random permutation of a given finite set. So, for a set of size $n$, a shuffling algorithm should return any of the $n!$ permutations of the set uniformly at random. </p>\n\n<p>Also, conceptually, a randomized algorithm can be viewed as a deterministic algorithm of the input and some random seed. Let $S$ be any shuffling algorithm. On input $X$ of size $n$, its output is a function of the randomness it has read. To produce $n!$ different outputs, $S$ must have read at least $\\log(n!) = \\Omega(n \\log n)$ bits of randomness. Hence, any shuffling algorithm must take $\\Omega(n \\log n)$ time.</p>\n\n<p>On the other hand, the <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle" rel="nofollow">Fisher-Yates shuffle</a> is <a href="http://www.codinghorror.com/blog/2007/12/the-danger-of-naivete.html" rel="nofollow">widely</a> <a href="http://c2.com/cgi/wiki?LinearShuffle" rel="nofollow">believed</a> to run in $O(n)$ time. Is there something wrong with my argument? If not, why is this belief so widespread?</p>\n', 'ViewCount': '161', 'Title': 'How can you shuffle in $O(n)$ time if you need $\\Omega(n \\log n)$ random bits?', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-08-29T08:52:52.413', 'LastEditDate': '2013-08-28T18:42:00.367', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9847', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><randomized-algorithms>', 'CreationDate': '2013-08-28T07:42:37.830', 'Id': '13990''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm studying for the computer science GRE, and as an exercise I need to provide a recursive algorithm to compute Fibonacci numbers and show its correctness by mathematical induction.</p>\n\n<p>Here is my recursive version of an algorithm to compute Fibonacci numbers:</p>\n\n<pre><code>Fibonacci(n):\n    if n = 0 then     // base case\n        return 0\n    elseif n = 1 then // base case\n        return 1\n    else\n        return Fibonacci(n - 1) + Fibonacci(n - 2)\n    endif\n</code></pre>\n\n<p>How can I prove the correctness of this algorithm by induction?</p>\n", 'ViewCount': '1036', 'Title': 'Prove correctness of recursive Fibonacci algorithm, using proof by induction', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-31T10:28:41.523', 'LastEditDate': '2013-08-31T05:55:13.280', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2013-08-29T18:27:55.383', 'FavoriteCount': '1', 'Id': '14025''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am new to Algorithms and am currently confused about the running time of the ThreeSum program. If we look at ThreeSum\'s count function:</p>\n\n<pre><code>public static int count(int[] a) {\n  int N     = a.length;\n  int count = 0;\n\n  for (int i = 0; i &lt; N; i++) &lt;-- A\n    for (int j = i + 1; j &lt; N; j++) &lt;-- B\n      for (int k = j + 1; k &lt; N; k++) &lt;-- C\n        // check each triple\n        if (a[i] + a[j] + a[k] == 0) \n          count++;\n  // loop i = 0; j = i+1; k = j+1 so that we get each triple just once\n\n  return count;\n  }\n</code></pre>\n\n<p>I understand that <code>int N = a.length</code> means that its frequency is 1 as the statement only runs once and that <code>count++</code>\'s frequency is x as it depends on input however with nested for loops. I understand that say <code>A</code>\'s frequency is $N$ (as it is repeated <code>N</code> times) but then I get a bit confused. </p>\n\n<p>It seems to be that for each nested loop, the other loop has to run which makes sense so <code>B</code>\'s frequency is $N^2$ but then in the book it says that B\'s frequency is $N^2/2$ and C is $N^3/6$ (where I would have assumed $N^2$ and $N^3$) where is the "$/2$" and "$/6$" coming from?</p>\n\n<p>Any help is appreciated, thanks. Sorry for the lengthy question!</p>\n', 'ViewCount': '687', 'ClosedDate': '2013-09-02T10:29:49.600', 'Title': 'Time complexity of a triple nested for loop for ThreeSum problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T10:28:53.567', 'LastEditDate': '2013-09-02T10:28:53.567', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9925', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-01T11:53:16.337', 'Id': '14065''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I would like to check time complexity of two procedures for which I am not totally convinced that I got it right. Now the first procedure is this:</p>\n\n<pre><code>public static int c(int n) {\n    int i, j, s = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i*=2) {\n            for(j = n; j &gt; 0; j/=2) {\n                s++;\n            }\n            s+=c(n-2);\n        }\n    }\n}\n</code></pre>\n\n<p>Now I have set the following recurrence equation:\n$T(n)=\\log_2n*T(n-2)+\\Theta(\\log_2n)$</p>\n\n<p>Now the height of the recurrence tree is $n/2$ so the $T(n) = n/2 * (\\log_2n+\\log_2n)=\\Theta(n*\\log_2n)$</p>\n\n<p>The next procedure is this:</p>\n\n<pre><code>public static int d(int n, int m) {\n    int i, j, k, ss = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i++) {\n            for(j = i; j &gt; n; j+=2) {\n                ss+=(i+1)-2*j+m;\n            }\n        }\n        for(k=0; k&lt;8; k++) {\n            ss+=d(n/2, m/(k+1))\n        }\n    }\n    return ss;\n}\n</code></pre>\n\n<p>Again I have set this equation:\n$T(n, m)=8*T(n/2, m/(k+1))+\\Theta(n^2)$</p>\n\n<p>Now I think the $m$ parameter is not important because it is not used in for loop as a counter. Where $n/2$ gives us a recurrence tree of height $\\log_2n$. So we get this:\n$T(n, m) = 8 * \\log_2n*n^2=\\Theta(n^2)$</p>\n\n<p>I know that recurrence equations that I have set up are probably right, while the I am not sure about next steps.</p>\n', 'ViewCount': '109', 'Title': 'Finding asymptotically tight bounds $\\Theta$ of two procedures', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-04T09:50:41.463', 'LastEditDate': '2013-09-04T09:22:55.420', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14124', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9974', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-04T08:14:21.707', 'Id': '14121''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I was going through a past paper question but I don't have any answers to know if I'm working out the problems correctly or not.</p>\n\n<p>I need to find the time complexity for:</p>\n\n<pre><code>i) repeat\n      n:=n div 2;\n   until n=1;\n\nii) for i:=1 to n do\n       begin\n          for j:=1 to n do\n             begin\n                for k:=1 to n do;\n             end;\n       end;\n\niii) repeat\n        for i:=1 to n do\n           begin\n              for j:=1 to n do;\n           end;\n     n:=n div 2\n     until n=1\n</code></pre>\n\n<p>In my opinion, the answer for (ii) is $O(\\log n)$ and the answer for (ii) is $O(n^3)$ but I'm not sure about my answers. Regarding question (iii) I have no idea how to come up with a solution.</p>\n", 'ViewCount': '83', 'Title': 'Measuring time complexity of algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T10:19:16.727', 'LastEditDate': '2013-09-09T10:19:16.727', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9979', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-04T12:44:31.107', 'Id': '14127''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Assume that I have a file that consists of pairs of numbers separated by a space. These numbers are the labels for vertices in my graph. For example:</p>\n\n<pre><code>0 5\n0 7\n2 3\n4 5\n1 5\n.\n.\n.\n</code></pre>\n\n<p>I want to create a graph (adjacency list) by reading this file line-by-line. For each line, I will create an edge between the two vertices. Of course, if the vertex doesn\'t exist yet, then I will add it before creating the edge.</p>\n\n<p>I read <a href="https://vismor.com/documents/network_analysis/graph_algorithms/S4.php" rel="nofollow">here</a> of an algorithm that builds a graph with a time complexity of $O(|V| + |E|)$ where $V$ = set of vertices and $E$ = set of edges. That makes sense to me. However my algorithm doesn\'t insert the vertices in a loop first and then insert all of the edges in another loop second. My algorithm just adds the vertices as it\'s looping through the edges.</p>\n\n<p>My question is if my algorithm is $O(|E|)$? It seems like that can\'t be right, but I read <a href="http://stackoverflow.com/questions/12231499/do-if-statements-affect-in-the-time-complexity-analysis">here</a> that when calculating the time complexity you don\'t take into account if statements. That\'s exactly what my vertex creation would be -- an if statement that checks if the node exists in the middle of my looping through all the edges.</p>\n', 'ViewCount': '261', 'Title': 'Time Complexity for Creating a Graph from a File', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T12:29:22.547', 'LastEditDate': '2013-09-09T12:29:22.547', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14224', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10045', 'Tags': '<algorithms><graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-09T04:28:52.703', 'Id': '14223''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was given the following code and was told to find the best and worst case running times in big theta notation. (Below is in python)</p>\n\n<pre><code>def find(a, target):\n    x = 0\n    y = len(a)\n    while x &lt; y:\n        m = (x+y)/2\n        if a[m] &lt; target:\n            x = m+1\n        elif a[m] &gt; target: \n            y = m\n        else:\n            return m\n    return -1\n</code></pre>\n\n<p>I know that the running time of this code in the worst case is $O(\\lg n)$. But the question I was given if the fifth line was changed from $m = \\frac{x+y}{2}$ to $m=\\frac{2x+y}{3}$, would the running time change?</p>\n\n<p>My intuition is that the running time gets a little larger as it is no longer cutting the list in half like binary search should do which is less efficient, but I am not sure how to calculate what the asymptotic runtime would be at this point.</p>\n', 'ViewCount': '98', 'Title': 'Runtime of various versions of binary search', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T18:05:11.853', 'LastEditDate': '2013-09-10T16:46:37.783', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '14253', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10062', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-10T16:15:23.313', 'Id': '14250''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '70', 'Title': 'Tight asymptotic bound for recursive algorithm', 'LastEditDate': '2013-09-11T11:39:35.377', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5066', 'FavoriteCount': '1', 'Body': "<p>I have this algorithm where:</p>\n\n<p>$$\nT(n) =\n\\begin{cases}\n  1 &amp; \\text{if}\\; n \\le 1 \\\\\n  T(n/2) + 1 &amp; \\text{otherwise} \\\\\n\\end{cases}\n$$</p>\n\n<p>So, evaluating for $T(0), T(1), T(2), T(3), \\ldots, T(n)$, I'm getting values like:\n$$ 1, 1, 2, 2, 3, 3, \\ldots, n, n $$</p>\n\n<p>I assume this is twice the sum of $1$ to $n$, that would be the same as $n (n+1)$ or $n^2+2$.</p>\n\n<p>Is my assumption ok?</p>\n", 'Tags': '<algorithm-analysis><asymptotics><recursion>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-11T13:19:20.067', 'CommentCount': '3', 'AcceptedAnswerId': '14269', 'CreationDate': '2013-09-11T05:05:47.037', 'Id': '14264''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose you are given an input set $S$ of $n$ numbers, and a black box that if given any sequence of real numbers and an integer $k$ instantly and correctly answers whether there is a subset of input sequence whose sum is exactly $k$. I want to show how to use the black box $O(n)$ times to find a subset of S that adds up to $k$.</p>\n\n<p>This is what I've done: the first time we enter our set $S$. If it returns yes we can continue, otherwise it isn't possible to form the sequence which sums up to $k$. The next step is to test our set without the first element. If the black box returns yes we can delete it from our set otherwise we know that it is needed. We do this for each element and our $S$ shrinks to a set which sums up to $k$. Can I use induction to prove this?</p>\n", 'ViewCount': '282', 'Title': 'Finding the subset of $S$ that sums up to $k$ using a black box in $O(n)$ time', 'LastEditorUserId': '472', 'LastActivityDate': '2014-01-14T22:38:12.713', 'LastEditDate': '2013-09-11T19:01:55.813', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><subsequences>', 'CreationDate': '2013-09-11T18:05:11.860', 'Id': '14270''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>One of my courses introduced the following question:</p>\n\n<blockquote>\n  <p>Given the recurrence relation for mergesort:</p>\n  \n  <p>$T(n) = 2T(n/2) + n$</p>\n  \n  <p>How would the following parameter passing strategies influence the relation and running time of the algorithm:</p>\n  \n  <ol>\n  <li><p>A pointer to the array is passed to the sorting function,</p></li>\n  <li><p>The entire array is copied to a separate location before applying the sorting function,</p></li>\n  <li><p>A subsection of the array is copied to a separate location before applying the sorting function on that subsection.</p></li>\n  </ol>\n</blockquote>\n\n<p>For 1., since this is how mergesort is used most of the time, we can solve it easily using the master theorem.</p>\n\n<p>$f(n) = \\Theta(n^{\\log_b a}) = \\Theta(n) \\implies T(n) = \\Theta(n \\log n)$</p>\n\n<p>For 2. however, I am a bit baffled. Although we do an additional work of $O(n)$, we are only doing so <em>once</em> at the beginning of the sort. Hence, doing one additional instance of $O(n)$ work should not influence the order of the running time (because it already <em>is</em> of a larger order). Hence, for both 2. and 3. the running time would remain at $T(n) = \\Theta(n \\log n)$. </p>\n\n<p>Is this reasoning valid? Something tells me that the $O(n)$ copying should have more of an impact, but I can't seem to give myself a good enough reason that it should be responsible to slow it down enough so that it would worse (i.e. $O(n^2)$). </p>\n\n<p>Any thoughts or hints would be quite appreciated!</p>\n", 'ViewCount': '134', 'Title': 'Do different variants of Mergesort have different runtime?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-16T07:58:23.627', 'LastEditDate': '2013-09-16T06:58:55.213', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14349', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10093', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation><sorting>', 'CreationDate': '2013-09-12T03:25:30.877', 'Id': '14275''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>In the last section of chapter 3 (page 54) in <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em> by Mitzenmacher and Upfal, a randomized algorithm is discussed for finding the median of a set $S$ of distinct elements in $O(n)$ time. As a substep the algorithm requires sorting a multiset $R$ of values sampled from $S$, where $|R| = \\lceil n^{3/4} \\rceil$. I'm sure this is a fairly simple, and straight forward question compared to the others on this site, but how is sorting a multiset of this size bounded by $O(n)$? </p>\n\n<p>I assume the algorithm is using a typical comparison-based, deterministic sorting algorithm that runs in $O(n\\log$ $n)$. So, If I have the expression $O(n^{3/4}\\log(n^{3/4}))$, isn't this just equivalent to $O(\\frac{3}{4}n^{3/4}\\log(n)) = O(n^{3/4}\\log(n))$? How does the reduction down to $O(n)$ proceed?</p>\n", 'ViewCount': '107', 'Title': 'Randomized Median Element Algorithm in Mitzenmacher and Upfal: O(n) sorting step?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:46:22.310', 'LastEditDate': '2013-09-16T07:46:22.310', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10128', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><randomized-algorithms>', 'CreationDate': '2013-09-13T20:07:11.977', 'Id': '14296''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '82', 'Title': 'Time Complexity of Algorithm', 'LastEditDate': '2013-09-16T07:43:21.313', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4658', 'FavoriteCount': '0', 'Body': "<p>I need help with finding out the time complexity of the following algorithm:</p>\n\n<pre><code>procedure VeryOdd(integer n):\nfor i from 1 to n do\n  if i is odd then\n    for j from i to n do\n      x = x + 1\n    for j from 1 to i do\n      y = y + 1\n</code></pre>\n\n<p>This is my attempt:</p>\n\n<p>$$ Loop1 = \\Theta(n)$$\n$$ Loop2 = \\Theta(n)$$\n$$ Loop2 = O(n)$$</p>\n\n<p>And we also know that loop2 and loop3 will get executed every second time of the execution of the outer loop. So we know that:</p>\n\n<p>$$T(n) = \\Theta(n) * 1/2(\\Theta(n) + O(n)) = \\Theta(n^2)$$</p>\n\n<p>Now to the thing I'm not so sure about, nameley, is Loop3 really $$O(N)$$ and\nif yes, then is $$\\Theta(n) + O(n) = \\Theta(n)$$</p>\n\n<p>Thanks in advance</p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:43:21.313', 'CommentCount': '2', 'AcceptedAnswerId': '14303', 'CreationDate': '2013-09-13T22:40:48.113', 'Id': '14298''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am working on multiple patterns search matching algorithms and I found that two algorithms are the strongest candidates, namely <a href="http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm" rel="nofollow">Aho-Corasick</a> and <a href="http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm" rel="nofollow">Rabin-Karp</a> in terms of running time. However, there are few things still foggy. I could not find any comprehensive comparison between the two algorithms. Besides, what I need to know is which one of them is more suitable for parallel computing and multiple patterns search. Which one require less hardware resources.  </p>\n\n<p>For AC algorithm searching phase time complexity is O(n+m), while it is O(nm) for RK. However, running time for RK is O(n+m) which make it similar to AC. My conclusion is RK practically better as it does not need memory as AC. I need a confirmation of that.  </p>\n', 'ViewCount': '234', 'Title': 'A comparison between Aho-Corasick algorithm and Rabin-Karp algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-09T00:14:12.477', 'LastEditDate': '2013-09-16T07:47:49.863', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9855', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><strings>', 'CreationDate': '2013-09-14T16:34:08.873', 'Id': '14309''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose an algorithm has a runtime recurrence relation:</p>\n\n<p>$   T(n) = \\left\\{\n     \\begin{array}{lr}\n       g(n)+T(n-1) + T(\\lfloor\\delta n\\rfloor ) &amp; : n \\ge n_0\\\\\n       f(n) &amp; : n &lt; n_0\n     \\end{array}\n   \\right.\n$  </p>\n\n<p>for some constant $0 &lt; \\delta &lt; 1$. Assume that $g$ is polynomial in $n$, perhaps quadratic. Most likely, $f$ will be exponential in $n$.</p>\n\n<p>How would one go about analyzing the runtime ($\\Theta$ would be excellent)? The master theorem and the more general Akra-Bazzi method do not seem to apply.</p>\n', 'ViewCount': '175', 'Title': "Asymptotic approximation of a recurrence relation (Akra-Bazzi doesn't seem to apply)", 'LastEditorUserId': '8877', 'LastActivityDate': '2013-11-13T19:26:31.243', 'LastEditDate': '2013-11-13T19:26:31.243', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8877', 'Tags': '<algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2013-09-15T22:17:33.163', 'FavoriteCount': '1', 'Id': '14343''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>If we have $n$ elements $s_1, \\dots, s_n$ and build a kind of treap (tree-heap) out of it. Each $s_k$ has a priority, which is an integer in $\\{ 1, 2, 3 \\dots, \\lceil \\log n \\rceil\\}$. Since the priorities will have duplicates, I just want the treap the verify that for each node $s_k$, all the nodes in its right and left subtrees have smaller priority. </p>\n\n<p>Is there a way to find the expected depth of this tree?</p>\n', 'ViewCount': '96', 'Title': 'Expected depth of modified kind of treap', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:06:30.243', 'LastEditDate': '2013-09-18T09:06:30.243', 'AnswerCount': '0', 'CommentCount': '8', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10186', 'Tags': '<algorithm-analysis><data-structures><search-trees><heaps>', 'CreationDate': '2013-09-17T21:35:36.280', 'Id': '14391''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Some complicated algorithms (<a href="http://en.wikipedia.org/wiki/Union-find">union-find</a>) have the nearly-constant inverse Ackermann function that appears in the asymptotic time complexity, and are worst-case time optimal if the nearly constant inverse Ackermann term is ignored. </p>\n\n<p>Are there any examples of known algorithms with running times that involve functions that grow fundamentally slower than inverse Ackermann (e.g. inverses of functions that are not equivalent to Ackermann under polynomial or exponential etc. transformations), that give the best-known worst-case time complexity for solving the underlying problem?</p>\n', 'ViewCount': '207', 'Title': 'Do functions with slower growth than inverse Ackermann appear in runtime bounds?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-18T21:26:09.597', 'LastEditDate': '2013-09-18T21:26:09.597', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<reference-request><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-17T23:51:31.520', 'FavoriteCount': '3', 'Id': '14392''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am trying to calculate the runtime complexity of a function that does not have fixed size input, but uses several helper methods that do have fixed size input. I was unsure of how to include the helper methods in my calculations. </p>\n\n<p>If I have an array with a fixed size of 32 indices, and I have a function that sums up the elements in that array, will that function be $O(n)$, or $O(1)$?  I think that a function that sums up the elements of an array is $O(n)$ because each element of an $n$-length array must be visited, but if the function only sums up arrays of length 32, is it still $O(n)$?</p>\n', 'ViewCount': '61', 'Title': 'How to include calls to an $O(n)$ subroutine on finite-sized inputs in an analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:49:49.943', 'LastEditDate': '2013-09-18T09:49:49.943', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '14412', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9102', 'Tags': '<terminology><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-18T08:42:53.543', 'Id': '14408''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Consider the following sequence $a_n$:</p>\n\n<p>\\begin{align*}\n  a_0 &amp;= \\alpha \\\\\n  a_k &amp;= \\beta a_{k-1} + \\kappa\n\\end{align*}</p>\n\n<p>Now consider the implementation of this sequence via lisp:</p>\n\n<pre><code>(defun an (k)\n  (if (= k 0) alpha\n    (+ (* beta (an (- k 1))) kappa)))\n</code></pre>\n\n<p>What is the running time to compute <code>(an k)</code> just as it is written? Also, what would be the maximum stack depth reached? My hypothesis would be that both the running time and stack depth are $O(k)$ because it takes $k$ recursive calls to get to the base case and there is one call per step and one stack push per step.\\\n.</p>\n\n<p>Is this analysis corect?</p>\n', 'ViewCount': '50', 'Title': 'Running time and stack depth of a lisp recurrence', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-18T22:25:04.060', 'LastEditDate': '2013-09-18T22:25:04.060', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14425', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Tags': '<algorithm-analysis><programming-languages><recurrence-relation>', 'CreationDate': '2013-09-18T19:00:44.293', 'Id': '14419''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I know how to use the FFT for multiplying two equations in $O(n\\,log\\,n)$ time, but is there a way to use FFT to compute the expanded equation before simplifying?</p>\n\n<p>For example, if you are multiplying $A(x) = 1 + 2x + 6x^2$ and $B(x) = 4 + 5x + 3x^2$ such that you get $C(x) = A(x) \\cdot B(x) = 4 + 5x + 3x^2 + 8x + 10x^2 + 6x^3 + 24x^2 + 30x^3 + 18x^4$ without going directly to the simplified answer?</p>\n\n<p>Furthermore, is it possible to use FFT to do this expanded form multiplication in $O(n\\,log\\,n)$ time? If so, can you show me how to apply FFT to this scenario?</p>\n', 'ViewCount': '76', 'Title': 'FFT for expanded form of equation multiplication', 'LastEditorUserId': '10052', 'LastActivityDate': '2014-03-31T23:46:56.920', 'LastEditDate': '2013-09-22T04:25:36.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14510', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10052', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><fourier-transform>', 'CreationDate': '2013-09-22T04:17:41.320', 'Id': '14509''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose I have a program p that has time complexity $O(n)$ and a second program q that calls program p $m$ times. If we know the input size of p will be the same every one of those times (n), we can say that the complexity of q is $O(m \\times n)$. This case sounds pretty simple to me.</p>\n\n<p>However, suppose the input size of p varies over these $m$ times, from $0$ to $m-1$.</p>\n\n<p>If we knew that p ran precisely $f(n) = a \\times n$ instructions for an input of size $n$, it would be simple: q would run $a \\times 0 + a \\times 1 + \\ldots + a \\times (m-1) = am(m-1)/2$ instructions exactly, which is $O(m^2)$.</p>\n\n<p>However, in this case, all we know is that $f(n)$ is below $a \\times n$ for all $n$ greater than some $n_0$, by the definition of big-O. We can't say for sure that $f(0) + f(1) + ... + f(m-1)$ is below $a \\times 0 + a \\times 1 + \\ldots + a \\times (m-1)$, because we don't know that $0, 1, \\ldots, m-1$ are greater than this $n_0$. For this reason, I don't think we can say that q runs in $O(m^2)$.</p>\n\n<p>Is there a way, in this case, when all we know is the big-O behaviour of p, to analyze q?</p>\n\n<p>What if we replace all the big-O's with big-Theta's?</p>\n", 'ViewCount': '86', 'Title': 'Big O and program calls with varied input sizes', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-17T16:01:47.387', 'LastEditDate': '2013-11-17T16:01:47.387', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14535', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10283', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-09-22T23:26:38.763', 'Id': '14532''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m presented with two snippets of code, and I need to determine their time complexity. I\'m pretty convinced that both of these are $O(n^2)$, but I\'m not 100% sure</p>\n\n<p>1.)</p>\n\n<p><img src="http://i.stack.imgur.com/p9ZFx.png" alt="enter image description here"></p>\n\n<p>2.)</p>\n\n<p><img src="http://i.stack.imgur.com/aWgVg.png" alt="enter image description here"></p>\n', 'ViewCount': '156', 'Title': 'Are the following loops $O(n^2)$ complexity', 'LastEditorUserId': '10188', 'LastActivityDate': '2013-09-24T22:44:37.440', 'LastEditDate': '2013-09-24T02:00:31.457', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10304', 'Tags': '<algorithm-analysis>', 'CreationDate': '2013-09-23T20:34:00.170', 'Id': '14565''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Im studying for the computer science subject GRE test, as an exercise i need to implement the followin algorithm in java, any idea on how to aproach it?.</p>\n\n<p>Given a set $X$ and $z$ not in $X$, indicate between which would be the immediate positions $z$ in $X$</p>\n', 'ViewCount': '18', 'Title': 'Immediate positions algorithm?', 'LastActivityDate': '2013-09-25T04:13:11.850', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9744', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2013-09-25T03:45:39.477', 'Id': '14588''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Can anyone give me some hint on solving $T(n) = T(\\frac{n}{2} + \\sqrt{n}) + n$, with any method, master method, substitution....\nI have came up with this : $T(n) = T(\\frac{(\\sqrt{n} + 1)^{2} - 1}{2}) + n$ and then substituting $n = 2^{m}$ but things gets really messy !</p>\n\n<p>any idea?</p>\n', 'ViewCount': '114', 'Title': 'Solving a recurrence', 'LastActivityDate': '2013-10-06T10:21:32.623', 'AnswerCount': '4', 'CommentCount': '0', 'AcceptedAnswerId': '14857', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10030', 'Tags': '<algorithm-analysis>', 'CreationDate': '2013-09-26T17:05:02.350', 'Id': '14629''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '234', 'Title': 'Complexity of keeping track of $K$ smallest integers in a stream', 'LastEditDate': '2014-01-30T21:46:49.597', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10384', 'FavoriteCount': '2', 'Body': '<p>I need to analyze the time complexity of an online algorithm to keep track of minimum $K$ numbers from a stream of $R$ numbers. The algorithm is </p>\n\n<ul>\n<li>Suppose the $i$th number in the stream is $S_i$.</li>\n<li>Keep a max heap of size $K$.</li>\n<li>If the heap contains fewer than $K$ elements, add $S_i$ to the heap.</li>\n<li>Otherwise: if $S_i$ is smaller than the maximum element in the heap (i.e., the root of the heap), replace the root of the heap with $S_i$ and apply Max-Heapify to the heap; if $S_i$ is greater than or equal to the max element, do nothing.</li>\n</ul>\n\n<p>The problem now is to find the expected number of times the Max Heapify operation will be called, when the stream of integers is of length $R$ and each element of the stream is (iid) uniformly distributed on $[1,N]$.</p>\n\n<p>If the stream were guaranteed to contain only distinct elements, then the answer is easy: </p>\n\n<p>$$E[X] = E[X_1] + E[X_2] + \\dots + E[X_R],$$</p>\n\n<p>where $X_i$ is the random indicator variable for occurrence of the Max Heapify operation at the $i$th number in the stream.  Now  </p>\n\n<p>$$E[X_i] = \\Pr[\\text{$S_i$ is ranked $\\le K$ among first $i$ elements}] = \\min(K/i, 1).$$</p>\n\n<p>Hence,</p>\n\n<p>$$E[X] = K + K/(K+1) + \\cdots + K/R.$$</p>\n\n<p>That case is relatively easy.  But how do we handle the case where the elements are iid uniformly distributed?</p>\n\n<p>[This was actually a Google interview question.] </p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-07T12:35:40.240', 'CommentCount': '0', 'AcceptedAnswerId': '20261', 'CreationDate': '2013-09-28T19:59:47.083', 'Id': '14661''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Let $S$ be a convex polygon on $n$ points. Given two points $A$ and $B$, where $A$ is left of $S$, and $B$ is right of $S$, what's an algorithm to find the shortest path from $A$ to $B$, that avoids the interior of $S$? What about the longest path?</p>\n", 'ViewCount': '114', 'Title': 'Finding both the longest and shortest path in a convex polygon', 'LastEditorUserId': '9665', 'LastActivityDate': '2013-10-31T17:09:25.687', 'LastEditDate': '2013-10-01T16:38:51.820', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9665', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><computational-geometry>', 'CreationDate': '2013-10-01T16:03:11.983', 'FavoriteCount': '1', 'Id': '14736''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>There are $n$ elements in a hash table of size $m \\geq 2n$ which uses open addressing to avoid collisions. </p>\n\n<p>The hash function was chosen randomly among a set of uniform functions. A set $H$ of hash-functions $h:U\\to\\{0,\\dots,m-1\\}$ is called uniform, if for every tuple of different keys $x,y \\in U$ the number of hash-functions $h \\in H$ with $h(x) = h(y)$ is $\\frac{|H|}{m}$ at most.</p>\n\n<p>Show that the propability that for $i = 1, 2, \\dots,n$ the $i$-th insert operation needs more than $k$ attempts, is $2^{-k}$ at most.</p>\n\n<p>This is an assignment, which I got as homework. What I already worked out:</p>\n\n<p>The propability $p_1$ for a collision is 0 of course for an empty table.</p>\n\n<p>The propability $p_i$ for a collision after k attempts should be $\\frac{i - 1}{2n}\\cdot k$ assuming that the table is filled with $i-1$ elements to this point and the tables size is $2n$ as worst case.</p>\n\n<p>So I have\n$$\np_i= \\frac{i-1}{2n} \\cdot k \\leq 2^{-k},\n$$</p>\n\n<p>but I don't know where to go from here.</p>\n\n<p>The method of open hashing used here simply iterates over different hash-functions until a free place is found (for example $h(x) = (x \\bmod j) \\bmod n$ with increasing prime numbers for $j$.</p>\n", 'ViewCount': '140', 'Title': 'Proof that probability that hashing with open addressing needs more than $k$ attempts is $2^{-k}$ at most', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-08T16:59:37.943', 'LastEditDate': '2014-01-07T16:06:08.057', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6424', 'Tags': '<algorithm-analysis><data-structures><hash-tables>', 'CreationDate': '2013-10-06T22:13:05.387', 'FavoriteCount': '1', 'Id': '14862''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '268', 'Title': 'What is the asymptotic runtime of this nested loop?', 'LastEditDate': '2014-03-29T14:10:44.667', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8690', 'Body': '<p>I am trying to analyse the runtime of this algorithm:</p>\n\n\n\n<pre><code>for(i=1; i &lt; n; i++){\n   for(j=1; j &lt;= i; j++){\n         statement1;\n   }       \n}\n</code></pre>\n\n<p>Expanding the above loop into.</p>\n\n<ul>\n<li><p>First : </p>\n\n   \n\n<pre><code>for(j=1; j &lt;= 1; j++){\n   statement1;          //complexity O(1)\n}\n</code></pre></li>\n<li><p>Second:  </p>\n\n   \n\n<pre><code>for(j=1; j &lt;=2 ; j++){\n   statement1;          //complexity O(2)\n}       \n</code></pre></li>\n</ul>\n\n<p>... </p>\n\n<ul>\n<li><p>n-th:</p>\n\n<pre><code>for(j=1; j &lt;= n; j++){\n   statement1;          //complexity O(n)\n}\n</code></pre></li>\n</ul>\n\n<p>So the runtime of the loop should be</p>\n\n<p>$\\qquad \\displaystyle O(1) + \\dots + O(n) = O\\Bigl(\\frac{n(n+1)}{2}\\Bigr) = O(n^2)$.</p>\n\n<p>Can I reason like this, or what is the proper way to analyse nested <code>for</code>-loops?</p>\n', 'ClosedDate': '2014-03-29T13:23:48.603', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T21:51:39.133', 'CommentCount': '3', 'AcceptedAnswerId': '14893', 'CreationDate': '2013-10-07T06:50:10.650', 'Id': '14880''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have been reading about the Strassen Algorithm for matrix multiplication.</p>\n\n<p>As mentioned in Introduction to Algorithms by Cormen , the algorithm is not intuitive. However I am curious to know if there exists any rigorous mathematical proof of the algorithm and what actually went into the design of the algorithm.</p>\n\n<p>I tried searching on Google and stackoverflow, but all links are only on comparing Strassen\'s approach to standard matrix multiplication approach or they elaborate on the procedure presented by the algorithm.</p>\n\n<p>Note: The same question has been asked on <a href="http://stackoverflow.com/questions/19229454/strassens-algorithm-proof">http://stackoverflow.com/questions/19229454/strassens-algorithm-proof</a> , but without any definite answers</p>\n', 'ViewCount': '237', 'Title': "Strassen's Algorithm proof", 'LastActivityDate': '2013-10-08T17:37:31.217', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9548', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-10-08T11:51:57.770', 'FavoriteCount': '1', 'Id': '14907''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Given a sequence $(a)$ of lenght $n$, such $a_1 = a_n = 1$ and $a_i \\in \\left \\{ -1, 1 \\right\\}$.  </p>\n\n<p>We want to find a subsequence $(b) \\subset (a)$ which statisfies the following conditions:  </p>\n\n<ul>\n<li>The length of $(b)$, (let's call it $k$) is the largest possible</li>\n<li>$b_k = b_1 = 1$</li>\n<li>Each prefix sum $b_1$, $b_1+b_2$,  $b_1+b_2 + b_3,\\ \\ldots\\ , b_1 + b_2 + \\ldots + b_k$ is $\\geq 0$.</li>\n<li>Each suffix sum $b_k$, $b_{k} + b_{k-1}, b_{k} + b_{k-1} + b_{k-2}, \\ \\ldots\\ , b_k + b_{k-1} + \\ldots + b_1$ is $\\geq 0$. </li>\n<li>Sum of $(b)$ is the biggest possible.</li>\n</ul>\n\n<p>Is it possible to construct linear algorithm to solve it? </p>\n", 'ViewCount': '67', 'Title': 'Maximizing length of subsequence', 'LastEditorUserId': '10555', 'LastActivityDate': '2013-10-09T00:08:15.623', 'LastEditDate': '2013-10-08T21:30:50.137', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14922', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10555', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-10-08T18:09:51.267', 'Id': '14918''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '61', 'Title': 'Time complexity of an algorithm', 'LastEditDate': '2013-10-09T07:12:27.010', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10593', 'Body': '<p>Currently stuck on a question.</p>\n\n<p>"Assume the time complexity of an algorithm on input size is 6n^3. If the algorithm takes 10 seconds to execute for an input size of n. Then how many seconds will it take for an input size of 2n."</p>\n\n<p>Thanks!</p>\n', 'ClosedDate': '2013-10-30T10:04:58.103', 'Tags': '<algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-09T07:12:27.010', 'CommentCount': '5', 'AcceptedAnswerId': '14929', 'CreationDate': '2013-10-08T21:29:18.650', 'Id': '14925''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>What is the time complexity of the recurrence relation \nT(xn) + T((1 \u2212 x)n) + cn</p>\n', 'ViewCount': '37', 'ClosedDate': '2013-10-09T11:55:46.847', 'Title': u'Time Complexity of T(xn) + T((1 \u2212 x)n) + cn', 'LastActivityDate': '2013-10-09T03:41:12.370', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '10598', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-10-09T00:30:50.530', 'Id': '14936''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In class we saw the followin problem but i didnt undestand the solution. Do anybody could explain me with more detail the procedure to solve this problem or give me a better solution?:</p>\n\n<blockquote>\n  <p>Assume that $n$ points in the plane are given. Find a polygonal arc with $n-1$ sides whose vertices are given points, and whose sides do not intersect. (Adjacent sides may form a $180$ angle). The number of operations shold be of order $n$ $log$ $n$.</p>\n</blockquote>\n\n<p>The teacher solution was:</p>\n\n<blockquote>\n  <p>Sort all the points with respect to the x-coordinate; when x-coordinates are equal, take the y-coordinate into account, then connect all the vertices by line segments(in that order).</p>\n</blockquote>\n', 'ViewCount': '45', 'ClosedDate': '2013-11-28T21:51:16.080', 'Title': 'Finding a polygonal arc algorithm?', 'LastActivityDate': '2013-10-09T15:57:50.583', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><algorithm-analysis><graphs><sorting>', 'CreationDate': '2013-10-09T15:57:50.583', 'Id': '14955''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>So I have some graph, and I know what it's max flow is based of the Ford-Fulkerson Algorithm.</p>\n\n<p>With this information, I need to know how to find a new max flow when I remove an edge of this graph with flow capacity 1. The algorithm must run in O(v+e) time where v is the number of vertices in the graph, and e the number of edges.</p>\n\n<p>I have an idea, but I don't think it will run in O(v+e). \nI might just be confused about big O notation, but my algorithm would run in O(v+e+v)=O(2v+e) time. Would that just be considered O(v+e) time? If not, what part of my proposed algorithm should be reconsidered? Should I make a completely different approach (maybe considering if the deleted edge is in the min-cut or not?).</p>\n\n<p>Thanks in advance!</p>\n", 'ViewCount': '85', 'ClosedDate': '2013-10-14T07:48:03.477', 'Title': 'Recalculate max-flow after removing edge with 1 capacity', 'LastEditorUserId': '10680', 'LastActivityDate': '2013-10-13T21:45:05.603', 'LastEditDate': '2013-10-13T21:44:47.073', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10680', 'Tags': '<algorithms><graph-theory><algorithm-analysis><dynamic-programming>', 'CreationDate': '2013-10-12T20:18:55.893', 'Id': '15023''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>It seems to me that taken over the entire graph, the norm of the error vector must be monotonically decreasing, otherwise we couldn't guarantee that PageRank would ever converge.</p>\n\n<p>However, is the same true on a per-vertex basis? I.e., from iteration t to iteration t+1, is the squared error of a vertex guaranteed to always decrease as it moves closer to its PageRank value? Or is it possible that the vertex squared error would ever increase?</p>\n\n<p>This also seems to me to have some broader relationship with power iterations in general? Some explanation or proof with the answer would be appreciated.</p>\n", 'ViewCount': '48', 'Title': 'Is the per-vertex error over a PageRank iteration monotonically decreasing?', 'LastActivityDate': '2013-10-15T02:00:25.790', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16094', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '10683', 'Tags': '<graph-theory><algorithm-analysis><error-estimation>', 'CreationDate': '2013-10-12T20:56:11.653', 'Id': '15024''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was asked to "Compute the average runtime for a binary search, ordered array, and the key is in the array." I\'m not quite sure how to approach this problem. Isn\'t the runtime of binary search O(log n)? And the average would be something like n + n/2 + n/4... etc?</p>\n\n<p>I\'m then asked to "Implement a program performing an empirical test of the binary search (using a fixed number of random arrays for each n), then do a ratio test justifying your analytical answer." How would I go about doing this? Could I perform a basic binary search algorithm on a number of random arrays, counting the basic operations, and compare that to my original analysis from the first question?</p>\n\n<p>I appreciate any help/guidance here.</p>\n', 'ViewCount': '699', 'Title': 'How to analyze/test a binary search algorithm?', 'LastActivityDate': '2013-10-14T02:03:59.603', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16060', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10719', 'Tags': '<algorithm-analysis><runtime-analysis><binary-search>', 'CreationDate': '2013-10-13T22:58:49.533', 'Id': '16057''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I was given the following homework question:</p>\n\n<blockquote>\n  <p>Implement an extendable table using arrays that can increase in size\n  as elements are added. Perform an experimental analysis of each of the\n  running times for performing a sequence of n add methods, assuming the\n  array size is increased from N to the following possible values: </p>\n  \n  <ul>\n  <li>2N  </li>\n  <li>N + ceiling(\u221aN)</li>\n  <li>N + ceiling(log N)  </li>\n  <li>N + 100</li>\n  </ul>\n</blockquote>\n\n<p>I\'m just a little confused about what this question is asking, and was hoping for some help/clarification. The way I understand it, you could implement something like this in Python with a two-dimensional array (the "extendable table"), and then append varying numbers of values for each scenario. Am I understanding the implementation correctly?</p>\n\n<p>Then, I\'m also a bit unclear on what number of values you\'d be appending. Would you literally first test it with say, 16 values, then 32 (2N), then 20 (ceiling(\u221aN)), etc? Or is it more complex than that?\nAny help is appreciated!</p>\n', 'ViewCount': '38', 'ClosedDate': '2013-10-30T10:04:14.270', 'Title': 'experimental analysis of running times in extendable table', 'LastActivityDate': '2013-10-14T21:04:29.030', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16087', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10719', 'Tags': '<algorithm-analysis><runtime-analysis><arrays>', 'CreationDate': '2013-10-14T20:02:02.757', 'Id': '16086''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>This is an exercise in the book Introduction to Algorithm, 3rd Edition.</p>\n\n<p>The original question is:</p>\n\n<p>Show how to implement GREEDY-SET-COVER in such a way that it runs in time $O(\\sum_{S\\in\\mathcal{F}}|S|)$.</p>\n\n<p>The GREEDY-SET-COVER in the text book is as follows:</p>\n\n<p><img src="http://i.stack.imgur.com/v55Gn.png" alt="greedy-set-cover-in-text-book"></p>\n\n<p>Definition for $(X,\\mathcal{F})$ is given as:</p>\n\n<p><img src="http://i.stack.imgur.com/3aVbz.png" alt="enter image description here"></p>\n', 'ViewCount': '397', 'ClosedDate': '2013-10-30T10:05:07.743', 'Title': 'How to implement GREEDY-SET-COVER in a way that it runs in linear time', 'LastActivityDate': '2013-10-16T18:49:41.563', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4662', 'Tags': '<algorithms><algorithm-analysis><time-complexity><greedy-algorithms>', 'CreationDate': '2013-10-16T17:15:40.530', 'Id': '16142''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>To find the maximum flow in a graph, why doesn't it suffice to only saturate all augmenting paths with the minimum edge capacity in that path without considering the back-edges? I mean, what is the point calling it a back-edge if we assume flow from it?</p>\n", 'ViewCount': '52', 'Title': 'Saturating all augmenting paths with the minimum edge capacity in max flow', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-18T21:07:48.467', 'LastEditDate': '2013-10-18T21:07:48.467', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10525', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs><network-flow>', 'CreationDate': '2013-10-18T17:15:17.717', 'Id': '16202''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Given a procedure/function </p>\n\n<p>$Select(S,r)$ - which selects element of rank r from set S\nwhich uses at most $|S|. constant$ comparisons</p>\n\n<p>We design another function $Multiselect(S,R)$\nwith $R =\\{r_1&lt;r_2&lt;...&lt;r_k\\}$<br>\nreturns $X =\\{x_1&lt;x_2&lt;...&lt;x_k\\}$  such that rank of $x_i$ is $r_i$</p>\n\n<p>What is the minimum no of comparisons for this function?</p>\n\n<p>From Wikipedia I found the upper bound for it -\n<img src="http://i.stack.imgur.com/Nmfem.png" alt="upper bound"></p>\n\n<p>But while utilising this for deriving the answer I got it totally messed up!<br>\nI tried using algorithm to utilise Select(S,r) repeatedly each time<br>\neach time decreasing the search space by logn  ... but I guess the answer could be found<br>\nin a more concise mathematical way.</p>\n\n<p>By the way, answer given was - $constant.|S|(1+logR)$  </p>\n', 'ViewCount': '44', 'Title': 'Selection of K elements of given ranks', 'LastEditorUserId': '8514', 'LastActivityDate': '2013-10-21T20:13:13.250', 'LastEditDate': '2013-10-21T20:13:13.250', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '16243', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8514', 'Tags': '<complexity-theory><algorithm-analysis>', 'CreationDate': '2013-10-19T21:20:45.603', 'Id': '16239''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've been stuck with this problem for 2 weeks. Any idea of how to aproach it?.</p>\n\n<blockquote>\n  <p>Let $L$ be a list of $n$ different integer numbers, assume that the elements of $L$ are in the range $[1,750]$. Design a linear ordering algorithm to order the elements of $L$.</p>\n</blockquote>\n\n<p>I already tried with insertion sort. But I'm not sure if my approach is right:</p>\n\n<ul>\n<li>Construct an array of bits. Initialize them to zero.</li>\n<li>Read the input, for each value you see set the respective bit in the array to 1.</li>\n<li>Scan the array, for each bit set, output the respective value.</li>\n</ul>\n\n<p>Complexity: $O(2n) = O(n)$</p>\n\n<p>I also wanted to use radix sort but I can't understand how to apply it, any idea?</p>\n", 'ViewCount': '146', 'Title': 'Sorting in O(n) time in a finite domain', 'LastEditorUserId': '39', 'LastActivityDate': '2013-10-23T10:11:12.493', 'LastEditDate': '2013-10-23T10:11:12.493', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><algorithm-analysis><time-complexity><sorting><radix-sort>', 'CreationDate': '2013-10-23T06:11:54.673', 'Id': '16350''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Is it practically possible or even near possible to make parity game to be solved in polynomial time? If yes, how? and if No, why?</p>\n', 'ViewCount': '113', 'Title': 'Is it possible to solve parity game problem in polynomial time?', 'LastActivityDate': '2013-10-24T15:35:31.673', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16401', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10700', 'Tags': '<algorithms><algorithm-analysis><polynomial-time>', 'CreationDate': '2013-10-24T14:53:19.353', 'Id': '16399''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Consider the following problem:</p>\n\n<blockquote>\n  <p>Give an algorithm to find the $1^{st}, 2^{nd}, 3^{th}$ fastest horses from 25 horses. In each round, at most 5 horses can race and you can get the exact position of these horses. Analyze the lower bound of this problem using <em>adversary argument</em>. One race is considered as one critical operation.</p>\n</blockquote>\n\n<p>I can figure out a solution using 7 races:</p>\n\n<blockquote>\n  <ol>\n  <li>Divide 25 horses into 5 groups with 5 horses for each: $A: a_1, a_2, a_3, a_4, a_5$; $B: b_1, b_2, b_3, b_4, b_5$; $C: c_1, c_2, c_3, c_4, c_5$; $D: d_1, d_2, d_3, d_4, d_5$; and $E: e_1, e_2, e_3, e_4, e_5$.</li>\n  <li>One race within each group. Suppose that the position of each horse in each group is consistent with its index: e.g., $A: a_1 &gt; a_2 &gt; a_3 &gt; a_4 &gt; a_5$.</li>\n  <li>One race for $a_1, b_1, c_1, d_1, e_1$ and get $a_1 &gt; b_1 &gt; c_1 &gt; d_1 &gt; e_1$. Thus, $a_1$ is the fastest horse.</li>\n  <li>The second and third ones are among $a_2, a_3, b_1, b_2, c_1$. So one more race is enough.</li>\n  </ol>\n</blockquote>\n\n<p>However I have difficulty in analyzing its lower bound using adversary argument.\nSo my problem is:</p>\n\n<blockquote>\n  <p>How to analyze its lower bound using <em>adversary argument</em>? What is the adversary strategy?</p>\n</blockquote>\n', 'ViewCount': '219', 'Title': 'How to analyze the lower bound of the horse racing problem using adversary argument?', 'LastActivityDate': '2013-10-25T07:05:30.847', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16420', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithms><algorithm-analysis><lower-bounds>', 'CreationDate': '2013-10-25T06:05:44.517', 'Id': '16417''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I am unclear about finding the memory complexity of an algorithm.</p>\n\n<p>Some places refer memory complexity as what container would be carrying for instance:</p>\n\n<pre><code>for i = 1 to n-1\n     if d[i] == d[i + 1]\n           d[i] = (d[i] + 5) mod 13\n</code></pre>\n\n<p>Is considered as having $\\theta(N)$ memory complexity.</p>\n\n<p>At some other places how much data we write to a container is a complexity for instance:</p>\n\n<pre><code>reverse_list(n)\n\n    Stack res\n    while (n != NULL)\n         res push n\n         n = n-&gt;next\n    while (res != null) \n         a = pop res\n         print a\n</code></pre>\n\n<p>Is considered as having a memory complexity of $\\theta(N)$ too. Moreover:</p>\n\n<p>Such thing is considered having $\\theta(1)$ memory complexity</p>\n\n<pre><code>reverse_list(head)\n    last = NULL;\n    while(last != head)\n        current = head\n        while(current-&gt;next != last)\n            current = current-&gt;next\n        print current\n        last = current\n</code></pre>\n\n<p>I know how these algorithms work and what they do, but I don't understand how are we meant to be analysing their memory complexity. Could someone explain that please?</p>\n", 'ViewCount': '472', 'Title': 'Memory complexity?', 'LastEditorUserId': '8849', 'LastActivityDate': '2013-10-28T15:14:42.793', 'LastEditDate': '2013-10-28T15:14:42.793', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '16464', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics><space-complexity>', 'CreationDate': '2013-10-27T04:26:35.907', 'Id': '16461''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I cannot really find a source that does not use the same examples provided by CLRS. I need a simpler example than <code>MULTI-POP</code> example. Could someone provide an example and explain me:</p>\n\n<p>a) What is the difference between worst-case analysis and amortised analysis?</p>\n\n<p>b) What is the difference between average-case analysis and amortised analysis?</p>\n\n<p>c) In plain English(with using minimal notations), how can we provide a complexity(especially I am interested in potential method)</p>\n', 'ViewCount': '89', 'Title': 'Basics of Amortised Analysis', 'LastActivityDate': '2013-10-28T16:55:38.243', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithm-analysis><asymptotics><amortized-analysis><average-case>', 'CreationDate': '2013-10-28T15:14:12.807', 'FavoriteCount': '1', 'Id': '16502''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>For the following function (not in particular coding style or programming languange)</p>\n\n<pre><code>f (N, y)                // y is an integer such that 0 &lt;= y &lt; N\n{\n\n    x = rand (N)            // rand (N) returns a random integer x satisfying 0 &lt;= x &lt; N\n\n    if (y == rand (N))\n        return\n    else f (N, y)\n\n}\n</code></pre>\n\n<p>I need to find best, worst and average cases using O-notation. So my assumption is that the line</p>\n\n<pre><code>x = rand (N)\n</code></pre>\n\n<p>does nothing particular but in each recursion it takes O(1) (correct me if my assumption is wrong).</p>\n\n<p>Now, the best case is quite obvious - rand() will give y or O(1).</p>\n\n<p>The thing that I have stuck in are the worst and average cases. In the worst case it will just go into infinite iteration, but what is the complexity for a function that does not halt? And what is the average case? My assumption about the average case is that it will go through all N integers and then find y.</p>\n', 'ViewCount': '138', 'Title': 'Best, worst and average cases for a function that uses random number generator', 'LastActivityDate': '2013-10-30T18:02:07.800', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '7809', 'Tags': '<algorithm-analysis>', 'CreationDate': '2013-10-30T17:55:01.890', 'Id': '16586''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Say that you have a program with only total functions, or if that is impossible, consider a subset of the program with only total functions. Is it theoretically possible to statically determine the maximum size of the stack frame at runtime? The language may have other characteristics that are relevant/necessary (such as a specific type system).</p>\n', 'ViewCount': '43', 'Title': 'Say that a stack frame only contains total functions: could the maximum stack size be statically determined?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-02T09:40:56.593', 'LastEditDate': '2013-10-30T21:32:01.120', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '16593', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11089', 'Tags': '<algorithm-analysis><compilers>', 'CreationDate': '2013-10-30T21:00:34.483', 'Id': '16592''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '62', 'Title': 'Converting a recursive algorithm to a runtime function', 'LastEditDate': '2013-11-12T16:48:25.630', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10875', 'Body': "<pre><code>void Sort(int A[], int left, int right)\n{\n    int p;\n\n    if (left &lt; right)\n    {\n        p = (right + left + 2)/3;\n\n        Sort(A, left, left+p-1);\n        Sort(A, left+p, left+2*p-1);\n\n        MergeSort(A, left+2*p, right);\n\n        Merge3(A, left, left+p, left+2*p, right);\n    }\n}\n</code></pre>\n\n<p>I need to convert this function into a mathematical expression in order to solve it's run-time complexity.</p>\n\n<p>I know  that <code>MergeSort()</code>'s complexity is of $\\Theta(n \\log n)$ and that <code>Merge3()</code>'s complexity is of $\\Theta(n)$.</p>\n\n<p>I can't figure how to transform this into a recursive mathematical expression.</p>\n", 'ClosedDate': '2013-11-15T15:38:52.877', 'Tags': '<algorithm-analysis><runtime-analysis><recursion>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-12T18:03:24.563', 'CommentCount': '11', 'AcceptedAnswerId': '17964', 'CreationDate': '2013-11-12T12:37:24.567', 'Id': '17956''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>Is it possible to modify Dijkstra\xb4s algorithm in order to get the longest path from $s$ to $t$ ?.</p>\n\n<blockquote>\n  <p>My intuition says that I\xb4ll need a different algorithm entirely. Finding the longest path is the same as finding the shortest path on a graph with negative weights. However, Dijkstra\u2019s algorithm requires that the weights are positive, so it cannot be modified to calculate the longest path. A better algorithm to use could be: <a href="http://en.wikipedia.org/wiki/Longest_path_problem" rel="nofollow">http://en.wikipedia.org/wiki/Longest_path_problem</a></p>\n</blockquote>\n\n<p>Any idea of how to modify it?</p>\n', 'ViewCount': '1734', 'ClosedDate': '2013-11-17T01:20:19.727', 'Title': 'Is it possible to modify dijkstra algorithm in order to get the longest path?', 'LastActivityDate': '2013-11-14T03:25:44.443', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10610', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs>', 'CreationDate': '2013-11-13T07:40:20.250', 'Id': '17980''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I encountered these few recursive relations:</p>\n\n<p><code>T(n) = (n/2)T(n/2) + O(n^2)</code></p>\n\n<p><code>T(n) = T(n/2) + T(n/2-1) + ... + T(1) + O(n^2)</code></p>\n\n<p>I do notice that these recursions can be solved faster by DP. I'm just curious as to:</p>\n\n<ol>\n<li>What's the big-O of the running time of these relations, if we follow the resursive formula</li>\n<li>How do you analysis recursive relations like these.</li>\n<li>Does the trailing term (in this case O(n^2)) matter?</li>\n</ol>\n\n<p>Any help is much appreciated.</p>\n", 'ViewCount': '60', 'ClosedDate': '2013-11-13T21:56:00.057', 'Title': 'Running time of this recursive relation: T(n) = (n/2)T(n/2) + O(n^2)', 'LastActivityDate': '2013-11-13T18:49:11.920', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '17993', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11356', 'Tags': '<algorithm-analysis><asymptotics>', 'CreationDate': '2013-11-13T18:21:34.540', 'Id': '17992''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>here is a competitive programming question:</p>\n\n<p>You have a number of chores to do. You can only do one chore at a time and some of them depend on others. \nSuppose you have four tasks to complete. For convenience, we assume the\ntasks are numbered from 1 to 4. Suppose that task 4 depends on both task 2 and task\n3, and task 2 depends on task 1. One possible sequence in which we can complete the\ngiven tasks is [3,1,2,4] - in this sequence, no task is attempted before any of the other\ntasks that it depends on. The sequence [3,2,1,4] would not be allowed because task\n2 depends on task 1 but task 2 is scheduled before task 1. In this example, you can\ncheck that there exactly three possible sequences compatible with the dependencies:\n[3,1,2,4], [1,2,3,4] and [1,3,2,4].\nIn each of the cases below, you have N tasks numbered 1 to N with some dependencies\nbetween the tasks. You have to calculate the number of ways you can reorder all N\ntasks into a sequence that does not violate any dependencies.</p>\n\n<p>[Task, Dependency(s)] : [1, NA], [2,1], [3,2], [4,1], [5,4], [6, 3 and 5], [7,6], [8,7], [9,6], [10, 8 and 9].</p>\n\n<p>I inferred the following:</p>\n\n<ul>\n<li>Any sequence will always start with 1, since it has no dependencies.</li>\n<li>6 will always be in the 6th position of any sequence.</li>\n<li>10 will always be at the last position.</li>\n</ul>\n\n<p>Then, by trial and error, and listing all possibilities for the two separate parts of the sequence (1 _ _ _ _ 6 and 6 _ _ _ 10), I got 6x3 = 18 possibilities. However, for a larger set of data, these deductions would not be easy. What is the way to calculate this logically and find the number of possibilities, and how can this be integrated into a program?</p>\n\n<p>(I have tried to represent the question as clearly as possible, but you can visit this link to view the question (Q. No 4): <a href="http://www.iarcs.org.in/inoi/2013/zio2013/zio2013-qpaper.pdf" rel="nofollow">http://www.iarcs.org.in/inoi/2013/zio2013/zio2013-qpaper.pdf</a>)</p>\n\n<p>I am a high school student preparing for a programming competition, and I haven\'t taken many courses on algorithm design, so this might be a trivial question - please excuse me!</p>\n', 'ViewCount': '89', 'Title': 'Task Dependency/Combinatorics', 'LastActivityDate': '2013-11-17T00:54:12.167', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11415', 'Tags': '<algorithms><algorithm-analysis><combinatorics>', 'CreationDate': '2013-11-16T09:24:13.547', 'FavoriteCount': '1', 'Id': '18067''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>consider following code</p>\n\n<pre><code>int f(int x)\n{\n  if(x&lt;1) return 1;\n  else return f(x-1)+g(x);\n}\nint g(int x)\n{\n  if(x&lt;2) return 1;\n  else return f(x-1)+g(x/2);\n}\n</code></pre>\n\n<p><b>Questions:</b></p>\n\n<blockquote>\n  <p>How do I find the growth of <code>f(x)</code>, being that it contains a recursion to another function?</p>\n  \n  <p>Are growth and time complexity of a a function same thing? Are they the same for <code>f(x)</code>?</p>\n</blockquote>\n', 'ViewCount': '147', 'Title': 'Finding growth of "inter-recursive" functions', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-27T07:04:48.907', 'LastEditDate': '2013-11-24T01:34:40.410', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10380', 'Tags': '<algorithm-analysis><recurrence-relation><recursion>', 'CreationDate': '2013-11-23T19:07:15.820', 'FavoriteCount': '1', 'Id': '18283''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>Below is a homework problem where we have been asked to alter a greedy algorithm to return n element instance of a set problem. The original algorithm is also below. I was thinking that I could alter line 3 so that it would run until the size of C was equal to n, and change the logic in line 4 so that it would pick and remove vertices until the size of n. A vertex would be removed when the size of C doesn't equal to n but the cover is complete. I can't really think of any other way to do it. The real problem is that I'm not entirely sure how to make the algorithm run in exponential time like they are asking. </p>\n\n<blockquote>\n  <p>GREEDY-SET-COVER can return a number of different solutions, depending on\n  how we break ties in line 4. Give a procedure BAD-SET-COVER-INSTANCE.n/\n  that returns an n-element instance of the set-covering problem for which, depending\n  on how we break ties in line 4, GREEDY-SET-COVER can return a number of\n  different solutions that is exponential in n.</p>\n  \n  <p>$X$ \u2014 some finite set<br>\n  $F$ \u2014 a family of subsets of $X$<br>\n  $C$ \u2014 cover being constructed    </p>\n  \n  <p>GREEDY-SET-COVER($n$)<br>\n  1    let $U = X$<br>\n  2    let $C = \\varnothing$<br>\n  3    while $U \\ne \\varnothing$<br>\n  3a            select an $S \\in F$ that maximizes $\\left|S \\cap U\\right|$<br>\n  3b            set $U = U \\setminus S$<br>\n  3c            set $C = C \\cup \\{S\\}$<br>\n  4    return $C$   </p>\n</blockquote>\n\n<p>Could it be said that since the number of subsets a set has is $2^n$ and that in the worst case this algorithm will end up finding all of those subsets before settling on an n-instance set to return?</p>\n", 'ViewCount': '112', 'Title': 'Finding an instance of an n-element set cover', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-28T21:30:30.133', 'LastEditDate': '2013-11-28T21:30:30.133', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11580', 'Tags': '<algorithms><algorithm-analysis><greedy-algorithms><set-cover><approximation-algorithms>', 'CreationDate': '2013-11-23T23:48:31.680', 'Id': '18287''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Some guy on the internet <a href="http://networkengineering.stackexchange.com/a/5133/296">recommends</a> using the same ntp server when it is required to troubleshoot asymmetric routes through ICMP, and it\'s somewhat important to have synchronised time between the two machines doing ICMP.</p>\n\n<p>Granularity of timestamps in ICMP is 1ms (unique per 24h period), assume packet roundtrip between the source and destination of at least 100ms, each way of at least 50ms, plus jitter.</p>\n\n<p>I find the recommendation of using the same ntp server unreasonable; for one, because it would seem that the likelihood of any given reliable ntp server, anywhere in the world, carrying correct time is much higher than the likelihood of transmitting said time through the internet over longer distances (plus with potential jitter and packet loss), e.g. a good collection of local servers is already the best you could do for the task at stake.</p>\n\n<p>Basically, my conjecture is that, should a single ntp server be shared, it won\'t necessarily be a good server for both hosts doing ICMP, and would not contribute to the clock between the two (and only two) machines being the most synchronised, compared to a good collection of local servers instead.</p>\n\n<p>What\'s the mathematical take on this?</p>\n', 'ViewCount': '45', 'Title': 'NTP: synchronisation of time between two machines for ICMP timestamping', 'LastActivityDate': '2013-11-24T20:59:38.447', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11551', 'Tags': '<algorithm-analysis><reference-request><optimization><computer-networks><synchronization>', 'CreationDate': '2013-11-24T20:59:38.447', 'Id': '18310''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Considering two sets $A, B$ containing some $p$-dimensional points $x \\in \\mathbb{R}^p$. Let $d_x^S = \\min_{x' \\in S \\setminus \\{x\\}} \\lVert \\mathbf{x} - \\mathbf{x'} \\rVert$ denote the Euclidean distance from $x$ to its nearest point in $S$. We have a very simple algorithm:</p>\n\n<ol>\n<li>$\\forall x \\in A$, if $d_x^A &gt; d_x^B$ then move $x$ from $A$ to\n$B$.</li>\n<li>$\\forall x \\in B$, if $d_x^A &lt; d_x^B$ then move $x$ from $B$ to\n$A$.</li>\n<li>Repeat (1) and (2) until convergence</li>\n</ol>\n\n<p>Convergence is when there is no more $x \\in A$ such that $d_x^A &gt; d_x^B$, and there is no more $x \\in B$ such that $d_x^A &lt; d_x^B$.</p>\n\n<p>How could I figure out which function does this algorithm minimize or maximize at each iteration ? The function $\\Phi(A)+\\Phi(B) = \\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^B$ does not seem to decrease at each iteration.</p>\n\n<p>Note: another version of this algorithm is when we define $d_x^S$ as the mean distance from $x$ to its $k$ nearest points in $S$, instead of the distance to its nearest point in $S$. I don't know if $k &gt; 1$ would make the proof more complicated or not.</p>\n", 'ViewCount': '87', 'Title': 'Which potential function does this algorithm minimize or maximize?', 'LastEditorUserId': '2895', 'LastActivityDate': '2013-11-26T16:29:35.140', 'LastEditDate': '2013-11-26T16:29:35.140', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2895', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><optimization>', 'CreationDate': '2013-11-25T20:40:26.780', 'FavoriteCount': '2', 'Id': '18333''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '230', 'Title': 'Easy way to prove that this algorithm eventually terminates', 'LastEditDate': '2013-11-26T19:24:53.680', 'AnswerCount': '1', 'Score': '10', 'OwnerDisplayName': 'user995434', 'PostTypeId': '1', 'OwnerUserId': '2895', 'FavoriteCount': '3', 'Body': '<h2>Introduction and notations:</h2>\n\n<p>Here is a new and simple version of my algorithm which seems to terminates (according to my experiments), and now I would like to prove that.</p>\n\n<p>Let the notation $x_i \\in \\mathbb{R}^p$ refer to a $p$ dimensional data point (a vector). I have three sets A, B and C, such that $|A| = n$, $|B| = m$, $|C| = l$:\n$$A = \\{x_i | i = 1, .., n\\}$$\n$$B = \\{x_j | j = n+1, .., n+m\\}$$\n$$C = \\{x_u | u = n+m+1, .., n+m+l\\}$$</p>\n\n<p>Given $k \\in \\mathbb{N^*}$, let $d_{x_i}^A$ denote the mean Euclidean distance from $x_i$ to its $k$ nearest points in $A$; and $d_{x_i}^C$ denote the mean Euclidean distance from $x_i$ to its $k$ nearest points in $C$.</p>\n\n<h2>Algorithm:</h2>\n\n<p>I have the following algorithm which iteratively modifies the sets A and B by moving some selected elements from A to B and vis versa, and C remains always the same (do not change). To make it simple: the purpose of the algorithm is to better separate the sets $A$ and $B$ such that "the points of $B$ are more similar to those of a known fixed set $C$" and "the points of $A$ are finally self-similar and farther from those of $C$ and the final set $B$":</p>\n\n<ul>\n<li>$A\' = \\{ x_i \\in A \\mid d_{x_i}^A &gt; d_{x_i}^C \\}$ ... (1)</li>\n<li>$A = A \\setminus A\'$; $B = B \\cup A\'$ ... (2)</li>\n<li>$B\' = \\{ x_i \\in B \\mid d_{x_i}^A &lt; d_{x_i}^C$ } ... (3)</li>\n<li>$B = B \\setminus B\'$; $A = A \\cup B\'$ ... (4)</li>\n<li>Repeat (1), (2), (3), and (4) until: (no element moves from $A$ to $B$ or from $B$ to $A$, that is A\' and B\' become empty) or ($|A| \\leq k$ or $|B| \\leq k$)</li>\n</ul>\n\n<p>The algorithm terminates in two cases:</p>\n\n<ul>\n<li>when $|A|$ or $|B|$ becomes less than or equals to $k$</li>\n<li>or the most standard case, when $A\' = B\' = \\emptyset$, which means that no more elements moves between A and B.</li>\n</ul>\n\n<h2>Question:</h2>\n\n<p>How to prove that this algorithm eventually terminates ? I didn\'t found a convenient potential function which can be strictly minimized or maximized by the algorithm.\nI have unsuccessfully tried some functions: the function $\\sum_{x \\in A} d_x^C + \\sum_{x \\in B} d_x^A$ but it is not increasing at each iteration. The function $\\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^C$ but it is not decreasing at each iteration. The function $\\sum_{x \\in A} d_x^A + \\sum_{x \\in B} d_x^B$ seems not to be decreasing at each iteration. The function $\\sum_{x \\in A} d_x^B + \\sum_{x \\in B} d_x^A$ seems not to be increasing at each iteration. So what is the convenient potential function which can be show to either increase or decrease at each iteration ? Or should we show that the function decreases but not at each iteration (after some iterations rather) ? How ?</p>\n\n<h2>Notes:</h2>\n\n<ul>\n<li>The $k$ nearest points to $x$ in a set $S$, means: the $k$ points\n(others than $x$) in $S$, having the smallest Euclidean distance to\n$x$. You can just take $k = 1$ to simplify the analysis.</li>\n<li>I don\'t know if this may help or not, but I have the following\nproperty for my initial sets $A, B, C$: initially $\\forall x_i \\in B,\n   x_j \\in A$, if $x_b \\in C$ is the nearest point to $x_i$ and $x_a \\in\n   C$ is the nearest point to $x_j$ then always $distance(x_i, x_b) &lt;\n   distance(x_j, x_a)$. This intuitively means that points in $B$ are\ncloser to $C$ than points in $A$.</li>\n<li>If that makes the analysis easier: it is totally possible to consider a slightly different version of the Algorithm where as soon as a point from $A$ should be moved to $B$, it is moved from $A$ to $B$ (without passing by $A\'$), and vis versa for $B$.</li>\n</ul>\n', 'Tags': '<algorithms><complexity-theory><algorithm-analysis><optimization>', 'LastEditorUserId': '2895', 'LastActivityDate': '2013-11-27T22:36:33.060', 'CommentCount': '11', 'CreationDate': '2013-11-22T10:53:45.843', 'Id': '18393''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have been doing a little reading up on bubble sort and have read on wikipedia that it's complexity is measured as $\\Theta(n^2)$</p>\n\n<p>This bubble sort however is slightly more efficient. I thought this would be the best place to ask how I would work out this particular implementations complexity seeing that the number of iterations in the inner loop is reduced with each pass.</p>\n\n<pre><code>for (top = items.Length; top &gt; 0; top--)\n            {\n                for (low = 0, high = 1; high &lt; top; low++, high++)\n                {\n                    if (items[low].CompareTo(items[high]) &gt; 0)\n                    {\n                        tmp = items[high];\n                        items[high] = items[low];\n                        items[low] = tmp;\n                    }\n                }\n            }\n</code></pre>\n", 'ViewCount': '82', 'ClosedDate': '2013-12-05T23:08:19.347', 'Title': 'What is the complexity of this bubble sort algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-05T23:06:12.697', 'LastEditDate': '2013-12-05T23:06:12.697', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11855', 'Tags': '<algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2013-12-05T21:41:38.547', 'Id': '18662''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a rough idea that the time complexity is O(mn) where m and n are based on the length of signals, as per this <a href="http://en.wikipedia.org/wiki/Dynamic_time_warping" rel="nofollow">http://en.wikipedia.org/wiki/Dynamic_time_warping</a>. </p>\n\n<p>How can I determine the time complexity of the following code (<a href="http://www.mathworks.com/matlabcentral/fileexchange/16350-continuous-dynamic-time-warping" rel="nofollow">source</a>)</p>\n\n<pre><code>function [Dist,D,k,w,rw,tw]=dtw3(t,r)\n%\n% [Dist,D,k,w,rw,tw]=dtw(r,t,pflag)\n%\n% Dynamic Time Warping Algorithm\n% Dist is unnormalized distance between t and r\n% D is the accumulated distance matrix\n% k is the normalizing factor\n% w is the optimal path\n% t is the vector you are testing against\n% r is the vector you are testing\n% rw is the warped r vector\n% tw is the warped t vector\n\n\n[row,M]=size(r); \nif (row &gt; M) \n     M=row; \n     r=r\'; \nend;\n[row,N]=size(t); \nif (row &gt; N) \n     N=row; \n     t=t\'; \nend;\n\n\nd=((repmat(r\',1,N)-repmat(t,M,1)).^2);\n\nD=zeros(size(d));\nD(1,1)=d(1,1);\n\nfor m=2:M\n    D(m,1)=d(m,1)+D(m-1,1);\nend\nfor n=2:N\n    D(1,n)=d(1,n)+D(1,n-1);\nend\nfor m=2:M\n    for n=2:N\n        D(m,n)=d(m,n)+min(D(m-1,n),min(D(m-1,n-1),D(m,n-1))); \n    end\nend\n\nDist=D(M,N);\nn=N;\nm=M;\nk=1;\nw=[M N];\nwhile ((n+m)~=2)\n    if (n-1)==0\n        m=m-1;\n    elseif (m-1)==0\n        n=n-1;\n    else \n      [values,number]=min([D(m-1,n),D(m,n-1),D(m-1,n-1)]);\n      switch number\n      case 1\n        m=m-1;\n      case 2\n        n=n-1;\n      case 3\n        m=m-1;\n        n=n-1;\n      end\n    end\n    k=k+1;\n    w=[m n; w]; \nend\n\n% warped waves\nrw=r(w(:,1));\ntw=t(w(:,2));\nend\n</code></pre>\n', 'ViewCount': '87', 'Title': 'How can I analyze the time complexity of this Dynamic Time Warping algorithm implemented in MATLAB?', 'LastActivityDate': '2013-12-08T07:54:28.550', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18733', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11922', 'Tags': '<algorithms><algorithm-analysis><time-complexity>', 'CreationDate': '2013-12-08T06:33:52.807', 'Id': '18732''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Given a collection of $n$ numbers, $S$, the question is to decide whether all the elements of $S$ are distinct from each other. If they are distinct from each other (no two of them are the same), print "Yes". Otherwise print "No".</p>\n\n<p>I know the worst case time complexity of this question is $\\Theta \\left ( n\\log n \\right )$. Of course it is based on comparison among elements. But I can\'t figure out how to prove this. Perhaps using the decision tree?</p>\n', 'ViewCount': '66', 'Title': 'About the complexity of deciding whether no two elements of a collection are the same', 'LastEditorUserId': '11975', 'LastActivityDate': '2013-12-10T10:45:57.403', 'LastEditDate': '2013-12-10T10:45:57.403', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '18818', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11975', 'Tags': '<algorithm-analysis><time-complexity>', 'CreationDate': '2013-12-10T01:41:12.900', 'Id': '18807''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Consider the following problem: given a set of $m$ red points and $n$ blue points in the plane, find a minimum length cycle that separates the red points from the blue points. That is, the red points are inside the cycle and the blue points are outside the cycle, or vice versa. This problem is called the <em>red blue separation problem</em>.</p>\n\n<p>I am trying to reduce the Traveling Salesman Problem (TSP) to this but I am getting nowhere. Can you please help me with this? Any help is appreciated.</p>\n', 'ViewCount': '122', 'Title': 'Prove the red blue separation problem is NP-complete', 'LastEditorUserId': '472', 'LastActivityDate': '2013-12-13T18:02:00.340', 'LastEditDate': '2013-12-13T18:02:00.340', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18872', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12004', 'Tags': '<algorithms><algorithm-analysis><np-complete><np-hard><np>', 'CreationDate': '2013-12-11T00:44:22.820', 'Id': '18852''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>So given a input of lets say 10 strings, what way can we input these so we get the best or worst case for these two given sorts?</p>\n\n<pre><code>Heap sort:\nbest case - nlogn\nworst case - nlogn\n\nQuick sort:\nbest case - nlogn\nworst case - n^2\n</code></pre>\n\n<p>Where i get confused on these two is:</p>\n\n<p>heap- Since the best and worst case are the same does it not matter the input order? The number of comparisons and assignments will always be the same? I imagine in a heap sort it may be the same since the real work is done in the insertion, but the sorting only uses the removal of the max/min heap? Is that why?</p>\n\n<p>quick sort- This one I don't know for sure. I'm not sure what the best case and worst case situations are for this. If its a already sorted list of 10 strings for example wouldn't we always have to choose the same amount of pivots to get complete the recursive algorithm? Any help on this explanation would really help.</p>\n\n<p>Thanks</p>\n", 'ViewCount': '2076', 'Title': 'Best and worse case inputs for heap sort and quick sort?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-13T07:41:27.657', 'LastEditDate': '2013-12-13T07:41:27.657', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'OwnerDisplayName': 'user3037172', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><runtime-analysis><sorting><quicksort>', 'CreationDate': '2013-11-26T16:04:07.403', 'Id': '18945''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose we want to arrange n numbers stored in an array such that all negative value occur before the positive ones. What will be the minimum number of exchanges in the worst case ? </p>\n', 'ViewCount': '98', 'Title': 'Minimum number of exchanges needed to get all negative values left of all positive ones', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T21:01:41.950', 'LastEditDate': '2014-03-14T21:01:41.950', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11984', 'Tags': '<algorithms><algorithm-analysis><sorting>', 'CreationDate': '2013-12-13T13:48:34.530', 'Id': '18952''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm trying to understand why the sorting algorithm Selection Sort has a time complexity of O(n^2).</p>\n\n<p>Looking at the math, the time complexity is</p>\n\n<p>T(n) = (n-1) + (n-2) + ... + 2 + 1</p>\n\n<p>And this is stated to be equal to</p>\n\n<p>O(n^2)</p>\n\n<p>However I just don't understand the intuition. I have tried several practical experiments for n=10 up to n=5000, and all point to that the time complexity of e.g. 5000 can never be greater T(5000) = 12.497.500 -- not T(5000) = 5000^2 = 25.000.000.</p>\n\n<p>Now, I know that 5000 is not the same as infinity, but I just don't understand the intuition behind</p>\n\n<p>(n-1) + (n-2) + ... + 2 + 1 = O(n^2)</p>\n\n<p>Does someone have a great pedagogical explanation that my dim-witted mind can understand?</p>\n", 'ViewCount': '1122', 'Title': 'Selection Sort Time Complexity using Big O notation', 'LastActivityDate': '2013-12-18T16:54:33.630', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19096', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12219', 'Tags': '<algorithm-analysis><asymptotics><sorting>', 'CreationDate': '2013-12-18T16:43:12.773', 'Id': '19094''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Let\'s assume that I\'ve implemented a method for the extraction of the principal lines of the palmprint. For instance, let\'s say that I\'m able to do a transformation like the following one (image from <a href="http://kst.buu.ac.th/proceedings/KST2010/docs/en08.pdf" rel="nofollow">Patprapa Tunkpien, Sasipa Panduwadeethorn and Suphakant Phimoltares</a>):</p>\n\n<p><img src="http://i.stack.imgur.com/vc7YJ.png" alt="Example of extraction of principal lines of the palmprint"></p>\n\n<p>Now, I want to make an evaluation of the accuracy level of my algorithm. In order to do this, I have only the algorithm, and a big set of palmprint images (on which I can execute the extraction procedure to obtain the corresponding set of binary images of principal lines of the palmprint).</p>\n\n<p>How can I evaluate the accuracy level of my algorithm, without having any kind of reference extraction method?</p>\n', 'ViewCount': '34', 'Title': 'How to evaluate the accuracy of a method for the extraction of the principal lines of the palmprint', 'LastEditorUserId': '12301', 'LastActivityDate': '2013-12-22T20:05:22.463', 'LastEditDate': '2013-12-22T16:43:22.697', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19197', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12301', 'Tags': '<algorithms><algorithm-analysis><image-processing>', 'CreationDate': '2013-12-22T16:35:11.107', 'Id': '19192''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose I want to run Dijkstra's algorithm on a graph whose edge weights are integers in the\nrange 0, ..., W, where W is a relatively small number.\nHow can I modify that algorithm so that it takes time just O((|V| + |E|) logW) and relatively easy implement that in C/C++?</p>\n", 'ViewCount': '259', 'Title': "Dijkstra's algorithm for edge weights in range 0, ..., W", 'LastActivityDate': '2013-12-25T17:06:47.797', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12351', 'Tags': '<algorithms><algorithm-analysis><data-structures><shortest-path><weighted-graphs>', 'CreationDate': '2013-12-24T16:15:19.157', 'Id': '19252''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>T (n) = T(\u221an) + 1\nThe easy way to do this is with a change of variables. Let m = lg n and\nS(m) = T (2^m).</p>\n\n<p>T(2^m) = T (2^(m/2)) + 1</p>\n\n<p>S(m) = S(m/2) + 1</p>\n\n<p>Can any one explain why 1 and 2 are same and this works ??</p>\n', 'ViewCount': '52', 'Title': "Can't under stand change of variable", 'LastActivityDate': '2013-12-25T11:55:52.397', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19273', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2013-12-24T21:23:15.767', 'Id': '19258''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm having a hard time with the following question:</p>\n\n<p>Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 4T(n/2 + 2) + n$. Use the substitution method to verify your answer.</p>\n\n<p>This is not homework, I'm just practicing myself for an upcoming exam.</p>\n\n<p>The thing I'm having a hard time with is the $n/2 + 2$, what will the height be of this recursion tree?</p>\n\n<p>I came up with the following formula to calculate the cost of each level after a lot of labor:\n$2^i n + 2^{i+2}(2^i -1)$ not 100% sure this is correct either.</p>\n\n<p>Any help appreciated, really looking forward to the answer :D<br>\nI often make stupid mistakes and I just started doing algo's for my first time.</p>\n", 'ViewCount': '636', 'Title': 'CLRS 4.4-3 Height of recursion tree for T(N) = 4T(n/2 +2) + n', 'LastActivityDate': '2013-12-28T21:03:49.723', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19350', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12422', 'Tags': '<algorithm-analysis><recursion>', 'CreationDate': '2013-12-28T17:42:51.827', 'Id': '19343''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<pre><code>i=n; \nwhile(i&gt;0) {\n  k=1;\n  for(j=1;j&lt;=n:j+=k)\n    k++;\n  i=i/2;\n}\n</code></pre>\n\n<p>The while loop has the complexity of $\\lg(n)$ the j value of inner loop runs 1,3,6,10,15...\nincrease like 2,3,4,5,...</p>\n\n<p>But how to find the overall complexity ?</p>\n', 'ViewCount': '136', 'ClosedDate': '2014-01-03T15:56:24.630', 'Title': 'How to find the asymptotic runtime of these nested loops?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-04T19:36:17.830', 'LastEditDate': '2014-01-04T19:36:17.830', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-03T10:45:35.863', 'Id': '19480''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Foldl and folr are 2 very important functions for FP and Haskell, but I have never heard much about the unsided fold:</p>\n\n<pre><code>fold f [a,b,c,d] = (f (f a b) (f c d))\n</code></pre>\n\n<p>That is, a fold that operates on binary associative functions (so the order of application doesn't matter). If I recall correctly, this is very common in databases as it can be parallelized. So, about it, I ask:</p>\n\n<ol>\n<li>Is it, like foldr, universal?</li>\n<li>Like foldr, can you define every important function using it?</li>\n<li>Is there a fusion rule for it, similar to those for foldr/build and unfoldr/destroy?</li>\n<li>Why is it barely mentioned?</li>\n<li>Any consideration worth mentioning?</li>\n</ol>\n", 'ViewCount': '29', 'Title': 'What are the properties of the unsided fold?', 'LastActivityDate': '2014-01-06T02:07:05.993', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><functional-programming>', 'CreationDate': '2014-01-06T02:07:05.993', 'Id': '19524''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Short version: I want to know where the $-2$ comes from in the formula on p. 221 of <a href="https://mitpress.mit.edu/books/introduction-algorithms" rel="nofollow">CLRS 3rd edition</a>.</p>\n\n<p>Long version: CLRS (3rd ed.) give an algorithm for $O(n)$ worst case arbitrary order statistic of $n$ distinct numbers.  The algorithm is roughly:</p>\n\n<blockquote>\n  <p><strong>Input</strong>: an array of $n$ elements and $i$, the number of the order statistic to return from the elements.</p>\n  \n  <ol>\n  <li>Divide the $n$ elements into $\\lfloor n/5 \\rfloor$ groups of 5 elements each along with an optional group containing $n\\mod{5}$ elements (resulting in $\\lceil n/5 \\rceil$ groups.)</li>\n  <li>Find the median of each of the groups by sorting.</li>\n  <li>Recurse, using the $\\lceil n/5 \\rceil$ medians as the array and $\\lfloor\\lceil n/5 \\rceil/2\\rfloor$ as the order statistic, resulting in the median-of-medians.</li>\n  <li>Partition the $n$ elements around the median-of-medians (using a quicksort-like $O(n)$ partitioning algorithm.</li>\n  <li>Letting $k-1$ be the number of elements less than the median-of-medians, if $i = k$, return the median-of-medians.  Otherwise recurse: if $i &lt; k$ then recurse finding the $i$th order statistic of the $k-1$ elements less than the median-of-medians; if $i &gt; k$,  then recurse finding the $i-k$th order statistic of the $n-k$ elements greater than the median-of-medians.</li>\n  </ol>\n  \n  <p><strong>Output</strong>: the $i$th order statistic of the $n$ numbers.</p>\n</blockquote>\n\n<p>In the proof of the runtime, CLRS argue that the number of elements greater than the median-of-medians is at least:</p>\n\n<p>$$\n3 \\bigg(\\bigg\\lceil  \\frac{1}2 \\bigg\\lceil{\\frac{n}5} \\bigg\\rceil \\bigg\\rceil - 2\\bigg)\n$$</p>\n\n<p>The reasoning is that half of the medians are greater than the median-of-medians, and each of those medians\' groups has at least three elements greater than the median-of-medians (the median itself plus the two elements greater than the median.)  That would result in </p>\n\n<p>$$\n3 \\bigg(\\bigg\\lceil  \\frac{1}2 \\bigg\\lceil{\\frac{n}5} \\bigg\\rceil \\bigg\\rceil\\bigg)\n$$</p>\n\n<p>for the lower bound on the number of elements greater than the median-of-medians.  </p>\n\n<p>But we must account for two things: the group containing the median-of-medians (the median-of-medians is not greater than itself) and the group that contains the modulo leftovers.  To account for the group containing the median-of-medians, we subtract 1, resulting in:</p>\n\n<p>$$\n3 \\bigg(\\bigg\\lceil  \\frac{1}2 \\bigg\\lceil{\\frac{n}5} \\bigg\\rceil \\bigg\\rceil\\bigg) - 1\n$$</p>\n\n<p>and I <em>think</em> that for the modulo leftovers group, we should subtract 4, because the least number of elements in the group is 1.  So that would give:</p>\n\n<p>$$\n3 \\bigg(\\bigg\\lceil  \\frac{1}2 \\bigg\\lceil{\\frac{n}5} \\bigg\\rceil \\bigg\\rceil\\bigg) - 5\n$$</p>\n\n<p>which can be transformed into </p>\n\n<p>$$\n3 \\bigg(\\bigg\\lceil  \\frac{1}2 \\bigg\\lceil{\\frac{n}5} \\bigg\\rceil \\bigg\\rceil - 2\\bigg) + 1\n$$</p>\n\n<p>Why does my analysis lead to a lower-bound 1 greater than that given in CLRS?</p>\n', 'ViewCount': '87', 'Title': 'Counting elements that are greater than the median of medians', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-07T19:33:23.323', 'LastEditDate': '2014-01-07T11:46:43.480', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19563', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12635', 'Tags': '<algorithm-analysis><combinatorics><discrete-mathematics>', 'CreationDate': '2014-01-06T20:02:49.843', 'Id': '19542''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am curious how to do a line by line analysis of this piece of code using the "Big O" notation.</p>\n\n<pre><code>i = 0;\nj = 0;\n\nwhile ( ( i &lt; n ) &amp;&amp; ( j &lt; m ) )\n{\n      //do something\n      i++;\n      j++;\n}\n</code></pre>\n\n<p>How I should represent number of iterations for the loop? Is good if I will do some assumptions or I should write min(n, m)?</p>\n\n<p>Small extension after @Patrick87\'s comment to show why I am not sure that min() is a general solution:</p>\n\n<pre><code>i = 0;\nj = 0;\n\nwhile ( ( i &lt; n ) &amp;&amp; ( j &lt; m ) )\n{\n      //do something\n      i++;\n      j++;\n}\nif ( i &lt; n )\n{\n      while ( ( i &lt; n ) )\n      {\n         //do something\n         i++;\n      }\n}\nelse\n{\n      while ( ( j &lt; m ) )\n      {\n         //do something\n         j++;\n      }\n}\n</code></pre>\n\n<p>How right now we can connect a number of iterations of the first loop and second one if we don\'t know which condition broke the condition of the first loop? </p>\n', 'ViewCount': '237', 'Title': 'Complexity analysis of while loop with two conditions', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-08T08:43:36.637', 'LastEditDate': '2014-01-08T08:42:42.400', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19571', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12652', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-07T18:48:21.657', 'Id': '19564''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a simple directed graph $G(V,E)$ that has a source $s$ and sink $t$. Each edge $e$ of $G$ has positive integer capacity $c(e)$ and positive integer cost $a(e)$. I am trying to find the minimum cost maximum flow from $s$ to $t$ using the <a href="http://wwwhome.math.utwente.nl/~uetzm/do/DO_Lecture4.pdf" rel="nofollow">well-known Dijkstra potential method for finding augmenting paths</a>. It goes something like this:</p>\n\n<pre><code>Initialize all edge flows to 0.\nInitialize all potentials pi[v] to 0.\nWhile there exists an augmenting path in G_f (the residual network):\n    Set the costs of all edges e = uv to be:\n        b(e) = a(e) + pi[u] - pi[v], if e exists in G or\n        b(e) = -a(e_reverse) + pi[u] - pi[v], where e_reverse = vu otherwise\n    # We are now assured all edges have nonnegative costs\n    Using Dijkstra method with costs b(e) in G_f:\n        Find the cheapest augmenting path from s to t\n        Calculate dist(v), the cost of cheapest path from s to v\n    Augment the cheapest path to t to current flow\n    Set pi[v] = pi[v] + dist(v) for all vertices v\nThe current flow gives the minimum cost maximum flow.\n</code></pre>\n\n<p>Obviously, if all costs $a(e) \\le a_{max}$ and all capacities $c(e) \\le c_{max}$, then there is a loose bound $|E|c_{max}a_{max}$ for cost of minimum cost maximum flow. However, the bound on the potentials $\\pi(v)$ and Dijkstra distances $dist(v)$ is not so obvious. In fact, judging by how it adds $dist(v)$ to $\\pi(v)$ each iteration, $\\pi(v)$ can possibly be multiplied by $|V|$ each iteration!</p>\n\n<p><strong>My question is</strong>, is there a way to calculate a non-exponential bound for $\\pi(v)$? If not, say all capacities and costs are at most $10^4$, $|V| = 200$, $|E| = 5000$. The minimum cost of the maximum flow is at most $5000 \\times 10^4 \\times 10^4 = 5 \\times 10^{11}$. But is it possible that $\\pi(v)$ and $dist(v)$ exceeds 64-bit integers? How do so many implementations not use Big Integers?</p>\n', 'ViewCount': '104', 'Title': 'Potential values of minimum cost maximum flow algorithm', 'LastEditorUserId': '7137', 'LastActivityDate': '2014-01-15T14:37:32.410', 'LastEditDate': '2014-01-15T14:37:32.410', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7137', 'Tags': '<algorithms><graph-theory><algorithm-analysis><asymptotics><network-flow>', 'CreationDate': '2014-01-11T04:25:09.883', 'Id': '19645''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '25', 'Title': 'How feasible is it for a non-distributed web crawler running on consumer hardware to search the internet?', 'LastEditDate': '2014-01-22T20:12:45.690', 'AnswerCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2605', 'Body': '<p>I am looking for an automated way to answer the question: what are the URLs on the world wide web that contain at least two strings from a set of strings. </p>\n\n<p>So if I have a set of strings {"A", "B" and "C"} -- I want to know, what pages on the world wide web contain "A" and B", "A" and "C", "B" and "C" or "A", "B" and "C."</p>\n\n<p>Obviously, for this simple example: Google it! </p>\n\n<p>But I want a scaleable, automated, and free solution. Google does <a href="https://support.google.com/webmasters/answer/66357?hl=en" rel="nofollow">not permit</a> automated queries. Yahoo makes you pay.</p>\n\n<p>One idea I have is (1) start with a URL, (2) check the text at that URL for the search strings (3) parse out the links from the text (4) record that you have checked the page and if it contains the strings then (5) search the links from the initial URL. Repeat until you have searched the tree. </p>\n\n<p>How feasible is this in terms of time and space on a single commodity machine -- given the size the internet? The internet is really, really big -- but only a comparatively few pages will contain these strings (they are proper names). </p>\n\n<p>I don\'t want to index the whole web as if my laptop were google!</p>\n\n<p>Most of the crawler\'s time will be spent confirming that the pages don\'t contain the strings. </p>\n\n<p>I\'m trying to get a rough ballpark to understand if this is even remotely feasible. </p>\n', 'ClosedDate': '2014-01-22T21:35:12.443', 'Tags': '<algorithm-analysis><search-problem><searching>', 'LastEditorUserId': '2605', 'LastActivityDate': '2014-01-22T20:47:27.130', 'CommentCount': '3', 'AcceptedAnswerId': '19902', 'CreationDate': '2014-01-22T19:00:09.453', 'Id': '19898''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>For each of the procedures below, let $T(n)$ be the running time. Find the order of $T(n)$\n(i.e., \ufb01nd $f(n)$ such that $T(n) \u2208 \u0398(f(n))$. Do not worry about how rounding errors a\ufb00ect running time.</p>\n\n<pre><code>Procedure Fum(int n):\n  for i from 1 to n do\n    \u03b4 \u2190 1/i\n    x \u2190 i\n    while x &gt; 0 do\n      x \u2190 x \u2212 \u03b4\n    end while\n  end for\n</code></pre>\n\n<p>Since it goes from 1 to n, and these operations</p>\n\n<pre><code>\u03b4 \u2190 1/i\nx \u2190 i\nx \u2190 x \u2212 \u03b4\n</code></pre>\n\n<p>Take the most time, so is the answer $O(n^3)$?</p>\n\n<p>2.</p>\n\n<pre><code>Procedure BinarySearch(table T [a . . . b], int k):\n  if a &gt; b then\n    return -1\n  end if\n  middle \u2190 \u230a(a + b)/2\u230b\n  if T [middle] = k then\n    return middle\n  end if\n  if k &lt; T [middle] then\n    return BinarySearch(T [a . . .middle], k)\n  else\n    return BinarySearch(T [middle . . . b], k)\n  end if\n</code></pre>\n\n<p>Counting the number of if-else operations, would this one also be $O(n^3)$?</p>\n', 'ViewCount': '73', 'ClosedDate': '2014-01-23T09:08:13.143', 'Title': 'FInd the running time complexity of functions', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-23T09:08:01.860', 'LastEditDate': '2014-01-23T09:08:01.860', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13051', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-23T03:14:31.247', 'Id': '19907''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '110', 'Title': 'Finding a O(n) solution to: max difference of pairs array question', 'LastEditDate': '2014-01-29T06:21:08.720', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13215', 'Body': "<p>I don't know an O(n) solution to the following:</p>\n\n<p>Given an array of n integers, find the largest difference between any two pairs in the array: however, the larger integer must have a higher index in the array than the other.</p>\n\n<p>Ex: alg({9, 2, 6, 7}) = 5</p>\n\n<p>It seems straightforward, yet it eludes me.</p>\n", 'ClosedDate': '2014-01-29T17:01:49.113', 'Tags': '<algorithm-analysis><arrays>', 'LastEditorUserId': '13215', 'LastActivityDate': '2014-01-29T14:27:56.283', 'CommentCount': '3', 'AcceptedAnswerId': '20057', 'CreationDate': '2014-01-29T05:50:21.657', 'Id': '20056''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<blockquote>\n  <p>The algorithm task is to find an integer (range is not known). the function <code>guess(num)</code> returns one of three chars: \'>\',\'&lt;\' or \'=\'.<br>\n  Find the secret number with <code>O(logS)</code> guesses, where <code>S</code> is the secret number.  You can use <code>find_secret(N1, N2)</code> which operate with <code>O(log(N2-N1))</code>.. What it does is simply a binary search.   </p>\n</blockquote>\n\n<p>So, the algorithm implemented (with Python) as follows:  </p>\n\n<pre><code>def find_secret2():\n    low = 1\n    high = 2\n    answer = guess(high)\n    while answer == "&gt;":\n        low *= 2\n        high *= 2\n        answer = guess(high)\n\n    if answer == "=":\n        return high\n    return find_secret(low, high)\n</code></pre>\n\n<p>my thoughts about the complexity of this algorithm:  </p>\n\n<p>it takes <code>O(logS)</code> to reach the range where <code>low &lt; secret &lt; high</code>.<br>\nthen, it takes <code>O(log(high-low))</code> - because we\'re using <code>find_secret(N1, N2)</code> method.</p>\n\n<p>I\'ll be glad if you could help me explain why the algorithm\'s complexity is <code>O(logS)</code> in a mathematical/rigorous way using the O-notation.<br>\nThanks!</p>\n', 'ViewCount': '55', 'Title': 'Runtime analysis of a "find the secret number" algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T20:39:11.993', 'LastEditDate': '2014-01-29T20:39:11.993', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20080', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13229', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-01-29T19:33:20.053', 'Id': '20078''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>So I have this code:</p>\n\n<pre><code>  done &lt;- false                                     [1]\n  n &lt;- 0                                            [1]\n  while (n &lt; a) and (done = false)                  [(n+1)(1+1+1)]\n    done &lt;- true                                    [n]\n    for m &lt;- (a- 1) downto n                        [n(1+1+1+1)]\n       if list[m] &lt; list[m - 1] then                [n]\n         tmp &lt;- list[m]                             [n]\n         list[m] &lt;- list[m-1]                       [n]\n         list[m - 1] &lt;- tmp                         [n]\n         done &lt;- false                              [n]\n       n &lt;- n + 1                                   [1]\n  return list                                       [1]\n</code></pre>\n\n<p>Am I doing this right? My conclusions are that the inne for-loop runs (n^2 + n) / 2 times and the outher while-loop runs n+1 times. I don't know how to properly argue for that the bubble sort has the complexity O(n^2) </p>\n", 'ViewCount': '207', 'Title': 'Bubble sort complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T16:00:25.457', 'LastEditDate': '2014-01-31T14:19:33.393', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13277', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><loops>', 'CreationDate': '2014-01-31T13:41:59.570', 'Id': '20155''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I have an algorithm and I determined the asymptotic worst-case runtime, represented by Landau notation. Let's say $T(n) = O(n^2)$; this is measured in number of operations.</p>\n\n<p>But this is the worst case, how about in average? I tried to run my algorithm 1000 times for each $n$ from $1$ to $1000$.I get another graph which is the average running time against $n$ but measured in real seconds.</p>\n\n<p>Is there any possible way to compare these figures?</p>\n", 'ViewCount': '56', 'Title': 'Compare asymptotic WC runtime with measured AC runtime', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-02T14:37:42.050', 'LastEditDate': '2014-02-02T14:22:32.553', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11506', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><average-case>', 'CreationDate': '2014-02-01T13:58:46.173', 'FavoriteCount': '1', 'Id': '20186''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '94', 'Title': 'Which computational model is used to analyse the runtime of matrix multiplication algorithms?', 'LastEditDate': '2014-02-03T11:42:45.030', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2091', 'FavoriteCount': '1', 'Body': '<p>Although I have already learned something about the asymptotic runtimes of matrix multiplication algorithms (Strassen\'s algorithm and similar things), I have never found any explicit and satisfactory reference to a model of computation, which is used to measure this complexity. In fact, I have found three possible answers, neither of which seems to me as absolutely satisfactory:</p>\n\n<ul>\n<li><a href="http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations" rel="nofollow">Wikipedia</a> says that the model used here is the Multitape Turing Machine. This does not seem to make much sense to me, since in the analysis of matrix multiplication, scalar multiplication is supposed to have a constant time complexity. This is not the case on Turing Machines.</li>\n<li>Some texts describe the complexity only vaguely as the number of arithmetic operations used. However, what <em>exactly</em> are arithmetic operations in this context? I suppose that addition, multiplication, and probably subtraction. But what about division, integer division, remainder, etc.? And what about bitwise operations - how do they fit into this setting?</li>\n<li>Finally, I have recently discovered an article, which uses the <a href="http://en.wikipedia.org/wiki/Blum%E2%80%93Shub%E2%80%93Smale_machine" rel="nofollow">BSS machine</a> as the model of computation. However, this also seems little bit strange to me, since for, e.g., integer matrices, it does not make much sense to me to disallow operations such as, e.g., integer division.</li>\n</ul>\n\n<p>I would be grateful to anyone, who could help me to sort these things out.</p>\n', 'Tags': '<algorithm-analysis><runtime-analysis><matrices><machine-models>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-03T15:33:24.143', 'CommentCount': '7', 'AcceptedAnswerId': '20255', 'CreationDate': '2014-02-03T08:48:07.433', 'Id': '20245''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I want to analyze the runtime of this algorithm:</p>\n\n<pre><code>int fun (int arr[], int n) {\n    int result = 1;\n    int i, j;\n\n    if (n == 1)\n        return 1;\n\n    else {\n            result = fun(arr, 2n/3);\n            for (i = 1; i &lt;= sqrt(n); i=i*2);\n                for (j=0; j&lt;sqrt(n)/i; j++)\n                    result += arr[j];\n\n            return result;\n    }\n}\n</code></pre>\n\n<p>I can see that the runtime recurrence should be something like </p>\n\n<p>$\\qquad\\displaystyle T(n) = T\\left(\\frac{2n}{3}\\right) + \\Theta(X)$</p>\n\n<p>where $X$ is the time of the extra operations per recursion.</p>\n\n<p>I can also see that the extra operations are:</p>\n\n<p>$\\qquad\\begin{align*}\n  \\sum_{i=1}^{\\log(\\sqrt{n})} \\sum_{j=0}^{\\frac{\\sqrt{n}}{i}}1 \n    &amp;= \\sum_{i=1}^{\\log(\\sqrt{n})}\\frac{\\sqrt{n}}{i} \\\\\n    &amp;= \\sqrt{n} \\cdot \\sum_{i=1}^{\\log(\\sqrt{n})} \\frac{1}{i} \\\\\n    &amp;= \\sqrt{n}\\cdot \\log(\\log(\\sqrt{n}))\n\\end{align*}$</p>\n\n<p>So all in all:</p>\n\n<p>$\\qquad\\begin{align*}\n  T(1) &amp;= 1 \\\\\n  T(n) &amp;= T\\left(\\frac{2n}{3}\\right) + \\sqrt{n}\\cdot \\log(\\log(\\sqrt{n}))\n\\end{align*}$</p>\n\n<p>But I could not continue from here to solve this recursion.</p>\n', 'ViewCount': '57', 'Title': 'Runtime analysis of a recursive algorithm with a tricky amount of work per recursive call', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-05T09:42:21.443', 'LastEditDate': '2014-02-05T09:42:21.443', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '21313', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10875', 'Tags': '<algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2014-02-05T07:31:18.833', 'Id': '21311''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have a genetic algorithm for an optimization problem.\nI plotted the running time of the algorithm on several runs on the same input and the same parameters (population size, generation size, crossover, mutation). </p>\n\n<p>The execution time changes between executions.\nIs this normal?</p>\n\n<p>I also noticed that against my expectation the running time sometimes decreases in place of increasing when I run it on a larger input.\nIs this expected?</p>\n\n<p>How can I analyze the performance of my genetic algorithm experimentally?</p>\n', 'ViewCount': '93', 'Title': 'How to analyze the performance of a genetic algorithm experimentally?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T23:51:30.663', 'LastEditDate': '2014-02-09T22:59:55.253', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'OwnerDisplayName': 'user2963216', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><genetic-algorithms><algorithm-engineering>', 'CreationDate': '2014-01-07T13:02:11.520', 'FavoriteCount': '1', 'Id': '21352''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Consider this nested loop:</p>\n\n<pre><code>for (i=0 to n)\n   for(j=0 to n)\n      for (k=0 to n)\n         sum := sum +k\n      end for\n   end for\nend for\n</code></pre>\n\n<p>Do <code>i</code>,<code>j</code> and <code>k</code> have temporal or spatial locality?</p>\n\n<p>I've considered <code>j</code> and <code>k</code> as temporal since they are revisited again in the loop. But I'm confused about <code>i</code> as it does not match the definitions of temporal or spatial.</p>\n", 'ViewCount': '41', 'Title': 'Are loop counters spatially or temporally local?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-08T16:00:13.993', 'LastEditDate': '2014-02-08T16:00:13.993', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10564', 'Tags': '<terminology><algorithm-analysis>', 'CreationDate': '2014-02-08T06:22:46.200', 'Id': '21443''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>It says to solve the recurrence exactly for <code>n</code> a power of 2. </p>\n\n<pre><code>T(n) = {1}, if n = 1\n\nT(n) = {5T(n/2) + (n lg n)^2, otherwise\n</code></pre>\n\n<p>I'm supposed to use <code>\u0398</code> notation to express my answer. </p>\n\n<p>Normally I would try to look for the characteristic equation but for this one I'm lost. </p>\n\n<p>Is this the first step?</p>\n\n<pre><code>T(n) - 5T(n/2) = (n lg n )^2\n</code></pre>\n\n<p>This is not a duplicate as it has not been posted there. </p>\n", 'ViewCount': '16', 'ClosedDate': '2014-02-09T21:04:35.363', 'Title': 'Can someone help solve recurrence relation?', 'LastEditorUserId': '14527', 'LastActivityDate': '2014-02-09T21:54:01.780', 'LastEditDate': '2014-02-09T21:54:01.780', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14527', 'Tags': '<algorithm-analysis>', 'CreationDate': '2014-02-09T20:18:34.037', 'Id': '21474''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>$$t(n)=\\begin{cases}n&amp;\\text{if }n=0,1,2,\\text{ or }3\\\\t(n-1)+t(n-3)-t(n-4)&amp;\\text{otherwise.}\\end{cases} $$\nExpress your answer as simply using the theta notation.</p>\n\n<p>I don't know where to go with this.</p>\n\n<p>$$t(n) - t(n-1) - t(n-3) + t(n-4) = 0$$</p>\n\n<p>Is the characteristic polynomial $x^3 - x^2 - x +1 = 0$?</p>\n", 'ViewCount': '18', 'ClosedDate': '2014-02-09T22:41:59.900', 'Title': 'Recurrence relation help?', 'LastEditorUserId': '14527', 'LastActivityDate': '2014-02-10T17:43:12.060', 'LastEditDate': '2014-02-10T17:43:12.060', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14527', 'Tags': '<algorithm-analysis><time-complexity>', 'CreationDate': '2014-02-09T22:26:41.967', 'Id': '21476''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I am somewhat confused with the running time analysis of a program here which has recursive calls which depend on a RNG.  (Randomly Generated Number)</p>\n\n<p>Let\'s begin with the pseudo-code, and then I will go into what I have thought about so far related to this one.</p>\n\n<pre><code>    Func1(A, i, j)\n    /* A is an array of at least j integers */\n\n 1  if (i \u2265 j) then return (0);\n 2  n \u2190 j \u2212 i + 1 ; /* n = number of elements from i to j */\n 3  k \u2190 Random(n);\n 4  s \u2190 0; //Takes time of Arbitrary C\n\n 5  for r \u2190 i to j do\n 6      A[r] \u2190 A[r] \u2212 A[i] \u2212 A[j]; //Arbitrary C\n 7      s \u2190 s + A[r]; //Arbitrary C\n 8  end\n\n 9  s \u2190 s + Func1(A, i, i+k-1); //Recursive Call 1\n10  s \u2190 s + Func1(A, i+k, j); //Recursive Call 2\n11  return (s);\n</code></pre>\n\n<p>Okay, now let\'s get into the math I have tried so far.  I\'ll try not to be too pedantic here as it is just a rough, estimated analysis of expected run time.  </p>\n\n<p>First, let\'s consider the worst case.  Note that the K = Random(n) must be at least 1, and at most n.  Therefore, the worst case is the K = 1 is picked.  This causes the total running time to be equal to T(n) = cn + T(1) + T(n-1).  Which means that overall it takes somewhere around cn^2 time total (you can use Wolfram to solve recurrence relations if you are stuck or rusty on recurrence relations, although this one is a fairly simple one).  </p>\n\n<p>Now, here is where I get somewhat confused.  For the expected running time, we have to base our assumption off of the probability of the random number K.  Therefore, we have to sum all the possible running times for different values of k, plus their individual probability.  By lemma/hopefully intuitive logic: the probability of any one Randomly Generated k, with k between 1 to n, is equal 1/n.  </p>\n\n<p><strong>Therefore, (in my opinion/analysis) the expected run time is:</strong></p>\n\n<p><strong>ET(n) = cn + (1/n)*Summation(from k=1 to n-1) of (ET(k-1) + ET(n-k))</strong></p>\n\n<p>Let me explain a bit.  The cn is simply for the loop which runs i to j.  This is estimated by cn.  The summation represents all of the possible values for k.  The (1/n) multiplied by this summation is there because the probability of any one k is (1/n).  <strong>The terms inside the summation represent the running times of the recursive calls of Func1.</strong>  The first term on the left takes ET(k-1) because this recursive call is going to do a loop from i to k-1 (which is roughly ck), and then possibly call Func1 again.  The second is a representation of the second recursive call, which would loop from i+k to j, which is also represented by n-k.</p>\n\n<p><strong>Upon expansion of the summation, we see that the overall function ET(n)  is of the order n^2.</strong>  <em>However</em>, as a test case, plugging in k=(n/2) gives a total running time for Func 1 of roughly nlog(n).  <em>This</em> is why I am confused.  How can this be, if the estimated running time is of the order n^2?  Am I considering a "good" case by plugging in n/2 for k?  Or am I thinking about k in the wrong sense in some way?  </p>\n', 'ViewCount': '66', 'Title': 'Algorithm Analysis: Expected Running Time of Recursive Function Based on a RNG', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-17T19:50:53.527', 'LastEditDate': '2014-02-12T09:12:25.723', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14596', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><average-case>', 'CreationDate': '2014-02-12T05:31:29.490', 'Id': '21558''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '163', 'Title': 'Why is this function computable in $O(n^{1.5})$ time?', 'LastEditDate': '2014-02-15T02:13:40.423', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '14654', 'FavoriteCount': '0', 'Body': '<p>My textbook says: "We define the function $f\\colon \\mathbb{N}\\to\\mathbb{N}$ as follows: $f(1)=2$ and $f(i+1)=2^{f(i)^{1.2}}$. Note that given $n$, we can easily find in $O(n^{1.5})$ time the number $i$ such that $n$ is sandwiched between $f(i)$ and $f(i+1)$."</p>\n\n<p>How can I convince myself that we can in fact easily find $i$ in $O(n^{1.5})$ time? As $f$ is defined recursively, I think we have to compute $f(1),f(2),f(3)\\dots f(j)$ until $f(j)\\geq n$. In order to find out the time that these computations take, I think we have to find a suitable upper bound for $i$ dependent on $n$ and we have to find an upper bound on the execution time of the function $x\\to2^{x^{1.2}}$. In the end, we can hopefully show the quoted proposition. Unfortunately, I don\'t see neither one thing nor the other.</p>\n\n<p>I forgot to mention: Please note that we are in a nondeterministic context. So $f$ is claimed to be computable in $O(n^{1.5})$ by a nondeterministic Turing machine.</p>\n\n<hr>\n\n<p>As quite a few people have already read this question, with some of them finding it useful and interesting too, but nobody answered so far, I want to provide some more information on the context: The quoted claim is an integral part of the proof of the nondeterministic time hierarchy theorem. The proof (with the claim) can be found e. g. in the <a href="http://www.cs.princeton.edu/theory/complexity/diagchap.pdf" rel="nofollow">book by Arora and Barak</a>, but I have found quite a few other resources on the Web too which present the same proof. Each of those calls the claim easy or trivial and does not elaborate on how to find $i$ in $O(n^{1.5})$ time. So either all these resources just copied from Arora and Barak or the claim is in fact not so difficult.</p>\n', 'Tags': '<complexity-theory><algorithm-analysis><nondeterminism>', 'LastEditorUserId': '683', 'LastActivityDate': '2014-02-15T02:25:32.873', 'CommentCount': '2', 'AcceptedAnswerId': '21654', 'CreationDate': '2014-02-14T14:31:23.487', 'Id': '21636''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I know that:</p>\n\n<p>If $f(n) = O(g(n))$ , then there are constants $M$ and $x_0$ , such that </p>\n\n<p>$f(n) &lt;= M*g(n), \\forall n &gt; n_0$</p>\n\n<p>The other, plain English way of defining it is,</p>\n\n<p>If $f(n)=O(g(n))$ then for large $n$ , $f(n)$ would <em>grow</em> as fast as $g(n)$.</p>\n\n<p>I got confused when comparing $2^n$ with $2^{2n}$. Here , $f(n) = 2^n$ and $g(n) = 2^{2n}$. Clearly , $f(n)$ is smaller than $g(n)$ by a factor of $2^n$. So there will be constants $A$ and $x_0$ such that the first definition above is met.</p>\n\n<p>However, for large $n$ , $2^{2n}$ would grow much faster than $2^n$, leaving $2^n$ far behind. That is $2^{2n}$  won't be an asymptotic/tight bound for $2^n$ .</p>\n\n<p>So, is $2^n = O(2^{2n})$ or not? (or did I just create a confusing situation out of nothing)</p>\n", 'ViewCount': '69', 'Title': 'Big O relation between $2^n$ and $2^{2n}$', 'LastActivityDate': '2014-02-16T03:48:30.417', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '21688', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11837', 'Tags': '<complexity-theory><algorithm-analysis><asymptotics><landau-notation>', 'CreationDate': '2014-02-15T17:49:54.007', 'Id': '21675''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I am a high school student computationally studying the 3-dimensional structure of chromosomes by 40 kilobase loci. In a nutshell, loci that are close in space tend to express their genes at the same time --- loci are different stops on a 3D-winding DNA chain.</p>\n\n<p>The best way to understand the 3D structure is by gathering what are basically distances between loci.</p>\n\n<p>Now I have an $n\\times n$ ($n$ = number of loci studied) matrix where $(i,j)$ is the distance between locus $i$ and locus $j$. I also have a (somewhat miraculous) 3-dimensional of the same chromosome that maps each locus to a certain point in a 3D $(x,y,z)$ coordinate system.</p>\n\n<p>My task is to find all of the loci within a certain radius of locus $L$. With the matrix, I would have to go to $L$ and traverse many nearby locus-distance chains, possibly for a long time, before being any bit certain that I had everything I wanted (i.e. brute force). With the spatial model, I would only have to conduct a simple search within that radius.</p>\n\n<p>Here is my question. What is the complexity of finding nearby loci in the 3D model and the 2D matrix with respect to loci count and radius size (whichever you think is more complex)? (Compare the two complexities and give both)</p>\n\n<p>I am not very studied in CS, but here is what I guess:</p>\n\n<p>$$\nC_{2D search best-case} = O(n^2)\n$$\n$$\nC_{2D search worst-case} = O(2^n)\n$$</p>\n\n<p>Best-case is what you'd expect, and worst-case would be going through every permutation of the distance.</p>\n\n<p>$$\nC_{3D search any case} = O(n)\n$$</p>\n\n<p>This is just my rather fallible intuition.</p>\n", 'ViewCount': '57', 'Title': 'Time complexity of proximity search in distance matrix', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-17T19:14:00.307', 'LastEditDate': '2014-02-17T09:59:29.453', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21721', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14736', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><ralgorithms>', 'CreationDate': '2014-02-17T00:27:33.090', 'Id': '21711''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose we had 2 arrays of the same size with positive numbers and we wanted to pair up the elements of each array such that the total difference between the pairs is minimized.</p>\n\n<p>The first thought would be to choose pairs with the minimum difference and so on. But it turns out the correct algorithm is to sort them and them pair accordingly.</p>\n\n<p>Any ideas on how to prove that the latter algorithm correctly minimizes the sum of differences?</p>\n', 'ViewCount': '85', 'Title': 'How can we minimize the total distance of cross pairs in an array', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-21T16:31:05.690', 'LastEditDate': '2014-02-21T16:31:05.690', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21815', 'Score': '2', 'OwnerDisplayName': 'user14805', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><optimization><correctness-proof><permutations>', 'CreationDate': '2014-02-18T14:51:18.950', 'Id': '21767''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>How can I find the cost of pseudocode with a nested loop and a nested if statement?</p>\n\n<p><img src="http://i.stack.imgur.com/bk2g6.png" alt="Exercise"></p>\n\n<p>On the left hand side is an example from a textbook I am following. On the right hand side is pseudo code that I found. I took a guess, but I don\'t know what the time would be for the inner code fragments.</p>\n\n<p>I am especially unsure what the code segments inside the if statement would be because the if statement doesn\'t always occur.</p>\n', 'ViewCount': '86', 'Title': 'How to find the cost of pseudocode with a nested loop and a nested if statement?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-20T21:50:41.273', 'LastEditDate': '2014-02-20T11:11:16.393', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '21831', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14864', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-02-19T23:42:49.297', 'Id': '21829''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have two functions $S$ and $T$ which are interrelated and I want to find the asymptotic worst case runtime. The fact that they are interrelated is stumping me...</p>\n\n<p>How would I find the asymptotic runtime $S(n)$ and $T(n)$?</p>\n\n<p>$$\n\\begin{align*}\nS(n) &amp;= 2S(n/4) +  T(n/4) \\\\\nT(n) &amp;=  S(n/2) + 2T(n/2)\n\\end{align*}\n$$</p>\n', 'ViewCount': '55', 'Title': 'Asymptotic Runtime of Interrelated Functions', 'LastEditorUserId': '683', 'LastActivityDate': '2014-03-01T23:51:07.287', 'LastEditDate': '2014-03-01T23:51:07.287', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11204', 'Tags': '<algorithms><algorithm-analysis><asymptotics><search-algorithms><master-theorem>', 'CreationDate': '2014-03-01T02:34:54.417', 'Id': '22149''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>How many times does the statement count in line 5 executes in terms of $n$?</p>\n\n<pre><code>1.  count=0; \n2.  for (i=1; i&lt;=n; i++) { \n3.  for (j=1; j&lt;=n; j*=2) { \n4.  for (k=1; k&lt;=j; k++) {\n5.        count = count + 1;\n6.      }\n7.    }\n8.  }\n</code></pre>\n\n<p>For the loop in line 3, we can list the numbers for $j$ as, $2^0,2^1,...,2^{\\log n}$\nTherefore, we can refer to the iterations of the exponents as $r$. Doing so would lead to the summations below for counting the number of executions.</p>\n\n<p>$\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}\\sum_{k=1}^{j}1 =\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}j $</p>\n\n<p>I am stuck here, because $\\sum_{r=0}^{\\log n}$ depends on $j$ but I am not sure how to incorporate them.</p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '20', 'Title': 'The number of executions of the count statement; how many?', 'LastActivityDate': '2014-03-01T05:18:02.957', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22154', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2014-03-01T04:34:54.633', 'Id': '22150''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>consider the pseudo code for calculating failure function:\nI partial understand the algorithm.\nKNUTH-MORRIS-PRATT FAILURE (P)</p>\n\n<p>Input:    Pattern with m characters\nOutput: Failure function f for P[i . . j]</p>\n\n<pre><code>i \u2190 1\nj \u2190 0\nf(0) \u2190 0\nwhile i &lt; m do\n    if P[j] = P[i]\n        f(i) \u2190 j +1\n        i \u2190 i +1\n        j\u2190 j + 1\nelse if j!=0\n     j \u2190 f(j - 1)\nelse\n    f(i) \u2190 0\n    i \u2190 i +1\n</code></pre>\n\n<p>The j in above code signifies the length of longest equal perfect prefix and suffix. So when P[i]==P[j], j is increased by 1, signifying that prefix suffix length has increased by 1(by appending P[i] to suffix and P[j] to prefix).</p>\n\n<p>But when P[i]!=P[j] and j!=0, why does the algo assign f(j-1) to j. What does that signify?</p>\n\n<p>I felt that when P[i]!=P[j], then we can't use the previous j value, so we have to assume lps[i]=0 and compare all possible prefixes to all possible suffixes and then find the longest prefix suffix match. I realise that the algorithm's way of dealing with P[i]!=P[j] is more efficient O(string len), but I just can't understand how it works. </p>\n", 'ViewCount': '38', 'Title': 'O(pattern_length) failure function in kmp algorithm', 'LastActivityDate': '2014-03-02T06:13:21.870', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15186', 'Tags': '<algorithms><algorithm-analysis><strings>', 'CreationDate': '2014-03-02T06:13:21.870', 'FavoriteCount': '1', 'Id': '22182''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have the following homework question that I am struggling with. I have read the corresponding chapter from the book, but no guidance there.</p>\n\n<p>Consider a linked list $X: X_1 \\to X_2 \\to X_3 \\ldots$.\nAssume that the cost of examining a particular element $X_i$ is $C_i$. Note that to examine $X_i$, \none needs to scan through all elements in front of $X_i$. Let $P_i$ be the probability of \nsearching for element $X_i$, so the total cost for all searches is \n$$\n\\sum_{j=1}^{n} \\left( P_j \\cdot \\sum_{i=1}^{j} C_i \\right)\n$$</p>\n\n<ol>\n<li><p>Show that storing elements in non-increasing order of $P_i/C_i$ does not necessarily minimize the total cost. </p></li>\n<li><p>Show that storing elements in non-decreasing order of $P_i$ does not necessarily minimize the total cost. </p></li>\n</ol>\n\n<p>Any help and direction how to approach the problem will be highly appreciated.</p>\n', 'ViewCount': '127', 'Title': 'Impact on the order of elements on the cost of searching in a linked list', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-07T03:59:28.483', 'LastEditDate': '2014-03-04T11:09:47.673', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22365', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15266', 'Tags': '<algorithm-analysis><data-structures><linked-lists>', 'CreationDate': '2014-03-04T09:44:59.877', 'Id': '22264''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In the Shamos-Hoey algorithm for finding whether or not any two of $n$ line segments intersect, which is available at this site: <a href="http://geomalgorithms.com/a09-_intersect-3.html" rel="nofollow">http://geomalgorithms.com/a09-_intersect-3.html</a>, there is use of "nearest line above" and "nearest line below". The algorithm is supposed to run in time $O(n\\log n)$. Here is their pseudocode:</p>\n\n<pre><code>Initialize event queue EQ = all segment endpoints;\nSort EQ by increasing x and y;\nInitialize sweep line SL to be empty;\n\nWhile (EQ is nonempty) {\n    Let E = the next event from EQ;\n    If (E is a left endpoint) {\n        Let segE = E\'s segment;\n        Add segE to SL;\n        Let segA = the segment Above segE in SL;\n        Let segB = the segment Below segE in SL;\n        If (I = Intersect( segE with segA) exists) \n            return TRUE;   // an Intersect Exists\n        If (I = Intersect( segE with segB) exists) \n            return TRUE;   // an Intersect Exists\n    }\n    Else {  // E is a right endpoint\n        Let segE = E\'s segment;\n        Let segA = the segment above segE in SL;\n        Let segB = the segment below segE in SL;\n        Delete segE from SL;\n        If (I = Intersect( segA with segB) exists) \n            return TRUE;   // an Intersect Exists\n    }\n    remove E from EQ;\n}\nreturn FALSE;      // No  Intersections\n</code></pre>\n\n<p>If one studies the C++ code provided at the bottom of the webpage, we see that this is simply a "next" and "previous" in a BST, however, I can\'t seem to tell which information is being used as the BST key.</p>\n\n<p>My issue is the following: if we are considering all $y$-values at the current $x$-value of the sweep line, this is not merely a check for next or previous endpoint value in a BST, and can not take $O(\\log n)$ time. However, if we are checking per endpoint coordinate, this could not possibly be correct, since the following situation would lead to an incorrect execution:</p>\n\n<p><img src="http://i.stack.imgur.com/cgl2r.png" alt="Counterexample"></p>\n\n<p>The algorithm should find that $B$ and $C$ intersection on insertion of $B$ into the search tree ("SweepLine" / "SL") while sweeping. However, if we are ordering by endpoint coordinates, $A$ is $B$\'s previous, $C$ is not, and this would run into problems.</p>\n', 'ViewCount': '37', 'Title': 'Shamos-Hoey Line segment intersection runtime', 'LastEditorUserId': '15463', 'LastActivityDate': '2014-03-09T23:52:12.310', 'LastEditDate': '2014-03-09T23:52:12.310', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15463', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-03-09T23:24:01.920', 'Id': '22443''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>This is a question I have stumbled upon in my textbook, and didn't really know how to approach:</p>\n\n<p>Given a $k$-bit binary counter.\nWe have an operation Increment, which adds 1 to the counter.\nWe add a new operation Decrement, which subtracts 1 from the counter.\nProve that the cost for executing $n$ operations is $\\Theta(nk)$.</p>\n", 'ViewCount': '32', 'ClosedDate': '2014-03-12T09:25:27.037', 'Title': 'Binary counter amortized analysis', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-10T12:12:13.747', 'LastEditDate': '2014-03-10T11:34:25.743', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<algorithms><algorithm-analysis><amortized-analysis>', 'CreationDate': '2014-03-10T10:34:20.833', 'Id': '22460''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '45', 'Title': 'Examples of algorithms that have runtime O(N + M) resp O(NM)', 'LastEditDate': '2014-03-14T09:40:03.310', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '544', 'FavoriteCount': '1', 'Body': "<p>I'm looking for examples of loops that have running time $O(nm)$, $O(n+m)$ and $O(n\\log m)$ to help me understand these concepts.  Could anybody give some examples and explain why they have the given running time?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T09:40:03.310', 'CommentCount': '4', 'AcceptedAnswerId': '22612', 'CreationDate': '2014-03-14T05:44:15.117', 'Id': '22611''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '39', 'Title': "What's the time complexity of this append method?", 'LastEditDate': '2014-03-14T16:03:43.027', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15687', 'Body': '<p>I made a method that appends a sequence to another sequence.</p>\n\n<p>So: (append [1,2,3] [4,5,6]) = [1,2,3,4,5,6]</p>\n\n<p><strong>CODE In C#</strong></p>\n\n<pre><code>IEnumerable&lt;int&gt; Append(IEnumerable&lt;int&gt; xs,IEnumerable&lt;int&gt; ys)\n{\n    using(var iteratorX = xs.GetEnumerator())\n    using(var iteratorY = ys.GetEnumerator())\n    {\n        bool isTrueForX = false;\n        bool isTrueForY = false;\n        while((isTrueForX = iteratorX.MoveNext()) || (isTrueForY = iteratorY.MoveNext()))\n        {\n            if(isTrueForX) yield return iteratorX.Current;\n            else if(isTrueForY) yield return iteratorY.Current;\n        }\n    }\n}\n</code></pre>\n\n<p>I would like to know what is the time-complexity of this algorithm.</p>\n', 'ClosedDate': '2014-03-14T16:28:00.820', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'LastEditorUserId': '15687', 'LastActivityDate': '2014-03-14T16:03:43.027', 'CommentCount': '2', 'AcceptedAnswerId': '22617', 'CreationDate': '2014-03-14T11:37:38.200', 'Id': '22615''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>We all know how to use \u201cBig O\u201d notation to show how CPU instructions run increase as the size of the dataset increases.   E.g.  a quick sort is O(n log n).</p>\n\n<p>However for the last few years, instructions that don\u2019t access any data outside of the level 1 cache have been close to free compared to instructions that hit main memory.</p>\n\n<p>So is there some notation that is as clear as the \u201cBig O\u201d notation to help capture the sort of effect?     </p>\n', 'ViewCount': '24', 'Title': 'How do you compare algorithms based on scaling of their cache misses?', 'LastEditorUserId': '15725', 'LastActivityDate': '2014-03-15T14:58:13.743', 'LastEditDate': '2014-03-15T14:58:13.743', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15725', 'Tags': '<algorithm-analysis><cpu-cache>', 'CreationDate': '2014-03-15T14:49:07.390', 'Id': '22648''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>The is the running time of an algorithm : $0.5 X^2 + 3X$ , </p>\n\n<p>Q1) i dont understand why my lecturer says that the Big O is $O(X^3)$, shouldnt it be $O(X^2)$ as it is bounded by the quadratic power of $X$.</p>\n\n<p>Who is correct , me or my teacher ???</p>\n\n<p>Q2) Why is the Theta of the equation : $X^2$, i cant understand why</p>\n', 'ViewCount': '66', 'ClosedDate': '2014-03-17T11:30:36.683', 'Title': 'Why is Big $O(X^3)$ instead of Big $O(X^2)$ for this algorithm', 'LastEditorUserId': '15763', 'LastActivityDate': '2014-03-17T11:25:46.300', 'LastEditDate': '2014-03-17T11:25:46.300', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-03-17T01:08:52.797', 'Id': '22688''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>My teacher pointed out to us during lectures that we could use <strong>Graph Search</strong> to help us solve Sudoku puzzles which has left me puzzled . </p>\n\n<p>I dont see how this is possible as <strong>Graph Search</strong> is mostly about getting from Node A to Node B. He mentioned about how its a directed graph where the nodes correspond to partially completed puzzle</p>\n\n<p>What is the general idea behind using <strong>Graph Search</strong> to solve Sudoku Puzzle</p>\n', 'ViewCount': '108', 'Title': 'How to implement graph search to solve Sudoku puzzle', 'LastEditorUserId': '12448', 'LastActivityDate': '2014-03-17T12:30:24.120', 'LastEditDate': '2014-03-17T07:52:15.367', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs>', 'CreationDate': '2014-03-17T07:45:30.290', 'FavoriteCount': '1', 'Id': '22695''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I've searched online for this but I only seem to find answers for a similar equation: </p>\n\n<pre><code>T(n) = T(n/3) + T(2n/3) + cn\n</code></pre>\n\n<p>But the one I'm trying to solve is:</p>\n\n<pre><code>T(n) = T(n/3) + T(2n/3)\n</code></pre>\n\n<p>Base case: We can assume <code>T(a) = Theta(1)</code> for any constant <code>a</code>.</p>\n\n<p>I've succeeded in proving (by induction) that <code>T(n) = O(n*log(n))</code>. I thought the answer should be <code>Theta(n*log(n))</code>, but I cannot prove that <code>T(n) = Omega(n*log(n))</code>.</p>\n\n<p>So my question is - am I correct that the answer is <code>O(n*log(n))</code>, and NOT <code>Theta(n*log(n))</code>? IF that's true that would really be great...</p>\n\n<p>If I'm wrong I will of course explain where I'm stuck in the induction process...</p>\n\n<p>Thanks!</p>\n\n<p>P.S. If you need to, please try to explain using induction, because I haven't learned all methods for solving these problems yet.</p>\n", 'ViewCount': '64', 'ClosedDate': '2014-03-20T09:00:30.173', 'Title': 'Recurrence of T(n) = T(n/3) + T(2n/3)', 'LastActivityDate': '2014-03-19T21:53:26.597', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15868', 'Tags': '<algorithm-analysis><data-structures><runtime-analysis><recurrence-relation>', 'CreationDate': '2014-03-19T14:46:24.537', 'Id': '22809''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u"<p>I am reading Prim's MST for the first time and wanted to implement the fast version of it . </p>\n\n<p>$m$ - The number of edges in the graph </p>\n\n<p>$n$ - The number of vertices in the graph </p>\n\n<p>Here's the algorithm :</p>\n\n<p>1) Create a Min Heap of size $V$ where $V$ is the number of vertices in the given graph. Every node of min heap contains vertex number and key value of the vertex.</p>\n\n<p>2) Initialize Min Heap with first vertex as root (the key value assigned to first vertex is $0$ ). The key value assigned to all other vertices is $\\infty$ .</p>\n\n<p>3) While Min Heap is not empty, do following</p>\n\n<p>\u2026..a) Extract the min value node from Min Heap. Let the extracted vertex be u.</p>\n\n<p>\u2026..b) For every adjacent vertex $v$ of $u$, check if $v$ is in Min Heap (not yet included in MST). If $v$ is in Min Heap and its key value is more than weight of $u-v$, then update the key value of $v$ as weight of $u-v$.</p>\n\n<p>Now my point is during implementation ( I am doing in C++) in step 3(b) I have to check whether the vertex is there in the heap or not . As we know , searching in a heap is done in $O(n)$ time . So in the main while loop which will run ( $n$ number of times ) although extract-min is $O(\\log n)$ but the search ( whether $v$ is min heap or not takes time proportional to size of the heap ( although it is decreasing in each step ) . </p>\n\n<p>So is it correct to say that the above algorithm is $O(m+n\\log n)$</p>\n", 'ViewCount': '86', 'Title': "Prim's Minimum Spanning Tree implementation $O(mn)$ or $O(m+n \\log n)$?", 'LastEditorUserId': '15879', 'LastActivityDate': '2014-03-19T19:06:06.820', 'LastEditDate': '2014-03-19T18:56:58.260', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15879', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs><runtime-analysis>', 'CreationDate': '2014-03-19T18:20:22.067', 'Id': '22817''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Consider the following code segment :</p>\n\n<pre><code>for (int i = 1; i &lt;= n; i++ ) {\n    for (int j = 1; j &lt;= n; j = j + i ) {\n          printf("Hi");\n    }\n}\n</code></pre>\n\n<p>Here, the outer loop will execute $ n $ times, but the execution of inner loop depends upon the value of $ i $.  </p>\n\n<ul>\n<li>When $ i = 1 $ inner loop will execute $ n $ times.</li>\n<li>When $ i = 2 $ inner loop will execute $ \\frac{n}{2} $ times.</li>\n<li>When $ i = 3 $ inner loop will execute $ \\frac{n}{3} $ times.<br>\n$ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\vdots  $</li>\n<li>When $ i = n $ inner loop will execute $ 1 $ time  </li>\n</ul>\n\n<p>So complexity will be given by<br>\n$$\n \\begin{align}\n  T(n) &amp;= \\frac{n}{1} + \\frac{n}{2} + \\frac{n}{3} + \\cdots + \\frac{n}{n}\\\\\n\\\\\n       &amp;= n \\left( 1 + \\frac{1}{2} + \\frac{1}{3} + \\cdots + \\frac{1}{n} \\right) \\\\\\\\\n       &amp;= n \\sum_{k = 1}^{n} { \\frac{1}{k} }\n \\end {align}\n $$\nI am not able to solve $ \\sum_{k=1}^{n} \\frac{1}{k} $. Upon searching I found that it is the $ n^{th} $ Harmonic number ( $ H_n $), but couldn\'t find any closed formula for it. How can I proceed further to calculate $ T(n) $?</p>\n', 'ViewCount': '84', 'Title': 'Calculating time complexity of two interdependent nested for loops', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T08:06:56.143', 'LastEditDate': '2014-03-31T08:06:56.143', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22825', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11131', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-03-19T20:10:26.453', 'Id': '22823''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose an algorithm goes through a list of n integers and for every iteration of the loop it is needs to check if the current evaluated element of the list is even. If it is even, return the index of the integer that is evaluated as even.</p>\n\n<p>How come the algorithm would have 2n+1 comparison?</p>\n\n<p>I thought linear search would have n comparision because it is going through n elements. +1 comparison for the if statement. So that would make the algorithm O(n+1) comparison, no?. Where did the extra n come from?</p>\n\n<p>Pseudo-code:</p>\n\n<pre><code>procedure last_even_loc(a1,a2,...,an:integers);\nlocation = 0;\nfor i = 1 to n\n\n    if (a_i = 0) (mod 2) then location = i\n\nreturn location;\n</code></pre>\n', 'ViewCount': '66', 'Title': 'Why is there a 2n+1 comparison for a linear search algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-23T11:52:41.557', 'LastEditDate': '2014-03-23T11:52:41.557', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22853', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15555', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2014-03-20T03:45:36.143', 'Id': '22849''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I'm a having trouble analyzing this algorithm.\nThis is a binary counter that supports only increments in $2^i$ values\nit's implemented in this way:\nstarting from the $i$-th location change all the straight $1$'s to $0$'s and the first $0$ to $1$.</p>\n\n<p>So I analyzed the W.C to be $O(\\log n)$ because the worst case is we need to increment by $1$ a $2^i-1$ number.\nNow for the amortized I thought using the accounting method, charging for each change from $0$ to $1$ $2\\$$ amortized cost, since each time we increment we flip at most one $0$ to $1$. and put $1\\$$ on each $1$ bit to pay from flipping it back to $0$. so at most the amortized cost is $2\\$$ which means amortized $O(n)$. if it's correct than what's the difference from a regular binary counter? I don't think I understand...</p>\n", 'ViewCount': '39', 'Title': 'Custom binary counter supports only increment in $2^i$ values amortized analysis', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-03-22T06:26:40.153', 'LastEditDate': '2014-03-22T06:26:40.153', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15693', 'Tags': '<algorithms><algorithm-analysis><amortized-analysis>', 'CreationDate': '2014-03-21T22:43:51.283', 'Id': '22916''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>How can we add n positive integers with binary expansion $l_1$, $l_2$,...$l_n$ bits so that the total complexity is $O (\\sum l_i)$ for $i = {1,...,n}$ ? More importantly, how can show this complexity using amortized analysis (the potential method)?</p>\n\n<p>I know the elementary school addition of 2 numbers of length $s$ and $r$ is $O(r+s)$ and hence, the addition of n integers is $O(\\sum l_i)$. However, what potential function would you use to prove that bound? I don't seem to have any intuition on that... Maybe use a function similar to the standard binary counter example (number of 1's in the binary representation of the number)..?</p>\n", 'ViewCount': '57', 'Title': 'Amortized Analysis for Addition of $n$ numbers', 'LastActivityDate': '2014-03-26T09:43:41.157', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13063', 'Tags': '<algorithms><algorithm-analysis><amortized-analysis>', 'CreationDate': '2014-03-25T02:55:52.887', 'Id': '23027''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>We have a 0-1 knapsack in which the increasing order of items by weight is the same as the decreasing order of items by value. Design a greedy algorithm and prove that the greedy choice guarantees an optimal solution.</p>\n\n<p>Given the two orders I imagined that we could just choose the first k elements from either sequence and use them to fill knapsack until it was full. This would be similar to choosing the items with the greatest ratio of value to weight. But I don't think that is an optimal solution. </p>\n\n<p>So what I need help with is whether or not this solution is optimal. And how would I prove the correctness of a greedy algorithm. </p>\n", 'ViewCount': '63', 'Title': 'Correctness proof of greedy algorithm for 0-1 knapsack problem', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T16:45:47.887', 'LastEditDate': '2014-03-26T08:38:55.053', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23060', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15512', 'Tags': '<algorithms><algorithm-analysis><correctness-proof><greedy-algorithms><knapsack-problems>', 'CreationDate': '2014-03-26T03:23:03.477', 'Id': '23058''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '94', 'Title': u'How do O and \u03a9 relate to worst and best case?', 'LastEditDate': '2014-03-26T15:50:22.980', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12756', 'FavoriteCount': '1', 'Body': '<p>Today we discussed in a lecture a very simple algorithm for finding an element in a sorted array using <a href="http://en.wikipedia.org/wiki/Binary_search_algorithm" rel="nofollow">binary search</a>. We were asked to determine its asymptotic compelxity for an array of $n$ elements.</p>\n\n<p>My idea was, that it is obvisously $O(\\log n)$, or $O(\\log_2 n)$ to be more specific because $\\log_2 n$ is the number of operations in the worst case. But I can do better, for example if I hit the searched element the first time - then the lower bound is $\\Omega(1)$.</p>\n\n<p>The lecturer presented the solution as $\\Theta(\\log n)$ since we usually consider only worst case inputs for algorithms.</p>\n\n<p>But when considering only worst cases, whats the point of having $O$ and $\\Omega$-notation when all worst cases of the given problem have the same complexity ($\\Theta$ would be all we need, right?).</p>\n\n<p>What am I missing here?</p>\n', 'Tags': '<algorithm-analysis><asymptotics>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-26T18:04:51.830', 'CommentCount': '4', 'AcceptedAnswerId': '23082', 'CreationDate': '2014-03-26T11:10:34.473', 'Id': '23068''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was wondering, what is so special about a majority of nodes having a value? Why is that the key fact for consensus to work on Paxos?</p>\n\n<p>It says on the paper that "any two majorities have at least one acceptor in common, this works if an acceptor can accept at most one value" (page 2). Its true that if we have two majorities then at least one node between the two must be the same one, but I was not entirely sure if I fully appreciated why that was special for consensus. How does this guarantee that a single value gets accepted? Or how does it aid in that goal?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '29', 'Title': 'What is so special about a majority and why is it the key for Paxos to work? (Paxos made simple)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:34:51.380', 'LastEditDate': '2014-04-30T16:34:51.380', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T06:45:51.627', 'FavoriteCount': '2', 'Id': '23165''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>On page 3 it says "Since all numbers are totally ordered, condition P2 guarantees the crucial safety property that only a single value is chosen."</p>\n\n<p>Where P2 = "If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v."</p>\n\n<p>What I was a little confused was what "totally ordered" meant and how does it guarantee p2?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '34', 'Title': 'What does it mean that numbers are "totally ordered"? (paxos made simple)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:34:05.207', 'LastEditDate': '2014-04-30T16:34:05.207', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23186', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T16:57:03.753', 'Id': '23185''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was reading page 4 of the paper where it says condition $P2^c$:</p>\n\n<p>"For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that </p>\n\n<ul>\n<li><p>(a) no acceptor in S has accepted any proposal numbered less than n, or </p></li>\n<li><p>(b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S."</p></li>\n</ul>\n\n<p>(Where S = any set consisting of a majority of acceptors. \nC = the set of acceptors that have accepted some value c, the letter C stands for the majority that has Chosen a value).</p>\n\n<p>I was trying to understand better the conditions for issuing a proposal, specifically (b) is the one causing troubles for me.</p>\n\n<p>For me (a), makes sense because we don\'t want to issue a new proposal with a higher sequence number if there has been any proposal that has been chosen from the majority we are able to see (i.e. S). Since anything that has been chosen is accepted and since something chosen is part of the majority C, if we issue a new proposal, we could risk confusing the current paxos instance when its already chosen a value.</p>\n\n<p>However, (b) is less clear to me why we want it to hold. Recall (b) is:</p>\n\n<p>(b) = "v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S".</p>\n\n<p>Why are we interested in having that condition? Which safety properties does that condition help us maintain?</p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '54', 'Title': 'Rational of why Paxos only issues new values if its value is the largest one in the majority that he can see?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:33:18.723', 'LastEditDate': '2014-04-30T16:33:18.723', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T17:50:41.200', 'FavoriteCount': '1', 'Id': '23187''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>On page 4 of the paper, at the bottom where they outline what a prepare request looks like it says:</p>\n\n<blockquote>\n  <p>"A proposer chooses a new proposal number n and sends a request to\n  each member of some set of acceptors, asking it to respond with:</p>\n  \n  <p>(a) A promise never again to accept a proposal numbered less than n, </p>\n  \n  <p>(b) ... "</p>\n</blockquote>\n\n<p>Why does prepare message extract such promise from the set of acceptors that receive such a prepare message? What is the goal and what safety guarantee does it provide? How does it aid Paxos into reaching a consensus to a certain value? How is it crucial so that a majority can reach agreement?</p>\n\n<p>I did some further research to try to answer this question and went to the following yale university notes:</p>\n\n<p><a href="http://pine.cs.yale.edu/pinewiki/Paxos" rel="nofollow">http://pine.cs.yale.edu/pinewiki/Paxos</a></p>\n\n<p>These notes try to justify this point by saying:</p>\n\n<blockquote>\n  <p>"... proposer tests the waters by sending a prepare(n) message to all\n  accepters where n is the proposal number. An accepter responds to this\n  with a promise never to accept any proposal with a number less than n\n  (<strong>so that old proposals don\'t suddenly get ratified</strong>)... "</p>\n</blockquote>\n\n<p>Basically, the way they justify that step is by saying so that old proposals don\'t get ratified/accepted. Which I kind of see why they said that, since if a decision has happened, that rule definitively avoids the consensus from being reversed, which is nice. But, however, the reason I did not feel very convinced about it yet is because it seems to damage termination a lot. What if if a majority was about to be formed in the past and the proposer was ready to send the propose(n) and now because a different prepare(n\') was sent its not able to reach agreement? I can\'t seem to convince myself that that this step is 100% a good idea. </p>\n\n<p>Unless the only reason for that step is because they just want to guarantee that once a value has been chosen/decided, by adopting this rule, its impossible that any old proposal reverses the decision?</p>\n\n<p>Again, I did some further research by reading the yale article and now it says:</p>\n\n<blockquote>\n  <p>"The rule that an acceptor doesn\'t accept any proposal earlier than a\n  round it has acknowledged means that the value v in an ack(n, v, n_v)\n  message never goes out of date\u2014there is no possibility that an\n  acceptor might retroactively accept some later value in round n\' with\n  nv &lt; n\' &lt; n. So the ack message values tell a consistent story about\n  the history of the protocol, even if the rounds execute out of order."</p>\n</blockquote>\n\n<p>However, its not obvious to me why retroactivity might be bad (unless in the case that I already mentioned). Retroactivity could be good if it somehow manages our protocol to reach consensus. Still the only reason I see for such a rule is so that one doesn\'t reverse a decision by accident once a value has been agreed on. </p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '55', 'Title': 'Why does a prepare message wants a promise that an acceptor is never to accept a proposal numbered less than its propose sequence value? (Paxos)', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T17:25:43.177', 'LastEditDate': '2014-04-30T17:25:43.177', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-28T18:47:38.740', 'Id': '23190''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>I believe I understand the concepts of algorithm analysis. However, I'm not fully confident in applying those concepts. I'd appreciate help in bridging the gap between concept and application. </p>\n\n<p>I understand that if $f(n) \\in O(g(n))$ and $f(n) \\in \\Omega(g(n))$ then $f(n) \\in \\Theta(g(n))$</p>\n\n<p>Essentially, when Big-O and Big-Omega form the upper and lower bounds of algorithmic performance, Big-Theta represents the optimal solution. But how does one determine those bounds and thus optimal performance? </p>\n\n<p>How does/should one determine <em>Big-O, Big-Omega, Big-Theta</em> for the following algorithm?</p>\n\n<pre><code>int summation(int[] values)\n{\n  int sum = 0;\n  for(int i = 0; i &lt; values.length; i++)\n  {\n    sum += values[i];\n  }\n}\n</code></pre>\n", 'ViewCount': '35', 'ClosedDate': '2014-03-29T11:47:10.477', 'Title': 'Analysis of Algorithms: Applying Concepts', 'LastEditorUserId': '16252', 'LastActivityDate': '2014-03-28T21:55:52.333', 'LastEditDate': '2014-03-28T21:55:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16252', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-03-28T21:14:52.010', 'Id': '23192''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m trying to work through various exercises in Skiena\'s "Algorithm Design Manual." One problem that I am stuck on is as follows:</p>\n\n<blockquote>\n  <p>What value is returned by the following function? Express your answer as a function of n. Give the worst-case running time using Big Oh notation.</p>\n\n<pre><code>function conundrum(n)\n    r:=0\n    for i:=1 to n do\n        for j:=i+1 to n do\n            for k:=i+j-1 to n do\n                r:=r+1\n    return(r)\n</code></pre>\n</blockquote>\n\n<p>My first attempt at this problem began with trying to solve it when $n$ is even and my first observation was that $r$ is never incremented for any value of $i$ greater than $n/2$ (I am assuming that a statement along the lines of "$k:=l$ to $n$" is not executed for any $l&gt;n$. Indeed, upon working out the above function for several different values of $n$, I thought that I could intuit that the sum calculated by the function had the following form $\\sum_{p=1}^{n/2}\\sum_{q=1}^{2p-1}q$-- to intuit this, I began the first loop at $i=n/2$ and worked backwards. Working this sum out, I got $\\frac 1 {24} n(n(2n-3)-26)$. To my dismay, upon checking this answer, I found that the correct answer for even $n$ is $\\frac 1 {24} n(n+2)(2n-1)$. Any thoughts or hints about what I am doing wrong?</p>\n', 'ViewCount': '23', 'ClosedDate': '2014-03-29T12:00:47.007', 'Title': 'Complexity of a nested for loop', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T11:58:37.577', 'LastEditDate': '2014-03-29T11:58:37.577', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16262', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-03-29T08:30:15.913', 'Id': '23215''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Say that for a particular problem, e.g., the independent set problem, it has been shown that no polynomial-time algorithm exists to solve it.</p>\n\n<p>Could we get around this by finding an algorithm which approximates the solution to a certain accuracy?</p>\n\n<p>That is, would the result above bar the existence of an algorithm which finds a maximum independent set to an accuracy of 0.5? I.e., it is guaranteed to be less than 0.5 away from the size of a maximum set? (And hence implying that it actually <em>is</em> a maximum independent set.)</p>\n\n<p>It seems to me that the latter wouldn't violate our proofs of non-tractability, which are discrete in nature, while still giving an answer that satisfies the problem from a practical perspective.</p>\n", 'ViewCount': '252', 'Title': 'Approximating NP-complete problems', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-30T16:47:19.050', 'LastEditDate': '2014-03-30T16:47:19.050', 'AnswerCount': '4', 'CommentCount': '1', 'AcceptedAnswerId': '23239', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12588', 'Tags': '<algorithms><graph-theory><algorithm-analysis><time-complexity>', 'CreationDate': '2014-03-30T01:43:13.067', 'Id': '23236''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was studying Paxos from:</p>\n\n<p><a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" rel="nofollow">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></p>\n\n<p>and I was confused about one specific part. How does or why does property $P2^b$ satisfy property $P2^c$?</p>\n\n<p>These are the properties:</p>\n\n<p>$P2^b = $ If a proposal with value $v$ is chosen, then every higher-numbered (i.e. approx. later in time)  proposal issue by any proposer has value $v$.</p>\n\n<p>$P2^c$ = For any $v$ and $n$, if a proposal with value $v$ and number $n$ is issued, then there is a set $S$ (some majority) of acceptors such that either:</p>\n\n<p>(a)no acceptor in $S$ has accepted any proposal numbered less than $n$, or</p>\n\n<p>(b)$v$ is the value of the highest-numbered proposal among all proposals numbered less than $n$ accepted by the acceptors in $S$ (some majority).</p>\n\n<p>The paper uses $S$ to denote some majority and $C$ to denote some majority that has actually <em>chosen</em> a value.</p>\n\n<p>The thing that I am confused about is, for me $P2^b$ is saying, ok once a value has been chosen, say at sequence number $n$ (i.e. roughly time $n$), then after that time, we want to make sure that any proposer is only able to propose the value of the majority (chosen value). If we have that, then, we do not risk the already formed majority from reverting weirdly. i.e. once we have formed a majority, we want it to stick and stay like that. However, it was not 100% clear to me why property $P2^c$ satisfied that requirement. I kind of see why (a) is a nice property to have, since, having (a) means that its safe to issue a new proposal $(n, v)$ since we contacted some majority $S$ and none of them had accepted anything in a time earlier than now $n$. So, if a majority had formed we would have seen at least one value and we did not see anything accepted, its safe to propose something since a majority has not formed.  </p>\n\n<hr>\n\n<p>Author: Leslie Lamport</p>\n\n<p>Title: Paxos made simple</p>\n\n<p>Institution: Microsoft Research</p>\n', 'ViewCount': '16', 'Title': "How do we make sure in Paxos that we don't propose a different value if a majority has formed?", 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:35:20.363', 'LastEditDate': '2014-04-30T16:35:20.363', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-03-30T06:57:57.560', 'FavoriteCount': '1', 'Id': '23247''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I came across the following problem in a exam. </p>\n\n<p>We choose a permutation of n elements $[1,n]$ uniformly at random. Now a variable MIN holds the minimum value seen so far at it is defined to $\\infty$ initially. Now during our inspection if we see a smaller value than MIN, then MIN is updated to the new value. </p>\n\n<p>For example, if we consider the permutation, </p>\n\n<p>$$5\\ 9\\ 4\\ 2\\ 6\\ 8\\ 0\\ 3\\ 1\\ 7$$\nthe MIN is updated 4 times as $5,4,2,0$. Then the expected no. of times MIN is updated?</p>\n\n<p>I tried to find the no. of permutations, for which MIN is updated $i$ times, so that I can find the value by $\\sum_{i=1}^{n}iN(i)$, where $N(i)$, is the no. of permutations for which MIN is updated $i$ times.</p>\n\n<p>But for $i\\geq2$, $N(i)$ is getting very complicated and unable to find the total sum.</p>\n', 'ViewCount': '190', 'Title': 'Expected number of updates of minimum', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T22:33:27.977', 'LastEditDate': '2014-03-31T17:47:01.783', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16323', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms>', 'CreationDate': '2014-03-31T15:20:35.840', 'Id': '23295''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>What are the time complexities of the following code?  I posted this on the general stackexchange website, but it was suggested that I post it here.</p>\n\n<pre><code>def func(n):\n    for _ in range(n):\n        if n == 4:\n            for _ in range(n):\n                &lt;O(1) operation&gt;\n</code></pre>\n\n<p>It will only be O(n^2) for one specific input (n = 4) but is O(n) for all other inputs. In this case the worst case is obviously O(n^2), yet my instructor says that O(n) is the correct answer. If "big-Oh" notation is to indicate the worst case scenario, why is it not O(n^2)?</p>\n\n<p>Another one is:</p>\n\n<pre><code>def func2(n):\n    for _ in range(n):\n        if n%2 == 0:\n            for _ in range(n):\n                &lt;O(1) operation&gt;\n</code></pre>\n\n<p>I am not so certain about the run time of this piece of code. Again, worst case is O(n^2). This time half of all possible inputs results in the worst case. Would this suffice in saying that the code runs in O(n^2) time?</p>\n\n<p>If the first part is O(n) and the second part is O(n^2), is there a general rule of thumb when you choose the truly worst case for the "big-Oh" representation?</p>\n', 'ViewCount': '15', 'ClosedDate': '2014-04-01T07:30:59.673', 'Title': 'Time complexity of complex nested for loops', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T07:29:16.013', 'LastEditDate': '2014-04-01T07:29:16.013', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16345', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-04-01T04:51:22.973', 'Id': '23311''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Please can anyone explain me the worst ,average and best case running time for the Unifrom binary search .. Also how can the lookup table be explained?</p>\n', 'ViewCount': '19', 'ClosedDate': '2014-04-01T07:40:45.420', 'Title': 'Uniform Binary Search explanation and lookup table', 'LastEditorUserId': '16347', 'LastActivityDate': '2014-04-01T08:37:14.043', 'LastEditDate': '2014-04-01T08:37:14.043', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16347', 'Tags': '<algorithms><algorithm-analysis><search-algorithms><binary-search>', 'CreationDate': '2014-04-01T06:48:15.420', 'Id': '23313''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'ViewCount': '47', 'Title': "Why don't we calculate swaps and other steps except comparison for finding time complexity of a sorting algorithm?", 'LastEditDate': '2014-04-02T15:14:24.200', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14498', 'Body': '<p>I was learning some basic sorting techniques with their complexity. However I cannot understand why only the number of comparisons are taken into account while calculating time complexity and operations such as swap are ignored. <a href="http://en.wikipedia.org/wiki/Selection_sort#Analysis" rel="nofollow">Link to selection sort analysis</a>. Please help me understand.</p>\n', 'ClosedDate': '2014-04-02T15:14:58.270', 'Tags': '<algorithm-analysis><runtime-analysis><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-02T15:14:24.200', 'CommentCount': '2', 'AcceptedAnswerId': '23341', 'CreationDate': '2014-04-02T13:06:10.697', 'Id': '23339''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m studying for my finals and I can across this statement.</p>\n\n<p>"For a fixed set of (unique) keys, any binary search tree containing those keys can\nbe converted to any other BST on the same set of keys via a sequence of left- and/or right-\nrotations."</p>\n\n<p>I\'m interested in a proof. Does anyone know any references?</p>\n', 'ViewCount': '78', 'Title': 'Unique keys in a binary search tree', 'LastActivityDate': '2014-04-10T16:49:59.190', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15826', 'Tags': '<algorithms><algorithm-analysis><binary-trees>', 'CreationDate': '2014-04-06T18:43:38.257', 'Id': '23481''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>There are lots of <a href="/questions/tagged/algorithm-analysis" class="post-tag" title="show questions tagged &#39;algorithm-analysis&#39;" rel="tag">algorithm-analysis</a> questions around. Many are similar, for instance those asking for an analysis of nested loops or divide &amp; conquer algorithms, but most answers seem to be tailor-made.</p>\n\n<p>On the other hand, the answers <a href="http://cs.stackexchange.com/questions/192/how-to-come-up-with-the-runtime-of-algorithms">to another general question</a> explain the larger picture (in particular regarding asymptotic analysis) with some examples, but not how to get your hands dirty.</p>\n\n<p>Is there a structured, general method for analysing algorithms?</p>\n\n<p><sup>This is supposed to become a <a href="http://meta.cs.stackexchange.com/questions/599/reference-questions">reference question</a> that can be used to point beginners to; hence its broader-than-usual scope. Please take care to give general, didactically presented answers that are illustrated by at least one example but nonetheless cover many situations. Thanks!</sup></p>\n', 'ViewCount': '2051', 'Title': 'Is there a system behind the magic of algorithm analysis?', 'LastActivityDate': '2014-04-10T15:08:23.810', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '32', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><reference-question>', 'CreationDate': '2014-04-09T12:59:52.003', 'FavoriteCount': '30', 'Id': '23593''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p><a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a> is often quoted as being used to find the shortest path route however I was surprised to know that there exist A* search which is a extension of Dijkstra\'s algorithm. </p>\n\n<p>How is it that A* search algorithm is able to perform better compared to Dijkstra\'s algorithm , what sort of technique does it used that Dijkstra\'s algorithm did not use ???</p>\n', 'ViewCount': '84', 'Title': "How is A* search superior to Dijkstra's algorithm", 'LastActivityDate': '2014-04-13T01:53:10.240', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<algorithms><algorithm-analysis><search-algorithms>', 'CreationDate': '2014-04-10T02:18:11.010', 'Id': '23619''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>What is the time complexity of Halley\'s Method? </p>\n\n<p>I am thinking ${\\cal O}(\\log(n)F(n))$, or something very similar to Newton-Raphson, but I feel as though there should be some change to the complexity in order to yield the greater convergence. However, I can\'t seem to find any solid ground to support any of my ideas (as nobody has ever seemed to have asked this question on the internet before)...</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Halley%27s_method" rel="nofollow">Halley\'s Method - Wikipedia</a></p>\n', 'ViewCount': '29', 'Title': "Time Complexity of Halley's Method", 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-16T01:10:02.363', 'LastEditDate': '2014-04-11T15:25:57.560', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16683', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-04-11T14:05:23.880', 'Id': '23674''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Using gradient descent in <em>d</em> dimensions to find a local minimum requires computing gradients, which is computationally much faster than Newton's method, because Newton's method requires computing both gradients and Hessians.</p>\n\n<p>However, gradient descent generally requires many more iterations than Newton's method to converge within the same accuracy.</p>\n\n<p>My question, then, is:</p>\n\n<p>Assuming they both converge, in terms of the <em>number of elementary floating-point operations</em>, which is usually faster:  Newton's method or gradient descent?  Why?</p>\n", 'ViewCount': '32', 'Title': "Gradient descent vs. Newton's method: which is more efficient?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T13:38:17.120', 'LastEditDate': '2014-04-12T13:38:17.120', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<algorithms><algorithm-analysis><optimization><numerical-algorithms>', 'CreationDate': '2014-04-12T11:27:02.383', 'Id': '23701''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose I have an algorithm that has a performance of $O(n + 2)$. Here if n gets really large the 2 becomes insignificant. In this case it's perfectly clear the real performance is $O(n)$.</p>\n\n<p>However, say another algorithm has a performance of $O(n^2/2)$. Here if n gets really large then $n^2/2$ is exactly half of $n^2$, which is not significantly smaller than $n^2$. So why we drop 1/2 from $O(n^2/2)$ and it becomes $O(n^2)$?</p>\n", 'ViewCount': '392', 'Title': 'Why is constant always dropped from big O analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T02:45:53.917', 'LastEditDate': '2014-04-12T15:22:48.923', 'AnswerCount': '3', 'CommentCount': '5', 'AcceptedAnswerId': '23704', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16713', 'Tags': '<terminology><algorithm-analysis><asymptotics><landau-notation>', 'CreationDate': '2014-04-12T14:54:50.047', 'Id': '23703''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have the following algorithm</p>\n\n<pre><code>void NQueen(int k,int n)\n{\n  int i;\n  for(i=1;i&lt;=n;i++)\n  {\n    if(place(k,i)==1)\n    {     x[k]=i;\n            if(k==n)\n            {\n                printf("Solution\\n");\n                printboard(n);\n            }\n            else\n                NQueen(k+1,n);\n    }\n  }\n}\n\nint place(int k,int i)\n{\n  int j;\n  for(j=1;j&lt;k;j++)\n  {\n    if((x[j]==i)||abs(x[j]-i)==abs(j-k))\n        return 0;\n  }\n  return 1;\n}\n\nvoid printboard(int n)\n{\n  int i;\n  for(i=1;i&lt;=n;i++)\n    printf("%d  ",x[i]);\n}\n\nvoid main()\n{\n    int n;\n    printf("Enter Value of N:");\n    scanf("%d",&amp;n);\n    NQueen(1,n);\n}\n</code></pre>\n\n<p>I am having trouble understanding the time complexity of the following algorithm.It has time complexity: $O(n^n)$, As NQueen function is recursively called n times.But is there is any tighter bound possible for this program? what about best case, and worst case time complexity?</p>\n\n<p>Can someone help me understand the time complexity of the algorithm?</p>\n', 'ViewCount': '24', 'ClosedDate': '2014-04-12T21:38:52.353', 'Title': 'How to get time complexity of n queen puzzle algorithm(Using Backtracking)?', 'LastEditorUserId': '16713', 'LastActivityDate': '2014-04-14T13:12:49.753', 'LastEditDate': '2014-04-14T13:12:49.753', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16713', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-04-12T20:06:22.897', 'Id': '23713''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>According to  <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">Introduction to algorithms by Cormen et al</a>,\n$$T(n)=2T(n/2)+n\\log n$$ is not case 3 of Master Theorem. Can someone explain me why?</p>\n\n<p>And which case of master theorem is it?</p>\n', 'ViewCount': '35', 'ClosedDate': '2014-04-13T13:12:06.137', 'Title': '$T(n)=2T(n/2)+n\\log n$ and the Master theorem', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-13T13:49:05.647', 'LastEditDate': '2014-04-13T12:18:48.650', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16715', 'Tags': '<algorithms><algorithm-analysis><master-theorem>', 'CreationDate': '2014-04-13T12:01:52.337', 'Id': '23735''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Suppose we have an $N \\times N \\times N$ 3-d sorted array meaning that every row,column, and file is in sorted order. Searching for an element in this structure can be done using $O(N^2)$ comparisons. However are $\\Omega(N^2)$ comparisons needed in the worst-case? For an $N \\times N$ 2-d sorted array I recall a proof that $\\Omega(N)$ comparisons are needed; I'm having trouble seeing how to extend to the 3-d case though</p>\n", 'ViewCount': '24', 'Title': 'Lower bound on number of comparisons needed to search for a number in a sorted 3-d array', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-14T18:45:49.407', 'LastEditDate': '2014-04-14T18:28:24.510', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23793', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><arrays><lower-bounds>', 'CreationDate': '2014-04-14T17:58:35.807', 'Id': '23791''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>In section 9.2 of CLRS (<em>Introduction to Algorithms; page 185 in the 2nd edition and page 215 in the 3rd edition</em>), a randomized selection algorithm is presented. </p>\n\n<p>For its analysis, $T(n)$ is a random variable denoting the time required on an input array $A[p \\cdots r]$ of $n$ elements and $X_k$ is an indicator random variable $X_k = I \\{ \\text{the subarray } A[p \\cdots q] \\text{ has exactly } k \\text{ elements (due to the pivot)} \\}$. </p>\n\n<p>It has been claimed that $X_k$ and $T(\\max(k-1, n-k))$ are independent (page 187 in the 2nd edition and page 218 in the 3rd edition). However, I find it quite counter-intuitive to understand. How to verify it?</p>\n', 'ViewCount': '26', 'Title': 'Why are the two random variables independent in the analysis of Randomized Selection algorithm in CLRS?', 'LastActivityDate': '2014-04-15T17:13:42.643', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23814', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithm-analysis><randomized-algorithms>', 'CreationDate': '2014-04-15T13:18:26.367', 'Id': '23811''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I have done the proof until the point when $T(n) \\leq cn^{\\log7}$.</p>\n\n<p>But when it comes to finding the value of constant $c$, I am getting stuck.</p>\n\n<p>The given recurrence relation is $T(n) = 7T(n/2) + n^2$. </p>\n\n<p>Since we already calculated the solution above which is $cn^{\\log 7}$.</p>\n\n<p>Inductive step:</p>\n\n<p>Now we have to prove that $T(n) \\leq c n^{\\log7}$ where $c$ is a positive constant.\nIf we consider that the solution holds good for $n/2$ then we can prove that it works  for $n$ also: \n$$T(n/2) \\leq c(n/2)^{\\log7}.$$\nSubstituting these values in the recurrence relation:</p>\n\n<p>$$\n\\begin{align*}\nT(n) &amp;\\leq 7c/(2)^{\\log7} \\times (n)^{\\log7} + n^2 \\\\\n     &amp;\\leq cn^{\\log7}, \\text{ since $7/(2)^{\\log7}$  is constant so can be ignored and $cn^{\\log7} \\gg n^2$ for large $n$} \\\\\n     &amp;\\leq cn^{\\log7} \\text{ assuming $c$ is a constant $\\geq 1$.}\n\\end{align*}\n$$</p>\n\n<p>Finally to find constant $c$, </p>\n\n<p>$$(7/(2)^{\\log7}) \\times cn^{\\log7} + n^2 \\leq cn^{\\log7}. $$</p>\n\n<p>I am not able to find appropriate $c$ for which the condition holds true. </p>\n', 'ViewCount': '59', 'Title': 'To prove the recurrence by substitution method $T(n) = 7T(n/2) + n^2$', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-18T02:50:15.323', 'LastEditDate': '2014-04-17T17:55:46.163', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16666', 'Tags': '<algorithms><algorithm-analysis><recurrence-relation>', 'CreationDate': '2014-04-17T17:22:36.890', 'Id': '23892''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am in an entry-level algorithms class, and for our final project we are coding and thoroughly analyzing 6 different sorting methods. Part of the analyzation is timing the methods and comparing the runtime results depending on the original order of the array (in order to more fully grasp the concept of constant costs, I suppose). I coded the bubble sort in Java, and when I run it on an array that is in descending order, it returns a sorted array FASTER than when I run it on an array of random ints, even though it is doing, on average, twice as many swaps. It seems to me that doing twice as many operations should result in taking much longer to finish. I have NO idea what could be causing this discrepancy, and any help would be appreciated.</p>\n', 'ViewCount': '56', 'Title': 'Why is my bubble sort taking longer to sort a random array as opposed to a descending array?', 'LastActivityDate': '2014-04-18T03:09:06.577', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16861', 'Tags': '<algorithms><algorithm-analysis>', 'CreationDate': '2014-04-17T17:34:49.310', 'Id': '23893''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose we have $n$ balls and $n$ bins. We put the balls into the bins randomly. If we count the maximum number of balls in any bin, the expected value of this is  $\\Theta(\\ln n/\\ln\\ln n)$. How can we derive this fact? Are Chernoff bounds helpful?</p>\n', 'ViewCount': '63', 'ClosedDate': '2014-04-23T16:47:14.277', 'Title': 'Expected maximum bin load, for balls in bins with equal number of balls and bins', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-19T00:38:01.780', 'LastEditDate': '2014-04-19T00:38:01.780', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15406', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><probabilistic-algorithms><chernoff-bounds>', 'CreationDate': '2014-04-18T22:43:53.940', 'Id': '23925''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was working on a programming puzzle were given a list of words, the anagrams should be printed together. For example, <code>{"cat", "bat", "act", "tab", "mat", "listen","silent"}</code> should result in <code>[listen, silent], [mat], [bat, tab], [cat, act]</code>.</p>\n\n<p>I have the following pseudocode:</p>\n\n<pre><code>mapStrings():\n For each string in the string array:\n    Find a key for string using a hash function h() \n    If that key does not exist in the hash table, add the key \n    If the key exists, append its value with the string\n\n Print value for each key\n\nh():\n  sum = 0\n  For each character in string:\n    Add ASCII value of the character to sum\n  return sum\n</code></pre>\n\n<p>The time complexity of the for loop in the <code>mapStrings()</code> method would be $\\cal O(n)$ as it goes over each element in the string array of length n. </p>\n\n<p>The call to the method <code>h()</code> would be $\\cal O(m)$, where $m = m_{1} + m_{2} + ... + m_{n}$ is the sum of lengths of each string in the string array.</p>\n\n<p>Hence the total time complexity would be $\\cal O(n+m)$ = $\\cal O(N)$, where $N = n+m$.</p>\n\n<ol>\n<li>Are my statements correct?  </li>\n<li>If it is, then what would the time complexity\nbe if the method <code>h()</code>\'s complexity was $\\cal O(n^2)$ instead of $\\cal O(n)$? Would it be\n$\\cal O(n^2 + m) \\approx \\cal O(n^2)$?</li>\n</ol>\n', 'ViewCount': '13', 'ClosedDate': '2014-04-24T20:14:46.580', 'Title': 'Runtime of adding strings to a hashtable', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-24T20:41:25.567', 'LastEditDate': '2014-04-24T20:41:25.567', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5092', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-04-24T20:12:24.817', 'Id': '24087''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': u'<p>Please consider the following Divide-And-Conquer Problem:</p>\n\n<blockquote>\n  <p>You\u2019re consulting for a small computation-intensive investment\n  company, and they have the following type of problem that they want to\n  solve over and over. A typical instance of the problem is the\n  following. They\u2019re doing a simulation in which they look at n\n  consecutive days of a given stock, at some point in the past. Let\u2019s\n  number the days i=1,2,...,n; for each day i, they have a price p(i)\n  per share for the stock on that day. (We\u2019ll assume for simplicity that\n  the price was fixed during each day.) Suppose during this time period,\n  they wanted to buy 1,000 shares on some day and sell all these shares\n  on some (later) day. They want to know: When should they have bought\n  and when should they have sold in order to have made as much money as\n  possible? (If there was no way to make money during the n days, you\n  should report this instead.)</p>\n</blockquote>\n\n<p>There already is a <a href="http://cs.stackexchange.com/questions/10050/confusion-related-to-a-divide-and-conquer-problem">discussion about the same problem</a>, but I don\'t understand why they try to make it that complicated. I think the following Algorithm will yield the result they ask for:</p>\n\n<pre><code>DivideAndConquer(1,..,n)\n(i1,j1) := DivideAndConquer(1,..,n/2)\n(i2,j2) := DivideAndConquer(n/2 + 1,..,n)\nreturn (min(p(i1),pi2),max(p(j1),p(j2)))\n</code></pre>\n\n<p>Even if they ask for the range, we can get it from <code>(i,j) := DivideAndConquer(1,..,n)</code> by <code>j-i</code>. So, what am I missing?</p>\n\n<p>PS: I don\'t see the point of using a DivideAndConquer approch at all. Why not sorting the input twice. Once in ascending and once in descending order induced by <code>p(i)</code>.</p>\n\n<p>EDIT: According to the explanation of Yuval Filmus, the algorithm should look like the following:</p>\n\n<pre><code>DivideAndConquer(1,..,n)\n(i1,j1) := DivideAndConquer(1,..,n/2)\n(i2,j2) := DivideAndConquer(n/2 + 1,..,n)\nLet p(i) be minimal in 1,..,n/2\nLet p(j) be maximal in n/2 + 1,..,n\nreturn (i,j)\n</code></pre>\n', 'ViewCount': '31', 'Title': 'Just another Divide-And-Conquer question - but somehow different', 'LastEditorUserId': '12502', 'LastActivityDate': '2014-04-28T19:03:31.463', 'LastEditDate': '2014-04-28T19:03:31.463', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12502', 'Tags': '<algorithms><algorithm-analysis><divide-and-conquer>', 'CreationDate': '2014-04-28T18:15:22.217', 'Id': '24195''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I was reading the Paxos notes from yale from the following link:</p>\n\n<p><a href="http://pine.cs.yale.edu/pinewiki/Paxos" rel="nofollow">http://pine.cs.yale.edu/pinewiki/Paxos</a></p>\n\n<p>Recall that Paxos is a distributed system algorithm with the goal that the processes participating in its protocol will reach consensus on one of the valid values.</p>\n\n<p>I was trying to better understand the revoting mechanism to avoid deadlocks in Paxos. The revoting mechanism is explained as follows in the article: </p>\n\n<blockquote>\n  <p>The revoting mechanism now works like this: before taking a vote, a\n  proposer tests the waters by sending a prepare(n) message to all\n  accepters where n is the proposal number. An accepter responds to this\n  with a promise never to accept any proposal with a number less than n\n  (so that old proposals don\'t suddenly get ratified) <strong>together with\n  the highest-numbered proposal that the accepter has accepted</strong> (so\n  that the proposer can substitute this value for its own, in case the\n  previous value was in fact ratified).</p>\n</blockquote>\n\n<p>The bold section is the one that I was trying to understand better. The author tries to justify it with:</p>\n\n<blockquote>\n  <p>"...so that the proposer can substitute this value for its own, in case the\n  previous value was in fact ratified..."</p>\n</blockquote>\n\n<p>But I didn\'t really understand why the proposer would want to ratify the previous value. By doing this, what crucial safety property is he guaranteeing? Why is he responding with that and not something else? Responding with the highest could be problem, right, since the current proposal would get lost?</p>\n', 'ViewCount': '22', 'Title': 'Why does an acceptor send the highest numbered proposal with number less than n as a response to prepare(n) in paxos?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-04-30T16:32:48.753', 'LastEditDate': '2014-04-30T16:32:48.753', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<algorithms><algorithm-analysis><distributed-systems>', 'CreationDate': '2014-04-29T04:00:18.340', 'Id': '24209''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Hello guys I will write a thesis about intelligent System and im just wondering if someone can give me an idea on how can i build my title or what program i can make in intelligent systems.. or Like articles Lectures i can read ETC.</p>\n\n<p>I just need tips and some books you can suggest for me in choosing my title. sorry for questioning it here but im preparing it because im a little nervous so i need some suggestion lectures books article i can read. Tnx for those who will answer I thank you in advance</p>\n\n<p>im planning on using java.</p>\n\n<p>sorry for the bad english.</p>\n', 'ViewCount': '10', 'ClosedDate': '2014-04-29T20:31:22.503', 'Title': 'Intelligent System questions', 'LastActivityDate': '2014-04-29T14:10:29.403', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '17179', 'Tags': '<algorithms><algorithm-analysis><artificial-intelligence>', 'CreationDate': '2014-04-29T14:10:29.403', 'Id': '24222''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>This question is from a practice exam in my algorithms class. I'm posting the question and the answer listed in that practice exam:</p>\n\n<blockquote>\n  <p>Let $W$ be an $n\\times n$ matrix whose $(i,j)$-th entry is $\\omega_n^{ij}$, where $\\omega_n$ is a principal $n$th root of unity. Let $X=(X_0,\\dots,X_{n-1})$ be an $n$-vector. The product $W \\times X$ can be computed in $O(n\\log n)$ time. Let $FFT(X)$ denote the vector that results by applying the FFT evaluation algorithm to the vector $X$. </p>\n  \n  <p>Describe an $O(n)$ algorithm to compute $FFT(FFT(x))$.</p>\n  \n  <p>Answer: $FFT(FFT(x))$ is $W^2\\times X$<br>\n  $(W^2)_{jk} = 0$ if $j+k$ is not a multiple of $n$, and $n$ otherwise.</p>\n</blockquote>\n\n<p>I'm confused about everything:<br>\n1. How does this run in $O(n)$? To compute $(W^2)_{jk}$ I'm already  iterating through $n^2$ elements....<br>\n2. Why is $FFT(FFT(x)) = W^2\\times X$?<br>\n3. Why is $(W^2)_{jk} = 0$ if $j+k$ is not a multiple of $n$, $n$ otherwise?</p>\n\n<p>I would appreciate an answer that isn't too advanced as my math skills are limited to basic undergrad math classes of an engineering major.</p>\n", 'ViewCount': '40', 'Title': 'An $O(n)$ algorithm to FFT-evaluate an FFT evaluation', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T18:54:56.350', 'LastEditDate': '2014-04-30T18:54:56.350', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24270', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '17225', 'Tags': '<algorithms><algorithm-analysis><fourier-transform>', 'CreationDate': '2014-04-30T18:13:56.290', 'Id': '24269''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I\'m studying binomial heaps in anticipation for my finals and the <a href="http://en.wikipedia.org/wiki/CLRS" rel="nofollow">CLRS</a> book tells me that insertion in a binomial heap takes $\\Theta(\\log n)$ time. So given an array of numbers it would take $\\Theta(n\\log n)$ time to convert it a a binomial heap. To me that seems a bit pessimistic and like a naive implementation. Does anyone know of a method/implementation that can convert an array of numbers to a binary heap in $\\Theta(n)$ time?</p>\n', 'ViewCount': '46', 'Title': 'efficient binomial heap', 'LastActivityDate': '2014-05-03T15:47:26.787', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24356', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15826', 'Tags': '<algorithms><algorithm-analysis><heaps>', 'CreationDate': '2014-05-03T14:47:09.500', 'Id': '24355''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}