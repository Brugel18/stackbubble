2190:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I was reading <strong>Linux Kernel Development</strong> by Robert Love, where I came across this</p>\n\n<blockquote>\n  <p>Linux takes an interesting approach to thread support: It does not\n  differentiate between threads and normal processes.To the kernel, all\n  processes are the same\u2014 some just happen to share resources.</p>\n</blockquote>\n\n<p>I do not know much about OSs (aspire to know more) and kernels and hence the above quote raised a question about thread implementations in different OSs(at least the popular ones like Windows, Linux and Unix).</p>\n\n<p>Can someone please explain the different techniques for providing thread-support in an OS? ( and optionally contrast them)</p>\n', 'ViewCount': '512', 'Title': 'How are threads implemented in different OSs?', 'LastEditorUserId': '59', 'LastActivityDate': '2012-06-07T23:11:00.057', 'LastEditDate': '2012-06-07T23:11:00.057', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '59', 'Tags': '<operating-systems><process-scheduling><threads>', 'CreationDate': '2012-06-07T07:07:14.337', 'Id': '2248'},2191:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What would be the process state in a multi threaded process, in which threads are in different states (running, waiting, blocked etc)</p>\n', 'ViewCount': '101', 'Title': 'Process state in multi threaded process', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-09T07:57:00.760', 'LastEditDate': '2012-11-09T07:57:00.760', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4508', 'Tags': '<terminology><operating-systems><process-scheduling><threads>', 'CreationDate': '2012-11-08T10:47:53.373', 'Id': '6558'},2192:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given a program consisting of variables and instructions which modify these variables, and a synchronization primitive (a monitor, mutex, java's synchronized or C#'s lock), is it possible to prove that such a program is thread safe? </p>\n\n<p>Is there even a formal model for describing things like thread safety or racing conditions? </p>\n", 'ViewCount': '110', 'Title': 'Is it possible to prove thread safety?', 'LastActivityDate': '2013-09-17T14:26:49.070', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '14365', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10169', 'Tags': '<proof-techniques><correctness-proof><concurrency><threads>', 'CreationDate': '2013-09-16T15:57:39.443', 'FavoriteCount': '2', 'Id': '14356'},2193:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>What are the reasons for decreasing the number of threads in a parallel implementation ? </p>\n\n<p>Assume that we have two implementations, the first one with 4 threads, and a second one with 8 threads, and both have exactly the same run time. What are the various reasons to prefer the first one?</p>\n\n<p>I stress on the fact that the two implementations have the same run times.</p>\n\n<p>The obvious reasons are the following:</p>\n\n<ul>\n<li><p>If we have 8 processors, we can execute at the same time this implementation on two different sets of inputs thus performing twice the work,</p></li>\n<li><p>Less resources consumption by the OS which handles less number of threads.</p></li>\n</ul>\n\n<p>I'm looking for other reasons...</p>\n", 'ViewCount': '67', 'Title': 'Reasons for decreasing the number of threads in a parallel implementation', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-18T21:03:32.200', 'LastEditDate': '2013-10-18T21:03:32.200', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8767', 'Tags': '<parallel-computing><threads>', 'CreationDate': '2013-10-18T06:07:34.130', 'Id': '16190'},2194:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>An OS contains 10 identical processes that were initiated at the same time.Each process contains 15 identical requests. Each request consume 20 msec of CPU time.A request is followed by an I/O operation which consumes 10 msec.The CPU scheduling overhead is 2 msec. The system uses Round Robin scheduling with the time quantum of 10 msec.</p>\n\n<p>Q1) What is the response time of 1st request of last process ? \n  A) 210 msec  B) 140 msec  c) 230 msec  D) 240 msec</p>\n\n<p>Q2) The subsequent request of the processes receives a response times of \n A) 110 msec  B) 220 msec  C) 230 msec  D) 240 msec</p>\n\n<p>Ans: Q1) D</p>\n\n<pre><code> Q2) C\n</code></pre>\n\n<p>What I thought : </p>\n\n<p>For 1 process, there are 15 request so \n15 * ( 20 + 10) = 450 msec  But all answers are so small than this approach. So no need to think about further i.e. CPU overhead then 2nd...3rd...processes. </p>\n\n<p>Here my problem is I didn't get the concept behind this question properly. CPU overhead ( i.e context switching ) will take place between 10 processes or each process's 15 request.\nSo please tell how this scenario will work. </p>\n\n<p>I didn't get the meaning of 2nd question. </p>\n", 'ViewCount': '60', 'Title': 'processes response time confusion', 'LastEditorUserId': '9343', 'LastActivityDate': '2013-11-12T08:21:02.980', 'LastEditDate': '2013-11-12T05:00:57.927', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '9343', 'Tags': '<operating-systems><memory-management><process-scheduling><threads>', 'CreationDate': '2013-11-11T18:54:01.780', 'Id': '17919'},2195:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>At 15:30 in <a href="http://www.infoq.com/presentations/clojure-core-async" rel="nofollow">this talk</a>  (p13 of <a href="http://qconsf.com/system/files/presentation-slides/Clojure.pdf" rel="nofollow">this presentation here</a>) Rich Hickey mentions the formalisms available for reasoning about Communicating Sequential Processes. He then goes on to mention that these haven\'t yet been applied to Clojure\'s core.async. </p>\n\n<p>The only reference I know of for CSP is <a href="http://www.usingcsp.com/cspbook.pdf" rel="nofollow">Hoare\'s book</a>. I can see some examples of <a href="http://en.wikipedia.org/wiki/Communicating_sequential_processes#Algebraic_operators" rel="nofollow">formalisms to describe CSP here</a>. </p>\n\n<p>Suppose I wanted to do the most basic of CSP as per Rob Pike\'s presentation in Go (slide 43 <a href="http://talks.golang.org/2012/concurrency.slide#43" rel="nofollow">of this</a>). </p>\n\n<pre><code>var (\n    Web = fakeSearch("web")\n    Image = fakeSearch("image")\n    Video = fakeSearch("video")\n)\n\ntype Search func(query string) Result\n\nfunc fakeSearch(kind string) Search {\n        return func(query string) Result {\n              //time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)\n              return Result(fmt.Sprintf("%s result for %q\\n", kind, query))\n        }\n}\n</code></pre>\n\n<p><strong>How would I express this as an Algebraic statement? What benefit would it give me?</strong></p>\n\n<p>Taking a stab at it - I get</p>\n\n<p><code>(Web &rarr; P) &Pi; (Search &rarr; Q)  &Pi; (Video &rarr; R)</code></p>\n\n<p>Which tells me that the Search process can choose to interact with Web or Video depending on which environment the Search Process chooses to communicate with. </p>\n\n<p><strong>Assumptions</strong></p>\n\n<p>I\'m told that you can\'t do this in pure CSP because of the timer. For an answer I\'m happy to  assume we\'ll ignore the timer part of this. </p>\n', 'ViewCount': '45', 'Title': "How to express Rob Pike's classic Go Code presentation in Hoare's CSP Algebra?", 'LastActivityDate': '2013-12-30T13:02:34.043', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19375', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1709', 'Tags': '<concurrency><process-algebras><threads>', 'CreationDate': '2013-12-30T06:41:12.887', 'Id': '19372'},2196:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Basically, I using an algorithm called 'miranda' to look at miRNA targets and it only runs on a single thread. It compares everything in one file against everything in another file, produces a file as an output and runs off of the command line in terminal. The process took roughly 20 hours to create the output file.</p>\n\n<p>I was advised by my supervisor that if i split one of the files up into say 4 equally sized parts, and ran them in four separate terminal windows this would decrease the overall time it took for the process to be completed.</p>\n\n<p>I found that when I was using a single terminal window, the process would take up about 100-120% of the CPU. However, when running four terminal windows, each individual process only takes between 30-40% of the CPU.</p>\n\n<p>How much effect does splitting the file up like this have in the overall time it takes to run the process? Although I split it across four threads, will the effect only be an increase in speed of about 1.5 times?</p>\n", 'ViewCount': '44', 'Title': 'Does splitting a process across 4 terminal windows decrease the time it takes?', 'LastActivityDate': '2014-03-05T18:02:23.337', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9799', 'Tags': '<algorithms><cpu-pipelines><threads>', 'CreationDate': '2014-03-04T17:57:45.533', 'Id': '22277'},2197:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have the memory reference string for a multithreaded application and want to run this through a simulator which implements/approximates Belady\'s OPT page replacement algorithm.</p>\n\n<p>But what is the best way to do this?</p>\n\n<p>There is no problem with a single threaded approach - just look for the page in memory with longest reuse distance and get rid of it. But with multithreaded this becomes much more complex - we know what the reuse distances for each thread is, but we don\'t know the "combined" reuse distance.</p>\n\n<p>Practically, it\'s easy to get into the position where a page for one thread has a very long reuse distance and so gets chucked out when memory space is needed, only for it to be quickly faulted back in by another thread.</p>\n\n<p>I cannot find any scientific literature on this - but cannot believe it hasn\'t been studied before: does anyone know of any papers that consider this issue?</p>\n', 'ViewCount': '25', 'Title': "Implementing/approximating Belady's OPT for multithreaded environments (papers?)", 'LastActivityDate': '2014-03-16T22:39:32.250', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6712', 'Tags': '<memory-management><paging><program-optimization><threads>', 'CreationDate': '2014-03-16T22:39:32.250', 'Id': '22687'},2198:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '32', 'Title': 'If a thread containing main terminates, can another thread do anything?', 'LastEditDate': '2014-03-29T20:42:43.353', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11711', 'FavoriteCount': '1', 'Body': '<p>I\'m studying threads in C and I have this theoretical question in mind that is driving me crazy.\nAssume the following code:</p>\n\n<pre><code>1) void main() {\n2)     createThread(...); // create a new thread that does "something"\n3) }\n</code></pre>\n\n<p>After line 2 is executed, two paths of execution are created. However I believe that immediately after line 2 is executed then it doesn\'t even matter what the new thread does, which was created at line 2, because the original thread that executed line 2 will end the entire program at its next instruction. Am I wrong? is there any chance the original thread gets suspended somehow and the new thread get its chance to do something (assume the code as is, no sync between threads or join operations are performed)</p>\n', 'Tags': '<parallel-computing><concurrency><threads>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T20:42:43.353', 'CommentCount': '1', 'AcceptedAnswerId': '23136', 'CreationDate': '2014-03-27T15:08:12.743', 'Id': '23129'},2199:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>suppose Threads 1 and 2 are executing concurrently with shared integer variables A, B, and C. Thread 1 executes:A=4, B=5, C=B-A; Thread 2 executes: A=3, B=6, C=A+B; </p>\n\n<p>I'm trying to find what can the possible values of C be after execution of this fragment when synchronization is not implemented.</p>\n\n<p>I know if there is no synchronization then the reads see writes that occur later in the execution order and it will be counter intuitive ( execution will be happens-before consistency)</p>\n\n<p>I am confused what can be the possible values of C. Can the possible values for C be 1,2,3,8,9,10 after substituting the all possible values for A &amp; B from both thread 1 and thread 2 for C=B-A &amp; C=A+B </p>\n", 'ViewCount': '33', 'Title': 'executing threads with no synchronization', 'LastActivityDate': '2014-04-02T17:37:34.043', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '23357', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11716', 'Tags': '<threads>', 'CreationDate': '2014-04-02T16:39:19.420', 'Id': '23349'},21910:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am currently reading <a href="https://www.kernel.org/doc/ols/2002/ols2002-pages-479-495.pdf">Fuss, Futexes and Furwocks: Fast Userland Locking in Linux</a> and came across this quote:</p>\n\n<blockquote>\n  <p>In a fair locking scheme the lock is granted in the order it was\n  requested. This can have negative impact on throughput due to the\n  increased number of context switches. At the same time it can lead to\n  the so called convoy problem. Since the locks are granted in the order\n  of arrival, they all proceed at the speed of the slowest process,\n  slowing down all waiting processes. A common solution to the convoy\n  problem has been to mark the lock available upon release, wake all\n  waiting processes and have them recontend for the lock. This is\n  referred to as random fairness. However, this also leads to the\n  thundering herd problem. Despite this, it can work well on\n  uni-processor systems if the first task to wake releases the lock\n  before being preempted or scheduled, allowing the second herd member\n  to obtain the lock, etc...</p>\n</blockquote>\n\n<p>I have a few questions about this quote.</p>\n\n<p>First, does a fair locking scheme result in an increased number of context switches because different tasks put processes into the wait queue at different times and thus by serving processes in the order they were received, we\'d be context switching between multiple tasks?</p>\n\n<p>Second, how does granting locks in the order of arrival cause processes to proceed at the speed of the slowest process? Wouldn\'t this only be the case if the slowest process is granted the lock before the other processes? Similarly, how does having processes contending randomly for the lock solve the convoy problem?</p>\n\n<p>Finally, I don\'t understand how random fairness is any better on uni-processor systems in comparison to multiprocessor systems. For example, in both cases, all of the waiting processors are woken up, one gets the lock, and the others have to go to sleep again, right? So how does this work well on uni-processor systems?</p>\n', 'ViewCount': '81', 'Title': 'Question about threads and locks', 'LastActivityDate': '2014-04-14T21:09:55.147', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23798', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '16760', 'Tags': '<concurrency><threads>', 'CreationDate': '2014-04-14T15:12:48.010', 'FavoriteCount': '1', 'Id': '23785'},21911:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Thread_%28computing%29" rel="nofollow">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>Operating systems schedule threads in one of two ways:</p>\n  \n  <p><strong>Preemptive multitasking</strong> is generally considered the superior approach, as it allows the operating system to determine when a\n  context switch should occur. The disadvantage of preemptive\n  multithreading is that the system may make a context switch at an\n  inappropriate time, causing lock convoy, priority inversion or other\n  negative effects, which may be avoided by cooperative multithreading.</p>\n  \n  <p><strong>Cooperative multithreading</strong>, on the other hand, relies on the threads themselves to relinquish control once they are at a stopping\n  point. This can create problems if a thread is waiting for a resource\n  to become available.</p>\n</blockquote>\n\n<p>I wonder if in  parallel computing (writing and running parallelized programs in OpenMP, OpenMPI, pThread), which of Cooperative multithreading and Preemptive multitasking is/are used, or does the way OS scheduling threads have nothing to do with the multi-process or multi-thread within a parallelized program?</p>\n', 'ViewCount': '53', 'Title': 'Is the way an OS schedule threads related to parallel computing?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-27T11:29:29.867', 'LastEditDate': '2014-04-27T11:29:29.867', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<operating-systems><parallel-computing><threads>', 'CreationDate': '2014-04-26T18:16:11.453', 'Id': '24137'},21912:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have following structure\nstruct data{\nint count;\nchar *name;\n}</p>\n\n<p>kthread_run (threadfn,\n    data,\n    namefmt,\n    );</p>\n\n<p>Now from threadfn i am able to access data->count but i am not able to access data->name, it gives kernel cannot handle paging request.</p>\n\n<p>Any ideas how to fix this and access pointers declared in structure from a threadfn.</p>\n\n<p>Thanks</p>\n', 'ViewCount': '7', 'ClosedDate': '2014-04-28T20:57:21.077', 'Title': 'Passing Structure to Thread', 'LastActivityDate': '2014-04-28T16:53:56.677', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15406', 'Tags': '<programming-languages><functional-programming><threads><kernel>', 'CreationDate': '2014-04-28T16:53:56.677', 'Id': '24187'}