1910:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Here I\'m really interested in lowering barriers to mathematical education.</p>\n\n<p>Target:</p>\n\n<p>I\'d like to see created for the JavaScript community, an equivalent of the Python-based/linked <strong>scientific and high-performance computing</strong> libraries (great lists of which are available through <a href="http://sagemath.org/download-packages.html" rel="nofollow">Sage</a> and <a href="http://wiki.python.org/moin/NumericAndScientific" rel="nofollow">otherwise</a>). And I want that, because I\'d like to make it easy for people who learn JavaScript to get into scientific and numerical computing without having to learn Python (&amp; company). (I know it\'s easy to learn Python, as I basically did it at some point, but this suggests that perhaps it\'ll be easy to compile some restricted subset of JavaScript to Python.)</p>\n\n<p>Hypothesised method:</p>\n\n<p>I\'m primarily interested in a new language with minimal difference from JavaScript, because the market ("human compilers") I\'m targeting are programmers who already know JavaScript. What I want to target those people for, is to give them a minimally different language in which to write code that compiles to faster C, in the manner that RPython and Cython do for Python. I\'m willing to throw out a lot of JavaScript features, I just want to be careful to add a minimum number of features back in. I\'ll definitely be looking at Lua, Dart, ECMA Harmony (which has no formal date of release, or am I mistaken?), etc. as these are all close resemblances to contemporary (2012) implementations of JavaScript.</p>\n\n<p>Questionable Motivations:</p>\n\n<p>I\'m personally willing to learn any language/toolset that gets things done faster (I\'m learning Erlang myself, for this), but here, I am specifically interested in lowering the bar (sorry) for other people who may not have such willingness. This is just one of those "want to have my cake, and eat it too, so I am putting some time into researching the problem" situations. I have very limited prior experience in computer language design, but so far from a hacking-the-ecosystem point of view, the problem seems interesting enough to study, so, I hope to be doing more of that soon.</p>\n', 'ViewCount': '1892', 'Title': 'A faster, leaner JavaScript for scientific computing: what features should I keep?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-20T22:00:49.383', 'LastEditDate': '2012-09-02T14:22:18.533', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1394', 'Tags': '<programming-languages><compilers><performance>', 'CreationDate': '2012-05-06T19:39:35.367', 'FavoriteCount': '2', 'Id': '1693'},1911:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I first asked this question at cstheory, but they suggested to ask my question here, so here it goes ...</p>\n\n<p>I'm working on my masters thesis and I need to have theoretical value of the (average) speed gain that a quadcore processor brings compared to a singlecore processor, when they both use the same frequency. So for example the speed gain of a 2 GHz singlecore vs. 2 Ghz quadcore.</p>\n\n<p>Somewhere on the internet I've read that a quadcore is 2.6 times faster than singlecore, but the author didn't mention any source so I cannot use that in my thesis.</p>\n\n<p>I've been trying to calculate some things myself, but didn't come to a conclusion. I tried like this:</p>\n\n<pre><code>threads | quad core | single core | ratio \n--------|-----------|-------------|-------\n1       | 1         | 1           | 1\n2       | 2         | 1/2         | 4\n3       | 3         | 1/3         | 9\n4       | 4         | 1/4         | 16\n5       | 3+1/2     | 1/5         | 17.5\n6       | 2+2(1/2)  | 1/5         | 15\n7       | 1+3(1/2)  | 1/5         | 12.5\n...\n</code></pre>\n\n<p>This table represents the timeslices available to execute a task (I've taken a fair 50/50 usage for each thread). For example when using a singlecore and a application uses 3 threads, each thread can work 1/3 of the time, while with a quadcore each thread can work 100% of the time, because 3 threads can be spread accross a separate core. After playing with some calculations in Excel, I could not come to any conclusion.</p>\n\n<p>I'm a bit stuck and need some fresh ideas on how to get a theoretical number that represents how much faster a quad core is (on average) compared to a single core. Maybe some of you know some empirical numbers with a good reference to the source? Anyway, all the help is welcome, because I'm a bit stuck and this is the last part I need to cover in my thesis.</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '1055', 'Title': 'Theoretical speed gain of quad core vs. single core', 'LastActivityDate': '2012-06-10T05:32:08.937', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1807', 'Tags': '<computer-architecture><performance>', 'CreationDate': '2012-06-09T14:41:45.407', 'Id': '2300'},1912:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '748', 'Title': 'How fast can we find all Four-Square combinations that sum to N?', 'LastEditDate': '2012-08-02T16:54:45.280', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2334', 'FavoriteCount': '2', 'Body': '<p>A question was asked at Stack Overflow (<a href="http://stackoverflow.com/questions/11732555/how-to-find-all-possible-values-of-four-variables-when-squared-sum-to-n#comment15599644_11732555">here</a>):</p>\n\n<blockquote>\n  <p>Given an integer $N$, print out all possible\n  combinations of integer values of $A,B,C$ and $D$ which solve the equation $A^2+B^2+C^2+D^2 = N$.</p>\n</blockquote>\n\n<p>This question is of course related to <a href="http://en.wikipedia.org/wiki/Lagrange%27s_four-square_theorem" rel="nofollow">Bachet\'s Conjecture</a> in number theory (sometimes called Lagrange\'s Four Square Theorem because of his proof).  There are some papers that discuss how to find a single solution, but I have been unable to find anything that talks about how fast we can find <em>all</em> solutions for a particular $N$ (that is, all <em>combinations</em>, not all <em>permutations</em>).</p>\n\n<p>I have been thinking about it quite a bit and it seems to me that it can be solved in $O(N)$ time and space, where $N$ is the desired sum. However, lacking any prior information on the subject, I am not sure if that is a significant claim on my part or just a trivial, obvious or already known result.</p>\n\n<p>So, the question then is, how fast can we find all of the Four-Square Sums for a given $N$?</p>\n\n<hr>\n\n<p>OK, here\'s the (nearly) O(N) algorithm that I was thinking of.  First two supporting functions, a nearest integer square root function:</p>\n\n<pre><code>    // the nearest integer whose square is less than or equal to N\n    public int SquRt(int N)\n    {\n        return (int)Math.Sqrt((double)N);\n    }\n</code></pre>\n\n<p>And a function to return all TwoSquare pairs summing from 0 to N:</p>\n\n<pre><code>    // Returns a list of all sums of two squares less than or equal to N, in order.\n    public List&lt;List&lt;int[]&gt;&gt; TwoSquareSumsLessThan(int N)\n    {\n        //Make the index array\n        List&lt;int[]&gt;[] Sum2Sqs = new List&lt;int[]&gt;[N + 1];\n\n        //get the base square root, which is the maximum possible root value\n        int baseRt = SquRt(N);\n\n        for (int i = baseRt; i &gt;= 0; i--)\n        {\n            for (int j = 0; j &lt;= i; j++)\n            {\n                int sum = (i * i) + (j * j);\n                if (sum &gt; N)\n                {\n                    break;\n                }\n                else\n                {\n                    //make the new pair\n                    int[] sumPair = { i, j };\n                    //get the sumList entry\n                    List&lt;int[]&gt; sumLst;\n                    if (Sum2Sqs[sum] == null)\n                    {   \n                        // make it if we need to\n                        sumLst = new List&lt;int[]&gt;();\n                        Sum2Sqs[sum] = sumLst;\n                    }\n                    else\n                    {\n                        sumLst = Sum2Sqs[sum];\n                    }\n                    // add the pair to the correct list\n                    sumLst.Add(sumPair);\n                }\n            }\n        }\n\n        //collapse the index array down to a sequential list\n        List&lt;List&lt;int[]&gt;&gt; result = new List&lt;List&lt;int[]&gt;&gt;();\n        for (int nn = 0; nn &lt;= N; nn++)\n        {\n            if (Sum2Sqs[nn] != null) result.Add(Sum2Sqs[nn]);\n        }\n\n        return result;\n    }\n</code></pre>\n\n<p>Finally, the algorithm itself:</p>\n\n<pre><code>    // Return a list of all integer quads (a,b,c,d), where:\n    //      a^2 + b^2 + c^2 + d^2 = N,\n    // and  a &gt;= b &gt;= c &gt;= d,\n    // and  a,b,c,d &gt;= 0\n    public List&lt;int[]&gt; FindAllFourSquares(int N)\n    {\n        // get all two-square sums &lt;= N, in descending order\n        List&lt;List&lt;int[]&gt;&gt; Sqr2s = TwoSquareSumsLessThan(N);\n\n        // Cross the descending list of two-square sums &lt;= N with\n        // the same list in ascending order, using a Merge-Match\n        // algorithm to find all combinations of pairs of two-square\n        // sums that add up to N\n        List&lt;int[]&gt; hiList, loList;\n        int[] hp, lp;\n        int hiSum, loSum;\n        List&lt;int[]&gt; results = new List&lt;int[]&gt;();\n        int prevHi = -1;\n        int prevLo = -1;\n\n        //  Set the Merge sources to the highest and lowest entries in the list\n        int hi = Sqr2s.Count - 1;\n        int lo = 0;\n\n        //  Merge until done ..\n        while (hi &gt;= lo)\n        {\n            // check to see if the points have moved\n            if (hi != prevHi)\n            {\n                hiList = Sqr2s[hi];\n                hp = hiList[0];     // these lists cannot be empty\n                hiSum = hp[0] * hp[0] + hp[1] * hp[1];\n                prevHi = hi;\n            }\n            if (lo != prevLo)\n            {\n                loList = Sqr2s[lo];\n                lp = loList[0];     // these lists cannot be empty\n                loSum = lp[0] * lp[0] + lp[1] * lp[1];\n                prevLo = lo;\n            }\n\n            // do the two entries\' sums together add up to N?\n            if (hiSum + loSum == N)\n            {\n                // they add up, so cross the two sum-lists over each other\n                foreach (int[] hiPair in hiList)\n                {\n                    foreach (int[] loPair in loList)\n                    {\n                        // make a new 4-tuple and fill it\n                        int[] quad = new int[4];\n                        quad[0] = hiPair[0];\n                        quad[1] = hiPair[1];\n                        quad[2] = loPair[0];\n                        quad[3] = loPair[1];\n\n                        // only keep those cases where the tuple is already sorted\n                        //(otherwise it\'s a duplicate entry)\n                        if (quad[1] &gt;= quad[2]) //(only need to check this one case, the others are implicit)\n                        {\n                            results.Add(quad);\n                        }\n                        //(there\'s a special case where all values of the 4-tuple are equal\n                        // that should be handled to prevent duplicate entries, but I\'m\n                        // skipping it for now)\n                    }\n                }\n                // both the HI and LO points must be moved after a Match\n                hi--;\n                lo++;\n            }\n            else if (hiSum + loSum &lt; N)\n            {\n                lo++;   // too low, so must increase the LO point\n            }\n            else    // must be &gt; N\n            {\n                hi--;   // too high, so must decrease the HI point\n            }\n        }\n        return results;\n    }\n</code></pre>\n\n<p>As I said before, it should be pretty close to O(N), however, as Yuval Filmus points out, as the number of Four Square solutions to N can be of order (N ln ln N), then this algorithim could not be less than that.</p>\n', 'Tags': '<algorithms><complexity-theory><performance><number-theory>', 'LastEditorUserId': '2334', 'LastActivityDate': '2012-08-02T16:54:45.280', 'CommentCount': '5', 'AcceptedAnswerId': '3003', 'CreationDate': '2012-08-01T20:29:11.273', 'Id': '2988'},1913:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>How can we assert that the performance of an implementation matches the expected performance defined in terms of Big-$\\mathcal{O}$ and friends by generating best/worst case inputs? I'm thinking of a table whose columns are <strong>input length</strong>, <strong>expected running time</strong>, <strong>actual running time</strong>. Are there general methods for generating inputs for evaluation?</p>\n", 'ViewCount': '30', 'Title': 'How to generate inputs to evaluate the performance of an implementation?', 'LastActivityDate': '2012-09-15T18:08:12.130', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2499', 'Tags': '<performance><benchmarking>', 'CreationDate': '2012-09-15T18:08:12.130', 'FavoriteCount': '2', 'Id': '3570'},1914:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Parallel processes interfere with each other in many ways, by competing for shared resources such as shared caches, memory, disks, etc.</p>\n\n<p>Would it be possible to determine latent factors just with a runtime analysis, by means of comparing the runtime of the respective applications while running them many times in different combinations? Or would the required sample size be too big for the method to be feasible?</p>\n\n<p>If comparing only the runtimes would not result in significant results, which variable would be best suited to observe? L1-cache-misses, L2-cache-misses, page faults, total memory usage, tlb-misses, etc.</p>\n', 'ViewCount': '48', 'Title': 'Determine interference factors in parallel computing', 'LastActivityDate': '2012-10-17T10:27:52.867', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4230', 'Tags': '<parallel-computing><cpu-cache><performance>', 'CreationDate': '2012-10-17T10:27:52.867', 'Id': '6120'},1915:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have shown my work below for the problem and would appreciate if someone can let me know if I'm on the right track or point me in the right direction if not. </p>\n\n<p>An 8 processor file server handles a million operations each day. If each processor remains idle 5% of the time what is the utilization of the entire file server? </p>\n\n<p>I believe the utilization of the entire file server is 95% because it says all processors at idle 5% of the time. Is this correct?</p>\n\n<p>If the file server is fully utilized how much more work could it do? </p>\n\n<p>We know the file server does 10^6 operations each day and we know each processor is utilized 95% of the time.  Thus 10^6 / .95 = 1,052,632 operations / day would be the total number of operation if it were fully utilized.  </p>\n\n<p>The last question has me a little stumped: If 10% of the operations are used to move files between processors, how efficient is the file server? </p>\n", 'ViewCount': '118', 'Title': 'How to determine the Utilization and Efficiency of a file server?', 'LastEditorUserId': '41', 'LastActivityDate': '2014-04-24T23:19:09.250', 'LastEditDate': '2013-01-21T09:31:32.920', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '1480', 'Tags': '<computer-architecture><databases><performance>', 'CreationDate': '2012-10-20T22:14:29.313', 'Id': '6201'},1916:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '356', 'Title': 'Finding maximum and minimum of consecutive XOR values', 'LastEditDate': '2012-12-28T21:15:49.023', 'AnswerCount': '3', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5218', 'FavoriteCount': '1', 'Body': '<p>Given an integer array (maximum size 50000), I have to find the minimum and maximum $X$ such that $X = a_p \\oplus a_{p+1} \\oplus \\dots \\oplus a_q$ for some $p$, $q$ with $p \\leq q$.</p>\n\n<p>I have tried this process: $\\text{sum}_i = a_0 \\oplus a_1 \\oplus  \\dots \\oplus a_i$ for all $i$. I pre-calculated it in $O(n)$ and then the value of $X$ for some $p$, $q$ such that $(p\\leq q)$ is: $X = \\text{sum}_q \\oplus \\text{sum}_{p-1}$. Thus:</p>\n\n<p>$$\n\\mathrm{MinAns} = \\min_{(p,q) \\text{ s.t. } p\\le q}{\\text{sum}_q \\oplus \\text{sum}_{p-1}} \\\\\n\\mathrm{MaxAns} = \\max_{(p,q) \\text{ s.t. } p\\le q}{\\text{sum}_q \\oplus \\text{sum}_{p-1}} \\\\\n$$</p>\n\n<p>But this process is of $O(n^2)$. How can I do that more efficiently?</p>\n', 'Tags': '<algorithms><algorithm-analysis><performance><binary-arithmetic><arithmetic>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-29T05:03:58.663', 'CommentCount': '1', 'AcceptedAnswerId': '7640', 'CreationDate': '2012-12-28T03:12:04.687', 'Id': '7622'},1917:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a system in existence which works on parent-child model.</p>\n\n<ol>\n<li>They share common attributes, as they belong to same class.</li>\n<li>A parent can enforces certain data on any user selected attribute, and the data is required to be propagated down the hierarchy for the same attribute in all children and their children.</li>\n<li>For a given attribute, if a child's parent dies (object gets destroyed) or child has not been enforced by parent, child can behave the same way, like in point #2, (on user request) for its children.</li>\n</ol>\n\n<p>My system is pretty big (in database). For example:</p>\n\n<ul>\n<li>Each object of the class has 150 atributes.</li>\n<li>There are in total about 8 million attributes for objects created. Hence, the objects are close to 53,333.</li>\n<li>There is a single parent for the objects. Its a big tree.</li>\n</ul>\n\n<p>If I want to enforce data on the top most parent for a single attribute, it takes about 8 mins for it to complete.</p>\n\n<p>Does anyone know a model which can perform faster than hierarchical model, but similar to behaviour?</p>\n", 'ViewCount': '47', 'Title': 'Optimization of Hierarchical Data Propagation', 'LastEditorUserId': '5402', 'LastActivityDate': '2013-08-05T14:39:58.180', 'LastEditDate': '2013-08-05T11:48:31.827', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5402', 'Tags': '<algorithms><databases><performance>', 'CreationDate': '2013-01-13T11:08:33.473', 'Id': '7922'},1918:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume there was a database system that had a data type called <code>VARINT</code> or some variant that allowed instead of fixed-length <code>INT</code>s regardless of value, a <code>1</code> would only take 1 <code>BIT</code> (<code>1</code>), <code>2</code> would take 2 <code>BIT</code>s (<code>10</code>), etc.</p>\n\n<p>In this perfect world, a <code>VARINT</code> could be used to <code>autoincrement</code> a <code>PRIMARY</code> column.  However, there\'s still the issue of the field growing in size, so the "older" parts of this <code>TABLE</code> would be very fast, but the "younger" parts would get slower and slower.</p>\n\n<p>What counting system could be used to hold the space consumed constant and small or at least grow at a much slower rate?</p>\n', 'ViewCount': '19', 'Title': 'VAR autoincrement with constant space consumption for super large tables', 'LastActivityDate': '2013-04-05T20:34:02.580', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '11064', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7386', 'Tags': '<integers><database-theory><performance>', 'CreationDate': '2013-03-23T21:49:04.237', 'Id': '10724'},1919:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have developed two existing data structures and I want to see their performances over a certain algorithm. In this case I use Dijkstra's algorithm with binary and Fibonacci heaps. Just to ask, if I have 100 to 1000 number of vertices in the tested sparse digraphs, how many times should I execute my program for a single n vertices? How do I know that the empirical differences in performance that I've obtained between data structures are not due to chance?</p>\n", 'ViewCount': '69', 'Title': 'performance between the data structures', 'LastEditorUserId': '4736', 'LastActivityDate': '2013-04-29T09:50:43.620', 'LastEditDate': '2013-04-29T09:50:43.620', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7956', 'Tags': '<data-structures><priority-queues><performance><empirical-research>', 'CreationDate': '2013-04-29T04:33:05.927', 'Id': '11651'},19110:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There seems to be a sense of competition in creating the fastest computer in the world, and I feel like technically you just have to continue increasing the core number to gain more flops, which is exactly what some countries are doing, so how would you really bench mark the technological level of a supercomputer</p>\n\n<p>for example </p>\n\n<p>United States currently have two supercomputer on the top list with similar performance, however one has significantly less core than the other</p>\n\n<p>Titan     has 560000 cores    17590 Rmax Tflops   with   8000 kW of power</p>\n\n<p>Sequoia   has 1570000 cores   17173 Rmax Tflops   with   7800 kW of power</p>\n\n<p>Does this mean that Titan is a much superior supercomputer than Sequoia as it only uses 1/3 of cores to reach a similar performance, and if it does, this will also imply that judging supercomputer by flops is completely meaningless?</p>\n\n<p>Is there something I am missing here? Such as the increasing complexity of software as the amount of core gets larger and the synchronization of overhead to assign task to each core gets more sophisticated?</p>\n\n<p>Any info would be much appreciated </p>\n', 'ViewCount': '99', 'Title': 'Regarding to the speed of supercomputer', 'LastActivityDate': '2013-06-23T19:50:14.173', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12751', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8738', 'Tags': '<performance>', 'CreationDate': '2013-06-18T18:23:36.560', 'Id': '12745'},19111:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have about 500,000,000 64-bit integers, so these numbers could be very large.\nI want to sort them as quickly as possible. I have a couple of questions:</p>\n\n<ol>\n<li><p>What data structure do you suggest for storing this data?</p></li>\n<li><p>What algorithm do you suggest for sorting these numbers?</p></li>\n</ol>\n\n<p>My main restriction is speed.</p>\n', 'ViewCount': '224', 'Title': 'How should I store and sort a large number of 64-bit integers?', 'LastEditorUserId': '1055', 'LastActivityDate': '2013-07-17T20:07:11.840', 'LastEditDate': '2013-07-17T05:13:23.943', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '13291', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1055', 'Tags': '<algorithms><data-structures><sorting><integers><performance>', 'CreationDate': '2013-07-15T15:49:46.370', 'Id': '13290'},19112:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '169', 'Title': "Is the memory-runtime tradeoff an equivalent of Heisenberg's uncertainty principle?", 'LastEditDate': '2013-08-08T10:40:15.420', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9571', 'FavoriteCount': '3', 'Body': '<p>When I work on an algorithm to solve a computing problem, I often experience that speed can be increased by using more memory, and memory usage can be decreased at the price of increased running time, but I can never force the product of running time and consumed memory below a clearly palpable limit. This is formally similar to Heisenberg\'s uncertainty principle: the product of the uncertainty in position and the uncertainty in momentum of a particle cannot be less than a given threshold.</p>\n\n<p>Is there a theorem of computer science, which asserts the same thing? I guess it should be possible to derive something similar from the theory of Turing Machines.</p>\n\n<p>(I asked this question originally on <a href="http://stackoverflow.com/questions/18108578/the-uncertainty-principle-of-computer-science">StackOverflow</a>.)</p>\n', 'Tags': '<algorithms><time-complexity><space-complexity><performance>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-08T10:40:15.420', 'CommentCount': '3', 'AcceptedAnswerId': '13664', 'CreationDate': '2013-08-07T16:45:48.973', 'Id': '13661'},19113:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If you have two computers with only one thread, a and b.</p>\n\n<p>And computer a runs twice as fast as computer b.</p>\n\n<p>Does this mean that computer a and can do twice as many operations in a given time in comparison to computer b? </p>\n\n<p>To increase the Hertz of a computer, i.e. the speed of the computer and the number of operations it can achieve with in a given time frame, what has to change in the architecture?</p>\n', 'ViewCount': '111', 'Title': "What does it mean when a computer runs 'faster'?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-26T07:27:01.047', 'LastEditDate': '2013-08-26T07:27:01.047', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '13899', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9799', 'Tags': '<terminology><performance>', 'CreationDate': '2013-08-23T18:14:45.107', 'Id': '13894'},19114:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am a newbie here I recently read tutorials on java about collection framework. <code>The time complexity of various objects in reading/writing data</code> What does it mean? what is <code>O</code> notations? can anyone make me understand these notations and about time complexity measures using these notations</p>\n', 'ViewCount': '25', 'ClosedDate': '2013-09-05T11:08:25.157', 'Title': 'What are these notations O(1)/O(n)/O(log n)?', 'LastEditorUserId': '9990', 'LastActivityDate': '2013-09-05T10:58:18.283', 'LastEditDate': '2013-09-05T10:58:18.283', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9990', 'Tags': '<performance><notation>', 'CreationDate': '2013-09-05T10:49:57.843', 'Id': '14144'},19115:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Scott Meyers describes <a href="http://www.aristeia.com/TalkNotes/ACCU2011_CPUCaches.pdf" rel="nofollow">here</a> that traversing a symmetric matrix row-wise performes significantly better over traversing it column-wise - which is also significantly counter-intuitive. </p>\n\n<p>The reasoning is connected with how the CPU-caches are utilized. But I do not really understand the explanation and I would like to get it because I think it is relevant to me.</p>\n\n<p>Is it possible to put it in more simple terms for somebody not holding a PhD in computer architecture and lacking experience in hardware-level programming?</p>\n', 'ViewCount': '183', 'Title': 'Performance of row- vs. column-wise matrix traversal', 'LastActivityDate': '2013-10-05T14:33:16.390', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14829', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9994', 'Tags': '<cpu-cache><performance>', 'CreationDate': '2013-10-05T11:20:40.243', 'Id': '14826'},19116:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to flatten a binary tree into a linear array, and I wonder if there are specific algorithms to improve locality in the linearized representation (for instance, ensuring that all data from the left child of the root node appears before the data from the right child is an easy optimization.) What is the recommended approach to maintain high locality?</p>\n', 'ViewCount': '78', 'Title': 'Tree flattening with layout guarantees', 'LastActivityDate': '2013-10-16T14:15:15.257', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '16136', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10792', 'Tags': '<binary-trees><performance>', 'CreationDate': '2013-10-16T12:44:44.580', 'Id': '16134'},19117:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm writing a paper on the topic of applications affected more by memory performance than processor performance. I've got a lot written regarding the gap between the two, however I can't seem to find anything about the applications that might benefit more from memory performance.</p>\n\n<p>I suppose these are applications that make a large amount of memory references, but I have no idea what kind of applications would make such large number of references to make it stand out?</p>\n\n<p>Can you please give me any pointers on how to proceed, some links?</p>\n", 'ViewCount': '27', 'Title': 'Applications affected by memory performance', 'LastActivityDate': '2013-10-19T14:48:07.750', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10855', 'Tags': '<performance><memory-access>', 'CreationDate': '2013-10-19T14:48:07.750', 'Id': '16230'},19118:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know when we access elements  in rows it will be much faster than if it is accessed column wise. In matrix multiplication one of the matrices must be accessed column wise. \nIn GPUs with CUDA/OpenCL I could resolve this issue by explicitly copying both matrices row-wise into user-managed shared(cache)-memory (or local memory in OpenCL), and then can access data for matrix multiplication in anyway- no penalty for column access.</p>\n\n<p>My question is how do I avoid such column access if I am implementing Matrix-multiplication on CPU, where I do not have any access to such shared memory as in GPU?        </p>\n', 'ViewCount': '84', 'Title': 'How to avoid column wise access in matrix multiplication?', 'LastActivityDate': '2013-11-17T01:45:10.273', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><cpu-cache><performance>', 'CreationDate': '2013-11-16T13:01:29.680', 'FavoriteCount': '1', 'Id': '18072'},19119:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I do not understand the following options in windows. Could any one explain\n<img src="http://i.stack.imgur.com/S011N.png" alt="enter image description here"></p>\n', 'ViewCount': '42', 'ClosedDate': '2013-12-24T00:50:57.443', 'Title': 'Task Manager Performance tab in windows', 'LastActivityDate': '2013-12-07T11:48:47.570', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '18713', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9990', 'Tags': '<operating-systems><performance>', 'CreationDate': '2013-12-07T09:02:13.400', 'Id': '18709'},19120:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I want to create a fast way to detect whether a file might or might not be the same. For almost 100% sureness I would use an existing hash algorithm, e.g. SHA256.\nHowever, the files are expected to be huge video files with several GB, so calculating the SHA256 hash could take some time, especially over the network.</p>\n\n<p>Therefore I want to combine different other techniques:</p>\n\n<ul>\n<li>file size: if the file size has changed, the content has changed (sure)</li>\n<li>head / tail hash</li>\n<li>random hash</li>\n</ul>\n\n<p>The latter 2 are part of my question:</p>\n\n<p>My guess would be that in the header there are things like:</p>\n\n<ul>\n<li>frame rates (e.g. Videos)</li>\n<li>resolution (e.g. Videos, Images)</li>\n<li>(file) length (e.g. in frames, pixels etc.)</li>\n<li>last change date (e.g. Word documents, not specifically Videos)</li>\n</ul>\n\n<p>Why I consider checking the tail is:</p>\n\n<ul>\n<li>MP3 has the tag information there</li>\n<li>EXIF adds custom data at the end if I'm right</li>\n</ul>\n\n<p>Random hashes would select e.g. 126 regions at random positions in the file with a specific length, e.g. 64 kB and create a hash for them. Of course I remember the offsets for later comparison.\nAll in all I would use (1+126+1)*64 kB of data for my hash, so I need to read only 8 MB instead of several GB to get the hash.</p>\n\n<p>Maybe it's more a Math question now, but: <strong>how likely is it to detect a change using the combination of file size, head, tail and random data to generate this quick hash sum?</strong></p>\n\n<p>I assume that the files are always legal files. There's no benefit in manipulating single bytes. The user would use a normal video editing tool to change the files.</p>\n\n<p><strong>UPDATE</strong>:\nI unaccepted this answer which came from Crypto.StackExchange. I agree that my proposal is not cryptographic and not intended to be secure. I also agree that CRCing a file is fast, but in my case I really need a hash - I'll explain why:</p>\n\n<ul>\n<li>My application is expected to save bookmarks in videos. My database is expected to save the video hash and the bookmarks.</li>\n<li>Users sometimes move or rename files. My program will notice that a file does no longer exist, but will not delete the bookmarks from the database. Instead, when the same video is (accidentally) played again, I want to recognize that it's (probably) the same file.</li>\n<li>Users are expected to save files on network drives (NAS) and stream videos. Those are dumb storages. I cannot install a server component. And they might be quite slow, so I really don't want the full hash. Calculating a full hash on a 3 GB file takes at least 5 minutes @ 10 MB/s, no matter how fast the hashing algorithm is.</li>\n<li>If the user has edited the file, I somehow hope that the hash won't match any more, because otherwise I would display wrong bookmarks.</li>\n</ul>\n\n<p>I'd be ok with a <strong>~80% chance</strong> of having the correct bookmarks. How many hash pieces should I put together and where in the file would that be?</p>\n", 'ViewCount': '176', 'Title': 'Fast hashing: combination of different techniques to identify changes in a file?', 'LastEditorUserId': '12100', 'LastActivityDate': '2013-12-20T23:50:06.913', 'LastEditDate': '2013-12-20T23:50:06.913', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '7', 'OwnerDisplayName': 'Thomas W.', 'PostTypeId': '1', 'OwnerUserId': '12100', 'Tags': '<hash><performance>', 'CreationDate': '2013-12-13T14:22:32.663', 'Id': '19042'},19121:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I recently had a lengthy exchange with someone about the most efficient way to remove duplicates from a collection.  The debate was mostly centered around the specific behavior of C# collections, such as HashSet(T) and HashTable(T).</p>\n\n<p>I think we agreed on the fundamentals, but where we couldn\u2019t come to an agreement was what the constraints are with very large sets.  </p>\n\n<p>He told me to investigate the theoretical (ie absolute) bounds for duplicate detection in a set of objects\u2026 But I don\u2019t really know what that means.  I don\u2019t think there\u2019s an absolute bounds if you can break up the problem sufficiently.  </p>\n\n<p>There may be a point at which you can no longer use a HashSet / HashTable in .NET because I know that if you are using the framework au-naturale you are constrained by the number of unique values an Int32 can express and the amount of memory you have available.</p>\n\n<p>But the issue of storage and memory issue comes much sooner than the number of unique values, which is typically what the theoretical is concerned with... For example if the data type is integers for the initial set you're removing duplicates from, you run out of memory before the numeric range of the integer type becomes a problem:</p>\n\n<ul>\n<li>If I am storing integers, I know that there are 2^32 possible integer values.</li>\n<li>To store an integer, I need 32 bits of space, so the total memory required to store all distinct integers is 2^32*4 bytes, which is 17.18 gigabytes.</li>\n<li>The amount of memory that can be addressed in a 32 bit architecture is 2^32 bytes or approximately 4.295 gigabytes</li>\n</ul>\n\n<p>Even if I\u2019m not using a hashset, I can see that in order to store that many values, I would need at least four times the amount of memory addressable by the architecture for the initial collection.  And that's not even factoring in the duplicates we're seeking to remove.</p>\n\n<p>I would also need memory proportional to the size of the initial set for the hash set implementation and hash set value storage\u2026 So the use of a hashset quickly becomes unfeasible when you exceed millions of unique values.</p>\n\n<p>I\u2019ve already asserted to him that if you had a large digit such as a long, and you had billions of values, you would not use a hashset.  Hypothetically I might store the data in distributed nodes that are pre-sorted by the unique field, or implement a distributed mapreduce sort implementation, followed by a mapreduce duplicate value removal algorithm.</p>\n\n<p>He didn\u2019t acknowledge the feasibility of the sort/remove duplicates approach though\u2026 And was quite insistent that there is an upper bound for duplicate detection.</p>\n\n<p>Could anyone tell me what the \u2018absolute\u2019 bounds for duplicate detection for a set of objects is?  Or what he was was referring to by it?</p>\n", 'ViewCount': '47', 'Title': 'The theoretical upper bounds for duplicate detection in a set of objects?', 'LastActivityDate': '2013-12-21T04:50:15.577', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19172', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12253', 'Tags': '<hash-tables><performance>', 'CreationDate': '2013-12-21T01:22:35.833', 'Id': '19170'},19122:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's have an array where first half are of value 50 and the second half 100. What would be the asymptotic performance when sorting using Quicksort.</p>\n\n<p>I think it it should be $O(n^2)$ as for array of same elements the complexity is $O(n^2)$ and this particular problem could be rewritten as sorting the first half + sorting the second hald $O(2*(\\frac{n}{2})^2 + n)$ which is still $O(n^2)$.</p>\n\n<p>But my schoolmates claim it should be $O(n log(n))$.. so which one is correct?</p>\n", 'ViewCount': '87', 'LastEditorDisplayName': 'user12779', 'Title': "Quicksort's asymptotic performance for array of [50,...,50,100,...100]", 'LastActivityDate': '2014-01-29T21:37:14.940', 'LastEditDate': '2014-01-12T18:30:54.990', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'user12779', 'PostTypeId': '1', 'Tags': '<algorithms><asymptotics><performance><quicksort>', 'CreationDate': '2014-01-12T18:18:39.200', 'Id': '19675'},19123:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I wonder if high-performance computing and parallel computing always mean the same? \nIf not, </p>\n\n<ul>\n<li>what are the differences and relations between  them?</li>\n<li>what are some examples of high-performance computing that are not parallel computing?</li>\n<li>what are some examples of parallel computing that are not high-performance computing?</li>\n</ul>\n\n<p>Are high-throughput computing and high-performance computing the same concept?</p>\n', 'ViewCount': '87', 'Title': 'Differences and relations between high-performance/throughput computing and parallel computing?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-03T21:26:08.297', 'LastEditDate': '2014-02-03T16:15:43.477', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<terminology><parallel-computing><high-performance>', 'CreationDate': '2014-02-03T15:02:47.050', 'Id': '20253'},19124:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>How do people measure performance overhead? Whenever someone is bragging about how their program or application performs better than another, they talk about particular measurements, eg time, performance etc. </p>\n\n<p>Whilst I understand how time to perform a task can be calculated, I cannot understand how performance is measured, and I can't find out thought Google.</p>\n\n<p>Are there multiple ways? Are they all accepted or do people have issues with certain methods?</p>\n", 'ViewCount': '51', 'Title': 'How do people measure performance overhead?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-09T23:26:53.140', 'LastEditDate': '2014-04-09T21:43:47.333', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '23614', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16580', 'Tags': '<algorithms><terminology><performance>', 'CreationDate': '2014-04-09T11:25:17.617', 'Id': '23591'},19125:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What is the definition of a "c-competitive" algorithm? For example what does it mean, if we say that there is a 2-competitive algorithm for packet routing?</p>\n', 'ViewCount': '17', 'Title': u'Definition of \u201cc-competitive\u201d algorithm', 'LastActivityDate': '2014-05-03T20:33:48.200', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24365', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2748', 'Tags': '<algorithms><performance>', 'CreationDate': '2014-05-03T19:38:26.040', 'Id': '24364'}