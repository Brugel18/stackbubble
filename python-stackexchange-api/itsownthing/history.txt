{'ViewCount': '2298', 'Title': 'Why has research on genetic algorithms slowed?', 'LastEditDate': '2012-04-06T23:24:25.237', 'AnswerCount': '5', 'Score': '28', 'PostTypeId': '1', 'OwnerUserId': '258', 'FavoriteCount': '6', 'Body': '<p>While discussing some intro level topics today, including the use of genetic algorithms; I was told that research has really slowed in this field. The reason given was that most people are focusing on machine learning and data mining. <br>\n<strong>Update:</strong> Is this accurate? And if so, what advantages does ML/DM have when compared with GA?</p>\n', 'Tags': '<machine-learning><data-mining><evolutionary-computing><history>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-06T23:24:25.237', 'CommentCount': '2', 'AcceptedAnswerId': '565', 'CreationDate': '2012-03-21T01:17:25.527', 'Id': '561'}{'Body': '<p>It seems as though modern speech recognition (e.g., through Android, iOS phones) make use of grammar or sentence structure. (e.g., it might have a tough time distinguishing between "grammar" and "grandma" but can distinguish between "I\'m going to see grandma" and "I\'m reading a book on english grammar". (yes, I just tried it with my Android phone with vLingo app)</p>\n\n<p>That is much improved (with Speaker Independent SR (i.e., no training)) over what I experienced with Dragon Dictate even using Speaker Dependent SR (with 30m of training).</p>\n\n<p>So, I\'m wondering whether my guess is right: When did the commercially available SR software start using grammar and sentence structure to "guess" the right speech?</p>\n', 'ViewCount': '98', 'Title': 'When did commercial Speech Recognition first begin using grammar (sentence structure) for prediction?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-06T23:23:33.617', 'LastEditDate': '2012-04-06T23:23:33.617', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '972', 'Tags': '<natural-lang-processing><history>', 'CreationDate': '2012-04-06T15:51:14.253', 'Id': '1079'}{'ViewCount': '546', 'Title': "Why is a regular language called 'regular'?", 'LastEditDate': '2012-05-14T15:15:21.780', 'AnswerCount': '2', 'Score': '20', 'PostTypeId': '1', 'OwnerUserId': '1434', 'FavoriteCount': '3', 'Body': '<p>I have just completed the first chapter of the <a href="http://www-math.mit.edu/~sipser/book.html"><em>Introduction to the Theory of Computation</em></a> by <em>Michael Sipser</em> which explains the basics of finite automata. </p>\n\n<p>He defines a regular language as anything that can be described by a finite automata. But I could not find where he explains why a regular language is called "regular?"  What is the origin of the term "regular" in this context?</p>\n\n<p>NOTE: I am a novice so please try to explain in simple terms!</p>\n', 'Tags': '<formal-languages><regular-languages><terminology><finite-automata><history>', 'LastEditorUserId': '1298', 'LastActivityDate': '2012-05-14T15:15:21.780', 'CommentCount': '1', 'AcceptedAnswerId': '1772', 'CreationDate': '2012-05-10T02:07:18.947', 'Id': '1771'}{'Body': '<p>There are lots of attempts at proving either $\\mathsf{P} = \\mathsf{NP} $ or $\\mathsf{P} \\neq \\mathsf{NP}$, and naturally many people think about the question, having ideas for proving either direction.</p>\n\n<p>I know that there are approaches that have been proven to not work, and there are probably more that have a history of failing. There also seem to be so-called <em>barriers</em> that many proof attemps fail to overcome. </p>\n\n<p>We want to avoid investigating into dead-ends, so what are they?</p>\n', 'ViewCount': '6113', 'Title': 'How not to solve P=NP?', 'LastEditorUserId': '6716', 'LastActivityDate': '2014-03-26T03:42:53.937', 'LastEditDate': '2013-06-06T14:08:25.930', 'AnswerCount': '5', 'CommentCount': '5', 'Score': '41', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<complexity-theory><reference-request><history><p-vs-np><reference-question>', 'CreationDate': '2012-05-17T01:24:29.327', 'FavoriteCount': '24', 'Id': '1877'}{'ViewCount': '300', 'Title': 'Why is Turing completeness right?', 'LastEditDate': '2012-06-23T15:17:09.140', 'AnswerCount': '2', 'Score': '9', 'OwnerDisplayName': 'MathematicalOrchid', 'PostTypeId': '1', 'OwnerUserId': '1951', 'FavoriteCount': '1', 'Body': '<p>I am using a digital computer to write this message. Such a machine has a property which, if you think about it, is actually quite remarkable: It is <em>one machine</em> which, if programmed appropriately, can perform <em>any possible computation</em>.</p>\n\n<p>Of course, calculating machines of one kind or another go back to antiquity. People have built machines which for performing addition and subtraction (e.g., an abacus), multiplication and division (e.g., the slide rule), and more domain-specific machines such as calculators for the positions of the planets.</p>\n\n<p>The striking thing about a computer is that it can perform <em>any</em> computation. Any computation at all. And all without having to rewire the machine. Today everybody takes this idea for granted, but if you stop and think about it, it\'s kind of amazing that such a device is possible.</p>\n\n<p>I have two actual <em>questions</em>:</p>\n\n<ol>\n<li><p>When did mankind figure out that such a machine was possible? Has there ever been any serious <em>doubt</em> about whether it can be done? When was this settled? (In particular, was it settled before or after the first actual implementation?)</p></li>\n<li><p>How did mathematicians <em>prove</em> that a Turing-complete machine really can compute everything?</p></li>\n</ol>\n\n<p>That second one is fiddly. Every formalism seems to have some things that <em>cannot</em> be computed. Currently "computable function" is <em>defined as</em> "anything a Turing-machine can compute". But how do we know there isn\'t some slightly more powerful machine that can compute more stuff? How do we know that Turing-machines are the correct abstraction?</p>\n', 'Tags': '<computability><turing-machines><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-23T22:50:06.447', 'CommentCount': '7', 'AcceptedAnswerId': '2465', 'CreationDate': '2012-06-23T13:58:23.750', 'Id': '2462'}{'Body': '<p>I have asked this question at <a href="http://math.stackexchange.com/questions/167462/historical-relation-between-computer-science-and-the-theory-of-dynamical-systems">math.stackexchange</a>, but I have been redirected here.</p>\n\n<p>My question is, if there is any historical relation between the fields of Dynamical systems (and related fields such as Optimal control) and (theoretical) Computer science. The reason for which I ask this question is, that several older major journals, now considered to be primarily focused on theoretical computer science, were originally devoted both to computer science and to dynamical systems.</p>\n\n<p>The example of such journals can be for instance Journal of Computer and System Sciences. Mainly in its beginnings (however, I am too young to remember these times), it published papers both on computer science and on system sciences, and both of the fields seemed to be of equal importance. Moreover, also the name of the journal Information and Control suggests the relation to optimal control. However, this journal was always focused mainly on theoretical computer science and information theory.</p>\n\n<p>So therefore I wonder, if these fields (nowadays, up to my knowledge, considered to be substantially different) have something in common or if there is some purpose to consider these fields to be related. </p>\n', 'ViewCount': '142', 'Title': 'Historical relation between computer science and the theory of dynamical systems', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-05T19:29:20.517', 'LastEditDate': '2012-07-09T08:47:01.003', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2091', 'Tags': '<history>', 'CreationDate': '2012-07-06T15:27:32.753', 'Id': '2637'}{'ViewCount': '256', 'Title': 'Origins of the term "distributed hash table"', 'LastEditDate': '2012-07-23T09:49:38.700', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1837', 'FavoriteCount': '1', 'Body': "<p>I am currently researching for my diploma thesis in computer science with a topic in the area of distributed hash tables. Naturally, I came to the question were the term <em>distributed hash table</em> came from. (I know it is not rocket science to just derive it from <em>distributing a hash table</em>, but somebody somewhere must have come up with it).</p>\n\n<p>Most papers I read referred to the original paper on <em>consistent hashing</em> and one of the first algorithms making use of it (e.g Chord). I know that there was a lot of research on distributed databases in the 80s, so I figure that the term, or maybe the idea behind it, should be older than ~15 years.</p>\n\n<p>The motivation behind this question is that knowing an earlier date and maybe another term for a similar idea would possibly widen the range of useful information I could gather for my research. For example, what have others done that is similar to what I want to do and where have they failed. Etc. etc.</p>\n\n<p>I tried to find more on this subject using <em>Structured Overlay Networks</em> as a search keyword, but the resulting definitions/papers are also quite young, which leaves me with the impression that the research topic might not be so old after all.</p>\n\n<p>Does anybody of you know of earlier research (maybe pre-90s?) in the topics of distributed hash tables and/or structured overlay networks? I'd be glad to hear some keywords which could lead me to more historic papers.</p>\n", 'Tags': '<data-structures><terminology><distributed-systems><hash-tables><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-21T07:15:27.943', 'CommentCount': '6', 'AcceptedAnswerId': '8961', 'CreationDate': '2012-07-23T09:43:04.950', 'Id': '2872'}{'Body': "<p>I'm trying to trace back the origin of the general concept of Discrete Event Simulation and found a 1968 article by Fishman and Kiviat mentioning the term. It is titled <em>The statistics of discrete-event simulation</em>.</p>\n\n<p>However, I'm unable to verify this as the origin of the term.</p>\n\n<p>Is there further historical information on this?</p>\n", 'ViewCount': '81', 'Title': 'Who conceived the concept of Discrete Event Simulation, and when?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-24T19:39:30.363', 'LastEditDate': '2012-10-24T15:09:26.083', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '26', 'Tags': '<reference-request><simulation><history>', 'CreationDate': '2012-10-24T11:21:38.397', 'Id': '6285'}{'ViewCount': '259', 'Title': 'Don Knuth and MMIXAL vs. Chuck Moore and Forth -- Algorithms and Ideal Machines -- was there any cross-pollination / influence in their ideas / work?', 'LastEditDate': '2012-11-12T20:00:39.820', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'AKE', 'PostTypeId': '1', 'OwnerUserId': '4569', 'Body': '<p><strong>Question:</strong></p>\n\n<p><strong>To what extent is it known (or believed) that Chuck Moore and Don Knuth had influence on each other\'s thoughts on ideal machines, or their work on algorithms?</strong></p>\n\n<p>I\'m interested in citations, interviews, articles, links, or any other sort of evidence.  It could also be evidence of the form of A and B here suggest that Moore might have borrowed or influenced C and D from Knuth here, or vice versa.  (Opinions are of course welcome, but references / links would be better!)</p>\n\n<p><strong>Context:</strong></p>\n\n<p>Until fairly recently, I have been primarily familiar with Knuth\'s work on algorithms and computing models, mostly through TAOCP but also through his interviews and other writings.</p>\n\n<p>However, the more I have been using Forth, the more I am struck by both the power of a stack-based machine model, and the way in which the spareness of the model makes fundamental algorithmic improvements more readily apparent.  </p>\n\n<p>A lot of what Knuth has done in fundamental analysis of algorithms has, it seems to me, a very similar flavour, and I can easily imagine that in a parallel universe, Knuth might perhaps have chosen Forth as his computing model.</p>\n\n<p>That\'s the software / algorithms / programming side of things.</p>\n\n<p>When it comes to "ideal computing machines", Knuth in the 70s came up with the MIX computer model, and then, collaborating with designers of state-of-the-art RISC chips through the 90s, updated this with the modern MMIX model and its attendant assembly language MMIXAL.</p>\n\n<p>Meanwhile, Moore, having been using and refining Forth as a language, but using it on top of whatever processor happened to be in the computer he was programming, began to imagine a world in which the efficiency and value of stack-based programming were reflected in hardware.  So he went on in the 80s to develop his own stack-based hardware chips, defining the term MISC (Minimal Instruction Set Computers) along the way, and ending up eventually with the first Forth chip, the MuP21.</p>\n\n<p>Both are brilliant men with keen insight into the art of programming and algorithms, and both work at the intersection between algorithms, programs, and bare metal hardware (i.e. hardware without the clutter of operating systems).</p>\n\n<p>Which leads to the question as headlined...</p>\n\n<p><strong>Question: To what extent is it known (or believed) that Chuck Moore and Don Knuth had influence on each other\'s thoughts on ideal machines, or their work on algorithms?</strong></p>\n', 'Tags': '<programming-languages><history>', 'LastEditorUserId': '31', 'LastActivityDate': '2013-02-13T08:25:58.027', 'CommentCount': '4', 'AcceptedAnswerId': '6670', 'CreationDate': '2012-11-12T19:48:16.480', 'Id': '6640'}{'ViewCount': '83', 'Title': 'Who (and when) first defined interval graphs?', 'LastEditDate': '2012-11-21T14:59:55.280', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '26', 'FavoriteCount': '1', 'Body': '<p>I\'ve been searching google scholar for references and narrowed down the first mention to somewhere around <a href="http://books.google.com/ngrams/graph?content=interval%20graph&amp;year_start=1800&amp;year_end=2000&amp;corpus=15&amp;smoothing=0&amp;share=">1963</a> with a very weird jitter in 1949.</p>\n\n<p>So, I\'m trying to track down the original paper introducing interval graphs for citation, but it\'s been rather elusive so far.</p>\n', 'Tags': '<graph-theory><reference-request><history>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-21T17:36:52.917', 'CommentCount': '0', 'AcceptedAnswerId': '6816', 'CreationDate': '2012-11-21T13:26:08.517', 'Id': '6812'}{'Body': '<p>I was wondering if someone knew the origin of the client server model. Where does the term come from (paper, software application, book)?</p>\n', 'ViewCount': '349', 'Title': 'What is the origin of the client server model?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-04T23:06:54.407', 'LastEditDate': '2012-12-02T11:34:11.250', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '7060', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '4809', 'Tags': '<terminology><reference-request><distributed-systems><history>', 'CreationDate': '2012-11-30T10:05:42.000', 'Id': '7038'}{'Body': '<p>Who was/were the first person/people to introduce the topic of quantum complexity theory and problem classes like BQP and QMA?</p>\n', 'ViewCount': '93', 'Title': 'Origin of quantum complexity theory', 'LastActivityDate': '2012-12-10T17:22:34.170', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7306', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12230', 'Tags': '<complexity-theory><quantum-computing><history>', 'CreationDate': '2012-12-10T07:48:42.793', 'FavoriteCount': '1', 'Id': '7293'}{'Body': "<p>I am seeking for some popular science videos on computer algorithms for my last lecture delivered in this term. The topic of the videos includes but are limited to algorithm design, algorithm analysis, computational complexity, the application of algorithms, the history of algorithms (even computer science), the story about the people working on the field, and so on.</p>\n\n<p>By ``popular science videos'', I am just ruling out the videos which are focusing on some specific theory and targeted at a special interest group. It will be better to meet the following specifications (not mandatory):</p>\n\n<ul>\n<li>produced by known university</li>\n<li>produced by known corporation, like BBC</li>\n<li>film or record, not just a speech given by one person </li>\n<li>lasts for more than half an hour</li>\n<li>amusing</li>\n</ul>\n\n<p>Any recommendation will be appreciated.</p>\n", 'ViewCount': '82', 'Title': 'Are there some popular science videos on computer algorithms to recommend?', 'LastActivityDate': '2012-12-15T14:28:46.113', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithms><history>', 'CreationDate': '2012-12-15T14:28:46.113', 'FavoriteCount': '1', 'Id': '7411'}{'ViewCount': '334', 'Title': 'Importance of recursion in computability theory', 'LastEditDate': '2013-01-05T01:35:23.707', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '947', 'FavoriteCount': '3', 'Body': '<p>It is said that computability theory is also called recursion theory. Why is it called like that? Why recursion has this much importance?</p>\n', 'Tags': '<computability><terminology><history>', 'LastEditorUserId': '41', 'LastActivityDate': '2013-01-05T13:01:20.953', 'CommentCount': '0', 'AcceptedAnswerId': '7762', 'CreationDate': '2013-01-04T15:15:03.867', 'Id': '7759'}{'Body': '<p>I wonder where those terms came from.</p>\n\n<p>Is it computer science related?</p>\n\n<p>Or are they related to civil engineering, or swimming sports?</p>\n\n<p>Is it some analogy with filling water in a pool, or something like that?</p>\n', 'ViewCount': '88', 'Title': 'Where did the terms memory pool and memory leak come from?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-30T13:19:42.900', 'LastEditDate': '2013-01-30T13:19:42.900', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '5282', 'Tags': '<terminology><history>', 'CreationDate': '2013-01-30T03:43:16.227', 'Id': '9293'}{'Body': '<p>I recently discovered that I have a fondness for the work of Dijkstra. I am currently reading random EWDs, and stumbled across one <a href="http://www.cs.utexas.edu/users/EWD/ewd10xx/EWD1014.PDF" rel="nofollow">with some notation that I couldn\'t quite grasp</a>.</p>\n\n<p>Dijkstra states that $[ x &lt; y \\implies f.x.y &lt; f.y.x]$</p>\n\n<p>OK, so. This isn\'t a list comprehension, right? Why the brackets?</p>\n\n<p>Also, how could this be read aloud? What does $\\implies$ mean?</p>\n\n<p>And finally, what does $f.x.y$ and $f.y.x$ mean?</p>\n\n<p>And how could this statement be written more conventionally? And does Dijkstra\'s notation derive from anything, or is it entirely his own invention?</p>\n', 'ViewCount': '96', 'Title': "Understanding Dijkstra's notation $f.x.y$", 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-31T15:40:28.900', 'LastEditDate': '2013-01-31T15:40:28.900', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1693', 'Tags': '<terminology><history>', 'CreationDate': '2013-01-31T09:20:46.907', 'Id': '9345'}{'ViewCount': '490', 'Title': 'How are programming languages and foundations of mathematics related?', 'LastEditDate': '2013-02-15T00:34:04.040', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '268', 'FavoriteCount': '3', 'Body': '<p>Basically I am aware of three foundations for math</p>\n\n<ol>\n<li>Set theory</li>\n<li>Type theory</li>\n<li>Category theory</li>\n</ol>\n\n<p>So in what ways are programming languages and foundations of mathematics related?</p>\n\n<p>EDIT</p>\n\n<p>The original question was "Programming languages based on foundations of math" </p>\n\n<p>with the added paragarph of</p>\n\n<p>And implementations of theory<br>\n1. Type theory in <a href="http://coq.inria.fr/">Coq</a><br>\n2. Set theory in <a href="http://c2.com/cgi/wiki?SetlLanguage">SETL</a><br>\n3. Category theory in <a href="http://www.haskell.org/haskellwiki/Category_theory">Haskell</a></p>\n\n<p>Based on a suggestion this was changed to "How are programming languages and foundations of mathematics related"</p>\n\n<p>Since this is one of those questions were I did not know enough about what I was asking but wanted to learn something, I am modifing the question to make it more valuable for learning and others, yet leaving the details in so as not to make the <a href="http://cs.stackexchange.com/a/9763/268">current answer</a> by Andrej Bauer seem off topic.</p>\n\n<p>Thanks for all the comments and the answer so far, I am learning from them.</p>\n', 'Tags': '<programming-languages><history><type-theory><category-theory>', 'LastEditorUserId': '699', 'LastActivityDate': '2013-02-15T00:37:33.880', 'CommentCount': '2', 'AcceptedAnswerId': '9763', 'CreationDate': '2013-02-14T00:48:23.320', 'Id': '9756'}{'Body': '<p>I\'ve taken an introductory database course where like many good database courses, they taught relational algebra, including $\\theta$-joins. However, I\'ve recently read E. F. Codd\'s 1970 paper, <a href="http://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf" rel="nofollow"><em>A Relational Model of Data for Large Shared Databanks</em></a>, and the concept of $\\theta$-joins is nowhere to be found. He does use the symbol  $\\theta$ in the paper in section 2.2 though, but to refer to something much more general than joins.</p>\n\n<p>In fact, Natural Joins are even defined slightly differently then the definition I\'m familiar with. It requires that the original relations be completely recoverable from the joined relation.</p>\n\n<p>So my question is when/in what paper/by who was the modern notion of the $\\theta$-join introduced that is often taught by universities? </p>\n\n<p><a href="http://en.wikipedia.org/wiki/Relational_algebra#.CE.B8-join_and_equijoin" rel="nofollow">(A link to Wikipedia\'s definition of a $\\theta$-join, similar to the one I learned in my course)</a></p>\n', 'ViewCount': '98', 'Title': 'Origin of theta-joins', 'LastActivityDate': '2013-03-03T05:53:03.093', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10211', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7046', 'Tags': '<reference-request><history><relational-algebra>', 'CreationDate': '2013-02-28T22:56:28.393', 'FavoriteCount': '1', 'Id': '10158'}{'ViewCount': '98', 'Title': 'Why the name Recursively Enumerable and Recursive?', 'LastEditDate': '2013-04-10T18:35:14.180', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '947', 'FavoriteCount': '1', 'Body': '<p>Why did the sets of languages accepted/decided by a TM get the name Recursively Enumerable and Recursive, respectively?</p>\n', 'ClosedDate': '2013-12-10T22:13:09.560', 'Tags': '<terminology><computability><turing-machines><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T20:02:27.980', 'CommentCount': '1', 'CreationDate': '2013-04-10T17:08:52.637', 'Id': '11197'}{'Body': '<p>Why is the laplace transform not popular for image processing convolution? Most textbooks only conver the Fourier transforms.</p>\n', 'ViewCount': '694', 'Title': 'Why is the laplace transform not popular for image processing convolution?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-31T10:49:19.287', 'LastEditDate': '2013-04-11T07:35:40.143', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7239', 'Tags': '<history><image-processing>', 'CreationDate': '2013-04-11T02:44:03.157', 'Id': '11216'}{'Body': '<p>Parallel computing is not new but it is becoming common now a days. This is essentially driven by the need of the users (they need to process more data now) and also because of physical limitations on chip designing e.g power dissipation limits forcing to manufacturers to move to multicore CPUs.</p>\n\n<p>My question is how is the parallel computing different now from what they used to do 30 years back?       </p>\n', 'ViewCount': '112', 'Title': 'Parallel Computing: Past Vs Present', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-20T23:28:55.600', 'LastEditDate': '2013-04-20T23:28:55.600', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<parallel-computing><history>', 'CreationDate': '2013-04-20T13:32:59.313', 'FavoriteCount': '1', 'Id': '11426'}{'Body': u'<p>In the seminal distributed systems paper <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf" rel="nofollow">The Part Time Parliament</a> (the Paxos protocol), Leslie Lamport names fictional legislators who are involved in the Paxon parliament protocol.</p>\n\n<p>According to <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos" rel="nofollow">this writing</a>, he notes that:</p>\n\n<blockquote>\n  <p>I gave the Greek legislators the names of computer scientists working in the field, transliterated with Guibas\'s help into a bogus Greek dialect.</p>\n</blockquote>\n\n<p>Does anyone have any information on the scientists that the legislators are named after? A list of the legislators in the paper and the corresponding computer scientists would be the ideal answer.</p>\n\n<p>I think the first legislator mentioned in the paper, "\u039b\u03b9\u03bd\u03c7\u2202", is named after <a href="http://en.wikipedia.org/wiki/Nancy_Lynch" rel="nofollow">Nancy Lynch</a> since it could be pronounced as "Linch". Also, "\u039b\u03b5\u03c9\u03bd\u03af\u03b4\u03b1\u03c2 \u0393\u03ba\u03af\u03bc\u03c0\u03b1\u03c2" from the bibliography is <a href="https://en.wikipedia.org/wiki/Leonidas_J._Guibas" rel="nofollow">Leo Guibas</a>. I\'m completely lost as to who the others are.</p>\n', 'ViewCount': '147', 'Title': 'Who are the legislators of Paxos?', 'LastEditorUserId': '6785', 'LastActivityDate': '2013-05-31T19:25:53.577', 'LastEditDate': '2013-05-31T19:25:53.577', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6785', 'Tags': '<distributed-systems><history>', 'CreationDate': '2013-05-31T15:24:36.223', 'FavoriteCount': '1', 'Id': '12401'}{'Body': "<p>I'm not sure if one can compare the two, mainly because what I've read so far, I didn't quite understand, but what are the key differences between these 2 engines?</p>\n", 'ViewCount': '80', 'Title': 'Difference between Analytical and Difference Engines', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-10T09:41:29.027', 'LastEditDate': '2013-06-10T09:41:29.027', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'OwnerDisplayName': 'Eduard Luca', 'PostTypeId': '1', 'Tags': '<terminology><history><machine-models>', 'CreationDate': '2013-05-27T03:18:39.130', 'FavoriteCount': '1', 'Id': '12552'}{'Body': '<p>We mostly write programme in high level language. So while studying I came across assembly language. So an assembler converts assembly language to machine language and a compiler does the same with high level language. I found assembly language has instructions like move r1 r3 , move a 5 etc. And it is rather hard to study. So why was assembly language created?or was it the one that came first even before high level language? Why am I studying about assemblers in my computer engineering class?</p>\n', 'ViewCount': '2092', 'Title': 'Why do we need assembly language?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-05T22:35:01.817', 'LastEditDate': '2013-07-16T08:48:48.453', 'AnswerCount': '6', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9094', 'Tags': '<programming-languages><education><history>', 'CreationDate': '2013-07-15T15:05:46.733', 'FavoriteCount': '1', 'Id': '13287'}{'Body': u'<p>If the running time of an algorithm scales linearly with the size of its input, we say it has $O(N)$ complexity, where we understand <code>N</code> to represent input size.</p>\n\n<p>If the running time does not vary with input size, we say it\'s $O(1)$, which is essentially saying it varies proportionally to 1; i.e., doesn\'t vary at all (because 1 is constant).</p>\n\n<p>Of course, 1 is not the only constant. <em>Any</em> number could have been used there, right? (Incidentally, I think this is related to the common mistake many CS students make, thinking "$O(2N)$" is any different from $O(N)$.)</p>\n\n<p>It seems to me that 1 was a sensible choice. Still, I\'m curious if there is more to the etymology there\u2014why not $O(0)$, for example, or $O(C)$ where $C$ stands for "constant"? Is there a story there, or was it just an arbitrary choice that has never really been questioned?</p>\n', 'ViewCount': '225', 'Title': 'Why is it O(1) (and not, say, O(2))?', 'LastEditorUserId': '9574', 'LastActivityDate': '2013-09-19T17:17:20.410', 'LastEditDate': '2013-09-18T20:36:25.443', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9574', 'Tags': '<terminology><time-complexity><asymptotics><landau-notation><history>', 'CreationDate': '2013-09-18T16:39:15.317', 'FavoriteCount': '2', 'Id': '14416'}{'ViewCount': '1299', 'Title': 'What is the definition of Computer Science, and what is the Science within Computer Science?', 'LastEditDate': '2013-10-30T21:27:36.957', 'AnswerCount': '7', 'Score': '28', 'PostTypeId': '1', 'OwnerUserId': '10744', 'FavoriteCount': '8', 'Body': '<p>I am pursuing a BS in Computer Science, but I am at an early point of it, and I am pretty sure I will be happy with my choice given that it seems like an academically and career flexible education to pursue.</p>\n\n<p>Having said that, there seems to be a variety of definitions about what Computer Science really is in respects to academia, the private-sector, and the actual "Science" in "Computer Science" I would love to have answers(Or shared pondering) as to the breadth of things an education in Computer Science can be applied to, and ultimately the variety of paths those within Computer Science have pursued.</p>\n', 'Tags': '<terminology><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-30T21:27:36.957', 'CommentCount': '1', 'AcceptedAnswerId': '16093', 'CreationDate': '2013-10-15T00:44:35.543', 'Id': '16092'}{'Body': "<p>When reading the literature on declarative programming technology. A particular technology that sounded promising is active databases. It essentially allows the programmer to specify rules about data that generate more data. On face value it seems like a great thing to have when compared to your neighborhood average imperative code.</p>\n\n<p>There are many production rules systems out there, but they are not integrated with a database system. As I understand it, active databases would have been the logical evolution for  production systems.</p>\n\n<p>But, the subject seems to have died off. It is hard to find articles on Active Databases. And I can't really find a product that implements anything like that. My question is, what happened?</p>\n", 'ViewCount': '57', 'Title': 'What happened to Active Databases?', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-22T00:01:21.793', 'LastEditDate': '2013-10-22T00:01:21.793', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9964', 'Tags': '<programming-languages><history><database-theory>', 'CreationDate': '2013-10-21T15:02:14.870', 'Id': '16295'}{'ViewCount': '100', 'Title': 'Dijkstras "on the cruelty of really teaching computer science"', 'LastEditDate': '2013-12-03T08:47:38.057', 'AnswerCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '1', 'Body': u'<p>an old remarkable essay of Dijkstras (called a classic by some) entitled <a href="http://www.cs.utexas.edu/~EWD/ewd10xx/EWD1036.PDF" rel="nofollow">On the cruelty of really teaching computer science</a> (1988) has recently been recirculating in cyberspace. its opinionated and wideranging yet has some specific suggestions for rearranging/reorienting CS education in a more formal/mathematical way.</p>\n\n<blockquote>\n  <p>exactly \xbc-century later, with 2020 hindsight, are any of Dijkstra\'s ideas on CS educational strategies viable or playing out in academia? have they had some influence on CS education? what is the general philosophical approach/background in CS education, and why is it the dominant approach, how has it evolved to be that way?</p>\n</blockquote>\n', 'ClosedDate': '2013-12-24T00:57:43.453', 'Tags': '<education><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-03T08:47:38.057', 'CommentCount': '6', 'CreationDate': '2013-12-03T05:08:03.300', 'Id': '18558'}{'Body': '<p>I have a Computer Science degree and I\'m employed as a software engineer. I\'m regularly approached by recruiters on LinkedIn with "programmer", "software developer", or general "software and/or computer industry" propositions.</p>\n\n<p>Every now and then though a recruiter will approach me with an "IT position" opportunity. I understand that in the absolute general sense anything and everything between the keywords <em>computer</em> and <em>software</em> will at some point include IT, but when it comes to more specific discussions, particularly offers for an interview for an "IT position", I often find that to be a bit odd. It often strikes me as if I am being offered to interview for the position of a DBA, sys admin, or an IT staff manager, positions that are as foreign to me as the medical field.</p>\n\n<p>To avoid subjectivity, I\'ll make my question is simple. Is a <em>software engineer</em>, <em>coder</em>, <em>programmer</em> or a <em>software developer</em>, in an IT position? IT industry (app development, scientific, multimedia/game)?</p>\n', 'ViewCount': '36', 'ClosedDate': '2014-05-03T09:01:19.970', 'Title': 'Computer Science and the IT industry', 'LastActivityDate': '2014-05-03T03:29:09.333', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '17287', 'Tags': '<terminology><history>', 'CreationDate': '2014-05-03T00:58:01.853', 'Id': '24333'}