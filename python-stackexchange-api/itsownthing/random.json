{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to generate a completely random <a href="http://en.wikipedia.org/wiki/Sudoku" rel="nofollow">Sudoku</a>.</p>\n\n<p>Define a Sudoku grid as a $9\\times9$ grid of integers between $1$ and $9$ where some elements can be omitted. A grid is a valid puzzle if there is a <strong>unique</strong> way to complete it to match the Sudoku constraints (each line, column and aligned $3\\times3$ square has no repeated element) and it is minimal in that respect (i.e. if you omit any more element the puzzle has multiple solutions).</p>\n\n<p>How can I generate a random Sudoku puzzle, such that all Sudoku puzzles are equiprobable?</p>\n', 'ViewCount': '1340', 'Title': 'Random Sudoku generator', 'LastEditorUserId': '39', 'LastActivityDate': '2012-03-09T16:52:33.813', 'LastEditDate': '2012-03-09T16:52:33.813', 'AnswerCount': '1', 'CommentCount': '20', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '122', 'Tags': '<algorithms><random><sudoku>', 'CreationDate': '2012-03-07T05:09:28.573', 'FavoriteCount': '2', 'Id': '72'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '194', 'Title': 'Proving the security of Nisan-Wigderson pseudo-random number generator', 'LastEditDate': '2012-04-07T23:33:31.787', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '41', 'FavoriteCount': '1', 'Body': "<p>Let $\\cal{S}=\\{S_i\\}_{1\\leq i\\leq n}$ be a partial $(m,k)$-design and $f: \\{0,1\\}^m \\to \\{0,1\\}$ be a Boolean function. The Nisan-Wigderson generator $G_f: \\{0,1\\}^l \\to \\{0,1\\}^n$ is defined as follows:</p>\n\n<p>$$G_f(x) = (f(x|_{S_1}) , \\ldots, f(x|_{S_n}) )$$</p>\n\n<p>To compute the $i$th bit of $G_f$ we take the bits of $x$ with indexes in $S_i$ and then apply $f$ to them.</p>\n\n<blockquote>\n  <p>Assume that $f$ is $\\frac{1}{n^c}$-hard for circuits of size $n^c$ where $c$ is a constant.\n  How can we prove that $G_f$ is $(\\frac{n^c}{2}, \\frac{2}{n^c})$-secure pseudo-random number generator?</p>\n</blockquote>\n\n<h3>Definitions:</h3>\n\n<p>A partial $(m,k)$-design is a collection of subsets $S_1, \\ldots, S_n \\subseteq [l] = \\{1, \\ldots, l\\}$ such that </p>\n\n<ul>\n<li>for all $i$: $|S_i|=m$, and</li>\n<li>for all $i \\neq j$: $|S_i \\cap S_j| \\leq k$.</li>\n</ul>\n\n<p>A function $f$ is $\\epsilon$-hard for circuits of size $s$ iff no circuit of size $s$ can predict $f$ with probability $\\epsilon$ better than a coin toss.</p>\n\n<p>A function $G:\\{0,1\\}^l \\to \\{0,1\\}^n$ is $(s, \\epsilon)$-secure pseudo-random number generator iff no circuit of size $s$ can distinguish between a random number and a number generated by $G_f$ with probability better than $\\epsilon$.</p>\n\n<p>We use $x|_A$ for the string composed of $x$'s bits with indexes in $A$.</p>\n", 'Tags': '<cryptography><pseudo-random-generators>', 'LastEditorUserId': '157', 'LastActivityDate': '2012-09-04T15:21:55.373', 'CommentCount': '6', 'AcceptedAnswerId': '3417', 'CreationDate': '2012-03-13T04:08:58.890', 'Id': '286'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>You have one coin. You may flip it as many times as you want. </p>\n\n<p>You want to generate a random number $r$ such that $a \\leq r &lt; b$ where $r,a,b\\in \\mathbb{Z}^+$. </p>\n\n<p>Distribution of the numbers should be uniform. </p>\n\n<p>It is easy if $b -a = 2^n$:</p>\n\n<pre><code>r = a + binary2dec(flip n times write 0 for heads and 1 for tails) \n</code></pre>\n\n<p>What if $b-a \\neq 2^n$?</p>\n', 'ViewCount': '2142', 'Title': 'Generating uniformly distributed random numbers using a coin', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-24T21:55:20.950', 'LastEditDate': '2012-04-29T20:51:09.547', 'AnswerCount': '7', 'CommentCount': '0', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '244', 'Tags': '<algorithms><probability-theory><randomness><random-number-generator>', 'CreationDate': '2012-03-21T03:12:00.883', 'FavoriteCount': '4', 'Id': '570'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I <a href="http://blog.computationalcomplexity.org/2004/04/blum-complexity-measures.html" rel="nofollow">read</a> that the number of coin tosses of a probabilistic Turing machine (PTM) is not a <a href="http://en.wikipedia.org/wiki/Blum_axioms" rel="nofollow">Blum complexity measure</a>. Why?</p>\n\n<p>Clarification:</p>\n\n<p>Note that since the execution of the machine is not deterministic, one should be careful about defining the number of coin tosses for a PTM $M$ on input $x$ in a way similar to the time complexity for NTMs and PTMs. One way is to define it as the maximum number of coin tosses over possible executions of $M$ on $x$.</p>\n\n<p>We need the definition to satisfy the axiom about decidability of $m(M,x)=k$. We can define it as follows:</p>\n\n<p>$$\nm(M,x) =\n\\begin{cases}\nk &amp; \\text{all executions of $M$ on $x$ halt, $k=\\max$ #coin tosses} \\\\\n\\infty &amp; o.w. \\\n\\end{cases}\n$$</p>\n\n<p>The number of random bits that an algorithm uses is a complexity measure that appears in papers, e.g. "algorithm $A$ uses only $\\lg n$ random bits, whereas algorithm $B$ uses $n$ random bits".</p>\n', 'ViewCount': '119', 'Title': 'Is the number of coin tosses of a probabilistic Turing machine a Blum complexity measure?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-28T17:32:38.167', 'LastEditDate': '2012-03-28T17:32:38.167', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<computability><complexity-theory><randomness><probabilistic-algorithms>', 'CreationDate': '2012-03-27T20:35:16.207', 'Id': '835'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am confused about how taps are chosen for Linear Feedback Shift Registers.</p>\n\n<p>I have a diagram which shows a LFSR with connection polynomial $C(X) = X^5 + X^2 + 1$. The five stages are labelled: $R4, R3, R2, R1$ and $R0$ and the taps come out of $R0$ and $R3$.</p>\n\n<p>How are these taps decided? When I am given a connection polynomial but no diagram, how do I know what values I should XOR?</p>\n\n<p><img src="http://i.stack.imgur.com/ubz8P.jpg" alt="enter image description here"></p>\n', 'ViewCount': '221', 'Title': 'Choosing taps for Linear Feedback Shift Register', 'LastEditorUserId': '157', 'LastActivityDate': '2012-04-08T00:26:01.017', 'LastEditDate': '2012-04-07T23:33:01.787', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1123', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '995', 'Tags': '<cryptography><pseudo-random-generators><shift-register>', 'CreationDate': '2012-04-07T22:38:27.080', 'Id': '1121'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1359', 'Title': 'Randomized Selection', 'LastEditDate': '2012-04-18T19:51:56.180', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1134', 'FavoriteCount': '2', 'Body': u'<p>The randomized selection algorithm is the following:</p>\n\n<p>Input: An array $A$ of $n$ (distinct, for simplicity) numbers and a number $k\\in [n]$</p>\n\n<p>Output: The the "rank $k$ element" of $A$ (i.e., the one in position $k$ if $A$ was sorted)</p>\n\n<p>Method:</p>\n\n<ul>\n<li>If there is one element in $A$, return it</li>\n<li>Select an element $p$ (the "pivot") uniformly at random</li>\n<li>Compute the sets $L = \\{a\\in A : a &lt; p\\}$ and $R = \\{a\\in A : a &gt; p\\}$</li>\n<li>If $|L| \\ge k$, return the rank $k$ element of $L$.</li>\n<li>Otherwise, return the rank $k - |L|$ element of $R$</li>\n</ul>\n\n<p>I was asked the following question:</p>\n\n<blockquote>\n  <p>Suppose that $k=n/2$, so you are looking for the median, and let $\\alpha\\in (1/2,1)$\n  be a constant.  What is the probability that, at the first recursive call, the \n  set containing the median has size at most $\\alpha n$?</p>\n</blockquote>\n\n<p>I was told that the answer is $2\\alpha - 1$, with the justification "The pivot selected should lie between $1\u2212\\alpha$ and $\\alpha$ times the original array"</p>\n\n<p>Why? As $\\alpha \\in (0.5, 1)$, whatever element is chosen as pivot is either larger or smaller than more than half the original elements. The median always lies in the larger subarray, because the elements in the partitioned subarray are always less than the pivot. </p>\n\n<p>If the pivot lies in the first half of the original array (less than half of them), the median will surely be in the second larger half, because once the median is found, it must be in the middle position of the array, and everything before the pivot is smaller as stated above. </p>\n\n<p>If the pivot lies in the second half of the original array (more than half of the elements), the median will surely first larger half, for the same reason, everything before the pivot is considered smaller. </p>\n\n<p>Example:</p>\n\n<p>3 4 5 8 7 9 2 1 6 10</p>\n\n<p>The median is 5.</p>\n\n<p>Supposed the chosen pivot is 2. So after the first iteration, it becomes:</p>\n\n<p>1 2 ....bigger part....</p>\n\n<p>Only <code>1</code> and <code>2</code> are swapped after the first iteration. Number 5 (the median) is still in the first greater half (accroding to the pivot 2). The point is, median always lies on greater half, how can it have a chance to stay in a smaller subarray?</p>\n', 'Tags': '<algorithms><algorithm-analysis><probability-theory><randomized-algorithms>', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-18T19:51:56.180', 'CommentCount': '5', 'AcceptedAnswerId': '1343', 'CreationDate': '2012-04-18T08:21:27.190', 'Id': '1334'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The usual simple algorithm for finding the median element in an array $A$ of $n$ numbers is:</p>\n\n<ul>\n<li>Sample $n^{3/4}$ elements from $A$ with replacement into $B$</li>\n<li>Sort $B$ and find the rank $|B|\\pm \\sqrt{n}$ elements $l$ and $r$ of $B$</li>\n<li>Check that $l$ and $r$ are on opposite sides of the median of $A$ and that there are at most $C\\sqrt{n}$ elements in $A$ between $l$ and $r$ for some appropriate constant $C &gt; 0$.  Fail if this doesn\'t happen.</li>\n<li>Otherwise, find the median by sorting the elements of $A$ between $l$ and $r$</li>\n</ul>\n\n<p>It\'s not hard to see that this runs in linear time and that it succeeds with high probability. (All the bad events are large deviations away from the expectation of a binomial.)</p>\n\n<p>An alternate algorithm for the same problem, which is more natural to teach to students who have seen quick sort is the one described here: <a href="http://cs.stackexchange.com/questions/1334/randomized-selection/1343">Randomized Selection</a></p>\n\n<p>It is also easy to see that this one has linear expected running time: say that a "round" is a sequence of recursive calls that ends when one gives a 1/4-3/4 split, and then observe that the expected length of a round is at most 2.  (In the first draw of a round, the probability of getting a good split is 1/2 and then after actually increases, as the algorithm was described so round length is dominated by a  geometric random variable.)</p>\n\n<p>So now the question: </p>\n\n<blockquote>\n  <p>Is it possible to show that randomized selection runs in linear time with high probability?</p>\n</blockquote>\n\n<p>We have $O(\\log n)$ rounds, and each round has length at least $k$ with probability at most $2^{-k+1}$, so a union bound gives that the running time is $O(n\\log\\log n)$ with probability $1-1/O(\\log n)$.</p>\n\n<p>This is kind of unsatisfying, but is it actually the truth?</p>\n', 'ViewCount': '98', 'Title': 'Sharp concentration for selection via random partitioning?', 'LastEditorUserId': '657', 'LastActivityDate': '2012-04-19T19:52:47.013', 'LastEditDate': '2012-04-19T19:52:47.013', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1350', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '657', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms>', 'CreationDate': '2012-04-18T20:22:01.597', 'Id': '1346'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '250', 'Title': 'Algorithm to chase a moving target', 'LastEditDate': '2012-04-20T22:53:41.330', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '4', 'Body': "<p>Suppose that we have a black-box $f$ which we can query and reset. When we reset $f$, the state $f_S$ of $f$ is set to an element chosen uniformly at random from the set $$\\{0, 1, ..., n - 1\\}$$ where $n$ is fixed and known for given $f$. To query $f$, an element $x$ (the guess) from $$\\{0, 1, ..., n - 1\\}$$ is provided, and the value returned is $(f_S - x) \\mod n$. Additionally, the state $f_S$ of$f$ is set to a value $f_S&#39; = f_S \\pm k$, where $k$ is selected uniformly at random from $$\\{0, 1, 2, ..., \\lfloor n/2 \\rfloor - ((f_S - x) \\mod n)\\} $$</p>\n\n<p>By making uniformly random guesses with each query, one would expect to have to make $n$ guesses before getting $f_S = x$, with variance $n^2 - n$ (stated without proof).</p>\n\n<p>Can an algorithm be designed to do better (i.e., make fewer guesses, possibly with less variance in the number of guesses)? How much better could it do (i.e., what's an optimal algorithm, and what is its performance)?</p>\n\n<p>An efficient solution to this problem could have important cost-saving implications for shooting at a rabbit (confined to hopping on a circular track) in a dark room.</p>\n", 'Tags': '<algorithms><probability-theory><randomized-algorithms>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-10T13:45:31.237', 'CommentCount': '3', 'AcceptedAnswerId': '1976', 'CreationDate': '2012-04-20T14:48:58.600', 'Id': '1392'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'m trying to find the best algorithm for converting an \u201cordinary\u201d linked list into an \u201cideal" skip list. </p>\n\n<p>The definition of an \u201cideal skip list\u201d is that in the first level we\'ll have all the elements, half of them in the next level, a quarter of them in the level after that, and so on.</p>\n\n<p>I\'m thinking about a $\\mathcal{O}(n)$ run-time algorithm involving throwing a coin for each node in the original linked-list, to determine for any given node whether it should be placed in a higher or lower level, and create a duplicate node for the current node at a higher level. This algorithm should work in $\\mathcal{O}(n)$; is there any better algorithm? </p>\n', 'ViewCount': '302', 'Title': 'Building ideal skip lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-16T14:45:04.503', 'LastEditDate': '2012-05-09T10:47:32.457', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '776', 'Tags': '<algorithms><data-structures><randomized-algorithms><lists>', 'CreationDate': '2012-04-30T13:03:01.867', 'FavoriteCount': '1', 'Id': '1589'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/1692/prove-fingerprinting">Prove fingerprinting</a>  </p>\n</blockquote>\n\n\n\n<p>Let $a \\neq b$ be two integers from the interval $[1, 2^n].$ Let $p$ be a random prime with $ 1 \\le p \\le n^c.$ Prove that\n$$\\text{prob}(a \\equiv  b \\pmod{p}) \\le c \\ln(n)/(n^{c-1}).$$</p>\n\n<p>Hint: As a consequence of the prime number theorem, exactly $n/ \\ln(n) \\pm O(n/\\ln(n))$ many numbers from $\\{ 1, \\ldots, n \\}$ are prime.</p>\n\n<p>Conclusion: we can compress $n$ bits to $O(\\log(n))$ bits and get a quite small false-positive rate.</p>\n', 'ViewCount': '22', 'ClosedDate': '2012-05-17T01:30:53.740', 'Title': 'Prove fingerprinting', 'LastActivityDate': '2012-05-17T01:29:53.493', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user1374864', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<random>', 'CreationDate': '2012-05-06T16:26:22.240', 'Id': '1878'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '79', 'Title': 'Making random sources uniformly distributed', 'LastEditDate': '2012-05-19T14:50:50.590', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '0', 'Body': '<p>How do I build a random source that outputs the bits 0 and 1 with $prob(0) = prob(1) = 0.5$. We have access to another random source $S$ that outputs $a$ or $b$ with independent probabilities $prob(a)$ and $prob(b) = 1 - prob(a)$ that are unknown to us.</p>\n\n<p>How do I state an algorithm that does the job and that does not consume more than an expected number of\n$(prob(a) \\cdot prob(b))^{-1}$ symbols of $S$ between two output bits and prove its correcteness?</p>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-19T19:13:23.613', 'CommentCount': '1', 'AcceptedAnswerId': '1934', 'CreationDate': '2012-05-19T14:14:55.337', 'Id': '1921'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '174', 'Title': 'Deterministic and randomized communication complexity of set equality', 'LastEditDate': '2012-06-06T13:47:19.663', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '1', 'Body': u'<p>Two processors $A, B$ with inputs $a \\in  \\{0, 1\\}^n$ (for $A$) and $b \\in  \\{0, 1\\}^n$\n(for $B$) want to decide whether $a = b$. $A$ does not know $B$\u2019s input and vice versa.</p>\n\n<p>A can send a message $m(a) \\in  \\{0, 1\\}^n$ which $B$ can use to decide $a = b$. The communication and computation rules are called a <em>protocol</em>.</p>\n\n<ul>\n<li>Show that every deterministic protocol must satisfy $|m(a)| \\ge  n$.</li>\n<li>State a randomized protocol that uses only $O(\\log_2n)$ Bits. The protocol should always accept if $a = b$ and accept with probability at most $1/n$ otherwise. Prove its correctness.</li>\n</ul>\n', 'Tags': '<algorithms><probability-theory><randomized-algorithms>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-06-06T13:47:19.663', 'CommentCount': '5', 'AcceptedAnswerId': '1978', 'CreationDate': '2012-05-19T14:30:57.650', 'Id': '1922'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A source provides a stream of items $x_1, x_2,\\dots$ . At each step $n$ we want to save a random sample $S_n \\subseteq \\{ (x_i, i)|1 \\le i \\le n\\}$ of size $k$, i.e. $S_n$ should be a uniformly chosen sample from all $\\tbinom{n}{k}$ possible samples consisting of seen items. So at each step $n \\ge k$ we must decide whether to add the next item to $S$ or not. If so we must also decide which of the current items to remove from $S$ .</p>\n\n<p>State an algorithm for the problem. Prove its correctness.</p>\n', 'ViewCount': '158', 'Title': 'Online generation of uniform samples', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-03T01:02:26.967', 'LastEditDate': '2012-05-19T15:31:20.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1931', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><probability-theory><randomized-algorithms><randomness><online-algorithms>', 'CreationDate': '2012-05-19T14:38:52.510', 'Id': '1923'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If an encryption algorithm is meant to convert a string to another string which can then be decrypted back to the original, how could this process involve any randomness?  </p>\n\n<p>Surely it has to be deterministic, otherwise how could the decryption function know what factors were involved in creating the encrypted string?</p>\n', 'ViewCount': '210', 'Title': 'How can encryption involve randomness?', 'LastEditorUserId': '157', 'LastActivityDate': '2012-05-26T18:25:38.433', 'LastEditDate': '2012-05-26T18:25:38.433', 'AnswerCount': '5', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1622', 'Tags': '<cryptography><encryption><randomness>', 'CreationDate': '2012-05-23T21:13:58.847', 'Id': '2030'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>There is a family of random graphs $G(n, p)$ with $n$ nodes (<a href="https://en.wikipedia.org/wiki/Random_graph">due to Gilbert</a>). Each possible edge is independently inserted into $G(n, p)$ with probability $p$. Let $X_k$ be the number of cliques of size $k$ in $G(n, p)$.</p>\n\n<p>I know that $\\mathbb{E}(X_k)=\\tbinom{n}{k}\\cdot p^{\\tbinom{k}{2}}$, but how do I prove it?</p>\n\n<p>How to show that $\\mathbb{E}(X_{\\log_2n})\\ge1$ for $n\\to\\infty$? And how to show that $\\mathbb{E}(X_{c\\cdot\\log_2n}) \\to 0$ for $n\\to\\infty$ and a fixed, arbitrary constant $c&gt;1$?</p>\n', 'ViewCount': '297', 'Title': 'Number of clique in random graphs', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-28T09:46:02.827', 'LastEditDate': '2012-05-28T09:46:02.827', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '2119', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<graph-theory><combinatorics><probability-theory><random-graphs>', 'CreationDate': '2012-05-27T23:41:29.403', 'Id': '2118'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '785', 'Title': 'How to prove correctness of a shuffle algorithm?', 'LastEditDate': '2012-05-30T08:13:00.493', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': "<p>I have two ways of producing a list of items in a random order and would like to determine if they are equally fair (unbiased).</p>\n\n<p>The first method I use is to construct the entire list of elements and then do a shuffle on it (say a Fisher-Yates shuffle). The second method is more of an iterative method which keeps the list shuffled at every insertion. In pseudo-code the insertion function is:</p>\n\n<pre><code>insert( list, item )\n    list.append( item )\n    swap( list.random_item, list.last_item )\n</code></pre>\n\n<p>I'm interested in how one goes about showing the fairness of this particular shuffling. The advantages of this algorithm, where it is used, are enough that even if slightly unfair it'd be okay. To decide I need a way to evaluate its fairness.</p>\n\n<p>My first idea is that I need to calculate the total permutations possible this way versus the total permutations possible for a set of the final length. I'm a bit at a loss however on how to calculate the permutations resulting from this algorithm. I also can't be certain this is the best, or easiest approach.</p>\n", 'Tags': '<algorithms><proof-techniques><probability-theory><randomized-algorithms><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-01T11:23:35.340', 'CommentCount': '5', 'AcceptedAnswerId': '2156', 'CreationDate': '2012-05-29T07:11:15.180', 'Id': '2152'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Consider an array $X$ of $n$ cells, each containing a number from $\\{1,..., n\\}$. There is at least\none duplicate number, i.e., a number that appears at least twice. I want output <em>some</em> duplicate number. When streaming we may pass over $X$ more than once. The inspection of a cell generates cost $1$. The cost of a run of an algorithm is the sum of all individual costs. I can at most store $\\log_2n$ bit numbers.\nI tried to do that with a streaming algorithm that uses additional memory $O(1)$ with costs $O(n^2)$. Is it possible to state a ramdom access algorithm that uses additional memory $O(\\log_2n)$ with costs $O(n)$?.</p>\n\n<p>Which algorithm solves the problem by using additional memory $O(1)$ with costs $O(n^2)$?.\nWhich algorithm solves the problem by using additional memory $O(\\log_2n)$ with costs $O(n)$?.</p>\n\n<p>My problem is similar to the cycle detection problem, but I don't know how to use the cycle detection problem to solve mine. Is there maybe a simpler way that I can't see now?</p>\n", 'ViewCount': '115', 'Title': 'Streaming algorithm and random access', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-03T22:21:04.637', 'LastEditDate': '2012-06-02T18:27:50.347', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1393', 'Tags': '<algorithms><randomized-algorithms><random><streaming-algorithm>', 'CreationDate': '2012-06-02T14:46:56.317', 'FavoriteCount': '2', 'Id': '2199'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '749', 'Title': 'Sorting algorithms which accept a random comparator', 'LastEditDate': '2012-06-12T20:52:01.087', 'AnswerCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1642', 'FavoriteCount': '2', 'Body': u'<p>Generic sorting algorithms generally take a set of data to sort and a comparator function which can compare two individual elements. If the comparator is an order relation\xb9, then the output of the algorithm is a sorted list/array.</p>\n\n<p>I am wondering though which sort algorithms would actually <em>work</em> with a comparator that is not an order relation (in particular one which returns a random result on each comparison). By "work" I mean here that they continue return a permutation of their input and run at their typically quoted time complexity (as opposed to degrading to the worst case scenario always, or going into an infinite loop, or missing elements). The ordering of the results would be undefined however. Even better, the resulting ordering would be a uniform distribution when the comparator is a coin flip.</p>\n\n<p>From my rough mental calculation it appears that a merge sort would be fine with this and maintain the same runtime cost and produce a fair random ordering. I think that something like a quick sort would however degenerate,  possibly not finish, and not be fair.</p>\n\n<p>What other sorting algorithms (other than merge sort) would work as described with a random comparator?</p>\n\n<hr>\n\n<ol>\n<li><p>For reference, a comparator is an order relation if it is a proper function (deterministic) and satisfies the axioms of an order relation:</p>\n\n<ul>\n<li>it is deterministic: <code>compare(a,b)</code> for a particular <code>a</code> and <code>b</code> always returns the same result.</li>\n<li>it is transitive: <code>compare(a,b) and compare(b,c) implies compare( a,c )</code></li>\n<li>it is antisymmetric <code>compare(a,b) and compare(b,a) implies a == b</code></li>\n</ul></li>\n</ol>\n\n<p>(Assume that all input elements are distinct, so reflexivity is not an issue.)</p>\n\n<p>A random comparator violates all of these rules. There are however comparators that are not order relations yet are not random (for example they might violate perhaps only one rule, and only for particular elements in the set).</p>\n', 'Tags': '<algorithms><randomized-algorithms><sorting>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-03-20T18:11:29.530', 'CommentCount': '11', 'AcceptedAnswerId': '2349', 'CreationDate': '2012-06-12T11:39:54.473', 'Id': '2336'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Are there any problems in $\\mathsf{P}$ that have randomized algorithms beating lower bounds on deterministic algorithms? More concretely, do we know any $k$ for which $\\mathsf{DTIME}(n^k) \\subsetneq \\mathsf{PTIME}(n^k)$? Here $\\mathsf{PTIME}(f(n))$ means the set of languages decidable by a randomized TM with constant-bounded (one or two-sided) error in $f(n)$ steps. </p>\n\n<blockquote>\n  <p>Does randomness buy us anything inside $\\mathsf{P}$?</p>\n</blockquote>\n\n<p>To be clear, I am looking for something where the difference is asymptotic (preferably polynomial, but I would settle for polylogarithmic), not just a constant.</p>\n\n<p><em>I am looking for algorithms asymptotically better in the worst case. Algorithms with better expected complexity are not what I am looking for. I mean randomized algorithms as in RP or BPP not ZPP.</em></p>\n', 'ViewCount': '332', 'Title': 'Problems in P with provably faster randomized algorithms', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-29T08:14:04.840', 'LastEditDate': '2013-07-29T08:08:49.313', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '208', 'Tags': '<algorithms><complexity-theory><randomized-algorithms>', 'CreationDate': '2012-06-13T13:44:19.383', 'FavoriteCount': '4', 'Id': '2362'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can I prove that the cover time for a directed graph G can be exponential in the size of G?</p>\n', 'ViewCount': '161', 'Title': 'Prove: cover time for directed graph is exponential', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-27T02:51:16.120', 'LastEditDate': '2013-01-27T02:51:16.120', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><graph-theory><random-walks>', 'CreationDate': '2012-06-22T12:29:32.200', 'Id': '2449'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '961', 'Title': 'Most efficient algorithm to print 1-100 using a given random number generator', 'LastEditDate': '2012-07-02T19:46:10.570', 'AnswerCount': '4', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2042', 'FavoriteCount': '4', 'Body': u'<p>We are given a random number generator <code>RandNum50</code> which generates a random integer uniformly in the range 1\u201350.\nWe may use only this random number generator to generate and print all integers from 1 to 100 in a random order. Every number must come exactly once, and the probability of any number occurring at any place must be equal.</p>\n\n<p>What is the most efficient algorithm for this?</p>\n', 'Tags': '<algorithms><integers><randomness><random-number-generator>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-07-24T20:32:31.727', 'CommentCount': '6', 'AcceptedAnswerId': '2578', 'CreationDate': '2012-07-02T05:57:26.373', 'Id': '2576'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '445', 'Title': 'Is rejection sampling the only way to get a truly uniform distribution of random numbers?', 'LastEditDate': '2012-08-17T17:42:18.317', 'AnswerCount': '3', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '140', 'FavoriteCount': '2', 'Body': '<p>Suppose that we have a random generator that outputs\nnumbers in the range $[0..R-1]$ with uniform distribution and we\nneed to generate random numbers in the range $[0..N-1]$\nwith uniform distribution.</p>\n\n<p>Suppose that $N &lt; R$ and $N$ does not evenly divide $R$;\nin order to get a <strong>truly uniform distribution</strong> we can use the\n<a href="http://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> method:</p>\n\n<ul>\n<li>if $k$ is the greatest integer such that $k N &lt; R$</li>\n<li>pick a random number $r$ in $[0..R-1]$</li>\n<li>if $r &lt; k N$ then output $r \\mod N$, otherwise keep trying with other random numbers r\', r", ... until the condition is met</li>\n</ul>\n\n<blockquote>\nIs rejection sampling the only way to get a truly uniform discrete distribution?\n</blockquote>\n\n<p>If the answer is yes, why? </p>\n\n<p>Note: if $N &gt; R$ the idea is the same: generate a random number $r\'$ in $[0..R^m-1], R^m &gt;= N$, for example $r\' = R(...R(R r_1 + r_2)...)+r_m$ where $r_i$ is a random number in the range $[0..R-1]$</p>\n', 'Tags': '<probability-theory><randomness><random-number-generator><sampling>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-08-18T14:29:51.573', 'CommentCount': '1', 'AcceptedAnswerId': '2619', 'CreationDate': '2012-07-04T07:46:04.420', 'Id': '2605'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have read that the degree of nodes in a "knowledge" graph of people roughly follows a power law distribution, and more exactly can be approximated with a Pareto-Lognormal distribution.</p>\n\n<p>Where can I find a kind of algorithm that will produce a random graph with this distribution?</p>\n\n<p>See for example the paper <a href="http://www.cs.ucsb.edu/~alessandra/papers/ba048f-sala.pdf" rel="nofollow">Revisiting Degree Distribution Models for Social Graph Analysis</a> (page 4, equation 1) for a mathematical description (distribution function) of the kind of distribution I\'m interested in.</p>\n', 'ViewCount': '226', 'Title': 'How to random-generate a graph with Pareto-Lognormal degree nodes?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T04:40:16.137', 'LastEditDate': '2012-07-05T07:42:21.527', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2808', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2060', 'Tags': '<algorithms><graph-theory><probability-theory><randomness>', 'CreationDate': '2012-07-04T11:43:54.893', 'Id': '2608'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We are given a set of objects, say integers, $S$. In addition, we are given a predicate $P$, for example $P(i): \\Leftrightarrow i \\geq 0$. We don't know in advance how many elements of $S$ satisfy the predicate $P$, but we would like to sample or choose an element uniformly at random from $S' = \\{ i \\mid i \\in S \\wedge P(i) \\}$.</p>\n\n<p>The naive approach is to scan $S$ and for example record all the integers or indices for which $P$ holds, then choose one of them uniformly at random. The downside is that in the worst-case, we need $|S|$ space.</p>\n\n<p>For large sets or in say a streaming environment the naive approach is not acceptable. Is there an in-place algorithm for the problem?</p>\n", 'ViewCount': '112', 'Title': 'Choosing an element from a set satisfying a predicate uniformly at random in $O(1)$ space', 'LastActivityDate': '2012-07-21T21:45:31.217', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2856', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><randomized-algorithms><streaming-algorithm><in-place>', 'CreationDate': '2012-07-21T21:45:31.217', 'Id': '2855'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '415', 'Title': 'Generating inputs for random-testing graph algorithms?', 'LastEditDate': '2012-08-04T16:01:43.273', 'AnswerCount': '1', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '2', 'Body': '<p>When testing algorithms, a common approach is random testing: generate a significant number of inputs according to some distribution (usually uniform), run the algorithm on them and verify correctness. Modern testing frameworks can generate inputs automatically given the algorithms signature, with some restrictions.</p>\n\n<p>If the inputs are numbers, lists or strings, generating such inputs in straight-forward. Trees are harder, but still easy (using stochastic context-free grammars or similar approaches).</p>\n\n<p>How can you generate random graphs (efficiently)? Usually, picking graphs uniformly at random is not what you want: they should be connected, or planar, or cycle-free, or fulfill any other property. Rejection sampling seems suboptimal, due to the potentially huge set of undesirable graphs.</p>\n\n<p>What are useful distributions to look at? Useful here means that</p>\n\n<ul>\n<li>the graphs are likely to test the algorithm at hand well and</li>\n<li>they can be generated effectively and efficiently.</li>\n</ul>\n\n<p>I know that there are many models for random graphs, so I\'d appreciate some insight into which are best for graph generation in this regard.</p>\n\n<p>If "some algorithm" is too general, please use shortest-path finding algorithms as a concrete class of algorithms under test. Graphs for testing should be connected and rather dense (with high probability, or at least in expectation). For testing, the optimal solution would be to create random graphs around a shortest path so we <em>know</em> the desired result (without having to employ another algorithm).</p>\n', 'Tags': '<algorithms><graphs><randomness><software-testing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-08T16:33:06.190', 'CommentCount': '1', 'AcceptedAnswerId': '2953', 'CreationDate': '2012-07-30T21:36:29.087', 'Id': '2952'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '780', 'Title': 'Uniform sampling from a simplex', 'LastEditDate': '2012-08-17T09:22:28.433', 'AnswerCount': '2', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '2553', 'FavoriteCount': '1', 'Body': "<p>I am looking for an algorithm to generate an array of N random numbers, such that the sum of the N numbers is 1, and all numbers lie within 0 and 1. For example, N=3, the random point (x, y, z) should lie within the triangle:</p>\n\n<pre><code>x + y + z = 1\n0 &lt; x &lt; 1\n0 &lt; y &lt; 1\n0 &lt; z &lt; 1\n</code></pre>\n\n<p>Ideally I want each point within the area to have equal probability. If it's too hard, I can drop the requirement. Thanks.</p>\n", 'Tags': '<algorithms><randomness><random-number-generator><sampling>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-09-27T21:24:11.037', 'CommentCount': '2', 'AcceptedAnswerId': '3229', 'CreationDate': '2012-08-16T19:45:16.167', 'Id': '3227'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to calculate the output of the sequence generated by this shift register but I cannot find anywhere how to do it. Everywhere the results are just given but there is no explanation how to do compute them. I know the the sequence will repeat every 2^3-1=7 times. Could anyone explain me how to do it. Thank you.</p>\n\n<p><img src="http://i.stack.imgur.com/sEhMT.jpg" alt="enter image description here"></p>\n', 'ViewCount': '86', 'Title': 'LFSR sequence computation', 'LastEditorUserId': '2639', 'LastActivityDate': '2012-08-26T19:19:44.063', 'LastEditDate': '2012-08-26T14:18:02.693', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3338', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2639', 'Tags': '<cryptography><pseudo-random-generators><shift-register>', 'CreationDate': '2012-08-26T13:28:54.913', 'Id': '3337'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am going to attempt to write a random number generator using exisiting randomize algorithms. Can you suggest which algorithm has the biggest sequence that never repeats? I don't care if they are fast or slow.</p>\n", 'ViewCount': '128', 'Title': 'Random algorithm with biggest sequence that never repeats', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-08T05:42:39.437', 'LastEditDate': '2012-09-08T05:42:39.437', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '3429', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2731', 'Tags': '<algorithms><random>', 'CreationDate': '2012-09-04T13:19:14.110', 'Id': '3421'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let Alice and Bob be two users chosen uniformly at random from a social network (e.g. Facebook). What is the probability that they are friends assuming that they share $k$ mutual friends?</p>\n\n<p>I am interested both in the experimental values (or estimates) from currently existing social networks (e.g. Facebook) and values predicted by random graph models for these social networks.</p>\n', 'ViewCount': '193', 'Title': 'What is the probability of friendship conditioned on the number of mutual friends', 'LastActivityDate': '2012-09-17T07:17:51.607', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<graph-theory><combinatorics><social-networks><random-graphs>', 'CreationDate': '2012-09-16T00:29:01.343', 'Id': '4571'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I am trying to teach myself the principles of cryptograhpy, and want to solve the following question:</p>\n\n<blockquote>\n  <p>Let G be the algorithm that takes an input x = (x1, . . . , xn) from\n  {0, 1} n (so each xi \u2208 {0, 1}) and outputs the string G(x1, . . . ,\n  xn) = (x1, . . . , xn, x1 \u2295 x2) in {0, 1} n+1. Rigorously prove that G\n  is not a pseudorandom generator.</p>\n</blockquote>\n\n<p>So, we would want to prove that either <strong>expansion</strong> doesn't hold (for all n, l(n) > n), which, in this case, would mean that for each x, the corresponding G > x, or <strong>pseudorandomness</strong> doesn't hold ( For efficient algorithm D, there is a negligible function 'n' such that: </p>\n\n<blockquote>\n  <p>Pr[D(r) = 1]-Pr[D(G(s)) = 1] $\\leq$ $\\epsilon$(n), </p>\n  \n  <p>where 'r' is uniform on {0,1}$^{l(n)}$ and 's' is uniform on {0,1}$^n$</p>\n</blockquote>\n\n<p>), which, in this case, means that we need to prove that there does not exist a function n such that the difference of the probabilities of the function D(r) and D(G(s)) is very small.</p>\n\n<p>Now, I understand these concepts, but I am  having trouble approaching this problem practically... what would be the best way to start?</p>\n", 'ViewCount': '78', 'Title': 'Rigorous proof against pseudorandom generator', 'LastActivityDate': '2012-09-27T06:54:15.687', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '4757', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1132', 'Tags': '<correctness-proof><pseudo-random-generators>', 'CreationDate': '2012-09-26T22:28:27.563', 'Id': '4754'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to derive the <a href="http://alexandria.tue.nl/repository/freearticles/597601.pdf">classic paper</a> in the title only by elementary means (no generating functions, no complex analysis, no Fourier analysis) although with much less precision. In short, I "only" want to prove that the average height $h_n$ of a tree with $n$ nodes (that is, the maximum number of nodes from the root to a leaf) satisfies $h_n \\sim \\sqrt{\\pi n}$.</p>\n\n<p>The outline is as follows. Let $A_{nh}$ be the number of trees with height less than or equal to $h$ (with the convention $A_{nh} = A_{nn}$ for all $h \\geqslant n$) and $B_{nh}$ the number of trees of $n$ nodes with height greater than or equal to $h+1$ (that is, $B_{nh} = A_{nn} - A_{nh}$). Then $h_n = S_n/A_{nn}$, where $S_n$ is the finite sum\n$$\nS_n = \\sum_{h \\geqslant 1} h(A_{nh} - A_{n,h-1}) = \\sum_{h \\geqslant 1} h(B_{n,h-1} - B_{nh}) = \\sum_{h \\geqslant 0} B_{nh}.\n$$\nIt is well known that $A_{nn} = \\frac{1}{n}\\binom{2n-2}{n-1}$, for the set of general trees with $n$ nodes is in bijection with the set of binary trees with $n-1$ nodes, counted by the Catalan numbers.</p>\n\n<p>Therefore, the first step is to find $B_{nh}$ and then the main term in the asymptotic expansion of $S_n$.</p>\n\n<p>At this point the authors use analytical combinatorics (three pages) to derive\n$$\nB_{n+1,h-1} = \\sum_{k \\geqslant 1} \\left[\\binom{2n}{n+1-kh} - 2\\binom{2n}{n-kh} + \\binom{2n}{n-1-kh}\\right].\n$$</p>\n\n<blockquote>\n  <p>My own attempt is as follows. I consider the bijection between trees with $n$ nodes\n  and monotonic paths on a square grid $(n-1) \\times (n-1)$ from $(0,0)$ to $(n-1,n-1)$ which do not cross the diagonal (and are made of two kinds of steps: $\\uparrow$ and $\\rightarrow$). These paths are sometimes called <em>Dyck paths</em> or <em>excursions</em>. I can express now $B_{nh}$ in terms of lattice paths: it is the number of Dyck paths of length 2(n-1) and height greater than or equal to $h$. (Note: a tree of height $h$ is in bijection with a Dyck path of height $h-1$.)</p>\n  \n  <p>Without loss of generality, I assume that they start with $\\uparrow$ (hence stay above the diagonal). For each path, I consider the first step crossing the line $y = x + h - 1$, if any. From the point above, all the way back to the origin, I change $\\uparrow$ into $\\rightarrow$ and vice versa (this is a <em>reflection</em> wrt the line $y=x+h$). It becomes apparent that the paths I want to count ($B_{nh}$) are in bijection with the monotonic paths from $(-h,h)$ to $(n-1,n-1)$ which avoid the boundaries $y=x+2h+1$ and $y=x-1$. (See <a href="http://www.filedropper.com/lattice">figure</a>.)</p>\n</blockquote>\n\n<p>In the classic book <em>Lattice Path Counting and Applications</em> by Mohanty (1979, page 6) the formula\n$$\n\\sum_{k \\in \\mathbb{Z}} \\left[\\binom{m+n}{m-k(t+s)} - \\binom{m+n}{n+k(t+s)+t}\\right],\n$$\ncounts the number of monotonic paths in a lattice from $(0,0)$ to $(m,n)$, which avoid the boundaries $y = x - t$ and $y = x + s$, with $t &gt; 0$ and $s &gt; 0$. (This result was first established by Russian statisticians in the 50s.) Therefore, by considering a new origin at $(-h,h)$, we satisfy the conditions of the formula: $s=1$, $t=2h+1$ and the destination point (the upper right corner) is now $(n+h-1,n-h-1)$. Then\n$$\nB_{nh} = \\sum_{k \\in \\mathbb{Z}} \\left[\\binom{2n-2}{n+h-1-k(2h+2)} - \\binom{2n-2}{n-h-1+k(2h+2) + 2h+1}\\right].\n$$\nThis can be simplified in\n$$\nB_{n+1,h-1} = \\sum_{k \\in \\mathbb{Z}} \\left[\\binom{2n}{n+1-(2k+1)h} - \\binom{2n}{n-(2k+1)h}\\right],\n$$\nwhich, in turn, is equivalent to\n$$\nB_{n+1,h-1} = \\sum_{k \\geqslant 0} \\left[\\binom{2n}{n+1-(2k+1)h} - 2\\binom{2n}{n-(2k+1)h} + \\binom{2n}{n-1-(2k+1)h}\\right].\n$$\nThe difference with the expected formula is that I sum over the odd numbers ($2k+1$), instead of all positive integers ($k$).</p>\n\n<p>Any idea where the problem is?</p>\n', 'ViewCount': '202', 'Title': 'On "The Average Height of Planted Plane Trees" by Knuth, de Bruijn and Rice (1972)', 'LastEditorUserId': '2993', 'LastActivityDate': '2012-09-30T09:32:05.513', 'LastEditDate': '2012-09-30T09:32:05.513', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2993', 'Tags': '<combinatorics><discrete-mathematics><trees><average-case><random-walks>', 'CreationDate': '2012-09-28T12:20:51.920', 'Id': '4777'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What differences and relationships are between randomized algorithms and nondeterministic algorithms?</p>\n\n<p>From Wikipedia</p>\n\n<blockquote>\n  <p>A <strong>randomized algorithm</strong> is an algorithm which employs a degree of\n  randomness as part of its logic. The algorithm typically uses\n  uniformly random bits as an auxiliary input to guide its behavior, in\n  the hope of achieving good performance in the "average case" over all\n  possible choices of random bits. Formally, the algorithm\'s performance\n  will be a random variable determined by the random bits; thus either\n  the running time, or the output (or both) are random variables.</p>\n  \n  <p>a <strong>nondeterministic algorithm</strong> is an algorithm that can exhibit\n  different behaviors on different runs, as opposed to a deterministic\n  algorithm. There are several ways an algorithm may behave differently\n  from run to run. A <strong>concurrent algorithm</strong> can perform differently on\n  different runs due to a race condition. A <strong>probabalistic algorithm</strong>\'s\n  behaviors depends on a random number generator. An algorithm that\n  solves a problem in nondeterministic polynomial time can run in\n  polynomial time or exponential time depending on the choices it makes\n  during execution.</p>\n</blockquote>\n\n<p>Are randomized algorithms and probablistic algorithms the same concept? </p>\n\n<p>If yes, so are randomized algorithms just a kind of nondeterministic algorithms?</p>\n', 'ViewCount': '1555', 'Title': 'Differences and relationships between randomized and nondeterministic algorithms?', 'LastEditorUserId': '31', 'LastActivityDate': '2014-02-02T18:42:47.173', 'LastEditDate': '2014-01-23T10:11:12.273', 'AnswerCount': '4', 'CommentCount': '7', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<algorithms><terminology><randomized-algorithms><machine-models><nondeterminism>', 'CreationDate': '2012-10-11T05:19:33.520', 'FavoriteCount': '5', 'Id': '5008'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In some literature, as well as in <a href="http://en.wikipedia.org/wiki/Mersenne_twister" rel="nofollow">Wikipedia</a>, the middle term parameter <em>m</em> of Mersenne twister is called "number of parallel sequences". Why? What is meant here by "parallel sequences"?</p>\n', 'ViewCount': '32', 'Title': 'Mersenne twister middle word', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-20T00:43:31.747', 'LastEditDate': '2012-10-20T00:43:31.747', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4276', 'Tags': '<terminology><cryptography><pseudo-random-generators>', 'CreationDate': '2012-10-19T20:39:43.797', 'Id': '6178'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '172', 'Title': 'Classfication of randomized algorithms', 'LastEditDate': '2012-10-22T00:56:03.857', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '3', 'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Randomized_algorithm">Wikipedia</a> about randomized algorithms</p>\n\n<blockquote>\n  <p>One has to distinguish between <strong>algorithms</strong> that use the random\n  input to reduce the expected running time or memory usage, but always\n  terminate with a correct result in a bounded amount of time, and\n  <strong>probabilistic algorithms</strong>, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo\n  algorithms) or fail to produce a result (Las Vegas algorithms) either\n  by signalling a failure or failing to terminate.</p>\n</blockquote>\n\n<ol>\n<li>I was wondering how the first kind of "<strong>algorithms</strong> use the random\ninput to reduce the expected running time or memory usage, but\nalways  terminate with a correct result in a bounded amount of time?</li>\n<li>What differences are between it and Las Vegas algorithms which may\nfail to produce a result?</li>\n<li>If I understand correctly,  probabilistic algorithms and randomized algorithms are not the same concept. Probabilistic algorithms are just one\nkind of randomized algorithms, and the other kind is those use the\nrandom  input to reduce the expected running time or memory usage,\nbut always  terminate with a correct result in a bounded amount of\ntime?</li>\n</ol>\n', 'Tags': '<algorithms><terminology><randomized-algorithms><nondeterminism><machine-models>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-04-07T14:10:29.767', 'CommentCount': '0', 'AcceptedAnswerId': '6222', 'CreationDate': '2012-10-22T00:53:00.617', 'Id': '6221'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '157', 'Title': 'Are randomized algorithms constructive?', 'LastEditDate': '2012-10-25T11:05:48.563', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '336', 'FavoriteCount': '1', 'Body': '<p>From , the proofs by the probabilistic method are often said to be non-constructive.</p>\n\n<p>However,  a proof by probabilistic method indeed designs a randomized algorithm and uses it for proving existence. Quoted from p103 of <a href="http://books.google.com/books?id=QKVY4mDivBEC&amp;pg=PR5&amp;lpg=PP1&amp;dq=randomized%20algorithms#v=onepage&amp;q&amp;f=false">Randomized Algorithms\n By Rajeev Motwani, Prabhakar Raghavan</a>:</p>\n\n<blockquote>\n  <p>We could view the proof  by the probabilistic method as a randomized\n  algorithm. This would then require a further analysis bounding the\n  probability that the algorithm fails to find a good partition on a\n  given execution. The main difference between a thought experiment in\n  the probabilistic method and a randomized algorithm is the end that\n  each yields. When we use the probabilistic method, we are only\n  concerned with showing that a combinatorial object exists; thus, we\n  are content with showing that a favorable event occurs with non-zero\n  probability. With a randomized algorithm, on the other hand,\n  efficiency is an important consideration - we cannot tolerate a\n  miniscule success probability.</p>\n</blockquote>\n\n<p>So I wonder if randomized algorithms are viewed as not constructive, although they do output a solution at the end of each run, which may or may not be an ideal solution.</p>\n\n<p>How is an algorithm or proof being "constructive" defined?</p>\n\n<p>Thanks!</p>\n', 'Tags': '<algorithms><terminology><randomized-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-30T16:24:48.083', 'CommentCount': '5', 'AcceptedAnswerId': '6289', 'CreationDate': '2012-10-24T12:19:26.950', 'Id': '6288'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>We have a stochastic random source. This gives the bit $0$ (or $1$) with probability $1/2$. \nWe want to generate a uniform distribution on the set S = $\\{0, 1,..., n-1\\}$. </p>\n\n<p>Which algorithm gives with probability $1/n$ the value $i\\in S$. And how many bits are needed.</p>\n', 'ViewCount': '47', 'Title': 'Stochastical algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-06T10:58:11.537', 'LastEditDate': '2012-11-06T10:35:24.340', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><random><sampling>', 'CreationDate': '2012-11-06T09:57:05.557', 'Id': '6507'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '439', 'Title': 'How asymptotically bad is naive shuffling?', 'LastEditDate': '2012-11-06T20:49:40.330', 'AnswerCount': '2', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '242', 'FavoriteCount': '3', 'Body': "<p>It's well-known that this 'naive' algorithm for shuffling an array by swapping each item with another randomly-chosen one doesn't work correctly:</p>\n\n<pre><code>for (i=0..n-1)\n  swap(A[i], A[random(n)]);\n</code></pre>\n\n<p>Specifically, since at each of $n$ iterations, one of $n$ choices is made (with uniform probability), there are $n^n$ possible 'paths' through the computation; because the number of possible permutations $n!$ doesn't divide evenly into the number of paths $n^n$, it's impossible for this algorithm to produce each of the $n!$ permutations with equal probability.  (Instead, one should use the so-called <em>Fischer-Yates</em> shuffle, which essentially changes out the call to choose a random number from [0..n) with a call to choose a random number from [i..n); that's moot to my question, though.)</p>\n\n<p>What I'm wondering is, how 'bad' can the naive shuffle be?  More specifically, letting $P(n)$ be the set of all permutations and $C(\\rho)$ be the number of paths through the naive algorithm that produce the resulting permutation $\\rho\\in P(n)$, what is the asymptotic behavior of the functions </p>\n\n<p>$\\qquad \\displaystyle M(n) = \\frac{n!}{n^n}\\max_{\\rho\\in P(n)} C(\\rho)$ </p>\n\n<p>and </p>\n\n<p>$\\qquad \\displaystyle m(n) = \\frac{n!}{n^n}\\min_{\\rho\\in P(n)} C(\\rho)$?  </p>\n\n<p>The leading factor is to 'normalize' these values: if the naive shuffle is 'asymptotically good' then </p>\n\n<p>$\\qquad \\displaystyle \\lim_{n\\to\\infty}M(n) = \\lim_{n\\to\\infty}m(n) = 1$.  </p>\n\n<p>I suspect (based on some computer simulations I've seen) that the actual values are bounded away from 1, but is it even known if $\\lim M(n)$ is finite, or if $\\lim m(n)$ is bounded away from 0?  What's known about the behavior of these quantities?</p>\n", 'Tags': '<algorithms><algorithm-analysis><asymptotics><probability-theory><randomness>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-10T04:07:42.843', 'CommentCount': '13', 'AcceptedAnswerId': '6596', 'CreationDate': '2012-11-06T19:22:56.410', 'Id': '6519'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>There are $N$ number of people and $X$ amount of objects with different values. Each person will choose an object and will obtain that objects value. If multiple people choose the same object then the value of the object is shared among the people. For example, If there are 2 objects and 3 people, one of the objects will be chosen by at least 2 people, thus the value of the object is divided over 2. </p>\n\n<p>Each round the people make their decisions with new objects and new values, how can a person maximize their expected values accumulated from their choices in Q number of rounds, in order to win(i.e accumulated most value)? ($Q$ belongs to [1, +infinite).</p>\n\n<p>Note that keeping track of other player's decisions and their accumulated values can help in future decision making.</p>\n\n<p>Note: An approximation would be great as well, so far my strategy is to choose a random object at each round, I am looking for ways to maximize this strategy. </p>\n", 'ViewCount': '142', 'Title': 'Guessing the best choice to maximize returns', 'LastEditorUserId': '4365', 'LastActivityDate': '2012-11-16T09:25:51.250', 'LastEditDate': '2012-11-16T08:50:52.197', 'AnswerCount': '2', 'CommentCount': '10', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4365', 'Tags': '<artificial-intelligence><randomized-algorithms>', 'CreationDate': '2012-11-15T21:33:25.423', 'Id': '6688'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume that we are given a real life graph, DBLP network in my case, where degree distribution of nodes follows a power law (many nodes have 1, 2 neighbors, and only a few nodes have hundreds of neighbors).</p>\n\n<p>A random walk ends when it returns to the initial node or when the walk takes 3 steps.\nIf we start random walks from each node on this graph, should we start equal number of walks from each node? If so, nodes with small degrees will often return to where they started, and we will not learn big portions of the network. This is because small degree nodes are neighbors of small degree nodes more often, so there will not be many paths to walk on.</p>\n\n<p>I believe there should be a way to decide on the number of walks to minimize computational costs. </p>\n', 'ViewCount': '174', 'Title': 'How many random walks to start from each node?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-11-22T03:31:55.453', 'LastEditDate': '2012-11-22T03:31:55.453', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4649', 'Tags': '<graph-theory><graphs><probability-theory><random-walks>', 'CreationDate': '2012-11-19T06:40:05.413', 'FavoriteCount': '1', 'Id': '6758'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $P$ be a transition matrix of a random walk in an undirected <strong>(may not regular)</strong> graph $G$. Let $\\pi$ be a distribution on $V(G)$. The Shannon entropy of $\\pi$ is defined by </p>\n\n<p>$$H(\\pi)=-\\sum_{v \\in V(G)}\\pi_v\\cdot\\log(\\pi_v).$$</p>\n\n<p>How do we prove that $H(P\\pi)\\ge H(\\pi)$ ?</p>\n', 'ViewCount': '79', 'Title': 'Increasing entropy of random walk', 'LastEditorUserId': '4706', 'LastActivityDate': '2012-11-24T10:17:50.253', 'LastEditDate': '2012-11-24T10:17:50.253', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6862', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4706', 'Tags': '<entropy><random-walks>', 'CreationDate': '2012-11-23T17:39:47.947', 'Id': '6859'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '164', 'Title': 'Randomized Rounding of Solutions to Linear Programs', 'LastEditDate': '2012-11-30T04:10:17.983', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '19', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/Linear_programming#Integral_linear_programs" rel="nofollow">Integer linear programming</a> (ILP) is an incredibly powerful tool in combinatorial optimization. If we can formulate some problem as an instance of an ILP then solvers are guaranteed to find the global optimum. However, enforcing integral solutions has runtime that is exponential in the worst case. To cope with this barrier, several approximation methods related to ILPs can be used,</p>\n\n<ul>\n<li>Primal-Dual Schema</li>\n<li>Randomized Rounding</li>\n</ul>\n\n<p>The Primal-Dual Schema is a versatile method that gives us a "packaged" way to come up with a greedy algorithm and prove its approximation bounds using the relaxed dual LP. Resulting combinatorial algorithms tend to be very fast and perform quite well in practice. However its relation to linear programming is closer tied to the analysis. Further because of this analysis, we can easily show that constraints are not violated.</p>\n\n<p>Randomized rounding takes a different approach and solves the relaxed LP (using interior-point or ellipsoid methods) and rounds variables according to some probability distribution. If approximation bounds can be proven this method, like the Primal-Dual schema, is quite useful. However, one portion is not quite clear to me:</p>\n\n<blockquote>\n  <p>How do randomized rounding schemes show that constraints are not violated?</p>\n</blockquote>\n\n<p>It would appear that naively flipping a coin, while resulting in a 0-1 solution, could violate constraints! Any help illuminating this issue would be appreciated. Thank you.</p>\n', 'Tags': '<optimization><randomized-algorithms><linear-programming><approximation>', 'LastEditorUserId': '19', 'LastActivityDate': '2012-11-30T04:10:17.983', 'CommentCount': '4', 'AcceptedAnswerId': '6949', 'CreationDate': '2012-11-27T05:05:34.050', 'Id': '6941'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a graph $G = (V,E)$, where $|V| = n$. What is a fast algorithm for generating the collection of all 2-hop neighborhood lists of all nodes in $V$. </p>\n\n<p>Naively, you can do that in $O(n^3)$. With power of matrices, you can do that with $O(n^{2.8})$ using Strassen algorithm. You can do better than this using another matrix multiplication algorithm. Any better method ? Any Las Vegas algorithm ? </p>\n', 'ViewCount': '422', 'Title': 'Algorithm to find all 2-hop neighbors lists in a graph', 'LastActivityDate': '2012-12-06T15:48:01.947', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '7183', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '867', 'Tags': '<algorithms><algorithm-analysis><graphs><randomized-algorithms>', 'CreationDate': '2012-12-05T04:50:09.537', 'Id': '7169'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Technical question: is there any open source program/code/library which can compute (minimal) conductance of a given graph, probably by some simulated annealing?</p>\n\n<p>Think it is quite well-known problem, but I cannot find anything like I mentioned above, but maybe you guys know? I know that it is a NP-hard task, but anyway, probably someone has written some programs.</p>\n', 'ViewCount': '82', 'Title': 'Graph conductance - program/code/library', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-03T22:28:30.283', 'LastEditDate': '2013-06-03T22:28:30.283', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'Wojtek', 'PostTypeId': '1', 'Tags': '<graph-theory><random-walks>', 'CreationDate': '2012-12-08T16:29:39.240', 'Id': '7268'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Count-Min_sketch">Count-Min Sketch</a> is an awesome data structure for estimating the frequencies of different elements in a data stream.  Intuitively, it works by picking a variety of hash functions, hashing each element with those hash functions, and incrementing the frequencies of various slots in various tables.  To estimate the frequency of an element, the Count-Min sketch applies the hash functions to those elements and takes the minimum value out of all the slots that are hashed to.</p>\n\n<p>The <a href="http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf">original paper on the Count-Min Sketch</a> mentions that the data structure requires pairwise independent hash functions in order to get the necessary guarantees on its expected performance.  However, looking over the structure, I don\'t see why pairwise independence is necessary.  Intuitively, I would think that all that would be required would be that the hash function be <a href="http://en.wikipedia.org/wiki/Universal_hashing">a universal hash function</a>, since universal hash functions are hash functions with low probabilities of collisions.  The analysis of the collision probabilities in the Count-Min Sketch looks remarkably similar to the analysis of collision probabilities in a chained hash table (which only requires a family of universal hash functions, not pairwise independent hash functions), and I can\'t spot the difference in the analyses.</p>\n\n<p>Why is it necessary for the hash functions in the Count-Min Sketch to be pairwise independent?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '467', 'Title': 'Why does the Count-Min Sketch require pairwise independent hash functions?', 'LastActivityDate': '2012-12-09T21:05:41.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7279', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><data-structures><randomized-algorithms><hash>', 'CreationDate': '2012-12-09T19:52:37.693', 'Id': '7275'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For a random undirected graph with $n$ nodes, where each node has $k$ incident edges ($nk/2$ edges in total), the vertex set is  partitioned into two sets each having $n/2$ nodes.  </p>\n\n<blockquote>\n  <p>What is the order of the number of edges that start in one partition and end in the other?  </p>\n</blockquote>\n\n<p>My back of the napkin calculation is half of total number of edges, $nk/4$. If I place randomly an edge it has $1/4$ chance of having both ends in one partition, $1/4$ chance of having both ends in the other partition and $1/2$ chance of having the ends in both. I find it surprising that it could be half.</p>\n', 'ViewCount': '52', 'Title': 'Mean number of edges between two equal partitions', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-16T19:05:20.267', 'LastEditDate': '2012-12-16T19:05:20.267', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7435', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5081', 'Tags': '<graph-theory><graphs><random-graphs>', 'CreationDate': '2012-12-16T10:12:42.143', 'Id': '7433'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In their book <a href="http://www.amazon.co.uk/Randomized-Algorithms-Cambridge-International-Computation/dp/0521474655/ref=sr_1_1?ie=UTF8&amp;qid=1356380410&amp;sr=8-1"><em>Randomized Algorithms</em>,</a> Motwani and Raghavan open the introduction with a description of their RandQS function -- Randomized quicksort -- where the pivot, used for partitioning the set into two parts, is chosen at random.</p>\n\n<p>I have been racking my (admittedly somewhat underpowered) brains over this for some time, but I haven\'t been able to see what advantage this algorithm has over simply picking, say, the middle element (in index, not size) each time.</p>\n\n<p>I suppose what I can\'t see is this: if the initial set is in a random order, what is the difference between picking an element at a random location in the set and picking an element at a fixed position?</p>\n\n<p>Can someone enlighten me, in fairly simple terms? </p>\n', 'ViewCount': '1069', 'Title': 'What is the advantage of Randomized Quicksort?', 'LastActivityDate': '2014-05-03T22:52:13.613', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '7583', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '5178', 'Tags': '<algorithm-analysis><sorting><randomized-algorithms>', 'CreationDate': '2012-12-24T20:26:07.713', 'FavoriteCount': '1', 'Id': '7582'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '432', 'Title': 'Discrepancy between heads and tails', 'LastEditDate': '2012-12-26T14:53:11.573', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '5195', 'FavoriteCount': '4', 'Body': "<p>Consider a sequence of $n$ flips of an unbiased coin. Let $H_i$ denote the absolute value of the excess of the number of heads over tails seen in the first $i$ flips. Define $H=\\text{max}_i H_i$. Show that $E[H_i]=\\Theta ( \\sqrt{i} )$ and $E[H]=\\Theta( \\sqrt{n} )$. </p>\n\n<p>This problem appears in the first chapter of `Randomized algorithms' by Raghavan and Motwani, so perhaps there is an elementary proof of the above statement. I'm unable to solve it, so I would appreciate any help.</p>\n", 'Tags': '<probability-theory><randomized-algorithms>', 'LastEditorUserId': '3011', 'LastActivityDate': '2012-12-31T01:55:08.807', 'CommentCount': '0', 'AcceptedAnswerId': '7601', 'CreationDate': '2012-12-26T07:03:36.157', 'Id': '7600'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '206', 'Title': 'randomized algorithm for checking the satisfiability of s-formulas, that outputs the correct answer with probability at least $\\frac{2}{3}$', 'LastEditDate': '2013-01-05T20:18:01.783', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '1', 'Body': "<p>I'm trying to practice myself with random algorithms.</p>\n\n<p>Lets call a CNF formula over n variables s-formula if it is either unsatisable or it has at\nleast $\\frac{2^n}{n^{10}}$ satisfying assignments.</p>\n\n<p>I would like your help with show a randomized algorithm for checking the\nsatisfiability of s-formulas, that outputs the correct answer with probability at\nleast $\\frac{2}{3}$.</p>\n\n<p>I'm not really sure how to prove it. First thing that comes to my head is this thing- let's accept with probability $\\frac{2}{3}$ every input. Then if the input in the language, it was accepted whether in the initial toss($\\frac{2}{3}$) or it was not and then the probability to accept it is $\\frac{1}{3}\\cdot proability -to-accept$ which is bigger than $\\frac{2}{3}$. Is this the way to do that or should I use somehow Chernoff inequality which I'm not sure how.</p>\n", 'Tags': '<complexity-theory><time-complexity><randomized-algorithms>', 'LastEditorUserId': '1589', 'LastActivityDate': '2013-01-05T20:18:01.783', 'CommentCount': '0', 'AcceptedAnswerId': '7748', 'CreationDate': '2012-12-29T09:38:45.680', 'Id': '7641'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '414', 'Title': 'How can it be detected that a number generator is not really random?', 'LastEditDate': '2013-12-31T20:49:26.480', 'AnswerCount': '5', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '4409', 'FavoriteCount': '3', 'Body': "<p>I heard that random number generation in computers isn't really random, but there is no efficient algorithm to detect it. How can it be detected at all ? </p>\n", 'Tags': '<random><random-number-generator>', 'LastEditorUserId': '10637', 'LastActivityDate': '2014-01-03T06:32:42.663', 'CommentCount': '3', 'AcceptedAnswerId': '7742', 'CreationDate': '2013-01-03T11:58:09.193', 'Id': '7729'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm attempting to use the MT19937 variant of the Mersenne Twister PRNG to accomplish something.  Whether or not this something is feasible depends upon the answers to these two questions:</p>\n\n<p>What is the greatest value of <strong>m</strong> for which the following statements hold true:</p>\n\n<p>1 - For all seed values, the algorithm eventually produces every integer list of length <strong>m</strong>.</p>\n\n<p>2 - There exists a seed value for which the algorithm would eventually produce a given integer list of length <strong>m</strong>.</p>\n", 'ViewCount': '93', 'Title': 'Will the Mersenne Twister PRNG eventually produce all integer sequences of a certain length?', 'LastEditorUserId': '5308', 'LastActivityDate': '2013-01-05T18:42:00.607', 'LastEditDate': '2013-01-05T18:42:00.607', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5308', 'Tags': '<algorithms><pseudo-random-generators>', 'CreationDate': '2013-01-05T05:17:40.293', 'Id': '7775'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '241', 'Title': 'Concrete understanding of difference between PP and BPP definitions', 'LastEditDate': '2013-02-15T07:31:32.413', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5356', 'FavoriteCount': '3', 'Body': '<p>I am confused about how  <strong>PP</strong> and <strong>BPP</strong> are defined. Let us assume $\\chi$ is the characteristic function for a language $\\mathcal{L}$. <em>M</em> be the probabilistic Turing Machine. Are the following definitions correct:<br>\n$BPP =\\{\\mathcal{L} :Pr[\\chi(x) \\ne M(x)] \\geq \\frac{1}{2} + \\epsilon \\quad \\forall x \\in \\mathcal{L},\\ \\epsilon &gt; 0 \\}$<br>\n$PP =\\{\\mathcal{L} :Pr[\\chi(x) \\ne M(x)] &gt; \\frac{1}{2} \\}$  </p>\n\n<p>If the definition are wrong, please try to make minimal change to make them correct (i.e. do not give other equivalent definition which use counting machine or some modified model). I can not properly distinguish the conditions on probability on both the definitions.  </p>\n\n<p>Some concrete examples with clear insight into the subtle points would be very helpful. </p>\n', 'Tags': '<complexity-theory><terminology><randomized-algorithms><complexity-classes>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-15T07:31:32.413', 'CommentCount': '0', 'AcceptedAnswerId': '7849', 'CreationDate': '2013-01-09T11:28:04.633', 'Id': '7848'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I try to figure out <strong>a redundant power of two-sided error randomized Karp - reduction.</strong></p>\n\n<p>It\'s well known fact and it is relatively hard to show that <a href="http://en.wikipedia.org/wiki/BPP_(complexity)" rel="nofollow">BPP</a> is reducible by a one-sided error randomized Karp-reduction to coRP (in case of promise problem).</p>\n\n<p>Without delving into details it make sense that the combination of the one - sided error probability of the reduction and the one-sided error probability of coRP leads to two-sided error probability of BPP. Of course the proof of that is not so intuitive.</p>\n\n<p>The question it is possible by two-sided error randomized Karp-reduction to reduce BPP to some constant set in P? In the light of the power of one - sided randomized Karp - reduction, it make sense that two-sided randomized Karp - reduction is strong enough to reduce BPP to constant set, but how to show it formally?</p>\n\n<p><strong>Addendum:</strong></p>\n\n<p><strong>BPP</strong> is the set of the problems that is solvable in polynomial time by two-sided error randomized algorithm, so as a result of two - sided error randomized algorithm we will get some output, them the problem in BPP can be reduced to problem P by two-sided error randomized Karp - reduction in sense that reduction is allowed to make error on both sides. Does it mean that two - sided error randomized reduction will justify the two-sided error that was made by the algorithm in solving the problem in BPP?</p>\n', 'ViewCount': '97', 'Title': 'The Power of Randomized Reduction', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-10-05T07:50:18.583', 'LastEditDate': '2013-01-21T09:54:55.740', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<complexity-theory><reductions><randomized-algorithms>', 'CreationDate': '2013-01-19T09:56:05.707', 'Id': '9037'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to solve exercise 6.5 on page 309 from Richard Crandall\'s "Prime numbers - A computational perspective". It basically asks for an algorithm to factor integers in randomized polynomial time given an oracle for taking square roots modulo $n$.</p>\n\n<p>I think, the basic idea is the following: Given a composite number $n$, to take a random element $r$ in $\\left.\\mathbb{Z}\\middle/n\\mathbb{Z}\\right.$ and square it. If $r$ was a square, $r^2$ can have up to $4$ different square roots and the basic idea of the algorithm is that the oracle has some chance not to choose $\\pm r$, but one of the other two roots. It will turn out that we then can determine a factor of $n$ using Euclidean\'s algorithm. </p>\n\n<p>I formalized this to</p>\n\n<p><strong>Input</strong>: $n=pq\\in\\mathbb{Z}$ with primes $p$ and $q$.</p>\n\n<p><strong>Output</strong>: $p$ or $q$</p>\n\n<ol>\n<li>Take a random number $r$ between $1$ and $n-1$</li>\n<li>If $r\\mid n$ then return $r$ (we were lucky)</li>\n<li>$s:= r^2\\pmod{n}$</li>\n<li>$t:=\\sqrt{s}\\pmod{n}$ (using the oracle)</li>\n<li>If $t\\equiv \\pm r\\pmod{n}$ then goto step 1.</li>\n<li>Return $\\gcd(t-r,n)$</li>\n</ol>\n\n<p>One can show that $t \\not\\equiv \\pm r\\pmod{n}$ implies that $\\gcd(t-r,n)\\neq 1,n$ and therefore get that the return value of the algorithm is a non-trivial factor of $n$. </p>\n\n<p>Inspired by my main question "How do I prove that the running time is polynomial in the bit-size of the input?" I have some follow up questions:</p>\n\n<ol>\n<li>Do I have to show that a lot of numbers between $1$ and $n-1$ are squares? There must be a well-known theorem or easy fact that shows this (well... not well-known to me ;-). </li>\n<li>Are there any more details I have consider? </li>\n<li>Has every square of a square exactly $4$ square roots modulo $n$? </li>\n</ol>\n', 'ViewCount': '109', 'Title': 'Solve Integer Factoring in randomized polynomial time with an oracle for square root modulo $n$', 'LastActivityDate': '2013-01-23T15:57:01.553', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9114', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2103', 'Tags': '<randomized-algorithms><integers><factoring>', 'CreationDate': '2013-01-23T09:39:28.187', 'FavoriteCount': '1', 'Id': '9106'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The ideal random permutation algorithm of <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#The_modern_algorithm" rel="nofollow">Fisher and Yates</a> (Algorithm P in Knuth vol.2) for a sequence of $n$ objects requires $n-1$ random numbers. </p>\n\n<p>In some card games one first does a "cut" and then a ripple shuffle. The cutting point is a random value, the subsequent shuffling could be considered as deterministic. That is, only one random number is being used to generate the permutation, which understandably can\'t be ideal. On the other hand, theoretical perfection isn\'t always necessary in practice. I like hence to know whether, if one keeps the constraint of using one random number in a run, the quality of randomness of the permutation obtained couldn\'t eventually be improved through certain appropriate modifications of the procedure commonly employed in card games, if one is willing to take the trade-off of more work/time, inconvenience, etc. Such trade-offs may not be acceptable for real games, but I suppose there may be other practical applications that could advantageously exploit the same idea, thus without being required to acquire, e.g. via a chosen PRNG, the larger number of random numbers needed for executing the algorithm of Fisher and Yates.</p>\n', 'ViewCount': '184', 'Title': 'Best random permutation employing only one random number', 'LastEditorUserId': '6437', 'LastActivityDate': '2013-01-28T16:06:34.070', 'LastEditDate': '2013-01-28T11:28:18.590', 'AnswerCount': '2', 'CommentCount': '12', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6437', 'Tags': '<randomness><random><sampling><permutations>', 'CreationDate': '2013-01-27T10:54:47.110', 'Id': '9199'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What I only got currently from PCP theorem is that it needs at most $O(\\log n)$ randomness and $O(1)$ query of proof to approximate. So how does this result relate to the fact that solution to NP problems are hard to approximate?</p>\n', 'ViewCount': '84', 'Title': 'Why does PCP theorem imply that NP problems are hard to approximate?', 'LastActivityDate': '2013-03-05T19:11:45.947', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10299', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7154', 'Tags': '<complexity-theory><randomized-algorithms>', 'CreationDate': '2013-03-05T12:56:15.360', 'Id': '10289'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Inspired by <a href="http://cs.stackexchange.com/q/2336/71">this question</a> in which the asker wants to know if the running time changes when the comparator used in a standard search algorithm is replaced by a fair coin-flip, and also <a href="http://www.robweir.com/blog/2010/02/microsoft-random-browser-ballot.html">Microsoft\'s</a> prominent failure to write a uniform permutation generator, my question is thus:</p>\n\n<p>Is there a comparison based sorting algorithm which will, depending on our implementation of the comparator:</p>\n\n<ol>\n<li>return the elements in sorted order when using a <em>true</em> comparator (that is, the comparison does what we expect in a standard sorting algorithm) </li>\n<li>return a uniformly random permutation of the elements when the comparator is replaced by a fair coin flip (that is, return <code>x &lt; y = true</code> with probability 1/2, regardless of the value of x and y)</li>\n</ol>\n\n<p>The code for the sorting algorithm must be the same. It is only the code inside the comparison "black box" which is allowed to change.</p>\n', 'ViewCount': '296', 'Title': 'Is there a "sorting" algorithm which returns a random permutation when using a coin-flip comparator?', 'LastActivityDate': '2013-03-23T11:27:52.923', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '10658', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<sorting><randomized-algorithms><permutations>', 'CreationDate': '2013-03-20T18:14:44.773', 'Id': '10656'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was wondering since randomness is embedded in genetic algorithms at almost every level, is there a really fine line between genetic algorithms and pure random search?</p>\n\n<p>Ever since I finished my implementation of a GA , since randomness is present in the mutation function,the initialization part (as well as the reinitialization part) and crossbreeding part as well... other than a encoder which tries to sense of the chromsomes (encoder tailored to make sense of the chromosome in context of the problem) and a fitness function , it feels like genetic algorithms are just random search functions in disguise .</p>\n\n<p>So my question is : are GA implementations just plain old random searches with a shot of memory to make it look like there is some sort of meaningful feedback? </p>\n', 'ViewCount': '108', 'ClosedDate': '2013-04-02T22:15:50.887', 'Title': 'Are genetic algorithms special instances of random search done in an unexpectedly short run-time?', 'LastEditorUserId': '7545', 'LastActivityDate': '2013-04-02T21:07:18.593', 'LastEditDate': '2013-04-02T20:35:30.713', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7545', 'Tags': '<search-algorithms><efficiency><randomness><evolutionary-computing><genetic-algorithms>', 'CreationDate': '2013-04-02T15:57:17.913', 'Id': '10975'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m a bit confused about the definition of BPP. The way BPP is defined in typical text books (Arora/Barak for example) is that if M(x) is a Probabilistic Turing Machine (PTM) that recognizes a language $L(x)$, then $Pr[M(x)=L(x)]&gt; 2/3$. My question is, what is the probability taken over? Arora/Barak remark (<a href="http://www.cs.princeton.edu/theory/complexity/bppchap.pdf" rel="nofollow">7.2</a>) that the probability is taken over internal coin tosses of $M(x)$, i.e., fix a value of $x$, and run all possible $2^{T(|x|)}$ experiments of internal coin tosses, and compute the majority of accept state. But if this is true, then Amplification theorem cannot hold because by definition if the probability is computed by executing all $2^{T(|x|)}$ possible coin-flips, then no matter how many times I run the algorithm, the probability is not going to change. (For example, if I have a bag with 2 red balls and 1 blue ball, then no matter how many times I pick a ball from the bag (and return it), the probability of picking a red ball is going to remain 2/3.)</p>\n\n<p>Basically, a PTM is a random process in two variables: The input string $x \\in \\{0,1\\}^*$ and random coin tosses $ r \\in \\{0,1\\}^{T(|x|)}$. For the amplification theorem to hold, I think one needs to fix a value of $r$ and run the machine on all values of $x$, and compute $Pr[M(x) = L(x)]$. Then for a fixed $x$, running $M(x)$ multiple times will have amplification effect, but if the probability is computed over internal coin tosses, then the Amplification theorem cannot hold.</p>\n\n<p>What am I misunderstanding here?</p>\n', 'ViewCount': '94', 'Title': 'Accurate definition of BPP', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-08T14:39:16.167', 'LastEditDate': '2013-04-08T14:39:16.167', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'OwnerDisplayName': 'MachPortMassenger', 'PostTypeId': '1', 'Tags': '<complexity-theory><terminology><complexity-classes><randomness>', 'CreationDate': '2013-04-07T03:51:38.153', 'FavoriteCount': '1', 'Id': '11119'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From <a href="http://programmers.stackexchange.com/questions/194552/what-is-the-difference-between-quantum-annealing-and-simulated-annealing">that question</a> about differences between Quantum annealing and simulated annealing, we found (in commets to answer) that physical implementation of quantum annealing is exists (D-Wave quantum computers).</p>\n\n<p>Can anyone explain that algorithm in terms of quantum gates and quantum algorithms, or in physical terms (a part of algorithm that depends on quantum hardware)?</p>\n\n<p>Does anyone have any ideas about that?\nPlease tell me, if you know some links related this question.</p>\n', 'ViewCount': '419', 'Title': 'The physical implementation of quantum annealing algorithm', 'LastActivityDate': '2013-04-19T20:20:45.790', 'AnswerCount': '4', 'CommentCount': '3', 'AcceptedAnswerId': '11362', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7672', 'Tags': '<algorithms><randomized-algorithms><quantum-computing>', 'CreationDate': '2013-04-11T07:05:16.517', 'FavoriteCount': '2', 'Id': '11218'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Cormen talks briefly about the advantages of picking a random pivot in quicksort.  However as pointed out <a href="http://algoviz.org/OpenDSA/Books/OpenDSA/html/Quicksort.html" rel="nofollow">here</a>(4th to the last paragraph):</p>\n\n<blockquote>\n  <p>Using a random number generator to choose the positions is relatively\n  expensive</p>\n</blockquote>\n\n<p>So how is picking a random pivot actually implemented in practice, and how random is it?  It can\'t be too expensive, since from what I understand one of quicksort\'s main advantages over other $\\cal{O}(n \\lg n)$ sorts is the small constant factors, and spending allot of cycles picking pivots would undermine that advantage.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>As an example, the <code>C</code> code from "<a href="http://www.catonmat.net/blog/three-beautiful-quicksorts/" rel="nofollow">Three Beautiful Quicksorts</a>" actually calls the <code>C</code> library <code>rand</code> function:</p>\n\n<pre><code>int randint(int l, int u) {\n    return rand()%(u-l+1)+l;\n}\n\nvoid quicksort(int l, int u) {\n    int i, m;\n    if (l &gt;= u) return;\n    swap(l, randint(l, u));\n    m = l;\n    for (i = l+1; i &lt;= u; i++)\n        if (x[i] &lt; x[l])\n            swap(++m, i);\n    swap(l, m);\n    quicksort(l, m-1);\n    quicksort(m+1, u);\n}\n</code></pre>\n\n<p>While the pivot picking code here is clearly $\\cal{O}(1)$, it would seem that the hidden $c$ here is relatively high.</p>\n', 'ViewCount': '207', 'Title': 'From Whence the Randomization in Randomized Quicksort', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-23T09:45:57.250', 'LastEditDate': '2013-04-21T14:02:28.190', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '11387', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<algorithms><randomized-algorithms><quicksort>', 'CreationDate': '2013-04-18T18:04:50.873', 'Id': '11385'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>It is well known that the efficiency of randomized algorithms (at least those in BPP and RP) depends on the quality of the random generator used. Perfect random sources are unavailable in practice. Although it is proved that for all $0 &lt; \\delta \\leq \\frac{1}{2}$ the identities BPP = $\\delta$-BPP and RP = $\\delta$-RP hold, it is not true that the original algorithm used for a prefect random source can be directly used also for a $\\delta$-random source. Instead, some simulation has to be done. This simulation is polynomial, but the resulting algorithm is not so efficient as the original one.</p>\n\n<p>Moreover, as to my knowledge, the random generators used in practice are usually not even $\\delta$-sources, but pseudo-random sources that can behave extremely badly in the worst case.</p>\n\n<p>According to <a href="http://en.wikipedia.org/wiki/Randomized_algorithm" rel="nofollow" title="Wikipedia">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.</p>\n</blockquote>\n\n<p>In fact, the implementations of randomized algorithms that I have seen up to now were mere implementations of the algorithms for perfect random sources run with the use of pseudorandom sources.</p>\n\n<p>My question is, if there is any justification of this common practice. Is there any reason to expect that in most cases the algorithm will return a correct result (with the probabilities as in BPP resp. RP)? How can the "approximation" mentioned in the quotation from Wikipedia be formalized? Can the deviation mentioned be somehow estimated, at least in the expected case? Is it possible to argue that a Monte-Carlo randomized algorithm run on a perfect random source will turn into a well-behaved stochastic algorithm when run on a pseudorandom source? Or are there any other similar considerations?</p>\n', 'ViewCount': '92', 'Title': 'Random generator considerations in the design of randomized algorithms', 'LastActivityDate': '2013-05-02T22:23:28.033', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '11744', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2091', 'Tags': '<randomized-algorithms><randomness><pseudo-random-generators>', 'CreationDate': '2013-05-02T10:41:01.760', 'Id': '11726'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Could someone please provide a reference giving an algorithm to generate uniformly random binary trees?</p>\n', 'ViewCount': '378', 'Title': 'How to generate uniformly random binary trees?', 'LastActivityDate': '2013-06-06T22:33:38.807', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6437', 'Tags': '<trees><random>', 'CreationDate': '2013-05-07T19:17:27.750', 'Id': '11862'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm taking a grad level randomized algorithms course in the fall. The professor is known for being very detail oriented and mathematically rigorous, so I will be required to have an in-depth understanding of probability. What would be a good probability book to learn from that would be intuitive, but also have some mathematical rigor to it?</p>\n", 'ViewCount': '79', 'Title': 'Randomized Algorithms Probability', 'LastActivityDate': '2013-05-19T13:03:22.573', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12133', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8242', 'Tags': '<probability-theory><randomized-algorithms>', 'CreationDate': '2013-05-18T14:20:52.533', 'Id': '12113'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '724', 'Title': 'What randomness really is', 'LastEditDate': '2013-05-19T17:50:20.767', 'AnswerCount': '8', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '8255', 'FavoriteCount': '4', 'Body': '<p>I\'m a Computer Science student and am currently enrolled in System Simulation &amp; Modelling course. It involves dealing with everyday systems around us and simulating them in different scenarios by generating random numbers in different distributional curves, like IID, Gaussian etc. for instance. I\'ve been working on the boids project and a question just struck me that what exactly "random" really is? I mean, for instance, every random number that we generate, even in our programming languages like via the <code>Math.random()</code> method in Java, essentially is generated following an "algorithm".</p>\n\n<p>How do we really know that a sequence of numbers that we produce is in fact, random and would it help us, to simulate a certain model as accurately as possible?</p>\n', 'Tags': '<simulation><randomness><modelling>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-17T01:36:33.863', 'CommentCount': '2', 'AcceptedAnswerId': '12137', 'CreationDate': '2013-05-19T16:49:58.083', 'Id': '12136'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been recently studying Monte-Carlo and other randomized methods for a lot of applications, and one that popped into my mind was making an (approximate) convex hull by examining random points, and try to get them inside the convex hull. I would like to know if there are algorithms for convex hulls that can improve the $O(n \\log n)$ bound of comparison based algorithms, and the $O(n\\cdot h)$ bound for Jarvis march and related to $O(n)$, either by building an approximate convex hull in $O(n)$ (with or without some approximation criteria) or by building an exact convex hull in expected linear time.</p>\n", 'ViewCount': '43', 'Title': 'Randomized convex hull', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-05-21T23:02:42.530', 'LastEditDate': '2013-05-21T19:34:43.807', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12208', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2138', 'Tags': '<algorithms><asymptotics><computational-geometry><randomized-algorithms>', 'CreationDate': '2013-05-21T19:30:47.060', 'Id': '12199'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m studying the <a href="http://en.wikipedia.org/wiki/PCP_theorem" rel="nofollow">PCP theorem</a>. </p>\n\n<p>While it is easy to prove that $\\mathsf{P}=\\text{PCP}(O(\\log n),0)$ , proving that $\\text{PCP}(O(\\log n),1)\\subseteq \\mathsf{P}$  i.e. PCP that uses $O(\\log n)$ random bits and read 1 bit of the proof is less obvious, what I tried to do is to take some proof $\\pi$ of length $n^{O(1)}$ (because effectively the message sent by the prover is bounded by $2^{r(n)}q(n)=2^{O(\\log n)}=n^{O(1)}$) then try all the coin tosses each time the verifier read some bit of the message so if the proof is not correct we flip the bit in the proof!   </p>\n', 'ViewCount': '109', 'Title': 'Proving that $\\text{PCP}(O(\\log n),1)\\subseteq \\mathsf{P}$', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-03T06:59:43.993', 'LastEditDate': '2014-03-03T06:59:43.993', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12282', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7934', 'Tags': '<complexity-theory><randomness>', 'CreationDate': '2013-05-25T20:05:31.100', 'Id': '12276'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to understand how the Mersenne Twister random number generator works (in particular, the 32-bit TinyMT). I am still relatively new to the concept of RNG. As I read the source code, I noticed there were two ways to seed the MT: with a single integer seed or an array of integer seeds. What is the point in seeding with an array? Does it produce a better distribution or a longer period? </p>\n\n<p>Also, I would appreciate it if somebody could explain to me what is meant by the "state" of the RNG, as I am seeing that word all over the source code. Is it like a finite state machine in a way? </p>\n\n<p>Thanks for your time!</p>\n', 'ViewCount': '152', 'Title': 'Seeding the Mersenne Twister Random Number Generator', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-24T14:19:28.083', 'LastEditDate': '2014-03-24T14:19:28.083', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<algorithms><randomized-algorithms><pseudo-random-generators><random-number-generator>', 'CreationDate': '2013-06-20T16:09:56.607', 'Id': '12792'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to implement and optimize the <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/TINYMT/" rel="nofollow">Tiny Mersenne Twister (TinyMT)</a> algorithm as required by an API I am developing with my team at work. The algorithm utilizes a C structure with 32-bit unsigned integers "mat1", "mat2", "tmat", and an array called "status" which is four 32-bit unsigned integers wide.</p>\n\n<p>I am relatively new to the subject of random number generation; however, I have been able to teach myself a lot about the subject over the past couple of weeks. I know what the purpose of a seed is, different methods such as linear congruent, LSFR, GFSR, etc. So I\'ve been doing my "homework" and researching the topic to the best of my ability (contrary to what the guys at Stack Overflow think). Unfortunately, the Mersenne Twister in general is extremely poorly documented and very few documents exist to explain the code and math side-by-side. The TinyMT\'s documentation is even worse, it\'s virtually non-existent! So developing accurate Doxygen comments for this part of the API is going to be tricky.</p>\n\n<p>With that said, hopefully somebody more qualified than I can help me out here. What is the significance of the aforementioned parameters? What do they do, what do they mean, what do they stand for, etc? My guess would be as follows:</p>\n\n<ul>\n<li>mat1 - Matrix 1</li>\n<li>mat2 - Matrix 2</li>\n<li>tmat - Tempering Matrix</li>\n<li>status - 127 bit wide "seed" (where the last bit goes, I\'m not sure)</li>\n</ul>\n\n<p>Given that the user provides values for "mat1", "mat2", and "tmat," are there any precautions they need to take before supplying values for them? Again, this is for an API and its documentation, so I would like to be able to give the customers a good idea of what they need to use the RNG and hopefully make the lives of other fellow programmers easier. Thanks!</p>\n', 'ViewCount': '76', 'Title': 'Significance of parameters in Tiny Mersenne Twister algorithm', 'LastActivityDate': '2013-06-26T02:46:19.197', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12904', 'Score': '1', 'OwnerDisplayName': 'audiFanatic', 'PostTypeId': '1', 'OwnerUserId': '8783', 'Tags': '<linear-algebra><randomized-algorithms><matrices>', 'CreationDate': '2013-06-25T18:06:30.620', 'Id': '12902'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '236', 'Title': 'What is a good algorithm for generating random DFAs?', 'LastEditDate': '2013-06-29T08:40:23.377', 'AnswerCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8184', 'FavoriteCount': '1', 'Body': "<p>I am generating random DFAs to test a DFA reduction algorithm on them.</p>\n\n<p>The algorithm that I'm using right now is as follows: for each state $q$, for each symbol in the alphabet $c$, add $\\delta (q, c)$ to some random state.  Each state has the same probability of becoming a final state.</p>\n\n<p>Is this a good method of generating unbiased DFAs?  Also, this algorithm doesn't generate a trim DFA (a DFA with no obsolete states) so I'm wondering if there is a better way of generating random DFAs that can somehow ensure that it is trim?</p>\n", 'Tags': '<algorithms><finite-automata><random><pseudo-random-generators><random-graphs>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-18T10:13:05.943', 'CommentCount': '9', 'AcceptedAnswerId': '12949', 'CreationDate': '2013-06-28T05:14:15.410', 'Id': '12943'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From a very strictly adhering sense to the hardware and circuit-level operations of any standard (non-specialized, DSPs, or supercomputing systems, etc.) microprocessor follow very similar, almost exact in some ways, operations.</p>\n\n<p>The typical role of the (main) processor in a computer, integrated with other hardware circuits or not, and excluding DMA is to have a memory subsystem fetch byte(s) for it to "process" in whatever way. To have a processor "randomly" selective something can be abstracted and seen from a data algorithm <a href="http://en.wikipedia.org/wiki/High-level_programming_language" rel="nofollow">HLL-type</a> point of view, but on the circuit-level the operations can only get so complex. I know some Assembly of x86, so I can demonstrate further on the details of what I\'m asking.</p>\n\n<p>If you fetch a byte, or series of bytes, and then use some schematic to cycle through potential jump offsets, that is the only way to do randomness? Are their other ways?</p>\n', 'ViewCount': '39', 'Title': 'Speaking of "randomness" in computing terms, to what sense can any extant digital processor make "random" results?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-03T07:33:53.753', 'LastEditDate': '2013-07-03T07:33:53.753', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8963', 'Tags': '<randomized-algorithms><randomness>', 'CreationDate': '2013-07-01T21:46:50.027', 'Id': '13021'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>Let us consider the following game: there are some players and a computer. Each player inputs one positive integer and his name (player doesn't know another's numbers, just his own). When all the players made their moves, computer outputs a name of winner \u2013 who submitted the <em>lowest unique</em> number.</p>\n\n<p>How do you think, what is the best strategy for this game?</p>\n", 'ViewCount': '280', 'Title': 'Guessing the smallest unique positive integer', 'LastActivityDate': '2013-07-04T13:07:44.233', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13085', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '8991', 'Tags': '<game-theory><randomness>', 'CreationDate': '2013-07-03T13:25:27.730', 'Id': '13061'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $G$ be a $d$-regular expander graph. What is the electrical resistance of $G$? Is it a constant independent of the number of nodes $n$ once $d$ is large enough? If not, can we give matching upper and lower bounds in terms of $n,d$?</p>\n', 'ViewCount': '63', 'Title': 'Electrical resistance of expander graphs', 'LastActivityDate': '2013-07-08T04:58:52.567', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '13155', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9070', 'Tags': '<graph-theory><random-walks><expanders>', 'CreationDate': '2013-07-07T17:14:21.943', 'FavoriteCount': '1', 'Id': '13137'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>During my involvement in a course on dealing with NP-hard problems I have encountered the PCP theorem, stating</p>\n\n<p>$\\qquad\\displaystyle \\mathsf{NP} = \\mathsf{PCP}(\\log n, 1)$. </p>\n\n<p>I understand the technical definition of a PCP verifier, so I know in principle what kind of algorithm has to exist for every NP problem: a randomised algorithm that checks $O(1)$ bits of the given certificate for the given input using $O(\\log n)$ random bits, so that this algorithm is essentially a one-sided error Monte-Carlo verifier.</p>\n\n<p>However, I have trouble imagining how such an algorithm can deal with an NP-complete problem. Short of reading the proof of the PCP theorem, are there concrete examples for such algorithms?</p>\n\n<p>I skimmed the relevant sections of <a href="http://www.cs.princeton.edu/theory/complexity/" rel="nofollow">Computational Complexity: A Modern Approach</a> by Arora and Barak (2009) but did not find any.</p>\n\n<p>An example using a $\\mathsf{PCP}(\\_,\\ll n)$ algorithm would be fine.</p>\n', 'ViewCount': '220', 'Title': 'Example for a non-trivial PCP verifier for an NP-complete problem', 'LastActivityDate': '2013-07-19T09:24:04.750', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><complexity-theory><np-complete><approximation><randomized-algorithms>', 'CreationDate': '2013-07-12T11:10:36.380', 'Id': '13246'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a set $S$ of $k$ numbers in $[0, N)$.\nThe task is to randomly generate numbers in the range $[0, N)$ such that none belongs to $S$.</p>\n\n<p><strong>Edit</strong> - Also given an API to generate random numbers between $[0, N)$. We have to use it  to randomly generate numbers in the range $[0, N)$ such that none belongs to $S$.</p>\n\n<p>I would also like a generic strategy for such questions. Another one I came across was to generate random numbers between [0,7] given a random number generator that generates numbers in range [0, 5].</p>\n', 'ViewCount': '225', 'Title': 'Generate random numbers from an interval with holes', 'LastEditorUserId': '9166', 'LastActivityDate': '2013-07-17T16:03:32.970', 'LastEditDate': '2013-07-15T07:05:30.703', 'AnswerCount': '2', 'CommentCount': '6', 'AcceptedAnswerId': '13272', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9166', 'Tags': '<algorithms><probability-theory><sampling><random-number-generator>', 'CreationDate': '2013-07-14T13:12:52.417', 'Id': '13271'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>does $BPP\\subseteq P^{NP}$ ? it seems reasonable but I don't know if there is a proof of this!could any one post a proof or any material that discusses the statement or something that look like this .  </p>\n", 'ViewCount': '54', 'Title': 'BPP upper bound', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-17T05:48:37.233', 'LastEditDate': '2013-07-17T05:48:37.233', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13301', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8570', 'Tags': '<complexity-theory><complexity-classes><randomized-algorithms><oracle-machines>', 'CreationDate': '2013-07-16T12:50:19.600', 'Id': '13300'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose I want to write a classical software simulator of a quantum circuit with $N$ qubits.  When it comes time to simulate the quantum Fourier transform I can evaluate all $2^N$ states to determine the probability amplitudes, and then perform a Fast-Fourier Transform on the probability amplitudes in time $o(N 2^N)$.  Finally in $o(2^N)$ time I can generate a scan of the partial sums of the probabilities of all the result states.  Then I can choose a random number in the range $[0,1]$ and use it to do a binary search of the partial sums.</p>\n\n<p>This results in a simulator that, each time is run outputs a single $N$ bit binary number with the probability distribution predicted by theory.</p>\n\n<p>Can I do better?  Of course I can\'t do exponentially better in general, but perhaps I could reduce the time to simulate a single experiment to $o(2^N)$?</p>\n\n<p>I can do significantly better under some circumstances.  For example, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.7607">Gilbert, Guha, Indyk, Muthukrishnan, and Strauss, "Near-Optimal Sparse Fourier Representations via Sampling", <em>ACM Symp Theory Comp</em>, STOC-44:152-161, 2002,</a> seems to indicate that if there are only $B$ frequencies (or if the $B$ frequencies make up "most" of the power of the signal) then there is a randomized algorithm that will recover all of them (and their amplitudes) in time, space and number of samples polynomial in $B$ and $N$.</p>\n\n<p>I guess I\'m hoping for something like that, but only to get one frequency, and to have some guarantee that the probability of getting a particular frequency will be proportional to the amplitude of its coefficient.</p>\n', 'ViewCount': '61', 'Title': '(Slightly) faster simulation of quantum Fourier transform', 'LastActivityDate': '2013-07-18T03:50:14.887', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7459', 'Tags': '<randomized-algorithms><quantum-computing><fourier-transform>', 'CreationDate': '2013-07-18T03:50:14.887', 'FavoriteCount': '1', 'Id': '13325'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m studying randomized algorithms and I sometimes come across results like</p>\n\n<blockquote>\n  <p>(1) The algorithm has an expected $O(f(n))$ cost.</p>\n</blockquote>\n\n<p>and</p>\n\n<blockquote>\n  <p>(2) With constant probability, the cost is bounded by $O(f(n))$.</p>\n</blockquote>\n\n<p>I\'m perfectly fine with statements like (2), but I\'m puzzled to what extent a statement like (1) is useful: \nFor certain probability distributions of a random variable, the expected value itself occurs with less than constant probability; for other distributions, it occurs with $1-1/n$ probability. Of course, in many cases, (1) is extended via concentration bounds to show high probability, but in cases where this isn\'t done, it doesn\'t seem that a statement on the "expected cost" lets us derive any implications on the actual performance of the algorithm, right?</p>\n', 'ViewCount': '88', 'Title': 'Interpretation of "expected cost" of an algorithm', 'LastEditorUserId': '9398', 'LastActivityDate': '2013-07-29T15:43:02.297', 'LastEditDate': '2013-07-29T03:06:35.717', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9398', 'Tags': '<probability-theory><randomized-algorithms>', 'CreationDate': '2013-07-29T01:58:55.503', 'Id': '13484'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>While reading a cryptography textbook, i find the definition of a function that is hard on the average.(More precisely, it is 'hard on the average but easy with auxiliary input', but i omit latter for simplicity.)</p>\n\n<blockquote>\n  <p><strong>Definition : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>there exists</strong> a probabilistic polynomial-time algorithm $G$ such that<br>\n  for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>My question is why the statement of the existence of qualified algorithm G is sufficient? </p>\n\n<p>In other words, why the above definition gives a formal definition of 'hardness on the average' instead of following definition, which is more intuitive(?) to understand and more strict.\nWhy is the above definition sufficient? </p>\n\n<p>( Now I'm thinking that problem might occur when $G$ has only polynomial number of possible outputs, but if so, let's replace 'for any $G$' with 'for any $G$ which have exponentially many possible outputs' in following definition.)</p>\n\n<blockquote>\n  <p><strong>(strong?) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>for any</strong> probabilistic polynomial-time algorithm $G$ and for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>Another question is that whether a following simpler definition is equivalent to original definition or not?</p>\n\n<blockquote>\n  <p><strong>(simple) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(U_n)=h(U_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $U_n$ is a random variable uniformly distributed over $\\{0,1\\}^n$.</p>\n</blockquote>\n", 'ViewCount': '64', 'Title': "Completeness of formal definition of 'hardness on the average'", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-09T07:02:13.240', 'LastEditDate': '2013-08-09T07:02:13.240', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13678', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9565', 'Tags': '<complexity-theory><terminology><cryptography><randomized-algorithms><average-case>', 'CreationDate': '2013-08-08T12:43:21.820', 'Id': '13674'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have just used a function 'rand()' in my algorithm. In fact, it was arc4random() that I used. However, it got me thinking, how is randomness created in a computer system? \nCan anything ever truly be random when it is produced by a computer?</p>\n", 'ViewCount': '126', 'Title': "How do computers create 'randomness'?", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-26T08:02:26.080', 'LastEditDate': '2013-08-26T08:02:26.080', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9799', 'Tags': '<randomness>', 'CreationDate': '2013-08-23T17:53:37.550', 'FavoriteCount': '1', 'Id': '13893'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A shuffling algorithm is supposed to generate a random permutation of a given finite set. So, for a set of size $n$, a shuffling algorithm should return any of the $n!$ permutations of the set uniformly at random. </p>\n\n<p>Also, conceptually, a randomized algorithm can be viewed as a deterministic algorithm of the input and some random seed. Let $S$ be any shuffling algorithm. On input $X$ of size $n$, its output is a function of the randomness it has read. To produce $n!$ different outputs, $S$ must have read at least $\\log(n!) = \\Omega(n \\log n)$ bits of randomness. Hence, any shuffling algorithm must take $\\Omega(n \\log n)$ time.</p>\n\n<p>On the other hand, the <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle" rel="nofollow">Fisher-Yates shuffle</a> is <a href="http://www.codinghorror.com/blog/2007/12/the-danger-of-naivete.html" rel="nofollow">widely</a> <a href="http://c2.com/cgi/wiki?LinearShuffle" rel="nofollow">believed</a> to run in $O(n)$ time. Is there something wrong with my argument? If not, why is this belief so widespread?</p>\n', 'ViewCount': '161', 'Title': 'How can you shuffle in $O(n)$ time if you need $\\Omega(n \\log n)$ random bits?', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-08-29T08:52:52.413', 'LastEditDate': '2013-08-28T18:42:00.367', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9847', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><randomized-algorithms>', 'CreationDate': '2013-08-28T07:42:37.830', 'Id': '13990'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I would like to sample a uniformly random point in a polygon...</p>\n\n<p>If sample a large number they\'d be equally likely to fall into two regions if they have the same area.</p>\n\n<p>This would be quite simple if it were a square since I would take two random numbers in [0,1] as my coordinates.</p>\n\n<p>The shape I have is a regular polygon, but I\'d like it to work for any polygon.</p>\n\n<p><a href="http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle">http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle</a></p>\n\n<p><img src="http://mathworld.wolfram.com/images/eps-gif/TrianglePointPicking_700.gif" width="400"></p>\n', 'ViewCount': '192', 'Title': 'Random sampling in a polygon', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-29T14:38:32.687', 'LastEditDate': '2013-08-29T14:38:32.687', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<algorithms><randomness><sampling><random-number-generator>', 'CreationDate': '2013-08-29T00:36:47.990', 'Id': '14007'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I formulate the question in terms of universal distributions. Fix a version of Solomonoff\'s universal distribution $\\mathbf M$ and consider the following procedure for generating an infinite binary sequence $\\omega$. </p>\n\n<p>Start with some $\\omega_0$. Each subsequent element is given by $\\omega_n=\\arg \\max_a\\mathbf M(a|\\omega_{1:n-1})$.</p>\n\n<p>What are the properties of $\\omega$? For example, is it necessarily computable? Does the answer depend on the choice of $\\mathbf M$?</p>\n\n<p>The equivalent formulation is the following but it does not really help me advance with the problem. </p>\n\n<p>Is there a universal lower semicomputable $\\lambda$-supermartingale $t$ and a binary sequence $\\tilde\\omega$ such that $t$ always "goes up" along $\\tilde\\omega$ but never succeeds?</p>\n\n<p>This type of problem arises when a player plays against a reactive environment and the sequence the player observes is not exogenous. A more general question is what is the short-run behavior of $\\mathbf M$ since asymptotic results do not seem to help here.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>As an example, consider a simple coordination game with payoff matrix:</p>\n\n<p>(1,1)     (0,0)</p>\n\n<p>(0,0)     (1,1)</p>\n\n<p>Consider two players who infinitely repeatedly play this game. Their goal is to maximize the payoff. Assume that the player are myopic that is they care only about the payoff in the current period. </p>\n\n<p>Each player observes his opponent\'s history and tries to match him. Each player has a prior distribution about his opponent\'s play and updates it using Bayes\'s rule. </p>\n\n<p>Now assume that the prior distribution is Solomonoff\'s $\\mathbf M$. Then every period a player chooses an action $\\arg \\max_a\\mathbf M(a|\\omega_{1:n-1})$ where $\\omega_{1:n-1}$ is the opponent\'s history. The other player does the same, thus the rule $\\omega_n=\\arg \\max_a\\mathbf M(a|\\omega_{1:n-1})$.</p>\n', 'ViewCount': '67', 'Title': 'The sequence in which every symbol minimizes conditional complexity?', 'LastEditorUserId': '9931', 'LastActivityDate': '2013-09-02T15:46:46.007', 'LastEditDate': '2013-09-02T15:46:46.007', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9931', 'Tags': '<computability><randomness>', 'CreationDate': '2013-09-01T20:30:50.230', 'Id': '14073'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In the last section of chapter 3 (page 54) in <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em> by Mitzenmacher and Upfal, a randomized algorithm is discussed for finding the median of a set $S$ of distinct elements in $O(n)$ time. As a substep the algorithm requires sorting a multiset $R$ of values sampled from $S$, where $|R| = \\lceil n^{3/4} \\rceil$. I'm sure this is a fairly simple, and straight forward question compared to the others on this site, but how is sorting a multiset of this size bounded by $O(n)$? </p>\n\n<p>I assume the algorithm is using a typical comparison-based, deterministic sorting algorithm that runs in $O(n\\log$ $n)$. So, If I have the expression $O(n^{3/4}\\log(n^{3/4}))$, isn't this just equivalent to $O(\\frac{3}{4}n^{3/4}\\log(n)) = O(n^{3/4}\\log(n))$? How does the reduction down to $O(n)$ proceed?</p>\n", 'ViewCount': '107', 'Title': 'Randomized Median Element Algorithm in Mitzenmacher and Upfal: O(n) sorting step?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:46:22.310', 'LastEditDate': '2013-09-16T07:46:22.310', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10128', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><randomized-algorithms>', 'CreationDate': '2013-09-13T20:07:11.977', 'Id': '14296'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know how to use the FFT for multiplying two equations in $O(n\\,log\\,n)$ time, but is there a way to use FFT to compute the expanded equation before simplifying?</p>\n\n<p>For example, if you are multiplying $A(x) = 1 + 2x + 6x^2$ and $B(x) = 4 + 5x + 3x^2$ such that you get $C(x) = A(x) \\cdot B(x) = 4 + 5x + 3x^2 + 8x + 10x^2 + 6x^3 + 24x^2 + 30x^3 + 18x^4$ without going directly to the simplified answer?</p>\n\n<p>Furthermore, is it possible to use FFT to do this expanded form multiplication in $O(n\\,log\\,n)$ time? If so, can you show me how to apply FFT to this scenario?</p>\n', 'ViewCount': '76', 'Title': 'FFT for expanded form of equation multiplication', 'LastEditorUserId': '10052', 'LastActivityDate': '2014-03-31T23:46:56.920', 'LastEditDate': '2013-09-22T04:25:36.837', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14510', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10052', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><fourier-transform>', 'CreationDate': '2013-09-22T04:17:41.320', 'Id': '14509'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>C++11 has a convenient Bernoulli RNG, illustrated at \n<a href="http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution" rel="nofollow">http://en.cppreference.com/w/cpp/numeric/random/bernoulli_distribution</a> .\nHowever, distilling an entire random integer into a single random bit seems inefficient when the expectation parameter $p$ is rational with a small or power-of-two denominator.\nIs there a reasonably fast way to generate 32 random Bernoulli bits at once in such cases? My application uses long streams of bits, so I can keep track of statistics if needed (but this would consume runtime).</p>\n', 'ViewCount': '78', 'Title': "Isn't std::bernoulli_distribution inefficient? Designing a bit-parallel Bernoulli generator", 'LastEditorUserId': '5189', 'LastActivityDate': '2013-09-25T04:26:33.097', 'LastEditDate': '2013-09-25T04:26:33.097', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5189', 'Tags': '<randomized-algorithms><integers><randomness><binary-arithmetic>', 'CreationDate': '2013-09-22T21:10:38.360', 'Id': '14525'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I'm working on a NFA to DFA conversion tool that is different from the Subset Construction and I need to test this tool.</p>\n\n<p>In order to be sure that the immplementation has no bug I'd like to generate a random NFA with these properties:</p>\n\n<ul>\n<li>The NFA should be connected</li>\n<li>The NFA should have one initial state and one or more final states</li>\n<li>The NFA should have \u03b5-moves</li>\n</ul>\n\n<p>Is there a known algorithm or a paper that explains how to generate this NFA automaton as random as possible? </p>\n", 'ViewCount': '100', 'Title': 'NFA random generator', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-23T18:09:45.097', 'LastEditDate': '2013-09-23T18:09:45.097', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10118', 'Tags': '<finite-automata><pseudo-random-generators><random>', 'CreationDate': '2013-09-23T15:19:46.953', 'Id': '14555'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Motivated by <a href="http://cs.stackexchange.com/q/17917/2755">Efficiently sampling shortest s-t paths uniformly and independently at random</a>,</p>\n\n<p>The answers give methods of randomly sampling shortest $s\\text{-}t$ paths. However, they use a lot of seemingly unnecessary random bits.</p>\n\n<p>My question is:</p>\n\n<blockquote>\n  <p>Can the solution be improved to use a single random number in interval $[0,w(t))$, where $w(t)$ is the total number of shortest paths from $s\\text{-}t$.</p>\n  \n  <p>Alternatively, can the solution be improved to use $\\left\\lceil \\log_2 w(t)\\right\\rceil $ random bits?</p>\n</blockquote>\n', 'ViewCount': '74', 'Title': 'Uniformly random efficient sampling of shortest s-t paths, with optimal random bits', 'LastActivityDate': '2013-11-17T19:21:56.460', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '18091', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<graph-theory><shortest-path><sampling><random>', 'CreationDate': '2013-11-17T01:51:40.173', 'FavoriteCount': '1', 'Id': '18089'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>consider a program that generates a <a href="http://en.wikipedia.org/wiki/Random_walk" rel="nofollow">random walk</a> using a <a href="http://en.wikipedia.org/wiki/Pseudorandom_number_generator" rel="nofollow">PRNG</a>, as in following pseudocode. it uses <a href="http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic" rel="nofollow">arbitrary precision arithmetic</a> such that there is no limit on variable values (ie no overflow).</p>\n\n<pre><code>srand(x)\nz = 0\nwhile (z &gt;= 0)\n{\n  r = rand(100)\n  if (r &lt;= 50) z -= 1\n  else z += 1\n}\n</code></pre>\n\n<p>the PRNG is inited with seed <code>x</code> <em>(also arbitrary precision).</em> the PRNG <code>rand(100)</code> generates a value between <code>0..99</code>. hence for 51 values the accumulator var <code>z</code> is decremented, for 49 values it is incremented.</p>\n\n<p>it is expected due to the <a href="http://en.wikipedia.org/wiki/Law_of_large_numbers" rel="nofollow">law of large numbers</a> that this program will halt for all initial seeds <code>x</code>. however, </p>\n\n<blockquote>\n  <p>how does one prove it will halt for all initial seeds <code>x</code>?</p>\n</blockquote>\n\n<p>it seems such a proof must depend on the details of the construction of the PRNG. am assuming there exist PRNGs such that a different random sequence is generated for every initial seed <code>x</code> (ie the infinite set of naturals). that in itself may be up for question. are such PRNGs known? where are they used? etc.. so an answer may come up with an arbitrary PRNG for the purposes of the question. a single example fulfilling the criteria would be an acceptable answer.</p>\n\n<p>looking for related literature, similar problems/proof considered, etc.</p>\n', 'ViewCount': '58', 'Title': 'proof of convergence in arbitrary precision PRNGs', 'LastActivityDate': '2013-11-19T00:38:41.127', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><reference-request><probability-theory><pseudo-random-generators><random-walks>', 'CreationDate': '2013-11-18T21:52:05.343', 'FavoriteCount': '1', 'Id': '18132'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '64', 'Title': 'Are there pseudorandom number generators (PRNG) with no finite period?', 'LastEditDate': '2013-11-22T11:16:09.327', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '699', 'FavoriteCount': '1', 'Body': '<blockquote>\n  <p>The typical and widely used <a href="http://en.wikipedia.org/wiki/Pseudorandom_number_generator" rel="nofollow">PRNG</a>, the <a href="http://en.wikipedia.org/wiki/Linear_congruential_generator" rel="nofollow">linear congruential generator</a> always has a finite (though possibly "long") period. Are there PRNGs that have no finite period?</p>\n</blockquote>\n\n<p>For this question it is not necessary that it be practical or used in real-world implementations.</p>\n', 'Tags': '<reference-request><number-theory><pseudo-random-generators>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-22T19:53:32.883', 'CommentCount': '1', 'AcceptedAnswerId': '18251', 'CreationDate': '2013-11-22T03:12:52.983', 'Id': '18250'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve got 30 elements which has to be  grouped/sorted into 10 ordered 3-tuple. \nThere are several rules and constraints about grouping/sorting.\nFor example: Element $A$ must not be in the same tuple same unit $B$. \nElement $C$ must not be right in front of element $A$, etc.</p>\n\n<p>I am searching for an approximated algorithm:</p>\n\n<ol>\n<li>We don\'t need to achieve the exact optimum </li>\n<li>It is OK for some rules not to be satisfied, if it helps to fulfill more rules.</li>\n</ol>\n\n<p>Do you know of any algorithm/proceeding that solve this problem or a similar one?\nI fear to solve it in an optimal way, you have to try out every possible solution-> $2 ^ {30}$</p>\n\n<p><strong>EDIT</strong>: Sorry for the bad explanation. I am trying to make it a bit clearer:\nI got 30 elements for example: $\\{1,2,3,\\ldots,30\\}$.\nI need to group them into 3-tuples so that i get something like: $(1,2,3)$, $(4,5,6)$,$\\ldots$,$(28,29,30)$.</p>\n\n<p>There are several constraints. For example: </p>\n\n<ul>\n<li>1 cannot precede 2 in an ordered tuple, so, for instance  $(1,2,3)$ is not a valid tuple.</li>\n<li>5 must be together with 4. </li>\n</ul>\n\n<p>Those constraints can be broken and its possible that there is no solution where all rules can be fulfilled. <br />An solution is considered as good if the amount of rules broken is "low".</p>\n\n<p>Hope that makes it clearer and thanks for the help so far.</p>\n', 'ViewCount': '81', 'Title': 'Algorithm for sorting with constraints', 'LastEditorUserId': '157', 'LastActivityDate': '2013-12-16T01:48:30.027', 'LastEditDate': '2013-12-16T00:37:15.273', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '18345', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11601', 'Tags': '<algorithms><sorting><randomized-algorithms><greedy-algorithms>', 'CreationDate': '2013-11-25T00:14:58.160', 'Id': '18312'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '110', 'Title': 'Chernoff bounds and Monte Carlo algorithms', 'LastEditDate': '2013-11-25T14:30:48.973', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8508', 'FavoriteCount': '1', 'Body': "<p>One of Wikipedia examples of use of Chernoff bounds is the one where an algorithm $A$ computes the correct value of function $f$ with probability $p &gt; 1/2$. Basically, Chernoff bounds are used to bound the error probability of $A$ using repeated calls to $A$ and majority agreement.</p>\n\n<p>I don't understand how, to be frank. It would be nice if somebody could break it down piece by piece. Moreover, does it matter whether $A$ is a decision algorithm or can return more values? How are Chernoff bounds in general used for Monte Carlo algorithms?</p>\n", 'Tags': '<randomized-algorithms><randomness><chernoff-bounds>', 'LastEditorUserId': '8508', 'LastActivityDate': '2013-11-25T19:22:19.073', 'CommentCount': '2', 'AcceptedAnswerId': '18329', 'CreationDate': '2013-11-25T06:16:42.047', 'Id': '18321'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Presume I have the following case:</p>\n\n<ul>\n<li>int value <code>a</code></li>\n<li>int value <code>b</code>, for which <code>a &lt; b</code></li>\n<li>int value i, for which <code>a &lt;= i &lt; b</code></li>\n<li>I need an int value <code>x</code>, for which <code>a &lt;= x &lt; b</code>, randomly chosen, according to a non-uniform distribution: the distribution should follow a bell curve which is centered around <code>i</code>.</li>\n</ul>\n\n<p>In Java, I have the following methods available on <code>java.util.Random</code>:</p>\n\n<ul>\n<li><code>nextInt(int m)</code>: int between <code>0</code> and <code>m</code></li>\n<li><code>nextDouble()</code>: double between <code>0.0</code> and <code>1.0</code></li>\n<li><code>nextGaussian()</code>: double between <code>-Infinity</code> and <code>+Infinity</code>, usually close to <code>0.0</code>.</li>\n</ul>\n\n<p>How do I build such a non-uniform distribution from these building blocks? How do I reliably and efficiently transform <code>nextGaussian()</code> into <code>nextGaussian(a, b, i)</code>? The part I am struggling with is to enforce that x is selected between a and b (without doing a trial-and-error).</p>\n', 'ViewCount': '147', 'Title': 'Non-uniform random distribution: How do I get a random between 100 and 180 that is on average close to 120? (like in a Gaussian distribution)', 'LastActivityDate': '2013-12-08T19:39:23.743', 'AnswerCount': '3', 'CommentCount': '4', 'AcceptedAnswerId': '18661', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11699', 'Tags': '<random>', 'CreationDate': '2013-11-29T12:32:13.410', 'FavoriteCount': '1', 'Id': '18467'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '127', 'Title': 'NP-complete decision problems - how close can we come to a solution?', 'LastEditDate': '2014-01-02T15:28:30.023', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '2', 'Body': '<p>After we prove that a certain <strong>optimization</strong> problem is NP-hard, the natural next step is to look for a polynomial algorithm that comes close to the optimal solution - preferrably with a constant approximation factor.</p>\n\n<p>After we prove that a certain <strong>decision</strong> problem is NP-complete, what is the natural next step? Obviously we cannot "approximate" a boolean value...</p>\n\n<p>My guess is that, the next step is to look for a randomized algorithm that returns the correct solution with a high probability. Is this correct?</p>\n\n<p>If so, what probability of being correct can we expect to get from such a randomized algorithm?</p>\n\n<p>As far as I understand from Wikipedia, <a href="https://en.wikipedia.org/wiki/PP_%28complexity%29" rel="nofollow">PP contains NP</a>. This means that, if the problem is in NP, it should be easy to write an algorithm that is correct more than $0.5$ of the times.</p>\n\n<p>However, <a href="https://en.wikipedia.org/wiki/Bounded-error_probabilistic_polynomial" rel="nofollow">it is not known whether BPP contains NP</a>. This means that, it may be difficult (if not impossible) to write an algorithm that is correct more than $0.5+\\epsilon$ of the times, for every positive $\\epsilon$ independent of the size of input.</p>\n\n<p>Did I understand correctly?</p>\n', 'Tags': '<np-complete><approximation><randomized-algorithms>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-01-03T01:18:29.533', 'CommentCount': '2', 'AcceptedAnswerId': '19419', 'CreationDate': '2013-12-31T16:03:52.760', 'Id': '19412'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If the clock shows 14:15:36.909302, why not just use the fractions of a second part (09302) as a kind of random number? </p>\n\n<p>What is wrong with this form of generating random numbers?</p>\n\n<p>I am aware that obtaining truly random numbers is a difficult task, so I am assuming that there is something wrong with this method.</p>\n', 'ViewCount': '67', 'Title': "What's the problem of using the clock to generate random numbers?", 'LastActivityDate': '2013-12-31T21:13:25.033', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19426', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10637', 'Tags': '<randomness><random><random-number-generator>', 'CreationDate': '2013-12-31T20:18:04.223', 'Id': '19421'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can I generate a random integer through calls to a function that generates one random bit at a time.</p>\n\n<p>Do I place the bit randomly generated at a random position?</p>\n', 'ViewCount': '8', 'ClosedDate': '2014-01-22T18:55:05.407', 'Title': 'Uniform Random Integer Generation', 'LastActivityDate': '2014-01-22T18:45:10.583', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10950', 'Tags': '<pseudo-random-generators>', 'CreationDate': '2014-01-22T18:45:10.583', 'Id': '19896'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was trying to understand better the definition of a strong PSRG and I came across this expression which I am trying to understand better:</p>\n\n<p>$$ Pr_{r \\in \\{0,1\\}^l}[A(r) = \\text{"yes"}]$$</p>\n\n<p>Where r is a truly random bit string and A is a polynomial time deterministic machine.\nI\'ve been having some problems understanding what this expression means conceptually (or intuitively). </p>\n\n<p>So far these are some of my thoughts and I will try to point out my doubts too.</p>\n\n<p>A is just a standard TM so we can image that on l steps, it will yield $2^l$ branches. Each branch has a chance of occurring depending on which r occurs. Therefore, I was wondering if the above probability expression just mean "the fraction of branches that out yes"? Is that basically the same as the chance that A will output yes on the given random bit string? The thing that was confusing me and I was not sure how to deal with it was that, A(r) always outputs the same thing ("yes" or "no") on a given r (say it always accepts or rejects if r = 1010100 or something), it didn\'t seem to me that it a probabilistic sense, unless we randomly choose r. So I was wondering how the community interpreted this equation and what it mean.</p>\n\n<p>Also, since this is a probability, it seems to me that A(r) is just r.v. that only takes two values (yes or no), right? So this distribution only has two probability values, the one that A outputs yes or no, right? I was wondering how that related to the string r and I was not sure how to resolve this.</p>\n', 'ViewCount': '32', 'Title': 'Interpreting probabilistic time turning machines', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-01-24T19:23:21.040', 'LastEditDate': '2014-01-24T19:23:21.040', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19943', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<terminology><probability-theory><randomized-algorithms><randomness>', 'CreationDate': '2014-01-24T04:54:38.433', 'Id': '19932'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have tried hard , but i'm unable to come up with the expected running time for the number of comparisons to find the Randomized Median (find the median of an unsorted array). \nAlso i wanted to make sure that we CANNOT take expectation of the recurrence that we use to find the randomized mean , or any other recurrence in any other problem as they belong to different probability spaces? Is this statement right?</p>\n", 'ViewCount': '32', 'ClosedDate': '2014-01-29T17:01:16.563', 'Title': 'Randomised Median', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T17:01:04.093', 'LastEditDate': '2014-01-29T17:01:04.093', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13212', 'Tags': '<algorithms><randomized-algorithms>', 'CreationDate': '2014-01-29T05:10:11.183', 'Id': '20055'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given normally distributed integers with a mean of 0 and a standard deviation $\\sigma$ around 1000, how do I compress those numbers (almost) perfectly?  Given the entropy of the Gaussian distribution, it should be possible to store any value $x$ using $$\\frac{1}{2} \\mathrm{log}_2(2\\pi\\sigma^2)+\\frac{x^2}{2\\sigma^2}\\rm{log}_2\\rm{e}$$<br>\nbits.  The way to accomplish this perfect compression would be <a href="http://en.wikipedia.org/wiki/Arithmetic_coding" rel="nofollow">arithmetic coding</a>.  In principle it\'s not too hard, I can calculate the interval boundaries from the <a href="http://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="nofollow">cumulative distribution function</a> of the gaussian.  In practice I hit considerable difficulties because when using floating point operations I cannot achieve perfect reproduction of the results, and I have no idea how to do this without FP operations.  Perfect reproduction is necessary because the uncompressing code must come up with exactly the same interval boundaries as the compressing code.  So the question is:  How do I compute the interval boundaries?  Or is there any other way to achieve (near) perfect compression of such data?</p>\n\n<p><strong>Edit:</strong>  As Raphael said, strictly speaking the normal distribution is defined only for continuous variables.  So what I mean here are integers x with a probability distribution function $$P(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\rm e^{-\\frac{x^2}{2\\sigma^2}}$$.  </p>\n\n<p><strong>Edit2:</strong>  As Yuval said, this distribution does not sum up exactly to 1, however, for $\\sigma&gt;100$  the difference from 1 is less than $10^{-1000}$ and hence it\'s more precise than any practical calculation would be.</p>\n', 'ViewCount': '101', 'Title': 'Compressing normally distributed data', 'LastEditorUserId': '12710', 'LastActivityDate': '2014-03-04T06:50:22.477', 'LastEditDate': '2014-01-31T17:46:33.920', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22261', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12710', 'Tags': '<information-theory><randomness><data-compression><entropy>', 'CreationDate': '2014-01-31T14:34:01.367', 'Id': '20156'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am somewhat confused with the running time analysis of a program here which has recursive calls which depend on a RNG.  (Randomly Generated Number)</p>\n\n<p>Let\'s begin with the pseudo-code, and then I will go into what I have thought about so far related to this one.</p>\n\n<pre><code>    Func1(A, i, j)\n    /* A is an array of at least j integers */\n\n 1  if (i \u2265 j) then return (0);\n 2  n \u2190 j \u2212 i + 1 ; /* n = number of elements from i to j */\n 3  k \u2190 Random(n);\n 4  s \u2190 0; //Takes time of Arbitrary C\n\n 5  for r \u2190 i to j do\n 6      A[r] \u2190 A[r] \u2212 A[i] \u2212 A[j]; //Arbitrary C\n 7      s \u2190 s + A[r]; //Arbitrary C\n 8  end\n\n 9  s \u2190 s + Func1(A, i, i+k-1); //Recursive Call 1\n10  s \u2190 s + Func1(A, i+k, j); //Recursive Call 2\n11  return (s);\n</code></pre>\n\n<p>Okay, now let\'s get into the math I have tried so far.  I\'ll try not to be too pedantic here as it is just a rough, estimated analysis of expected run time.  </p>\n\n<p>First, let\'s consider the worst case.  Note that the K = Random(n) must be at least 1, and at most n.  Therefore, the worst case is the K = 1 is picked.  This causes the total running time to be equal to T(n) = cn + T(1) + T(n-1).  Which means that overall it takes somewhere around cn^2 time total (you can use Wolfram to solve recurrence relations if you are stuck or rusty on recurrence relations, although this one is a fairly simple one).  </p>\n\n<p>Now, here is where I get somewhat confused.  For the expected running time, we have to base our assumption off of the probability of the random number K.  Therefore, we have to sum all the possible running times for different values of k, plus their individual probability.  By lemma/hopefully intuitive logic: the probability of any one Randomly Generated k, with k between 1 to n, is equal 1/n.  </p>\n\n<p><strong>Therefore, (in my opinion/analysis) the expected run time is:</strong></p>\n\n<p><strong>ET(n) = cn + (1/n)*Summation(from k=1 to n-1) of (ET(k-1) + ET(n-k))</strong></p>\n\n<p>Let me explain a bit.  The cn is simply for the loop which runs i to j.  This is estimated by cn.  The summation represents all of the possible values for k.  The (1/n) multiplied by this summation is there because the probability of any one k is (1/n).  <strong>The terms inside the summation represent the running times of the recursive calls of Func1.</strong>  The first term on the left takes ET(k-1) because this recursive call is going to do a loop from i to k-1 (which is roughly ck), and then possibly call Func1 again.  The second is a representation of the second recursive call, which would loop from i+k to j, which is also represented by n-k.</p>\n\n<p><strong>Upon expansion of the summation, we see that the overall function ET(n)  is of the order n^2.</strong>  <em>However</em>, as a test case, plugging in k=(n/2) gives a total running time for Func 1 of roughly nlog(n).  <em>This</em> is why I am confused.  How can this be, if the estimated running time is of the order n^2?  Am I considering a "good" case by plugging in n/2 for k?  Or am I thinking about k in the wrong sense in some way?  </p>\n', 'ViewCount': '66', 'Title': 'Algorithm Analysis: Expected Running Time of Recursive Function Based on a RNG', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-17T19:50:53.527', 'LastEditDate': '2014-02-12T09:12:25.723', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14596', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><average-case>', 'CreationDate': '2014-02-12T05:31:29.490', 'Id': '21558'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '58', 'Title': 'Advantage of the Monte Carlo method over a regular periodic sampling', 'LastEditDate': '2014-02-27T10:10:16.087', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15001', 'Body': "<p>I am unclear on when to use the Monte Carlo random sampling method for algorithm design. The classic example that I keep seeing is using random points within some bounding rectangle to determine the area of some irregular figure. Wouldn't a regular periodic sampling provide more repeatable results for this application then using the Monte Carlo (random sampling) method?</p>\n", 'ClosedDate': '2014-02-27T07:44:07.457', 'Tags': '<algorithms><randomized-algorithms><monte-carlo>', 'LastEditorUserId': '39', 'LastActivityDate': '2014-02-27T10:10:16.087', 'CommentCount': '1', 'AcceptedAnswerId': '22004', 'CreationDate': '2014-02-24T20:37:44.313', 'Id': '22002'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given n, I want to randomly generate a binary tree (unlabelled) that has n end nodes. Could someone kindly provide a reference containing an algorithm for doing that?</p>\n\n<p>I attempted to do as follows: From a PRNG obtain n PRNs in [0.0, 1.0) as (relative) frequencies of n symbols for generating a Huffman tree (used in data compression). But, if the PRNs used are uniform, then I think this would highly favour generation of those Huffman trees that are more flat and Huffman trees corresponding to widely different frequencies of the symbols would be highly suppressed in the generation process. If this is correct, how could one do better? Thanks in advance.</p>\n', 'ViewCount': '63', 'Title': 'Generation of random binary trees', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T13:47:45.200', 'LastEditDate': '2014-02-28T08:39:33.350', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6437', 'Tags': '<data-structures><sampling><random-graphs>', 'CreationDate': '2014-02-27T17:34:00.827', 'Id': '22098'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '126', 'Title': 'Selecting random points at general position', 'LastEditDate': '2014-03-01T22:33:17.750', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1342', 'FavoriteCount': '1', 'Body': '<p>How will you find a random collection of $n$ points in the plane, all with integer coordinates in a specified range (e.g. -1000 to 1000), such that no 3 of them are on the same line?</p>\n\n<p>The following algorithm eventually works, but seems highly inefficient:</p>\n\n<ol>\n<li>Select $n$ points at random.</li>\n<li>Check all $O(n^3)$ triples of points. If any of them are on the same line, then discard one of the points, select an alternative point at random, and go back to 2.</li>\n</ol>\n\n<p>Is there a more efficient algorithm?</p>\n', 'Tags': '<algorithms><computational-geometry><randomized-algorithms>', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-03-01T22:50:12.840', 'CommentCount': '1', 'AcceptedAnswerId': '22171', 'CreationDate': '2014-03-01T20:49:34.203', 'Id': '22167'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have been working on a challenge i found on the internet. It is as follows:</p>\n\n<blockquote>\n  <p>You\'ve stumbled onto a significant vulnerability in a commonly used cryptographic library. It turns out that the random number generator it uses frequently produces the same primes when it is generating keys.</p>\n  \n  <p>Exploit this knowledge to factor the (hexadecimal) keys below, and enter your answer as the last six digits of the largest factor you find (in decimal).</p>\n  \n  <p>Key 1: 1c7bb1ae67670f7e6769b515c174414278e16c27e95b43a789099a1c7d55c717b2f0a0442a7d49503ee09552588ed9bb6eda4af738a02fb31576d78ff72b2499b347e49fef1028182f158182a0ba504902996ea161311fe62b86e6ccb02a9307d932f7fa94cde410619927677f94c571ea39c7f4105fae00415dd7d</p>\n  \n  <p>Key 2: \n   2710e45014ed7d2550aac9887cc18b6858b978c2409e86f80bad4b59ebcbd90ed18790fc56f53ffabc0e4a021da2e906072404a8b3c5555f64f279a21ebb60655e4d61f4a18be9ad389d8ff05b994bb4c194d8803537ac6cd9f708e0dd12d1857554e41c9cbef98f61c5751b796e5b37d338f5d9b3ec3202b37a32f</p>\n</blockquote>\n\n<p>These seem to be common RSA 1024-bit keys.</p>\n\n<p>My approach to the problem was to implement <a href="http://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm" rel="nofollow">Pollard\'s rho algorithm</a> to find factors, then once a factor is found, try dividing the decimal form of the keys by that factor until it is not divisible anymore. Iterate. </p>\n\n<p>Now, i used Pollard\'s rho and tried to divide until the key is not divided anymore because of the information the problem gave: the keys are not completely random. </p>\n\n<p>But here comes the question: assuming the algorithm generates two primes and multiplies them to get a co-prime, which is the key, the low randomness doesn\'t help much, does it? I mean, even if both keys share a common factor, finding it the first time would take the regular-impractical-exponential time.</p>\n\n<p>That seems to be the case, as my Python algorithm is running for about 5 hours now and has not found any factor to the second key, which i decided to start with.</p>\n\n<p>As it is a challenge, i assume there is a practical way of finding the answer. \nSo what im i doing wrong? Is just the algorithm choice wrong, as Pollard\'s rho is intended mainly for integer with small factors? Is my assumption that i can only use the lack of randomness after i find the first of the four factor, to then try to break the other key with the same factor, wrong?</p>\n\n<p>I would like if someone could just point me in right direction, instead of just giving the answer. Thank you. </p>\n', 'ViewCount': '33', 'Title': 'Finding prime factors of non-random key generator', 'LastActivityDate': '2014-03-05T05:41:29.360', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22291', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15293', 'Tags': '<algorithms><cryptography><randomness><primes>', 'CreationDate': '2014-03-05T03:22:05.833', 'Id': '22289'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm wondering why the following argument doesn't work for showing that the existence of a Las Vegas algorithm also implies the existence of a deterministic algorithm:</p>\n\n<p>Suppose that there is a Las Vegas algorithm $A$ that solves some graph problem $P$, i.e., $A$ takes an $n$-node input graph $G$ as input (I'm assuming the number of edges is $\\le n$) and eventually yields a correct output, while terminating within time $T(G)$ with some nonzero probability.</p>\n\n<p>Suppose that there is no deterministic algorithm that solves $P$. Let $A^\\rho$ be the deterministic algorithm that is given by running the Las Vegas algorithm $A$ with a fixed bit string $\\rho$ as its random string. \nLet $k=k(n)$ be the number of $n$-node input graphs (with $\\le n$ edges).\nSince there is no deterministic algorithm for $P$, it follows that, for any $\\rho$, the deterministic algorithm $A^\\rho$ fails on at least one of the $k$ input graphs. Returning to the Las Vegas algorithm $A$, this means that $A$ has a probability of failure of $\\ge 1/k$, a contradiction to $A$ being Las Vegas. </p>\n", 'ViewCount': '194', 'Title': 'Relationship between Las Vegas algorithms and deterministic algorithms', 'LastActivityDate': '2014-03-10T21:34:04.607', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15471', 'Tags': '<algorithms><complexity-theory><randomized-algorithms>', 'CreationDate': '2014-03-10T02:22:57.270', 'Id': '22448'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say I plugged in a <a href="http://en.wikipedia.org/wiki/Hardware_random_number_generator" rel="nofollow">hardware true-random number generator (TRNG)</a> to my computer, then wrote programs with output that depends on the TRNG\'s output. Can it do anything non-trivial that a Turing machine with a psuedo-random number generator can\'t do? (a trivial thing it can do would be generating truly random numbers)</p>\n', 'ViewCount': '37', 'Title': 'Are there any practical differences between a Turing machine with a PRNG and a probabilistic Turing machine?', 'LastActivityDate': '2014-03-17T20:49:58.900', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15790', 'Tags': '<turing-machines><randomness><probabilistic-algorithms>', 'CreationDate': '2014-03-17T19:39:13.557', 'Id': '22720'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2223', 'LastEditorDisplayName': 'user15782', 'Title': 'Can PRNGs be used to magically compress stuff?', 'LastEditDate': '2014-03-26T15:26:43.373', 'AnswerCount': '6', 'Score': '23', 'OwnerDisplayName': 'user15782', 'PostTypeId': '1', 'FavoriteCount': '3', 'Body': '<p>This idea occurred to me as a kid learning to program and\non first encountering PRNG\'s. I still don\'t know how realistic\nit is, but now there\'s stack exchange.</p>\n\n<p>Here\'s a 14 year-old\'s scheme for an amazing compression algorithm: </p>\n\n<p>Take a PRNG and seed it with seed <code>s</code> to get a long sequence \nof pseudo-random bytes. To transmit that sequence to another party, \nyou need  only communicate a description of the PRNG, the appropriate seed \nand the length of the message. For a long enough sequence, that \ndescription would be much shorter then the sequence itself.</p>\n\n<p>Now suppose I could invert the process. Given enough time and \ncomputational resources, I could do a brute-force search and find \na seed (and PRNG, or in other words: a program) that produces my\ndesired sequence (Let\'s say an amusing photo of cats being mischievous).</p>\n\n<p>PRNGs repeat after a large enough number of bits have been generated,\nbut compared to "typical" cycles my message is quite short so this \ndosn\'t seem like much of a problem.</p>\n\n<p>Voila, an effective (if rube-Goldbergian) way to compress data.</p>\n\n<p>So, assuming:</p>\n\n<ul>\n<li>The sequence I wish to compress is finite and known in advance.</li>\n<li>I\'m not short on cash or time (Just as long as a finite amount \nof both is required)</li>\n</ul>\n\n<p>I\'d like to know:</p>\n\n<ul>\n<li>Is there a fundamental flaw in the reasoning behind the scheme? </li>\n<li>What\'s the standard way to analyse these sorts of thought experiments?</li>\n</ul>\n\n<p><em>Summary</em></p>\n\n<p>It\'s often the case that good answers make clear not only the answer, \nbut what it is that I was really asking. Thanks for everyone\'s patience \nand detailed answers. </p>\n\n<p>Here\'s my nth attempt at a summary of the answers:</p>\n\n<ul>\n<li>The PRNG/seed angle doesn\'t contribute anything, it\'s no more \nthen a program that produces the desired sequence as output.</li>\n<li>The pigeonhole principle: There are many more messages of \nlength > k then there are (message generating) programs of \nlength &lt;= k. So some sequences simply cannot be the output of a \nprogram shorter then the message. </li>\n<li>It\'s worth mentioning that the interpreter of the program \n(message) is necessarily fixed in advance. And it\'s design \ndetermines the (small) subset of messages which can be generated\nwhen a message of length k is received.</li>\n</ul>\n\n<p>At this point the original PRNG idea is already dead, but there\'s \nat least one last question to settle:</p>\n\n<ul>\n<li>Q: Could I get lucky and find that my long (but finite) message just \nhappens to be the output of a program of length &lt; k bits?</li>\n</ul>\n\n<p>Strictly speaking, it\'s not a matter of chance since the \nmeaning of every possible message (program) must be known \nin advance. Either it <em>is</em> the meaning of some message \nof &lt; k bits <em>or it isn\'t</em>.</p>\n\n<p>If I choose a random message of >= k bits randomly (why would I?),\nI would in any case have a vanishing probability of being able to send it\nusing less then k bits, and an almost certainty of not being able \nto send it at all using less then k bits.</p>\n\n<p>OTOH, if I choose a specific message of >= k bits from those which\nare the output of a program of less then k bits (assuming there is \nsuch a message), then in effect I\'m taking advantage of bits already\ntransmitted to the receiver (the design of the interpreter), which \ncounts as part of the message transferred.</p>\n\n<p>Finally:</p>\n\n<ul>\n<li>Q: What\'s all this <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy</a>/<a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" rel="nofollow">kolmogorov complexity</a> business?</li>\n</ul>\n\n<p>Ultimately, both tell us the same thing as the (simpler) piegonhole \nprinciple tells us about how much we can compress: perhaps \nnot at all, perhaps some, but certainly not as much as we fancy\n(unless we cheat).</p>\n', 'Tags': '<information-theory><randomness><data-compression>', 'LastActivityDate': '2014-03-26T15:26:43.373', 'CommentCount': '13', 'AcceptedAnswerId': '23020', 'CreationDate': '2014-03-24T17:02:03.550', 'Id': '23010'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '360', 'Title': 'Is this method really uniformly random?', 'LastEditDate': '2014-03-25T22:35:47.140', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16107', 'FavoriteCount': '2', 'Body': "<p>I have a list and want to select a random item from the list.</p>\n\n<p>An algorithm which is said to be random:</p>\n\n<blockquote>\n  <p>When you see the first item in the list, you set it as the selected item.</p>\n  \n  <p>When you see the second item, you pick a random integer in the range [1,2]. If it's 1, then the new item becomes the selected item. Otherwise you skip that item.</p>\n  \n  <p>For each item you see, you increase the count, and with probability 1/count, you select it. So at the 101st item, you pick a random integer in the range [1,101]. If it's 100, that item is the new selected node.</p>\n</blockquote>\n\n<p>Is it really uniformly random?</p>\n\n<p><strong>My thoughts are:</strong></p>\n\n<p>As the number of nodes increases, the probability for them being selected \ndecreases, so the best chance of selection is for items 1, 2, 3, ..., not for 20, 21, ..., 101.</p>\n\n<p>Each node will not have equal probability of being selected.</p>\n\n<p>Please clarify, as I have trouble understanding this.</p>\n", 'Tags': '<algorithms><probability-theory><randomized-algorithms><sampling>', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-03-25T22:35:47.140', 'CommentCount': '5', 'AcceptedAnswerId': '23041', 'CreationDate': '2014-03-25T17:55:00.713', 'Id': '23038'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's say I can solve problem $A$ in polynomial time using only $\\log n$ bits of randomness, with a $\\ge \\frac{2}{3}$ chance of a correct answer.  Then surely I can solve $A$ determistically by running my algorithm for $A$ over all random strings of length $\\log n$ (of which there are a polynomial number) and take a popular vote of the outcomes.</p>\n\n<p>I don't understand, then, why we would ever talk about $O(\\log n)$ amounts of randomness in complexity classes that are closed under polynomial factors.  More specifically, the PCP theorem says $NP = PCP[O(\\log n), O(1)]$ - why isn't that the same as $PCP[0, O(1)]$?</p>\n", 'ViewCount': '25', 'Title': "Why can't we derandomize the PCP theorem by iterating over all possible $\\log n$ random strings?", 'LastActivityDate': '2014-03-26T23:35:06.183', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23107', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16172', 'Tags': '<randomized-algorithms>', 'CreationDate': '2014-03-26T23:13:50.927', 'Id': '23104'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '137', 'Title': 'Quantum algorithms and quantum computation', 'LastEditDate': '2014-04-04T12:48:55.663', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16266', 'FavoriteCount': '3', 'Body': "<p>Is my (very high-level) understanding correct here regarding quantum algorithms &mdash;</p>\n\n<p>Quantum computers can process a massive amount of operations in parallel to the nature of qubits and their ability to have states that are superpositions of $|0\\rangle$ and $|1\\rangle$.</p>\n\n<p>Yet when we measure the qubits all the possible states collapse into a single state of either $|0\\rangle$ or $|1\\rangle$, which seems to negate the potential benefits of parallel operations. All we really know are the probabilities that the states will end up as.</p>\n\n<p>However, we can exploit quantum properties to increase the probability that we end up with a certain result. I believe Shor's algorithm is based on exploiting quantum properties too, although I'm not sure in what way?</p>\n\n<p><em>e.g.</em> in a quantum walk, quantum interference means the walk spreads faster than a classical random walk and hence can out-perform classical walks.</p>\n\n<p>That is my very high level understanding of what is going on with quantum algorithms. Am I correct, 'sort-of' correct, or way-off? Can someone clarify my understanding?</p>\n", 'Tags': '<quantum-computing><random-walks>', 'LastEditorUserId': '2152', 'LastActivityDate': '2014-04-04T13:08:09.397', 'CommentCount': '0', 'AcceptedAnswerId': '23418', 'CreationDate': '2014-04-04T12:43:38.880', 'Id': '23417'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A town has $N$ people.\nAt Day 0, a person has a secret. At Day 1, he calls a random person and tells him the secret. At Day 2, every person who knows the secret calls a person at random to tell the secret. In this way secret propagates.</p>\n\n<p>Let X be the number of days till everybody knows the secret. What will be the Expectation of X? If $X_i$ be the number of people who know the secret at end of day $i$, what will be the Expectation of $X_i$?</p>\n\n<p>$Z$ = min{$k$|$X_k = N$}. Then what will be the bound of $Z$? What is the expected number of phone calls required so that $X_i = N$?</p>\n', 'ViewCount': '20', 'ClosedDate': '2014-04-07T13:54:25.063', 'Title': 'Expected time taken to spread message in gossip-based protocol', 'LastActivityDate': '2014-04-07T11:47:03.013', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'user3505352', 'PostTypeId': '1', 'OwnerUserId': '16533', 'Tags': '<randomized-algorithms>', 'CreationDate': '2014-04-07T05:21:23.793', 'Id': '23507'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In section 9.2 of CLRS (<em>Introduction to Algorithms; page 185 in the 2nd edition and page 215 in the 3rd edition</em>), a randomized selection algorithm is presented. </p>\n\n<p>For its analysis, $T(n)$ is a random variable denoting the time required on an input array $A[p \\cdots r]$ of $n$ elements and $X_k$ is an indicator random variable $X_k = I \\{ \\text{the subarray } A[p \\cdots q] \\text{ has exactly } k \\text{ elements (due to the pivot)} \\}$. </p>\n\n<p>It has been claimed that $X_k$ and $T(\\max(k-1, n-k))$ are independent (page 187 in the 2nd edition and page 218 in the 3rd edition). However, I find it quite counter-intuitive to understand. How to verify it?</p>\n', 'ViewCount': '26', 'Title': 'Why are the two random variables independent in the analysis of Randomized Selection algorithm in CLRS?', 'LastActivityDate': '2014-04-15T17:13:42.643', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23814', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<algorithm-analysis><randomized-algorithms>', 'CreationDate': '2014-04-15T13:18:26.367', 'Id': '23811'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose we have $n$ balls and $n$ bins. We put the balls into the bins randomly. If we count the maximum number of balls in any bin, the expected value of this is  $\\Theta(\\ln n/\\ln\\ln n)$. How can we derive this fact? Are Chernoff bounds helpful?</p>\n', 'ViewCount': '63', 'ClosedDate': '2014-04-23T16:47:14.277', 'Title': 'Expected maximum bin load, for balls in bins with equal number of balls and bins', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-19T00:38:01.780', 'LastEditDate': '2014-04-19T00:38:01.780', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15406', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><probabilistic-algorithms><chernoff-bounds>', 'CreationDate': '2014-04-18T22:43:53.940', 'Id': '23925'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Whilst reading up on <a href="https://en.wikipedia.org/wiki/Xorshift" rel="nofollow">Xorshift</a> I came across the following (emphases added):</p>\n\n<blockquote>\n  <p>The following xorshift+ generator, instead, has 128 bits of state, a maximal period of 2^128 \u2212 1 and passes BigCrush:</p>\n  \n  <p><code>[snip code]</code></p>\n  \n  <p>This generator is one of the fastest generator passing BigCrush; however, <strong>it is only 1-dimensionally equidistributed</strong>.</p>\n</blockquote>\n\n<p>Earlier in the article there\'s the following:</p>\n\n<blockquote>\n  <p><code>[snip code]</code></p>\n  \n  <p>Both generators, as all xorshift* generators of maximal period, emit a sequence of 64-bit values <strong>that is equidistributed in the maximum possible dimension</strong>.</p>\n</blockquote>\n\n<p>What does it mean for a sequence to be equidistributed in one dimension vs. multiple dimensions vs. not at all?</p>\n', 'ViewCount': '64', 'Title': "What does it mean for a random number generator's sequence to be only 1-dimensionally equidistributed?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-25T03:28:25.933', 'LastEditDate': '2014-04-23T18:52:36.400', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24038', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10487', 'Tags': '<randomness><statistics><pseudo-random-generators>', 'CreationDate': '2014-04-23T02:03:10.837', 'Id': '24037'}