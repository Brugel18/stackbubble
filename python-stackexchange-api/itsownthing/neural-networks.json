{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Fully connected (at least layer to layer with more than 2 hidden layers) backprop networks are universal learners. Unfortunately, they are often slow to learn and tend to over-fit or have awkward generalizations. </p>\n\n<p>From fooling around with these networks, I have observed that pruning some of the edges (so that their weight is zero and impossible to change) tends to make the networks learn faster and generalize better. Is there a reason for this? Is it only because of a decrease in the dimensionality of the weights search space, or is there a more subtle reason?</p>\n\n<p>Also, is the better generalization an artifact of the 'natural' problems I am looking at?</p>\n", 'ViewCount': '302', 'Title': 'Why do neural networks seem to perform better with restrictions placed on their topology?', 'LastEditorUserId': '31', 'LastActivityDate': '2012-03-18T04:38:07.220', 'LastEditDate': '2012-03-17T08:36:17.587', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '22', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<machine-learning><network-topology><neural-networks>', 'CreationDate': '2012-03-17T07:00:56.503', 'FavoriteCount': '3', 'Id': '455'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My goal is to solve the following problem, which I have described by its input and output:</p>\n\n<p><strong>Input:</strong></p>\n\n<p>A directed acyclic graph $G$ with $m$ nodes, $n$ sources, and $1$ sink ($m > n \\geq 1$).</p>\n\n<p><strong>Output:</strong></p>\n\n<p>The <a href="https://en.wikipedia.org/wiki/Vc_dimension">VC-dimension</a> (or an approximation of it) for the neural network with topology $G$.</p>\n\n<p><strong>More specifics</strong>: </p>\n\n<ul>\n<li>Each node in $G$ is a sigmoid neuron. The topology is fixed, but the weights on the edges can be varied by the learning algorithm.</li>\n<li>The learning algorithm is fixed (say backward-propagation).</li>\n<li>The $n$ source nodes are the input neurons and can only take strings from $\\{-1,1\\}^n$ as input.</li>\n<li>The sink node is the output unit. It outputs a real value from $[-1,1]$ that we round up to $1$ or down to $-1$ if it is more than a certain fixed threshold $\\delta$ away from $0$. </li>\n</ul>\n\n<p>The naive approach is simply to try to break more and more points, by attempting to train the network on them. However, this sort of simulation approach is not efficient.</p>\n\n<hr>\n\n<h3>Question</h3>\n\n<p>Is there an efficient way (i.e. in $\\mathsf{P}$ when changed to the decision-problem: is VC-dimension less than input parameter $k$?) to compute this function? If not, are there hardness results?</p>\n\n<p>Is there a works-well-in-practice way to compute or approximate this function? If it is an approximation, are there any guarantees on its accuracy?</p>\n\n<h3>Notes</h3>\n\n<p>I asked a <a href="http://stats.stackexchange.com/q/25952/4872">similar question</a> on stats.SE but it generated no interest.</p>\n', 'ViewCount': '204', 'Title': 'Efficiently computing or approximating the VC-dimension of a neural network', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T02:12:14.187', 'LastEditDate': '2012-04-25T16:58:07.127', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><complexity-theory><machine-learning><neural-networks><vc-dimension>', 'CreationDate': '2012-04-25T15:21:46.690', 'FavoriteCount': '1', 'Id': '1504'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Are there any advantages of having more than 2 hidden layers in a Neural Network?</p>\n\n<p>I've seen some places that recommend it, others prove that there is no advantage.</p>\n\n<p>Which one is right?</p>\n", 'ViewCount': '209', 'Title': 'How many layers should a neural network have?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-24T22:06:52.650', 'LastEditDate': '2012-05-24T22:06:52.650', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '2051', 'Score': '8', 'OwnerDisplayName': 'user1631', 'PostTypeId': '1', 'Tags': '<artificial-intelligence><neural-networks><neural-computing>', 'CreationDate': '2012-05-24T19:50:20.057', 'Id': '2049'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is proven that neural networks with rational weights has the computational power of the Universal Turing Machine <a href="http://www.math.rutgers.edu/~sontag/FTP_DIR/aml-turing.pdf" rel="nofollow">Turing computability with Neural Nets</a>. From what I get, it seems that using real-valued weights yields even more computational power, though I\'m not certain of this one.</p>\n\n<p>However, is there any correlation between the computational power of a neural net and its activation function? For example, if the activation function compares the input against a limit of a Specker sequence (something you can\'t do with a regular Turing machine, right?), does this make the neural net computationally "stronger"? Could someone point me to a reference in this direction?</p>\n', 'ViewCount': '204', 'Title': 'Is computational power of Neural networks related to the activation function', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-14T21:29:14.587', 'LastEditDate': '2012-06-14T21:29:14.587', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1789', 'Tags': '<computability><neural-networks>', 'CreationDate': '2012-06-12T23:23:51.467', 'FavoriteCount': '1', 'Id': '2353'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '911', 'Title': 'Must Neural Networks always converge?', 'LastEditDate': '2012-06-20T19:04:21.237', 'AnswerCount': '2', 'Score': '9', 'OwnerDisplayName': 'user1631', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': '<h2>Introduction</h2>\n\n<p><strong>Step One</strong></p>\n\n<p>I wrote a standard backpropegating neural network, and to test it, I decided to have it map XOR.</p>\n\n<p>It is a 2-2-1 network (with tanh activation function)</p>\n\n<pre><code>X1  M1\n        O1\nX2  M2\n\nB1  B2\n</code></pre>\n\n<p>For testing purposes, I manually set up the top middle neuron (M1) to be an AND gate and the lower neuron (M2) to be an OR gate (both output 1 if true and -1 if false).</p>\n\n<p>Now, I also manually set up the connection M1-O1 to be -.5, M2-O1 to be 1, and \nB2 to be -.75</p>\n\n<p>So if M1 = 1 and M2 = 1, the sum is (-0.5 +1 -0.75 = -.25) tanh(0.25) = -0.24</p>\n\n<p>if M1 = -1 and M2 = 1, the sum is ((-0.5)*(-1) +1 -0.75 = .75) tanh(0.75) = 0.63</p>\n\n<p>if M1 = -1 and M2 = -1, the sum is ((-0.5)*(-1) -1 -0.75 = -1.25) tanh(1.25) = -0.8</p>\n\n<p>This is a relatively good result for a "first iteration".</p>\n\n<p><strong>Step Two</strong></p>\n\n<p>I then proceeded to modify these weights a bit, and then train them using error propagation algorithm (based on gradient descent). In this stage, I leave the weights between the input and middle neurons intact, and just modify the weights between the middle (and bias) and output. </p>\n\n<p>For testing, I set the weights to be and .5 .4 .3 (respectively for M1, M2 and bias)</p>\n\n<p>Here, however, I start having issues.</p>\n\n<hr>\n\n<h2>My Question</h2>\n\n<p>I set my learning rate to .2 and let the program iterate through training data (A B A^B) for 10000 iterations or more.</p>\n\n<p><em>Most</em> of the time, the weights converge to a good result. However, at times, those weights converge to (say) 1.5, 5.7, and .9 which results in a +1 output (even) to an input of {1, 1} (when the result should be a -1).</p>\n\n<p>Is it possible for a relatively simple ANN which has a solution to not converge at all or is there a bug in my implementation?</p>\n', 'Tags': '<machine-learning><neural-networks>', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-06-21T00:03:44.897', 'CommentCount': '0', 'AcceptedAnswerId': '2413', 'CreationDate': '2012-06-19T02:17:19.860', 'Id': '2406'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '526', 'Title': 'Could an artificial neural network algorithm be expressed in terms of map-reduce operations?', 'LastEditDate': '2012-06-27T12:56:50.677', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1983', 'FavoriteCount': '0', 'Body': '<p>Could an artificial neural network algorithm be expressed in terms of map-reduce operations? I am also interested more generally in methods of parallelization as applied to ANNs and their application to cloud computing. </p>\n\n<p>I would think one approach would involve running a full ANN on each node and somehow integrating the results in order to treat the grid like a single entity (in terms of input/output and machine learning characteristics.) I would be curious even in this case what such an integrating strategy might look like.</p>\n', 'Tags': '<parallel-computing><artificial-intelligence><neural-networks>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-27T13:12:05.760', 'CommentCount': '0', 'AcceptedAnswerId': '2514', 'CreationDate': '2012-06-26T23:53:38.440', 'Id': '2508'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '118', 'Title': 'How would a neural network deal with an arbitrary length output?', 'LastEditDate': '2012-07-17T19:44:49.720', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2150', 'FavoriteCount': '1', 'Body': "<p>I've been looking into Recurrent Neural Networks, but I don't understand what the architecture of a neural network would look like when the output length is not necessarily fixed. </p>\n\n<p>It seems like most networks I've read descriptions of require the output length to be equal to the input length or at least a fixed size. But how would you do something like convert a word to the string of corresponding phonemes? </p>\n\n<p>The string of phonemes might be longer or shorter than the original word. I know you could sequence in the input characters using 8 input nodes (bitcode of the character) in a recurrent network, provided there's a loop in the network, but is this a common pattern for the output stream as well? Can you let the network provide provide something like a 'stop codon'?</p>\n\n<p>I suppose a lot of practical networks, like those that do speech synthesis should have an output that is not fixed in length. How do people deal with that?</p>\n", 'Tags': '<neural-networks>', 'LastEditorUserId': '2150', 'LastActivityDate': '2012-07-17T19:44:49.720', 'CommentCount': '0', 'AcceptedAnswerId': '2724', 'CreationDate': '2012-07-13T09:19:58.747', 'Id': '2722'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Consider one specific useful function of our human brain: abstraction of object. Take the example of two pictures: if we are told the pictures are similar, we actually make conclusion about the aspects in which they are close to each other.</p>\n\n<p>I'm considering whether machine can have the ability described. More accurately, is it possible to find and select a set of feature representations of two samples (e.g. image, sound) such that under those representations, the samples are similar with respect to a metric, say weighted euclidean norm?</p>\n", 'ViewCount': '99', 'Title': 'Finding Feature Representation Such That Two Samples Are Similar in Feature Space', 'LastActivityDate': '2012-07-16T17:17:41.727', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '2768', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2012-07-15T12:01:45.340', 'FavoriteCount': '2', 'Id': '2749'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am studying for an AI exam and I\'m looking for a better way of solving the following problem:</p>\n\n<blockquote>\n  <p>Graph shows a classification problem in the unit square $[0,1]^2$, where Class A\n  is denoted by the grey area in the figure (not including the points on the dotted lines),\n  and Class B occurs otherwise.</p>\n</blockquote>\n\n<p><img src="http://i.stack.imgur.com/u87ly.jpg" alt="Graph shows a classification problem in the unit square [0,1]^2"></p>\n\n<p>To solve this problem, there is a need of constructing 2 perceptrons: both perceptrons output 1 if the input lies in the grey area, and at least one of the perceptrons outputs 0 otherwise.</p>\n\n<p>There are 3 ways of finding appropriate weights for inputs and biases that I know of:</p>\n\n<ol>\n<li>Trial and error</li>\n<li>Perceptron learning algorithm (involving random weights, learning rate, multiple epochs)</li>\n<li>Geometrical way of finding the vectors of weights (involving finding an orthogonal line to boundary function)</li>\n</ol>\n\n<p>All of them are quite time consuming when you do it with just pen and paper.</p>\n\n<p>Is there a simple way of calculating/finding perceptron weights based on a classification graph?</p>\n', 'ViewCount': '523', 'Title': 'Is there a simple way of calculating perceptron weights based on a classification graph?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-21T18:12:17.233', 'LastEditDate': '2012-08-13T22:24:29.180', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2497', 'Tags': '<artificial-intelligence><neural-networks>', 'CreationDate': '2012-08-13T20:37:14.513', 'FavoriteCount': '2', 'Id': '3153'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have been studying <a href="http://en.wikipedia.org/wiki/Spiking_neural_network" rel="nofollow">Spiking Neural Networks</a> online from various papers, mainly <a href="http://mathsci.kaist.ac.kr/~nipl/am621/lecturenotes/spiking_neurons_2.pdf" rel="nofollow">Maass (1997)</a>. I am not entirely sure I understand what makes SNN\'s pulse-code in contrast to earlier <a href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="nofollow">ANN</a>s which are considered rate-code. \nI have background in neuroscience so I understand the terms and ratio, I\'m asking regarding the actual implementation. </p>\n\n<p>Is the practical difference in the fact that when each neuron updates its current state in an SNN it deals with the entire history of every pre-synaptic neuron and not only the last step? Is that what gives it temporal characteristics which previous generation ANN\'s lack?</p>\n', 'ViewCount': '128', 'Title': "The essential difference between spiking neural networks and earlier generation ANN's", 'LastEditorUserId': '55', 'LastActivityDate': '2012-08-15T19:06:21.157', 'LastEditDate': '2012-08-15T19:06:21.157', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '7', 'OwnerDisplayName': 'Amir', 'PostTypeId': '1', 'Tags': '<terminology><artificial-intelligence><neural-networks>', 'CreationDate': '2012-03-10T16:47:52.833', 'FavoriteCount': '2', 'Id': '3195'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am coding a neural network implementation, but a I have problems in the design. I was wondering about how to compare the <em>output</em> with the <em>target</em>, my neural networks has three outputs</p>\n\n<p><code> groups = {'Iris-virginica':[0,0,1], 'Iris-setosa':[0,1,0], 'Iris-versicolor':[1,0,0]}</code></p>\n\n<p>I know I must translate each output to 0 and 1. </p>\n\n<p>I meant if my result is <code>Iris-virginica</code> and my output is more or less: <code>[0.999979082561091, 0.9999918549147135, 0.9998408912106317]</code>, the subtraction would yield the following result: </p>\n\n<p><code>[-0.999979082561091, -0.9999918549147135, 0.000159]</code></p>\n\n<p>Is that correct, or I need to follow a different approach. Is possible train my net with 0, 1 and 2 values. Do I need to know any more?</p>\n", 'ViewCount': '79', 'Title': 'How to compare the output of a neural network with his target?', 'LastActivityDate': '2012-08-23T19:30:48.420', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2615', 'Tags': '<algorithms><neural-networks>', 'CreationDate': '2012-08-23T17:29:31.833', 'Id': '3303'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '527', 'Title': 'What piece am I missing to turn this idea into a programming language?', 'LastEditDate': '2013-07-20T09:50:34.237', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2161', 'FavoriteCount': '2', 'Body': '<p>I\'ve been doing some reading (I\'ll name drop along the way) and have selected a few scattered ideas that I think could be cobbled together into a nifty esoteric programming language. But I\'m having some difficulty assembling the parts.</p>\n\n<p>Kleene\'s Theorem states: Any Regular Set can be recognized by some Finite-State Machine (Minsky 4.3).</p>\n\n<p>Minsky\'s Theorem 3.5: Every Finite-State machine is equivalent to, and can be "simulated by", some neural net.</p>\n\n<p>"There is a natural way to represent any forest as a binary tree." (Knuth, v1, 333).</p>\n\n<p>And according to Bentley (Programming Pearls, p.126) a binary tree can be encoded as a flat array.</p>\n\n<p>So I\'m imagining an array of bit-fields (say 4 bits so it can easily be worked with in hexadecimal). Each field indicates a type of automaton, and the positions of the array encode (via an intermediary binary tree representation) a forest which approximates (? missing piece ?) the power of a graph.</p>\n\n<p>I\'m somewhat bewildered by the possibilities of automaton sets to try, and of course the fun Universal Automata require three inputs (I worked up an algorithm inspired by Bentley to encode a ternary tree implicitly in a flat array, but it feels like the wrong direction). So I\'d appreciate any side-bar guidance on that. Current best idea: the normal set: and or xor not nand nor, with remaining bits used for threshold weights on the inputs.</p>\n\n<p>So the big piece I\'m missing is a formalism for applying one of these nibble-strings to a datum. Any ideas or related research I should look into?</p>\n\n<hr>\n\n<p><em>Edit:</em> My theoretical support suggests that the type of computations will probably be limited to RL acceptors (and maybe generators, but I haven\'t thought that through).</p>\n\n<p>So, I tried to find an example to flesh this out. The C <code>int isdigit(int c)</code> function performs a logical computation on (in effect) a bit-string. Assuming ASCII, where the valid digits are <code>0x30 0x31 0x32 0x33 0x34 0x35 0x36 0x37 0x38 0x39</code>, so bit 7 must be off, bit 6 must be off, bit 5 must be on, and bit 4 must be on: these giving us the 0x30 prefix; then bit 3 must be off (0-7) or if bit 3 is on, bit 2 must be off and bit 1 must be off (suppressing A-F), and don\'t care about bit 0 (allowing 8 and 9). If you represent the input <em>c</em> as a bit-array (<code>c[0]</code>..<code>c[7]</code>), this becomes</p>\n\n<pre><code>~c[7] &amp; (~c[6] &amp; (c[5] &amp; (c[4] &amp; (~c[3] | (~c[2] &amp; ~c[1])))))\n</code></pre>\n\n<p>Arranging the operators into a tree (colon (:) represents a wire since pipe (|) is logical or),</p>\n\n<pre><code>c[7]  6   5   4   3   2   1   0\n ~    ~   :   :   ~   ~   ~   :\n    &amp;     :   :   :     &amp;\n       &amp;      :      |\n           &amp;        :  \n                &amp;\n</code></pre>\n\n<p>My thought based on this is to insert "input lead" tokens into the tree which receive the values of the input bit assigned in a left-to-right manner. And I also need a <em>ground</em> or <em>sink</em> to explicitly ignore certain inputs (like c[0] above).</p>\n\n<p>This leads me to make NOT (~) a binary operator which negates the left input and simply absorbs right input. And in the course of trying this, I also realized the necessity for a ZERO token to build masks (and to provide dummy input for NOTs).</p>\n\n<p>So the new set is: &amp;(and) |(or) ^(xor) ~(not x, sink y) 0(zero) I(input)</p>\n\n<p>So the tree becomes (flipping up for down)</p>\n\n<pre><code>                 ^\n           &amp;           &amp;\n       &amp;       |      I 0\n     &amp;   I  ~     &amp;\n   &amp;   I   I 0  ~   ~\n ~   ~         I 0 I 0\nI 0 I 0\n=   =  = = =   =   =  =\n7   6  5 4 3   2   1  0 \n</code></pre>\n\n<p>Which encodes into the array (skipping the "forest&lt;=>tree" part, "_" represents a blank)</p>\n\n<pre><code>_ ^ &amp; &amp; &amp; | I 0 &amp; I ~ &amp; _ _ _ _ &amp; I _ _ I 0 ~ ~ _\n  _ _ _ _ _ _ _ ~ ~ _ _ _ _ _ _ _ _ _ _ I 0 I 0 _\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ I 0 I 0 \n</code></pre>\n\n<p>The tree->array encoding always put the root in array(1) so with zero-indexed array, there\'s a convenient blank at the beginning that could be used for linkage, I think.</p>\n\n<p>With only 6 operators, I suppose it could be encoded in octal.</p>\n\n<p>By packing a forest of trees, we could represent a chain of acceptors each applied on the next input depending on the result of the previous.</p>\n', 'Tags': '<programming-languages><finite-automata><arrays><neural-networks><machine-models>', 'LastEditorUserId': '2161', 'LastActivityDate': '2013-11-29T03:20:30.750', 'CommentCount': '6', 'AcceptedAnswerId': '4623', 'CreationDate': '2012-09-19T19:58:45.397', 'Id': '4618'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm currently doing some reading into AI and up to this point couldn't find a satisfying answer to this question: what's the difference between a rule based system and an artificial neural network?</p>\n\n<p>From my understanding both are trying to do inference based on a variety of different inputs.</p>\n", 'ViewCount': '240', 'Title': "What's the difference between a rule based system and an artificial neural network?", 'LastActivityDate': '2012-10-09T16:40:02.740', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '4976', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3121', 'Tags': '<artificial-intelligence><neural-networks>', 'CreationDate': '2012-10-09T09:20:22.983', 'Id': '4972'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we have a classification problem and we wish to solve the problem by Neural Network. What factors must one consider choosing an NN structure? e.g Feed Forward, Recurrent and other available structures. </p>\n', 'ViewCount': '35', 'Title': 'What factors must one consider choosing an NN structure?', 'LastActivityDate': '2013-01-03T06:58:07.757', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7715', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '51', 'Tags': '<artificial-intelligence><neural-networks><neural-computing>', 'CreationDate': '2013-01-03T06:21:59.343', 'Id': '7714'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '226', 'Title': 'Neural Network weight selection using Genetic Algorithm', 'LastEditDate': '2013-01-13T05:13:50.257', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'NAYOSO', 'PostTypeId': '1', 'OwnerUserId': '4573', 'Body': "<p>Hi I want to ask about weight selection in neural network using genetic algorithm. </p>\n\n<p>Right now what I understand is </p>\n\n<ol>\n<li>Initialize population</li>\n<li>Encode the weight of the neural network to the chromosome</li>\n<li>Calculating the error and fitness</li>\n<li>crossover and mutation</li>\n<li>looping until satisfy the condition</li>\n</ol>\n\n<p>Is it the right thing?</p>\n\n<p>if yes what I'm still not sure are :</p>\n\n<ol>\n<li>If I have 50 chromosome in one population that means I must create 50 neural network?</li>\n<li>Let's say I have 100 different input and I want the network to learn it by using weight selection only (not using backpropagation) and how I calculate the error? Testing and calculating the error of every input(using MSE) and divide it by 100?</li>\n</ol>\n\n<p>I think that's all for now</p>\n\n<p>Thank you</p>\n", 'Tags': '<neural-networks>', 'LastEditorUserId': '157', 'LastActivityDate': '2013-01-13T05:13:50.257', 'CommentCount': '0', 'AcceptedAnswerId': '7818', 'CreationDate': '2013-01-04T11:28:57.970', 'Id': '7817'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a multilayer perceptron. It has an input layer with two neurons, a hidden layer with an arbitrary number of neurons, and an output layer with two neurons.</p>\n\n<p>Given that <code>randomboolean</code> and <code>targetboolean</code> are random boolean values, and the network operates as such:</p>\n\n<pre><code>input(randomboolean); //Set the input neurons to reflect the random boolean\npropagateforwards(); //Perform standard forward propagation\noutputboolean = output(); //To get the networks output\nideal(targetboolean); //Performs connection updating via back-prop\n</code></pre>\n\n<p>Is it possible to get the network to map the <code>randomboolean</code> value to the <code>targetboolean</code> value in such a way as the the <code>outputboolean</code> value will correctly match the <code>targetboolean</code> while running in an 'on-line' (where prediction occurs along with continued learning) mode after some arbitrary number of training cycles. </p>\n\n<p>I hear that the network needs to be recurrent to process this as it may be temporal behaviour, however the MLP is a universal computing platform and I assume it should be able to approximate the temporal behaviour needed for this task.</p>\n", 'ViewCount': '144', 'Title': 'About the behaviour of multi-layer perceptrons', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-30T16:06:02.490', 'LastEditDate': '2013-01-25T11:18:32.820', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '9152', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '814', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-01-24T19:14:58.373', 'Id': '9137'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'m very new to neural networks, and have been trying to figure some things out.  So, let\'s say you come across a neural network which has 100 inputs, a hidden layer with 200 nodes, and 32 outputs. Let\'s also say that you, the "discoverer" of this particular instance of a neural network, are able to read the weights of the individual neurons. What could you figure out about it\'s function?</p>\n\n<p>1) Are you able to determine what the algorithm or logic is contained within the neural network?  Other than feeding in all possible inputs and studying the outputs it produces.</p>\n\n<p>2) If you were given information about the connection of the neural network (maybe the network isn\'t fully connected), would solving question one above be easier?</p>\n', 'ViewCount': '181', 'Title': 'What can be learned from the weights in a neural network?', 'LastActivityDate': '2013-03-19T00:03:49.407', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7157', 'Tags': '<machine-learning><neural-networks>', 'CreationDate': '2013-03-05T16:44:38.130', 'FavoriteCount': '1', 'Id': '10295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have on a few occasions trained neural networks (back propagation networks) with some rather complicated data sets (backgammon positions and OCR). When doing this, it seems that a lot of the work involves trying out different configurations of the networks, in order to find the optimal configuration for learning. Often there is a compromise between small nets that are faster to use/learn, and bigger nets, that are able to represent more knowledge.</p>\n\n<p>Then I wonder if it could be possible to make some networks that are both fast and big. I\'m thinking that at network where every neuron ain\'t fully connected ought to be faster to calculate than nets with full connection on all layers. It could be the training that detected that certain inputs are not needed by certain neurons, and therefore remove those connections. In the same way the training could also involve adding new neurons if some neurons seems to be "overloaded".</p>\n\n<p>Is this something that have been tried out with any success ? Does any classes of networks exists with this kind of behavior ?</p>\n', 'ViewCount': '110', 'Title': 'Adapting neural network', 'LastActivityDate': '2013-04-04T00:39:42.330', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10986', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7516', 'Tags': '<machine-learning><neural-networks><learning-theory>', 'CreationDate': '2013-03-31T15:39:51.687', 'FavoriteCount': '2', 'Id': '10937'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Across my studies I have noticed the following statement in my Subject Guide; namely, that two-layer feed-forward neural networks using the sigmoidal activation function are universal. My question is how are the networks 'universal' and what does 'universal' actually mean in this instance?</p>\n\n<p>Thanks in advance.</p>\n", 'ViewCount': '58', 'Title': 'How are two layer feed-forward neural networks universal?', 'LastActivityDate': '2013-04-26T20:49:19.350', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11587', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7919', 'Tags': '<neural-networks>', 'CreationDate': '2013-04-26T20:20:40.117', 'FavoriteCount': '1', 'Id': '11586'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am learning artificial neural networks and i am very new to artificial neural networks. How to create artificial neural network programs and test them?, how to make my artificial neural networks learn?. I mean i there any software trough which we create artificial neural network simulations and test them? Can java be used in these simulations to control these networks? Can these simulations only be used for actual problem solving? Please tell in detail with addition links if required about how to begin developing artificial neural networks?</p>\n', 'ViewCount': '25', 'Title': 'How to test and create ANN models?', 'LastActivityDate': '2013-06-05T19:32:13.090', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8052', 'Tags': '<artificial-intelligence><neural-networks><software-testing>', 'CreationDate': '2013-05-06T15:46:36.523', 'Id': '11833'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am a CS undergraduate (but I don\'t know much about AI though, did not take any courses on it, and definitely nothing about NN until recently) who is about to do a school project in AI, so I pick a topics regarding grammar induction (of context-free language and perhaps some subset of context-sensitive language) using reinforcement learning on a neural network. I started to study previous successful approach first to see if they can be tweaked, and now I am trying to understand the approach using supervised learning with Long Short Term Memory.\nI am reading <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.4170" rel="nofollow">"Learning to Forget: Continual Prediction with LSTM"</a>. I am also reading the paper on peephole too, but it seems even more complicated and I\'m just trying something simpler first. I think I get correctly how the memory cell and the network topology work. What I do not get right now is the training algorithm. So I have some questions to ask:</p>\n\n<ul>\n<li><p>How exactly does different input get distinguished? Apparently the network is not reset after each input, and there is no special symbol to delimit different input. Does the network just receive a continuous stream of strings without any clues on where the input end and the next one begin?</p></li>\n<li><p>What is the time lag between the input and the corresponding target output? Certainly some amount of time lag are required, and thus the network can never be trained to get a target output from an input that it have not have enough time to process. If it was not Reber grammar that was used, but something more complicated that could potentially required a lot more information to be stored and retrieved, the amount of time need to access the information might varied depending on the input, something that probably cannot be predicted while we decide on the time lag to do training.</p></li>\n<li><p>Is there a more intuitive explanation of the training algorithm? I find it difficult to figure out what is going on behind all the complicated formulas, and I would need to understand it because I need to tweak it into a reinforced learning algorithm later.</p></li>\n<li><p>Also, the paper did not mention anything regarding noisy <strong>training</strong> data. I have read somewhere else that the network can handle very well noisy testing data. Do you know if LSTM can handle situation where the training data have some chances of being corrupted/ridden with superfluous information?</p></li>\n</ul>\n', 'ViewCount': '184', 'Title': 'Intuitive description for training of LSTM (with forget gate/peephole)?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T10:53:14.773', 'LastEditDate': '2013-06-26T10:53:14.773', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8835', 'Tags': '<formal-languages><machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-06-24T17:04:54.560', 'Id': '12871'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I just recently learnt about the existence of this Hierarchical Temporal Memory. I already read the <a href="https://www.groksolutions.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf" rel="nofollow">main document</a> (which seems rather easy to understand), but one red flag is that the document is neither peer-reviewed nor attempting to explain why it should work in details. I tried to look around for some independent sources, and found a few papers that compare its performance against others, but none explain why it perform well or not well. I noticed some comments claiming that it was looked down by mainstream expert, but I was unable to find any actual criticisms.</p>\n\n<p>So I would like to ask, what are the criticism regarding the performance of HTM? Assuming the following:</p>\n\n<p>-There are huge amount of training data to use, enough for even months long training session. Basically, any criticisms regarding size or length of training is not relevant.</p>\n\n<p>-Since HTM is meant to be generic, any domain-specific criticism should be related to a more fundamental problem.</p>\n\n<p>Thank you for your help.</p>\n', 'ViewCount': '530', 'Title': 'Some criticisms of Hierarchical Temporal Memory?', 'LastEditorUserId': '8835', 'LastActivityDate': '2013-09-11T23:01:53.720', 'LastEditDate': '2013-07-05T23:24:16.697', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8835', 'Tags': '<neural-networks>', 'CreationDate': '2013-07-04T21:49:23.513', 'FavoriteCount': '2', 'Id': '13089'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can someone please point me towards a rigorous derivation of the energy function of a discrete Hopfield network. What I want, is the derivation must start out with the structure of the network and prove that the critical points of the dynamical system can be obtained by minimization of some function (possibly by constructing the Lyapunov function of the dynamical system). I cannot seem to find such a rigorous proof. Thankyou! </p>\n', 'ViewCount': '135', 'Title': 'Derivation of the energy function of a hopfield network', 'LastActivityDate': '2013-07-09T15:40:53.530', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8912', 'Tags': '<reference-request><machine-learning><neural-networks>', 'CreationDate': '2013-07-07T14:17:15.630', 'Id': '13132'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have implemented a neural network (using CUDA) with 2 layers. (2 Neurons per layer). I'm trying to make it learn 2 simple quadratic polynomial functions using <strong>backpropagation</strong>.</p>\n\n<p>But instead of converging, it is diverging (the output is becoming infinity)</p>\n\n<p>Here are some more details about what I've tried:</p>\n\n<ul>\n<li>I had set the initial weights to 0, but since it was diverging I have\nrandomized the initial weights (Range: -0.5 to 0.5)</li>\n<li>I read that a neural network might diverge if the learning rate is\ntoo high so I reduced the learning rate to 0.000001</li>\n<li>The two functions I am trying to get it to add are: 3 * i + 7 * j+9\nand j*j + i*i + 24 (I am giving the layer i and j as input)</li>\n<li>I had implemented it as a single layer previously and that could\napproximate the polynomial functions better than it is doing now</li>\n<li>I am thinking of implementing momentum in this network but I'm not\nsure it would help it learn</li>\n<li>I am using a linear (as in no) activation function</li>\n<li>There is oscillation in the beginning but the output starts diverging\nthe moment any of weights become greater than 1</li>\n</ul>\n\n<p>I have checked and rechecked my code but there doesn't seem to be any kind of issue with it.</p>\n\n<p>So here's my question: what is going wrong here?</p>\n\n<p>Any pointer will be appreciated.</p>\n", 'ViewCount': '140', 'Title': 'Neural network diverging instead of converging', 'LastActivityDate': '2013-08-03T18:49:20.880', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-08-03T13:20:13.500', 'Id': '13587'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've had a bit of experience programming Neural networks but I am fairly new with genetic algorithms (I'm only 17). I have a major issue that I can't understand. If a child get's one chromatid from one parent and another from another parent, then the genes are crossed over, how do you determine which alleles are active in the child? How do other people go about mating with genetic algorithms. No need to go deep into it because I also want to work somethings out for myself but a general idea would be good enough. Thanks in advance.</p>\n", 'ViewCount': '106', 'Title': "Something I don't understand about Genetic Algorithms", 'LastActivityDate': '2013-08-04T12:46:31.363', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13595', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9506', 'Tags': '<artificial-intelligence><neural-networks><genetic-algorithms>', 'CreationDate': '2013-08-04T12:05:08.643', 'Id': '13594'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Why are neural networks initial weights initialized as random numbers?\nI had read somewhere that this is done to "break the symmetry" and this makes the neural network learn faster.\nHow does breaking the symmetry make it learn faster?</p>\n\n<p>Would\'nt initializing the weights to 0 be a better idea? That way the weights would be able to find their values (whether positive or negative)  faster?</p>\n\n<p>Is there some other underlying philosophy behind randomizing the weights apart from hoping that they would be near their optimum values when initialized?</p>\n', 'ViewCount': '119', 'Title': 'Why are weights of Neural Networks initialized with random numbers?', 'LastActivityDate': '2013-08-23T04:01:53.217', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13883', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-08-23T02:49:56.567', 'Id': '13882'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The energy function in a Hopfield network to determine whether it has converged seems to be the major sink of computational time and makes the Hopfield network run very slowly. Is there a fast substitute for it? </p>\n', 'ViewCount': '20', 'Title': 'Fast energy function substitute for Hopfield network?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:35:57.190', 'LastEditDate': '2013-09-16T08:35:57.190', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10158', 'Tags': '<algorithms><artificial-intelligence><efficiency><neural-networks><heuristics>', 'CreationDate': '2013-09-15T18:54:05.840', 'Id': '14335'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am using neural networks to predict a time series.\nThe question I\'m facing now is how do I encode date/time/serial no. of each input set as an input to the neural network?</p>\n\n<p>Should I use 1 of C encoding (used for encoding categories) as described <a href="ftp://ftp.sas.com/pub/neural/FAQ2.html#A_cat" rel="nofollow">here</a>?</p>\n\n<p>Or Should I just feed it the time (in milliseconds since 1-1-1970)?</p>\n\n<p>Or is feeding it the time unnecessary as long as I feed it the rest of the data chronologically?</p>\n', 'ViewCount': '201', 'Title': 'How to encode date as input in neural network?', 'LastActivityDate': '2013-09-30T15:01:48.540', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '14702', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-09-27T03:58:02.437', 'FavoriteCount': '1', 'Id': '14634'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am reading "Neural Networks and Learning Machines" and in Chapter 11 the book covers Boltzman machines and it is stated "the network [Boltzmann machine] can perform pattern completion", but does not show how this would be done in practice, or how a Boltzmann machine could be used at all.</p>\n\n<p>As I am more the "hands-on" person, and the book is more on the theoretical side of science. I am currently looking for an practical example/paper/article on an Boltzmann machine, maybe even a simple (therefore small) educational implementation. </p>\n\n<p>I\'d appreciate any link/paper/article covering the (<strong>non-restricted</strong>) Boltzmann machine in an understandable ("hands-on") way.</p>\n', 'ViewCount': '43', 'Title': 'practical use of a Boltzmann machine', 'LastActivityDate': '2013-10-09T16:49:03.440', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '78', 'Tags': '<reference-request><neural-networks><boltzmann-machine>', 'CreationDate': '2013-10-09T16:49:03.440', 'Id': '14958'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've been learning about neural networks and SVMs.  The tutorials I've read have emphasized how important kernelization is, for SVMs.  Without a kernel function, SVMs are just a linear classifier.  With kernelization, SVMs can also incorporate non-linear features, which makes them a more powerful classifier.</p>\n\n<p>It looks to me like one could also apply kernelization to neural networks, but none of the tutorials on neural networks I've seen have mentioned this.  Do people commonly use the kernel trick with neural networks?  I presume someone must have experimented with it to see if it makes a big difference.  Does kernelization help neural networks as much as it helps SVMs?  Why or why not?</p>\n\n<p>(I can imagine several ways to incorporate the kernel trick into neural networks.  One way would be to use a suitable kernel function to preprocess the input, a vector in $\\mathbb{R}^n$, into a higher-dimensional input, a vector in $\\mathbb{R}^{m}$ for $m\\ge n$.  For multiple-layer neural nets, another alternative would be to apply a kernel function at each level of the neural network.)</p>\n", 'ViewCount': '123', 'Title': 'Kernelization trick, for neural networks', 'LastActivityDate': '2013-11-02T04:10:50.153', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-10-19T07:01:58.443', 'FavoriteCount': '1', 'Id': '16220'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What is the difference between a Neural Network, a Deep Learning System and a Deep Belief System?</p>\n\n<p>As I recall your basic neural network is a 3 layers kinda thing,\nand I have had Deep Belief Systems described as being neural networks stacked on top of each other.</p>\n\n<p>I've not personally heard of a Deep Learning Systems, but i strongly suspect it is a synonym for Deep Belief System. In fact I believe that Deep Belief System is the less common term. Can anyone confirm this?</p>\n", 'ViewCount': '414', 'Title': 'What is the difference between a Neural Network, a Deep Learning System and a Deep Belief System?', 'LastEditorUserId': '11043', 'LastActivityDate': '2013-10-31T03:35:06.613', 'LastEditDate': '2013-10-31T02:47:39.893', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '11043', 'Tags': '<machine-learning><neural-networks><boltzmann-machine>', 'CreationDate': '2013-10-29T14:04:58.587', 'FavoriteCount': '1', 'Id': '16545'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My aim is to classify types of cars (Sedans,SUV,Hatchbacks) and earlier I was using corner features for classification but it didn\'t work out very well so now I am trying Gabor features.<br/></p>\n\n<p><a href="http://www.mathworks.in/matlabcentral/fileexchange/38844-gabor-image-features" rel="nofollow">code from here</a><br/></p>\n\n<p>Now the features are extracted and suppose when I give an image as input then for 5 scales and 8 orientations I get 2 [1x40] matrices.</p>\n\n<p><strong>1. 40 columns of squared Energy.</strong></p>\n\n<p><strong>2. 40 colums of mean Amplitude.</strong></p>\n\n<p>Problem is I want to use these two matrices for classification and I have about 230 images of 3 classes (SUV,sedan,hatchback).</p>\n\n<p>I do not know how to create a [N x 230] matrix which can be taken as vInputs by the neural netowrk in matlab.(where N be the total features of one image).</p>\n\n<p>My question:</p>\n\n<ol>\n<li><p>How to create a one dimensional image vector from the 2 [1x40] matrices for one image.(should I append the mean Amplitude to square energy matrix to get a [1x80] matrix or something else?)</p></li>\n<li><p>Should I be using these gabor features for my purpose of classification in first place? if not then what?</p></li>\n</ol>\n\n<p>Thanks in advance</p>\n', 'ViewCount': '53', 'Title': 'Making feature vector from Gabor filters for classification', 'LastActivityDate': '2013-11-16T13:58:45.183', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18073', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11370', 'Tags': '<machine-learning><neural-networks><image-processing><classification>', 'CreationDate': '2013-11-14T06:23:16.160', 'Id': '18009'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Neural networks can have multiple inputs.\nBut some times two or more of these inputs can often be related to a single entity.</p>\n\n<p>E.g : Height and weight of a person to predict the probability of disease or price and Volume of a stock to predict it's value.</p>\n\n<p>How can we make a neural network understand that two inputs are related to the same entity?\nIs there any efficient (or inefficient) way of '<em>tagging</em>' them together?</p>\n", 'ViewCount': '82', 'Title': 'How to make a Neural network understand that multiple inputs are related to the same entity?', 'LastEditorUserId': '9479', 'LastActivityDate': '2013-12-19T07:01:09.173', 'LastEditDate': '2013-12-19T03:40:55.333', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9479', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2013-12-18T08:25:48.263', 'Id': '19090'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I want to use <a href="http://www.sciencedirect.com/science/article/pii/S0952197609001626" rel="nofollow">\u201cFingerprint matching using multi-dimensional ANN\u201d\nby Rajesh Kumar and B.R. Deva Vikram</a> [<a href="https://www.dropbox.com/s/6uxx9874bvc4hqo/neural_network_fingerprint.pdf" rel="nofollow">content link</a>] for fingerprint identification. But I have a serious problem understanding what is a Multidimensional Artificial Neural Network.</p>\n\n<p>I searched for keywords MDANN and Multidimensional ANN but I can\'t find anything. I also can\'t be sure what Kumar meant about a MDANN in his article. So far, the closest thing I can relate with Kumar\'s MDANN is Convolution Nets but I don\'t believe this is what he meant. Can anybody help me about understanding this type of ANN?</p>\n', 'ViewCount': '36', 'Title': 'Multi-dimensional Neural Network for fingerprint matching', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-18T22:55:00.277', 'LastEditDate': '2013-12-18T22:55:00.277', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19104', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11579', 'Tags': '<artificial-intelligence><neural-networks>', 'CreationDate': '2013-12-18T21:40:31.647', 'Id': '19103'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>This is more of a conceptual question.</p>\n\n<p>I have learned about Neural Nets, and I have some clue as to how Support Vector Machines work. I read somewhere however that given the appropriate kernel (is that right?), the SVM is identical to the Neural Net. Could someone who understands this please enlighten me as to how that's possible?</p>\n", 'ViewCount': '25', 'Title': 'Support Vector Machines as Neural Nets?', 'LastActivityDate': '2013-12-28T06:36:46.847', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12414', 'Tags': '<machine-learning><neural-networks><kernel>', 'CreationDate': '2013-12-28T06:36:46.847', 'FavoriteCount': '1', 'Id': '19337'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am developing a simple backprop neural network with n inputs and 1 output. I am using a sigmoid activation function. [Aforge.Net]</p>\n\n<p>I have read that it is good to normalise the input and output data prior to training, which I have done using a simple linear relation (max/min mapping) to normalise between [0, 1]. </p>\n\n<p>My question is, what happens when upon using the NN after training, I get inputs that are larger/smaller than the max/min of the training sets (used for normalisation)? Should I be storing new max/mins and using these to re-normalise my new inputs? or do I just normalise the data with the parameters used in the training set and simply give the network inputs that may be greater than 1 or less than 0?</p>\n', 'ViewCount': '51', 'Title': 'Neural Network Normalization and de-Normalisation of data', 'LastEditorUserId': '12442', 'LastActivityDate': '2013-12-29T21:11:35.707', 'LastEditDate': '2013-12-29T20:22:38.927', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12442', 'Tags': '<neural-networks>', 'CreationDate': '2013-12-29T11:58:34.200', 'Id': '19364'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Deep Learning, now one of the most popular fields in Artificial Neural Network, has shown great promise in terms of its accuracies on data sets. How does it compare to Spiking Neural Network. Recently Qualcomm unveils its zeroth processor on SNN, so I was thinking if there are any difference if deep learning is used instead. </p>\n', 'ViewCount': '71', 'Title': 'What are the key differences between Spiking Neural Network and Deep Learning', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-25T19:49:57.550', 'LastEditDate': '2014-02-10T08:40:54.117', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14541', 'Tags': '<machine-learning><neural-networks>', 'CreationDate': '2014-02-10T07:43:27.300', 'Id': '21487'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>For many machine learning projects that we do, we start with the k Nearest Neighbour classifier. This is an ideal starting classifier as we usually have sufficient time to calculate all distances and the number of parameters is limited (k, distance metric and weighting)</p>\n\n<p>However, this has often the effect that we stick with the knn classifier as later in the project there is no room for switching to another classifier. What would be good reason to try a new classifier. Obvious ones are memory and time restraints, but are there cases when another classifier can actually improve the accuracy? </p>\n', 'ViewCount': '24', 'Title': 'When should I move beyond k nearest neighbour', 'LastActivityDate': '2014-02-24T18:16:51.620', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '21980', 'Score': '8', 'OwnerDisplayName': 'Rhand', 'PostTypeId': '1', 'Tags': '<neural-networks><classification>', 'CreationDate': '2014-02-11T12:18:44.347', 'Id': '21977'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am having an AI exam in two weeks, and I am still figuring out certain concepts and ideas, related to Bayesian Nets, Hidden Markov Chains, Conditional Random Fields and Neural Nets (yes it is all going to be tested and yes we have a text (AI - A Modern Approach), but no, we did not cover everything in class).</p>\n\n<p>I "know" a few things about the mathematical descriptions of all of those, but I know pretty much nothing about their usage or practical applications.</p>\n\n<p>Here are my questions (and I apologize for my naivety):</p>\n\n<ol>\n<li>What kind of machine learning algorithm classes do they belong to? Since they all need training, does it mean that they are all supervised learning algorithms?</li>\n<li>They all have an underlying structure that allows a graph to represent them, where directed edges denote dependencies between states. The probability of being in a state is computed as a conditional probability from ancestors of the state. Does that sound about right?</li>\n<li>In what kind of situation do you want to use which of the algorithms? Is it possible to some it up, or does it require subtle differentiation and expert-level knowledge?</li>\n<li>Why do Neural Nets get special treatment? I heard of many classes teaching Neural Nets, but I have heard of no such thing for the other guys.</li>\n</ol>\n', 'ViewCount': '52', 'Title': 'How are Bayesian Nets, Hidden Markov Chains, Conditional Random Fields and Neural Nets related?', 'LastActivityDate': '2014-02-28T16:11:53.907', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15080', 'Tags': '<machine-learning><artificial-intelligence><neural-networks><probabilistic-algorithms><markov-chains>', 'CreationDate': '2014-02-26T20:00:12.453', 'Id': '22062'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm not sure what the purpose of the o(1-o) in the back propagation algorithm achieves? I'm guessing it's related to using the sigmoid function on the output but I'd like to have a proper understanding of the math behind it. Thanks!</p>\n", 'ViewCount': '44', 'Title': 'What\'s the purpose of the "o(1-o)" in the back propagation algorithm', 'LastEditorUserId': '55', 'LastActivityDate': '2014-03-02T12:29:42.360', 'LastEditDate': '2014-03-02T12:29:42.360', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'OwnerDisplayName': 'user11406', 'PostTypeId': '1', 'Tags': '<neural-networks>', 'CreationDate': '2014-03-01T20:41:03.517', 'Id': '22189'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>To calculate the error in back propagation you would use,\n(target_output - actual_output) * actual_output * (1 - actual_output)</p>\n\n<p>So what does, actual_output * (1 - actual_output) solve?</p>\n\n<p>Wouldn't, [target_output - actual_output] be the amount it's incorrect by?</p>\n", 'ViewCount': '45', 'ClosedDate': '2014-03-27T07:55:51.317', 'Title': 'In back propagation why is this necessary, o (1 - o)', 'LastEditorUserId': '9479', 'LastActivityDate': '2014-03-17T16:29:15.307', 'LastEditDate': '2014-03-12T10:21:12.580', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'OwnerDisplayName': 'user15206', 'PostTypeId': '1', 'Tags': '<machine-learning><artificial-intelligence><neural-networks>', 'CreationDate': '2014-03-02T21:39:20.853', 'Id': '22207'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If my goal were to compress say 10,000 images and I could include a dictionary or some sort of common database that the compressed data for each image would reference, could I use a large dictionary shared by the entire catalog and therefore get much smaller file sizes?  Could this be expanded to work with images in general, i.e. to replace something like JPEG?</p>\n\n<p>Are there existing compression systems that operate like this, where there is a large common set of bits transmitted and loaded before decompression, that has been built by analyzing many images?</p>\n\n<p>For example, is there an existing computer science/machine learning research effort using sparse autoencoding over a large set of images and this concept of distributing a network derived from that encoding with the decompressor?</p>\n\n<p>Note: I have deleted quite a bit of context from this question because it was claimed that this made the question too "opinionated".  I also had to edit the title after someone else modified the title in such a way as to change the meaning of the question.  If you are looking for clarification about my question, please ask.</p>\n', 'ViewCount': '165', 'Title': 'Does there exist a data compression algorithm that uses a large dataset distributed with the encoder/decoder?', 'LastEditorUserId': '15323', 'LastActivityDate': '2014-04-06T22:59:48.903', 'LastEditDate': '2014-03-07T02:16:22.923', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15323', 'Tags': '<machine-learning><neural-networks><data-compression><computer-vision>', 'CreationDate': '2014-03-05T21:00:31.943', 'Id': '22317'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Phase space learning <a href="http://cseweb.ucsd.edu/users/gary/pubs/nips94.pdf" rel="nofollow">Paper1</a> and <a href="http://gliamac.mit.edu/people/seung/papers/continuous.pdf" rel="nofollow">Paper2</a> in neural network represents the input  in higher dimension in auto associative learning. So, the network functions as an auto-associative memory where dynamical attractors are fixed points, each corresponding to one of the patterns that we want to store in the network. If the network has 3 input nodes, the the number of outputs will also be 3. Thus, for a time instant, $t$ Input is a vector. </p>\n\n<p>The Authors mention that a trajectory is traced out. In my case, for every time instant I have a feature vector of dimension 3 i.e there are three features. In my opinion, for every time step, t a learning algorithm is used to update the weights. Also. the Authors mention delay embedding for time series reconstruction which is an essential point in any phase space representation. Based on these, the following concepts are unclear and I will be thankful for an intuitive explanation. </p>\n\n<p>Q1. How come a feature vector is delay embedded and a trajectory is formed since we have only one example at every time instant? </p>\n\n<p>Q2. I will appreciate ideas in how to apply phase space learning with particle Swarm optimization and the objective function is the mean square error between the actual output of the network and the target. In my application, I only have the final target vector not a target vector for every time step. </p>\n', 'ViewCount': '14', 'Title': 'Supervised learning based on phase space representation', 'LastEditorUserId': '10583', 'LastActivityDate': '2014-03-11T03:06:09.200', 'LastEditDate': '2014-03-11T03:06:09.200', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10583', 'Tags': '<machine-learning><neural-networks><pattern-recognition><research>', 'CreationDate': '2014-03-11T02:56:38.210', 'Id': '22489'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I start to suspect this problem is very hard now that I cannot find a single relevant literature on the subject, but it\'s too late to change the class project topics now, so I hope any pointers to a solution. Please pardon the somewhat artificial scenerio of this question, but here goes:</p>\n\n<p>Technical version: </p>\n\n<p>Let $\\Sigma_{c}$ and $\\Sigma_{q}$ and $\\Sigma_{a}$ be 3 disjoint finite alphabet (c, q, a stand for content, query and answer respectively). Let $L_{c}\\in\\Sigma_{c}^{*}$ and $L_{q}\\in\\Sigma_{q}^{*}$ be FINITE languages, wherein $L_{q}$ have the property that for every string in the language all of its prefix are in the language too. There is an unknown function $f:L_{c}\\times L_{q}\\rightarrow\\Sigma_{a}^{*}$. Consider a mysterious machine that receive continuous stream of symbol through a channel one at a time step (we assume that the symbol are clearly distinguishable). This machine, whenever being feed with a string in $c\\in L_{c}$ (with the symbol in correct temporal order) followed by a string in $q\\in L_{q}$ will output (through a different output channel) the value of $f(c,q)$ as a temporal sequence, one symbol at a time. Note that the machine always output after every new symbol from $\\Sigma_{q}$. Note that the empty string is in $L_{q}$, which means the machine also output something before any symbol on $\\Sigma_{q}$ have arrived, but only if it is certain with high probability that the full string in $L_{c}$ have been received.</p>\n\n<p>The objective is to construct a neural network that emulate that mysterious machine, if we have only access to its input and output channel to use as training data, and we do not know $f$. We also have to assume that the input channel are noisy in the following sense: random noise are inserted into the input channel at high probability, delaying input symbol, and we initially do not know which one is noise and which one is authentic; also symbol in the input channel are sometimes lost at low probability. EDIT: Note: we do not know $L_{c}$ nor $L_{q}$, only the mysterious machine know, in fact we do not even know the alphabet $\\Sigma_{c}$ and $\\Sigma_{q}$ other than the fact that they are disjoint and are subset of the set of all possible input symbol (input symbol not in either set are certainly noise, but we can\'t tell which set it belongs to initially; note that it is still possible for symbol from the alphabet to be noise).</p>\n\n<p>(why neural network: beside the noise problem, also because that\'s what I wrote in my class project proposal)</p>\n\n<p>(layman version: consider Sherlock Holmes sitting in his chair, bored. Dr. Watson give a short description of the client. Once he\'s done, Sherlock Holmes give a conclusion about the client. Dr. Watson is astonished, and ask more question, and Sherlock Holmes reply. The conclusion must obviously based on the description alone; and subsequent answer have to answer the question being asked, taking into account the contexts which consists of question already being asked (for example, the same "How did you know?" following "Age?" demands different answer than when following "Height?"). Now you want to make a neural network that simulate Sherlock Holmes, having all the recordings of those session. Dr. Watson however tend to insert in long description that are rather irrelevant, making long statement before finally getting around to ask question, and sometimes accidentally omit crucial information, but otherwise describe people in a rather fixed order of details. The neural network must be able to deal with that. Of course, this is a just a layman\'s description, the situation is much less complex.)</p>\n\n<p>I have looked through various relevant literature, and I cannot find anything relevant. Conversion to spatial domain is useless due to high amount of noise causing very long input sequence. I have looked into LSTM to deal with the memory problem over arbitrary long time lag, but I for the life of me cannot figure out how is the network is supposed to be trained when there are arbitrarily long noise insertion everywhere or possibilities of missing symbol (every method I found seems to force a fixed time-lag between input and output, and missing symbol immediately wreck any method based on predicting the next item in the sequence). Also, is it too much to ask for network that isn\'t too hard to code? Integrate-and-fire neuron is even worse than LSTM in term of difficulty in coding.</p>\n\n<p>Thanks for your help. It\'s due in 2 days, so please be fast.</p>\n', 'ViewCount': '37', 'Title': 'Neural network: noisy temporal sequence converter (transducer?producer?) on demand?', 'LastEditorUserId': '15741', 'LastActivityDate': '2014-04-15T10:54:18.623', 'LastEditDate': '2014-03-16T10:28:14.080', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15741', 'Tags': '<formal-languages><neural-networks>', 'CreationDate': '2014-03-16T06:32:31.097', 'Id': '22666'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Which kind of machine learning concept / model is used in <a href="http://www.20q.net" rel="nofollow">20 Questions</a>?<br>\nIs this kind of thing best solved by a neural network?<br>\nWhere I can read something about it?</p>\n', 'ViewCount': '44', 'Title': 'What kind of model is used by 20 Questions?', 'LastActivityDate': '2014-03-20T22:24:41.397', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22885', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15915', 'Tags': '<machine-learning><neural-networks><data-mining>', 'CreationDate': '2014-03-20T16:48:31.383', 'FavoriteCount': '2', 'Id': '22872'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>My neural net is having trouble switching the sign of a weight. The issue is that the deltas applied to the weight are proportional to that weight, so when it gets closer to zero, the deltas become smaller and are never sufficient to get it past that point. I tried adding a momentum term with 5% of the previous iteration contributing to the current iteration without success.</p>\n\n<p>In my example I have a very simple neural net with one input and one output and no hidden layer. The transfer function in the neural net is a sigmoid, and the function I am trying to learn is also a sigmoid. Specifically: $y = S(10 x - 5)$ where S is the sigmoid function. The neural net gets to a point where its internal weights are essentially generating a function very close to $y = S(2x + 0)$. At this point it gets stuck because the fitted function is sometimes above and sometimes below the desired function, but the weight on $x$ needs to go up and the weight on the constant term needs to go down. I ran it for 1,000 epochs and 100,000 epochs and it stays in the same place. The initial weights were 0.6 and 0.2, so it made improvements over that.</p>\n\n<p>How can a net get over this hump? Should I not be scaling the weight delta by the weight value?</p>\n\n<p><a href="http://www.willamette.edu/~gorr/classes/cs449/linear2.html" rel="nofollow">Here\'s</a> the reference I\'m using for computing the weights for back-propagation.  I am using a learning rate of 0.05 and apply the deltas after each training example.  The x-values (input) used in training are 0.00, 0.01, 0.02, ... 0.99, 1.00.  I always play them in order.</p>\n\n<p>Thanks for your help.</p>\n\n<p>Note: I <a href="http://stackoverflow.com/questions/22725336/how-can-an-artificial-neural-net-change-the-sign-of-a-weight?noredirect=1#comment34633956_22725336">originally posted</a> this on stack overflow and was directed here.</p>\n', 'ViewCount': '39', 'Title': 'How can an artificial neural net change the sign of a weight?', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-24T12:55:05.880', 'LastEditDate': '2014-03-29T05:18:10.917', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '23221', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16260', 'Tags': '<neural-networks>', 'CreationDate': '2014-03-29T04:40:03.687', 'Id': '23209'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am doing a project in order to recognize an apple. (I am using Emgucv with Visual Studio 2010 C#, if that's relevant). My project is a classification (is or is not an apple). I have 2000 images of apples but I need images for the second class. </p>\n\n<p>I have read about classification using ANN but they have multiple classes but I need to recognize if image contain an apple or not. So what kind of images do I need for the second classes? I want to use background of apples like second class. Is that a good idea?</p>\n\n<p>I only want that ANN recognize if a image contain a pen or not so I have 2 classes(pen and non-pen) My question is What kind of image can I use to the second class? For example My first class are images of pen and second class are images of non pen (pencil, apples, grapes, tables); is that correct?</p>\n", 'ViewCount': '54', 'ClosedDate': '2014-04-29T23:35:57.197', 'Title': 'What are good counter-examples when training an apple classifier?', 'LastEditorUserId': '39', 'LastActivityDate': '2014-04-05T11:35:16.640', 'LastEditDate': '2014-04-05T11:35:16.640', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16399', 'Tags': '<machine-learning><neural-networks><data-sets>', 'CreationDate': '2014-04-02T23:33:24.867', 'Id': '23374'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am doing a project to recognize a kind of leaf using ANNs with Emgu CV in C#. My project is to get frames from camera then present them to the ANN and have the ANN tell me if that frame contain a leaf or does not contain a leaf. </p>\n\n<p>I am using 1000 images of leaves that I want to recognize, so if the frame contain a leaf, the ANN will tell me that frame contains one.  I do not know what kind of images I should use for the second class. I want to use images perhaps, for example, like pens, rocks, or telephones all together in the second class.</p>\n', 'ViewCount': '14', 'ClosedDate': '2014-04-10T22:30:27.440', 'Title': 'What types of images should I use for negative examples in a classification problem?', 'LastEditorUserId': '88', 'LastActivityDate': '2014-04-10T19:51:49.737', 'LastEditDate': '2014-04-10T19:51:49.737', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16457', 'Tags': '<artificial-intelligence><neural-networks><classification>', 'CreationDate': '2014-04-04T20:25:40.320', 'Id': '23434'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>i\'m a little bit lost ... can you help me ?</p>\n\n<p>So I have this table of date (each row give a point with its group)</p>\n\n<p><img src="http://i.stack.imgur.com/h3wef.png" alt="data table"></p>\n\n<p><img src="http://i.stack.imgur.com/RnEgL.jpg" alt="enter image description here"></p>\n\n<p>So i took a random weight let\'s say : [1, -2]\nH = 1 if n =&lt; 0\n    0 otherwise</p>\n\n<p>a= H([1,-2][6,3]) = H(0) = 1 but the target output is 0 ... so we have to update the weight:\nw -> w - p = [5 , -5] .\nNext : </p>\n\n<p>a= H([5,-5][3,3]= H(0)=1 the problem is : there the target output is 1 so we don\'t have to update the weight , but that\'s strange because the [5 -5] vector doesn\'t draw a linear separation between the 2 groups ...</p>\n\n<p>Thanks for your help ? :) </p>\n', 'ViewCount': '55', 'ClosedDate': '2014-04-14T17:40:03.533', 'Title': "Perceptron learning rule doesn't work", 'LastActivityDate': '2014-04-08T12:53:53.387', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16532', 'Tags': '<algorithms><machine-learning><neural-networks>', 'CreationDate': '2014-04-07T14:53:01.470', 'Id': '23518'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>That's the problem</p>\n\n<p>$$y=(x,w,\\rho) = \\begin{cases}\n 1 &amp; \\sum_{i=1}^3 w_ix_i &gt;\\rho\\\\\n 0 &amp; \\text{otherwise}\n\\end{cases},$$</p>\n\n<p>where $x=\\{x_1,x_2,x_3\\}$ are inputs, $w=\\{w_1,w_2,w_3\\}$ are weights and\n$\\rho$ is the threshold value.</p>\n\n<p>The problem asks to find an opportune set of weights that can \nclassify these inputs.</p>\n\n<p>$$A = \\{(1, 2, 0), (\u22121, 3, 0), (\u22122, \u22123, 0)\\},$$ \n$$B = \\{(0,1,2),(9,0,1),(\u22123,\u22123,3)\\}.$$</p>\n\n<p>The first step is to assign a random weights to all inputs</p>\n\n<p>$w_1= 0.5$, $w_2= 0.7$, $w_3=0.3$.</p>\n\n<p>For the first example\n$(1,2,0)$ that I know is part of the A class</p>\n\n<p>$\\sum_{i=1}^3 w_ix_i=1.9 &lt; \\rho~ \\Rightarrow y=0$ is the result.</p>\n\n<p>I need to update the weights but I can't understand how.</p>\n\n<p>The formula is</p>\n\n<p>$w_i'=w_i*n*x(t-y)$.</p>\n\n<p>Correct?</p>\n", 'ViewCount': '51', 'Title': 'Perceptron learning rule for classification', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T08:18:17.777', 'LastEditDate': '2014-04-09T11:47:08.043', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '-3', 'PostTypeId': '1', 'OwnerUserId': '12495', 'Tags': '<artificial-intelligence><probability-theory><neural-networks><classification>', 'CreationDate': '2014-04-09T09:05:18.430', 'Id': '23589'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have been reading a lot and I am still unsure of how to determine this. Let's say I have an initial binary state vector (1, 1, 1). How would I go about determining whether (1, 1, 1) is a fixed point in the network? </p>\n\n<p>My initial understanding was to put the state through the network and if it converged to (1, 1, 1) then it is a fixed point. However I have a feeling it is not as simple as that.</p>\n\n<p>Thanks.</p>\n", 'ViewCount': '50', 'Title': 'How to determine if a state is a fixed point in a Hopfield network?', 'LastActivityDate': '2014-04-23T18:01:06.973', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '24004', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16944', 'Tags': '<neural-networks>', 'CreationDate': '2014-04-20T22:42:10.000', 'Id': '23969'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have made and trained a simple neural network which now seems to produce outputs reasonable  in all the aspects but one: it gives negative values from time to time even though the outputs are always either 0 or 1 (positive) in all my treaining samples (the actual neural outputs show to be more or less close to the answer most probable but never actually reach them, I consider this ok). Won\'t "telling" the network what output values are just impossible improve its performance.</p>\n\n<p>Perhaps my mistake is in choosing activation functions. As far as my input values domain is (-1, 1) I use TANH for input neurons, but I have no idea what is the logic to choose hidden and output neurons activation functions and so I set the same (TANH) for all of them (update: I have just noticed that I have mistakenly chosen linear activation function for the output layer while inpit and hidden layers use TANH).</p>\n\n<p>Clearly a negative value means "the answer is very probable to be zero" in this case and it works just fine from the practical point of view but it still gives me a sense that I am doing something wrong.</p>\n', 'ViewCount': '23', 'Title': 'How do I "tell" a simple perceptron/adaline neural netrork its output can\'t be negative?', 'LastEditorUserId': '16894', 'LastActivityDate': '2014-04-25T14:32:37.770', 'LastEditDate': '2014-04-23T18:23:43.567', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16894', 'Tags': '<neural-networks>', 'CreationDate': '2014-04-23T18:11:40.750', 'Id': '24052'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>As far as I know choosing an activation function for the input layer is relatively straightforward: I use Sigmoid if the input data domain is (0,1) and TANH if it is (-1,1).</p>\n\n<p>But what activation functions to set for hidden and output layers? Is there any conventional logic for making thish choice reasonably? How do I know/set the domain of a neuron layer output?</p>\n', 'ViewCount': '17', 'Title': 'How to choose proper activation functions for hidden and output layers of a perceptron neural network?', 'LastActivityDate': '2014-04-23T20:18:15.173', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16894', 'Tags': '<neural-networks>', 'CreationDate': '2014-04-23T20:18:15.173', 'Id': '24061'}},