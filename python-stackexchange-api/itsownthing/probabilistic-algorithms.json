1310:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I <a href="http://blog.computationalcomplexity.org/2004/04/blum-complexity-measures.html" rel="nofollow">read</a> that the number of coin tosses of a probabilistic Turing machine (PTM) is not a <a href="http://en.wikipedia.org/wiki/Blum_axioms" rel="nofollow">Blum complexity measure</a>. Why?</p>\n\n<p>Clarification:</p>\n\n<p>Note that since the execution of the machine is not deterministic, one should be careful about defining the number of coin tosses for a PTM $M$ on input $x$ in a way similar to the time complexity for NTMs and PTMs. One way is to define it as the maximum number of coin tosses over possible executions of $M$ on $x$.</p>\n\n<p>We need the definition to satisfy the axiom about decidability of $m(M,x)=k$. We can define it as follows:</p>\n\n<p>$$\nm(M,x) =\n\\begin{cases}\nk &amp; \\text{all executions of $M$ on $x$ halt, $k=\\max$ #coin tosses} \\\\\n\\infty &amp; o.w. \\\n\\end{cases}\n$$</p>\n\n<p>The number of random bits that an algorithm uses is a complexity measure that appears in papers, e.g. "algorithm $A$ uses only $\\lg n$ random bits, whereas algorithm $B$ uses $n$ random bits".</p>\n', 'ViewCount': '119', 'Title': 'Is the number of coin tosses of a probabilistic Turing machine a Blum complexity measure?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-28T17:32:38.167', 'LastEditDate': '2012-03-28T17:32:38.167', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<computability><complexity-theory><randomness><probabilistic-algorithms>', 'CreationDate': '2012-03-27T20:35:16.207', 'Id': '835'},1311:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '114', 'Title': 'Randomized String Searching', 'LastEditDate': '2012-05-10T14:52:46.263', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1393', 'FavoriteCount': '2', 'Body': "<p>I need to detect whether a binary pattern $P$ of length $m$ occurs in a binary text $T$ of length $n$ where $m &lt; n$.</p>\n\n<p>I want to state an algorithm that runs in time $O(n)$ where we assume that arithmetic operations on $O(\\log_2 n)$ bit numbers can be executed in constant time. The algorithm should accept with probability $1$ whenever $P$ is a substring of $T$ and reject with probability of at least $1 - \\frac{1}{n}$ otherwise.</p>\n\n<p>I think fingerprinting could help here. But I can't get it.</p>\n", 'Tags': '<algorithms><strings><searching><probabilistic-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T14:52:46.263', 'CommentCount': '3', 'AcceptedAnswerId': '1718', 'CreationDate': '2012-05-07T07:56:27.833', 'Id': '1712'},1312:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given three matrices $A, B,C \\in \\mathbb{Z}^{n \\times n}$ we want to test whether $AB  \\neq C$. Assume that the arithmetic operations $+$ and $-$ take constant time when applied to numbers from $\\mathbb{Z}$.</p>\n\n<p>How can I state an algorithm with one-sided error that runs in $O(n^2)$ time and prove its correctness?</p>\n\n<p>I tried it now for several hours but I can't get it right. I think I have to use the fact that for any $x \\in \\mathbb{Z}^n$ at most half of the vectors $s \\in S = \\left\\{1, 0\\right\\}^n$  satisfy $x \\cdot s = 0$, where $x \\cdot s$ denotes the scalar product$\\sum_{i=1}^{n} x_is_i$.</p>\n", 'ViewCount': '232', 'Title': 'Probabilistic test of matrix multiplication with one-sided error', 'LastEditorUserId': '41', 'LastActivityDate': '2013-05-24T03:04:37.160', 'LastEditDate': '2012-05-12T20:19:48.560', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '1811', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<algorithms><probabilistic-algorithms><matrices><linear-algebra>', 'CreationDate': '2012-05-12T19:15:56.887', 'Id': '1809'},1313:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the usual definition of probabilistic poly-time machine it is said that the machine halts in polynomial time for all inputs. </p>\n\n<p>Is the intention really to say that the machine halts for all inputs, or that if it halts it must be in polynomial time?</p>\n', 'ViewCount': '124', 'Title': 'Probabilistic poly-time machine always halts on all inputs?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-25T20:13:41.407', 'LastEditDate': '2012-05-25T11:15:04.400', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'OwnerDisplayName': 'user583311', 'PostTypeId': '1', 'Tags': '<complexity-theory><terminology><turing-machines><probabilistic-algorithms>', 'CreationDate': '2012-05-24T14:21:16.507', 'Id': '2047'},1314:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Is there a way to create a single edge on a graph that connects 3 or more nodes? For example, let's say that the probability of Y occurring after X is 0.1, and the probability of Z occurring after Y is 0.001, but the probability of Z occurring after <em>both</em> X and Y occur is 0.95. If the probabilities are assigned to each edge as weights, how can I make this happen?</p>\n\n<p>$$X _\\overrightarrow{0.1} Y$$</p>\n\n<p>$$Y _\\overrightarrow{0.001} Z$$</p>\n\n<p>$$\\overrightarrow{X \\underrightarrow{} Y \\underrightarrow{0.95}} Z$$</p>\n", 'ViewCount': '110', 'Title': 'An edge that connects more than two nodes in a graph?', 'LastEditorUserId': '2214', 'LastActivityDate': '2012-07-20T01:26:43.280', 'LastEditDate': '2012-07-19T21:55:12.563', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2214', 'Tags': '<algorithms><graphs><probabilistic-algorithms><weighted-graphs>', 'CreationDate': '2012-07-19T21:42:57.053', 'Id': '2826'},1315:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Wondering about any known relations between <a href="http://qwiki.stanford.edu/index.php/Complexity_Zoo%3aR#rl" rel="nofollow">$\\mathsf{RL}$</a> complexity class (one sided error with logarithmic space) and its complementary class, $\\mathsf{coRL}$.</p>\n\n<p>Are they the same class?</p>\n\n<p>What are $\\mathsf{coRL}$\'s relation to $\\mathsf{NL}$, $\\mathsf{P}$?</p>\n', 'ViewCount': '99', 'Title': 'What is known about coRL and RL?', 'LastActivityDate': '2012-08-03T21:42:01.333', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '3032', 'Score': '4', 'OwnerDisplayName': 'Uri', 'PostTypeId': '1', 'OwnerUserId': '2356', 'Tags': '<complexity-theory><complexity-classes><probabilistic-algorithms>', 'CreationDate': '2012-08-01T23:47:44.633', 'Id': '3031'},1316:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to create a bloom filter of 208 million URLs. What would be a good choice of bit vector size and number of hash functions? I tried a bit vector of size 1 GB and 4 hash functions, but it resulted in too many false positives while reading.</p>\n\n<p>I have a huge web corpus containing web content of billions of URLs. I need to process the web content of URLs satisfying certain criteria: the URL should have appeared in web search results in the past 7 days at least 5 times. This is represented by a list of 208 million URLs. Joining the list directly with the web corpus is not feasible because of volume. So I am considering creation of a bloom filter out of the list and then using the bloom filter to prune out unnecessary URLs from the web corpus.</p>\n', 'ViewCount': '387', 'Title': 'Bloom Filter for 208 million URLs', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:17:46.310', 'LastEditDate': '2013-06-26T13:17:46.310', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2881', 'Tags': '<data-structures><probabilistic-algorithms><searching><big-data><bloom-filters>', 'CreationDate': '2012-09-19T15:58:11.313', 'Id': '4616'},1317:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '121', 'Title': 'BPP search: what does boosting correctness entail?', 'LastEditDate': '2012-11-07T10:22:52.953', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4488', 'FavoriteCount': '2', 'Body': u'<p>It is not really clear to me, how and if I can do boosting for correctness (or error reduction) on a <a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29" rel="nofollow">BPP</a> (bounded-error probabilistic polynomial-time) search problem. Can anyone of you explain me how it works?</p>\n\n<p>With BPP search, I mean a problem that can have false positive-negative, correct solution, and no-solution. Here\'s a definition:</p>\n\n<p>A probabilistic polynomial-time algorithm $A$ solves the search problem of the relation $R$ if</p>\n\n<ul>\n<li>for every $x \u2208 S$, $Pr[A(x) \u2208 R(x)] &gt; 1 - \u03bc(|x|)$</li>\n<li>for every $x \u2209 SR$, $Pr[A(x) = \\text{no-solution}] &gt; 1 - \u03bc(|x|)$</li>\n</ul>\n\n<p>were $R(x)$ is the set of solution for the problem and $\u03bc(|x|)$ is a negligible function (it is rare that it fails).</p>\n\n<p>So now I would like to increase my probability of getting a good answer, how can I do it?</p>\n\n<hr>\n\n<p>~ ".. boosting for correctness.." : a way to increase the probability of the algorithm (generally by multile runs of the probabilistic algorithm), i.e., when the problem have a solution then the algorithm likely return a valid one.</p>\n', 'Tags': '<probabilistic-algorithms><search-problem>', 'LastEditorUserId': '4221', 'LastActivityDate': '2012-11-08T06:49:27.833', 'CommentCount': '3', 'AcceptedAnswerId': '6537', 'CreationDate': '2012-11-06T08:05:34.063', 'Id': '6503'},1318:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '298', 'Title': 'Prove or refute: BPP(0.90,0.95) = BPP', 'LastEditDate': '2013-01-07T22:38:45.893', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '1589', 'FavoriteCount': '3', 'Body': '<p>I\'d really like your help with the proving or refuting the following claim: $BPP(0.90,0.95)=BPP$. In computational complexity theory, BPP, which stands for <a href="http://en.wikipedia.org/wiki/BPP_%28complexity%29">bounded-error probabilistic polynomial time</a> is the class of decision problems solvable by a probabilistic Turing machine in polynomial time, with an error probability of at most $\\frac{1}{3}$ for all instances. $BPP=BPP(\\frac{1}{3},\\frac{2}{3})$.</p>\n\n<p>It is not immediate that any of the sets are subset of the other, since if the probability for an error is smaller than $0.9$ it doesn\'t have to be smaller than $\\frac{1}{3}$ and if it is bigger than $\\frac{2}{3}$ it doesn\'t have to be bigger than $0.905$. </p>\n\n<p>I\'m trying to use Chernoff\'s inequality for proving the claim, I\'m not sure exactly how.\nI\'d really like your help. Is there a general claim regarding these relations that I can use?</p>\n', 'Tags': '<complexity-theory><time-complexity><probabilistic-algorithms>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-08T19:20:38.810', 'CommentCount': '4', 'AcceptedAnswerId': '7829', 'CreationDate': '2013-01-07T21:40:43.267', 'Id': '7820'},1319:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I know that $\\sf BPP[2/3,1/3]= BPP[\\alpha,\\beta]$ when $\\alpha\\lt\\beta$, but I read something on Wikipedia which got me confused:</p>\n\n<blockquote>\n  <p>In practice, an error probability of $1/3$ might not be acceptable, however, the choice of $1/3$ in the definition is arbitrary. It can be any constant between $0$ and $1/2$ (exclusive) and the set $\\mathsf{BPP}$ will be unchanged.</p>\n</blockquote>\n\n<p>The reason for my question is the this question that I'm trying to answer:</p>\n\n<p>We define the class $PP_{\\frac{7}{8}}$: $L \\in PP_{\\frac{7}{8}}$. There's a probabilistic Turing machine that  for $x \\in L$ accepts $x$ with probability $&gt;$ than $\\frac{7}{8}$ and for $x \\notin L$ it accepts $x$ with probabilty $\\leq \\frac{7}{8}$.</p>\n\n<p>So by the $\\alpha, \\beta$ first definition I can conclude  that $PP_{\\frac{7}{8}}$ which  equals to $\\sf BPP[7/8,7/8+\\epsilon]$ also equals to $\\sf BPP[2/3,1/3]$ but the I am asked to prove that $\\sf NP \\subseteq  BPP$ which we don't know yet.</p>\n", 'ViewCount': '52', 'Title': 'BPP clarification', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-21T21:48:07.393', 'LastEditDate': '2013-01-21T21:48:07.393', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1183', 'Tags': '<complexity-theory><time-complexity><probabilistic-algorithms>', 'CreationDate': '2013-01-21T21:06:27.130', 'Id': '9077'},13110:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Let $S$ be a finite set of integers (this set contains about 200000 elements). Let $T \\subset S$ be a particular subset of $S$ called <em>target</em>. $S$ keeps growing. So does $T$. Each new element of $S$ might or might not be in $T$.</p>\n\n<p>No (known, or practical) algorithm can determine if an element $s \\in S$ is in the <em>target</em> set: a human being must give the final word (ie, it is subjective). It is estimated that $T$ has about 30000-35000 elements. I already know $T_1$, a first approximation of $T$, with about 25000 elements. I also already know some thousands of elements of $S$ that are certainly not in $T$.</p>\n\n<p>What I want is a way to approximate $T$ as closely as possible, and present only those elements to a human being. Also, for each new element of $S$, I want to determine if it has high probability of being in $T$ -- and present only those with high probability to a human being.</p>\n\n<hr>\n\n<p>Now, I describe what I can use to try to approximate $T$.</p>\n\n<p>Each integer $s \\in S$ has some <em>labels</em> associated. These can be represented as subsets $L_i \\subset S, \\forall i \\in \\{1, ..., n\\}$ ($n$ is about 250). These subsets are known, determined by algorithms (ie, I have functions $l_i \\to \\{in,out\\}$ such that $l_i(s) = in \\iff s \\in L_i$).</p>\n\n<p>Some label algorithms are very fast, some are slow. Anyways, these labels (ie, the sets $L_i$) have already been determined. Some of these labels contain very few (1-100) elements, some contain a lot (100000-150000). Many labels are independent, some are closely related (ie, I know that some labels are subsets of others, I know that some are disjoint, etc).</p>\n\n<hr>\n\n<p>So, given this framework, what kind of algorithms can I use to approximate $T$? They can be interactive, ie, they could get better after each new approximation of $T$, if this makes the problem easier.</p>\n\n<p>I thought about using a <strong>genetic algorithm</strong> to determine which labels, when intersected, give good approximations of $T$. However, this can get slow, with a na\xefve intersection algorithm (ie, suppose $L_1, L_2, L_3$ are to be intersected; if they are all "big" (50000-150000), it can be quite time consuming to calculate the intersection! -- now, imagine a gene that would require to intersect, say, 50 labels...).</p>\n\n<p>How can I speed this, without sacrificing too much the precision?</p>\n', 'ViewCount': '43', 'Title': 'Approximate target subset by intersecting other subsets', 'LastEditorUserId': '7051', 'LastActivityDate': '2013-02-26T22:51:21.717', 'LastEditDate': '2013-02-26T22:51:21.717', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7051', 'Tags': '<algorithms><probabilistic-algorithms><finite-sets>', 'CreationDate': '2013-02-26T22:42:08.043', 'Id': '10128'},13111:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '143', 'Title': 'Are probabilistic search data structures useful?', 'LastEditDate': '2013-03-03T16:57:40.847', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7022', 'FavoriteCount': '1', 'Body': '<p>A SkipList provides the same $O(\\log n)$ bounds for search as a balanced tree with the advantage that rebalancing isn\'t necessary. Since the SkipList is constructed using random coin flips, these bounds only hold as long as the structure of the SkipList is sufficiently "balanced". In particular, with probability $1/n^c$ for some constant $c&gt;0$, the balanced structure might be lost after inserting an element.</p>\n\n<p>Let\'s say I want to use a skip list as a storage backend in a web application that potentially runs forever. So after some polynomial number of operations, the balanced structure of the SkipList is very likely to be lost. </p>\n\n<p>Is my reasoning correct? Do such probabilistic search/storage data structures have practical applications and if so, how is the above problem avoided? </p>\n\n<p>Edit: I\'m aware that there are deterministic variants of the SkipList, which, are much more complicated to implement in comparison to the (classic) randomized SkipList.</p>\n', 'Tags': '<data-structures><search-trees><probabilistic-algorithms>', 'LastEditorUserId': '7022', 'LastActivityDate': '2013-03-08T13:10:42.580', 'CommentCount': '1', 'AcceptedAnswerId': '10239', 'CreationDate': '2013-03-03T14:09:52.280', 'Id': '10229'},13112:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much for an expected running time. Am I making a mistake here?</p>\n", 'ViewCount': '321', 'Title': 'How to get expected running time of hash table?', 'LastActivityDate': '2013-03-12T23:38:43.887', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '10500', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><hash-tables><hash><probabilistic-algorithms>', 'CreationDate': '2013-03-12T23:20:41.050', 'Id': '10498'},13113:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I get values $x_t$ in an online fashion and want to buy "good" ones, where "good" means that some measure $P(x_t) &gt;T$. Consider the following simple algorithm.</p>\n\n<pre><code>T = 0.7\nN = 100 // or any value N &gt; B\nB = 20 // or any value 1 &lt; B &lt; N\n\nl = 0\n\nfor t from 1 to N:\n    input a new observation x_t\n    let P(x_t) the probability associated to x_t\n\n    if P(x_t) &gt; T:\n        l = l + 1\n        pay 1 dollar to buy y_t the label of x_t\n        output immediately the label y_t\n</code></pre>\n\n<p>If the condition $P(x_t) &gt; T$ is used then we get about $l = 100-70 = 30$, this is ok since the value of $T$ is set to $0.7$.</p>\n\n<p>Now if I want to add a constraint which is: additionally to the fact that elements $x_t$ for which the label $y_t$ is purchased are those for which $P(x_t) &gt; T$, I want also that we do not buy more than $B=20$ labels (for example because we only have 20 dollars as budget).</p>\n\n<p>But the problem is that, if I replace the the condition ($P(x_t) &gt; T$) by ($P(x_t) &gt; T \\wedge l &lt; B$), then the elements $x_t$ for which we buy a label are more likely to be among the first elements $t$ that we browse (that is, for an element $x_{95}$ for $t = 95$ for example we will never have a chance to buy its label even if its probability was $P(x_{95}) \\gg T$). But I want that all the elements from $t = 1$ to $N$ will have equal chance to buy their label (not advantaging only the first elements).</p>\n\n<p>Note: the condition that $P(x_t) &gt; T$ for buying the label of a new observation $x_t$, should not be removed from my code. This is important for me: only labels of observations for which $P(x_t)$ was higher than $T$ at time $t$, are possibly purchased; and we should not purchase more than our budget $B$. Note also that a purchased label should immediately be output after we buy it, we should not wait until the end to decide if we buy it or not.</p>\n\n<p>Also, note that we do not have the N elements beforehand; at each time $t$ we see just one new observation $x_t$. And note that you pay 1 dollar when you select a given $x_t$ to ask for its label and that you should output answer (label of selected $x_t$) immediately; so you can not select some $B$ elements then replace them with new selected other elements, because your budget $B$ will already be finished.</p>\n', 'ViewCount': '64', 'Title': 'How to sample uniformly from a stream of elements, some of which are unsuited?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-04-18T19:57:32.787', 'LastEditDate': '2013-04-18T17:53:54.317', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2895', 'Tags': '<probabilistic-algorithms><online-algorithms><sampling>', 'CreationDate': '2013-04-17T22:07:38.767', 'Id': '11370'},13114:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>This is more of an open-ended information question, but to make it concrete, here's an example problem I have thought up:</p>\n\n<p>Consider an $N\\times N$ grid, $N$ odd, and consider that a single chunk of rare, valuable metal, lies buried under one of the unit squares with uniform probability.</p>\n\n<p>An agent, $X$, starts at the center square of this grid, and this agent can do three things:</p>\n\n<ul>\n<li>Move to any adjacent square, (no diagonals), with cost $M &gt; 0$</li>\n<li>Dig at the current square, with cost $D &gt; 0$, in search of the treasure.  If the treasure is buried at said square, the agent is guaranteed to find it.</li>\n<li><p>With cost $S, 0 &lt; S &lt; D$, the agent may attempt to 'detect' the presence of the treasure with a radar device.  Given the actual location of the treasure, set $L$ to be the Euclidean distance between the center of $X$'s square and the square containing the treasure.</p>\n\n<p>The agent then receives, after conducting the test, a random real-valued number $r$, from the uniform distribution on the interval $[0, 1/(L^2 +1) ] $.  Note that the agent can, and will, with 100% probability, receive different signals from the same square over multiple tests.</p></li>\n</ul>\n\n<p>Given that the agent is a perfect logician, what is the expected total cost $C$ he will incur on a search for the treasure?</p>\n\n<hr>\n\n<p>My suspicion is that this problem is, at the very least, NP-hard in terms of $N^2$, the size of the grid.  And though I suspect a perfect agent would, given a proper adversarial input, incur infinite cost over time, there exists a comprehensive naive solution involving no signal tests, with worst-case cost $M*(N^2-1)+D*N^2$, and so the agent must always only consider choices with expected total-cost of finding the treasure less than this bound - i.e., stick to a finite number of decision paths.</p>\n\n<p><strong>Does anyone have any experience or familiarity with a problem like this?</strong> Is there a name for this variety of problem?  Is there, perhaps, some other choice of signal function that has been studied more in-depth?  I'm not interested in a deterministic function, which would be rather trivial to analyze - one key property of this function that makes the problem interesting is that, for any finite number of tests, there is non-zero probability that all the signals come back in the range $0 &lt; r &lt; \\epsilon$, where $\\epsilon$ is the minimum upper bound on a signal distribution given the grid size $N$ - such an adversarial set of signals would convey 0 information asymptotically over time, which means that a deterministic, signal-based solution algorithm (much less an optimal one) is impossible.</p>\n\n<hr>\n\n<p>Also, I realize that optimal strategies will likely vary wildly for different values of $M$, $S$, and $D$, so for the purpose of discussion, let's suppose they are $3$, $1$, and $10$ respectively.</p>\n", 'ViewCount': '48', 'Title': 'Signal-based Search', 'LastActivityDate': '2013-05-17T01:58:39.543', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7614', 'Tags': '<probabilistic-algorithms>', 'CreationDate': '2013-05-17T01:58:39.543', 'FavoriteCount': '1', 'Id': '12081'},13115:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Bloom filter use a hash function to test membership for S by checking if an item is present of not at the specified position. To mitigate the effect of hash collision, multiple functions are used, yielding probabilistic bound if using universal hash.\nWe can use 10 bits per elements to have 'reasonable' error rate.</p>\n\n<p>If we could build directly a perfect hashing function for the set  S + $\\infty$, where last the element is one not present in S, we could use only 1 bit per element and have perfect recovery.</p>\n\n<p>What are the fundamental reasons why this reasonning is wrong ?</p>\n", 'ViewCount': '182', 'Title': 'Bloom filter and perfect hashing', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:07.190', 'LastEditDate': '2014-04-29T11:35:07.190', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4469', 'Tags': '<data-structures><hash><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2013-06-03T15:20:55.557', 'Id': '12444'},13116:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '254', 'Title': 'Why do Bloom filters work?', 'LastEditDate': '2014-04-29T11:35:41.260', 'AnswerCount': '3', 'Score': '4', 'OwnerDisplayName': 'user220201', 'PostTypeId': '1', 'OwnerUserId': '8842', 'Body': "<p>Let's say I am using Bloom filters to create a function to check if a word exists in a document or not.  If I pick a hash function to fill out a bit bucket for all words in my document. Then if for a given number of words, wouldn't the whole bit bucket be all 1s? If so then checking for any word will return true? What am I missing here? </p>\n", 'Tags': '<data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:41.260', 'CommentCount': '0', 'AcceptedAnswerId': '12838', 'CreationDate': '2013-06-22T13:12:57.470', 'Id': '12834'},13117:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '180', 'Title': 'Computer science problems related to music?', 'LastEditDate': '2013-07-10T08:56:09.893', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2529', 'FavoriteCount': '1', 'Body': '<p>Are there any CS problems, preferably open, that are related to music or musical theory somehow? I would think of problem with musical notation but also probabilities when randomizing according to a scale or a tonality or general what is considered harmony in frequencies and physics, electromagnetism and waveforms. </p>\n\n<p>Can you give examples of the area I want to know of?</p>\n\n<p>For instance, given an algorithm that guesses a melody, how successful will the melody be in resembling an artist or likewise decision problem that could be feasible or what do you think?</p>\n', 'Tags': '<probability-theory><decision-problem><probabilistic-algorithms>', 'LastEditorUserId': '31', 'LastActivityDate': '2013-07-16T07:52:42.197', 'CommentCount': '2', 'AcceptedAnswerId': '13200', 'CreationDate': '2013-07-10T08:06:34.607', 'Id': '13199'},13118:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I guess at the heart of this is that I don\'t really understand hash functions.\n<img src="http://upload.wikimedia.org/wikipedia/commons/5/58/Hash_table_4_1_1_0_0_1_0_LL.svg" alt=""></p>\n\n<p>One article says any function mapping objects to an object of fixed size:</p>\n\n<blockquote>\n  <p>A hash function usually means a function that compresses, meaning the\n  output is shorter than the input. Often, such a function takes an\n  input of arbitrary or almost arbitrary length to one whose length is a\n  \ufb01xed number, like 160 bits.</p>\n</blockquote>\n\n<p>Why not just use a random number generator to generate the hash keys?  Wikipedia  suggests <a href="http://en.wikipedia.org/wiki/SHA-1" rel="nofollow">SHA1</a> which maps to $2^{160}\\approx 10^{48}$ possible outputs has never experienced a "collision".</p>\n\n<p>I was trying to understand why the hash is necessary in a <a href="http://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/" rel="nofollow">probabilistic counting algorithm</a>:</p>\n\n<pre><code>class LinearCounter():\n    def __init__(self, m, h):\n        self.array =  [False for x in range(m)]\n        self.hash = h\n\n    def add(value):\n        self.array(self.hash[ value ]) = True\n</code></pre>\n\n<p>Is the hash necessary? Why can\'t we use a random number generator right away?</p>\n\n<pre><code>def add(value):\n    self.array(int(m*random.random()))\n</code></pre>\n', 'ViewCount': '218', 'Title': 'why not just use a random number generator as a hash function?', 'LastActivityDate': '2013-09-25T19:24:41.857', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '3131', 'Tags': '<hash><probabilistic-algorithms>', 'CreationDate': '2013-09-25T17:24:39.487', 'Id': '14601'},13119:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume you have an object moving at a constant velocity(up, down, left, right) in a grid. You have unlimited resources (memory, time). At any given time step in the grid, you can "guess" the location of the object, and you win if it is currently at that position. Without relying on probability techniques, what would be the best way to approach catching this object?</p>\n', 'ViewCount': '30', 'Title': 'finding a an object with constant velocity on an infinite grid in discrete time steps', 'LastActivityDate': '2013-11-26T10:28:16.433', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11632', 'Tags': '<algorithms><search-algorithms><probabilistic-algorithms>', 'CreationDate': '2013-11-26T09:27:30.743', 'Id': '18366'},13120:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm using machine-learning algorithms to solve binary classification problem (i.e. classification can be 'good' or 'bad'). I'm using <code>SVM</code> based algorithms, <code>LibLinear</code> in particular. When I get a classification result, sometimes, the probability estimations over the result are pretty low. For example, I get a classification 'good' with probability of 52% - those kind of results I rather throw away or maybe classify them as 'unknown'.</p>\n\n<p><strong>EDITED - by D.W.'s suggestion</strong></p>\n\n<p>Just to be more clear about it, my output is not only the classification 'good' or 'bad', I also get the confidence level (in %). For example, If I'm the weather guy, I'm reporting that tomorrow it will be raining, and I'm 52% positive at my forecast. In this case, I'm sure you won't take your umbrella when you leave home tomorrow, right? So in those cases where my model does not have a high confidence level I throw away this prediction and don't count it in my estimations.</p>\n\n<p>Unfortunately, I can't find articles regarding thresholding the probability estimations... </p>\n\n<p>Does anyone have an idea what is a normal threshold that I can set over the probability estimations? or at least can refer me to a few articles about it?</p>\n", 'ViewCount': '69', 'Title': 'What would be a decent threshold for classification problem?', 'LastEditorUserId': '11754', 'LastActivityDate': '2014-04-20T10:08:52.793', 'LastEditDate': '2013-12-21T08:24:16.580', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11754', 'Tags': '<machine-learning><probabilistic-algorithms><statistics><classification>', 'CreationDate': '2013-12-20T07:53:10.210', 'Id': '19146'},13121:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I know that standard Bloom Filters only have operations like inserting elements and checking if an element belongs to filter, but are also some modification of Bloom filters which enable a delete operation--for example: counting Bloom filters. I heard also about another method, which uses a second filter. If I want to remove an element I have to 'insert' it into this second filter. I can't find how this proposed structure operates, any article about it, or even the name of the originator. Maybe someone can share with me with a link to any interesting articles about this method? I found a lot of articles about counting Bloom filters and other methods, but I can't find any description of this one.</p>\n", 'ViewCount': '101', 'Title': 'Deleting in Bloom Filters', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:49.423', 'LastEditDate': '2014-04-29T11:35:49.423', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12038', 'Tags': '<reference-request><data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2013-12-25T22:52:19.973', 'Id': '19292'},13122:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm a CS undergrad so my math/CS knowledge is not that deep so please correct me if my premise is flawed or I have made some incorrect assumptions.</p>\n\n<p>So I was thinking, much in the way that some primality testers are probabilistic(they give you yes or no but have a chance to be wrong). Would it be possible to build a probabilistic halting problem solver? One that reports within a certain degree of error, whether a problem halts or not?</p>\n", 'ViewCount': '70', 'Title': 'Possible to construct a probabilistic halting problem solver?', 'LastActivityDate': '2014-02-13T02:24:54.360', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '13245', 'Tags': '<probability-theory><halting-problem><probabilistic-algorithms>', 'CreationDate': '2014-02-12T23:13:06.320', 'FavoriteCount': '1', 'Id': '21581'},13123:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am having an AI exam in two weeks, and I am still figuring out certain concepts and ideas, related to Bayesian Nets, Hidden Markov Chains, Conditional Random Fields and Neural Nets (yes it is all going to be tested and yes we have a text (AI - A Modern Approach), but no, we did not cover everything in class).</p>\n\n<p>I "know" a few things about the mathematical descriptions of all of those, but I know pretty much nothing about their usage or practical applications.</p>\n\n<p>Here are my questions (and I apologize for my naivety):</p>\n\n<ol>\n<li>What kind of machine learning algorithm classes do they belong to? Since they all need training, does it mean that they are all supervised learning algorithms?</li>\n<li>They all have an underlying structure that allows a graph to represent them, where directed edges denote dependencies between states. The probability of being in a state is computed as a conditional probability from ancestors of the state. Does that sound about right?</li>\n<li>In what kind of situation do you want to use which of the algorithms? Is it possible to some it up, or does it require subtle differentiation and expert-level knowledge?</li>\n<li>Why do Neural Nets get special treatment? I heard of many classes teaching Neural Nets, but I have heard of no such thing for the other guys.</li>\n</ol>\n', 'ViewCount': '52', 'Title': 'How are Bayesian Nets, Hidden Markov Chains, Conditional Random Fields and Neural Nets related?', 'LastActivityDate': '2014-02-28T16:11:53.907', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15080', 'Tags': '<machine-learning><artificial-intelligence><neural-networks><probabilistic-algorithms><markov-chains>', 'CreationDate': '2014-02-26T20:00:12.453', 'Id': '22062'},13124:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For probabilistic algorithms such as <a href="http://en.wikipedia.org/wiki/PageRank" rel="nofollow">PageRank</a> and <a href="http://kamvar.org/assets/papers/eigentrust.pdf" rel="nofollow">EigenTrust</a>, the stopping case is given as $|R_{t+1} - R_{t}| &lt; \\epsilon$ (i.e. convergence is assumed). Neither the papers on EigenTrust or PageRank, or the PageRank wiki page, give any <em>clear</em> indication of what $\\epsilon$ should be. </p>\n\n<p>I believe it might be something to do with the damping factor $d = 0.85$; specifically $\\epsilon = 1 - d = 0.15$, but  I can\'t be sure. </p>\n\n<p>How is $\\epsilon$ determined, and if it\'s nothing more than an abitrary value $0 \\leq \\epsilon \\leq 1$, how would you choose a sensible value?</p>\n', 'ViewCount': '41', 'Title': 'PageRank and EigenTrust: How small should epsilon be?', 'LastEditorUserId': '755', 'LastActivityDate': '2014-03-11T02:45:24.150', 'LastEditDate': '2014-03-11T02:45:24.150', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10926', 'Tags': '<probabilistic-algorithms><markov-chains>', 'CreationDate': '2014-03-10T11:57:49.503', 'Id': '22467'},13125:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am reading the proof of PCP theorem in <a href="http://people.csail.mit.edu/madhu/papers/1992/almss-conf.pdf" rel="nofollow">Proof Verication and Hardness of Approximation Problems</a>. The following paragraph appears in section 3 (page 4), <em>"Outline of the Proof of the Main Theorem"</em>.</p>\n\n<blockquote>\n  <p>The results of these sections show that $NP \\subset OPT (poly(n), 1)$ (Theorem 5) and $NP \\subset OPT (\\log n, poly \\log n)$ (Theorem 8). Theorems 9 and 10 show that the recursion idea applies to these proof systems, and in particular shows the following:</p>\n  \n  <ol>\n  <li>$OPT (f (n), g(n)) \\subset OPT (f (n) + O(\\log g(n)), (\\log g(n))^{O(1)} )$ and</li>\n  <li>$OPT (f (n), g(n)) \\subset OPT (f (n) + (g(n))^{O(1)} , 1)$.</li>\n  </ol>\n  \n  <p>This allows us to conclude that $NP \\subset OPT (\\log n, poly \\log \\log n)$ (<strong>by composing</strong> two $OPT (\\log n, poly \\log n)$ proof systems) and then <strong>by composing</strong> this system with the $OPT (poly(n), 1)$ proof system we obtain $OPT (\\log n, 1)$ proof system for $NP$.</p>\n</blockquote>\n\n<p><strong>Edit.</strong> Composition of verifiers is defined in <a href="http://www.cs.umd.edu/~gasarch/pcp/AS.pdf" rel="nofollow">Probabilistic Checking of Proofs: A New Characterization of NP</a>  section 3 (page 13) <em>"Normal Form Verifiers and Their Use in Composition"</em>.</p>\n\n<blockquote>\n  <p>Let $r, q, s, t$ be any functions defined on the natural integers. Suppose there is a normal-form verifier $V_2$ that is $(r(n), s(n), q(n), t(n))$-constrained. Then, for all functions $R, Q, S, T$, $$RPCP(R(n), S(n), Q(n), T(n)) \\subseteq \\\\RPCP(R(n) + r(\\tau), s(\\tau), Q(n) + q(\\tau), Q(n)t(\\tau))$$ where $\\tau$ is a shorthand for $O((T(n))^2)$.</p>\n</blockquote>\n\n<p>Where $RPCP(r, s, q, t)$ is a $PCP(r, s \\cdot q)$ which takes $t$ time to accept of reject <em>after</em> reading the $s \\cdot q$ bits.</p>\n\n<p>I still don\'t see how this composition works. Is $OPT(r, q) = RPCP(r, q, 1, q)$ and a normal form verifier? In that case it seems to work, but then just composing $OPT(poly(n), 1)$ with $OPT(\\log n, poly \\log n)$ is enough, so why bother with $OPT(\\log n, poly \\log \\log n)$ or relations $1.$ and $2.$?</p>\n', 'ViewCount': '83', 'Title': 'Proof of PCP theorem', 'LastEditorUserId': '5167', 'LastActivityDate': '2014-03-23T10:41:07.547', 'LastEditDate': '2014-03-23T10:41:07.547', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '5167', 'Tags': '<complexity-theory><proof-techniques><probabilistic-algorithms>', 'CreationDate': '2014-03-15T07:51:32.813', 'Id': '22644'},13126:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say I plugged in a <a href="http://en.wikipedia.org/wiki/Hardware_random_number_generator" rel="nofollow">hardware true-random number generator (TRNG)</a> to my computer, then wrote programs with output that depends on the TRNG\'s output. Can it do anything non-trivial that a Turing machine with a psuedo-random number generator can\'t do? (a trivial thing it can do would be generating truly random numbers)</p>\n', 'ViewCount': '37', 'Title': 'Are there any practical differences between a Turing machine with a PRNG and a probabilistic Turing machine?', 'LastActivityDate': '2014-03-17T20:49:58.900', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15790', 'Tags': '<turing-machines><randomness><probabilistic-algorithms>', 'CreationDate': '2014-03-17T19:39:13.557', 'Id': '22720'},13127:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose we have $n$ balls and $n$ bins. We put the balls into the bins randomly. If we count the maximum number of balls in any bin, the expected value of this is  $\\Theta(\\ln n/\\ln\\ln n)$. How can we derive this fact? Are Chernoff bounds helpful?</p>\n', 'ViewCount': '63', 'ClosedDate': '2014-04-23T16:47:14.277', 'Title': 'Expected maximum bin load, for balls in bins with equal number of balls and bins', 'LastEditorUserId': '755', 'LastActivityDate': '2014-04-19T00:38:01.780', 'LastEditDate': '2014-04-19T00:38:01.780', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15406', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><probabilistic-algorithms><chernoff-bounds>', 'CreationDate': '2014-04-18T22:43:53.940', 'Id': '23925'},13128:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been playing around with a simple probabilistic data structure which is very similar to a Bloom filter. Where a Bloom filter would use $k$ independent hash functions to choose $k$ of the $m$ bits to set, this structure uses $m$ hash functions, and sets each bit with probability $p$.</p>\n\n<p>This structure doesn't produce as low a false-positive rate as Bloom filters, but it seems to be extremely fast to compute, particularly if $m$ is some multiple of the machine word size and $p = 2^{-b}$ for some integer $b$: The hash functions can be computed in parallel by AND-ing $b$ independent $m$-bit hashes, and no dependent indexing or variable bitshifts are required.</p>\n\n<p>I'm certain someone's come up with this idea before me, and done a lot more advanced analysis and comparison of it than I'm qualified to do. Is there a particular name for this type of structure?</p>\n", 'ViewCount': '33', 'Title': 'Bloom filter variant', 'LastEditorUserId': '8410', 'LastActivityDate': '2014-04-29T17:07:58.197', 'LastEditDate': '2014-04-29T13:27:12.427', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24230', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8410', 'Tags': '<reference-request><data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2014-04-29T10:55:19.287', 'Id': '24217'}