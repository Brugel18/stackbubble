{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am having a problem trying to solve a question on a past paper asking to design a security lattice. Here is the question:</p>\n\n<blockquote>\n  <p>The AB model (Almost Biba) is a model for expressing integrity policies rather\n      than confidentiality. It has the same setup as Bell-LaPadula, except that $L$ is now a set of\n      integrity levels which express the degree of confidence we have in the integrity of\n      subjects and objects. Subjects and data at higher integrity levels are considered\n      to be more accurate or safe. The set of subjects and objects may also be different,\n      for example, programs are naturally considered as subjects.</p>\n  \n  <p>Often, the set $L$ is actually a lattice of levels, with two operations: least\n      upper bound $l_1 \\vee l_2$ and greatest lower bound $l_1 \\wedge l_2$, where $l_1, l_2 \\in L$.</p>\n  \n  <p>i. Design an example integrity lattice for AB, by combining two degrees of\n          data integrity <strong>dirty</strong> and <strong>clean</strong> and two means by which a piece of input\n          may be received, <strong>website</strong> (external user input from a web site form) and\n          <strong>dataentry</strong> (internal user input by trusted staff).</p>\n</blockquote>\n\n<p>I have been looking for an explanation on how to build lattices but can't seem to find one on the internet or in textbooks. Can anyone point me in the right direction?</p>\n", 'ViewCount': '237', 'Title': 'Security Lattice Construction', 'LastEditorUserId': '157', 'LastActivityDate': '2012-04-17T15:40:08.717', 'LastEditDate': '2012-04-15T22:12:05.363', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '1321', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '995', 'Tags': '<security><lattices><integrity>', 'CreationDate': '2012-04-15T11:16:22.733', 'Id': '1288'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What are efficient and accurate techniques for monitoring the recoverability and integrity of files in very large preservation archives?</p>\n\n<p>In very large archives, the time taken to recompute checksums periodically (scrubbing) is substantial, perhaps taking more than all the available time depending on the read bandwidth available! Also, each access to a preserved file increases the risk of damage due to hardware or software failure. Tapes are most stable in a cold, dark place far from exposure to the hazards of data centers. Disks are most at risk when the read/write head is flying close to the medium. All approaches are probabilistic, so which are most efficient and accurate?</p>\n\n<p>To give the problem specificity, let's assume a fixed probability of local single-bit errors for each medium (one probability for tape, another for disk, SSD, etc) during a standard time period, and ignore all other types of errors (loss of an entire volume, for instance). We can also assume a fixed read bandwidth for each medium.</p>\n", 'ViewCount': '78', 'Title': 'Monitoring files in preservation archives', 'LastEditorUserId': '1038', 'LastActivityDate': '2012-05-27T07:38:35.820', 'LastEditDate': '2012-04-25T20:23:31.977', 'AnswerCount': '1', 'CommentCount': '10', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1038', 'Tags': '<filesystems><integrity><digital-preservation>', 'CreationDate': '2012-04-24T22:43:36.573', 'Id': '1490'}}