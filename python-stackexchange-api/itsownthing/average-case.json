{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '4932', 'Title': 'Evaluating the average time complexity of a given bubblesort algorithm.', 'LastEditDate': '2012-03-07T18:26:03.053', 'AnswerCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '78', 'FavoriteCount': '1', 'Body': '<p>Considering this pseudo-code of a bubblesort:</p>\n\n<pre><code>FOR i := 0 TO arraylength(list) STEP 1  \n    switched := false\n    FOR j := 0 TO arraylength(list)-(i+1) STEP 1\n        IF list[j] &gt; list[j + 1] THEN\n            switch(list,j,j+1)\n            switched := true\n        ENDIF\n    NEXT\n    IF switched = false THEN\n        break\n    ENDIF\nNEXT\n</code></pre>\n\n<p>What would be the basic ideas I would have to keep in mind to evaluate the average time-complexity? I already accomplished calculating the worst and best cases, but I am stuck  deliberating how to evaluate the average complexity of the inner loop, to form the equation. </p>\n\n<p>The worst case equation is:</p>\n\n<p>$$\r\n\\sum_{i=0}^n \\left(\\sum_{j=0}^{n -(i+1)}O(1) + O(1)\\right) = O(\\frac{n^2}{2} + \\frac{n}{2}) = O(n^2)\r\n$$</p>\n\n<p>in which the inner sigma represents the inner loop, and the outer sigma represents the outer loop. I think that I need to change both sigmas due to the "if-then-break"-clause, which might affect the outer sigma but also due to the if-clause in the inner loop, which will affect the actions done during a loop (4 actions + 1 comparison if true, else just 1 comparison).</p>\n\n<p>For clarification on the term average-time: This sorting algorithm will need different time on different lists (of the same length), as the algorithm might need more or less steps through/within the loops until the list is completely in order. I try to find a mathematical (non statistical way) of evaluating the average of those rounds needed. </p>\n\n<p>For this I expect any order to be of the same possibility.</p>\n', 'Tags': '<algorithms><time-complexity><average-case>', 'LastEditorUserId': '12', 'LastActivityDate': '2013-01-30T17:00:13.023', 'CommentCount': '16', 'AcceptedAnswerId': '26', 'CreationDate': '2012-03-06T20:51:24.880', 'Id': '20'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given an array $A$ of $N$ integers, each element in the array can be increased by a fixed number $b$ with some probability $p[i]$, $0 \\leq i &lt; n$. I have to find the expected number of swaps that will take place to sort the array using <a href="http://en.wikipedia.org/wiki/Bubble_sort" rel="nofollow">bubble sort</a>.</p>\n\n<p>I\'ve tried the following:</p>\n\n<ol>\n<li><p>The probability for an element $A[i] &gt; A[j]$ for $i &lt; j$ can be calculated easily from the given probabilities.</p></li>\n<li><p>Using the above, I have calculated the expected number of swaps as:</p>\n\n<pre><code>double ans = 0.0;\nfor ( int i = 0; i &lt; N-1; i++ ){\n    for ( int j = i+1; j &lt; N; j++ ) {\n        ans += get_prob(A[i], A[j]); // Computes the probability of A[i]&gt;A[j] for i &lt; j.\n</code></pre></li>\n</ol>\n\n<p>Basically I came to this idea because the expected number of swaps can be calculated by the number of inversions of the array. So by making use of given probability I am calculating whether a number $A[i]$ will be swapped with a number $A[j]$.</p>\n\n<p>Note that the initial array elements can be in any order, sorted or unsorted. Then each number can change with some probability. After this I have to calculate the expected number of swaps.</p>\n\n<p>I have posted <a href="http://stackoverflow.com/questions/11331314/number-of-swaps-in-bubble-sort">a similar question</a> before but it did not had all the constraints.</p>\n\n<p>I did not get any good hints on whether I am even on the right track or not, so I listed all the constraints here. Please give me some hints if I am thinking of the problem in an incorrect way.</p>\n', 'ViewCount': '2139', 'Title': 'Expected number of swaps in bubble sort', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T01:48:38.960', 'LastEditDate': '2012-07-09T08:40:49.420', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '9', 'OwnerDisplayName': 'TheRock', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><sorting><average-case>', 'CreationDate': '2012-07-05T08:44:10.770', 'FavoriteCount': '3', 'Id': '2630'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to derive the <a href="http://alexandria.tue.nl/repository/freearticles/597601.pdf">classic paper</a> in the title only by elementary means (no generating functions, no complex analysis, no Fourier analysis) although with much less precision. In short, I "only" want to prove that the average height $h_n$ of a tree with $n$ nodes (that is, the maximum number of nodes from the root to a leaf) satisfies $h_n \\sim \\sqrt{\\pi n}$.</p>\n\n<p>The outline is as follows. Let $A_{nh}$ be the number of trees with height less than or equal to $h$ (with the convention $A_{nh} = A_{nn}$ for all $h \\geqslant n$) and $B_{nh}$ the number of trees of $n$ nodes with height greater than or equal to $h+1$ (that is, $B_{nh} = A_{nn} - A_{nh}$). Then $h_n = S_n/A_{nn}$, where $S_n$ is the finite sum\n$$\nS_n = \\sum_{h \\geqslant 1} h(A_{nh} - A_{n,h-1}) = \\sum_{h \\geqslant 1} h(B_{n,h-1} - B_{nh}) = \\sum_{h \\geqslant 0} B_{nh}.\n$$\nIt is well known that $A_{nn} = \\frac{1}{n}\\binom{2n-2}{n-1}$, for the set of general trees with $n$ nodes is in bijection with the set of binary trees with $n-1$ nodes, counted by the Catalan numbers.</p>\n\n<p>Therefore, the first step is to find $B_{nh}$ and then the main term in the asymptotic expansion of $S_n$.</p>\n\n<p>At this point the authors use analytical combinatorics (three pages) to derive\n$$\nB_{n+1,h-1} = \\sum_{k \\geqslant 1} \\left[\\binom{2n}{n+1-kh} - 2\\binom{2n}{n-kh} + \\binom{2n}{n-1-kh}\\right].\n$$</p>\n\n<blockquote>\n  <p>My own attempt is as follows. I consider the bijection between trees with $n$ nodes\n  and monotonic paths on a square grid $(n-1) \\times (n-1)$ from $(0,0)$ to $(n-1,n-1)$ which do not cross the diagonal (and are made of two kinds of steps: $\\uparrow$ and $\\rightarrow$). These paths are sometimes called <em>Dyck paths</em> or <em>excursions</em>. I can express now $B_{nh}$ in terms of lattice paths: it is the number of Dyck paths of length 2(n-1) and height greater than or equal to $h$. (Note: a tree of height $h$ is in bijection with a Dyck path of height $h-1$.)</p>\n  \n  <p>Without loss of generality, I assume that they start with $\\uparrow$ (hence stay above the diagonal). For each path, I consider the first step crossing the line $y = x + h - 1$, if any. From the point above, all the way back to the origin, I change $\\uparrow$ into $\\rightarrow$ and vice versa (this is a <em>reflection</em> wrt the line $y=x+h$). It becomes apparent that the paths I want to count ($B_{nh}$) are in bijection with the monotonic paths from $(-h,h)$ to $(n-1,n-1)$ which avoid the boundaries $y=x+2h+1$ and $y=x-1$. (See <a href="http://www.filedropper.com/lattice">figure</a>.)</p>\n</blockquote>\n\n<p>In the classic book <em>Lattice Path Counting and Applications</em> by Mohanty (1979, page 6) the formula\n$$\n\\sum_{k \\in \\mathbb{Z}} \\left[\\binom{m+n}{m-k(t+s)} - \\binom{m+n}{n+k(t+s)+t}\\right],\n$$\ncounts the number of monotonic paths in a lattice from $(0,0)$ to $(m,n)$, which avoid the boundaries $y = x - t$ and $y = x + s$, with $t &gt; 0$ and $s &gt; 0$. (This result was first established by Russian statisticians in the 50s.) Therefore, by considering a new origin at $(-h,h)$, we satisfy the conditions of the formula: $s=1$, $t=2h+1$ and the destination point (the upper right corner) is now $(n+h-1,n-h-1)$. Then\n$$\nB_{nh} = \\sum_{k \\in \\mathbb{Z}} \\left[\\binom{2n-2}{n+h-1-k(2h+2)} - \\binom{2n-2}{n-h-1+k(2h+2) + 2h+1}\\right].\n$$\nThis can be simplified in\n$$\nB_{n+1,h-1} = \\sum_{k \\in \\mathbb{Z}} \\left[\\binom{2n}{n+1-(2k+1)h} - \\binom{2n}{n-(2k+1)h}\\right],\n$$\nwhich, in turn, is equivalent to\n$$\nB_{n+1,h-1} = \\sum_{k \\geqslant 0} \\left[\\binom{2n}{n+1-(2k+1)h} - 2\\binom{2n}{n-(2k+1)h} + \\binom{2n}{n-1-(2k+1)h}\\right].\n$$\nThe difference with the expected formula is that I sum over the odd numbers ($2k+1$), instead of all positive integers ($k$).</p>\n\n<p>Any idea where the problem is?</p>\n', 'ViewCount': '202', 'Title': 'On "The Average Height of Planted Plane Trees" by Knuth, de Bruijn and Rice (1972)', 'LastEditorUserId': '2993', 'LastActivityDate': '2012-09-30T09:32:05.513', 'LastEditDate': '2012-09-30T09:32:05.513', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2993', 'Tags': '<combinatorics><discrete-mathematics><trees><average-case><random-walks>', 'CreationDate': '2012-09-28T12:20:51.920', 'Id': '4777'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>This is a similar algorithm to one I used in a previous question, but I\'m trying to illustrate a different problem here.</p>\n\n<pre><code>for (int i = 0; i &lt; numbers.length - 1; i++) {\n    for (int j = i + 1; j &lt; numbers.length; j++) {\n        if (numbers[i] + numbers[j] == 10) {\n            System.out.println(i+" and "+j+" add up to 10!");\n            return;\n        }\n    }\n}\nSystem.out.println("None of these numbers add up to 10!");\nreturn;\n</code></pre>\n\n<p>Basically, I realised, that I have a decent understanding of how to work out the best case run time here. I.e. the first two numbers which are fed in, add up to 10, and therefore our best-case runtime is constant time. Also, I understand that in the worst-case, none of the numbers we provide add up to 10, so we must then iterate through all loops, giving us a quadratic runtime. However, in the slides I was going through (for this particular course) I noticed that I had missed this:</p>\n\n<p>Average case</p>\n\n<p>\u2022 Presume that we\'ve just randomly given\nintegers between 1 and 9 as input in\nnumbers to our previous example</p>\n\n<p>\u2022 Exercise: Work out the average running time</p>\n\n<p>I realise I don\'t have the first clue as to how to go about calculating average case run-time. I\'m not asking for the answer to: "what is the average-case runtime for this algorithm?", what I need to know is, how do you work something like this out? </p>\n', 'ViewCount': '253', 'Title': 'How to go about working the average case run time of this trivial algorithm (and other algorithms)?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T16:02:27.433', 'LastEditDate': '2012-10-28T16:02:27.433', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2012-10-10T10:06:09.990', 'Id': '4993'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is a GRE practice question. </p>\n\n<p><img src="http://i.stack.imgur.com/dUavw.png" alt="BST n=8"></p>\n\n<p>If a node in the binary search tree above is to be located by binary tree search, what is the expected number of comparisons required to locate one of the items (nodes) in the tree chosen at random?</p>\n\n<p>(A) 1.75 </p>\n\n<p>(B) 2 </p>\n\n<p>(C) 2.75 </p>\n\n<p>(D) 3 </p>\n\n<p>(E) 3.25</p>\n\n<p>My answer was 3 because $n=8$ and $\\lg(n)$ comparisons should be made, and $\\lg(8) = 3$. But the correct answer is 2.75. Can someone explain the correct answer? Thanks!</p>\n', 'ViewCount': '1385', 'Title': 'Average number of comparisons to locate item in BST', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-10-16T15:02:35.267', 'LastEditDate': '2012-10-15T08:13:09.373', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6089', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4198', 'Tags': '<binary-trees><probability-theory><search-trees><average-case>', 'CreationDate': '2012-10-15T06:49:28.390', 'Id': '6085'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1759', 'Title': 'Proof that a randomly built binary search tree has logarithmic height', 'LastEditDate': '2012-10-28T11:16:30.370', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '1', 'Body': '<p>How do you prove that the expected height of a randomly built <a href="http://en.wikipedia.org/wiki/Binary_search_tree" rel="nofollow">binary search tree</a> with $n$ nodes is $O(\\log n)$? There is a proof in CLRS <em>Introduction to Algorithms</em> (chapter 12.4), but I don\'t understand it.</p>\n', 'Tags': '<data-structures><algorithm-analysis><binary-trees><search-trees><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T12:56:45.620', 'CommentCount': '4', 'AcceptedAnswerId': '6356', 'CreationDate': '2012-10-27T19:37:43.787', 'Id': '6342'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I read some research that analyzes the hardness of SAT solving in the average case.\nIn fact, for a 3CNF formula if you compute the ratio of clause to variables there is an interval (more or less between 4 and 5) in which solving the formula is hard. But it is easy (between 0 and 4) high probability of satisfiable assignment and high probability of unsatisfiable assignment after ratio 5.</p>\n\n<p>My question is, what about a generic formula that is not in normal form. We can say something about its hardness?</p>\n', 'ViewCount': '121', 'Title': 'Hardness of finding a true or a false assignment into a generic boolean formula?', 'LastEditorUserId': '41', 'LastActivityDate': '2013-02-06T00:32:56.353', 'LastEditDate': '2013-02-06T00:32:56.353', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4887', 'Tags': '<complexity-theory><satisfiability><average-case>', 'CreationDate': '2012-12-05T18:54:17.610', 'Id': '7193'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '157', 'Title': 'How to get expected running time of hash table?', 'LastEditDate': '2013-03-13T08:22:56.710', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'omega', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much. Am I making a mistake here?</p>\n", 'ClosedDate': '2013-03-13T08:23:35.603', 'Tags': '<data-structures><runtime-analysis><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-13T08:22:56.710', 'CommentCount': '1', 'CreationDate': '2013-03-12T23:21:20.177', 'Id': '10501'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When analysing treaps (or, equivalently, BSTs or Quicksort), it is not too hard to show that</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[d(k)] \\in O(\\log n)$</p>\n\n<p>where $d(k)$ is the depth of the element with rank $k$ in the set of $n$ keys.\nIntuitively, this seems to imply that also</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\in O(\\log n)$</p>\n\n<p>where $h(T)$ is the height of treap $T$, since</p>\n\n<p>$\\qquad\\displaystyle h(T) = \\max_{k \\in [1..n]} d(k)$.</p>\n\n<p>Formally, however, there does not seem to be an (immediate) relationship. We even have</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\geq \\max_{k \\in [1..n]} \\mathbb{E}[d(k)]$</p>\n\n<p>by Jensen\'s inequality. Now, one can show expected logarithmic height via tail bounds, using more insight into the distribution of $d(k)$.</p>\n\n<p>It is easy to construct examples of distributions that skrew with above intuition, namely extremely asymmetric, heavy-tailed distributions. The question is, can/do such occur in the analysis of algorithms and data structures?</p>\n\n<p>Are there example for data structures $D$ (or algorithms) for which</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(D)] \\in \\omega(\\max_{e \\in D} \\mathbb{E}[d(e)])$?</p>\n\n<p>Nota bene:</p>\n\n<ul>\n<li><p>Of course, we have to interpret "depth" and "height" liberally if we consider structures that are not trees. Based on the posts Wandering Logic links to, "Expected average search time" (for $1/n \\cdot \\sum_{e \\in D} \\mathbb{E}[d(e)]$) and "expected maximum search time" (for $\\mathbb{E}[h(D)]$) seem to be used.</p></li>\n<li><p>A <a href="http://math.stackexchange.com/q/426998/3330">related question</a> on math.SE has yielded an interesting answer that may allow deriving useful bounds on $\\mathbb{E}[h(D)]$ given suitable bounds on $\\mathbb{E}[d(e)]$ and $\\mathbb{V}[d(e)]$.</p></li>\n</ul>\n', 'ViewCount': '93', 'Title': 'Can expected "depth" of an element and expected "height" differ significantly?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-23T20:06:53.820', 'LastEditDate': '2013-06-23T16:04:31.497', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12833', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><data-structures><algorithm-analysis><probability-theory><average-case>', 'CreationDate': '2013-06-22T16:23:33.990', 'Id': '12830'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Let there is a binary-string, $B$, of length $N$. The probability of occurrence of 0 and 1 in this binary-word is $p$  and $q$ , respectively. Each bit in the string is independent of any other bit.</p>\n\n<p>There is an algorithm (divide and conquer) which finds the location of 1\u2019s in a given binary-string, in Q # of steps (cost).</p>\n\n<p>I am looking for some close-form solution of the expected # of steps,$E[Q]$, with given probabilities $p$ and $q$ for a string of length $N$.</p>\n\n<p>For instance, for $N=4$ the cost ,${{Q}_{i}}$,  for each possible word is:<br>\n[\\begin{matrix}\n   {{B}_{i}} &amp; {{Q}_{i}} &amp; {{P}_{i}}  \\\\\n   0000 &amp; 1 &amp; {{p}^{4}}  \\\\\n   0001 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0010 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0011 &amp; 5 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0100 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0101 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0110 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0111 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1000 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   1001 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1010 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1011 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1100 &amp; 5 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1101 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1110 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1111 &amp; 7 &amp; {{q}^{4}}  \\\\\n\\end{matrix}]\nFrom the discrete probability theory, we can evaluate the expected cost of the above case ,$N=4$,as follow $\\begin{align}\n  &amp; \\therefore E[Q]=\\sum\\limits_{i=0}^{{{2}^{N}}-1}{{{Q}_{i}}({{p}^{N-i}}}{{q}^{i}}) \\\\ \n &amp; \\Rightarrow E[Q]=\\sum\\limits_{i=0}^{15}{{{Q}_{i}}({{p}^{N-i}}}{{q}^{i}}) \\\\ \n &amp; \\Rightarrow E[Q]={{p}^{4}}+4\\times 5\\times {{p}^{3}}q+2\\times 5\\times {{p}^{2}}{{q}^{2}}+4\\times 7\\times {{p}^{2}}{{q}^{2}}+4\\times 7\\times p{{q}^{3}}+1\\times 7\\times {{q}^{4}} \\\\ \n &amp; \\Rightarrow E[Q]={{p}^{4}}+20{{p}^{3}}q+10{{p}^{2}}{{q}^{2}}+28{{p}^{2}}{{q}^{2}}+28p{{q}^{3}}+7{{q}^{4}} \\\\ \n\\end{align}$ </p>\n\n<p>However, for very large values of N, say 1024, it is computationally not possible to evaluate the cost of each possible binary string (i-e ${{2}^{1024}}=\\text{1}\\text{.79}\\times \\text{1}{{\\text{0}}^{308}}$ binary words). So, this is the problem I am stuck in. </p>\n\n<p>Is it possible to deduce some analytical/ close-form expression for evaluating the expected value of the cost of this algorithm for a given length N and probabilities p and q (instead of brute-force method defined above)?</p>\n\n<p>I will be very thankful, if someone could help in this regard.</p>\n\n<p>ADDITIONAL INFORMATION:</p>\n\n<p>Divide &amp; Conquer Algorithm:\n    Lets take 0000 0001, for instance:</p>\n\n<ol>\n<li><p>First question: Is it (i-e 0000 0001) equal to ZERO (i-e 0000 0000) ? The answer is No, for our case. </p></li>\n<li><p>Then divide the original 8 bit word into two 4 bit segments, and again ask the same question for each of the two 4 bit words. So for our case it would be YES for the first segment (0000) and NO for the other segment (0001)?</p></li>\n<li><p>Now, this time I will be questioning only the segment where I got NO. In our case it was 0001. Then, I will now again divide this 4-bit segment into two segments and pose the same question. Hence, is 00 equal to ZERO? answer : YES. For the other segment , 01, the answer is NO.</p></li>\n<li><p>This is the final step. I will again divide the 2-bit word into two 1-bits, i-e 0 and 1. So, my first question: is 0 equal to 0? answer is YES. And for the other remaining bit, is 1 equal to 0? Answer is NO.</p></li>\n</ol>\n\n<p>So, I asked a total of 7 questions to find the location of 1 in a binary word of 0000 0001. Similarly, we will go through other binary words.</p>\n\n<p>Efficient Way for evaluating the cost of Above algorithm:\n(courtesy to Yuval Filmus)</p>\n\n<p>Here is how to calculate the cost of the algorithm. Start with the bit vector $x$, and consider the following operation $O$, which divides $x$ into pairs of bits and computes their ORs. Thus $|O(x)| = |x|/2$. Compute a sequence $O(x),O(O(x)),O(O(O(x))),\\ldots$ until you get a vector of width $1$. Count the total number of 1s in the sequence. If you got $N$, then the cost is $2N+1$.</p>\n\n<p>For example, suppose $x=0101$. Then the sequence is\n$$ 11,1, $$\nand so $N = 3$ and the cost is $7$.</p>\n', 'ViewCount': '155', 'Title': 'Closed-form Expression of the Expected value of the Cost of D&C Algorithm?', 'LastEditorUserId': '9000', 'LastActivityDate': '2013-08-02T16:50:37.103', 'LastEditDate': '2013-08-02T16:50:37.103', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '13377', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9000', 'Tags': '<algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2013-07-21T19:03:42.417', 'Id': '13376'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>While reading a cryptography textbook, i find the definition of a function that is hard on the average.(More precisely, it is 'hard on the average but easy with auxiliary input', but i omit latter for simplicity.)</p>\n\n<blockquote>\n  <p><strong>Definition : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>there exists</strong> a probabilistic polynomial-time algorithm $G$ such that<br>\n  for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>My question is why the statement of the existence of qualified algorithm G is sufficient? </p>\n\n<p>In other words, why the above definition gives a formal definition of 'hardness on the average' instead of following definition, which is more intuitive(?) to understand and more strict.\nWhy is the above definition sufficient? </p>\n\n<p>( Now I'm thinking that problem might occur when $G$ has only polynomial number of possible outputs, but if so, let's replace 'for any $G$' with 'for any $G$ which have exponentially many possible outputs' in following definition.)</p>\n\n<blockquote>\n  <p><strong>(strong?) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if <strong>for any</strong> probabilistic polynomial-time algorithm $G$ and for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(X_n)=h(X_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $X_n := G(1^n)$ is a random variable assigned the output of $G$.</p>\n</blockquote>\n\n<p>Another question is that whether a following simpler definition is equivalent to original definition or not?</p>\n\n<blockquote>\n  <p><strong>(simple) Def : Hard on the average</strong> : </p>\n  \n  <p>$h:\\{0,1\\}^*\\to \\{0,1\\}^* $ is hard on the average if for every probabilistic polynomial-time algorithm $A'$ every positive polynomial $p(\\cdot)$, and all sufficiently large $n$'s,  Pr$[A'(U_n)=h(U_n)]&lt;\\frac{1}{p(n)}$  </p>\n  \n  <p>where $U_n$ is a random variable uniformly distributed over $\\{0,1\\}^n$.</p>\n</blockquote>\n", 'ViewCount': '64', 'Title': "Completeness of formal definition of 'hardness on the average'", 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-09T07:02:13.240', 'LastEditDate': '2013-08-09T07:02:13.240', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13678', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9565', 'Tags': '<complexity-theory><terminology><cryptography><randomized-algorithms><average-case>', 'CreationDate': '2013-08-08T12:43:21.820', 'Id': '13674'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '234', 'Title': 'Complexity of keeping track of $K$ smallest integers in a stream', 'LastEditDate': '2014-01-30T21:46:49.597', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10384', 'FavoriteCount': '2', 'Body': '<p>I need to analyze the time complexity of an online algorithm to keep track of minimum $K$ numbers from a stream of $R$ numbers. The algorithm is </p>\n\n<ul>\n<li>Suppose the $i$th number in the stream is $S_i$.</li>\n<li>Keep a max heap of size $K$.</li>\n<li>If the heap contains fewer than $K$ elements, add $S_i$ to the heap.</li>\n<li>Otherwise: if $S_i$ is smaller than the maximum element in the heap (i.e., the root of the heap), replace the root of the heap with $S_i$ and apply Max-Heapify to the heap; if $S_i$ is greater than or equal to the max element, do nothing.</li>\n</ul>\n\n<p>The problem now is to find the expected number of times the Max Heapify operation will be called, when the stream of integers is of length $R$ and each element of the stream is (iid) uniformly distributed on $[1,N]$.</p>\n\n<p>If the stream were guaranteed to contain only distinct elements, then the answer is easy: </p>\n\n<p>$$E[X] = E[X_1] + E[X_2] + \\dots + E[X_R],$$</p>\n\n<p>where $X_i$ is the random indicator variable for occurrence of the Max Heapify operation at the $i$th number in the stream.  Now  </p>\n\n<p>$$E[X_i] = \\Pr[\\text{$S_i$ is ranked $\\le K$ among first $i$ elements}] = \\min(K/i, 1).$$</p>\n\n<p>Hence,</p>\n\n<p>$$E[X] = K + K/(K+1) + \\cdots + K/R.$$</p>\n\n<p>That case is relatively easy.  But how do we handle the case where the elements are iid uniformly distributed?</p>\n\n<p>[This was actually a Google interview question.] </p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-07T12:35:40.240', 'CommentCount': '0', 'AcceptedAnswerId': '20261', 'CreationDate': '2013-09-28T19:59:47.083', 'Id': '14661'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have done an assignment question which asks me to find the average case of pentary search. The one I came up with is:</p>\n\n<pre><code>C(n) = C(n/5) + 14/5\n</code></pre>\n\n<p>However, I got it wrong and the professor didn't really explain why.</p>\n\n<p>Can you help me?</p>\n\n<p>Thanks!</p>\n\n<p>Edit:</p>\n\n<p>Pentary search is dividing the array into 5 parts and look for a specific key.</p>\n\n<p>In perspective:</p>\n\n<p>1|2|3|4|4</p>\n\n<p>These are the number of comparisons made by the computer from pentary search.</p>\n\n<p>Adding all possible comparisons I get 14 over 5 slots. This is why I have 14/5</p>\n", 'ViewCount': '44', 'Title': 'Pentary Search Recurrence Relation', 'LastEditorUserId': '10398', 'LastActivityDate': '2013-10-14T02:40:21.000', 'LastEditDate': '2013-10-14T02:37:23.090', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10398', 'Tags': '<recurrence-relation><average-case>', 'CreationDate': '2013-10-14T00:06:20.280', 'Id': '16059'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I cannot really find a source that does not use the same examples provided by CLRS. I need a simpler example than <code>MULTI-POP</code> example. Could someone provide an example and explain me:</p>\n\n<p>a) What is the difference between worst-case analysis and amortised analysis?</p>\n\n<p>b) What is the difference between average-case analysis and amortised analysis?</p>\n\n<p>c) In plain English(with using minimal notations), how can we provide a complexity(especially I am interested in potential method)</p>\n', 'ViewCount': '89', 'Title': 'Basics of Amortised Analysis', 'LastActivityDate': '2013-10-28T16:55:38.243', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithm-analysis><asymptotics><amortized-analysis><average-case>', 'CreationDate': '2013-10-28T15:14:12.807', 'FavoriteCount': '1', 'Id': '16502'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '80', 'Title': 'Genetic algorithm: What is the expected number of strings that are explored?', 'LastEditDate': '2014-01-31T12:10:35.870', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'Remi.b', 'PostTypeId': '1', 'OwnerUserId': '13275', 'Body': "<p>My question concerns genetic algorithm searching along bit strings.</p>\n\n<p>Given:</p>\n\n<ul>\n<li>$N$ = population size</li>\n<li>$l$ = length of bit strings</li>\n<li>$p_c$ = probability that a single crossover occur (double crossover never occur)</li>\n<li>$p_m$ = probability for a given bit that a mutation occur</li>\n</ul>\n\n<p>$w(x)$, the fitness function is equal to the number of 1 in the strings. Therefore, the fitness can take any integer value between 0 and $l$ (the length of the strings).</p>\n\n<p>My question is (three ways of formulating the same question):</p>\n\n<ul>\n<li>What is the expected total number of possibilities explored in $G$ generations?</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>What is the expected proportion of the total possibility space (which equals $2^l$) that is explored in $G$ generations?</li>\n</ul>\n\n<p>or</p>\n\n<ul>\n<li>What is the expected size of the subset of strings that have ever existed in the population during a simulation that last $G$ generations?</li>\n</ul>\n\n<p>Secondary questions:</p>\n\n<ul>\n<li>How does the frequency distribution - of the total number of possibilities explored in $G$ generation - looks like?\n<ul>\n<li>is it a normal (Gauss) distribution?</li>\n<li>Is it skewed?</li>\n<li>...</li>\n</ul></li>\n</ul>\n\n<hr>\n\n<p>I don't quite know how complex is my question. Here are two assumptions that one would like to consider in order to ease the problem.</p>\n\n<ul>\n<li><p>one might want to assume that the population at start is not randomly drawn from the possibility space. He could assume that the whole population is made of identical strings (only one instance). For example, the string <code>000000000</code> (which length equals $l$).</p></li>\n<li><p>one might want to assume that $p_c = 0$</p></li>\n</ul>\n", 'Tags': '<algorithms><optimization><average-case><genetic-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T17:51:05.043', 'CommentCount': '3', 'AcceptedAnswerId': '20153', 'CreationDate': '2014-01-31T00:26:34.640', 'Id': '20152'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have an algorithm and I determined the asymptotic worst-case runtime, represented by Landau notation. Let's say $T(n) = O(n^2)$; this is measured in number of operations.</p>\n\n<p>But this is the worst case, how about in average? I tried to run my algorithm 1000 times for each $n$ from $1$ to $1000$.I get another graph which is the average running time against $n$ but measured in real seconds.</p>\n\n<p>Is there any possible way to compare these figures?</p>\n", 'ViewCount': '56', 'Title': 'Compare asymptotic WC runtime with measured AC runtime', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-02T14:37:42.050', 'LastEditDate': '2014-02-02T14:22:32.553', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11506', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><average-case>', 'CreationDate': '2014-02-01T13:58:46.173', 'FavoriteCount': '1', 'Id': '20186'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I am somewhat confused with the running time analysis of a program here which has recursive calls which depend on a RNG.  (Randomly Generated Number)</p>\n\n<p>Let\'s begin with the pseudo-code, and then I will go into what I have thought about so far related to this one.</p>\n\n<pre><code>    Func1(A, i, j)\n    /* A is an array of at least j integers */\n\n 1  if (i \u2265 j) then return (0);\n 2  n \u2190 j \u2212 i + 1 ; /* n = number of elements from i to j */\n 3  k \u2190 Random(n);\n 4  s \u2190 0; //Takes time of Arbitrary C\n\n 5  for r \u2190 i to j do\n 6      A[r] \u2190 A[r] \u2212 A[i] \u2212 A[j]; //Arbitrary C\n 7      s \u2190 s + A[r]; //Arbitrary C\n 8  end\n\n 9  s \u2190 s + Func1(A, i, i+k-1); //Recursive Call 1\n10  s \u2190 s + Func1(A, i+k, j); //Recursive Call 2\n11  return (s);\n</code></pre>\n\n<p>Okay, now let\'s get into the math I have tried so far.  I\'ll try not to be too pedantic here as it is just a rough, estimated analysis of expected run time.  </p>\n\n<p>First, let\'s consider the worst case.  Note that the K = Random(n) must be at least 1, and at most n.  Therefore, the worst case is the K = 1 is picked.  This causes the total running time to be equal to T(n) = cn + T(1) + T(n-1).  Which means that overall it takes somewhere around cn^2 time total (you can use Wolfram to solve recurrence relations if you are stuck or rusty on recurrence relations, although this one is a fairly simple one).  </p>\n\n<p>Now, here is where I get somewhat confused.  For the expected running time, we have to base our assumption off of the probability of the random number K.  Therefore, we have to sum all the possible running times for different values of k, plus their individual probability.  By lemma/hopefully intuitive logic: the probability of any one Randomly Generated k, with k between 1 to n, is equal 1/n.  </p>\n\n<p><strong>Therefore, (in my opinion/analysis) the expected run time is:</strong></p>\n\n<p><strong>ET(n) = cn + (1/n)*Summation(from k=1 to n-1) of (ET(k-1) + ET(n-k))</strong></p>\n\n<p>Let me explain a bit.  The cn is simply for the loop which runs i to j.  This is estimated by cn.  The summation represents all of the possible values for k.  The (1/n) multiplied by this summation is there because the probability of any one k is (1/n).  <strong>The terms inside the summation represent the running times of the recursive calls of Func1.</strong>  The first term on the left takes ET(k-1) because this recursive call is going to do a loop from i to k-1 (which is roughly ck), and then possibly call Func1 again.  The second is a representation of the second recursive call, which would loop from i+k to j, which is also represented by n-k.</p>\n\n<p><strong>Upon expansion of the summation, we see that the overall function ET(n)  is of the order n^2.</strong>  <em>However</em>, as a test case, plugging in k=(n/2) gives a total running time for Func 1 of roughly nlog(n).  <em>This</em> is why I am confused.  How can this be, if the estimated running time is of the order n^2?  Am I considering a "good" case by plugging in n/2 for k?  Or am I thinking about k in the wrong sense in some way?  </p>\n', 'ViewCount': '66', 'Title': 'Algorithm Analysis: Expected Running Time of Recursive Function Based on a RNG', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-17T19:50:53.527', 'LastEditDate': '2014-02-12T09:12:25.723', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14596', 'Tags': '<algorithms><algorithm-analysis><randomized-algorithms><average-case>', 'CreationDate': '2014-02-12T05:31:29.490', 'Id': '21558'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '161', 'Title': 'What is the expected number of nodes at depth d of a tree after i random insertions', 'LastEditDate': '2014-03-14T09:33:19.310', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2825', 'FavoriteCount': '0', 'Body': "<p>Suppose one wanted to build a tree at random.  Let the first insertion at step $i = 1$ be the root node.  From here, nodes are inserted into the tree at random one at a time.  How would one go about calculating the expected number of nodes $E(d)$ at depth $d$ after $i$ insertions?</p>\n\n<p>For example, At $i = 1$, it's just the root node, so $E(0) = 1$ and all other depths will have zero nodes.  At $i = 2$, $E(0) = 1$ and $E(1) = 1$ as the inserted node has to be at depth 1 from the root.  At $i = 3$, the next node may either be attached to the root node or the existing node at depth one, so the tree may either look like:</p>\n\n<p><code>\n    *\n   / \\\n  *   *\n</code></p>\n\n<p>Or:</p>\n\n<p><code>\n     *\n    /\n   *\n  /\n *\n</code></p>\n\n<p>Depending on what happend at $i = 3$, at $i = 4$ there's either a $2/3$ chance of the new node attaching at depth 2 and $1/3$ attaching at depth 1, or an even $1/3$ probability of the new node attaching at the root, the node at depth 1 or the node at depth 2.  Keeping random insertion in mind (not a binary or balanced tree in any way), how would one go about calculating the expected number of nodes at depth $d$ after $i$ insertions? </p>\n", 'Tags': '<data-structures><trees><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T09:33:19.310', 'CommentCount': '0', 'AcceptedAnswerId': '22583', 'CreationDate': '2014-03-13T11:50:12.127', 'Id': '22580'}