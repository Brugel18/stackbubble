64_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've not gone much deep into CS. So, please forgive me if the question is not good or out of scope for this site.</p>\n\n<p>I've seen in many sites and books, the big-O notations like $O(n)$ which tell the time taken by an algorithm. I've read a few articles about it, but I'm still not able to understand how do you calculate it for a given algorithm.</p>\n", 'ViewCount': '7841', 'Title': 'How to come up with the runtime of algorithms?', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-06-06T16:07:12.323', 'LastEditDate': '2013-06-06T16:07:12.323', 'AnswerCount': '5', 'CommentCount': '5', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '132', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><reference-question>', 'CreationDate': '2012-03-10T12:03:19.397', 'FavoriteCount': '11', 'Id': '192'},64_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>What are some good online resources that will help me better understand BigO notation, running time &amp; invariants?</p>\n\n<p>I'm looking for lectures, interactive examples if possible. </p>\n", 'ViewCount': '464', 'Title': 'BigO, Running Time, Invariants - Learning Resources', 'LastEditorUserId': '41', 'LastActivityDate': '2012-03-29T08:45:22.170', 'LastEditDate': '2012-03-26T05:30:58.720', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '326', 'Tags': '<algorithms><landau-notation><education><runtime-analysis>', 'CreationDate': '2012-03-21T03:05:09.847', 'FavoriteCount': '2', 'Id': '569'},64_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '702', 'Title': 'Complexity of finding the largest $m$ numbers in an array of size $n$', 'LastEditDate': '2012-04-24T20:40:13.420', 'AnswerCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '1', 'Body': "<p>What follows is my algorithm for doing this in what I believe to be $O(n)$ time, and my proof for that. My professor disagrees that it runs in $O(n)$ and instead thinks that it runs in $\\Omega(n^2)$ time. Any comments regarding the proof itself, or the style (i.e. my ideas may be clear but the presentation not).</p>\n\n<p>The original question:</p>\n\n<blockquote>\n  <p>Given $n$ numbers, find the largest $m \\leq n$ among them in time $o(n \\log n)$. You may not assume anything else about $m$.</p>\n</blockquote>\n\n<p>My answer:</p>\n\n<ol>\n<li>Sort the first $m$ elements of the array. This takes $O(1)$ time, as this is totally dependent on $m$, not $n$.</li>\n<li>Store them in a linked list (maintaining the sorted order). This also takes $O(1)$ time, for the same reason as above.</li>\n<li>For every other element in the array, test if it is greater than the least element of the linked list. This takes $O(n)$ time as $n$ comparisons must be done.</li>\n<li>If the number is in fact greater, then delete the first element of the linked list (the lowest one) and insert the new number in the location that would keep the list in sorted order. This takes $O(1)$ time because it is bounded by a constant ($m$) above as the list does not grow.</li>\n<li>Therefore, the total complexity for the algorithm is $O(n)$.</li>\n</ol>\n\n<p>I am aware that using a red-black tree as opposed to linked list is more efficient in constant terms (as the constant upper bound is $O(m\\cdot \\log_2(m))$ as opposed to $m$ and the problem of keeping a pointer to the lowest element of the tree (to facilitate the comparisons) is eminently doable, it just didn't occur to me at the time.</p>\n\n<p>What is my proof missing? Is there a more standard way of presenting it (even if it is incorrect)?</p>\n", 'Tags': '<algorithms><time-complexity><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-24T03:35:40.420', 'CommentCount': '9', 'AcceptedAnswerId': '1489', 'CreationDate': '2012-04-24T18:03:35.983', 'Id': '1485'},64_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume I want to insert elements $1$ to $n$ into a data structure exactly once, and perform predecessor queries while inserting these elements (so <code>insert(x)</code> and <code>pred(x)</code> always come in pairs). The predecessor of $x$ is the largest number in the data structure that is smaller than $x$.</p>\n\n<p>The data structure is created by preprocessing the list of insertions. </p>\n\n<p>When I start to insert elements, an adversary decides to delete some of the elements I have inserted, by adding any number of deletion operations between my insertions. </p>\n\n<p>A query input to the data structure is a sequence of insertions and deletions, which is the insertion sequence with deletions inserted. \nThe output of the query is the result of the $n$ predecessor queries executed when the elements are inserted. </p>\n\n<p>Can one design a data structure so the query takes $O(n)$?</p>\n\n<p>Here is an example.</p>\n\n<pre><code>Insertions = [1,3,5,4,2]\nDS = makeDataStructure(Insertions)//Runs in polynomial time\n//add some deletions into insertions\nOperations = [1,3,-3,5,-1,4,-5,-4,2,-2]\nDS.query(Operations)//this runs in O(n) time\n</code></pre>\n\n<p>Assume -i = delete i. And pred(x) = 0 if there is nothing before it.\nresult would be:</p>\n\n<pre><code>[pred(1)=0, pred(3)=1, pred(5)=1, pred(4)=0, pred(2)=0]\n</code></pre>\n\n<p>for example, the 3rd in the result is <code>pred(5)=1</code> instead of 3 because 3 is deleted when 5 is inserted.</p>\n', 'ViewCount': '162', 'Title': 'Predecessor query where the insertion order is known', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-05T15:55:44.163', 'LastEditDate': '2013-05-05T15:55:44.163', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<data-structures><runtime-analysis>', 'CreationDate': '2012-04-25T13:44:46.220', 'FavoriteCount': '1', 'Id': '1502'},64_4:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I have developed two algorithms and now they are asking me to find their running time.\nThe problem is to develop a singly linked list version for manipulating polynomials. The two main operations are <em>addition</em> and <em>multiplication</em>.</p>\n\n<p>In general for lists the running for these two operations are ($x,y$ are the lists lengths):</p>\n\n<ul>\n<li>Addition: Time $O(x+y)$, space $O(x+y)$</li>\n<li>Multiplication: Time $O(xy \\log(xy))$, space $O(xy)$</li>\n</ul>\n\n<p>Can someone help me to find the running times of my algorithms?\nI think for the first algorithm it is like stated above $O(x+y)$, for the second one I have two nested loops and two lists so it should be $O(xy)$, but why the $O(xy \\log(xy))$ above?</p>\n\n<p>These are the algorithms I developed (in Pseudocode):</p>\n\n<pre><code> PolynomialAdd(Poly1, Poly2):\n Degree := MaxDegree(Poly1.head, Poly2.head);\n while (Degree &gt;=0) do:\n      Node1 := Poly1.head;\n      while (Node1 IS NOT NIL) do:\n         if(Node1.Deg = Degree) then break;\n         else Node1 = Node1.next;\n      Node2 := Poly2.head;\n      while (Node2 IS NOT NIL) do:\n         if(Node2.Deg = Degree) then break;\n         else Node2 = Node2.next;\n      if (Node1 IS NOT NIL AND Node2 IS NOT NIL) then\n         PolyResult.insertTerm( Node1.Coeff + Node2.Coeff, Node1.Deg);\n      else if (Node1 IS NOT NIL) then\n         PolyResult.insertTerm(Node1.Coeff, Node1.Deg);\n      else if (Node2 IS NOT NIL) then\n         PolyResult.insertTerm(Node2.Coeff, Node2.Deg);\n      Degree := Degree \u2013 1;\n return PolyResult; \n\n PolynomialMul(Poly1, Poly2): \n Node1 := Poly1.head;\n while (Node1 IS NOT NIL) do:\n      Node2 = Poly2.head;\n      while (Node2 IS NOT NIL) do:\n           PolyResult.insertTerm(Node1.Coeff * Node2.Coeff, \n                              Node1.Deg + Node1.Deg);\n           Node2 = Node2.next;                 \n      Node1 = Node1.next;\n return PolyResult;\n</code></pre>\n\n<p><code>InsertTerm</code> inserts the term in the correct place depending on the degree of the term. </p>\n\n<pre><code> InsertTerm(Coeff, Deg):\n NewNode.Coeff := Coeff;\n NewNode.Deg := Deg;\n if List.head = NIL then\n    List.head := NewNode;\n else if NewNode.Deg &gt; List.head.Deg then\n    NewNode.next := List.head;\n    List.head := NewNode;\n else if NewNode.Deg = List.head.Deg then \n    AddCoeff(NewNode, List.head);\n else\n    Go through the List till find the same Degree and summing up the coefficient OR\n    adding a new Term in the right position if Degree not present;\n</code></pre>\n', 'ViewCount': '1224', 'Title': 'Running time - Linked Lists Polynomial', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-09T09:08:38.773', 'LastEditDate': '2012-05-09T09:04:04.817', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '1584', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-04-29T10:01:54.643', 'Id': '1567'},64_5:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am struggling with hashing and binary search tree material.\nAnd I read that instead of using lists for storing entries with the same hash values, it is also possible to use binary search trees. And I try to understand what the worst-case and average-case running time for the operations</p>\n\n<ol>\n<li><code>insert</code>, </li>\n<li><code>find</code> and</li>\n<li><code>delete</code></li>\n</ol>\n\n<p>is in worth- resp. average case. Do they improve with respect to lists?</p>\n', 'ViewCount': '1265', 'Title': 'Hashing using search trees instead of lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T15:37:46.957', 'LastEditDate': '2012-05-10T15:37:46.957', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<data-structures><time-complexity><runtime-analysis><search-trees><hash-tables>', 'CreationDate': '2012-05-08T21:46:33.920', 'Id': '1739'},64_6:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '234', 'Title': 'Recursion for runtime of divide and conquer algorithms', 'LastEditDate': '2012-05-10T16:31:21.383', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'user1422', 'PostTypeId': '1', 'FavoriteCount': '1', 'Body': "<p>A divide and conquer algorithm's work at a specific level can be simplified into the equation:</p>\n\n<p>$\\qquad \\displaystyle O\\left(n^d\\right) \\cdot \\left(\\frac{a}{b^d}\\right)^k$</p>\n\n<p>where $n$ is the size of the problem, $a$ is the number of sub problems, $b$ is the factor the size of the problem is broken down by at each recursion, $k$ is the level, and $d$ is the exponent for Big O notation (linear, exponential etc.).</p>\n\n<p>The book claims  if the ratio is greater than one the sum of work is given by the last term on the last level, but if it is less than one the sum of work is given by the first term of the first level. Could someone explain why this is true?</p>\n", 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><recursion><mathematical-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T16:31:21.383', 'CommentCount': '0', 'AcceptedAnswerId': '1746', 'CreationDate': '2012-05-09T03:13:56.910', 'Id': '1745'},64_7:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Brzozowski's DFA minimization algorithm builds a minimal DFA for DFA $G$ by:</p>\n\n<ol>\n<li>reversing all the edges in $G$, making the initial state an accept state, and the accept states initial, to get an NFA $N&#39;$ for the reverse language, </li>\n<li>using powerset construction to get $G&#39;$ for the reverse language, </li>\n<li>reversing the edges (and initial-accept swap) in $G&#39;$ to get an NFA $N$ for the original language, and</li>\n<li>doing powerset construction to get $G_{\\min}$.</li>\n</ol>\n\n<p>Of course, since some DFA's have an exponential large reverse DFA, this algorithm runs in exponential time in worst case in terms of the size of the input, so lets keep track of the size of the reverse DFA. </p>\n\n<p>If $N$ is the size of the input DFA, $n$ is the size of the minimal DFA, and $m$ the size of the minimal reverse DFA, then <strong>what is the run time of Brzozowski's algorithm in terms of $N$,$n$, and $m$?</strong></p>\n\n<p>In particular, <strong>under what relationship between $n$ and $m$ does Brzozowski's algorithm outperform Hopcroft's or Moore's algorithms?</strong></p>\n\n<p>I have heard that on typical examples in <em>practice/application</em>, Brzozowski's algorithm outperforms the others. <strong>Informally, what are these typical examples like?</strong></p>\n", 'ViewCount': '1327', 'Title': "Brzozowski's algorithm for DFA minimization", 'LastEditorUserId': '55', 'LastActivityDate': '2012-11-09T13:05:46.000', 'LastEditDate': '2012-05-16T18:52:28.467', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '55', 'Tags': '<algorithms><finite-automata><runtime-analysis>', 'CreationDate': '2012-05-16T16:43:10.513', 'FavoriteCount': '2', 'Id': '1872'},64_8:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '370', 'Title': 'How to go from a recurrence relation to a final complexity', 'LastEditDate': '2012-05-22T17:52:22.850', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1562', 'FavoriteCount': '1', 'Body': u'<p>I have an algorithm, shown below, that I need to analyze. Because it\'s recursive in nature I set up a recurrence relation.  </p>\n\n<pre><code>//Input: Adjacency matrix A[1..n, 1..n]) of an undirected graph G  \n//Output: 1 (true) if G is complete and 0 (false) otherwise  \nGraphComplete(A[1..n, 1..n]) {\n  if ( n = 1 )\n    return 1 //one-vertex graph is complete by definition  \n  else  \n    if not GraphComplete(A[0..n \u2212 1, 0..n \u2212 1]) \n      return 0  \n    else \n      for ( j \u2190 1 to n \u2212 1 ) do  \n        if ( A[n, j] = 0 ) \n          return 0  \n      end\n      return 1\n}\n</code></pre>\n\n<p>Here is what I believe is a valid and correct recurrence relation:  </p>\n\n<p>$\\qquad \\begin{align}\r\n  T(1) &amp;= 0 \\\\\r\n  T(n) &amp;= T(n-1) + n - 1 \\quad \\text{for } n \\geq 2\r\n\\end{align}$</p>\n\n<p>The "$n - 1$" is how many times the body of the for loop, specifically the "if A[n,j]=0" check, is executed.</p>\n\n<p>The problem is, where do I go from here? How do I convert the above into something that actually shows what the resulting complexity is?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-22T17:52:22.850', 'CommentCount': '1', 'AcceptedAnswerId': '1960', 'CreationDate': '2012-05-20T21:24:29.637', 'Id': '1959'},64_9:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I have a "smarter" version of Bellman-Ford here; this version is more clever about choosing the edges to relax.</p>\n\n<pre><code>//Queue Q; source s; vertices u, v; distance to v d(v)\nQ \u2190 s // Q holds vertices whose d(v) values have been updated recently.\nWhile (Q !empty) {\n  u \u2190 Dequeue(Q)\n  for each neighbor v of u {\n    Relax(u, v)\n    if d(v) was updated by Relax and v not in Q\n      Enqueue(v)\n  }\n}\n</code></pre>\n\n<p>But, can anyone explain why this improved version correctly finds the shortest path from $s$ to every other vertex in a directed graph with no negative cycles?</p>\n\n<p>Also, what is the <em>worst-case</em> runtime if every shortest path uses at most $v$ edges?</p>\n', 'ViewCount': '458', 'Title': 'Bellman-Ford variation', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-25T05:17:22.893', 'LastEditDate': '2012-05-24T07:55:34.530', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1624', 'Tags': '<algorithms><graphs><graph-theory><runtime-analysis><shortest-path>', 'CreationDate': '2012-05-24T01:44:55.237', 'FavoriteCount': '2', 'Id': '2039'},64_10:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p><a href="http://cs.stackexchange.com/questions/1984/proving-the-intractability-of-this-nth-prime-recurrence">As discussed in this question,</a> I drafted a spec algorithm that hinges on finding a specific root of a system of trigonometric equations satisfying the following recurrence:</p>\n\n<p>$\\qquad f_{p_0} = 0\\\\\n\\qquad p_0 = 2\\\\\n\\qquad \\displaystyle\n  f_{p_n}(x) = f_{p_{n-1}}(x) + \\prod_{k=2}^{p_{n-1}} (-\\cos(2\\pi(x+k-1)/p_{n-1}) + 1)\\\\\n \\qquad \\displaystyle\n p_n = \\min\\left\\{ x &gt; p_{n-1} \\mid f_{p_n}(x) = 0\\right\\}$</p>\n\n<p><a href="http://www.wolframalpha.com/input/?i=%E2%88%92cos%282%CF%80%28x%2b1%29/2%29%2b1%2b%28%E2%88%92cos%282%CF%80%28x%2b1%29/3%29%2b1%29%28%E2%88%92cos%282%CF%80%28x%2b2%29/3%29%2b1%29%2b%28%E2%88%92cos%282%CF%80%28x%2b1%29/5%29%2b1%29%28%E2%88%92cos%282%CF%80%28x%2b2%29/5%29%2b1%29%28%E2%88%92cos%282%CF%80%28x%2b3%29/5%29%2b1%29%28%E2%88%92cos%282%CF%80%28x%2b4%29/5%29%2b1%29=0%20for%20x" rel="nofollow">Playing with this system a bit over on Wolfram|Alpha</a>, it seems I can get specific answers to the recurrence from their <a href="http://en.wikipedia.org/wiki/Computer_algebra_system" rel="nofollow">computer algebra system</a>. Unfortunately, I can find no specific documentation on the methods they\'re using to solve my equations.</p>\n\n<p>My question, then: </p>\n\n<blockquote>\n  <p>What methods (and what time and space complexities) do computer algebra systems use to solve these forms of equations? I suspect the <a href="http://en.wikipedia.org/wiki/Gr%C3%B6bner_basis" rel="nofollow">Gr\xf6bner basis</a> is commonly used, but I could be very wrong.</p>\n</blockquote>\n', 'ViewCount': '163', 'Title': 'Complexity of computer algebra for systems of trigonometric equations', 'LastEditorUserId': '41', 'LastActivityDate': '2012-05-28T18:02:17.160', 'LastEditDate': '2012-05-28T18:02:17.160', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '958', 'Tags': '<reference-request><runtime-analysis><mathematical-analysis><computer-algebra><mathematical-software>', 'CreationDate': '2012-05-28T09:20:37.547', 'FavoriteCount': '1', 'Id': '2121'},64_11:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>We want to solve a minimal-cost-flow problem with a generic negative-cycle cancelling algorithm. That is, we start with a random valid flow, and then we do not pick any "good" negative cycles such as minimal average cost cycles, but use Bellman-Ford to discover a minimal cycle and augment along the discovered cycle. Let $V$ be the number of nodes in the graph, $A$ the number of edges, $U$ the maximal capacity of an edge in the graph, and $W$ the maximal costs of an edge in the graph. Then, my learning materials claim: </p>\n\n<ul>\n<li>The maximal costs at the beginning can be no more than $AUW$ </li>\n<li>The augmentation along one negative cycle reduces the costs by at least one unit </li>\n<li>The lower bound for the minimal costs is 0, because we don\'t allow negative costs </li>\n<li>Each negative cycle can be found in $O(VA)$ </li>\n</ul>\n\n<p>And they follow from it that the algorithm\'s complexity is $O(V\xb2AUW)$. I understand the logic behind each of the claims, but think that the complexity is different. Specifically, the maximal number of augmentations is given by one unit of flow per augmentation, taking the costs from $AUW$ to zero, giving us a maximum of $AUW$ augmentations. We need to discover a negative cycle for each, so we multiply the maximal number of augmentations by the time needed to discover a cycle ($VA$) and arrive at $O(A\xb2VUW)$ for the algorithm. </p>\n\n<p>Could this be an error in the learning materials (this is a text provided by the professor, not a student\'s notes from the course), or is my logic wrong? </p>\n', 'ViewCount': '258', 'Title': u'Why is the complexity of negative-cycle-cancelling $O(V\xb2AUW)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-06T00:47:27.197', 'LastEditDate': '2012-06-10T11:42:45.650', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1717', 'Tags': '<algorithms><graph-theory><algorithm-analysis><runtime-analysis><network-flow>', 'CreationDate': '2012-06-08T13:26:38.437', 'FavoriteCount': '1', 'Id': '2283'},64_12:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $f$ and $g$ be two functions and $p$ a number. Consider the following program:</p>\n\n<pre><code>Recurs(v,p) :\n  find s &lt; v such that f(s,v) &lt; v/2 and g(s,v-s) &lt; p\n\n  if no such s exists then\n    return v\n  else if s &lt;= v/4 then \n    return v-s U Recurs(s,p)\n  else if s &gt; v/4 then \n    return Recurs(s,p) U Recurs(v-s,p)\nend\n</code></pre>\n\n<p>Can the recurrence for the running time of this recursion be $T(v)=T\\left(\\frac{v}{4}\\right)+T\\left(\\frac{3v}{4}\\right)+1$?</p>\n', 'ViewCount': '154', 'Title': "Is the following recurrence for this program's runtime correct?", 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-11T14:11:29.740', 'LastEditDate': '2012-06-11T10:38:39.823', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '3', 'OwnerDisplayName': 'raarava', 'PostTypeId': '1', 'OwnerUserId': '1818', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2012-06-04T03:17:56.457', 'Id': '2295'},64_13:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '815', 'Title': 'How to describe algorithms, prove and analyse them?', 'LastEditDate': '2012-06-16T12:29:53.190', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '1715', 'FavoriteCount': '4', 'Body': "<p>Before reading <em>The Art of Computer Programming (TAOCP)</em>, I have not considered these questions deeply. I would use pseudo code to describe algorithms, understand them and estimate the running time only about orders of growth. The <em>TAOCP</em> thoroughly changes my mind.</p>\n\n<p><em>TAOCP</em> uses English mixed with steps and <em>goto</em> to describe the algorithm, and uses flow charts to picture the algorithm more readily. It seems low-level, but I find that there's some advantages, especially with flow chart, which I have ignored a lot. We can label each of the arrows with an assertion about the current state of affairs at the time the computation traverses that arrow, and make an inductive proof for the algorithm. The author says:</p>\n\n<blockquote>\n  <p>It is the contention of the author that we really understand why an algorithm is valid only when we reach the point that our minds have implicitly filled in all the assertions, as was done in Fig.4.</p>\n</blockquote>\n\n<p>I have not experienced such stuff. Another advantage is that, we can count the number of times each step is executed. It's easy to check with Kirchhoff's first law. I have not analysed the running time exactly, so some $\\pm1$ might have been omitted when I was estimating the running time.</p>\n\n<p>Analysis of orders of growth is sometimes useless. For example, we cannot distinguish quicksort from heapsort because they are all $E(T(n))=\\Theta(n\\log n)$, where $EX$ is the expected number of random variable $X$, so we should analyse the constant, say, $E(T_1(n))=A_1n\\lg n+B_1n+O(\\log n)$ and $E(T_2(n))=A_2\\lg n+B_2n+O(\\log n)$, thus we can compare $T_1$ and $T_2$ better. And also, sometimes we should compare other quantities, such as variances. Only a rough analysis of orders of growth of running time is not enough. As <em>TAOCP</em> translates the algorithms into assembly language and calculate the running time, It's too hard for me, so I want to know some techniques to analyse the running time a bit more roughly, which is also useful, for higher-level languages such as C, C++ or pseudo codes.</p>\n\n<p>And I want to know what style of description is mainly used in research works, and how to treat these problems.</p>\n", 'Tags': '<algorithms><proof-techniques><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T15:44:06.610', 'CommentCount': '9', 'AcceptedAnswerId': '2390', 'CreationDate': '2012-06-14T13:56:56.550', 'Id': '2374'},64_14:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the book <a href="http://www.cs.uu.nl/geobook/">"Computational Geometry: Algorithms and Applications"</a> by Mark de Berg et al., there is a very simple brute force algorithm for computing Delaunay triangulations. The algorithm uses the notion of <em>illegal edges</em> -- edges that may not appear in a valid Delaunay triangulation and have to be replaced by some other edges. On each step, the algorithm just finds these illegal edges and performs required displacements (called <em>edge flips</em>) till there are no illegal edges.</p>\n\n<blockquote>\n  <p>Algorithm <strong>LegalTriangulation</strong>($T$)</p>\n  \n  <p><em>Input</em>. Some triangulation $T$ of a point set $P$.<br>\n  <em>Output</em>. A legal triangulation of $P$.</p>\n  \n  <p><strong>while</strong> $T$ contains an illegal edge $p_ip_j$<br>\n  <strong>do</strong><br>\n  $\\quad$ Let $p_i p_j p_k$ and $p_i p_j p_l$ be the two triangles adjacent to $p_ip_j$.<br>\n  $\\quad$ Remove $p_ip_j$ from $T$, and add $p_kp_l$ instead.<br/>\n  <strong>return</strong> $T$.</p>\n</blockquote>\n\n<p>I\'ve heard that this algorithm runs in $O(n^2)$ time in worst case; however, it is not clear to me whether this statement is correct or not. If yes, how can one prove this upper bound?</p>\n', 'ViewCount': '769', 'Title': 'Brute force Delaunay triangulation algorithm complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-21T13:44:11.817', 'LastEditDate': '2012-06-17T13:28:32.350', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><time-complexity><algorithm-analysis><computational-geometry><runtime-analysis>', 'CreationDate': '2012-06-16T22:01:33.543', 'FavoriteCount': '2', 'Id': '2400'},64_15:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is an example in my lecture notes.\nIs this function with time complexity $O(n \\log n)$?.\nBecause the worst case is the funtion goes into <code>else</code> branch, and 2 nested loops with time complexity of $\\log n$ and $n$, so it is $O(n \\log n)$. Am I right?</p>\n\n<pre><code>int j = 3;\nint k = j * n / 345;\nif(k &gt; 100){\n    System.out.println("k: " + k);\n}else{\n    for(int i=1; i&lt;n; i*=2){\n        for(int j=0; j&lt;i; j++){\n            k++;\n        }\n    }\n}\n</code></pre>\n', 'ViewCount': '557', 'Title': 'What is the time complexity of this function?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-26T23:13:31.913', 'LastEditDate': '2012-06-25T17:21:54.770', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '2497', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Tags': '<complexity-theory><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-06-25T14:48:54.900', 'Id': '2487'},64_16:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given two strings, $r$ and $s$, where $n = |r|$, $m = |s|$ and $m \\ll n$, find the minimum edit distance between $s$ for each beginning position in $r$ efficiently.</p>\n\n<p>That is, for each suffix of $r$ beginning at position $k$, $r_k$, find the <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> of $r_k$ and $s$ for each $k \\in [0, |r|-1]$.  In other words, I would like an array of scores, $A$, such that each position, $A[k]$, corresponds to the score of $r_k$ and $s$.</p>\n\n<p>The obvious solution is to use the standard dynamic programming solution for each $r_k$ against $s$ considered separately, but this has the abysmal running time of $O(n m^2)$ (or $O(n d^2)$, where $d$ is the maximum edit distance).  It seems like you should be able to re-use the information that you\'ve computed for $r_0$ against $s$ for the comparison with $s$ and $r_1$.</p>\n\n<p>I\'ve thought of constructing a prefix tree and then trying to do dynamic programming algorithm on $s$ against the trie, but this still has worst case $O(n d^2)$ (where $d$ is the maximum edit distance) as the trie is only optimized for efficient lookup.</p>\n\n<p>Ideally I would like something that has worst case running time of $O(n d)$ though I would settle for good average case running time.  Does anyone have any suggestions?  Is $O(n d^2)$ the best you can do, in general?</p>\n\n<p>Here are some links that might be relevant though I can\'t see how they would apply to the above problem as most of them are optimized for lookup only:</p>\n\n<ul>\n<li><a href="http://stevehanov.ca/blog/index.php?id=114" rel="nofollow">Fast and Easy Levensthein distance using a Trie</a></li>\n<li><a href="http://stackoverflow.com/questions/3183149/most-efficient-way-to-calculate-levenshtein-distance">SO: Most efficient way to calculate Levenshtein distance</a></li>\n<li><a href="http://stackoverflow.com/questions/4057513/levenshtein-distance-algorithm-better-than-onm?rq=1">SO: Levenshtein Distance Algoirthm better than $O(n m)$</a></li>\n<li><a href="http://www.berghel.net/publications/asm/asm.php" rel="nofollow">An extension of Ukkonen\'s enhanced dynamic programming ASM algorithm</a></li>\n<li><a href="http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata" rel="nofollow">Damn Cool Algorithms: Levenshtein Automata</a></li>\n</ul>\n\n<p>I\'ve also heard some talk about using some type of distance metric to optimize search (such as a <a href="http://en.wikipedia.org/wiki/BK-tree" rel="nofollow">BK-tree</a>?) but I know little about this area and how it applies to this problem.</p>\n', 'ViewCount': '836', 'Title': 'Efficiently calculating minimum edit distance of a smaller string at each position in a larger one', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-28T15:40:06.263', 'LastEditDate': '2012-06-28T15:40:06.263', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '2526', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '67', 'Tags': '<algorithms><runtime-analysis><strings><dynamic-programming><string-metrics>', 'CreationDate': '2012-06-27T20:48:29.300', 'Id': '2519'},64_17:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<h3>Background</h3>\n\n<p>$\\newcommand\\ldotd{\\mathinner{..}}$Last month, I heard about a new linear-time algorithm to determine the <a href="https://en.wikipedia.org/wiki/Longest_palindromic_substring" rel="nofollow">longest palindromic substring</a> called Jeuring\'s algorithm. It seemed interesting, therefore I had a try to analyze the algorithm. It\'s not too difficult to show that it\'s an $\\Theta(n)$-time-algorithm, but I want to observe more closely.</p>\n\n<p>I wrote a C++ implementation, and an essay to prove and analyze the algorithm. It\'s somewhat long, therefore I will only post the critical part of the code, where you can learn the algorithm by heart, see the succeeding section. You can get all my previous work from <a href="https://github.com/FrankEular/pldrm" rel="nofollow">github</a>. pldrm.nw is the literate programming source file, which produces the pldrm.cc and pldrm.pdf. pldrm.pdf is the essay I\'ve written.</p>\n\n<h3>Problem</h3>\n\n<p>Let $A=$ the number of times <em>goto</em> statement is executed in <em>move loop</em>, and $B=$ the number of times <em>move loop</em> is executed where <em>goto</em> statement is <strong>NOT</strong> executed, $C=$ the number of times a[++j]=min(a[p],l) is executed in <em>move loop</em>. You can see the code in the following section.\nI found that $A+B+C=2n$ and $B=\\sum_{k=2}^{n+1}[b_k=1]$, where $b_k$ is the length of longest tail palindrome of $s[0\\ldotd k]$, and $[P]$ is <a href="http://en.wikipedia.org/wiki/Iverson_bracket" rel="nofollow">Iverson bracket</a>. For details, you can read my pdf from <a href="https://github.com/FrankEular/pldrm" rel="nofollow">github</a>.\nI\'m looking for somebody to help me analyze the quantity $A,C$. Any help? Thanks!</p>\n\n<h3>The critical part of the algorithm</h3>\n\n<p>Given that $s[1\\ldotd n]$ is the string inputted, and $n$ is the length of $s$, where $s[0]=1,s[n+1]=0$.\nWe say $l+r$ is the center of substring $s[l\\ldotd r]$. For example, $4$ is the center of $s[2\\ldotd2]$ or $s[1\\ldotd3]$.\n$a_k$ is the length of the longest palindrome whose center is $k$, and $a[0\\ldotd2n]$ is the array to save $\\langle a_k\\rangle$.\n<strong>The algorithm is used to determine $a_k$.</strong>\nWe call some string $A$ is a tail palindrome of the other string $B$ if and only if $A$ is a palindromic tail substring of $B$. For example, $A=aba$ and $B=aaaababa$, where $A$ is palindromic and $A$ is a tail substring of $B$.</p>\n\n<p>Here\'s the critical part of the code:</p>\n\n<pre><code>&lt;&lt;main loop&gt;&gt;=\nj = 1;\nl = 1;\na[0] = 1;\na[1] = 0;\nfor (int k=2; k&lt;=n+1; k++) {\n  &lt;&lt;process&gt;&gt;\n  advance:\n  ;\n}\n@\n</code></pre>\n\n<p>Process is made up of an infinite loop, which is used to find the longest tail palindrome of $s[0\\ldotd k]$. There are two exits of it. One is in extension subroutine, while the other one is in move loop. The way of exit is <em>goto advance;</em>.</p>\n\n<pre><code>&lt;&lt;process&gt;&gt;=\nfor (;;) {\n  &lt;&lt;check&gt;&gt;\n  &lt;&lt;move loop&gt;&gt;\n}\n@\n</code></pre>\n\n<p>The check subroutine checks whether a tail palindrome of $s[0\\ldotd k-1]$ could be extended to that of $s[0\\ldotd k]$. If so, exit from the process loop and advance $k$, otherwise start the move loop.</p>\n\n<pre><code>&lt;&lt;check&gt;&gt;=\nif (s[k] == s[k-l-1]) {\n  l += 2;\n  goto advance;\n}\n@\n</code></pre>\n\n<p>Here\'s the move loop, which looks short and easy. It\'s used to find a shorter tail palindrome of $s[0\\ldotd k-1]$.</p>\n\n<pre><code>&lt;&lt;move loop&gt;&gt;=\na[++j] = l;\nfor (p=j-1; --l&gt;=0&amp;&amp;l!=a[p]; p--) {\n  a[++j] = min(a[p], l);\n}\nif (l &lt; 0) {\n  l = 1;\n  goto advance;\n}\n@\n</code></pre>\n', 'ViewCount': '510', 'Title': 'Analysis of a linear-time algorithm for longest palindromic substring', 'LastEditorUserId': '29', 'LastActivityDate': '2012-08-12T13:37:49.487', 'LastEditDate': '2012-08-12T13:37:49.487', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1715', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-07-28T04:29:45.943', 'Id': '2936'},64_18:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve just begun this stage 2 Compsci paper on algorithms, and stuff like this is not my strong point. I\'ve come across this in my lecture slides.</p>\n\n<pre><code>int length = input.length();\nfor (int i = 0; i &lt; length - 1; i++) {\n    for (int j = i + 1; j &lt; length; j++) {\n        System.out.println(input.substring(i,j));\n    }\n}\n</code></pre>\n\n<p>"In each iteration, the outer loop executes $\\frac{n^{2}-(2i-1)n-i+i^{2}}{2}$ operations from the inner loop for $i = 0, \\ldots, n-1$."</p>\n\n<p>Can someone please explain this to me step by step?</p>\n\n<p>I believe the formula above was obtained by using Gauss\' formula for adding numbers... I think...</p>\n', 'ViewCount': '1969', 'Title': 'Time complexity formula of nested loops', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-02T08:36:54.223', 'LastEditDate': '2012-08-02T08:36:54.223', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '2996', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-02T04:56:03.523', 'Id': '2994'},64_19:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Of large sparse biparite graphs (say degree 4) with N verticies, roughly speaking, which of them cause the worst case running time of the <a href="https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm" rel="nofollow">Hopcroft-Karp algorithm</a>? What is their general structure and architecture, and why does it cause a problem?</p>\n\n<p>Further, in many implementations the DFS part is implemented using recursion, eg from Wikipedia:</p>\n\n<pre><code>function DFS (v)\n    if v != NIL\n        for each u in Adj[v]\n            if Dist[ Pair_G2[u] ] == Dist[v] + 1\n                if DFS(Pair_G2[u]) == true\n                    Pair_G2[u] = v\n                    Pair_G1[v] = u\n                    return true\n        Dist[v] = \u221e\n        return false\n    return true\n</code></pre>\n\n<p>What is the approximate maximum depth of the recursion in the worst case?</p>\n', 'ViewCount': '227', 'Title': 'Worst-case sparse graphs for Hopcroft-Karp Algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-08T06:54:35.197', 'LastEditDate': '2012-08-08T06:54:35.197', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1577', 'Tags': '<algorithms><graph-theory><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-07T05:31:16.787', 'Id': '3064'},64_20:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I notice that in a few CS research papers, to compare the efficiency of two algorithms, the total number of key comparison in the algorithms is used rather than the real computing times themselves. Why can't we compare which one is better by running both programs and counting the total time needed to run the algorithms? </p>\n", 'ViewCount': '310', 'Title': 'Why use comparisons instead of runtime for comparing two algorithms?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-12T01:53:58.833', 'LastEditDate': '2012-08-12T00:14:15.607', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2460', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-11T20:41:56.243', 'FavoriteCount': '3', 'Id': '3126'},64_21:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>This is a problem from CLRS 23-2 that I'm trying to solve. The problem assumes that given graph G is very sparse connected. It wants to improve further over Prim's algorithm $O(E + V \\lg V)$. The idea is to contract the graph, i.e. collapse two or more nodes into one node. So each reduction will reduce the graph by at least half nodes. The question is to come up with implementation such that time complexity of MST-REDUCE is $O(E)$. This uses set operations. MakeSet, Union and Find-Set. I've annotated my analysis in the picture along with algorithm. \nI'm thinking to implement the set as linked list here. So my make-set and find-set are $O(1)$. But Union sucks: $O(V)$. Since we are doing union for all the elements, we have total $O(V^2)$ time spent in union. Which gives amortized $O(V)$. Now the problem isn't clear whether it is expecting amortized time complexity or not. So I'm wondering if any better approach is possible. Note the algorithm is running for all nodes and all edges. Hence I think amortized makes sense.</p>\n\n<p>Here is my analysis (line, complexity)</p>\n\n<p>1-3 $V$ </p>\n\n<p>4-9 $\\frac{V}{2} \\cdot union = \\frac{V}{2} \\cdot \\frac{V}{2} = V^2 = V$ (amortized)</p>\n\n<p>10 $V \\cdot findset = V$</p>\n\n<p>12-21 $E \\cdot findset = E$</p>\n\n<p>Since $E &gt;= V - 1$, we have overall time complexity of $O(E)$. </p>\n\n<pre><code>0   MST-REDUCE(G, orig, c T)\n1   for each v in V[G]\n2       mark[v] &lt;- FALSE\n3       MAKE-SET(v)\n4   for each u in V[G]\n5       if mark[u] = FALSE\n6           choose v in Adj[u] such that c[u,v] is minimized.\n7           UNION(u,v)\n8           T &lt;- T union { orig(u,v) }\n9           mark[u] &lt;- mark[v] &lt;- TRUE\n10  V[G'] &lt;- { FIND-SET(v) : v in V[G] }\n11  E[G'] &lt;- { }\n12  for each (x,y) in E[G]\n13      u &lt;- FIND-SET(x)\n14      v &lt;- FIND-SET(y)\n15      if (u,v) doesn't belong E[G']\n16          E[G'] &lt;- E[G'] union {(u,v)}\n17          orig'[u,v] &lt;- orig[x,y]\n18          c'[u,v] &lt;- c[x,y]\n19      else if c[x,y] &lt; c'[u,v]\n20          orig'[u,v] &lt;- orig[x,y]\n21          c'[u,v] &lt;- c[x,y]\n22  construct adjacency list Adj for G'\n23  return G', orig', c', T\n</code></pre>\n", 'ViewCount': '671', 'Title': 'Show that the Minimum spanning tree Reduce Algorithm runs in O(E) on sparse graphs', 'LastEditorUserId': '2375', 'LastActivityDate': '2012-08-31T08:37:11.837', 'LastEditDate': '2012-08-31T08:37:11.837', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2375', 'Tags': '<algorithms><graph-theory><graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-08-30T22:19:05.153', 'Id': '3375'},64_22:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>If you have a quick-sort algorithm, and you always select the smallest (or largest) element as your pivot; am I right in assuming that if you provide an already sorted data set, you will always get worst-case performance regardless of whether your 'already sorted' list is in ascending or descending order? </p>\n\n<p>My thinking is that, if you always choose the smallest element for your pivot, then whether your 'already-sorted' input is sorted by ascending or descending doesn't matter because the subset chosen to be sorted relative to your pivot will always be the same size?</p>\n", 'ViewCount': '433', 'Title': 'Does Quicksort always have quadratic runtime if you choose a maximum element as pivot?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-27T12:58:30.307', 'LastEditDate': '2012-08-31T07:28:12.087', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '3379', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-08-31T04:24:50.107', 'Id': '3377'},64_23:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m confused to conclude the recursion tree method a guess for the next recurrence:\n$$T(n)=3T\\left (\\left\\lfloor \\frac{n}{2}\\right \\rfloor\\right) +n$$\nI write some costs for the levels of tree, you can see, but I\'m confusing in the final guess. I know for the master theorem that the answer for the guess is like that \n$$\\Theta(n^{\\log_2 3}) $$ but in the steps by tree and I don\'t belive, you can see my error (?). How can I finished or know the outcome for this method to calculate $T(n)$? \nthanks,\n<img src="http://i.stack.imgur.com/T7vlE.png" alt="enter image description here">\n<img src="http://i.stack.imgur.com/V00qK.png" alt="Recursion Tree by the my problem">\nAnd the final conclusion is (?)\n$$=2 n^{\\log_2 3}+\\Theta(n^{\\log_2 3})\\leq cn^{\\log_2 3} \\rightarrow \\ \\ T(n)\\in\\Theta(n^{\\log_2 3})$$ with $c=3$ ?</p>\n', 'ViewCount': '124', 'Title': 'Doubt with a problem of grown functions and recursion tree', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-22T15:42:55.307', 'LastEditDate': '2012-09-17T17:48:41.503', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2012-09-17T02:09:19.717', 'Id': '4583'},64_24:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '66', 'Title': 'Computing the running time of a divide-by-4-and-conquer algorithm', 'LastEditDate': '2012-09-18T22:12:40.627', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1152', 'FavoriteCount': '1', 'Body': "<p>I write this code in python:</p>\n\n<pre><code>def sub(ma):\n    n = len(ma); m = len(ma[0])\n    if n != m : return\n    n2 = int(ceil(n/2))\n    a = []; b = []; c = []; d = [] \n    for i in range(n2):\n        a.append(ma[i][0:n2])\n        b.append(ma[i][n2:n])\n        c.append(ma[n2+i][0:n2])\n        d.append(ma[n2+i][n2:n])\n    return [a,b,c,d] \n\ndef sum(ma):\n        if len(ma) == 1 : return ma[0][0]\n        div = sub(ma)       \n        return sum(div[0])+sum(div[1])+sum(div[2])+sum(div[3]) \n</code></pre>\n\n<p>Do you know what is a possibly recurrence equation $T(n)$ to the 'sum' method? \nI suppose that is like that\n$$T(n) = 4T(n/2) + f(n)$$\nwhat it is $f(n)$ ?\nThanks, </p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-18T22:12:40.627', 'CommentCount': '0', 'AcceptedAnswerId': '4586', 'CreationDate': '2012-09-17T06:29:26.353', 'Id': '4584'},64_25:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I was given a homework assignment with Big O. I'm stuck with nested for loops that are dependent on the previous loop. Here is a changed up version of my homework question, since I really do want to understand it:</p>\n\n<pre><code>sum = 0;\nfor (i = 0; i &lt; n; i++ \n    for (j = 0; j &lt; i; j++) \n        sum++;\n</code></pre>\n\n<p>The part that's throwing me off is the <code>j &lt; i</code> part. It seems like it would execute almost like a factorial, but with addition. Any hints would be really appreciated.</p>\n", 'ViewCount': '2880', 'Title': 'Big O: Nested For Loop With Dependence', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-18T20:24:32.557', 'LastEditDate': '2012-09-17T17:55:35.750', 'AnswerCount': '3', 'CommentCount': '1', 'Score': '3', 'OwnerDisplayName': 'user1000229', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><landau-notation>', 'CreationDate': '2012-09-07T23:38:26.383', 'Id': '4590'},64_26:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The source of this question comes from an undergraduate course I am taking, which covers an introduction to the analysis of algorithms. This is not for homework, but rather a question asked in CLRS.</p>\n\n<p>You have a slow machine running at $x$ MIPS, and a fast machine running at $y$ MIPS. You also have two algorithms of the same class, but different running time complexities: one "slow" algorithm runs at $T(n) = c_1n^2$ whereas a "fast" algorithm runs at $T(n) = c_2n \\log n$.</p>\n\n<p>You execute the slow algorithm on the fast machine, and the fast algorithm on the slow machine. What is the largest value of n such that the fast machine running the slow algorithm beats the slow machine running the fast algorithm?</p>\n\n<p><strong>My solution so far:</strong></p>\n\n<p>Find the set of all $n$ such that $$\\frac{c_2n\\log n}{x} &gt; \\frac{c_1n^2}{y}$$ where $n$ is a natural number.</p>\n\n<p>This is my work thus far:</p>\n\n<p>$$\\{n : \\frac{c_2 n \\log_2 n}{x} &gt; \\frac{c_1 n^2}{y}, n \\in \\mathbb{N}\\} = \\{n : n &lt; \\frac{c_2 y}{c_1 x} \\log_2 n, n \\in \\mathbb{N}\\}$$</p>\n\n<p>The only solution that comes to mind now is to plug-n-chug all values of $n$ until I find the first n where </p>\n\n<p>$$n &lt; \\frac{c_2y}{c_1x}\\log(n)$$ </p>\n\n<p>no longer holds.</p>\n', 'ViewCount': '288', 'Title': 'Given a fast and a slow computer, at what sizes does the fast computer running a slow algorithm beat the slow computer running a fast algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-19T10:41:35.640', 'LastEditDate': '2012-09-19T10:07:43.623', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2867', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><mathematical-analysis>', 'CreationDate': '2012-09-18T04:09:39.707', 'Id': '4600'},64_27:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The notion of best-case running time is kind of ambiguous for me. According to wikipedia, the definition of best case running time is:</p>\n\n<blockquote>\n  <p>The term best-case performance is used in computer science to describe the way of an algorithm behaves under optimal conditions. For example, the best case for a simple linear search on a list occurs when the desired element is the first element of the list.</p>\n</blockquote>\n\n<p>According to this definition, the best case running time for BST insertion should be $O(1)$ [consider that we are inserting to the root node]. But different resources says different things, some claim that it is $O(\\log n)$ [perfect balanced tree] and some others claim that it is $O(1)$ which one should I believe?</p>\n', 'ViewCount': '1986', 'Title': 'Best-Case Running Time For Binary Search Tree Insertion', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-24T22:16:37.427', 'LastEditDate': '2012-09-24T22:12:54.513', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4725', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2956', 'Tags': '<terminology><data-structures><runtime-analysis><binary-trees>', 'CreationDate': '2012-09-24T21:48:27.473', 'Id': '4723'},64_28:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What is the runtime complexity of the following implementation of Bubblesort (for integers)?</p>\n\n<pre><code>    #define SWAP(a,b)   { int t; t=a; a=b; b=t; }\n\n    void bubble( int a[], int n )\n    /* Pre-condition: a contains n items to be sorted */\n    {\n       int i, j;\n      /* Make n passes through the array */\n      for(i=0;i&lt;n-1;i++)\n      {\n     /* From the first element to the end\n       of the unsorted section */\n        for(j=1;j&lt;(n-i);j++)\n        {\n        /* If adjacent items are out of order, swap them */\n       if( a[j-1]&gt;a[j] ) SWAP(a[j-1],a[j]);\n       }\n    }\n}  \n</code></pre>\n', 'ViewCount': '282', 'Title': 'Complexity of optimized bubblesort', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-07T15:49:43.613', 'LastEditDate': '2012-10-07T15:49:43.613', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3087', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2012-10-07T09:52:54.087', 'Id': '4915'},64_29:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>This is a similar algorithm to one I used in a previous question, but I\'m trying to illustrate a different problem here.</p>\n\n<pre><code>for (int i = 0; i &lt; numbers.length - 1; i++) {\n    for (int j = i + 1; j &lt; numbers.length; j++) {\n        if (numbers[i] + numbers[j] == 10) {\n            System.out.println(i+" and "+j+" add up to 10!");\n            return;\n        }\n    }\n}\nSystem.out.println("None of these numbers add up to 10!");\nreturn;\n</code></pre>\n\n<p>Basically, I realised, that I have a decent understanding of how to work out the best case run time here. I.e. the first two numbers which are fed in, add up to 10, and therefore our best-case runtime is constant time. Also, I understand that in the worst-case, none of the numbers we provide add up to 10, so we must then iterate through all loops, giving us a quadratic runtime. However, in the slides I was going through (for this particular course) I noticed that I had missed this:</p>\n\n<p>Average case</p>\n\n<p>\u2022 Presume that we\'ve just randomly given\nintegers between 1 and 9 as input in\nnumbers to our previous example</p>\n\n<p>\u2022 Exercise: Work out the average running time</p>\n\n<p>I realise I don\'t have the first clue as to how to go about calculating average case run-time. I\'m not asking for the answer to: "what is the average-case runtime for this algorithm?", what I need to know is, how do you work something like this out? </p>\n', 'ViewCount': '253', 'Title': 'How to go about working the average case run time of this trivial algorithm (and other algorithms)?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T16:02:27.433', 'LastEditDate': '2012-10-28T16:02:27.433', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2337', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2012-10-10T10:06:09.990', 'Id': '4993'},64_30:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '3565', 'Title': 'A d-ary heap problem from CLRS', 'LastEditDate': '2012-10-17T19:50:03.620', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'lucasKoFromTW', 'PostTypeId': '1', 'OwnerUserId': '4195', 'Body': u'<p>I got confused while solving the following problem (questions 1\u20133).</p>\n\n<h3>Question</h3>\n\n<blockquote>\n  <p>A <em>d</em>-ary heap is like a binary heap, but(with one possible exception) non-leaf nodes have <em>d</em> children instead of 2 children.</p>\n  \n  <ol>\n  <li><p>How would you represent a <em>d</em>-ary heap in an array?</p></li>\n  <li><p>What is the height of a <em>d</em>-ary heap of <em>n</em> elements in terms of <em>n</em> and <em>d</em>?</p></li>\n  <li><p>Give an efficient implementation of EXTRACT-MAX in a <em>d</em>-ary max-heap. Analyze its running time in terms of <em>d</em> and <em>n</em>.</p></li>\n  <li><p><sub> Give an efficient implementation of INSERT in a <em>d</em>-ary max-heap. Analyze its running time in terms of <em>d</em> and <em>n</em>. </sub></p></li>\n  <li><p><sub> Give an efficient implementation of INCREASE-KEY(<em>A</em>, <em>i</em>, <em>k</em>), which flags an error if <em>k</em> &lt; A[i] = k and then updates the <em>d</em>-ary matrix heap structure appropriately. Analyze its running time in terms of <em>d</em> and <em>n</em>. </sub></p></li>\n  </ol>\n</blockquote>\n\n<h3>My Solution</h3>\n\n<ol>\n<li><p>Give an array $A[a_1 .. a_n]$</p>\n\n<p>$\\qquad \\begin{align}\n \\text{root} &amp;: a_1\\\\\n \\text{level 1} &amp;: a_{2} \\dots a_{2+d-1}\\\\\n \\text{level 2} &amp;: a_{2+d} \\dots a_{2+d+d^2-1}\\\\\n &amp;\\vdots\\\\\n \\text{level k} &amp;: a_{2+\\sum\\limits_{i=1}^{k-1}d^i} \\dots a_{2+\\sum\\limits_{i=1}^{k}d^i-1}\n \\end{align}$</p>\n\n<p>\u2192 <strong>My notation seems a bit sophisticated. Is there any other simpler one?</strong></p></li>\n<li><p>Let <em>h</em> denotes the height of the <em>d</em>-ary heap.</p>\n\n<p>Suppose that the heap is a complete <em>d</em>-ary tree\n$$\n 1+d+d^2+..+d^h=n\\\\\n \\dfrac{d^{h+1}-1}{d-1}=n\\\\\n h=log_d[n{d-1}+1] - 1\n $$</p></li>\n<li><p>This is my implementation:</p>\n\n<pre><code>EXTRACT-MAX(A)\n1  if A.heapsize &lt; 1\n2      error "heap underflow"\n3  max = A[1]\n4  A[1] = A[A.heapsize]\n5  A.heap-size = A.heap-size - 1\n6  MAX-HEAPIFY(A, 1)\n7  return max\n\nMAX-HEAPIFY(A, i)\n1  assign depthk-children to AUX[1..d]\n2  for k=1 to d\n3      compare A[i] with AUX[k]\n4      if A[i] &lt;= AUX[k]\n5          exchange A[i] with AUX[k]\n6          k = largest\n7  assign AUX[1..d] back to A[depthk-children]\n8  if largest != i\n9      MAX-HEAPIFY(A, (2+(1+d+d^2+..+d^{k-1})+(largest-1) )\n</code></pre>\n\n<ul>\n<li><p>The running time of MAX-HEAPIFY:</p>\n\n<p>$$T_M = d(c_8*d + (c_9+..+c_13)*d +c_14*d)$$\nwhere $c_i$ denotes the cost of <em>i</em>-th line above.</p></li>\n<li><p>EXTRACT-MAX:\n$$\n       T_E = (c_1+..+c_7) + T_M \\leq C*d*h\\\\\n       = C*d*(log_d[n(d-1)+1] - 1)\\\\\n       = O(dlog_d[n(d-1)])\n       $$</p></li>\n</ul>\n\n<p>\u2192 <strong>Is this an efficient solution? Or there is something wrong within my solution?</strong></p></li>\n</ol>\n', 'Tags': '<data-structures><time-complexity><runtime-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-12T17:23:56.180', 'CommentCount': '0', 'AcceptedAnswerId': '6097', 'CreationDate': '2012-10-15T03:45:12.463', 'Id': '6078'},64_31:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have some difficulties performing the worst case analysis on this algorithm.<br>\nThe outermost loop is executed $2N$ times.<br>\nThe while loop, in the worst case, will increase by $2$ each time, so it performs $i/2$ basic operations ($*2$ because double call)</p>\n\n<pre><code>for (i=1; i&lt;=2*N; i++) {\n        j = 0;\n        while (j &lt;= i) {\n            a[i] = function (function (a[i]));\n            if (c[i][j] != 0)\n                j = j + 6;\n            else\n                j = j + 2;\n        }\n    }\n</code></pre>\n\n<p><code>function</code> is the basic operation.<br>\nAm I going the right way? </p>\n', 'ViewCount': '525', 'Title': 'Runtime analysis of a nested loop', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-17T09:19:56.000', 'LastEditDate': '2012-11-17T09:19:56.000', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'eouti', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2012-10-17T16:35:50.160', 'Id': '6129'},64_32:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '985', 'Title': "Base of logarithm in runtime of Prim's and Kruskal's algorithms", 'LastEditDate': '2012-11-01T22:48:50.390', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'CodeKingPlusPlus', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Body': "<p>For Prim's and Kruskal's Algorithm there are many implementations which will give different running times. However suppose our implementation of Prim's algorithm has runtime $O(|E| + |V|\\cdot \\log(|V|))$ and Kruskals's algorithm has runtime $O(|E|\\cdot \\log(|V|))$.</p>\n\n<p>What is the base of the $\\log$?</p>\n", 'Tags': '<algorithms><time-complexity><graphs><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T22:48:50.390', 'CommentCount': '3', 'AcceptedAnswerId': '6436', 'CreationDate': '2012-10-27T20:48:47.810', 'Id': '6435'},64_33:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Take a Turing machine, with a terminating program, convert it to some representation of the machine which captures, in a lossless manner, its state as it performs the computation.</p>\n\n<p>So you have a complete representation of the machine, the program, and its internal organisation as it performs the computation.</p>\n\n<p>I am going to suggest a graphical form, nodes and edges, names for nodes.</p>\n\n<p>Take a second Turing machine with a slightly different program. This program is identical save that it performs a single unit of function from the first program in a non optimal way, say it performs the single unit 3 times, changing some value to the correct output the first time, taking it to some second result the second time and then finally returning again to the correct first result. Like a reflection.</p>\n\n<p>Would it not be possible for some statistical technique to analyse the graph of the two machines including their process and find a compression of the graph of the second machine, which is smaller in terms of the size of the graph of its process and yet consistent with the mode of operation of the machine.</p>\n\n<p>For instance a graph matching algorithm could find that there is a subgraph match between one portion of the the process graph of the first machine and one part of the process graph of the second machine and replace the subgraph of the second machine with the subgraph of the process of the first machine.</p>\n\n<p>How it would then alter the program of the second machine to generate that altered graph I am unsure of.</p>\n\n<p>Do such techniques exist? Where would I find them, or is the analysis flawed or incomplete in some way which prevents its operation? What should I learn to understand its implementation or the truth of its deficiency?</p>\n', 'ViewCount': '104', 'Title': 'Is it possible to analyse computation?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-02T21:06:44.393', 'LastEditDate': '2012-11-02T21:06:44.393', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4340', 'Tags': '<graphs><optimization><runtime-analysis><compilers><artificial-intelligence>', 'CreationDate': '2012-11-02T12:58:51.063', 'Id': '6452'},64_34:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '600', 'Title': 'Iterative binary search analysis', 'LastEditDate': '2012-11-04T15:41:46.187', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Hasan Tahsin', 'PostTypeId': '1', 'OwnerUserId': '4610', 'Body': '<p>I\'m a little bit confused about the analysis of <a href="http://en.wikipedia.org/wiki/Binary_search" rel="nofollow">binary search</a>.\nIn almost every paper, the writer assumes that the array size $n$ is always $2^k$.\nWell I truly understand that the time complexity becomes $\\log(n)$ (worst case) under this assumption. But what if $n \\neq 2^k$?</p>\n\n<p>For example if $n=24$, then we have\n5 iterations for 24<br>\n4 i. for 12<br>\n3 i. for 6<br>\n2 i. for 3<br>\n1 i. for 1</p>\n\n<p>So how do we get the result $k=\\log n$ in this example (I mean of course every similar example whereby $n\\neq2^k$)?</p>\n', 'Tags': '<algorithms><time-complexity><algorithm-analysis><runtime-analysis><search-algorithms>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-04T15:41:46.187', 'CommentCount': '4', 'AcceptedAnswerId': '6471', 'CreationDate': '2012-11-03T10:15:40.587', 'Id': '6470'},64_35:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose a <strong>build</strong> max-heap operation runs bubble down over a heap. How does its amortized cost equal $O(n)$?</p>\n', 'ViewCount': '1518', 'Title': "How can I prove that a build max heap's amortized cost is $O(n)$?", 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-18T07:58:21.743', 'LastEditDate': '2012-11-18T07:58:21.743', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '6659', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<data-structures><runtime-analysis><heaps>', 'CreationDate': '2012-11-14T03:58:41.463', 'Id': '6655'},64_36:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '325', 'Title': 'What is the time complexity of the following program?', 'LastEditDate': '2012-12-23T09:30:04.240', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4732', 'FavoriteCount': '2', 'Body': '<p>Please help me calculate the time complexity of the following program.</p>\n\n<pre><code>int fun (int n) {\n  if (n &lt;= 2)\n   return 1;\n  else\n   return fun(sqrt(n)) + n;\n}\n</code></pre>\n\n<p>Please explain.</p>\n\n<p>There were four choices given.</p>\n\n<ol>\n<li>$\\Theta(n^2)$</li>\n<li>$\\Theta(n \\log n)$</li>\n<li>$\\Theta(\\log n)$</li>\n<li>$\\Theta(\\log \\log n)$</li>\n</ol>\n', 'Tags': '<algorithm-analysis><runtime-analysis><landau-notation>', 'LastEditorUserId': '3016', 'LastActivityDate': '2012-12-23T09:30:04.240', 'CommentCount': '0', 'AcceptedAnswerId': '6904', 'CreationDate': '2012-11-26T02:06:18.790', 'Id': '6901'},64_37:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Sorry if this question is very simplistic; I'm just starting out and I'm trying to wrap my head around all this asymptotic bound stuff. When trying to find the upper bound for the worst case of a function does it need to take into account what the meat of the code actually does? I have some code that would (in the worst case) iterate through a while loop n times, but when you consider what that code actually does, it would always make it so that the condition for the while loop becomes false on the next iteration.</p>\n\n<p>Some people say that it doesn't matter what is actually happening within the code; just that if it has the ability to iterate n times (even though it's virtually impossible because of the body of the loop) then that would be the worst case vs. however many steps the code ACTUALLY runs. </p>\n\n<p>If anyone has any insight it would be greatly appreciated! Thanks!</p>\n", 'ViewCount': '144', 'Title': 'Input to make worst case on big O not possible?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-06T06:06:04.333', 'LastEditDate': '2012-12-06T06:06:04.333', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4888', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2012-12-05T21:47:04.640', 'Id': '7198'},64_38:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1077', 'Title': 'Triple nested for-loops', 'LastEditDate': '2012-12-08T19:44:52.043', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4935', 'FavoriteCount': '1', 'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/7008/a-puzzle-related-to-nested-loops">A puzzle related to nested loops</a>  </p>\n</blockquote>\n\n\n\n<p>I am trying to count the exact/total number of iterations the following nested for-loops are executed:</p>\n\n<pre><code>s=0\nfor (i: 1 to n)\n for (j: 1 to i)\n  for (k: j to i)\n   s = s + 1\n</code></pre>\n\n<p>I know that the first two for-loops will have <code>n(n+1)/2</code> iterations. My problem is with the third for-loop. Due to this third loop what factor am I supposed to multiply with <code>n(n+1)/2</code> to get the total number of iterations? </p>\n\n<p>Any help would be appreciated? Thanks</p>\n', 'ClosedDate': '2012-12-10T15:34:49.820', 'Tags': '<runtime-analysis><loops>', 'LastEditorUserId': '3016', 'LastActivityDate': '2013-03-10T22:16:03.350', 'CommentCount': '0', 'AcceptedAnswerId': '7258', 'CreationDate': '2012-12-08T19:42:52.560', 'Id': '7256'},64_39:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I came across a couple of solutions to one of the problems that is in the CLRS textbook (pg. 637 23.2-5 edition 3).  I am wondering if anyone can make a clarification as to the stated running time of the solution.</p>\n\n<p>Q:  Given that we know the edge weights on a graph are between $1$ and some constant $W$, how fast can we make Prim's algorithm run?</p>\n\n<p>Solution:</p>\n\n<ul>\n<li>Uses an array of linked lists where each index corresponds to a given weight [$1\\dots W + 1$]</li>\n<li>Each link linked lists contains a series of vertices with the weight of the index as their key</li>\n</ul>\n\n<p>The run time given is $O(E)$</p>\n\n<p>They break it down as follows:</p>\n\n<p>$O(1)$ to find the vertex with smallest weight\n       - I understand this part since we scan at most $W$ array slots to find a non-empty list and $W$ is constant.</p>\n\n<p>$O(1)$ for decrease key\n     - This makes sense as far as the actual removing of a link from one list and inserting it into another</p>\n\n<p>My question is about the decrease key step.  Say for example we pull the node $A$ from the 3rd slot, that is the next node to be processed.  We look at $A$'s adjacency matrix and see that it has edges to $B, C, F$.  Since we are using linked lists we have no choice other than to look at all array slots from $4$ to $W$ and see if we can find the vertices listed in the linked list for that particular index. Then delete the vertex from its current list and add it to the correct list if we find it. Possible search time = $O(V)$ [since we store vertices in the linked lists and could search all before finding our desired vertex] not $O(1)$.</p>\n\n<p>Since each decrease key takes $O(V)$ and we process all vertices running time = $O(V^2)$.  </p>\n\n<p>If anyone could let me know where I am going astray in my analysis I would greatly appreciate it.</p>\n", 'ViewCount': '489', 'Title': 'Question about Prims algorithm where weights are between 1 and some constant W', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-14T22:17:34.860', 'LastEditDate': '2013-01-14T22:17:34.860', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'cpowel2', 'PostTypeId': '1', 'OwnerUserId': '5050', 'Tags': '<algorithms><graph-theory><runtime-analysis><spanning-trees>', 'CreationDate': '2012-12-11T18:43:12.797', 'Id': '7345'},64_40:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When in a Parallel algorithm we say:</p>\n\n<blockquote>\n  <p>"This algorithm is done in $O(1)$ time using $O(n\\log n)$ work, with $n$-exponential probability, or alternatively, in $O(\\log n)$ time using $O(n)$ work, with $n$-exponential probability."</p>\n</blockquote>\n\n<p>Then Can we Implement this algorithm for a Quad-Core Computer (and just 4 threads) with $n=100,000$?</p>\n\n<p>The other question is what is the "$n$-exponential probability" in this sentence?</p>\n\n<p>Thanks.</p>\n', 'ViewCount': '152', 'Title': 'A question about parallel algorithm complexity', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-13T14:09:52.017', 'LastEditDate': '2012-12-13T14:09:52.017', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5058', 'Tags': '<algorithm-analysis><runtime-analysis><parallel-computing>', 'CreationDate': '2012-12-13T08:38:02.557', 'FavoriteCount': '0', 'Id': '7371'},64_41:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I saw in <a href="http://www.youtube.com/watch?v=7ayAEHS1-F8" rel="nofollow">this</a> video that computing clustering coefficient of central node  of a star graph using the following algorithm is $\\Theta(n^2)$ and for a clique it is $\\Theta(n^3)$. is that correct?</p>\n\n<pre><code>def clustering_coefficient(G,v):\n    neighbors = G[v].keys()\n    if len(neighbors) == 1: return 0.0\n    links = 0.0\n    for w in neighbors:\n        for u in neighbors:\n            if u in G[w]: links += 0.5\n    return 2.0*links/(len(neighbors)*(len(neighbors)-1))\n</code></pre>\n', 'ViewCount': '255', 'Title': 'Computing the clustering coefficient', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-28T17:39:00.513', 'LastEditDate': '2012-12-28T12:03:27.460', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7626', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5202', 'Tags': '<graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2012-12-28T10:30:04.533', 'Id': '7624'},64_42:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '426', 'Title': 'Simplify complexity of n multichoose k', 'LastEditDate': '2013-01-05T23:36:04.090', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '5271', 'FavoriteCount': '4', 'Body': "<p><strong>Edit:</strong>  In my case, $k$ may be greater than $n$ and they grow independently.</p>\n\n<p>I have a recursive algorithm with time complexity equivalent to choosing k elements from n with repetition, and I was wondering whether I could get a more simplified big-O expression.</p>\n\n<p>Specifically, I'd expect some explicit exponential expression.\nThe best I could find so far is that based on Stirling's approximation $O(n!) \\approx O((n/2)^n)$, so I can use that, but I wondered if I could get anything nicer.</p>\n\n<p>$$O\\left({{n+k-1}\\choose{k}}\\right) = O(?)$$</p>\n", 'Tags': '<asymptotics><combinatorics><runtime-analysis>', 'LastEditorUserId': '5271', 'LastActivityDate': '2013-01-05T23:36:04.090', 'CommentCount': '3', 'AcceptedAnswerId': '7694', 'CreationDate': '2013-01-02T10:10:10.627', 'Id': '7691'},64_43:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need help figuring the potential function for a max heap so that extract max is completed in $O(1)$ amortised time. I should add that I do not have a good understanding of the potential method. </p>\n\n<p>I know that the insert function should "pay" more in order to reduce the cost of the extraction, and this has to be in regards to the height of the heap (if $ \\lfloor \\log(n) \\rfloor $ gives the height of the heap should the insert be $2\\log(n)$ or $ \\sum_{k=1}^n 2\\log(k) $)</p>\n', 'ViewCount': '783', 'Title': 'Potential function binary heap extract max O(1)', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-14T20:34:01.887', 'LastEditDate': '2013-01-14T20:34:01.887', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7901', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5384', 'Tags': '<data-structures><runtime-analysis><heaps><amortized-analysis>', 'CreationDate': '2013-01-11T23:41:04.057', 'Id': '7894'},64_44:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '138', 'Title': 'Example for the analysis of a recursive function', 'LastEditDate': '2013-01-26T18:10:30.180', 'AnswerCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5222', 'FavoriteCount': '1', 'Body': "<pre><code>l is a matrix of size [1...n, 1...n]\n\nfunction: rec(i,j)\n  if (i*j == 0)\n    return 1\n  else\n    if (l[i,j] == 0)\n      l[i,j] = 1 * rec(i-1,j) + 2 * rec(i,j-1) + 3 * rec(i-1,j-1)\n    return l[i,j]\nend_function\n\nfor i=1 to n\n  for j=1 to n\n    l[i,j] = 0\n\nrec(n,n)\n</code></pre>\n\n<p>The nested for's are O(n<sup>2</sup>). But i have difficulties to analyse the recursive part. There is another variation of this example with l as 3d. And the essential part of 3drec function is defined as:</p>\n\n<pre><code>if (l[i,j,k] == 0)\n  l[i,j,k] = 2 * rec(i-1,j,k) + 2 * rec(i,j-1,k) + 2 * rec(i,j,k-1)\n</code></pre>\n\n<p>Anyway let's think about the 2d version again. I thought something like that (that's the running time for the whole code including the nested loops):</p>\n\n<p>T(n) = T(n-1, n<sup>2</sup>) + T(n, n-1<sup>2</sup>) + T(n-1<sup>2</sup>, n-1<sup>2</sup>)</p>\n\n<p>And i'm stuck here. Besides i don't know if i did right till this point.</p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-26T22:54:57.413', 'CommentCount': '4', 'AcceptedAnswerId': '9172', 'CreationDate': '2013-01-26T01:11:04.723', 'Id': '9162'},64_45:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p><strong>Problem:</strong> Consider a graph $G = (V, E)$ on $n$ vertices and $m &gt; n$ edges, $u$ and $v$ are two vertices of $G$.</p>\n  \n  <p>What is the asymptotic complexity to calculate the shortest path from $u$ to $v$ with Dijkstra\'s algorithm using <a href="http://en.wikipedia.org/wiki/Binary_heap" rel="nofollow">Binary Heap</a> ?</p>\n</blockquote>\n\n<p>To clarify, Dijkstra\'s algorithm is run from the source and allowed to terminate when it reaches the target. Knowing that the target is a neighbor of the source, what is the time complexity of the algorithm?</p>\n\n<p><strong>My idea:</strong></p>\n\n<p>Dijkstra\'s algorithm in this case makes $O(n)$ <strong>inserts</strong> ( $n$ if the graph is complete) and 1 <strong>extract min</strong> in the binary heap, before calculate the shortest path from $u$ to $v$.</p>\n\n<p>In a binary heap insert costs $O(\\log n)$ and extract min $O(\\log n)$ too.</p>\n\n<p>So the cost in my opinion is $O(n \\cdot \\log n + \\log n) = O(n \\log n)$</p>\n\n<p>But the answer is $\\Theta(n)$, so there is something wrong in my thinking.</p>\n\n<p>Where is my mistake?</p>\n', 'ViewCount': '793', 'Title': "What's the complexity of calculating the shortest path from $u$ to $v$ with Dijkstra's algorithm using binary heap?", 'LastEditorUserId': '3011', 'LastActivityDate': '2013-01-30T17:46:35.080', 'LastEditDate': '2013-01-30T17:41:32.873', 'AnswerCount': '1', 'CommentCount': '8', 'AcceptedAnswerId': '9319', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4765', 'Tags': '<algorithms><graphs><algorithm-analysis><runtime-analysis><shortest-path>', 'CreationDate': '2013-01-30T16:17:56.463', 'Id': '9314'},64_46:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>An assignment question asks me to find the complexity of a [tail] recursive algorithm, copied below. While I understand all the complexity specifics, for example that the while loop's complexity is $n-1$ and the complexity of setting $j$ to $0$ is 1, I don't understand how I could trace the code recursively, that is within itsel - it's too hard to keep track of. </p>\n\n<p>What I tried doing, is turning the algorithm into an iterative one, by simply putting all the code into a big while loop and thus avoiding the recursive call. But I'm not sure if this affects the complexity of the original algorithm.</p>\n\n<pre><code>Algorithm MyAlgorithm(A, n) \n   Input: Array of integer containing n elements \n   Output: Possibly modified Array A\n     done \u2190 true \n     j \u2190 0\n     while j \u2264 n - 2 do {\n       if A[j] &gt; A[j + 1] then {\n       swap(A[j], A[j + 1])\n       done:= false\n       }\n     j \u2190 j + 1\n     end while\n     j \u2190 n - 1\n     while j \u2265 1 do\n       if A[j] &lt; A[j - 1] then\n       swap(A[j - 1], A[j])\n       done:= false\n    j \u2190 j - 1\n    end while\n    if \xac done\n       MyAlgorithm(A, n)\n    else\n      return A\n</code></pre>\n", 'ViewCount': '278', 'Title': 'Finding the complexity of a recursive method', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-01T10:51:52.300', 'LastEditDate': '2013-02-01T10:51:52.300', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-01-31T16:59:09.883', 'Id': '9360'},64_47:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If they were all linked to make a condition such as ($1 &lt; i &lt; j &lt; k &lt; n$), I know how to solve, but the last loop is disconnected so I have no clue on how to do these...</p>\n\n<p>the ones like</p>\n\n<pre><code>for(i = 1 to n);\n   for(j = i to n);\n      x++;\n</code></pre>\n\n<p>I can use $1 \\leq i \\leq j \\leq n$ and use $x_1 + x_2 + x_3 = n-1$ and find out the general solution which is $\\frac{n(n+1)}{2}$. But disconnected loops I have no idea. Consider the following for loops.</p>\n\n<pre><code>    for(i = 1 to n);\n       for(j = i to n);\n          for(k = 1 to i*n);\n              x++; (constant time)\n\n\n    for(i = 1 to n-1);\n        for(j = i+1 to n);\n            for(k = 1 to j);\n                x++; (constant time)\n</code></pre>\n\n<p>I need to find the general solution.</p>\n', 'ViewCount': '228', 'Title': 'Analyzing programs with multiple for-loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-08T14:56:44.287', 'LastEditDate': '2013-02-04T12:24:30.640', 'AnswerCount': '2', 'CommentCount': '7', 'AcceptedAnswerId': '9606', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6695', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2013-02-04T02:45:11.513', 'Id': '9461'},64_48:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1038', 'Title': 'Worst case analysis of bucket sort using insertion sort for the buckets', 'LastEditDate': '2013-02-18T22:03:28.823', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6815', 'FavoriteCount': '1', 'Body': '<p>Suppose I am using the <a href="http://en.wikipedia.org/wiki/Bucket_sort#Pseudocode" rel="nofollow">Bucket-Sort</a> algorithm, and on each bucket/list I sort with insertion sort (replace nextSort with insertion sort in the wikipedia pseudocode).</p>\n\n<p>In the worst case, this would imply that we would have $O(n^2)$ performance, because if every element was in one bucket, then we would have to use insertion sort on $n$ elements which is $O(n^2)$. </p>\n\n<p>So the first thing that comes to mind to fix the worst case running time is to not use insertion-sort, because it is $O(n^2)$. Instead we could use merge-sort or heap-sort m, because the worst case running time for both of those algorithms is $O(n\\log n)$. However, if we use merge-sort and heap-sort, do they preserve the expected linear running-time of bucket-sort?</p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-02-18T22:03:28.823', 'CommentCount': '1', 'AcceptedAnswerId': '9882', 'CreationDate': '2013-02-18T01:16:55.900', 'Id': '9876'},64_49:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to analyze the running time of a bad implementation of generating the $n$th member of the fibonacci sequence (which requires generating the previous 2 values from the bottom up).</p>\n\n<p>Why does this algorithm have a time complexity of $\\Omega(2^{\\frac{n}{2}})$? Where does the exponent come from?</p>\n', 'ViewCount': '852', 'Title': 'Why does a recurrence of $T(n - 1) + T(n - 2)$ yield something in $\\Omega(2^{\\frac{n}{2}})$?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-19T23:48:57.897', 'LastEditDate': '2013-02-19T06:22:05.190', 'AnswerCount': '4', 'CommentCount': '5', 'AcceptedAnswerId': '9908', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2860', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis><recurrence-relation>', 'CreationDate': '2013-02-18T19:02:47.990', 'Id': '9899'},64_50:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The time complexity of merge (union) operation is said to be $O(\\lg (n_1 + n_2))$, where $n_1$ and $n_2$ are the numbers of elements in the merged heaps, respectively. I do not understand this - the algorithm has to go through all the elements of both rightmost paths of the original heaps - lengths of these paths are bound by $O(\\lg n_1)$ and $O(\\lg n_2)$. That makes $O(\\lg n_1 + \\lg n_2)$ in total, which is $O(\\lg (n_1 n_2))$. Where am I making a mistake in my assumptions?</p>\n\n<p>Arbitrary delete operation - the complexity should be $O(\\lg n)$, where $n$ is the size of the heap. But after the deletion, the algorithm has to go through all the nodes from the parent of the deleted node to the root and correct the Leftist property, and the lenght of this path is bound by $O(n)$. Again, where am I wrong?</p>\n', 'ViewCount': '116', 'Title': 'Leftist heap - determining time complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:22:09.107', 'LastEditDate': '2013-02-24T16:22:09.107', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7009', 'Tags': '<data-structures><runtime-analysis><heaps>', 'CreationDate': '2013-02-24T15:50:20.213', 'Id': '10056'},64_51:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have an algorithm which, basically given an array of $n$ numbers, checks if there is any repeated numbers in the array, and returns true if there is and false otherwise.</p>\n\n<p>It uses a direct access table (hashing function $h(x)=x$), which makes the running time linear. So it creates a new array, initializes all values to false, then iterates through the given array, and since each value is an index to the hash table, it accesses that array location and changes it to $1$. If it is already a $1$, then you know that it is a repeated number.</p>\n\n<p>But when getting the expected running time, you need to first define a probability space. This is the part I am confused about. How can you define a probability of a number being repeated in A, if you are just given an array with arbitrary numbers? </p>\n', 'ViewCount': '93', 'Title': 'How to get the expected running time of an algorithm', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-03-06T17:51:22.880', 'LastEditDate': '2013-03-06T17:51:22.880', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-03-06T15:35:43.183', 'FavoriteCount': '1', 'Id': '10320'},64_52:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I can think of functions such as $n^2 \\sin^2 n$ that don't have asymptotically tight bounds,  but are there actually common algorithms in computer science that don't have asymptotically tight bounds on their worst case running times?</p>\n", 'ViewCount': '205', 'Title': 'Common Algorithms without Asymptotically Tight Bounds', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-07T11:27:03.257', 'LastEditDate': '2013-03-07T11:27:03.257', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '10355', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<time-complexity><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-07T08:39:31.110', 'Id': '10354'},64_53:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've learned that a while loop such as </p>\n\n<pre><code>int i = 100;\nwhile (i &gt;= 1){\n     ...\n     ///Stuff\n     i = i/2\n}\n</code></pre>\n\n<p>will run in logarithmic time, specifically, <code>O(logn)</code>, since it keeps dividing in half each time (like a binary search).</p>\n\n<p>However, what if my while loop looks like this</p>\n\n<pre><code> int i = 100;\n    while (i &gt;= 1){\n         ...\n         ///Stuff\n         i = i/3\n    }\n</code></pre>\n\n<p>Is the complexity still <code>O(logn)</code>?</p>\n\n<p>Can someone explain yes/no and why?</p>\n", 'ViewCount': '1248', 'Title': 'Complexity of a while loop that divides by parameter by three each iteration', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-08T20:48:01.123', 'LastEditDate': '2013-03-08T20:48:01.123', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '-3', 'OwnerDisplayName': 'Imray', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-08T19:15:47.000', 'Id': '10389'},64_54:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to determine the complexity of this for loop:</p>\n\n<pre><code>for (int j =3; j &lt;= n-2; j+=2) {\n   ....\n}\n</code></pre>\n\n<p>By trying out lots of examples, I came up with $\\frac{n-4}{2} + 1$. This seems to work with every number now.</p>\n\n<p>However, I am looking for a systematic and quick way to find these sorts of complexities. For example, can you show me the proper way to find the complexity for the above loop?</p>\n", 'ViewCount': '135', 'Title': 'Complexity of slightly tricky for loop', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-10T17:54:06.147', 'LastEditDate': '2013-03-08T20:48:09.873', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-03-08T20:07:39.920', 'Id': '10391'},64_55:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '157', 'Title': 'How to get expected running time of hash table?', 'LastEditDate': '2013-03-13T08:22:56.710', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'omega', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much. Am I making a mistake here?</p>\n", 'ClosedDate': '2013-03-13T08:23:35.603', 'Tags': '<data-structures><runtime-analysis><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-13T08:22:56.710', 'CommentCount': '1', 'CreationDate': '2013-03-12T23:21:20.177', 'Id': '10501'},64_56:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to determine the worst case runtime of this program:</p>\n\n<pre><code>while n &gt; 1\n  for i = 1,..,n\n    m = log(n)\n  n = n/2\n</code></pre>\n\n<p>Obviously the outer loop runs <code>log(n)</code> times, because n is halfed after each step.</p>\n\n<p>But the inner for loop is dependent on the decreasing n and I am not quite sure how to deal with that.</p>\n\n<p>A simple bound for it would be <code>n</code>, so the whole thing runs in at most <code>log(n)*n</code>, but I am sure the inner loop is faster than that.</p>\n\n<p>If I look at each step of the outer loop, I see that the inner loop runs <code>n,n/2,n/4,..,2</code> times. So I would say all together the inner loop runs at most <code>2n</code> times which is in <code>O(n)</code>.</p>\n\n<p>Can I just deduct from that fact that the overall runtime is <code>O(log(n)+n) = O(n)</code>?</p>\n\n<p>I am unsure because I am not multiplying anything, which I feel like I should when dealing with multiple loops.</p>\n', 'ViewCount': '161', 'ClosedDate': '2013-03-27T11:54:26.093', 'Title': 'Decreasing runs of inner loop in outer loop', 'LastActivityDate': '2013-03-26T19:40:41.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '594', 'Tags': '<time-complexity><asymptotics><runtime-analysis>', 'CreationDate': '2013-03-26T19:04:39.817', 'Id': '10813'},64_57:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a> has a good cheat sheet, but however it does not involve no. of comparisons or swaps. (though no. of swaps is usually decides its complexity). So I created the following. Is the following info is correct ? Please let me know if there is any error, I will correct it.</p>\n\n<p><strong>Insertion Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case : $\\Theta(n^2)$ ; happens when input is\nalready sorted in descending order</li>\n<li>Best Case : $\\Theta(n)$ ; when input is already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(n)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in Best case</li>\n</ul>\n\n<p><strong>Selection Sort:</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best Case: $\\Theta(n^2)$ </li>\n<li>No. of comparisons : $\\Theta(n^2)$</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst/average case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Merge Sort :</strong></p>\n\n<ul>\n<li>Average Case / Worst Case / Best case : $\\Theta(nlgn)$ ; doesn\'t matter at all whether the input is sorted or not</li>\n<li>No. of comparisons : $\\Theta(n+m)$ in worst case &amp; $\\Theta(n)$ in best case ; assuming we are merging two array of size n &amp; m where $n&lt;m$</li>\n<li>No. of swaps : No swaps ! [but requires extra memory, not in-place sort]</li>\n</ul>\n\n<p><strong>Quick Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$ ; happens input is already sorted</li>\n<li>Best Case : $\\Theta(nlogn)$ ; when pivot divides array in exactly half</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; $\\Theta(nlogn)$ in best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Bubble Sort:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n^2)$</li>\n<li>Best Case : $\\Theta(n)$ ; on already sorted</li>\n<li>No. of comparisons : $\\Theta(n^2)$ in worst case &amp; best case</li>\n<li>No. of swaps : $\\Theta(n^2)$ in worst case &amp; $0$ in best case</li>\n</ul>\n\n<p><strong>Linear Search:</strong></p>\n\n<ul>\n<li>Worst Case : $\\Theta(n)$ ; search key not present or last element</li>\n<li>Best Case : $\\Theta(1)$ ; first element</li>\n<li>No. of comparisons : $\\Theta(n)$ in worst case &amp; $1$ in best case</li>\n</ul>\n\n<p><strong>Binary Search:</strong></p>\n\n<ul>\n<li>Worst case/Average case : $\\Theta(logn)$</li>\n<li>Best Case : $\\Theta(1)$ ; when key is middle element</li>\n<li>No. of comparisons : $\\Theta(logn)$ in worst/average case &amp; $1$ in best case</li>\n</ul>\n\n<hr>\n\n<ol>\n<li>I have considered only basic searching &amp; sorting algorithms. </li>\n<li>It is assumed above that sorting algorithms produce output in ascending order</li>\n<li>Sources : The awesome <a href="http://rads.stackoverflow.com/amzn/click/0262033844" rel="nofollow">CLRS</a> and this <a href="http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms" rel="nofollow">Wiki</a></li>\n</ol>\n', 'ViewCount': '9952', 'ClosedDate': '2014-02-09T15:34:28.957', 'Title': 'Complexities of basic operations of searching and sorting algorithms', 'LastActivityDate': '2014-02-09T07:07:24.577', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><sorting><searching>', 'CreationDate': '2013-04-03T13:09:39.333', 'FavoriteCount': '1', 'Id': '10991'},64_58:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'ve been reading about hypercube connection template for parallel algorithms. The general scheme is explained in <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node124.html#SECTION04310000000000000000" rel="nofollow"><em>Designing and Building Parallel Programs</em> by Ian Foster</a> and it\'s pretty clear.</p>\n\n<p>What I don\'t understand is how it\'s applied on the merge sort <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node127.html" rel="nofollow">in \xa711.4</a> The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<p>The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<pre><code>procedure parallel_mergesort(myid, d, data, newdata)\nbegin\n  data = sequential_mergesort(data)\n  for dim = 1 to d\n    data = parallel_merge(myid, dim, data)\n  endfor\n  newdata = data\nend\n</code></pre>\n\n<p>Please, explain to me step by step, assuming we have an array of twelve elements $(3,1,5,7,4,2,8,9,4,2,7,5)$ and we\'ve broken this data to four processors like this: </p>\n\n<p>$\\qquad ((3,1,5),(7,4,2),(8,9,4),(2,7,5))$. </p>\n\n<p>What data will have each process after each iteration? I understand why we use the hybercube template in this algorithm, but why do we have exactly $i$ compare-exchanges at the $i$-th level? I mean, when $i=1$, we compare-exchange data from processes $1-2, 3-4, .. P-1, P$. That\'s not $1$, that\'s $P/2$? Do I misunderstand something?</p>\n', 'ViewCount': '282', 'Title': 'Parallel merge sort using hypercube connection template', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T20:43:03.117', 'LastEditDate': '2013-04-10T20:43:03.117', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'OwnerDisplayName': u'\u0418\u0433\u043e\u0440\u044c \u041c\u043e\u0440\u043e\u0437\u043e\u0432', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><parallel-computing><machine-models>', 'CreationDate': '2013-04-10T18:14:08.597', 'Id': '11205'},64_59:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In one of the text book its mentioned that 'running time of this algorithm is 200 computer years'. Can somebody please explain what is the meaning of a computer year?</p>\n", 'ViewCount': '488', 'Title': 'What is a computer year?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-11T12:51:40.690', 'LastEditDate': '2013-04-11T12:51:40.690', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6522', 'Tags': '<algorithms><terminology><runtime-analysis>', 'CreationDate': '2013-04-11T10:17:15.833', 'FavoriteCount': '1', 'Id': '11220'},64_60:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A program takes as input a balanced binary search tree with $n$ leaf nodes and computes the value of a function $g(x)$ for each node $x$. If the cost of computing $g(x)$ is </p>\n\n<p>$\\qquad \\min(\\#\\text{leaves in } L(x), \\#\\text{leaves in } R(x))$</p>\n\n<p>for $L(x), R(x)$ the left resp. right subtree of $x$, then the worst-case time complexity of the program is</p>\n\n<ol>\n<li>$\\Theta(n)$</li>\n<li>$\\Theta(n \\log n)$</li>\n<li>$\\Theta(n^2)$ </li>\n<li>$\\Theta(n^2 \\log n)$</li>\n</ol>\n\n<p>I am actually looking for a subtle hint. </p>\n', 'ViewCount': '492', 'Title': 'Finding no. of leaf nodes for each node in a BST', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-22T14:55:46.203', 'LastEditDate': '2013-04-12T10:03:59.117', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '11254', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><binary-trees><search-trees>', 'CreationDate': '2013-04-12T08:03:52.183', 'Id': '11252'},64_61:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In database query processing, the approximate time for selection operation using primary index when equality is on key is $2(b_s + b_t)$ where $b_s$ is disk seek time and $b_t$ is disk transfer time (assuming one level of indexing), because one seek and transfer time will be needed for finding the index and another one will be for the actual data.  </p>\n\n<p>But what will happen if the equality is on a no- key value? Since now we cannot search in the index, don't we have to do a linear search?</p>\n", 'ViewCount': '52', 'Title': 'Approximate time for selection operation using index when equality is on nonkey', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-15T15:11:13.230', 'LastEditDate': '2013-04-14T11:22:36.033', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11326', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '778', 'Tags': '<runtime-analysis><search-algorithms><databases>', 'CreationDate': '2013-04-14T06:08:19.807', 'Id': '11301'},64_62:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '77', 'Title': 'Resolving this recurrence equation', 'LastEditDate': '2013-04-29T12:38:07.347', 'AnswerCount': '1', 'Score': '1', 'OwnerDisplayName': 'Federico Ponzi', 'PostTypeId': '1', 'OwnerUserId': '17193', 'Body': "<p>I have this recurrence equation:</p>\n\n<p>$T(n) = T(n/4) + T(3n/4) + \\mathcal{O}(n)$</p>\n\n<p>$T(1) = 1$</p>\n\n<p>I know that the result is $\\mathcal{O}(n \\log n)$ but i don't know how to proceed.</p>\n", 'ClosedDate': '2013-04-29T18:28:35.180', 'Tags': '<asymptotics><runtime-analysis><recurrence-relation><recursion>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-04-29T12:38:07.347', 'CommentCount': '6', 'AcceptedAnswerId': '11657', 'CreationDate': '2013-04-14T19:02:46.790', 'Id': '11656'},64_63:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Runtime for many programming languages is typically analyzed either assuming each operation takes a constant amount of time, or assuming each operation takes a logarithmic amount of time in the size the the data being manipulated. From a practical perspective, this is reasonable since modern computers have essentially constant time memory access (albeit for a fixed amount of memory).</p>\n\n<p>However, from a theoretical perspective, it seems to me that this is a bad assumption.</p>\n\n<p>Since bits can't be stored denser than the Planck scale (which we are fast approaching), the number of bits we can store in a certain volume of space can grow at most linearly as volume grows. Further, since the speed of memory retrieval and storage is bounded by the speed of light, it makes a lot of sense to say that as the size of the data we store grows, the cost for storing and accessing it should grow. For instance, one could assume that storing and accessing the $n$th bit of data requires $\\sqrt[3]{n}$ time. (Since space is 3-dimensional).</p>\n\n<p>Presumably this has been discussed in the literature somewhere. Could anyone point me to some references? Have algorithms been analyzed using this model?</p>\n", 'ViewCount': '93', 'Title': "Why don't we scale the cost of memory access when analyzing runtime of algorithms?", 'LastActivityDate': '2013-05-06T10:20:24.770', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8065', 'Tags': '<reference-request><time-complexity><asymptotics><runtime-analysis>', 'CreationDate': '2013-05-06T05:14:51.447', 'FavoriteCount': '1', 'Id': '11818'},64_64:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am struggling to calculate the lower bounds of an algorithm. What is the right way to proceed.</p>\n\n<p>For eg, I have the following algorithm</p>\n\n<pre><code>For i=1, 2,...,n\n    For j=i+1, i+2,...,n\n        Add up array entries A[i] through A[j]\n    Store the result in B[i, j] Endfor\nEndfor\n</code></pre>\n\n<p>How do I calculate the lower bounds of this algorithm</p>\n', 'ViewCount': '56', 'Title': 'Finding the lower bounds of an algorithm', 'LastActivityDate': '2013-05-08T00:11:49.063', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8086', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-05-07T22:57:21.287', 'Id': '11868'},64_65:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've ran some tests and found that Shellsort runs much faster on ordered and reversed lists compared to random lists and almost ordered lists.</p>\n\n<pre><code>Results:\n        Random Reverse Order AlmostOrder\n  time    24      5      4        29\n</code></pre>\n\n<p>The problem that is confusing me is that Shellsort performs insertion sorts on lists separated by gaps, and insertion sort only runs very fast on ordered lists, not reversed lists.</p>\n\n<p>So my question is why does Shellsort work well on ordered and reversed lists when it uses insertion sort and insertion sort doesn't work well on reversed lists?</p>\n", 'ViewCount': '160', 'Title': 'Why does Shellsort work well on Sorted and Reverse ordered lists?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-19T14:52:46.560', 'LastEditDate': '2013-05-19T14:52:46.560', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'clay', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-05-18T02:16:41.193', 'FavoriteCount': '0', 'Id': '12124'},64_66:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>First I apologize if the title is unclear, but I didn\'t find anything better.</p>\n\n<p>I\'m solving a differential equation that has two parameters , here denoted by points of a plane.These parameters are real numbers. For some points of the plane (or, equivalently, for some parameters of the differential equation) , the solution of the equation satisfies some condition and we denote such points of the plane with 1. This points make a simply-connected region in the plane, and I know that this region lies somewhere in $0&lt;x&lt;1.6, -.7&lt;y&lt;.3$ rectangle. My goal is to find this region.Remember that this region is on the real plane.($\\mathbf R^2$)</p>\n\n<p><strong>My main question is this:</strong> I already know these two things about the wanted region:</p>\n\n<ol>\n<li><p>The set of points on the plane that satisfy the conditions (the wanted region) , form a simply-connected region</p></li>\n<li><p>This region lies somewhere in $0&lt;x&lt;1.6, -.7&lt;y&lt;.3$ rectangle</p></li>\n</ol>\n\n<p>for example it may be something like this:</p>\n\n<p><img src="http://i.stack.imgur.com/Q8lvC.png" alt="enter image description here"></p>\n\n<p>I want to use this two facts to find the region with less computations; i.e. instead of checking the condition on all points on the rectangle, actually , on a very high-resolution grid (this the first approach below), use an algorithm (below : Variant 2) that more quickly converges to <strong>the boundary</strong> (and so determines the region without inspecting all points).  </p>\n\n<p>(If you know a better approach ,I\'ll be happy to hear) </p>\n\n<h3>Variant 1 (the naive approach, noted above)</h3>\n\n<p>Divide each axis to identical steps (of length $\\Delta$ ) and check the condition on each node to find the region.$\\Delta$ must be as small as possible to find the region with an acceptable accuracy ($\\Delta0.001$ suffices for my purpose) . (in the picture : nodes = intersections). This method needs a huge number of check operations , but can be used to find all kinds of regions; I mean if I didn\'t know that the wanted region is connected or it had sharp edges, etc. ,this method was the only way.</p>\n\n<p><img src="http://i.stack.imgur.com/P9RoC.png" alt="http://i.stack.imgur.com/MvhRH.png"></p>\n\n<h3>Variant 2</h3>\n\n<p>(It may be a famous method, but I haven\'t seen it before) </p>\n\n<p>Because the region will be simply connected, it suffices to find its boundary .We use a <strong>recursive</strong> approach. We start from a grid (like the first step, but with much larger distance between nodes, say, $100\\Delta$) and check the condition on this grid.For the next step, I assume the interval between two adjacent 1s is 1 everywhere and between two adjacent 0s is 0 everywhere. <em>If two adjacent nodes gave different results (1 on one them and 0 on the other) , I put a point between them and check the condition on that point (red points in the picture) check this point. If it was 1, I put a new point between this point and the adjacent 0 and check that point; and if it was 0, I put a new point between this zero and adjacent 1 and check that point</em>. I continue till I arrive at a distance of $\\Delta$ between points.So I\'ve found the boundary. (This method is like bisection method  for finding the roots of a function)</p>\n\n<p>In the picture, the first iteration is shown.Black and yellow points are the points of the initial grid (that are distributed on the whole rectangle) and red points are those that are added after checking the initial grid nodes. Black points are points that are determined to satisfy the condition (are 1) and so are certainly inside the region. Yellow points are those that did not satisfy the condition and so are outside, and red points are those added in the 2nd iteration , between adjacent nodes with different results (between a 1 and a 0) ,according to the  above paragraph.</p>\n\n<p><img src="http://i.stack.imgur.com/CMgqT.png" alt="http://i.stack.imgur.com/ZqtFN.png"></p>\n\n<p>So , using this method I\'ve found the region with the same accuracy as in the first method, and saved a lot of time too.</p>\n\n<p>I want to know <em>how much this method is faster.</em> A qualitative answer that shows if it is better to implement this method , suffices. My problem is so computational intensive that I can\'t use the first approach.</p>\n', 'ViewCount': '124', 'Title': 'Complexity of an algorithm for bounding a region in 2D', 'LastEditorUserId': '8381', 'LastActivityDate': '2013-05-28T18:01:59.797', 'LastEditDate': '2013-05-28T18:01:59.797', 'AnswerCount': '2', 'CommentCount': '13', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8381', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><computational-geometry>', 'CreationDate': '2013-05-27T10:46:57.837', 'Id': '12305'},64_67:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '132', 'Title': 'Why don\'t we emphasize "length of input string" when considering time complexity of sorting algorithms?', 'LastEditDate': '2013-06-26T13:42:50.850', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8809', 'FavoriteCount': '1', 'Body': "<p>The knapsack problem is $O(c\\,n)$ where $c$ is the capacity of knapsack and $n$ is the number of items. Yet it's exponential because the size of the input is $\\log(c)$.</p>\n\n<p>However, why don't we emphasize length of input in other algorithms? To name one example, what would be the <strong>input size $n$</strong> and <strong>worst case time complexity $T$</strong> of the following input when using insertion sort:</p>\n\n<p><code>111111111,101,11,10,1,0</code></p>\n\n<p>Answer A: $n=6$, $T=O(n^2)$<br>\nAnswer B: n = space(all_digits)+space(delimiters_between_numerics)</p>\n\n<p>If B is correct, what is the time complexity $T$?</p>\n", 'Tags': '<algorithms><terminology><time-complexity><runtime-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:42:50.850', 'CommentCount': '1', 'AcceptedAnswerId': '12827', 'CreationDate': '2013-06-22T07:43:42.833', 'Id': '12825'},64_68:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am a bit confused about calculating complexities.</p>\n\n<p>Above is a C++ program converting a char array into an int, incrementing the value, parsing it back to char array.</p>\n\n<pre><code>#include &lt;iostream&gt;\n\nint main() {\n    char number[] = {'4', '3', '1'};    \n    int num = 0;\n    //char to int conversion\n    for (int i = 0; i &lt; (int)sizeof(number); i++) {\n        num += number[i] - '0';\n        num*=10;\n    }\n    num/=10;\n\n    //incrementation\n    num++;\n\n    //int to char conversion\n    for (int i = (int)sizeof(number) -1; i &gt;= 0; i--) {\n        number[i] = '0' + num % 10;\n        num/=10;\n    }\n\n    //printing the result\n    std::cout &lt;&lt; number &lt;&lt; endl;\n    return 0;\n}\n</code></pre>\n\n<p>Now let's say array size(3) is n. In that case I would say that the complexity is O(n+n) which is O(2n). However I've heard that O(2n) is actually O(n) for some reason but I could not find any actual source about it. What is the time complexity of this program?</p>\n", 'ViewCount': '72', 'Title': 'Complexity of a particular algorithm', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-25T08:26:42.210', 'LastEditDate': '2013-06-25T08:26:42.210', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-06-24T18:53:48.697', 'Id': '12873'},64_69:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The main question is, how exactly is the big O analysis calculated on routines? Is there a specific formula that relates what each function in a program does to a big O calculation?</p>\n\n<p>Also, what about more complex iterations, such as colour conversions etc?</p>\n\n<p>I would like to point out that this is not a homework question, rather, it is a question from my own research/programming learning curve. I have code that I am working on, but would like to know how this analysis is carried out.</p>\n', 'ViewCount': '251', 'Title': "Analysis of algorithms, 'big O' question", 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-17T15:40:51.770', 'LastEditDate': '2013-07-17T10:26:48.837', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '12901', 'Score': '3', 'OwnerDisplayName': 'user8872', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><proof-techniques><runtime-analysis>', 'CreationDate': '2013-06-25T19:08:19.800', 'Id': '12899'},64_70:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have two for loops, one nested within the other. I have two int variables n and U, where <code>U &gt; n</code>. I know that the outer loop runs exactly U times (pretty much <code>for(int i = 0; i &lt; U -1; i++)</code>), and the inner loop runs exactly n times in total <strong>for the duration of the entire execution</strong>. It can run n times once and never run again, or it can run once in each iteration of the outer loop n times, or anything in between.</p>\n\n<p>It's obvious that this runs in O(nU) time. However, I think I can also state it runs in <code>theta(U + n)</code> time because the inner loop runs independently of the outer one. But still, they are nested, and not multiplying the runtimes of nested loops feels awkward. </p>\n\n<p>Intuitively I know I am right, but I don't really know how to show it. I'm not looking for a formal proof, but any kind of insight is welcome.   </p>\n", 'ViewCount': '137', 'Title': 'Theta Runtime of Nested for Loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-31T08:16:53.290', 'LastEditDate': '2013-07-31T08:16:53.290', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8943', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2013-06-30T17:08:51.800', 'Id': '12995'},64_71:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="http://en.wikipedia.org/wiki/Selection_sort#Comparison_to_other_sorting_algorithms">It is written on Wikipedia</a> that "... selection sort almost always outperforms bubble sort and gnome sort." Can anybody please explain to me why is selection sort considered faster than bubble sort even though both of them have:  </p>\n\n<ol>\n<li><p><strong>Worst case time complexity</strong>: $\\mathcal O(n^2)$  </p></li>\n<li><p><strong>Number of comparisons</strong>:      $\\mathcal O(n^2)$</p></li>\n<li><p><strong>Best case time complexity</strong> :  </p>\n\n<ul>\n<li>Bubble sort: $\\mathcal O(n)$</li>\n<li>Selection sort: $\\mathcal O(n^2)$</li>\n</ul></li>\n<li><p><strong>Average case time complexity</strong> :  </p>\n\n<ul>\n<li>Bubble sort: $\\mathcal O(n^2)$</li>\n<li>Selection sort: $\\mathcal O(n^2)$ </li>\n</ul></li>\n</ol>\n', 'ViewCount': '5657', 'Title': 'Why is selection sort faster than bubble sort?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-09T19:48:10.230', 'LastEditDate': '2013-07-06T14:23:05.397', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8951', 'Tags': '<algorithms><runtime-analysis><efficiency><sorting>', 'CreationDate': '2013-07-06T09:33:35.463', 'Id': '13106'},64_72:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Searching for a number in an array is said to have a runtime of O(n) because there may be cases where the number doesn't exist in the array. In such cases, you'd have to have gone through the entire array, which is O(n).</p>\n\n<p>But how about in the case where we know the number definately exists in the array? Does the runtime change then?</p>\n\n<p>Also is there a way to find out the average number of searches it would have to do before a number is found in an array based on its size?</p>\n", 'ViewCount': '290', 'Title': 'Runtime of searching for a number in an array?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-18T07:19:24.290', 'LastEditDate': '2013-07-12T14:49:49.223', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'garbage collection', 'PostTypeId': '1', 'OwnerUserId': '9155', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><arrays>', 'CreationDate': '2013-07-12T04:44:49.213', 'Id': '13244'},64_73:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>When calculating runtime dependence on the input, what calculations are considered? For instance, I think I learned that array indexing as well as assignment statements don't get counted, why is that?</p>\n", 'ViewCount': '86', 'Title': 'What constitutes one unit of time in runtime analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-13T22:31:00.860', 'LastEditDate': '2013-07-13T09:43:51.293', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8722', 'Tags': '<terminology><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-07-13T04:30:59.783', 'Id': '13254'},64_74:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Recently I have had this question in one of my interviews.</p>\n\n<p>You have 1 Million sorted integer, you have a value of $x$, compare each pair in this array and if the addition of two pair is less or equal to $x$ then increment the paircounter.</p>\n\n<p>I have implemented this solution in C++ however I will write pseudocode here(I am sorry I am not very good at pseudo-code)</p>\n\n<pre><code>Initialise ARR_SIZE to 1000000\n\nInitialise index_i to 0\n\nInitialise pairCount to 0\n\nInitialise x to 54321\n\nwhile index_i is less than ARR_SIZE\n\n  Initialise index_j to index_i+1\n\n    while index_j is less than ARR_SIZE\n\n      if array[index_i]+array[index_j] is less or equal to x\n\n        Increment pairCount\n\n      increment index_j\n\n    endof while\n\n    increment index_x\n\nendof while\n</code></pre>\n\n<p>At first I said it is $O(n \\log n)$ but then with the hint the second loop itself average complexity is O(n/2) so overall I said it would be $O(n\\cdot n/2)$ but in Big $O$ notation it would be $O(n)$ because $n/2$ is a constant(although I was not too sure). So what is the average complexity of this overall algorithm?</p>\n\n<p>PS: I know that I could have decreased the complexity by adding an extra else, index_j = ARR_SIZE, which would be $O(N)$ complexity, but I couldn't think of it during the interview.</p>\n", 'ViewCount': '818', 'Title': 'Average Time Complexity of Two Nested Loops', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-20T18:26:01.993', 'LastEditDate': '2013-07-16T16:38:41.020', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '13303', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8849', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-07-16T14:06:15.760', 'Id': '13302'},64_75:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Let there is a binary-string, $B$, of length $N$. The probability of occurrence of 0 and 1 in this binary-word is $p$  and $q$ , respectively. Each bit in the string is independent of any other bit.</p>\n\n<p>There is an algorithm (divide and conquer) which finds the location of 1\u2019s in a given binary-string, in Q # of steps (cost).</p>\n\n<p>I am looking for some close-form solution of the expected # of steps,$E[Q]$, with given probabilities $p$ and $q$ for a string of length $N$.</p>\n\n<p>For instance, for $N=4$ the cost ,${{Q}_{i}}$,  for each possible word is:<br>\n[\\begin{matrix}\n   {{B}_{i}} &amp; {{Q}_{i}} &amp; {{P}_{i}}  \\\\\n   0000 &amp; 1 &amp; {{p}^{4}}  \\\\\n   0001 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0010 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0011 &amp; 5 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0100 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   0101 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0110 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   0111 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1000 &amp; 5 &amp; {{p}^{3}}q  \\\\\n   1001 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1010 &amp; 7 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1011 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1100 &amp; 5 &amp; {{p}^{2}}{{q}^{2}}  \\\\\n   1101 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1110 &amp; 7 &amp; p{{q}^{3}}  \\\\\n   1111 &amp; 7 &amp; {{q}^{4}}  \\\\\n\\end{matrix}]\nFrom the discrete probability theory, we can evaluate the expected cost of the above case ,$N=4$,as follow $\\begin{align}\n  &amp; \\therefore E[Q]=\\sum\\limits_{i=0}^{{{2}^{N}}-1}{{{Q}_{i}}({{p}^{N-i}}}{{q}^{i}}) \\\\ \n &amp; \\Rightarrow E[Q]=\\sum\\limits_{i=0}^{15}{{{Q}_{i}}({{p}^{N-i}}}{{q}^{i}}) \\\\ \n &amp; \\Rightarrow E[Q]={{p}^{4}}+4\\times 5\\times {{p}^{3}}q+2\\times 5\\times {{p}^{2}}{{q}^{2}}+4\\times 7\\times {{p}^{2}}{{q}^{2}}+4\\times 7\\times p{{q}^{3}}+1\\times 7\\times {{q}^{4}} \\\\ \n &amp; \\Rightarrow E[Q]={{p}^{4}}+20{{p}^{3}}q+10{{p}^{2}}{{q}^{2}}+28{{p}^{2}}{{q}^{2}}+28p{{q}^{3}}+7{{q}^{4}} \\\\ \n\\end{align}$ </p>\n\n<p>However, for very large values of N, say 1024, it is computationally not possible to evaluate the cost of each possible binary string (i-e ${{2}^{1024}}=\\text{1}\\text{.79}\\times \\text{1}{{\\text{0}}^{308}}$ binary words). So, this is the problem I am stuck in. </p>\n\n<p>Is it possible to deduce some analytical/ close-form expression for evaluating the expected value of the cost of this algorithm for a given length N and probabilities p and q (instead of brute-force method defined above)?</p>\n\n<p>I will be very thankful, if someone could help in this regard.</p>\n\n<p>ADDITIONAL INFORMATION:</p>\n\n<p>Divide &amp; Conquer Algorithm:\n    Lets take 0000 0001, for instance:</p>\n\n<ol>\n<li><p>First question: Is it (i-e 0000 0001) equal to ZERO (i-e 0000 0000) ? The answer is No, for our case. </p></li>\n<li><p>Then divide the original 8 bit word into two 4 bit segments, and again ask the same question for each of the two 4 bit words. So for our case it would be YES for the first segment (0000) and NO for the other segment (0001)?</p></li>\n<li><p>Now, this time I will be questioning only the segment where I got NO. In our case it was 0001. Then, I will now again divide this 4-bit segment into two segments and pose the same question. Hence, is 00 equal to ZERO? answer : YES. For the other segment , 01, the answer is NO.</p></li>\n<li><p>This is the final step. I will again divide the 2-bit word into two 1-bits, i-e 0 and 1. So, my first question: is 0 equal to 0? answer is YES. And for the other remaining bit, is 1 equal to 0? Answer is NO.</p></li>\n</ol>\n\n<p>So, I asked a total of 7 questions to find the location of 1 in a binary word of 0000 0001. Similarly, we will go through other binary words.</p>\n\n<p>Efficient Way for evaluating the cost of Above algorithm:\n(courtesy to Yuval Filmus)</p>\n\n<p>Here is how to calculate the cost of the algorithm. Start with the bit vector $x$, and consider the following operation $O$, which divides $x$ into pairs of bits and computes their ORs. Thus $|O(x)| = |x|/2$. Compute a sequence $O(x),O(O(x)),O(O(O(x))),\\ldots$ until you get a vector of width $1$. Count the total number of 1s in the sequence. If you got $N$, then the cost is $2N+1$.</p>\n\n<p>For example, suppose $x=0101$. Then the sequence is\n$$ 11,1, $$\nand so $N = 3$ and the cost is $7$.</p>\n', 'ViewCount': '155', 'Title': 'Closed-form Expression of the Expected value of the Cost of D&C Algorithm?', 'LastEditorUserId': '9000', 'LastActivityDate': '2013-08-02T16:50:37.103', 'LastEditDate': '2013-08-02T16:50:37.103', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '13377', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9000', 'Tags': '<algorithm-analysis><runtime-analysis><average-case>', 'CreationDate': '2013-07-21T19:03:42.417', 'Id': '13376'},64_76:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>The paragraph on V-opt heuristic TSP algorithms at <a href="http://en.wikipedia.org/wiki/Travelling_salesman_problem#Heuristic_and_approximation_algorithms" rel="nofollow">this site</a> mentions a very effective algorithm due to Lin-Kernigham-Johnson. That page says: </p>\n\n<blockquote>\n  <p>For many years Lin\u2013Kernighan\u2013Johnson had identified optimal solutions for all TSPs where an optimal solution was known and had identified the best known solutions for all other TSPs on which the method had been tried.</p>\n</blockquote>\n\n<p>Impressive, so what is the complexity of that algorithm? Does the algorithm often work faster than predicted  based on theoretical complexity (if yes, how much)? Is that algorithm used most often in software that solves the TSP?</p>\n', 'ViewCount': '138', 'Title': 'Complexity of the V-opt heuristic Traveling Salesman algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-23T22:21:29.497', 'LastEditDate': '2013-07-22T14:15:56.187', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13404', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6641', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><traveling-salesman>', 'CreationDate': '2013-07-22T01:01:32.150', 'Id': '13380'},64_77:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to understand algorithm complexity, and a lot of algorithms are classified as polynomial. I couldn't find an exact definition anywhere. I assume it is the complexity that is not exponential. </p>\n\n<p>Do linear/constant/quadratic complexities count as polynomial? An answer in simple English will be appreciated :)</p>\n", 'ViewCount': '226', 'Title': 'What exactly is polynomial time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-07T16:32:18.580', 'LastEditDate': '2013-08-07T07:59:04.743', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '13626', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9538', 'Tags': '<algorithms><terminology><time-complexity><runtime-analysis><polynomial-time>', 'CreationDate': '2013-08-06T01:28:41.500', 'Id': '13625'},64_78:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A shuffling algorithm is supposed to generate a random permutation of a given finite set. So, for a set of size $n$, a shuffling algorithm should return any of the $n!$ permutations of the set uniformly at random. </p>\n\n<p>Also, conceptually, a randomized algorithm can be viewed as a deterministic algorithm of the input and some random seed. Let $S$ be any shuffling algorithm. On input $X$ of size $n$, its output is a function of the randomness it has read. To produce $n!$ different outputs, $S$ must have read at least $\\log(n!) = \\Omega(n \\log n)$ bits of randomness. Hence, any shuffling algorithm must take $\\Omega(n \\log n)$ time.</p>\n\n<p>On the other hand, the <a href="http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle" rel="nofollow">Fisher-Yates shuffle</a> is <a href="http://www.codinghorror.com/blog/2007/12/the-danger-of-naivete.html" rel="nofollow">widely</a> <a href="http://c2.com/cgi/wiki?LinearShuffle" rel="nofollow">believed</a> to run in $O(n)$ time. Is there something wrong with my argument? If not, why is this belief so widespread?</p>\n', 'ViewCount': '161', 'Title': 'How can you shuffle in $O(n)$ time if you need $\\Omega(n \\log n)$ random bits?', 'LastEditorUserId': '8246', 'LastActivityDate': '2013-08-29T08:52:52.413', 'LastEditDate': '2013-08-28T18:42:00.367', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9847', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><randomized-algorithms>', 'CreationDate': '2013-08-28T07:42:37.830', 'Id': '13990'},64_79:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am new to Algorithms and am currently confused about the running time of the ThreeSum program. If we look at ThreeSum\'s count function:</p>\n\n<pre><code>public static int count(int[] a) {\n  int N     = a.length;\n  int count = 0;\n\n  for (int i = 0; i &lt; N; i++) &lt;-- A\n    for (int j = i + 1; j &lt; N; j++) &lt;-- B\n      for (int k = j + 1; k &lt; N; k++) &lt;-- C\n        // check each triple\n        if (a[i] + a[j] + a[k] == 0) \n          count++;\n  // loop i = 0; j = i+1; k = j+1 so that we get each triple just once\n\n  return count;\n  }\n</code></pre>\n\n<p>I understand that <code>int N = a.length</code> means that its frequency is 1 as the statement only runs once and that <code>count++</code>\'s frequency is x as it depends on input however with nested for loops. I understand that say <code>A</code>\'s frequency is $N$ (as it is repeated <code>N</code> times) but then I get a bit confused. </p>\n\n<p>It seems to be that for each nested loop, the other loop has to run which makes sense so <code>B</code>\'s frequency is $N^2$ but then in the book it says that B\'s frequency is $N^2/2$ and C is $N^3/6$ (where I would have assumed $N^2$ and $N^3$) where is the "$/2$" and "$/6$" coming from?</p>\n\n<p>Any help is appreciated, thanks. Sorry for the lengthy question!</p>\n', 'ViewCount': '687', 'ClosedDate': '2013-09-02T10:29:49.600', 'Title': 'Time complexity of a triple nested for loop for ThreeSum problem', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-02T10:28:53.567', 'LastEditDate': '2013-09-02T10:28:53.567', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9925', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-01T11:53:16.337', 'Id': '14065'},64_80:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I would like to check time complexity of two procedures for which I am not totally convinced that I got it right. Now the first procedure is this:</p>\n\n<pre><code>public static int c(int n) {\n    int i, j, s = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i*=2) {\n            for(j = n; j &gt; 0; j/=2) {\n                s++;\n            }\n            s+=c(n-2);\n        }\n    }\n}\n</code></pre>\n\n<p>Now I have set the following recurrence equation:\n$T(n)=\\log_2n*T(n-2)+\\Theta(\\log_2n)$</p>\n\n<p>Now the height of the recurrence tree is $n/2$ so the $T(n) = n/2 * (\\log_2n+\\log_2n)=\\Theta(n*\\log_2n)$</p>\n\n<p>The next procedure is this:</p>\n\n<pre><code>public static int d(int n, int m) {\n    int i, j, k, ss = 0;\n    if (n &gt; 1) {\n        for(i = 0; i &lt; n; i++) {\n            for(j = i; j &gt; n; j+=2) {\n                ss+=(i+1)-2*j+m;\n            }\n        }\n        for(k=0; k&lt;8; k++) {\n            ss+=d(n/2, m/(k+1))\n        }\n    }\n    return ss;\n}\n</code></pre>\n\n<p>Again I have set this equation:\n$T(n, m)=8*T(n/2, m/(k+1))+\\Theta(n^2)$</p>\n\n<p>Now I think the $m$ parameter is not important because it is not used in for loop as a counter. Where $n/2$ gives us a recurrence tree of height $\\log_2n$. So we get this:\n$T(n, m) = 8 * \\log_2n*n^2=\\Theta(n^2)$</p>\n\n<p>I know that recurrence equations that I have set up are probably right, while the I am not sure about next steps.</p>\n', 'ViewCount': '109', 'Title': 'Finding asymptotically tight bounds $\\Theta$ of two procedures', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-04T09:50:41.463', 'LastEditDate': '2013-09-04T09:22:55.420', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14124', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9974', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-04T08:14:21.707', 'Id': '14121'},64_81:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I was going through a past paper question but I don't have any answers to know if I'm working out the problems correctly or not.</p>\n\n<p>I need to find the time complexity for:</p>\n\n<pre><code>i) repeat\n      n:=n div 2;\n   until n=1;\n\nii) for i:=1 to n do\n       begin\n          for j:=1 to n do\n             begin\n                for k:=1 to n do;\n             end;\n       end;\n\niii) repeat\n        for i:=1 to n do\n           begin\n              for j:=1 to n do;\n           end;\n     n:=n div 2\n     until n=1\n</code></pre>\n\n<p>In my opinion, the answer for (ii) is $O(\\log n)$ and the answer for (ii) is $O(n^3)$ but I'm not sure about my answers. Regarding question (iii) I have no idea how to come up with a solution.</p>\n", 'ViewCount': '83', 'Title': 'Measuring time complexity of algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T10:19:16.727', 'LastEditDate': '2013-09-09T10:19:16.727', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9979', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-04T12:44:31.107', 'Id': '14127'},64_82:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume that I have a file that consists of pairs of numbers separated by a space. These numbers are the labels for vertices in my graph. For example:</p>\n\n<pre><code>0 5\n0 7\n2 3\n4 5\n1 5\n.\n.\n.\n</code></pre>\n\n<p>I want to create a graph (adjacency list) by reading this file line-by-line. For each line, I will create an edge between the two vertices. Of course, if the vertex doesn\'t exist yet, then I will add it before creating the edge.</p>\n\n<p>I read <a href="https://vismor.com/documents/network_analysis/graph_algorithms/S4.php" rel="nofollow">here</a> of an algorithm that builds a graph with a time complexity of $O(|V| + |E|)$ where $V$ = set of vertices and $E$ = set of edges. That makes sense to me. However my algorithm doesn\'t insert the vertices in a loop first and then insert all of the edges in another loop second. My algorithm just adds the vertices as it\'s looping through the edges.</p>\n\n<p>My question is if my algorithm is $O(|E|)$? It seems like that can\'t be right, but I read <a href="http://stackoverflow.com/questions/12231499/do-if-statements-affect-in-the-time-complexity-analysis">here</a> that when calculating the time complexity you don\'t take into account if statements. That\'s exactly what my vertex creation would be -- an if statement that checks if the node exists in the middle of my looping through all the edges.</p>\n', 'ViewCount': '261', 'Title': 'Time Complexity for Creating a Graph from a File', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T12:29:22.547', 'LastEditDate': '2013-09-09T12:29:22.547', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '14224', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10045', 'Tags': '<algorithms><graphs><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-09T04:28:52.703', 'Id': '14223'},64_83:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was given the following code and was told to find the best and worst case running times in big theta notation. (Below is in python)</p>\n\n<pre><code>def find(a, target):\n    x = 0\n    y = len(a)\n    while x &lt; y:\n        m = (x+y)/2\n        if a[m] &lt; target:\n            x = m+1\n        elif a[m] &gt; target: \n            y = m\n        else:\n            return m\n    return -1\n</code></pre>\n\n<p>I know that the running time of this code in the worst case is $O(\\lg n)$. But the question I was given if the fifth line was changed from $m = \\frac{x+y}{2}$ to $m=\\frac{2x+y}{3}$, would the running time change?</p>\n\n<p>My intuition is that the running time gets a little larger as it is no longer cutting the list in half like binary search should do which is less efficient, but I am not sure how to calculate what the asymptotic runtime would be at this point.</p>\n', 'ViewCount': '98', 'Title': 'Runtime of various versions of binary search', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T18:05:11.853', 'LastEditDate': '2013-09-10T16:46:37.783', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '14253', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10062', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-10T16:15:23.313', 'Id': '14250'},64_84:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>One of my courses introduced the following question:</p>\n\n<blockquote>\n  <p>Given the recurrence relation for mergesort:</p>\n  \n  <p>$T(n) = 2T(n/2) + n$</p>\n  \n  <p>How would the following parameter passing strategies influence the relation and running time of the algorithm:</p>\n  \n  <ol>\n  <li><p>A pointer to the array is passed to the sorting function,</p></li>\n  <li><p>The entire array is copied to a separate location before applying the sorting function,</p></li>\n  <li><p>A subsection of the array is copied to a separate location before applying the sorting function on that subsection.</p></li>\n  </ol>\n</blockquote>\n\n<p>For 1., since this is how mergesort is used most of the time, we can solve it easily using the master theorem.</p>\n\n<p>$f(n) = \\Theta(n^{\\log_b a}) = \\Theta(n) \\implies T(n) = \\Theta(n \\log n)$</p>\n\n<p>For 2. however, I am a bit baffled. Although we do an additional work of $O(n)$, we are only doing so <em>once</em> at the beginning of the sort. Hence, doing one additional instance of $O(n)$ work should not influence the order of the running time (because it already <em>is</em> of a larger order). Hence, for both 2. and 3. the running time would remain at $T(n) = \\Theta(n \\log n)$. </p>\n\n<p>Is this reasoning valid? Something tells me that the $O(n)$ copying should have more of an impact, but I can't seem to give myself a good enough reason that it should be responsible to slow it down enough so that it would worse (i.e. $O(n^2)$). </p>\n\n<p>Any thoughts or hints would be quite appreciated!</p>\n", 'ViewCount': '134', 'Title': 'Do different variants of Mergesort have different runtime?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-16T07:58:23.627', 'LastEditDate': '2013-09-16T06:58:55.213', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14349', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10093', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><recurrence-relation><sorting>', 'CreationDate': '2013-09-12T03:25:30.877', 'Id': '14275'},64_85:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In the last section of chapter 3 (page 54) in <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em> by Mitzenmacher and Upfal, a randomized algorithm is discussed for finding the median of a set $S$ of distinct elements in $O(n)$ time. As a substep the algorithm requires sorting a multiset $R$ of values sampled from $S$, where $|R| = \\lceil n^{3/4} \\rceil$. I'm sure this is a fairly simple, and straight forward question compared to the others on this site, but how is sorting a multiset of this size bounded by $O(n)$? </p>\n\n<p>I assume the algorithm is using a typical comparison-based, deterministic sorting algorithm that runs in $O(n\\log$ $n)$. So, If I have the expression $O(n^{3/4}\\log(n^{3/4}))$, isn't this just equivalent to $O(\\frac{3}{4}n^{3/4}\\log(n)) = O(n^{3/4}\\log(n))$? How does the reduction down to $O(n)$ proceed?</p>\n", 'ViewCount': '107', 'Title': 'Randomized Median Element Algorithm in Mitzenmacher and Upfal: O(n) sorting step?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:46:22.310', 'LastEditDate': '2013-09-16T07:46:22.310', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10128', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><search-algorithms><randomized-algorithms>', 'CreationDate': '2013-09-13T20:07:11.977', 'Id': '14296'},64_86:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '82', 'Title': 'Time Complexity of Algorithm', 'LastEditDate': '2013-09-16T07:43:21.313', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '4658', 'FavoriteCount': '0', 'Body': "<p>I need help with finding out the time complexity of the following algorithm:</p>\n\n<pre><code>procedure VeryOdd(integer n):\nfor i from 1 to n do\n  if i is odd then\n    for j from i to n do\n      x = x + 1\n    for j from 1 to i do\n      y = y + 1\n</code></pre>\n\n<p>This is my attempt:</p>\n\n<p>$$ Loop1 = \\Theta(n)$$\n$$ Loop2 = \\Theta(n)$$\n$$ Loop2 = O(n)$$</p>\n\n<p>And we also know that loop2 and loop3 will get executed every second time of the execution of the outer loop. So we know that:</p>\n\n<p>$$T(n) = \\Theta(n) * 1/2(\\Theta(n) + O(n)) = \\Theta(n^2)$$</p>\n\n<p>Now to the thing I'm not so sure about, nameley, is Loop3 really $$O(N)$$ and\nif yes, then is $$\\Theta(n) + O(n) = \\Theta(n)$$</p>\n\n<p>Thanks in advance</p>\n", 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:43:21.313', 'CommentCount': '2', 'AcceptedAnswerId': '14303', 'CreationDate': '2013-09-13T22:40:48.113', 'Id': '14298'},64_87:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am new to Advanced Algorithms and I have studied various samples on Google and StackExchange. What I understand is:</p>\n\n<ol>\n<li><p>We use $O(\\log n)$ complexity when  there is division of any $n$ number on each recursion (especially in divide and conquer).</p></li>\n<li><p>I know that for binary search, we have time complexity $O(n \\log n)$, I understood $\\log n$ is because each time it halves the full $n$ size number list in a recursive manner until it finds the required element. But why is it multiplied with $n$ even we just traverse half of the $n$ size element for each execution so why we multiply $\\log n$ with $n$?</p></li>\n<li><p>Please give me any example explaining the complexity $O(n^2 \\log n)$. I hope this will help me in understanding much better the above two questions.</p></li>\n</ol>\n', 'ViewCount': '218', 'Title': 'Confusion regarding several time complexities including the logarithm', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T08:56:44.833', 'LastEditDate': '2013-09-16T08:56:44.833', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10129', 'Tags': '<algorithms><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2013-09-13T23:31:05.913', 'Id': '14299'},64_88:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am working on multiple patterns search matching algorithms and I found that two algorithms are the strongest candidates, namely <a href="http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm" rel="nofollow">Aho-Corasick</a> and <a href="http://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm" rel="nofollow">Rabin-Karp</a> in terms of running time. However, there are few things still foggy. I could not find any comprehensive comparison between the two algorithms. Besides, what I need to know is which one of them is more suitable for parallel computing and multiple patterns search. Which one require less hardware resources.  </p>\n\n<p>For AC algorithm searching phase time complexity is O(n+m), while it is O(nm) for RK. However, running time for RK is O(n+m) which make it similar to AC. My conclusion is RK practically better as it does not need memory as AC. I need a confirmation of that.  </p>\n', 'ViewCount': '234', 'Title': 'A comparison between Aho-Corasick algorithm and Rabin-Karp algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-09T00:14:12.477', 'LastEditDate': '2013-09-16T07:47:49.863', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9855', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><strings>', 'CreationDate': '2013-09-14T16:34:08.873', 'Id': '14309'},64_89:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose an algorithm has a runtime recurrence relation:</p>\n\n<p>$   T(n) = \\left\\{\n     \\begin{array}{lr}\n       g(n)+T(n-1) + T(\\lfloor\\delta n\\rfloor ) &amp; : n \\ge n_0\\\\\n       f(n) &amp; : n &lt; n_0\n     \\end{array}\n   \\right.\n$  </p>\n\n<p>for some constant $0 &lt; \\delta &lt; 1$. Assume that $g$ is polynomial in $n$, perhaps quadratic. Most likely, $f$ will be exponential in $n$.</p>\n\n<p>How would one go about analyzing the runtime ($\\Theta$ would be excellent)? The master theorem and the more general Akra-Bazzi method do not seem to apply.</p>\n', 'ViewCount': '175', 'Title': "Asymptotic approximation of a recurrence relation (Akra-Bazzi doesn't seem to apply)", 'LastEditorUserId': '8877', 'LastActivityDate': '2013-11-13T19:26:31.243', 'LastEditDate': '2013-11-13T19:26:31.243', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '8877', 'Tags': '<algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2013-09-15T22:17:33.163', 'FavoriteCount': '1', 'Id': '14343'},64_90:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Some complicated algorithms (<a href="http://en.wikipedia.org/wiki/Union-find">union-find</a>) have the nearly-constant inverse Ackermann function that appears in the asymptotic time complexity, and are worst-case time optimal if the nearly constant inverse Ackermann term is ignored. </p>\n\n<p>Are there any examples of known algorithms with running times that involve functions that grow fundamentally slower than inverse Ackermann (e.g. inverses of functions that are not equivalent to Ackermann under polynomial or exponential etc. transformations), that give the best-known worst-case time complexity for solving the underlying problem?</p>\n', 'ViewCount': '207', 'Title': 'Do functions with slower growth than inverse Ackermann appear in runtime bounds?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-18T21:26:09.597', 'LastEditDate': '2013-09-18T21:26:09.597', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<reference-request><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2013-09-17T23:51:31.520', 'FavoriteCount': '3', 'Id': '14392'},64_91:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to calculate the runtime complexity of a function that does not have fixed size input, but uses several helper methods that do have fixed size input. I was unsure of how to include the helper methods in my calculations. </p>\n\n<p>If I have an array with a fixed size of 32 indices, and I have a function that sums up the elements in that array, will that function be $O(n)$, or $O(1)$?  I think that a function that sums up the elements of an array is $O(n)$ because each element of an $n$-length array must be visited, but if the function only sums up arrays of length 32, is it still $O(n)$?</p>\n', 'ViewCount': '61', 'Title': 'How to include calls to an $O(n)$ subroutine on finite-sized inputs in an analysis?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:49:49.943', 'LastEditDate': '2013-09-18T09:49:49.943', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '14412', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9102', 'Tags': '<terminology><algorithm-analysis><runtime-analysis>', 'CreationDate': '2013-09-18T08:42:53.543', 'Id': '14408'},64_92:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have a program p that has time complexity $O(n)$ and a second program q that calls program p $m$ times. If we know the input size of p will be the same every one of those times (n), we can say that the complexity of q is $O(m \\times n)$. This case sounds pretty simple to me.</p>\n\n<p>However, suppose the input size of p varies over these $m$ times, from $0$ to $m-1$.</p>\n\n<p>If we knew that p ran precisely $f(n) = a \\times n$ instructions for an input of size $n$, it would be simple: q would run $a \\times 0 + a \\times 1 + \\ldots + a \\times (m-1) = am(m-1)/2$ instructions exactly, which is $O(m^2)$.</p>\n\n<p>However, in this case, all we know is that $f(n)$ is below $a \\times n$ for all $n$ greater than some $n_0$, by the definition of big-O. We can't say for sure that $f(0) + f(1) + ... + f(m-1)$ is below $a \\times 0 + a \\times 1 + \\ldots + a \\times (m-1)$, because we don't know that $0, 1, \\ldots, m-1$ are greater than this $n_0$. For this reason, I don't think we can say that q runs in $O(m^2)$.</p>\n\n<p>Is there a way, in this case, when all we know is the big-O behaviour of p, to analyze q?</p>\n\n<p>What if we replace all the big-O's with big-Theta's?</p>\n", 'ViewCount': '86', 'Title': 'Big O and program calls with varied input sizes', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-17T16:01:47.387', 'LastEditDate': '2013-11-17T16:01:47.387', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14535', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10283', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2013-09-22T23:26:38.763', 'Id': '14532'},64_93:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '234', 'Title': 'Complexity of keeping track of $K$ smallest integers in a stream', 'LastEditDate': '2014-01-30T21:46:49.597', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10384', 'FavoriteCount': '2', 'Body': '<p>I need to analyze the time complexity of an online algorithm to keep track of minimum $K$ numbers from a stream of $R$ numbers. The algorithm is </p>\n\n<ul>\n<li>Suppose the $i$th number in the stream is $S_i$.</li>\n<li>Keep a max heap of size $K$.</li>\n<li>If the heap contains fewer than $K$ elements, add $S_i$ to the heap.</li>\n<li>Otherwise: if $S_i$ is smaller than the maximum element in the heap (i.e., the root of the heap), replace the root of the heap with $S_i$ and apply Max-Heapify to the heap; if $S_i$ is greater than or equal to the max element, do nothing.</li>\n</ul>\n\n<p>The problem now is to find the expected number of times the Max Heapify operation will be called, when the stream of integers is of length $R$ and each element of the stream is (iid) uniformly distributed on $[1,N]$.</p>\n\n<p>If the stream were guaranteed to contain only distinct elements, then the answer is easy: </p>\n\n<p>$$E[X] = E[X_1] + E[X_2] + \\dots + E[X_R],$$</p>\n\n<p>where $X_i$ is the random indicator variable for occurrence of the Max Heapify operation at the $i$th number in the stream.  Now  </p>\n\n<p>$$E[X_i] = \\Pr[\\text{$S_i$ is ranked $\\le K$ among first $i$ elements}] = \\min(K/i, 1).$$</p>\n\n<p>Hence,</p>\n\n<p>$$E[X] = K + K/(K+1) + \\cdots + K/R.$$</p>\n\n<p>That case is relatively easy.  But how do we handle the case where the elements are iid uniformly distributed?</p>\n\n<p>[This was actually a Google interview question.] </p>\n', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><average-case>', 'LastEditorUserId': '755', 'LastActivityDate': '2014-02-07T12:35:40.240', 'CommentCount': '0', 'AcceptedAnswerId': '20261', 'CreationDate': '2013-09-28T19:59:47.083', 'Id': '14661'},64_94:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>For analyzing the running time of an algorithm , I'm stuck with this recursive equation :\n$$\nT(n) = \\log(n) \\cdot T(\\log n) + n\n$$\nObviously this can't be handled with the use of the Master Theorem, so I was wondering if anybody has any ideas for solving this recursive equation.</p>\n\n<p>I'm pretty sure that it should be solved with a change in the parameters, like considering $n$ to be $2^m$, but I couldn't manage to find any good fix.</p>\n", 'ViewCount': '371', 'Title': 'Recursive equation for complexity: T(n) = log(n) * T(log(n)) + n', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-03T11:25:54.990', 'LastEditDate': '2013-10-03T11:25:54.990', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14777', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '10478', 'Tags': '<asymptotics><runtime-analysis><recursion><master-theorem>', 'CreationDate': '2013-10-03T09:53:19.790', 'Id': '14775'},64_95:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>So I\'m studying for my data structures midterm and my professor gave out a sample midterm with the answers, but I\'m having a hard time understanding one of the questions. </p>\n\n<p>Here\'s a screen cap:</p>\n\n<p><img src="http://i.stack.imgur.com/EPFBo.png" alt="enter image description here"></p>\n\n<p>The answer is <code>n^2(n+1)/2</code>.</p>\n\n<p>I would appreciate a very simple walk through and/or a pointer to some good material. </p>\n', 'ViewCount': '96', 'Title': 'Deriving the exact number of execution times', 'LastEditorUserId': '10424', 'LastActivityDate': '2013-10-07T00:35:37.567', 'LastEditDate': '2013-10-06T21:36:25.240', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '14844', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10424', 'Tags': '<time-complexity><runtime-analysis>', 'CreationDate': '2013-10-06T00:39:07.513', 'Id': '14843'},64_96:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '268', 'Title': 'What is the asymptotic runtime of this nested loop?', 'LastEditDate': '2014-03-29T14:10:44.667', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8690', 'Body': '<p>I am trying to analyse the runtime of this algorithm:</p>\n\n\n\n<pre><code>for(i=1; i &lt; n; i++){\n   for(j=1; j &lt;= i; j++){\n         statement1;\n   }       \n}\n</code></pre>\n\n<p>Expanding the above loop into.</p>\n\n<ul>\n<li><p>First : </p>\n\n   \n\n<pre><code>for(j=1; j &lt;= 1; j++){\n   statement1;          //complexity O(1)\n}\n</code></pre></li>\n<li><p>Second:  </p>\n\n   \n\n<pre><code>for(j=1; j &lt;=2 ; j++){\n   statement1;          //complexity O(2)\n}       \n</code></pre></li>\n</ul>\n\n<p>... </p>\n\n<ul>\n<li><p>n-th:</p>\n\n<pre><code>for(j=1; j &lt;= n; j++){\n   statement1;          //complexity O(n)\n}\n</code></pre></li>\n</ul>\n\n<p>So the runtime of the loop should be</p>\n\n<p>$\\qquad \\displaystyle O(1) + \\dots + O(n) = O\\Bigl(\\frac{n(n+1)}{2}\\Bigr) = O(n^2)$.</p>\n\n<p>Can I reason like this, or what is the proper way to analyse nested <code>for</code>-loops?</p>\n', 'ClosedDate': '2014-03-29T13:23:48.603', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T21:51:39.133', 'CommentCount': '3', 'AcceptedAnswerId': '14893', 'CreationDate': '2013-10-07T06:50:10.650', 'Id': '14880'},64_97:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Here is my approach </p>\n\n<p>First I compute the longest non decreasing sub-sequence in $N \\log N$ time. Algorithm to do this (that only uses arrays and binary search) can be found here:\n<a href="http://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms" rel="nofollow">http://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms</a></p>\n\n<p>Let\'s suppose the longest subsequence has $L$ elements. Then if $L &lt; N - M$, there isn\'t any way to solve the problem since there\'s no subsequence of length $N - M$ that\'s still sorted.</p>\n\n<p>Otherwise, just remove the $N - L$ elements that aren\'t in the subsequence, and then remove more at random until exactly M total have been removed. In all this is an $N \\log N$ algorithm.</p>\n\n<p>I want to know, is there any more efficient algorithm (i.e. $O( N)$ ) to solve this problem ?</p>\n', 'ViewCount': '193', 'Title': 'Given an array of N integers, how can you find M elements to remove so that the array will end up in sorted order?', 'LastEditorUserId': '6890', 'LastActivityDate': '2013-10-08T23:01:04.927', 'LastEditDate': '2013-10-08T08:47:24.010', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8690', 'Tags': '<algorithms><time-complexity><runtime-analysis><subsequences>', 'CreationDate': '2013-10-08T08:40:39.700', 'FavoriteCount': '1', 'Id': '14899'},64_98:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '61', 'Title': 'Time complexity of an algorithm', 'LastEditDate': '2013-10-09T07:12:27.010', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '10593', 'Body': '<p>Currently stuck on a question.</p>\n\n<p>"Assume the time complexity of an algorithm on input size is 6n^3. If the algorithm takes 10 seconds to execute for an input size of n. Then how many seconds will it take for an input size of 2n."</p>\n\n<p>Thanks!</p>\n', 'ClosedDate': '2013-10-30T10:04:58.103', 'Tags': '<algorithm-analysis><runtime-analysis>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-10-09T07:12:27.010', 'CommentCount': '5', 'AcceptedAnswerId': '14929', 'CreationDate': '2013-10-08T21:29:18.650', 'Id': '14925'},64_99:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was asked to "Compute the average runtime for a binary search, ordered array, and the key is in the array." I\'m not quite sure how to approach this problem. Isn\'t the runtime of binary search O(log n)? And the average would be something like n + n/2 + n/4... etc?</p>\n\n<p>I\'m then asked to "Implement a program performing an empirical test of the binary search (using a fixed number of random arrays for each n), then do a ratio test justifying your analytical answer." How would I go about doing this? Could I perform a basic binary search algorithm on a number of random arrays, counting the basic operations, and compare that to my original analysis from the first question?</p>\n\n<p>I appreciate any help/guidance here.</p>\n', 'ViewCount': '699', 'Title': 'How to analyze/test a binary search algorithm?', 'LastActivityDate': '2013-10-14T02:03:59.603', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16060', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10719', 'Tags': '<algorithm-analysis><runtime-analysis><binary-search>', 'CreationDate': '2013-10-13T22:58:49.533', 'Id': '16057'},64_100:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I was given the following homework question:</p>\n\n<blockquote>\n  <p>Implement an extendable table using arrays that can increase in size\n  as elements are added. Perform an experimental analysis of each of the\n  running times for performing a sequence of n add methods, assuming the\n  array size is increased from N to the following possible values: </p>\n  \n  <ul>\n  <li>2N  </li>\n  <li>N + ceiling(\u221aN)</li>\n  <li>N + ceiling(log N)  </li>\n  <li>N + 100</li>\n  </ul>\n</blockquote>\n\n<p>I\'m just a little confused about what this question is asking, and was hoping for some help/clarification. The way I understand it, you could implement something like this in Python with a two-dimensional array (the "extendable table"), and then append varying numbers of values for each scenario. Am I understanding the implementation correctly?</p>\n\n<p>Then, I\'m also a bit unclear on what number of values you\'d be appending. Would you literally first test it with say, 16 values, then 32 (2N), then 20 (ceiling(\u221aN)), etc? Or is it more complex than that?\nAny help is appreciated!</p>\n', 'ViewCount': '38', 'ClosedDate': '2013-10-30T10:04:14.270', 'Title': 'experimental analysis of running times in extendable table', 'LastActivityDate': '2013-10-14T21:04:29.030', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16087', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10719', 'Tags': '<algorithm-analysis><runtime-analysis><arrays>', 'CreationDate': '2013-10-14T20:02:02.757', 'Id': '16086'},64_101:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have this code fragment:</p>\n\n<pre><code>for( i=0; i&lt;n; i++ )\n    for( j=0; j&lt;i; j++ )\n        for( k=0; k&lt;j; k++ )\n            S;\n</code></pre>\n\n<p>I need to find the number of times that S is executed in that code block. I <a href="http://www.wolframalpha.com/input/?i=sum%20%28sum%28sum%281%29%20from%20k=0%20to%20j%29%20from%20j=0%20to%20i%29%20from%20i=0%20to%20n" rel="nofollow">plugged it into Wolfram Alpha</a> but I am not sure if that is the right answer. </p>\n\n<p>I want to be able to do the math in order to figure the answer out so can someone point me in the right direction?</p>\n', 'ViewCount': '170', 'Title': 'Sum number of times statement is executed in triple nested loop', 'LastActivityDate': '2013-10-29T22:58:43.080', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '16558', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11052', 'Tags': '<runtime-analysis><loops>', 'CreationDate': '2013-10-29T21:07:44.987', 'FavoriteCount': '1', 'Id': '16555'},64_102:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '62', 'Title': 'Converting a recursive algorithm to a runtime function', 'LastEditDate': '2013-11-12T16:48:25.630', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10875', 'Body': "<pre><code>void Sort(int A[], int left, int right)\n{\n    int p;\n\n    if (left &lt; right)\n    {\n        p = (right + left + 2)/3;\n\n        Sort(A, left, left+p-1);\n        Sort(A, left+p, left+2*p-1);\n\n        MergeSort(A, left+2*p, right);\n\n        Merge3(A, left, left+p, left+2*p, right);\n    }\n}\n</code></pre>\n\n<p>I need to convert this function into a mathematical expression in order to solve it's run-time complexity.</p>\n\n<p>I know  that <code>MergeSort()</code>'s complexity is of $\\Theta(n \\log n)$ and that <code>Merge3()</code>'s complexity is of $\\Theta(n)$.</p>\n\n<p>I can't figure how to transform this into a recursive mathematical expression.</p>\n", 'ClosedDate': '2013-11-15T15:38:52.877', 'Tags': '<algorithm-analysis><runtime-analysis><recursion>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-12T18:03:24.563', 'CommentCount': '11', 'AcceptedAnswerId': '17964', 'CreationDate': '2013-11-12T12:37:24.567', 'Id': '17956'},64_103:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a practice exam question that I don't know how to set up a recurrence for. It is dealing with a hash table. The question is as follows:</p>\n\n<blockquote>\n  <p>Suppose that a hashing strategy is designed so that it starts with an initial hash table size of $H= 8$. You may assume that only insertions are performed (no deletions).</p>\n  \n  <p>Any time the hash table is going to be more than 50% full (when an attempt is made to add item $\\frac{H}{2} + 1$ to a table of size $H$), the hash table size is doubled to $2\\times H$, and then the $\\frac{H}{2}$ keys in the previous hash table are rehashed using $\\frac{H}{2}$ extraneous key insertions into the new table of size $2 \\times H$. The key insertions used to initially place each key into the hash table are called necessary key insertions (these are not extraneous).</p>\n</blockquote>\n\n<p>The question is asking to derive a recurrence relation $E(H)$ for the number of extraneous key insertions that have occurred in total up until the point in time that the hash table size is $H$ and to explain where the terms in the recurrence relation derive from.</p>\n\n<p>If someone could help me out with this, it would provide very helpful as I am practicing for an exam that I have in a week. Thanks everyone.</p>\n\n<p>I got the result $E(H)=2\\times E(\\frac{H}{2})$ because after each rehash there are $\\frac{H}{2}$ extraneous key insertions being put into the table of size $2\\times H$. So if the size is twice the amount of $H$, I figured the recurrence would be $E(H)=2\\times E(\\frac{H}{2})$. I only posted here because I was hoping someone could assist me with this because this question has me a bit stumped. </p>\n", 'ViewCount': '81', 'Title': 'Recurrence for total number of extraneous key insertions in a hash table', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T08:41:25.867', 'LastEditDate': '2014-03-26T08:41:25.867', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11491', 'Tags': '<data-structures><runtime-analysis><recurrence-relation><hash-tables>', 'CreationDate': '2013-11-20T00:24:50.667', 'Id': '18178'},64_104:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When can an algorithm be said to have $O(1)$ complexity? My doubt is if $n$ is specified to be a large number but constant and we cannot implement it in reality without a loop even then can we call it to have $O(1)$ time complexity? Consider the following examples.</p>\n\n<ol>\n<li><p>Algorithm to add first 1000 natural numbers (that is I mean to say if n is specified directly). Then can we say this has $O(1)$ time complexity?</p></li>\n<li><p>Finding the $7$th smallest element in a min heap. This element is present in anywhere in the first 6 levels of the heap (considering root at level 0). So to find the element we need to check $2^7 - 1$ elements. Then can we say this has $O(1)$ time complexity?</p></li>\n</ol>\n', 'ViewCount': '223', 'Title': 'When does an algorithm run in $O(1)$ time?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-02T12:21:30.010', 'LastEditDate': '2013-12-02T12:21:30.010', 'AnswerCount': '4', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8533', 'Tags': '<algorithms><asymptotics><runtime-analysis>', 'CreationDate': '2013-11-28T19:12:48.417', 'Id': '18451'},64_105:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have been doing a little reading up on bubble sort and have read on wikipedia that it's complexity is measured as $\\Theta(n^2)$</p>\n\n<p>This bubble sort however is slightly more efficient. I thought this would be the best place to ask how I would work out this particular implementations complexity seeing that the number of iterations in the inner loop is reduced with each pass.</p>\n\n<pre><code>for (top = items.Length; top &gt; 0; top--)\n            {\n                for (low = 0, high = 1; high &lt; top; low++, high++)\n                {\n                    if (items[low].CompareTo(items[high]) &gt; 0)\n                    {\n                        tmp = items[high];\n                        items[high] = items[low];\n                        items[low] = tmp;\n                    }\n                }\n            }\n</code></pre>\n", 'ViewCount': '82', 'ClosedDate': '2013-12-05T23:08:19.347', 'Title': 'What is the complexity of this bubble sort algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-05T23:06:12.697', 'LastEditDate': '2013-12-05T23:06:12.697', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11855', 'Tags': '<algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2013-12-05T21:41:38.547', 'Id': '18662'},64_106:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose we need to find a tight asymptotic bound on the worst case run time of the following program</p>\n\n<pre><code>t = 0;\nfor i = 0 to n-1 do\n  for j = i+1 to n-1 do\n    for k = j+1 to n-1 do\n      t++;\n</code></pre>\n\n<p>I can't for the life of me figure out what the run-time would be. I have a feeling it would have n! in there somewhere, but I can't seem to wrap my head around it. All help is appreciated</p>\n", 'ViewCount': '144', 'Title': 'Worst run-time for 3 nested loop', 'LastEditorUserId': '9550', 'LastActivityDate': '2013-12-11T11:29:27.627', 'LastEditDate': '2013-12-11T11:16:01.677', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12007', 'Tags': '<asymptotics><runtime-analysis><lower-bounds>', 'CreationDate': '2013-12-11T02:10:13.197', 'FavoriteCount': '1', 'Id': '18857'},64_107:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>While thinking about different calculi for predicate logic (like natural deduction and sequent calculus), I noticed that these calculi are (often) presented in a form suitable for "human computers". A "human computer" is limited to use <a href="http://en.wikipedia.org/wiki/Write_once_read_many" rel="nofollow">write once read many</a> (WORM) memory when processing large amounts of data. Pure functional programming also seems to favor a WORM memory model. In fact, it seems to me that the WORM memory model is so natural that classifying it as <a href="http://en.wikipedia.org/wiki/Unconventional_computing" rel="nofollow">unconventional computing</a> might underestimate its importance. (Understanding the strengths and limitations of the computing resources available to humans is important.)</p>\n\n<p>What is known about the relation between space and time complexity for machines with WORM memory? What are the keywords to google for available material related to these questions? Do we known whether the time complexity will remain the same, if a small (for example C*log(WORM memory)^n) amount of normal memory is added?</p>\n', 'ViewCount': '91', 'Title': 'Relation between space and time complexity for machines with write once read many (WORM) memory', 'LastEditorUserId': '1557', 'LastActivityDate': '2014-03-03T00:13:28.570', 'LastEditDate': '2013-12-13T09:09:01.290', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22210', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1557', 'Tags': '<time-complexity><runtime-analysis><space-complexity>', 'CreationDate': '2013-12-13T00:04:24.253', 'Id': '18939'},64_108:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So given a input of lets say 10 strings, what way can we input these so we get the best or worst case for these two given sorts?</p>\n\n<pre><code>Heap sort:\nbest case - nlogn\nworst case - nlogn\n\nQuick sort:\nbest case - nlogn\nworst case - n^2\n</code></pre>\n\n<p>Where i get confused on these two is:</p>\n\n<p>heap- Since the best and worst case are the same does it not matter the input order? The number of comparisons and assignments will always be the same? I imagine in a heap sort it may be the same since the real work is done in the insertion, but the sorting only uses the removal of the max/min heap? Is that why?</p>\n\n<p>quick sort- This one I don't know for sure. I'm not sure what the best case and worst case situations are for this. If its a already sorted list of 10 strings for example wouldn't we always have to choose the same amount of pivots to get complete the recursive algorithm? Any help on this explanation would really help.</p>\n\n<p>Thanks</p>\n", 'ViewCount': '2076', 'Title': 'Best and worse case inputs for heap sort and quick sort?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-12-13T07:41:27.657', 'LastEditDate': '2013-12-13T07:41:27.657', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'OwnerDisplayName': 'user3037172', 'PostTypeId': '1', 'Tags': '<algorithm-analysis><runtime-analysis><sorting><quicksort>', 'CreationDate': '2013-11-26T16:04:07.403', 'Id': '18945'},64_109:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am reading a unpublished paper describing an algorithm. In one step of the algorithm, there is a bipartite graph $G(X,Y,E)$, where $X=\\{1,...,n\\}$.</p>\n\n<p>For every subset $X' \\subseteq X$, they define</p>\n\n<p>$$f(X')=\\bigcup_{i \\in X'} \\{j \\in Y| (i,j)\\in E\\}.$$</p>\n\n<p>In other words, $f(X')$ is the set of neighbors of vertices in $X'$.</p>\n\n<p>Then they define:</p>\n\n<p>$$ X^+ = \\arg \\max_{X' \\subseteq \\{2,3,...,n\\}\\ s.t.\\ |X'|\\geq|f(X')|}\\{|X'|\\}$$</p>\n\n<p>i.e., $X^+$ is a largest subset of $\\{2,3,...,n\\}$ such that $X^+$ is at least as large as the set of its neighbors in $Y$ (i.e., the largest subset such that $|X^+| \\ge |f(X^+)|$).</p>\n\n<p>And then they do some stuff with this $X^+$.</p>\n\n<p>MY QUESTION IS: Can this $X^+$ be found in polynomial time? </p>\n\n<p>The authors do not prove that it can, but this is implied by the paper (otherwise the algorithm itself cannot be polynomial). Maybe it is so obvious that only I haven't seen this?</p>\n\n<p>EDIT: The following similar problem can be solved easily:</p>\n\n<pre><code>Find the largest set of vertices in X with less than |Y| neighbours.\n</code></pre>\n\n<p>Solution: find a $y \\in Y$ with a minimal number of neighbours in $X$. Return the set that includes all vertices in $X$ except $y$'s neighbours.</p>\n\n<p>What about the following similar problem?</p>\n\n<pre><code>Find the largest set of vertices in X with less than |Y|/2 neighbours.\n</code></pre>\n", 'ViewCount': '126', 'Title': 'Largest set of vertices that is larger than its set of neighbors', 'LastEditorUserId': '1342', 'LastActivityDate': '2014-01-01T15:40:34.427', 'LastEditDate': '2014-01-01T15:40:34.427', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1342', 'Tags': '<algorithms><graph-theory><time-complexity><runtime-analysis>', 'CreationDate': '2013-12-30T14:31:23.103', 'Id': '19377'},64_110:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<pre><code>i=n; \nwhile(i&gt;0) {\n  k=1;\n  for(j=1;j&lt;=n:j+=k)\n    k++;\n  i=i/2;\n}\n</code></pre>\n\n<p>The while loop has the complexity of $\\lg(n)$ the j value of inner loop runs 1,3,6,10,15...\nincrease like 2,3,4,5,...</p>\n\n<p>But how to find the overall complexity ?</p>\n', 'ViewCount': '136', 'ClosedDate': '2014-01-03T15:56:24.630', 'Title': 'How to find the asymptotic runtime of these nested loops?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-04T19:36:17.830', 'LastEditDate': '2014-01-04T19:36:17.830', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12340', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-03T10:45:35.863', 'Id': '19480'},64_111:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am curious how to do a line by line analysis of this piece of code using the "Big O" notation.</p>\n\n<pre><code>i = 0;\nj = 0;\n\nwhile ( ( i &lt; n ) &amp;&amp; ( j &lt; m ) )\n{\n      //do something\n      i++;\n      j++;\n}\n</code></pre>\n\n<p>How I should represent number of iterations for the loop? Is good if I will do some assumptions or I should write min(n, m)?</p>\n\n<p>Small extension after @Patrick87\'s comment to show why I am not sure that min() is a general solution:</p>\n\n<pre><code>i = 0;\nj = 0;\n\nwhile ( ( i &lt; n ) &amp;&amp; ( j &lt; m ) )\n{\n      //do something\n      i++;\n      j++;\n}\nif ( i &lt; n )\n{\n      while ( ( i &lt; n ) )\n      {\n         //do something\n         i++;\n      }\n}\nelse\n{\n      while ( ( j &lt; m ) )\n      {\n         //do something\n         j++;\n      }\n}\n</code></pre>\n\n<p>How right now we can connect a number of iterations of the first loop and second one if we don\'t know which condition broke the condition of the first loop? </p>\n', 'ViewCount': '237', 'Title': 'Complexity analysis of while loop with two conditions', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-08T08:43:36.637', 'LastEditDate': '2014-01-08T08:42:42.400', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19571', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12652', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-07T18:48:21.657', 'Id': '19564'},64_112:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>For each of the procedures below, let $T(n)$ be the running time. Find the order of $T(n)$\n(i.e., \ufb01nd $f(n)$ such that $T(n) \u2208 \u0398(f(n))$. Do not worry about how rounding errors a\ufb00ect running time.</p>\n\n<pre><code>Procedure Fum(int n):\n  for i from 1 to n do\n    \u03b4 \u2190 1/i\n    x \u2190 i\n    while x &gt; 0 do\n      x \u2190 x \u2212 \u03b4\n    end while\n  end for\n</code></pre>\n\n<p>Since it goes from 1 to n, and these operations</p>\n\n<pre><code>\u03b4 \u2190 1/i\nx \u2190 i\nx \u2190 x \u2212 \u03b4\n</code></pre>\n\n<p>Take the most time, so is the answer $O(n^3)$?</p>\n\n<p>2.</p>\n\n<pre><code>Procedure BinarySearch(table T [a . . . b], int k):\n  if a &gt; b then\n    return -1\n  end if\n  middle \u2190 \u230a(a + b)/2\u230b\n  if T [middle] = k then\n    return middle\n  end if\n  if k &lt; T [middle] then\n    return BinarySearch(T [a . . .middle], k)\n  else\n    return BinarySearch(T [middle . . . b], k)\n  end if\n</code></pre>\n\n<p>Counting the number of if-else operations, would this one also be $O(n^3)$?</p>\n', 'ViewCount': '73', 'ClosedDate': '2014-01-23T09:08:13.143', 'Title': 'FInd the running time complexity of functions', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-23T09:08:01.860', 'LastEditDate': '2014-01-23T09:08:01.860', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13051', 'Tags': '<algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-01-23T03:14:31.247', 'Id': '19907'},64_113:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p>The algorithm task is to find an integer (range is not known). the function <code>guess(num)</code> returns one of three chars: \'>\',\'&lt;\' or \'=\'.<br>\n  Find the secret number with <code>O(logS)</code> guesses, where <code>S</code> is the secret number.  You can use <code>find_secret(N1, N2)</code> which operate with <code>O(log(N2-N1))</code>.. What it does is simply a binary search.   </p>\n</blockquote>\n\n<p>So, the algorithm implemented (with Python) as follows:  </p>\n\n<pre><code>def find_secret2():\n    low = 1\n    high = 2\n    answer = guess(high)\n    while answer == "&gt;":\n        low *= 2\n        high *= 2\n        answer = guess(high)\n\n    if answer == "=":\n        return high\n    return find_secret(low, high)\n</code></pre>\n\n<p>my thoughts about the complexity of this algorithm:  </p>\n\n<p>it takes <code>O(logS)</code> to reach the range where <code>low &lt; secret &lt; high</code>.<br>\nthen, it takes <code>O(log(high-low))</code> - because we\'re using <code>find_secret(N1, N2)</code> method.</p>\n\n<p>I\'ll be glad if you could help me explain why the algorithm\'s complexity is <code>O(logS)</code> in a mathematical/rigorous way using the O-notation.<br>\nThanks!</p>\n', 'ViewCount': '55', 'Title': 'Runtime analysis of a "find the secret number" algorithm', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T20:39:11.993', 'LastEditDate': '2014-01-29T20:39:11.993', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20080', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13229', 'Tags': '<algorithms><algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-01-29T19:33:20.053', 'Id': '20078'},64_114:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>So I have this code:</p>\n\n<pre><code>  done &lt;- false                                     [1]\n  n &lt;- 0                                            [1]\n  while (n &lt; a) and (done = false)                  [(n+1)(1+1+1)]\n    done &lt;- true                                    [n]\n    for m &lt;- (a- 1) downto n                        [n(1+1+1+1)]\n       if list[m] &lt; list[m - 1] then                [n]\n         tmp &lt;- list[m]                             [n]\n         list[m] &lt;- list[m-1]                       [n]\n         list[m - 1] &lt;- tmp                         [n]\n         done &lt;- false                              [n]\n       n &lt;- n + 1                                   [1]\n  return list                                       [1]\n</code></pre>\n\n<p>Am I doing this right? My conclusions are that the inne for-loop runs (n^2 + n) / 2 times and the outher while-loop runs n+1 times. I don't know how to properly argue for that the bubble sort has the complexity O(n^2) </p>\n", 'ViewCount': '207', 'Title': 'Bubble sort complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T16:00:25.457', 'LastEditDate': '2014-01-31T14:19:33.393', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13277', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><loops>', 'CreationDate': '2014-01-31T13:41:59.570', 'Id': '20155'},64_115:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have an algorithm and I determined the asymptotic worst-case runtime, represented by Landau notation. Let's say $T(n) = O(n^2)$; this is measured in number of operations.</p>\n\n<p>But this is the worst case, how about in average? I tried to run my algorithm 1000 times for each $n$ from $1$ to $1000$.I get another graph which is the average running time against $n$ but measured in real seconds.</p>\n\n<p>Is there any possible way to compare these figures?</p>\n", 'ViewCount': '56', 'Title': 'Compare asymptotic WC runtime with measured AC runtime', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-02T14:37:42.050', 'LastEditDate': '2014-02-02T14:22:32.553', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11506', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis><average-case>', 'CreationDate': '2014-02-01T13:58:46.173', 'FavoriteCount': '1', 'Id': '20186'},64_116:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '94', 'Title': 'Which computational model is used to analyse the runtime of matrix multiplication algorithms?', 'LastEditDate': '2014-02-03T11:42:45.030', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2091', 'FavoriteCount': '1', 'Body': '<p>Although I have already learned something about the asymptotic runtimes of matrix multiplication algorithms (Strassen\'s algorithm and similar things), I have never found any explicit and satisfactory reference to a model of computation, which is used to measure this complexity. In fact, I have found three possible answers, neither of which seems to me as absolutely satisfactory:</p>\n\n<ul>\n<li><a href="http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations" rel="nofollow">Wikipedia</a> says that the model used here is the Multitape Turing Machine. This does not seem to make much sense to me, since in the analysis of matrix multiplication, scalar multiplication is supposed to have a constant time complexity. This is not the case on Turing Machines.</li>\n<li>Some texts describe the complexity only vaguely as the number of arithmetic operations used. However, what <em>exactly</em> are arithmetic operations in this context? I suppose that addition, multiplication, and probably subtraction. But what about division, integer division, remainder, etc.? And what about bitwise operations - how do they fit into this setting?</li>\n<li>Finally, I have recently discovered an article, which uses the <a href="http://en.wikipedia.org/wiki/Blum%E2%80%93Shub%E2%80%93Smale_machine" rel="nofollow">BSS machine</a> as the model of computation. However, this also seems little bit strange to me, since for, e.g., integer matrices, it does not make much sense to me to disallow operations such as, e.g., integer division.</li>\n</ul>\n\n<p>I would be grateful to anyone, who could help me to sort these things out.</p>\n', 'Tags': '<algorithm-analysis><runtime-analysis><matrices><machine-models>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-03T15:33:24.143', 'CommentCount': '7', 'AcceptedAnswerId': '20255', 'CreationDate': '2014-02-03T08:48:07.433', 'Id': '20245'},64_117:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to analyze the runtime of this algorithm:</p>\n\n<pre><code>int fun (int arr[], int n) {\n    int result = 1;\n    int i, j;\n\n    if (n == 1)\n        return 1;\n\n    else {\n            result = fun(arr, 2n/3);\n            for (i = 1; i &lt;= sqrt(n); i=i*2);\n                for (j=0; j&lt;sqrt(n)/i; j++)\n                    result += arr[j];\n\n            return result;\n    }\n}\n</code></pre>\n\n<p>I can see that the runtime recurrence should be something like </p>\n\n<p>$\\qquad\\displaystyle T(n) = T\\left(\\frac{2n}{3}\\right) + \\Theta(X)$</p>\n\n<p>where $X$ is the time of the extra operations per recursion.</p>\n\n<p>I can also see that the extra operations are:</p>\n\n<p>$\\qquad\\begin{align*}\n  \\sum_{i=1}^{\\log(\\sqrt{n})} \\sum_{j=0}^{\\frac{\\sqrt{n}}{i}}1 \n    &amp;= \\sum_{i=1}^{\\log(\\sqrt{n})}\\frac{\\sqrt{n}}{i} \\\\\n    &amp;= \\sqrt{n} \\cdot \\sum_{i=1}^{\\log(\\sqrt{n})} \\frac{1}{i} \\\\\n    &amp;= \\sqrt{n}\\cdot \\log(\\log(\\sqrt{n}))\n\\end{align*}$</p>\n\n<p>So all in all:</p>\n\n<p>$\\qquad\\begin{align*}\n  T(1) &amp;= 1 \\\\\n  T(n) &amp;= T\\left(\\frac{2n}{3}\\right) + \\sqrt{n}\\cdot \\log(\\log(\\sqrt{n}))\n\\end{align*}$</p>\n\n<p>But I could not continue from here to solve this recursion.</p>\n', 'ViewCount': '57', 'Title': 'Runtime analysis of a recursive algorithm with a tricky amount of work per recursive call', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-02-05T09:42:21.443', 'LastEditDate': '2014-02-05T09:42:21.443', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '21313', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10875', 'Tags': '<algorithm-analysis><runtime-analysis><recurrence-relation>', 'CreationDate': '2014-02-05T07:31:18.833', 'Id': '21311'},64_118:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>As per <a href="http://algs4.cs.princeton.edu/lectures/52Tries.pdf" rel="nofollow">Tries slides</a> (page 17) from Algorithm 4th edition book by Robert Sedgewick, the asymptotic expected runtime for an unsuccessful search in $R$-way tries miss is $O(\\log_R N)$. Can someone please explain how this number can be derived?</p>\n', 'ViewCount': '21', 'Title': 'Understanding expected time bound for unsuccessful search in R-way tries', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T16:21:49.280', 'LastEditDate': '2014-02-06T16:21:49.280', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21387', 'Score': '1', 'OwnerDisplayName': 'ManojGumber', 'PostTypeId': '1', 'Tags': '<data-structures><runtime-analysis><search-trees>', 'CreationDate': '2014-01-12T22:11:10.073', 'Id': '21386'},64_119:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am a high school student computationally studying the 3-dimensional structure of chromosomes by 40 kilobase loci. In a nutshell, loci that are close in space tend to express their genes at the same time --- loci are different stops on a 3D-winding DNA chain.</p>\n\n<p>The best way to understand the 3D structure is by gathering what are basically distances between loci.</p>\n\n<p>Now I have an $n\\times n$ ($n$ = number of loci studied) matrix where $(i,j)$ is the distance between locus $i$ and locus $j$. I also have a (somewhat miraculous) 3-dimensional of the same chromosome that maps each locus to a certain point in a 3D $(x,y,z)$ coordinate system.</p>\n\n<p>My task is to find all of the loci within a certain radius of locus $L$. With the matrix, I would have to go to $L$ and traverse many nearby locus-distance chains, possibly for a long time, before being any bit certain that I had everything I wanted (i.e. brute force). With the spatial model, I would only have to conduct a simple search within that radius.</p>\n\n<p>Here is my question. What is the complexity of finding nearby loci in the 3D model and the 2D matrix with respect to loci count and radius size (whichever you think is more complex)? (Compare the two complexities and give both)</p>\n\n<p>I am not very studied in CS, but here is what I guess:</p>\n\n<p>$$\nC_{2D search best-case} = O(n^2)\n$$\n$$\nC_{2D search worst-case} = O(2^n)\n$$</p>\n\n<p>Best-case is what you'd expect, and worst-case would be going through every permutation of the distance.</p>\n\n<p>$$\nC_{3D search any case} = O(n)\n$$</p>\n\n<p>This is just my rather fallible intuition.</p>\n", 'ViewCount': '57', 'Title': 'Time complexity of proximity search in distance matrix', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-17T19:14:00.307', 'LastEditDate': '2014-02-17T09:59:29.453', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '21721', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14736', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><ralgorithms>', 'CreationDate': '2014-02-17T00:27:33.090', 'Id': '21711'},64_120:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can I find the cost of pseudocode with a nested loop and a nested if statement?</p>\n\n<p><img src="http://i.stack.imgur.com/bk2g6.png" alt="Exercise"></p>\n\n<p>On the left hand side is an example from a textbook I am following. On the right hand side is pseudo code that I found. I took a guess, but I don\'t know what the time would be for the inner code fragments.</p>\n\n<p>I am especially unsure what the code segments inside the if statement would be because the if statement doesn\'t always occur.</p>\n', 'ViewCount': '86', 'Title': 'How to find the cost of pseudocode with a nested loop and a nested if statement?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-20T21:50:41.273', 'LastEditDate': '2014-02-20T11:11:16.393', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '21831', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '14864', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-02-19T23:42:49.297', 'Id': '21829'},64_121:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Are there dynamic programming examples that run in exponential time? Every example that I've seen so far constructs the top half of a matrix in a bottom-up fashion ($n^2$) from the base case and evaluates $n$ expressions to optimize each entry.</p>\n", 'ViewCount': '104', 'Title': 'Are there dynamic programming examples that run in exponential time?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-28T10:04:36.073', 'LastEditDate': '2014-02-28T08:38:41.003', 'AnswerCount': '3', 'CommentCount': '1', 'AcceptedAnswerId': '22101', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<algorithms><runtime-analysis><dynamic-programming>', 'CreationDate': '2014-02-27T16:33:50.553', 'Id': '22094'},64_122:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How many times does the statement count in line 5 executes in terms of $n$?</p>\n\n<pre><code>1.  count=0; \n2.  for (i=1; i&lt;=n; i++) { \n3.  for (j=1; j&lt;=n; j*=2) { \n4.  for (k=1; k&lt;=j; k++) {\n5.        count = count + 1;\n6.      }\n7.    }\n8.  }\n</code></pre>\n\n<p>For the loop in line 3, we can list the numbers for $j$ as, $2^0,2^1,...,2^{\\log n}$\nTherefore, we can refer to the iterations of the exponents as $r$. Doing so would lead to the summations below for counting the number of executions.</p>\n\n<p>$\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}\\sum_{k=1}^{j}1 =\\sum_{i=1}^{n}\\sum_{r=0}^{\\log n}j $</p>\n\n<p>I am stuck here, because $\\sum_{r=0}^{\\log n}$ depends on $j$ but I am not sure how to incorporate them.</p>\n\n<p>Thanks in advance.</p>\n', 'ViewCount': '20', 'Title': 'The number of executions of the count statement; how many?', 'LastActivityDate': '2014-03-01T05:18:02.957', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22154', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15072', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'CreationDate': '2014-03-01T04:34:54.633', 'Id': '22150'},64_123:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '39', 'Title': "What's the time complexity of this append method?", 'LastEditDate': '2014-03-14T16:03:43.027', 'AnswerCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '15687', 'Body': '<p>I made a method that appends a sequence to another sequence.</p>\n\n<p>So: (append [1,2,3] [4,5,6]) = [1,2,3,4,5,6]</p>\n\n<p><strong>CODE In C#</strong></p>\n\n<pre><code>IEnumerable&lt;int&gt; Append(IEnumerable&lt;int&gt; xs,IEnumerable&lt;int&gt; ys)\n{\n    using(var iteratorX = xs.GetEnumerator())\n    using(var iteratorY = ys.GetEnumerator())\n    {\n        bool isTrueForX = false;\n        bool isTrueForY = false;\n        while((isTrueForX = iteratorX.MoveNext()) || (isTrueForY = iteratorY.MoveNext()))\n        {\n            if(isTrueForX) yield return iteratorX.Current;\n            else if(isTrueForY) yield return iteratorY.Current;\n        }\n    }\n}\n</code></pre>\n\n<p>I would like to know what is the time-complexity of this algorithm.</p>\n', 'ClosedDate': '2014-03-14T16:28:00.820', 'Tags': '<algorithms><algorithm-analysis><time-complexity><runtime-analysis>', 'LastEditorUserId': '15687', 'LastActivityDate': '2014-03-14T16:03:43.027', 'CommentCount': '2', 'AcceptedAnswerId': '22617', 'CreationDate': '2014-03-14T11:37:38.200', 'Id': '22615'},64_124:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've searched online for this but I only seem to find answers for a similar equation: </p>\n\n<pre><code>T(n) = T(n/3) + T(2n/3) + cn\n</code></pre>\n\n<p>But the one I'm trying to solve is:</p>\n\n<pre><code>T(n) = T(n/3) + T(2n/3)\n</code></pre>\n\n<p>Base case: We can assume <code>T(a) = Theta(1)</code> for any constant <code>a</code>.</p>\n\n<p>I've succeeded in proving (by induction) that <code>T(n) = O(n*log(n))</code>. I thought the answer should be <code>Theta(n*log(n))</code>, but I cannot prove that <code>T(n) = Omega(n*log(n))</code>.</p>\n\n<p>So my question is - am I correct that the answer is <code>O(n*log(n))</code>, and NOT <code>Theta(n*log(n))</code>? IF that's true that would really be great...</p>\n\n<p>If I'm wrong I will of course explain where I'm stuck in the induction process...</p>\n\n<p>Thanks!</p>\n\n<p>P.S. If you need to, please try to explain using induction, because I haven't learned all methods for solving these problems yet.</p>\n", 'ViewCount': '64', 'ClosedDate': '2014-03-20T09:00:30.173', 'Title': 'Recurrence of T(n) = T(n/3) + T(2n/3)', 'LastActivityDate': '2014-03-19T21:53:26.597', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15868', 'Tags': '<algorithm-analysis><data-structures><runtime-analysis><recurrence-relation>', 'CreationDate': '2014-03-19T14:46:24.537', 'Id': '22809'},64_125:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I am reading Prim's MST for the first time and wanted to implement the fast version of it . </p>\n\n<p>$m$ - The number of edges in the graph </p>\n\n<p>$n$ - The number of vertices in the graph </p>\n\n<p>Here's the algorithm :</p>\n\n<p>1) Create a Min Heap of size $V$ where $V$ is the number of vertices in the given graph. Every node of min heap contains vertex number and key value of the vertex.</p>\n\n<p>2) Initialize Min Heap with first vertex as root (the key value assigned to first vertex is $0$ ). The key value assigned to all other vertices is $\\infty$ .</p>\n\n<p>3) While Min Heap is not empty, do following</p>\n\n<p>\u2026..a) Extract the min value node from Min Heap. Let the extracted vertex be u.</p>\n\n<p>\u2026..b) For every adjacent vertex $v$ of $u$, check if $v$ is in Min Heap (not yet included in MST). If $v$ is in Min Heap and its key value is more than weight of $u-v$, then update the key value of $v$ as weight of $u-v$.</p>\n\n<p>Now my point is during implementation ( I am doing in C++) in step 3(b) I have to check whether the vertex is there in the heap or not . As we know , searching in a heap is done in $O(n)$ time . So in the main while loop which will run ( $n$ number of times ) although extract-min is $O(\\log n)$ but the search ( whether $v$ is min heap or not takes time proportional to size of the heap ( although it is decreasing in each step ) . </p>\n\n<p>So is it correct to say that the above algorithm is $O(m+n\\log n)$</p>\n", 'ViewCount': '86', 'Title': "Prim's Minimum Spanning Tree implementation $O(mn)$ or $O(m+n \\log n)$?", 'LastEditorUserId': '15879', 'LastActivityDate': '2014-03-19T19:06:06.820', 'LastEditDate': '2014-03-19T18:56:58.260', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15879', 'Tags': '<algorithms><graph-theory><algorithm-analysis><graphs><runtime-analysis>', 'CreationDate': '2014-03-19T18:20:22.067', 'Id': '22817'},64_126:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the following code segment :</p>\n\n<pre><code>for (int i = 1; i &lt;= n; i++ ) {\n    for (int j = 1; j &lt;= n; j = j + i ) {\n          printf("Hi");\n    }\n}\n</code></pre>\n\n<p>Here, the outer loop will execute $ n $ times, but the execution of inner loop depends upon the value of $ i $.  </p>\n\n<ul>\n<li>When $ i = 1 $ inner loop will execute $ n $ times.</li>\n<li>When $ i = 2 $ inner loop will execute $ \\frac{n}{2} $ times.</li>\n<li>When $ i = 3 $ inner loop will execute $ \\frac{n}{3} $ times.<br>\n$ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\vdots  $</li>\n<li>When $ i = n $ inner loop will execute $ 1 $ time  </li>\n</ul>\n\n<p>So complexity will be given by<br>\n$$\n \\begin{align}\n  T(n) &amp;= \\frac{n}{1} + \\frac{n}{2} + \\frac{n}{3} + \\cdots + \\frac{n}{n}\\\\\n\\\\\n       &amp;= n \\left( 1 + \\frac{1}{2} + \\frac{1}{3} + \\cdots + \\frac{1}{n} \\right) \\\\\\\\\n       &amp;= n \\sum_{k = 1}^{n} { \\frac{1}{k} }\n \\end {align}\n $$\nI am not able to solve $ \\sum_{k=1}^{n} \\frac{1}{k} $. Upon searching I found that it is the $ n^{th} $ Harmonic number ( $ H_n $), but couldn\'t find any closed formula for it. How can I proceed further to calculate $ T(n) $?</p>\n', 'ViewCount': '84', 'Title': 'Calculating time complexity of two interdependent nested for loops', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T08:06:56.143', 'LastEditDate': '2014-03-31T08:06:56.143', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22825', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11131', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-03-19T20:10:26.453', 'Id': '22823'},64_127:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose an algorithm goes through a list of n integers and for every iteration of the loop it is needs to check if the current evaluated element of the list is even. If it is even, return the index of the integer that is evaluated as even.</p>\n\n<p>How come the algorithm would have 2n+1 comparison?</p>\n\n<p>I thought linear search would have n comparision because it is going through n elements. +1 comparison for the if statement. So that would make the algorithm O(n+1) comparison, no?. Where did the extra n come from?</p>\n\n<p>Pseudo-code:</p>\n\n<pre><code>procedure last_even_loc(a1,a2,...,an:integers);\nlocation = 0;\nfor i = 1 to n\n\n    if (a_i = 0) (mod 2) then location = i\n\nreturn location;\n</code></pre>\n', 'ViewCount': '66', 'Title': 'Why is there a 2n+1 comparison for a linear search algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-23T11:52:41.557', 'LastEditDate': '2014-03-23T11:52:41.557', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22853', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15555', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><sorting>', 'CreationDate': '2014-03-20T03:45:36.143', 'Id': '22849'},64_128:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I believe I understand the concepts of algorithm analysis. However, I'm not fully confident in applying those concepts. I'd appreciate help in bridging the gap between concept and application. </p>\n\n<p>I understand that if $f(n) \\in O(g(n))$ and $f(n) \\in \\Omega(g(n))$ then $f(n) \\in \\Theta(g(n))$</p>\n\n<p>Essentially, when Big-O and Big-Omega form the upper and lower bounds of algorithmic performance, Big-Theta represents the optimal solution. But how does one determine those bounds and thus optimal performance? </p>\n\n<p>How does/should one determine <em>Big-O, Big-Omega, Big-Theta</em> for the following algorithm?</p>\n\n<pre><code>int summation(int[] values)\n{\n  int sum = 0;\n  for(int i = 0; i &lt; values.length; i++)\n  {\n    sum += values[i];\n  }\n}\n</code></pre>\n", 'ViewCount': '35', 'ClosedDate': '2014-03-29T11:47:10.477', 'Title': 'Analysis of Algorithms: Applying Concepts', 'LastEditorUserId': '16252', 'LastActivityDate': '2014-03-28T21:55:52.333', 'LastEditDate': '2014-03-28T21:55:52.333', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16252', 'Tags': '<algorithm-analysis><asymptotics><runtime-analysis>', 'CreationDate': '2014-03-28T21:14:52.010', 'Id': '23192'},64_129:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to work through various exercises in Skiena\'s "Algorithm Design Manual." One problem that I am stuck on is as follows:</p>\n\n<blockquote>\n  <p>What value is returned by the following function? Express your answer as a function of n. Give the worst-case running time using Big Oh notation.</p>\n\n<pre><code>function conundrum(n)\n    r:=0\n    for i:=1 to n do\n        for j:=i+1 to n do\n            for k:=i+j-1 to n do\n                r:=r+1\n    return(r)\n</code></pre>\n</blockquote>\n\n<p>My first attempt at this problem began with trying to solve it when $n$ is even and my first observation was that $r$ is never incremented for any value of $i$ greater than $n/2$ (I am assuming that a statement along the lines of "$k:=l$ to $n$" is not executed for any $l&gt;n$. Indeed, upon working out the above function for several different values of $n$, I thought that I could intuit that the sum calculated by the function had the following form $\\sum_{p=1}^{n/2}\\sum_{q=1}^{2p-1}q$-- to intuit this, I began the first loop at $i=n/2$ and worked backwards. Working this sum out, I got $\\frac 1 {24} n(n(2n-3)-26)$. To my dismay, upon checking this answer, I found that the correct answer for even $n$ is $\\frac 1 {24} n(n+2)(2n-1)$. Any thoughts or hints about what I am doing wrong?</p>\n', 'ViewCount': '23', 'ClosedDate': '2014-03-29T12:00:47.007', 'Title': 'Complexity of a nested for loop', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T11:58:37.577', 'LastEditDate': '2014-03-29T11:58:37.577', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16262', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-03-29T08:30:15.913', 'Id': '23215'},64_130:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I came across the following problem in a exam. </p>\n\n<p>We choose a permutation of n elements $[1,n]$ uniformly at random. Now a variable MIN holds the minimum value seen so far at it is defined to $\\infty$ initially. Now during our inspection if we see a smaller value than MIN, then MIN is updated to the new value. </p>\n\n<p>For example, if we consider the permutation, </p>\n\n<p>$$5\\ 9\\ 4\\ 2\\ 6\\ 8\\ 0\\ 3\\ 1\\ 7$$\nthe MIN is updated 4 times as $5,4,2,0$. Then the expected no. of times MIN is updated?</p>\n\n<p>I tried to find the no. of permutations, for which MIN is updated $i$ times, so that I can find the value by $\\sum_{i=1}^{n}iN(i)$, where $N(i)$, is the no. of permutations for which MIN is updated $i$ times.</p>\n\n<p>But for $i\\geq2$, $N(i)$ is getting very complicated and unable to find the total sum.</p>\n', 'ViewCount': '190', 'Title': 'Expected number of updates of minimum', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T22:33:27.977', 'LastEditDate': '2014-03-31T17:47:01.783', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16323', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms>', 'CreationDate': '2014-03-31T15:20:35.840', 'Id': '23295'},64_131:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What are the time complexities of the following code?  I posted this on the general stackexchange website, but it was suggested that I post it here.</p>\n\n<pre><code>def func(n):\n    for _ in range(n):\n        if n == 4:\n            for _ in range(n):\n                &lt;O(1) operation&gt;\n</code></pre>\n\n<p>It will only be O(n^2) for one specific input (n = 4) but is O(n) for all other inputs. In this case the worst case is obviously O(n^2), yet my instructor says that O(n) is the correct answer. If "big-Oh" notation is to indicate the worst case scenario, why is it not O(n^2)?</p>\n\n<p>Another one is:</p>\n\n<pre><code>def func2(n):\n    for _ in range(n):\n        if n%2 == 0:\n            for _ in range(n):\n                &lt;O(1) operation&gt;\n</code></pre>\n\n<p>I am not so certain about the run time of this piece of code. Again, worst case is O(n^2). This time half of all possible inputs results in the worst case. Would this suffice in saying that the code runs in O(n^2) time?</p>\n\n<p>If the first part is O(n) and the second part is O(n^2), is there a general rule of thumb when you choose the truly worst case for the "big-Oh" representation?</p>\n', 'ViewCount': '15', 'ClosedDate': '2014-04-01T07:30:59.673', 'Title': 'Time complexity of complex nested for loops', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-01T07:29:16.013', 'LastEditDate': '2014-04-01T07:29:16.013', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '16345', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-04-01T04:51:22.973', 'Id': '23311'},64_132:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '47', 'Title': "Why don't we calculate swaps and other steps except comparison for finding time complexity of a sorting algorithm?", 'LastEditDate': '2014-04-02T15:14:24.200', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14498', 'Body': '<p>I was learning some basic sorting techniques with their complexity. However I cannot understand why only the number of comparisons are taken into account while calculating time complexity and operations such as swap are ignored. <a href="http://en.wikipedia.org/wiki/Selection_sort#Analysis" rel="nofollow">Link to selection sort analysis</a>. Please help me understand.</p>\n', 'ClosedDate': '2014-04-02T15:14:58.270', 'Tags': '<algorithm-analysis><runtime-analysis><sorting>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-02T15:14:24.200', 'CommentCount': '2', 'AcceptedAnswerId': '23341', 'CreationDate': '2014-04-02T13:06:10.697', 'Id': '23339'},64_133:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>As a programmer with non CS background, I am learning algorithms.</p>\n\n<p>When explaining the performance of quicksort in an <a href="http://www.amazon.de/Algorithms-Robert-Sedgewick/dp/032157351X/ref=sr_1_1?ie=UTF8&amp;qid=1394794069&amp;sr=8-1&amp;keywords=algorithm" rel="nofollow">Algorithm book</a> and also elsewhere on the web, I do not see any reference to the time/space needed for shuffling. I <em>do</em> understand that shuffling is important to have the $O(n\\log n)$ performance, but shuffling itself would have a complexity of $O(n)$. Why does this become irrelevant in the total performance?</p>\n', 'ViewCount': '165', 'Title': 'Performance impact due to time required for shuffling in Quicksort', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-03T22:02:32.870', 'LastEditDate': '2014-04-03T22:02:32.870', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '23394', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '16409', 'Tags': '<time-complexity><runtime-analysis><quicksort>', 'CreationDate': '2014-04-03T09:34:36.643', 'Id': '23387'},64_134:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let me ask my general question using a specific example, namely range searching:</p>\n\n<p>Given a set of points in the plane and an axis parallel rectangle, report all points lying in the rectangle. </p>\n\n<p>If the rectangle contains $k$ points, then this can be solved in $O(\\log n + k)$ time (after some preprocessing). I understand that this is in some sense optimal, but what if the points obey some regularity?</p>\n\n<p>Obviously if they are from the integer grid, this can be solved in constant time.</p>\n\n<p>What is less obvious to me is the situation where the points lie equally spaced on some complicated curves: I could report to the user a set of curve-intervals tuples from which he can deduce the number of points. </p>\n\n<p>But am I actually still solving the problem, or am I just outsourcing the work to the user? </p>\n\n<p>I realize that the question is not very precise and I am not sure if such topics have been studied before. \nI would be interested in any literature reference, where algorithms produce output in a non-explicit way.</p>\n', 'ViewCount': '26', 'Title': 'Lower-bounds of running-time for output sensitive Algorithms', 'LastActivityDate': '2014-04-04T23:00:00.927', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16358', 'Tags': '<reference-request><time-complexity><runtime-analysis>', 'CreationDate': '2014-04-04T23:00:00.927', 'Id': '23438'},64_135:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What is the time complexity of Halley\'s Method? </p>\n\n<p>I am thinking ${\\cal O}(\\log(n)F(n))$, or something very similar to Newton-Raphson, but I feel as though there should be some change to the complexity in order to yield the greater convergence. However, I can\'t seem to find any solid ground to support any of my ideas (as nobody has ever seemed to have asked this question on the internet before)...</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Halley%27s_method" rel="nofollow">Halley\'s Method - Wikipedia</a></p>\n', 'ViewCount': '29', 'Title': "Time Complexity of Halley's Method", 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-16T01:10:02.363', 'LastEditDate': '2014-04-11T15:25:57.560', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16683', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis>', 'CreationDate': '2014-04-11T14:05:23.880', 'Id': '23674'},64_136:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have the following simple algorithm to find duplicate characters in a string:</p>\n\n<pre><code>for i = 1 -&gt; n\n    for j = i + 1 -&gt; n\n        if A[i] == A[j] return true\nreturn false \n</code></pre>\n\n<p>Why is the running time of this algorithm $\\mathcal{O}(n^2)$?\nIf the first iteration is $n$ steps then, $n-1, n-2,n-3,..,1$ it seems to me that \nadding all these would never be $n^2$ or am I wrong?</p>\n', 'ViewCount': '48', 'Title': 'Confusion with the Running Time of an algorithm that finds duplicate character', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-14T07:47:17.597', 'LastEditDate': '2014-04-14T07:47:17.597', 'AnswerCount': '3', 'CommentCount': '2', 'AcceptedAnswerId': '23749', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16747', 'Tags': '<terminology><asymptotics><runtime-analysis><landau-notation>', 'CreationDate': '2014-04-13T19:40:14.433', 'Id': '23748'},64_137:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose we have an $N \\times N \\times N$ 3-d sorted array meaning that every row,column, and file is in sorted order. Searching for an element in this structure can be done using $O(N^2)$ comparisons. However are $\\Omega(N^2)$ comparisons needed in the worst-case? For an $N \\times N$ 2-d sorted array I recall a proof that $\\Omega(N)$ comparisons are needed; I'm having trouble seeing how to extend to the 3-d case though</p>\n", 'ViewCount': '24', 'Title': 'Lower bound on number of comparisons needed to search for a number in a sorted 3-d array', 'LastEditorUserId': '9584', 'LastActivityDate': '2014-04-14T18:45:49.407', 'LastEditDate': '2014-04-14T18:28:24.510', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23793', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9584', 'Tags': '<algorithm-analysis><runtime-analysis><search-algorithms><arrays><lower-bounds>', 'CreationDate': '2014-04-14T17:58:35.807', 'Id': '23791'},64_138:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Here's the code for the algorithm:</p>\n\n<pre><code>Foo(n)\n   lcm = 1\n   for i = 2 to n\n       lcm = lcm*i/Euclid(lcm,i)\nreturn lcm\n</code></pre>\n\n<p>The running time of <code>Euclid</code>$(a, b)$ is given as $O(\\log(\\min(a, b)))$</p>\n\n<p>So the running time of the for loop will be $O(n)$, so would this be the final running time? or do I have to take the $O(\\log(\\min(a, b)))$ into account as well?</p>\n", 'ViewCount': '82', 'Title': 'Big O running time for this algorithm?', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-04-16T07:27:49.683', 'LastEditDate': '2014-04-16T07:27:49.683', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '23836', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16810', 'Tags': '<algorithms><runtime-analysis>', 'CreationDate': '2014-04-15T23:26:54.843', 'Id': '23835'},64_139:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was working on a programming puzzle were given a list of words, the anagrams should be printed together. For example, <code>{"cat", "bat", "act", "tab", "mat", "listen","silent"}</code> should result in <code>[listen, silent], [mat], [bat, tab], [cat, act]</code>.</p>\n\n<p>I have the following pseudocode:</p>\n\n<pre><code>mapStrings():\n For each string in the string array:\n    Find a key for string using a hash function h() \n    If that key does not exist in the hash table, add the key \n    If the key exists, append its value with the string\n\n Print value for each key\n\nh():\n  sum = 0\n  For each character in string:\n    Add ASCII value of the character to sum\n  return sum\n</code></pre>\n\n<p>The time complexity of the for loop in the <code>mapStrings()</code> method would be $\\cal O(n)$ as it goes over each element in the string array of length n. </p>\n\n<p>The call to the method <code>h()</code> would be $\\cal O(m)$, where $m = m_{1} + m_{2} + ... + m_{n}$ is the sum of lengths of each string in the string array.</p>\n\n<p>Hence the total time complexity would be $\\cal O(n+m)$ = $\\cal O(N)$, where $N = n+m$.</p>\n\n<ol>\n<li>Are my statements correct?  </li>\n<li>If it is, then what would the time complexity\nbe if the method <code>h()</code>\'s complexity was $\\cal O(n^2)$ instead of $\\cal O(n)$? Would it be\n$\\cal O(n^2 + m) \\approx \\cal O(n^2)$?</li>\n</ol>\n', 'ViewCount': '13', 'ClosedDate': '2014-04-24T20:14:46.580', 'Title': 'Runtime of adding strings to a hashtable', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-24T20:41:25.567', 'LastEditDate': '2014-04-24T20:41:25.567', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5092', 'Tags': '<algorithm-analysis><runtime-analysis><loops>', 'CreationDate': '2014-04-24T20:12:24.817', 'Id': '24087'},64_140:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm using mat lab's implementation of hierarchical clustering algorithm, with pdist, linkage , cophenet, dendrogram, and cluster. But I'm not exactly sure where or how to find the time complexity</p>\n\n<p>It is an agglomerative type of hierarchal clustering, and although they're usually \n$O(n^3)$\nthis one in particular works pretty fast for 177 data points of 3 features (177x3), which got me curious. Is it $n^3$ or an optimal version?</p>\n", 'ViewCount': '15', 'Title': "What is the time complexity of Matlab's Hierarchical Clustering?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T11:39:59.340', 'LastEditDate': '2014-04-30T11:39:59.340', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6980', 'Tags': '<algorithms><runtime-analysis><cluster>', 'CreationDate': '2014-04-29T16:57:40.817', 'Id': '24229'}