{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p><a href="http://www.scholarpedia.org/article/Kolmogorov-Sinai_entropy" rel="nofollow">Kolmogorov-Sinai entropy</a> (KS) explains the mathematical concept behind KS entropy. </p>\n\n<p>$$h ( T ) =\\sup\\limits_{\\xi} \\, h ( T , \\xi )$$</p>\n\n<p>defines the formula for KS where the left-hand side is nothing but the Shannon\'s entropy. In many papers I have seen that KS is defined as the supremum over source entropy. These conflicting views raises the following questions</p>\n\n<ol>\n<li>What is the difference between Shannon\'s entropy and source entropy (source entropy given by $\\lim \\frac{1}{n}\\cdot h(T)$ where $n$ is the length of the word or sequence,unsure though). </li>\n<li><p>Would source entropy be the entropy of the raw signal before it has been quantized or should it be that of the quantized?</p></li>\n<li><p>Is \'n\' the length of the data series or the number of quantization levels></p></li>\n<li><a href="http://mathworld.wolfram.com/TopologicalEntropy.html" rel="nofollow">Topological entropy</a> (TS) explains topological entropy whose formula looks exactly as KS. So, what is the difference since KS is also known as metric entropy and not known as Topological entropy?</li>\n</ol>\n', 'ViewCount': '174', 'Title': 'Source entropy and other questions related to information theory', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T15:06:28.490', 'LastEditDate': '2012-10-23T02:38:01.893', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '783', 'Tags': '<reference-request><information-theory><entropy>', 'CreationDate': '2012-07-25T00:41:25.497', 'Id': '2902'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>We\'re running some benchmarks for an approximative query-answering system. It\'s sufficient to just think of it as running some SQL queries with joins. We are counting the results returned as part of the benchmark. However, the results often contain a lot of redundancy, so just counting results seems coarse.</p>\n\n<p>Consider the following table containing results for a query like "<em>for the US, give me its states and its car manufacturers</em>":</p>\n\n<pre><code>================================\n|| ?us_state | ?us_car_manu   ||\n||============================||\n|| Alabama   | Chrysler       ||\n|| Alaska    | Chrysler       ||\n|| ...         ...            ||\n|| Wyoming   | Chrysler       ||\n|| Alabama   | General Motors ||\n|| Alaska    | General Motors ||\n|| ...         ...            ||\n|| Wyoming   | General Motors ||\n|| Alabama   | Ford           ||\n|| Alaska    | Ford           ||\n|| ...         ...            ||\n|| Wyoming   | Ford           ||\n===============================\n</code></pre>\n\n<p>All 200 (50 \xd7 4) results are of course unique. However, given that there is an inherent Cartesian product, the number of results flatters the amount of "information content" or "entropy" of the table: every additional car manufacturer adds fifty results for the fifty US states. (Again, this is just an example; I\'m not interested in better ways to represent or run this particular query.)</p>\n\n<p>As such, we\'re looking for a metric that will give an indication as to the (loosely speaking) redundancy-free content in the table for better comparison of <em>content</em> across different results for different queries. Other result tables may contain a mix of different types of Cartesian products (e.g., consider generalising the query to any country, where each country itself has its own product of states and car manufacturers, etc.).</p>\n\n<p>Currently we\'re working off a simple metric which just counts unique term\u2013position combinations: for the above example, the metric gives 50 + 4 = 54. This may be sufficient for comparison, but is not sensitive to the combination of terms for individual results.</p>\n\n<p>Thanks to Wikipedia, I\'m aware of\u2014but not familiar with\u2014the notion of <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="nofollow">entropy in information theory</a>. However, I\'m unclear on how the concept of entropy could be applied to this use-case. (I\'m not interested in the entropy of the result strings; each term can be considered a "symbol".) Roughly speaking, each query variable could be considered as a free variable with the result terms in that column providing a set of possible outcomes and their frequency of occurrence being used as a probability mass function. This way I could compute the Shannon entropy for each column. But thereafter, I don\'t know how columns can be combined, or how tuples or results can be considered ... if a notion of conditional entropy would be better, etc.</p>\n\n<p>And so ...</p>\n\n<blockquote>\n  <p>Does anyone have pointers to related material on the measure of entropy/redundancy/etc. in tables or similar structures? </p>\n  \n  <p>Otherwise, does anyone have any ideas on how to use Shannon entropy in a convincing way for tabular data?</p>\n</blockquote>\n', 'ViewCount': '227', 'Title': 'Measuring entropy for a table (e.g., SQL results)', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T02:38:23.283', 'LastEditDate': '2012-10-23T02:38:23.283', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2357', 'Tags': '<reference-request><information-theory><data-compression><database-theory><entropy>', 'CreationDate': '2012-08-03T20:36:20.237', 'FavoriteCount': '1', 'Id': '3029'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Shannon's entropy [plog(1/p)] for an image is a probabilistic method for comparing two pixels or a group of pixels.Suppose an image with a  matrix of 3x3 has pixel intensity values</p>\n\n<pre><code>1 1 2\n2 3 3\n4 4 5\n</code></pre>\n\n<p>and another image with 3x3 matrix has group of pixels having intensity values</p>\n\n<pre><code>5 5 6\n6 7 7\n8 8 9\n</code></pre>\n\n<p>Then shannon's entropy for the images would be the same.So in this case the entropy values would point out that the images are same though in actual they are different.So image matching using this technique doesn't help.On basis of supervised classification where I classify an image based on trained databases of shannon's entropy ,we use the concept of entropy to find similarity between two images.Is there any method or research paper where this entropy can be used or modified for image matching for the above case..?</p>\n", 'ViewCount': '2300', 'Title': "Shannon's entropy for an image", 'LastEditorUserId': '157', 'LastActivityDate': '2013-10-11T21:42:02.737', 'LastEditDate': '2012-10-23T02:37:47.963', 'AnswerCount': '3', 'CommentCount': '7', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '3093', 'Tags': '<pattern-recognition><image-processing><entropy><computer-vision>', 'CreationDate': '2012-10-07T20:51:48.457', 'Id': '4935'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>The only examples I've seen use bits as a measurement of entropy, but all these examples happen to use binary code alphabets. If we wanted to see how well a coding with a code alphabet of length n works, would we measure entropy in units of n?</p>\n\n<p>Or would it make sense to stay using bits if we're comparing codings with binary and n-length code alphabets?</p>\n", 'ViewCount': '308', 'Title': 'What units should Shannon entropy be measured in?', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T02:37:05.093', 'LastEditDate': '2012-10-23T02:37:05.093', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6243', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1554', 'Tags': '<information-theory><entropy>', 'CreationDate': '2012-10-22T18:53:17.060', 'Id': '6237'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I'm reading a paper that refers to the limit as n goes to infinity of R\xe9nyi entropy. It defines it as ${{H}_{n}}\\left( X \\right)=\\dfrac{1}{1-n} \\log_2 \\left( \\sum\\limits_{i=1}^{N}{p_{i}^{n}} \\right)$. It then says that the limit as $n\\to \\infty $ is $-\\log_2 \\left( p_1 \\right)$. I saw another article that uses the maximum of the ${{p}_{i}}'s$ instead of ${{p}_{1}}$. I think that this works out fairly easily if all of the ${{p}_{i}}'s$ are equal (a uniform distribution). I have no idea how to prove this for anything other than a uniform distribution. Can anyone show me how it's done?</p>\n", 'ViewCount': '191', 'Title': u'R\xe9nyi entropy at infinity or min-entropy', 'LastEditorUserId': '157', 'LastActivityDate': '2012-10-23T03:33:07.530', 'LastEditDate': '2012-10-23T02:36:30.930', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6248', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2016', 'Tags': '<information-theory><entropy>', 'CreationDate': '2012-10-22T21:39:37.093', 'Id': '6244'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Let $P$ be a transition matrix of a random walk in an undirected <strong>(may not regular)</strong> graph $G$. Let $\\pi$ be a distribution on $V(G)$. The Shannon entropy of $\\pi$ is defined by </p>\n\n<p>$$H(\\pi)=-\\sum_{v \\in V(G)}\\pi_v\\cdot\\log(\\pi_v).$$</p>\n\n<p>How do we prove that $H(P\\pi)\\ge H(\\pi)$ ?</p>\n', 'ViewCount': '79', 'Title': 'Increasing entropy of random walk', 'LastEditorUserId': '4706', 'LastActivityDate': '2012-11-24T10:17:50.253', 'LastEditDate': '2012-11-24T10:17:50.253', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6862', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4706', 'Tags': '<entropy><random-walks>', 'CreationDate': '2012-11-23T17:39:47.947', 'Id': '6859'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '278', 'Title': 'Why is the Shannon entropy 0.94 in this example?', 'LastEditDate': '2013-02-06T02:47:56.627', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2860', 'FavoriteCount': '1', 'Body': '<p>Suppose I have a decision tree in which there is a label $L$ under which is the attribute $A$ as shown below. I am given that the Shannon entropy of label $L$ is $H(L) = 0.95$.</p>\n\n<p><img src="http://i.stack.imgur.com/21iRT.png" alt="enter image description here"></p>\n\n<p>I must find the Shannon entropy of $L$ given $A$ ($H(L \\mid A)$). Here\'s what I have tried.</p>\n\n<p>\\begin{eqnarray}\nH(L \\mid A) &amp;=&amp; -(\\frac{6}{8} \\log_2 \\frac{4}{6} + \\frac{2}{8} \\log_2 \\frac{1}{2}) \\\\\n &amp;\\approx&amp; 0.69\n\\end{eqnarray}</p>\n\n<p>However, $H(L \\mid A) \\approx 0.94$. Where did I err? Is my formula for Shannon entropy accurate? </p>\n', 'Tags': '<artificial-intelligence><entropy>', 'LastEditorUserId': '2860', 'LastActivityDate': '2013-02-06T03:06:49.900', 'CommentCount': '4', 'AcceptedAnswerId': '9535', 'CreationDate': '2013-02-06T00:53:15.047', 'Id': '9532'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am learning about information theory and mutual information. However, I am quite confused with MI(Mutual information) vs. PMI(Pointwise mutual information) especially signs of MI and PMI values. Here are my questions.    </p>\n\n<ul>\n<li><p>Is MI values a non-negative value or it can be either positive or negative? If it is always a non-negative value, why is it ?</p></li>\n<li><p>As I search online, the PMI can be positive or negative values and the MI is the expected value of all possible PMI. However, expected value can be positive or negative. If MI is really the expected value of PMI, why is it always positive ?</p></li>\n</ul>\n\n<p>Did I misunderstand anything of MI and PMI here ? Thank you very much,</p>\n', 'ViewCount': '320', 'Title': 'Pointwise mutual information vs. Mutual information?', 'LastActivityDate': '2013-03-09T07:54:28.433', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10401', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7161', 'Tags': '<probability-theory><information-theory><entropy>', 'CreationDate': '2013-03-09T00:08:48.040', 'Id': '10396'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>In many papers I've read that it is well known that the Shannon entropy of a random variable can be converted to min-entropy (up to small statistical distance) by taking independent copies of the variable.\nCan anyone explain to me what exactly this means?</p>\n", 'ViewCount': '182', 'Title': 'Shannon Entropy to Min-Entropy', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-29T02:00:25.320', 'LastEditDate': '2013-03-28T11:59:20.143', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10881', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7468', 'Tags': '<terminology><probability-theory><entropy>', 'CreationDate': '2013-03-28T09:38:33.157', 'Id': '10862'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '300', 'Title': "What's harder: Shuffling a sorted deck or sorting a shuffled one?", 'LastEditDate': '2013-04-14T15:00:30.950', 'AnswerCount': '2', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '4197', 'FavoriteCount': '1', 'Body': '<p>You have an array of $n$ distinct elements. You have access to a comparator (a black box function taking two elements $a$ and $b$ and returning true iff $a &lt; b$) and a truly random source of bits (a black box function taking no arguments and returning an independently uniformly random bit). Consider the following two tasks:</p>\n\n<ol>\n<li>The array is currently sorted. Produce a uniformly (or approximately uniformly) randomly selected permutation.</li>\n<li>The array consists of some permutation selected uniformly at random by nature. Produce a sorted array.</li>\n</ol>\n\n<p>My question is</p>\n\n<blockquote>\n  <p>Which task requires more energy asymptotically?</p>\n</blockquote>\n\n<p>I am unable to define the question more precisely because I don\'t know enough about the connection between information theory, thermodynamics, or whatever else is needed to answer this question. However, I think the question can be made well-defined (and am hoping someone helps me with this in an answer!).</p>\n\n<p>Now, algorithmically, my intuition is that they are equal. Notice that every sort is a shuffle in reverse, and vice versa. Sorting requires $\\log n! \\approx n \\log n$ comparisons, while shuffling, since it picks a random permutation from $n!$ choices, requires $\\log n! \\approx n \\log n$ random bits. Both shuffling and sorting require about $n$ swaps.</p>\n\n<p>However, I feel like there should be an answer applying <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle">Landauer\'s principle</a>, which says that it requires energy to "erase" a bit. Intuitively, I think this means that sorting the array is more difficult, because it requires "erasing" $n \\log n$ bits of information, going from a low-energy, high-entropy ground state of disorder to a highly ordered one. But on the other hand, for any given computation, sorting just transforms one permutation to another one. Since I\'m a complete non-expert here, I was hoping someone with a knowledge of the connection to physics could help "sort" this out!</p>\n\n<p>(The question didn\'t get any answers on <a href="http://math.stackexchange.com/questions/359911/which-takes-more-energy-shuffling-a-sorted-deck-or-sorting-a-shuffled-one">math.se</a>, so I\'m reposting it here. Hope that is ok.)</p>\n', 'Tags': '<algorithms><algorithm-analysis><information-theory><entropy>', 'LastEditorUserId': '6716', 'LastActivityDate': '2013-04-21T14:19:51.400', 'CommentCount': '15', 'AcceptedAnswerId': '11452', 'CreationDate': '2013-04-14T03:49:03.497', 'Id': '11299'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>We have well established theory for measuring the information content and randomness of binary strings. Notions such as Shanon entropy and Kolmogorov-complexity were developed for binary strings.</p>\n\n<p>For a binary square matrix, it is not sufficient to just convert the matrix into binary string and measure its information content or its randomness since naive unraveling of the binary matrix into binary string would lose the adjacency information in each row and in each column.</p>\n\n<p>My question: What are the analogous notions for measuring the information content and randomness of binary square matrix?</p>\n', 'ViewCount': '111', 'Title': 'Notions of information content and randomness of binary square matrix', 'LastEditorUserId': '96', 'LastActivityDate': '2013-04-27T14:40:58.330', 'LastEditDate': '2013-04-27T13:53:24.110', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '96', 'Tags': '<information-theory><entropy>', 'CreationDate': '2013-04-27T13:39:52.133', 'Id': '11601'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a constrainted optimization problem in the (Shannon) matrix entropy $\\mathtt{(sum(entr(eig(A))))}$. The matrix $A$ can be written as the sum of rank 1 matrices of the form $[v_i\\,v_i^T]$ where $v_i$ is a given normalized vector. The coefficients of the rank one matrices are the unknowns in which we optimize and they have to be larger than zero and sum up to 1.</p>\n\n<p>In a CVX-like syntax the problem goes as follows:\ngiven variable $\\mathtt{c(n)}$ </p>\n\n<p>$$\\text{minimize} \\qquad \\mathtt{sum(entr(eig(A)))}$$ </p>\n\n<p>$$\\begin{align} \\text{subject to} \\qquad A &amp;= \\sum c_i v_i v_i^T\\\\ \n\\sum c_i &amp;= 1\\\\ c_i &amp;\\ge 0\\end{align}$$.</p>\n\n<p>Does any have an idea how to solve this efficiently? I already know it probably can't be cast as a semi-definite programming (SDP) problem.</p>\n", 'ViewCount': '274', 'Title': 'Constrainted Optimization Problem in Matrix Entropy', 'LastEditorUserId': '903', 'LastActivityDate': '2013-09-28T08:34:27.347', 'LastEditDate': '2013-08-01T17:56:02.527', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '14575', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '9430', 'Tags': '<optimization><entropy>', 'CreationDate': '2013-07-30T16:05:17.310', 'Id': '13522'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is a kind of follow-up to a question I asked on superuser, where I asked for the definitions of a \'distinghuisable state\' and a \'memory cell\'. My questions where properly answered, but I was still confused about a certain matter.</p>\n\n<p>I\'m reading <a href="http://large.stanford.edu/courses/2012/ph250/kumar1/" rel="nofollow">this little paper</a>, and in particular these sentences:</p>\n\n<blockquote>\n  <p>To perform useful computation, we need to irreversibly change distinguishable states of memory cell(s) .... So the energy required to write information into one binary memory bit is $E_{bit}= k_BT \\ln2$</p>\n</blockquote>\n\n<p>So my interpretation of this is that the author says that to compute, you need to change the state of bits ($ 0 \\rightarrow 1$, $1 \\rightarrow 0$) in order to compute.</p>\n\n<p>But is that actually a reasonable statement? I\'m not really interested in the rest of the paper, only this part. Do computers (I\'m not talking about supercomputers, or future computers which use qubits and whatnot, but just your average computer) compute using bits with 2 dinstinghuisable states?</p>\n\n<p>I actually was able to find this so-called \'SNL-expression\' somewhere else, namely <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-701-introduction-to-nanoelectronics-spring-2010/readings/MIT6_701S10_part7.pdf" rel="nofollow">this</a> pdf, on page 223; it\'s actually a part of MIT\'s ocw.  </p>\n', 'ViewCount': '119', 'Title': 'How do computers compute?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-04T06:47:48.613', 'LastEditDate': '2013-10-04T06:47:48.613', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10481', 'Tags': '<entropy><memory-hardware>', 'CreationDate': '2013-10-03T14:41:59.803', 'FavoriteCount': '1', 'Id': '14787'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose I have $n$ independent observations $x_1,\\dots,x_n$ from some unknown distribution over a known alphabet $\\Sigma$, and I want to estimate the entropy of the distribution.  I can count the frequency $f_s$ of each symbol $s \\in \\Sigma$ among the observations; how should I use them to estimate the Shannon entropy of the source?</p>\n\n<hr>\n\n<p>The obvious approach is to estimate the probability of each symbol $s$ as $\\Pr[X=s]=f_s/n$, and then calculate the entropy using the standard formula for Shannon entropy.  This leads to the following estimate of the entropy $H(X)$:</p>\n\n<p>$$\\text{estimate}(H(X)) = - \\sum_{s \\in \\Sigma} {f_s \\over n} \\lg (f_s/n).$$</p>\n\n<p>However, this feels like it might not produce the best estimate.  Consider, by analogy, the problem of estimating the probability of symbol $s$ based upon its frequency $f_s$.  The naive estimate $f_s/n$ is likely an underestimate of its probability.  For instance, if I make 100 observations of birds in my back yard and none of them were a hummingbird, should my best estimate of the probability of seeing a hummingbird on my next observation be exactly 0?  No, instead, it\'s probably more realistic to estimate the probability is something small but not zero.  (A zero estimate means that a hummingbird is absolutely impossible, which seems unlikely.)</p>\n\n<p>For the problem of estimating the probability of symbol $s$, there are a number of standard techniques for addressing this problem.  <a href="https://en.wikipedia.org/wiki/Laplace_smoothing" rel="nofollow">Additive smoothing</a> (aka Laplace smoothing) is one standard technique, where we estimate the probability of symbol $s$ as $\\Pr[X=s] = (f_s + 1)/(n+|\\Sigma|)$.  Others have proposed Bayesian smoothing or other methods.  These methods are widely used in natural language processing and document analysis, where just because a word never appears in your document set doesn\'t mean that the word has probability zero.  In natural language processing, this also goes by the name <a href="https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques" rel="nofollow">smoothing</a>.</p>\n\n<p>So, taking these considerations into account, how should I estimate the entropy, based upon observed frequency counts?  Should I apply additive smoothing to get an estimate of each of the probabilities $\\Pr[X=s]$, then use the standard formula for Shannon entropy with those probabilities?  Or is there a better method that should be used for this specific problem?</p>\n', 'ViewCount': '108', 'Title': 'Estimate entropy, based upon observed frequency counts', 'LastActivityDate': '2013-10-13T18:18:02.167', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<information-theory><statistics><natural-lang-processing><entropy><information-retrieval>', 'CreationDate': '2013-10-11T21:26:30.400', 'Id': '15010'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Eagle, et al [1] discuss the notion of node entropy and this is captured in igraph via the diversity metric. I was wondering if there was any relationship between these node entropies and the idea of the entropy for the entire graph.</p>\n\n<p>A related question: Does the concept of edge entropy make sense? The probability would be the ratio of the weight of the edge in question and the sum of the weights of all edges connected to the nodes incident to the edge in question.</p>\n\n<p>[1]: N. Eagle, M. Macy, and R. Claxton, \u201cNetwork Diversity and Economic Development,\u201d Science, vol. 328, no. 5981, pp. 1029\u20131031, May 2010.</p>\n', 'ViewCount': '47', 'Title': 'Is there a relationship between graph entropy and node entropy?', 'LastActivityDate': '2013-10-25T20:57:44.423', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10978', 'Tags': '<graphs><entropy><weighted-graphs>', 'CreationDate': '2013-10-25T20:57:44.423', 'Id': '16432'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given normally distributed integers with a mean of 0 and a standard deviation $\\sigma$ around 1000, how do I compress those numbers (almost) perfectly?  Given the entropy of the Gaussian distribution, it should be possible to store any value $x$ using $$\\frac{1}{2} \\mathrm{log}_2(2\\pi\\sigma^2)+\\frac{x^2}{2\\sigma^2}\\rm{log}_2\\rm{e}$$<br>\nbits.  The way to accomplish this perfect compression would be <a href="http://en.wikipedia.org/wiki/Arithmetic_coding" rel="nofollow">arithmetic coding</a>.  In principle it\'s not too hard, I can calculate the interval boundaries from the <a href="http://en.wikipedia.org/wiki/Cumulative_distribution_function" rel="nofollow">cumulative distribution function</a> of the gaussian.  In practice I hit considerable difficulties because when using floating point operations I cannot achieve perfect reproduction of the results, and I have no idea how to do this without FP operations.  Perfect reproduction is necessary because the uncompressing code must come up with exactly the same interval boundaries as the compressing code.  So the question is:  How do I compute the interval boundaries?  Or is there any other way to achieve (near) perfect compression of such data?</p>\n\n<p><strong>Edit:</strong>  As Raphael said, strictly speaking the normal distribution is defined only for continuous variables.  So what I mean here are integers x with a probability distribution function $$P(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\rm e^{-\\frac{x^2}{2\\sigma^2}}$$.  </p>\n\n<p><strong>Edit2:</strong>  As Yuval said, this distribution does not sum up exactly to 1, however, for $\\sigma&gt;100$  the difference from 1 is less than $10^{-1000}$ and hence it\'s more precise than any practical calculation would be.</p>\n', 'ViewCount': '101', 'Title': 'Compressing normally distributed data', 'LastEditorUserId': '12710', 'LastActivityDate': '2014-03-04T06:50:22.477', 'LastEditDate': '2014-01-31T17:46:33.920', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22261', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12710', 'Tags': '<information-theory><randomness><data-compression><entropy>', 'CreationDate': '2014-01-31T14:34:01.367', 'Id': '20156'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '50', 'Title': 'Showing that the entropy of i.i.d. random variables is the sum of entropies', 'LastEditDate': '2014-02-11T11:03:53.987', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Danny', 'PostTypeId': '1', 'OwnerUserId': '13249', 'Body': "<p>The shannon entropy of a random variable $Y$ (with possible outcomes $\\Sigma=\\{\\sigma_{1},...,\\sigma_{k}\\}$) is given by<br>\n$H(Y)=-\\sum\\limits_{i=1}^{k}P(Y=\\sigma_{i})\\;\\log(P(Y=\\sigma_{i}))$.  </p>\n\n<p>For a second random variable $X=X_{1}X_{2}...X_{n}$, where all $X_{i}$'s are independent and equally distributed (each $X_{i}$ is a copy of the same random variable $Y$), the following equation is known to be true:  </p>\n\n<blockquote>\n  <p>$H(X)=n\\cdot H(Y)$</p>\n</blockquote>\n\n<p>I want to prove this simple equation, where the outcomes from $Y$ are interpreted as symbols from an alphabet $\\Sigma$ and therefore $X$ is the random variable for strings of length $n$ (based on the distribution of $Y$).</p>\n\n<p>It is easy to see, that\n$P(X=w)=P(X=w_{1}...w_{n})=P(X_{1}=w_{1})\\;\\cdot\\;...\\;\\cdot\\; P(X_{n}=w_{n})=\\prod\\limits_{i=1}^{n}P(Y=w_{i})$<br>\n... but my approach to prove $H(X)=n\\cdot H(Y)$ seems to be in a dead point<br>\n(every word $w$ has the form $w=w_{1}...w_{n}$ with $w_{i}\\in\\Sigma$ and  let $\\large |w|_{\\sigma_{i}}$ be the number of occurrences of $\\sigma_{i}$ in $w$):  </p>\n\n<p>$H(X)=-\\sum\\limits_{w\\in\\Sigma^{n}}P(X=w)\\;\\log(P(X=w))$<br>\n$=-\\sum\\limits_{w\\in\\Sigma^{n}}\\left(\\prod\\limits_{i=1}^{n}P(Y=w_{i})\\right)\\;\\log\\left(\\prod\\limits_{i=1}^{n}P(Y=w_{i})\\right)$<br>\n$=-\\sum\\limits_{w\\in\\Sigma^{n}}\\left(\\prod\\limits_{i=1}^{n}P(Y=w_{i})\\right)\\left(\\sum\\limits_{i=1}^{n}\\log\\left(P(Y=w_{i})\\right)\\right)$<br>\n$=-\\sum\\limits_{w\\in\\Sigma^{n}}\\left(\\prod\\limits_{i=1}^{k}P(Y=\\sigma_{i})^{\\large |w|_{\\sigma_{i}}}\\right)\\left(\\sum\\limits_{i=1}^{k}\\large |w|_{\\sigma_{i}}\\normalsize \\;\\log\\left(P(Y=\\sigma_{i})\\right)\\right)$  </p>\n\n<p>So, I am able to change some indices from word length to the length of the alphabet, which is used in $H(Y)$. But what now? Any help?</p>\n", 'Tags': '<information-theory><entropy>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-11T11:03:53.987', 'CommentCount': '0', 'AcceptedAnswerId': '21526', 'CreationDate': '2014-02-06T15:59:58.167', 'Id': '21525'}},