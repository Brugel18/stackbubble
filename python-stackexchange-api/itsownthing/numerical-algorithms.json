{'Body': "<p>I have this problem:</p>\n\n<blockquote>\n  <p>Consider the problem of calculating the integral</p>\n  \n  <p>$$y_n =\\int_{0}^{1} \\dfrac{x^n}{x+10} \\mathrm{d}x $$\n  for a positive integer $n$.</p>\n  \n  <p>Observe that $$y_n + 10y_{n-1} = \\int_{0}^{1} \\dfrac{x^n +10x^{n-1}}{x+10} \\mathrm{d}x  = \\int_{0}^{1} x^{n-1}\\mathrm{d}x = \\dfrac{1}{n}$$</p>\n  \n  <p>and that using this relationship in a forward recursion leads to a numerically unstable procedure.</p>\n  \n  <ol>\n  <li><p>Derive a formula for approximately computing these integrals based on evaluating $y_{n-1}$ given $y_n$.</p></li>\n  <li><p>Show that for any given value $\\epsilon &gt; 0$ and positive integer $n_0$, there exists an integer $n_1 \\geq n_0$ such that taking $y_{n_1} = 0$ as a starting value will produce integral evaluations $y_n$ with an absolute error smaller than $\\epsilon$ for all  $0 &lt; n \\leqslant n_0$. </p></li>\n  <li><p>Explain why your algorithm is stable.</p></li>\n  </ol>\n</blockquote>\n\n<p>Here is what I have so far,</p>\n\n<p>for part 1.</p>\n\n<p>$$y_{n-1} = \\dfrac{1}{10} \\left(\\dfrac{1}{n} - y_n\\right)$$</p>\n\n<p>and for part 3.</p>\n\n<p>The algorithm is stable because the magnitude of roundoff errors gets divided by 10 each time the recursion is applied.</p>\n\n<p>I really don't know how to start on the proof for part 2., any hints and help would be appreciated.</p>\n", 'ViewCount': '59', 'Title': 'Error accumulation in a numerical integration', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:12:24.380', 'LastEditDate': '2013-09-18T09:12:24.380', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10189', 'Tags': '<algorithms><numerical-analysis><error-estimation><numerical-algorithms>', 'CreationDate': '2013-09-18T00:43:25.283', 'Id': '14394''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>My question is about numerical methods for inverting integral transforms;</p>\n\n<p>I'm trying to numerically invert the following integral transform:</p>\n\n<p>$$F(y) = \\int_{0}^{\\infty} y\\exp{\\left[-\\frac{1}{2}(y^2 + x^2)\\right]} I_0\\left(xy\\right)f(x)\\;\\mathrm{d}x$$</p>\n\n<p>So for a given $F(y)$ I need to approximate $f(x)$\nwhere:</p>\n\n<ul>\n<li><strong>$f(x)$ and $F(y)$ are real and positive</strong> (they are continuous probability distributions)</li>\n<li><strong>$x,y$ are real and positive</strong> (they are magnitudes)</li>\n</ul>\n\n<p>I have a very messy and brute force method for doing this at the minute: </p>\n\n<p>I define $f(x)$ and the spline over a series of points, the values of the splined points are 'guessed' by random sampling, which yields a predicted $F(y)$. A basic genetic algorithm I wrote up minimises the difference between the predicted and measured $F(y)$ array. I then take the $f(x)$ which the algorithm converges to as my answer for the inversion.</p>\n\n<p>This approach works fairly well for some simple cases, but it feels messy to me and not particularly robust.</p>\n\n<p><strong>Can anyone give me guidance on better ways of solving this problem?</strong></p>\n\n<p>Thanks for your time &amp; help!</p>\n", 'ViewCount': '40', 'Title': 'Numerically approximating an inverse integral transform?', 'LastActivityDate': '2013-11-27T05:40:37.663', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'OwnerDisplayName': 'user11649', 'PostTypeId': '1', 'Tags': '<algorithms><optimization><numerical-analysis><numerical-algorithms>', 'CreationDate': '2013-11-27T05:40:37.663', 'Id': '18406''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>Suppose that I want to optimize a unimodal function defined on some real interval.  I can use the well-known algorithm as described in Wikipedia under the name of <a href="http://en.wikipedia.org/wiki/Ternary_search" rel="nofollow">ternary search</a>.</p>\n\n<p>In case of the algorithm that repeatedly halving intervals, it is common to reserve the term <em>binary search</em> for discrete problems and to use the term <em>bisection method</em> otherwise.  Extrapolating this convention, I suspect that the term <em>trisection method</em> might apply to the algorithm that solves my problem.</p>\n\n<p>My question is whether it is common among academics, and is safe to use in, e.g., senior theses, to apply the term <em>ternary search</em> even if the algorithm is applied to a continuous problem.  I need a reputable source for this.  I\'m also interested whether the term <em>trisection method</em> actually exists.</p>\n', 'ViewCount': '96', 'Title': 'Is "ternary search" an appropriate term for the algorithm that optimizes a unimodal function on a real interval?', 'LastEditorUserId': '12861', 'LastActivityDate': '2014-04-20T03:07:25.000', 'LastEditDate': '2014-01-16T03:45:59.587', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12861', 'Tags': '<algorithms><terminology><numerical-analysis><numerical-algorithms>', 'CreationDate': '2014-01-15T02:37:13.467', 'Id': '19734''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': '<p>I am working on an algorithm that requires me to interpolate a couple trillion positive discrete points with f(x) having low finite value (for example 0 - 5). It there a specialized algorithm specific for this reduced problem that can provide 0 error interpolation?\nCould you maybe point me to a few papers?</p>\n\n<p>EDIT: I noticed that i can make the trillions of points become less, but then the max value of f(x) rises exponentially (if i cut them by half, the max value gets squared). If i can get for example 4 points and interpolate with a 3rd degree function, is it guaranteed to pass through all points?</p>\n', 'ViewCount': '31', 'Title': '0 error interpolation for discrete finite value points', 'LastEditorUserId': '16647', 'LastActivityDate': '2014-04-10T15:24:00.573', 'LastEditDate': '2014-04-10T15:24:00.573', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16647', 'Tags': '<numerical-analysis><numerical-algorithms>', 'CreationDate': '2014-04-10T14:49:27.107', 'Id': '23642''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}{'Body': "<p>Using gradient descent in <em>d</em> dimensions to find a local minimum requires computing gradients, which is computationally much faster than Newton's method, because Newton's method requires computing both gradients and Hessians.</p>\n\n<p>However, gradient descent generally requires many more iterations than Newton's method to converge within the same accuracy.</p>\n\n<p>My question, then, is:</p>\n\n<p>Assuming they both converge, in terms of the <em>number of elementary floating-point operations</em>, which is usually faster:  Newton's method or gradient descent?  Why?</p>\n", 'ViewCount': '32', 'Title': "Gradient descent vs. Newton's method: which is more efficient?", 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-12T13:38:17.120', 'LastEditDate': '2014-04-12T13:38:17.120', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<algorithms><algorithm-analysis><optimization><numerical-algorithms>', 'CreationDate': '2014-04-12T11:27:02.383', 'Id': '23701''color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2}