{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose that a certain parallel application uses a master-slave design to process a large number of workloads. Each workload takes some number of cycles to complete; the number of cycles any given workload will take is given by a known random variable $X$. Assume that there are $n$ such workloads and $m$ equivalent slaves (processing nodes). Naturally, a more general version of this question addresses the case of slaves of differing capabilities, but we ignore this for now.</p>\n\n<p>The master cannot process workloads, but can distribute workloads to slave nodes and monitor progress of slave nodes. Specifically, the master can perform the following actions:</p>\n\n<ol>\n<li>Instantaneously begin processing of any $k$ workloads on any free node.</li>\n<li>Instantaneously receive confirmation of the completion by a node of a previously initiated batch of $k$ workloads.</li>\n<li>At any point in time, and instantaneously, determine the state of all nodes (free or busy) as well as the number of workloads completed and the number of workloads remaining.</li>\n</ol>\n\n<p>For simplicity\'s sake, assume $k$ divides $n$.</p>\n\n<p>There are at least two categories of load balancing strategies for minimizing the total execution time of all workloads using all slaves (to clarify, I\'m talking about the makespan or wall-clock time, not the aggregate process time, which is independent of the load-balancing strategy being used under the simplifying assumptions being made in this question): static and dynamic. In a static scheme, all placement decisions are made at time $t = 0$. In a dynamic scheme, the master can make placement decisions using information about the progress being made by some slaves, and as such, better utilization can be attained (in practice, there are overheads associated with dynamic scheduling as compared to static scheduling, but we ignore these). Now for some questions:</p>\n\n<ol>\n<li>Is there a better way to statically schedule workloads than to divide batches of $k$ workloads among the $m$ slaves as evenly as possible (we can also assume, for simplicity\'s sake, that $m$ divides $n/k$, so batches could be statically scheduled completely evenly)? If so, how?</li>\n<li>Using the best static scheduling policy, what should the mean and standard deviation be for the total execution time, in terms of the mean $\\mu$ and standard deviation $\\sigma$ of $X$? </li>\n</ol>\n\n<p>A simple dynamic load balancer might schedule $i$ batches of $k$ workloads to each slave initially, and then, when nodes complete the initial $i$ batches, schedule an additional batch of $k$ workloads to each slave on a first-come, first-served basis. So if two slave nodes are initially scheduled 2 batches of 2 workloads each, and the first slave finishes its two batches, an additional batch is scheduled to the first slave, while the second slave continues working. If the first slave finishes the new batch before the second batch finishes its initial work, the master will continue scheduling to the first slave. Only when the second slave completes executing its work will it be issued a new batch of workloads. Example:</p>\n\n<pre><code>         DYNAMIC           STATIC\n         POLICY            POLICY\n\n     slave1  slave2    slave1  slave2\n     ------  ------    ------  ------\n\nt&lt;0    --      --        --      --\n\nt&lt;1  batch1  batch3    batch1  batch3\n     batch2  batch4    batch2  batch4\n                       batch5  batch7\n                       batch6  batch8\n\nt=1    --    batch3    batch5  batch3\n             batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt&lt;2  batch5  batch3    batch5  batch3\n             batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt=2    --    batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt&lt;3  batch6  batch4    batch6  batch4\n                               batch7\n                               batch8\n\nt=3    --      --        --    batch7\n                               batch8\n\nt&lt;4  batch7  batch8      --    batch7\n                               batch8\n\nt=4    --      --        --    batch8\n\nt&lt;5      -DONE-          --    batch8\n\nt=5                      --      --\n\nt &lt; 6                      -DONE-\n</code></pre>\n\n<p>For clarification, batches 1 and 2 take 1/2 second each to be processed, batch 3 takes 2 seconds to be processed, and batches 4-8 take 1 second each to be processed. This information is not known a-priori; in the static scheme, all jobs are distributed at t=0, whereas in the dynamic scheme, distribution can take into account what the actual runtimes of the jobs "turned out" to be. We notice that the static scheme takes one second longer than the dynamic scheme, with slave1 working 3 seconds and slave2 working 5 seconds. In the dynamic scheme, both slaves work for the full 4 seconds.</p>\n\n<p>Now for the question that motivated writing this:</p>\n\n<ol>\n<li>Using the dynamic load balancing policy described above, what should the mean and standard deviation be for the total execution time, in terms of the mean $\\mu$ and standard deviation $\\sigma$ of $X$?</li>\n</ol>\n\n<p>Interested readers have my assurances that this isn\'t homework, although it probably isn\'t much harder than what one might expect to get as homework in certain courses. Given that, if anyone objects to this being asked and demands that I show some work, I will be happy to oblige (although I don\'t know when I\'ll have time in the near future). This question is actually based on some work that I never got around to doing a semester or two ago, and empirical results were where we left it. Thanks for help and/or effort, I\'ll be interested to see what you guys put together.</p>\n', 'ViewCount': '198', 'Title': 'Analyzing load balancing schemes to minimize overall execution time', 'LastEditorUserId': '69', 'LastActivityDate': '2012-03-09T17:09:49.673', 'LastEditDate': '2012-03-09T17:09:49.673', 'AnswerCount': '1', 'CommentCount': '9', 'AcceptedAnswerId': '140', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '69', 'Tags': '<scheduling><distributed-systems><parallel-computing>', 'CreationDate': '2012-03-08T16:00:04.720', 'Id': '134'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder whether the massively parallel computation units provided in graphic cards nowadays (one that is programmable in <a href="http://en.wikipedia.org/wiki/OpenCL" rel="nofollow">OpenCL</a>, for example) are good enough to simulate 1D cellular automata (or maybe 2D cellular automata?) efficiently.</p>\n\n<p>If we choose whatever finite grid would fit inside the memory of the chip, can we expect one transition of a cellular automaton defined on this grid to be computed in (quasi)constant time?</p>\n\n<p>I assume 2D cellular automata would require more bandwidth for communication between the different parts of the chips than 1D automata.</p>\n\n<p>I\'d also be interested by the same question in the case of FPGA programming or custom chips.</p>\n', 'ViewCount': '317', 'Title': "Are today's massive parallel processing units able to run cellular automata efficiently?", 'LastEditorUserId': '68', 'LastActivityDate': '2012-09-28T19:50:44.353', 'LastEditDate': '2012-09-17T14:03:09.767', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '68', 'Tags': '<computer-architecture><parallel-computing><cellular-automata>', 'CreationDate': '2012-03-12T22:02:55.090', 'FavoriteCount': '2', 'Id': '256'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '260', 'Title': 'How does variance in task completion time affect makespan?', 'LastEditDate': '2012-04-13T22:15:45.380', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '69', 'FavoriteCount': '0', 'Body': "<p>Let's say that we have a large collection of tasks $\\tau_1, \\tau_2, ..., \\tau_n$ and a collection of identical (in terms of performance) processors $\\rho_1, \\rho_2, ..., \\rho_m$ which operate completely in parallel. For scenarios of interest, we may assume $m \\leq n$. Each $\\tau_i$ takes some amount of time/cycles to complete once it is assigned to a processor $\\rho_j$, and once it is assigned, it cannot be reassigned until completed (processors always eventually complete assigned tasks). Let's assume that each $\\tau_i$ takes an amount of time/cycles $X_i$, not known in advance, taken from some discrete random distribution. For this question, we can even assume a simple distribution: $P(X_i = 1) = P(X_i = 5) = 1/2$, and all $X_i$ are pairwise independent. Therefore $\\mu_i = 3$ and $\\sigma^2 = 4$.</p>\n\n<p>Suppose that, statically, at time/cycle 0, all tasks are assigned as evenly as possible to all processors, uniformly at random; so each processor $\\rho_j$ is assigned $n/m$ tasks (we can just as well assume $m | n$ for the purposes of the question). We call the makespan the time/cycle at which the last processor $\\rho^*$ to finish its assigned work, finishes the work it was assigned. First question:</p>\n\n<blockquote>\n  <p>As a function of $m$, $n$, and the $X_i$'s, what is the makespan $M$? Specifically, what is $E[M]$? $Var[M]$?</p>\n</blockquote>\n\n<p>Second question:</p>\n\n<blockquote>\n  <p>Suppose $P(X_i = 2) = P(X_i = 4) = 1/2$, and all $X_i$ are pairwise independent, so $\\mu_i = 3$ and $\\sigma^2 = 1$. As a function of $m$, $n$, and these new $X_i$'s, what is the makespan? More interestingly, how does it compare to the answer from the first part?</p>\n</blockquote>\n\n<p>Some simple thought experiments demonstrate the answer to the latter is that the makespan is longer. But how can this be quantified? I will be happy to post an example if this is either (a) controversial or (b) unclear. Depending on the success with this one, I will post a follow-up question about a dynamic assignment scheme under these same assumptions. Thanks in advance!</p>\n\n<p><strong>Analysis of an easy case: $m = 1$</strong></p>\n\n<p>If $m = 1$, then all $n$ tasks are scheduled to the same processor. The makespan $M$ is just the time to complete $n$ tasks in a complete sequential fashion. Therefore,\n$$\\begin{align*}\r\n E[M]\r\n &amp;= E[X_1 + X_2 + ... + X_n] \\\\\r\n &amp;= E[X_1] + E[X_2] + ... + E[X_n] \\\\\r\n &amp;= \\mu + \\mu + ... + \\mu \\\\\r\n &amp;= n\\mu\r\n\\end{align*}$$\nand\n$$\\begin{align*}\r\n Var[M]\r\n &amp;= Var[X_1 + X_2 + ... + X_n] \\\\\r\n &amp;= Var[X_1] + Var[X_2] + ... + Var[X_n] \\\\\r\n &amp;= \\sigma^2 + \\sigma^2 + ... + \\sigma^2 \\\\\r\n &amp;= n\\sigma^2 \\\\\r\n\\end{align*}$$</p>\n\n<p>It seems like it might be possible to use this result to answer the question for $m &gt; 1$; we simply need to find an expression (or close approximation) for $\\max(Y_1, Y_2, ..., Y_m)$ where $Y_i = X_{i\\frac{n}{m} + 1} + X_{i\\frac{n}{m} + 2} + ... + X_{i\\frac{n}{m} + \\frac{n}{m}}$, a random variable with $\\mu_Y = \\frac{n}{m}\\mu_X$ and $\\sigma_Y^2 = \\frac{n}{m}\\sigma_X^2$. Is this heading in the right direction?</p>\n", 'Tags': '<probability-theory><scheduling><parallel-computing>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-12-07T11:20:11.647', 'CommentCount': '1', 'AcceptedAnswerId': '1251', 'CreationDate': '2012-04-12T20:03:55.620', 'Id': '1236'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '545', 'Title': 'P-Completeness and Parallel Computation', 'LastEditDate': '2012-04-21T20:26:03.557', 'AnswerCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '31', 'FavoriteCount': '1', 'Body': '<p>I was recently reading about algorithms for checking bisimilarity and read that the problem is <a href="http://en.wikipedia.org/wiki/P-complete">P-complete</a>. Furthermore, a consequence of this is that this problem, or any P-complete problem, is unlikely to have an efficient parallel algorithms.</p>\n\n<blockquote>\n  <p>What is the intuition behind this last statement?</p>\n</blockquote>\n', 'Tags': '<complexity-theory><parallel-computing>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-22T15:36:02.203', 'CommentCount': '1', 'AcceptedAnswerId': '1431', 'CreationDate': '2012-04-21T19:55:08.833', 'Id': '1415'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '7302', 'Title': 'Distributed vs parallel computing', 'LastEditDate': '2012-04-30T07:46:49.827', 'AnswerCount': '3', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '1307', 'FavoriteCount': '12', 'Body': "<p>I often hear people talking about <em>parallel</em> computing and <em>distributed</em> computing, but I'm under the impression that there is no clear boundary between the 2, and people tend to confuse that pretty easily, while I believe it is very different:</p>\n\n<ul>\n<li><em>Parallel</em> computing is more tightly coupled to multi-threading, or how to make full use of a single CPU.</li>\n<li><em>Distributed</em> computing refers to the notion of divide and conquer, executing sub-tasks on different machines and then merging the results.</li>\n</ul>\n\n<p>However, since we stepped into the <em>Big Data</em> era, it seems the distinction is indeed melting, and most systems today use a combination of parallel and distributed computing.</p>\n\n<p>An example I use in my day-to-day job is Hadoop with the Map/Reduce paradigm, a clearly distributed system with workers executing tasks on different machines, but also taking full advantage of each machine with some parallel computing.</p>\n\n<p>I would like to get some advice to understand how exactly to make the distinction in today's world, and if we can still talk about parallel computing or there is no longer a clear distinction. To me it seems distributed computing has grown a lot over the past years, while parallel computing seems to stagnate, which could probably explain why I hear much more talking about distributing computations than parallelizing.</p>\n", 'Tags': '<terminology><distributed-systems><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-20T12:16:12.150', 'CommentCount': '2', 'AcceptedAnswerId': '1582', 'CreationDate': '2012-04-29T21:13:33.690', 'Id': '1580'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have had problems accepting the complexity theoretic view of "efficiently solved by parallel algorithm" which is given by the class <a href="https://en.wikipedia.org/wiki/NC_%28complexity%29">NC</a>:</p>\n\n<blockquote>\n  <p>NC is the class of problems that can be solved by a parallel algorithm in time $O(\\log^cn)$ on $p(n) \\in O(n^k)$ processors with $c,k \\in \\mathbb{N}$.</p>\n</blockquote>\n\n<p>We can assume a <a href="https://en.wikipedia.org/wiki/Parallel_random_access_machine">PRAM</a>.</p>\n\n<p>My problem is that this does not seem to say much about "real" machines, that is machines with a finite amount of processors. Now I am told that "it is known" that we can "efficiently" simulate a $O(n^k)$ processor algorithm on $p \\in \\mathbb{N}$ processors.</p>\n\n<p>What does "efficiently" mean here? Is this folklore or is there a rigorous theorem which quantifies the overhead caused by simulation?</p>\n\n<p>What I am afraid that happens is that I have a problem which has a sequential $O(n^k)$ algorithm and also an "efficient" parallel algorithm which, when simulated on $p$ processors, also takes $O(n^k)$ time (which is all that can be expected on this granularity level of analysis if the sequential algorithm is asymptotically optimal). In this case, there is no speedup whatsover as far as we can see; in fact, the simulated parallel algorithm may be <em>slower</em> than the sequential algorithm. That is I am really looking for statements more precise than $O$-bounds (or a declaration of absence of such results).</p>\n', 'ViewCount': '260', 'Title': 'How to scale down parallel complexity results to constantly many cores?', 'LastActivityDate': '2012-05-03T22:04:13.333', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '1648', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<complexity-theory><reference-request><parallel-computing>', 'CreationDate': '2012-05-03T08:08:30.570', 'Id': '1647'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '132', 'Title': 'Explain $\\log_2(n)$ squared asymptotic run-time for naive nested parallel CREW PRAM mergesort', 'LastEditDate': '2012-05-09T15:58:31.630', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'Paul Caheny', 'PostTypeId': '1', 'OwnerUserId': '1427', 'Body': '<p>On from Page 1 of <a href="http://www.inf.ed.ac.uk/teaching/courses/dapa/note3.pdf" rel="nofollow">these lecture notes</a> it is stated in the final paragraph of the section titled CREW Mergesort:</p>\n\n<blockquote>\n  <p>Each such step (in a sequence of $\\Theta(\\log_2\\ n)$ steps) takes\n  time $\\Theta(\\log_2\\ s)$ with a sequence length of $s$. Summing these, we\n  obtain an overall run time of $\\Theta((\\log_2\\  n)^2)$ for $n$\n  processors, which is not quite (but almost!) cost-optimal.</p>\n</blockquote>\n\n<p>Can anyone show explicitly how the sum mentioned is calculated and the squared log result arrived at?</p>\n', 'Tags': '<algorithms><complexity-theory><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-09T15:59:10.290', 'CommentCount': '0', 'AcceptedAnswerId': '1755', 'CreationDate': '2012-05-09T13:35:17.113', 'Id': '1754'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '204', 'Title': 'Can joins be parallelized?', 'LastEditDate': '2012-06-07T14:49:58.440', 'AnswerCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1590', 'FavoriteCount': '1', 'Body': '<p>Suppose we want to join two relations on a predicate. Is this in NC?</p>\n\n<p>I realize that a proof of it not being in NC would amount to a proof that $P\\not=NC$, so I\'d accept evidence of it being an open problem as an answer.</p>\n\n<p>I\'m interested in the general case as well as specific cases (e.g. perhaps with some specific data structure it can be parallelized). </p>\n\n<p>EDIT: to bring some clarifications from the comments into this post:</p>\n\n<ul>\n<li>We could consider an equijoin $A.x = B.y$. On a single processor, a hash-based algorithm runs in $O(|A|+|B|)$ and this is the best we can do since we have to read each set</li>\n<li>If the predicate is a "black box" where we have to check each pair, there are $|A|\\cdot|B|$ pairs, and each one could be in or not, so $2^{ab}$ possibilities. Checking each pair divides the possibilities in half, so the best we can do is $O(ab)$.</li>\n</ul>\n\n<p>Could either of these (or some third type of join) be improved to $\\log^k n$ on multiple processors?</p>\n', 'Tags': '<complexity-theory><time-complexity><parallel-computing><database-theory><descriptive-complexity>', 'LastEditorUserId': '1590', 'LastActivityDate': '2012-07-07T01:35:55.073', 'CommentCount': '9', 'AcceptedAnswerId': '2410', 'CreationDate': '2012-06-05T17:38:32.240', 'Id': '2235'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '526', 'Title': 'Could an artificial neural network algorithm be expressed in terms of map-reduce operations?', 'LastEditDate': '2012-06-27T12:56:50.677', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1983', 'FavoriteCount': '0', 'Body': '<p>Could an artificial neural network algorithm be expressed in terms of map-reduce operations? I am also interested more generally in methods of parallelization as applied to ANNs and their application to cloud computing. </p>\n\n<p>I would think one approach would involve running a full ANN on each node and somehow integrating the results in order to treat the grid like a single entity (in terms of input/output and machine learning characteristics.) I would be curious even in this case what such an integrating strategy might look like.</p>\n', 'Tags': '<parallel-computing><artificial-intelligence><neural-networks>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-27T13:12:05.760', 'CommentCount': '0', 'AcceptedAnswerId': '2514', 'CreationDate': '2012-06-26T23:53:38.440', 'Id': '2508'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '383', 'Title': 'Getting parallel items in dependency resolution', 'LastEditDate': '2012-06-28T22:30:03.503', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1995', 'FavoriteCount': '2', 'Body': '<p>I have implemented a topological sort based on the <a href="http://en.wikipedia.org/wiki/Topological_sort" rel="nofollow">Wikipedia article</a> which I\'m using for dependency resolution, but it returns a linear list. What kind of algorithm can I use to find the independent paths?</p>\n', 'Tags': '<algorithms><graphs><parallel-computing><scheduling>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-06-28T22:30:03.503', 'CommentCount': '1', 'AcceptedAnswerId': '2525', 'CreationDate': '2012-06-28T09:12:35.827', 'Id': '2524'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '11355', 'Title': 'What is the novelty in MapReduce?', 'LastEditDate': '2012-08-04T09:11:58.820', 'AnswerCount': '4', 'Score': '39', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '16', 'Body': u'<p>A few years ago, <a href="https://en.wikipedia.org/wiki/Mapreduce">MapReduce</a> was hailed as revolution of distributed programming. There have also been <a href="http://craig-henderson.blogspot.de/2009/11/dewitt-and-stonebrakers-mapreduce-major.html">critics</a> but by and large there was an enthusiastic hype. It even got patented! [1]</p>\n\n<p>The name is reminiscent of <code>map</code> and <code>reduce</code> in functional programming, but when I read (Wikipedia)</p>\n\n<blockquote>\n  <p><strong>Map step:</strong> The master node takes the input, divides it into smaller sub-problems, and distributes them to worker nodes. A worker node may do this again in turn, leading to a multi-level tree structure. The worker node processes the smaller problem, and passes the answer back to its master node.</p>\n  \n  <p><strong>Reduce step:</strong> The master node then collects the answers to all the sub-problems and combines them in some way to form the output \u2013 the answer to the problem it was originally trying to solve.</p>\n</blockquote>\n\n<p>or [2] </p>\n\n<blockquote>\n  <p><strong>Internals of MAP:</strong> [...] MAP splits up the input value into words. [...] MAP is meant to associate each given key/value pair of the input with potentially many intermediate key/value pairs.</p>\n  \n  <p><strong>Internals of REDUCE:</strong> [...] [REDUCE] performs imperative aggregation (say, reduction): take many values, and reduce them to a single value.</p>\n</blockquote>\n\n<p>I can not help but think: this is <a href="https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm">divide &amp; conquer</a> (in the sense of Mergesort), plain and simple! So, is there (conceptual) novelty in MapReduce somewhere, or is it just a new implementation of old ideas useful in certain scenarios?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PALL&amp;p=1&amp;u=/netahtml/PTO/srchnum.htm&amp;r=1&amp;f=G&amp;l=50&amp;s1=7,650,331.PN.&amp;OS=PN/7,650,331&amp;RS=PN/7,650,331"> US Patent 7,650,331: "System and method for efficient large-scale data processing "</a> (2010)</li>\n<li><a href="http://dx.doi.org/10.1016/j.scico.2007.07.001">Google\u2019s MapReduce programming model \u2014 Revisited</a> by R. L\xe4mmel (2007)</li>\n</ol>\n', 'Tags': '<algorithms><distributed-systems><parallel-computing><algorithm-design>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-11T04:57:53.957', 'CommentCount': '4', 'AcceptedAnswerId': '3020', 'CreationDate': '2012-08-03T14:04:19.350', 'Id': '3019'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Surely <a href="http://www.spatial-computing.org/" rel="nofollow">spatial computing</a> and <a href="http://en.wikipedia.org/wiki/Amorphous_computing" rel="nofollow">amorphous computing</a> share similarities and overlap. Is spatial computing a subset of amorphous computing? How are the two different? </p>\n', 'ViewCount': '41', 'Title': 'How is amorphous computing different from spatial computing?', 'LastEditorUserId': '31', 'LastActivityDate': '2012-08-07T15:46:14.707', 'LastEditDate': '2012-08-07T15:46:14.707', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2389', 'Tags': '<distributed-systems><parallel-computing><agent-based-computing>', 'CreationDate': '2012-08-07T12:04:55.460', 'Id': '3073'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there an algorithm to perform batch processing in the increase-key operation? Let us say, a binary heap (min-heap) is used. In the normal increase-key function, if we perform increase key on one node, then we have to traverse paths from the node towards the children to re balance the heap. If we want to increase the keys of five nodes in the heap, we need to call the increase-key function five times. Is it possible to call only one increase-key function and perform increase-key on five nodes simultaneously?</p>\n', 'ViewCount': '217', 'Title': 'Batch processing in increase-key function using binary heap', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-15T11:00:10.347', 'LastEditDate': '2012-08-14T12:08:17.190', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2460', 'Tags': '<algorithms><data-structures><parallel-computing><concurrency><priority-queues>', 'CreationDate': '2012-08-14T00:39:37.677', 'FavoriteCount': '1', 'Id': '3163'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for answers which provide short overview of models and current state of research for <a href="http://en.wikipedia.org/wiki/Automatic_parallelization" rel="nofollow">auto-parallelisation</a> of sequential code.</p>\n', 'ViewCount': '105', 'Title': 'What are current approaches to auto-parallelisation?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-18T05:21:42.593', 'LastEditDate': '2012-08-14T12:08:43.540', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '191', 'Tags': '<reference-request><compilers><parallel-computing><code-generation>', 'CreationDate': '2012-08-14T05:31:42.663', 'FavoriteCount': '2', 'Id': '3168'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How exactly <a href="http://en.wikipedia.org/wiki/Loop_dependence_analysis" rel="nofollow">Loop Dependence Analysis</a> helps in <a href="http://en.wikipedia.org/wiki/Vectorization_%28parallel_computing%29" rel="nofollow">vectorization</a> ? Are there any standard rules of safety criterias for parallizing such loops ?</p>\n', 'ViewCount': '71', 'Title': 'Using Loop Dependence analysis for vectorization', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-08T09:27:02.593', 'LastEditDate': '2012-09-07T07:27:34.463', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '3465', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2741', 'Tags': '<compilers><parallel-computing><program-optimization>', 'CreationDate': '2012-09-07T00:25:42.440', 'Id': '3454'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a number of related questions about these two topics.</p>\n\n<p>First, most complexity texts only gloss over the class $\\mathbb{NC}$.  Is there a good resource that covers the research more in depth?  For example, something that discusses all of my questions below.  Also, I\'m assuming that $\\mathbb{NC}$ still sees a fair amount of research due to its link to parallelization, but I could be wrong.    The section in the complexity zoo isn\'t much help.</p>\n\n<p>Second, computation over a semigroup is in $\\mathbb{NC}^1$ if we assume the semigroup operation takes constant time.  But what if the operation does not take constant time, as is the case for unbounded integers?  Are there any known $\\mathbb{NC}^i$-complete problems?</p>\n\n<p>Third, since $\\mathbb{L} \\subseteq \\mathbb{NC}^2$, is there an algorithm to convert any logspace algorithm into a parallel version?</p>\n\n<p>Fourth, it sounds like most people assume that $\\mathbb{NC} \\ne \\mathbb{P}$ in the same way that $\\mathbb{P} \\ne \\mathbb{NP}$.  What is the intuition behind this?</p>\n\n<p>Fifth, every text I\'ve read mentions the class $\\mathbb{RNC}$ but gives no examples of problems it contains.  Are there any?</p>\n\n<p>Finally, <a href="http://cs.stackexchange.com/a/1656/2911">this answer</a> mentions problems in $\\mathbb{P}$ with sublinear parallel execution time.  What are some examples of these problems?  Are there other complexity classes that contain parallel algorithms that are not known to be in $\\mathbb{NC}$?</p>\n', 'ViewCount': '228', 'Title': 'Some questions on parallel computing and the class NC', 'LastEditorUserId': '2911', 'LastActivityDate': '2012-09-24T15:39:26.497', 'LastEditDate': '2012-09-21T19:18:26.083', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '2911', 'Tags': '<complexity-theory><reference-request><parallel-computing><complexity-classes>', 'CreationDate': '2012-09-21T18:50:22.743', 'FavoriteCount': '1', 'Id': '4659'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Parallel processes interfere with each other in many ways, by competing for shared resources such as shared caches, memory, disks, etc.</p>\n\n<p>Would it be possible to determine latent factors just with a runtime analysis, by means of comparing the runtime of the respective applications while running them many times in different combinations? Or would the required sample size be too big for the method to be feasible?</p>\n\n<p>If comparing only the runtimes would not result in significant results, which variable would be best suited to observe? L1-cache-misses, L2-cache-misses, page faults, total memory usage, tlb-misses, etc.</p>\n', 'ViewCount': '48', 'Title': 'Determine interference factors in parallel computing', 'LastActivityDate': '2012-10-17T10:27:52.867', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4230', 'Tags': '<parallel-computing><cpu-cache><performance>', 'CreationDate': '2012-10-17T10:27:52.867', 'Id': '6120'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The following equation is a matrix expression where $B_i$ and $C_i^T$ are $n\\times n$ matrices and k is a positive integer:</p>\n\n<p>$$P = \\sum_{i=1}^k  B_i  C_i^T $$</p>\n\n<p>So $P = B_1 C_1^T + B_2 C_2^T + \\cdots +B_k C_k^T   $</p>\n\n<p>If $B_i $ and $C_i$ are $n\\times n$ matrices themselves, we have a total of 2 $\\times$ k matrices that some how need to be stored in this vector architecture. </p>\n\n<p>So this means P will end up being an $n\\times n$ matrix after all the computation has completed.   </p>\n\n<p>What is the simplest possible vector processor architecture that is required to perform the matrix computation above?</p>\n\n<p>Is there any literature or articles out there that discuss how this can be done?</p>\n\n<p>Would appreciate all / any advise </p>\n', 'ViewCount': '161', 'Title': 'How do you go about designing a vector processor architecture for the sum of matrix products?', 'LastEditorUserId': '1480', 'LastActivityDate': '2012-12-13T02:27:21.687', 'LastEditDate': '2012-10-31T01:30:23.080', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '6430', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1480', 'Tags': '<computer-architecture><parallel-computing><matrices>', 'CreationDate': '2012-10-28T03:10:23.307', 'Id': '6347'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm currently learning computer science, and there is a slide of notes brief described the parallel radix sort under data parallelism.</p>\n\n<pre><code>number 101 110 011 001 111 (1st bit)\norder    2   1   3   4   5 (new order)\nnumber 110 101 011 001 111 (2nd bit)\norder    3   1   4   2   5 (new order)\nnumber 101 001 110 011 111 (3rd bit)\norder    3   1   4   2   5 (new order)\nnumber 001 011 101 110 111\n</code></pre>\n\n<p>I roughly know how to sort it from lecturer's explanation, but how is it related to parallel computing to increase the performance?</p>\n", 'ViewCount': '400', 'Title': 'how does the parallel radix sort work?', 'LastActivityDate': '2012-11-24T11:37:28.147', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1895', 'Tags': '<sorting><parallel-computing>', 'CreationDate': '2012-11-24T11:37:28.147', 'Id': '6871'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Assume a map-reduce program has $m$ mappers and $n$ reducers ($m &gt; n$). The output of each mapper is partitioned according to the key value and all records having the same key value go into the same partition (within each mapper), and then each partition is sent to a reducer. Thus there might be a case in which there are two partitions with the same key from two different mappers going to 2 different reducers. How to prevent this from occurring? That is, how to send all partitions (from different mappers) with the same key to the same reducer?</p>\n', 'ViewCount': '189', 'Title': 'How partitioning in map-reduce work?', 'LastActivityDate': '2012-12-04T19:29:25.950', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4868', 'Tags': '<distributed-systems><parallel-computing><algorithm-design>', 'CreationDate': '2012-12-04T17:54:20.870', 'FavoriteCount': '1', 'Id': '7159'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>When in a Parallel algorithm we say:</p>\n\n<blockquote>\n  <p>"This algorithm is done in $O(1)$ time using $O(n\\log n)$ work, with $n$-exponential probability, or alternatively, in $O(\\log n)$ time using $O(n)$ work, with $n$-exponential probability."</p>\n</blockquote>\n\n<p>Then Can we Implement this algorithm for a Quad-Core Computer (and just 4 threads) with $n=100,000$?</p>\n\n<p>The other question is what is the "$n$-exponential probability" in this sentence?</p>\n\n<p>Thanks.</p>\n', 'ViewCount': '152', 'Title': 'A question about parallel algorithm complexity', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-13T14:09:52.017', 'LastEditDate': '2012-12-13T14:09:52.017', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5058', 'Tags': '<algorithm-analysis><runtime-analysis><parallel-computing>', 'CreationDate': '2012-12-13T08:38:02.557', 'FavoriteCount': '0', 'Id': '7371'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>Suppose we have a function on GPU which calculates elements of array from which we need to select top K elements. The number of elements can be quite big so we can't store them in memory and our algorithm should be online. Another requirement for algorithm is its good parallelization so it could be effectively run on GPU.</p>\n\n<p>Is there any classic approach for the problem? I've thought only of building heap/balanced tree with $k$ elements and inserting new elements into it if new elements is better than existing and popping smaller element. It gives O($n$ log$k$) but it fails with parallelisation because requires synchronization between GPU threads on modifying tree.</p>\n", 'ViewCount': '147', 'Title': 'An online parallel algorithm for finding top K elements', 'LastActivityDate': '2013-01-02T14:58:14.107', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5273', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-01-02T14:39:37.623', 'Id': '7698'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am considering what elements an operating system for Network On Chip systems might have and the issue of file systems looms large. NoCs have an embedded heritage, and their domain of use (at least initially) is likely to be similar tasks to embedded systems.</p>\n\n<p>But I cannot find any studies of embedded system file access patterns - there are a number of published studies on scientific and super computing file access patterns I have been able to read but nothing on embedded systems.</p>\n\n<p>Can anyone point me to some good papers on this?</p>\n', 'ViewCount': '48', 'Title': 'Studies of file access patterns in embedded systems', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-06T20:02:27.383', 'LastEditDate': '2013-02-04T23:31:05.717', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'OwnerDisplayName': 'adrianmcmenamin', 'PostTypeId': '1', 'OwnerUserId': '6712', 'Tags': '<reference-request><parallel-computing><filesystems>', 'CreationDate': '2013-02-03T23:04:39.543', 'Id': '9487'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm really trying to wrap my head around vectorization but I can't seem to understand it. I don't know if I don't understand how to vectorize, or if I don't understand the array notation that's being used. An example of a loop I'm working on with school is below:</p>\n\n<pre><code>for (M=0; M&lt; number_of_iterations/2; M++){\n   for (i=2; i&lt;n-1; i++)  \n      for (j=1; j&lt;n-1; j++)  \n          y[i][j]= (x[i-1][j]+x[i][j-1]+x[i+1][j]+x[i][j+1]+x[i-2][j])/5.;\n</code></pre>\n\n<p>I'm not sure I quite understand the whole thing on dependencies - Is there a way to vectorize this using array notation as is, or do I need to adjust it somehow to account for dependencies throughout?</p>\n", 'ViewCount': '44', 'Title': 'Vectorizing and Array Notation?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-14T06:37:19.947', 'LastEditDate': '2013-02-14T06:37:19.947', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6733', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-02-13T03:16:26.610', 'Id': '9732'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am looking for good resources which have classified and solved typical large scale data processing in MapReduce framework (like graph algorithms, statistics, numerical algorithms ...). Any help is appreciated!</p>\n', 'ViewCount': '240', 'Title': 'Problem Solving in MapReduce Framework', 'LastEditorUserId': '157', 'LastActivityDate': '2013-03-17T21:28:11.707', 'LastEditDate': '2013-03-17T21:28:11.707', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '6550', 'Tags': '<algorithms><reference-request><parallel-computing>', 'CreationDate': '2013-03-11T16:34:31.853', 'FavoriteCount': '2', 'Id': '10453'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>How is Perfect shuffle a better interconnect scheme for parallel processing? For example if we consider a problem of sum reduction, I want to understand how this scheme is useful when implementing sum reduction in parallel , for example on a GPU?    </p>\n', 'ViewCount': '300', 'Title': 'Perfect shuffle in parallel processing', 'LastActivityDate': '2013-03-17T14:33:37.193', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10582', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<computer-architecture><parallel-computing>', 'CreationDate': '2013-03-17T05:12:07.500', 'FavoriteCount': '1', 'Id': '10572'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have $n\\times k$ matrix with $k&lt;n$ and I would like to find all its $n\\choose k$ submatrices which are $k\\times k$ matrices that are the concatenations of all possible $k$ rows. Actually I tried to do it with Matlab but it takes too long time specially when $n&gt;20$ and I couldn\'t find a way how to generate in parallel the $n\\choose k$ indices. </p>\n\n<p>I found online a paper titled <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1240626" rel="nofollow">A Parallel Algorithm for Enumerating Combinations</a> but they didn\'t provide their code in that paper.</p>\n', 'ViewCount': '116', 'Title': 'How to enumerate combinations in parallel', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-02T07:52:48.890', 'LastEditDate': '2013-04-02T07:52:48.890', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10902', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7487', 'Tags': '<algorithms><combinatorics><parallel-computing><matrices>', 'CreationDate': '2013-03-29T17:29:21.997', 'Id': '10899'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I\'ve been reading about hypercube connection template for parallel algorithms. The general scheme is explained in <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node124.html#SECTION04310000000000000000" rel="nofollow"><em>Designing and Building Parallel Programs</em> by Ian Foster</a> and it\'s pretty clear.</p>\n\n<p>What I don\'t understand is how it\'s applied on the merge sort <a href="http://www.mcs.anl.gov/~itf/dbpp/text/node127.html" rel="nofollow">in \xa711.4</a> The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<p>The point I\'m most interested is the <code>parallel_merge</code> function in the pseudocode, i.e. parallel merge algorithm.</p>\n\n<pre><code>procedure parallel_mergesort(myid, d, data, newdata)\nbegin\n  data = sequential_mergesort(data)\n  for dim = 1 to d\n    data = parallel_merge(myid, dim, data)\n  endfor\n  newdata = data\nend\n</code></pre>\n\n<p>Please, explain to me step by step, assuming we have an array of twelve elements $(3,1,5,7,4,2,8,9,4,2,7,5)$ and we\'ve broken this data to four processors like this: </p>\n\n<p>$\\qquad ((3,1,5),(7,4,2),(8,9,4),(2,7,5))$. </p>\n\n<p>What data will have each process after each iteration? I understand why we use the hybercube template in this algorithm, but why do we have exactly $i$ compare-exchanges at the $i$-th level? I mean, when $i=1$, we compare-exchange data from processes $1-2, 3-4, .. P-1, P$. That\'s not $1$, that\'s $P/2$? Do I misunderstand something?</p>\n', 'ViewCount': '282', 'Title': 'Parallel merge sort using hypercube connection template', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-10T20:43:03.117', 'LastEditDate': '2013-04-10T20:43:03.117', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'OwnerDisplayName': u'\u0418\u0433\u043e\u0440\u044c \u041c\u043e\u0440\u043e\u0437\u043e\u0432', 'PostTypeId': '1', 'Tags': '<algorithms><algorithm-analysis><runtime-analysis><parallel-computing><machine-models>', 'CreationDate': '2013-04-10T18:14:08.597', 'Id': '11205'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Studying parallel algorithms for CLRS, old edition Chapter 30.  Can some one explain with a simple example what is pointer jumping and how exactly it works ?   </p>\n', 'ViewCount': '185', 'Title': 'What is Pointer Jumping ?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T14:05:55.223', 'LastEditDate': '2013-04-21T14:05:55.223', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<algorithms><terminology><parallel-computing>', 'CreationDate': '2013-04-19T10:51:17.793', 'Id': '11407'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Parallel computing is not new but it is becoming common now a days. This is essentially driven by the need of the users (they need to process more data now) and also because of physical limitations on chip designing e.g power dissipation limits forcing to manufacturers to move to multicore CPUs.</p>\n\n<p>My question is how is the parallel computing different now from what they used to do 30 years back?       </p>\n', 'ViewCount': '112', 'Title': 'Parallel Computing: Past Vs Present', 'LastEditorUserId': '39', 'LastActivityDate': '2013-04-20T23:28:55.600', 'LastEditDate': '2013-04-20T23:28:55.600', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<parallel-computing><history>', 'CreationDate': '2013-04-20T13:32:59.313', 'FavoriteCount': '1', 'Id': '11426'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '235', 'Title': 'Best problems that are prone to parallelization?', 'LastEditDate': '2013-04-26T09:49:17.023', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'erkant', 'PostTypeId': '1', 'OwnerUserId': '9201', 'FavoriteCount': '1', 'Body': '<p>What are some problems that are prone to parallelization? When I think about this, the first thing that comes to my mind is matrix multiplication, which yields to faster calculations, meaning you can get speed ups easily. Any other examples like this?</p>\n', 'Tags': '<parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-26T09:49:17.023', 'CommentCount': '5', 'CreationDate': '2013-04-20T10:03:07.033', 'Id': '11565'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '144', 'Title': 'Which theoretical parallel model is closest to CUDA?', 'LastEditDate': '2013-11-19T12:33:52.610', 'AnswerCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '7617', 'FavoriteCount': '1', 'Body': '<p>Which theoretical parallel model is closest to CUDA/OpenCL programming model?</p>\n\n<p>For example, it fits at some degree to the generic Parallel Random Access Machine (PRAM) model. However, that is too generic, since it makes abstraction of various memory access latencies and synchronization issues.</p>\n\n<p>My question is which is the theoretical model that CUDA architecture fits closest (having in mind the hierarchical parallelism of threads and blocks of threads that cooperate)?</p>\n', 'Tags': '<parallel-computing><machine-models>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-19T12:33:52.610', 'CommentCount': '0', 'AcceptedAnswerId': '11806', 'CreationDate': '2013-05-05T13:21:27.837', 'Id': '11805'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I would like to learn more about Amdahl's Law and other similar topics.  In what branch of computer science would one place Amdahl's law?  Could someone point me to a textbook or further reading (aside from Wikipedia or other sites that are found on the first page of a Google search) that discusses it?</p>\n", 'ViewCount': '158', 'Title': "Amdahl's Law and Computer Science", 'LastEditorUserId': '1636', 'LastActivityDate': '2013-05-10T14:07:31.147', 'LastEditDate': '2013-05-09T00:10:39.940', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11934', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4429', 'Tags': '<reference-request><parallel-computing>', 'CreationDate': '2013-05-08T04:51:42.320', 'Id': '11878'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<h3>What I have</h3>\n\n<p>An iterative process based on the application of a very simple function to represent a rate, with total dependence among any iteration and its predecessor (one input parameter for (n)th iteration is the output parameter from the (n-1)th iteration). Specifically,</p>\n\n<p>$\\qquad  f(x, y, z) = x + \\frac{y - x}{z + 1}$</p>\n\n<p>with $x$ the modeled rate (typical value between 0 and 1),  $y$ the contribution of every new record to the learning about the rate (typical value 0 or 1) and $z$ some kind of convergence/learning speed parameter (typical value any integer between 30 and 900). While $y$ and $z$ are constant, $x$ would be the result of the previous calculation.</p>\n\n<h3>What I want</h3>\n\n<p>Some kind of manipulation of the function that allows some level of independence among sequential iterations, thus allowing parallelization.</p>\n\n<p>What I know (or believe to) so far:</p>\n\n<p>There is extensive published literature on parallel iterative methods, but most of them are about classical methods like <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_725.pdf" rel="nofollow">Map-Reduce for Machine Learning on Multicore</a> by Chu et al.</p>\n\n<p>But I\'m failing to recognize which of them I could "use" to help me with my simple function.</p>\n\n<p>Can someone help me pointing literature on the basics of function transformation aiming towards parallelization? Any thoughts on that matter will be very helpful.</p>\n', 'ViewCount': '90', 'Title': 'Parallelization of an iterative function application', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-28T17:28:44.497', 'LastEditDate': '2013-05-28T17:28:44.497', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8339', 'Tags': '<reference-request><parallel-computing>', 'CreationDate': '2013-05-24T19:55:24.613', 'Id': '12254'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I searched linear solver library and found out PETSc library which considered to be powerful and useful library. PETSc consists implementations of various iterative methods with preconditioners and sparse matrix storing methods. All methods are realized sequentially and in parallel using MPI.</p>\n\n<p>I was very glad for creaters of PETSc. I downloaded it and installed. However, when I start reading user's guide I encountered following text:</p>\n\n<pre><code>PETSc should not be used to attempt to provide a \u201cparallel linear solver\u201d in an otherwise sequential\ncode. Certainly all parts of a previously sequential code need not be parallelized but the matrix\ngeneration portion must be parallelized to expect any kind of reasonable performance. Do not expect\nto generate your matrix sequentially and then \u201cuse PETSc\u201d to solve the linear system in parallel.\n</code></pre>\n\n<p>I was surprised! <em>Did PETSc developers really parallelize only matrix generating part? What is a benefit of using PETSc as parallel solver if linear system solving part runs sequentially</em>?</p>\n", 'ViewCount': '118', 'Title': 'Does PETSc really give speedup?', 'LastActivityDate': '2013-06-14T12:35:01.667', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12671', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5207', 'Tags': '<parallel-computing><linear-algebra>', 'CreationDate': '2013-06-14T11:56:21.917', 'Id': '12670'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '141', 'Title': 'Largest reported speedup for a parallel computation?', 'LastEditDate': '2013-06-23T16:17:43.197', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '5323', 'FavoriteCount': '1', 'Body': u'<p>A parallel implementation of a computation is usually compared to a sequential version of the same computation.  The ratio of the time taken by the sequential version to the time taken by the parallel version is called the <em>speedup</em>.  So if 8 cores run their smaller parts of the computation in 2 time units, and one core runs the whole computation in 8 time units, then the speedup is 4.</p>\n\n<blockquote>\n  <p>What is the largest speedup reported for a real computation?</p>\n</blockquote>\n\n<p>It is possible to reach essentially infinite speedup in a search problem, since one of the parallel pieces of the search space may lead to a fast solution by that parallel instance, while the sequential solution has to work through the entire search space to get to that point.  More generally, I want to exclude any problem where one of the parallel processes can reach a shortcut.  So I am only interested in computations where the amount of work done by the parallel processes is the same as done by the sequential process.  This is common in solving PDEs by grid methods, or in discrete event simulation.\nSo with $n$ processors, one should never get more than $n$ speedup for these kinds of problems.</p>\n\n<p>I would also like to exclude embarrassingly parallel problems like parallel rendering, since there one really has a vast number of independent tiny problems.  I am interested in problems where it is not possible to partition the computation into strictly disjoint pieces.</p>\n\n<p>For a large speedup, one has to have many processors.  Given the restrictions on scope that I have conveniently labelled as "real computations", this question is then essentially about how efficiently the very large processor arrays that exist have been programmed.</p>\n\n<p>I am aware of reported speedups of ~500 using arrays of GPUs, but surely larger speedups exist.</p>\n\n<hr>\n\n<p><em>Edit:</em> To address some of the comments, I dug up some further motivation, which will hopefully be precise enough for the tastes of those more mathematically inclined.  This is quite different in style from the above, so I append it here as a postscript.</p>\n\n<p>For $n$ iid random variables $X_1, X_2,\\dots, X_n$ with mean $\\mu$,\ndenote their maximum by $X_{(n)}$ and their sum by $S_n$.\nO\'Brien has shown that $X_{(n)}/S_n \\to 0$ <a href="http://en.wikipedia.org/wiki/Convergence_of_random_variables#Almost_sure_convergence" rel="nofollow">almost surely</a> as $n \\to \\infty$ iff $\\mu &lt; \\infty$.</p>\n\n<p>Letting $X_i$ be the time taken by the $i$-th processor to complete its task, and assuming that there is a timeout/recompute mechanism to ensure that $\\mu$ is finite, this hints that the inverse of the speedup should be essentially unbounded.  (This is not necessarily the case: the techniques used may not carry over to the inverse, so I have to leave this a bit vague.)</p>\n\n<p>This is a nice theoretical prediction, and the question arises: <em>is this prediction borne out in practice?</em>  Or do implicit dependencies or diverging behaviours of the different processors (i.e. a breakdown in the iid assumption) tend to curtail this supposedly unbounded increase?</p>\n\n<p>The iid case corresponds to the embarrassingly parallel case.  Where the processors have to synchronize, independence breaks down.</p>\n\n<p>My question can therefore also be rephrased as: how badly does non-negligible dependence between parts of a computation as seen in practice affect the large-scale speedups that have been demonstrated?  Given that <a href="http://math.stackexchange.com/a/427162/3362">the bounds for expectation of the maximum in the non-independent case are quite weak</a>, some pointers to empirical data would be useful.</p>\n\n<ul>\n<li>G. L. O\'Brien, <em><a href="http://www.jstor.org/stable/3213043" rel="nofollow">A Limit Theorem for Sample Maxima and Heavy Branches in Galton-Watson Trees</a></em>, Journal of Applied Probability <strong>17</strong> 539\u2013545, 1980.</li>\n</ul>\n', 'Tags': '<reference-request><efficiency><parallel-computing>', 'LastEditorUserId': '5323', 'LastActivityDate': '2013-06-24T07:04:13.960', 'CommentCount': '18', 'AcceptedAnswerId': '12829', 'CreationDate': '2013-06-22T07:51:11.100', 'Id': '12826'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am trying to solve the question 6.12 in Arora-Barak (Computational Complexity: A modern approach). The question asks you to show that the\n$\\mathsf{PATH}$ problem (decide whether a graph $G$ has a path from a given node $s$ to another given node $t$) which is complete for $\\mathbf{NL}$ is also contained in $\\mathbf{NC}$ (this is easy). The question then also makes a remark that this implies that $\\mathbf{NL} \\subseteq \\mathbf{NC}$ which is not obvious to me.</p>\n\n<p>I think in order to show this, one has to show that $\\mathbf{NC}$ is closed under logspace reductions, i.e</p>\n\n<p>$$(1): B \\in \\mathbf{NC} \\hbox{ and } A \\le_l B \\Longrightarrow A \\in \\mathbf{NC}$$ </p>\n\n<p>where $\\le_l$ is the logspace reduction defined as</p>\n\n<p>$$A \\le_l B :\\Longleftrightarrow (\\exists M \\hbox{ TM}, \\forall x)[x \\in A \\Longleftrightarrow M(x) \\in B]$$</p>\n\n<p>($M$ is a TM which runs in logarithmic space).</p>\n\n<p>I would appreciate if someone could give a tip for proving the statement $(1)$.</p>\n', 'ViewCount': '105', 'Title': '$\\mathbf{NC}$ is closed under logspace reductions', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-22T15:25:58.423', 'LastEditDate': '2013-07-22T14:17:00.753', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13390', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9293', 'Tags': '<complexity-theory><reductions><closure-properties><complexity-classes><parallel-computing>', 'CreationDate': '2013-07-22T12:55:57.660', 'Id': '13387'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '117', 'Title': 'What is the impact of synchronisation overhead on parallel speedup?', 'LastEditDate': '2013-08-06T09:11:51.160', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8767', 'FavoriteCount': '0', 'Body': '<p>When implementing a parallel version of an algorithm, what is the impact of synchronization delays on speedup efficiency? Does this depend on the platform used?</p>\n\n<p>Is coarse-grained parallelism better than fine-grained parallelism in certain situations?</p>\n', 'ClosedDate': '2013-08-11T12:10:46.710', 'Tags': '<efficiency><parallel-computing><concurrency><synchronization>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-06T09:11:51.160', 'CommentCount': '5', 'CreationDate': '2013-08-05T12:48:59.380', 'Id': '13611'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Please suggest some books on Parallel Algorithms which teach efficient programming techniques like the ones taught in Udacity course(Introduction to Parallel Programming with CUDA). Also wanted to know that from which reference book or papers are the concepts in the udacity course on Parallel Computing taught...? I wish to study more about such techniques which will make me an efficient parallel programmer...!</p>\n', 'ViewCount': '456', 'Title': 'Reference book for Parallel Computing and Parallel algorithms.', 'LastEditorUserId': '947', 'LastActivityDate': '2013-08-22T12:39:11.050', 'LastEditDate': '2013-08-22T12:39:11.050', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '9779', 'Tags': '<reference-request><parallel-computing>', 'CreationDate': '2013-08-22T11:09:09.997', 'FavoriteCount': '1', 'Id': '13863'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Suppose we are using a dual core processor and we run 2 applications (1 with 80% resource requirements and the other with 20% resource requirements).<br><br>Now the first program is parallelizable for 40% and the second one not parallelizable.<br><br>My question is what would be the speed up achieved in this case.<br><br>By speed-up i mean the ratio between the execution times before and after parallelization.</p>\n', 'ViewCount': '43', 'Title': 'What would be the over all speed-up achieved?', 'LastActivityDate': '2013-08-27T18:50:23.160', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '13978', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8845', 'Tags': '<parallel-computing>', 'CreationDate': '2013-08-27T18:18:02.770', 'Id': '13977'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Wikipedia says that <a href="https://en.wikipedia.org/wiki/Shared_memory" rel="nofollow">shared memory</a> comes with lots of costs associated with cache coherence costs. But I thought the whole idea of shared memory is that all the CPUs access the same memory? So if one CPU changes that memory then other CPUs would access the same value? It would seem like this would require FEWER cache coherence costs? Is the idea that if one CPU changes its local cache before it writes to shared memory then other CPUs have to be notified?</p>\n', 'ViewCount': '249', 'Title': 'Why do you have to worry about cache coherence if you are using shared memory?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-10T10:29:48.617', 'LastEditDate': '2013-09-10T10:29:48.617', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '14241', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2605', 'Tags': '<computer-architecture><distributed-systems><parallel-computing>', 'CreationDate': '2013-09-10T00:25:05.073', 'Id': '14240'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '222', 'Title': 'Which of the common sorting algorithms can be parallelized?', 'LastEditDate': '2013-09-16T07:54:36.567', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10127', 'FavoriteCount': '2', 'Body': "<p>I want to know that whether which of the following algorithm can be parallelized?</p>\n\n<p>Bubble Sort,\nInsertion Sort,\nSelection Sort,\nShell Sort,\nQuick Sort,\nMerge Sort,\nRadix Sort.</p>\n\n<p>Those which can't be, please explain me briefly that how? Or please try to tell me in simple words that what is parallelism in sorting algorithms.</p>\n", 'ClosedDate': '2013-09-16T07:57:07.533', 'Tags': '<algorithms><sorting><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:54:36.567', 'CommentCount': '11', 'CreationDate': '2013-09-13T18:08:10.710', 'Id': '14294'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am playing around with MapReduce and wanted to ask why the <code>map</code> function takes two inputs <code>(key, value)</code>? Why not just pass along the value? </p>\n\n<p>Specifically, if you look at the <a href="http://en.wikipedia.org/wiki/MapReduce#Examples" rel="nofollow">word count example on Wikipedia page</a> you will see that the map function is:</p>\n\n<pre><code>function map(String name, String document):\n// name: document name\n// document: document contents\n  for each word w in document:\n    emit (w, 1)\n</code></pre>\n\n<p>However, the function never does anything with the parameter "name". Why even pass it in?</p>\n', 'ViewCount': '71', 'Title': 'Why does the map function in MapReduce take two parameters?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:38:52.103', 'LastEditDate': '2013-09-16T07:31:08.817', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '14350', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3174', 'Tags': '<terminology><programming-languages><parallel-computing>', 'CreationDate': '2013-09-13T19:41:03.380', 'Id': '14295'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following pseudo code for mapreduce to find the frequency of words in a collection of documents:</p>\n\n<pre><code>map(String key, String value)\n// key: document name\n// value: document contents\n for each word w in value\n   EmitIntermediate(w, "1")\n\nreduce(String key, Iterator values):\n// key: word\n// values: a list of counts\n  for each v in values:\n    result += ParseInt(v);\n    Emit(AsString(result));\n</code></pre>\n\n<p>So the map step gets each word as a key and output its frequency in a document. Does the reduce step sum the counts of each word? </p>\n', 'ViewCount': '515', 'ClosedDate': '2013-11-11T13:53:15.630', 'Title': 'MapReduce Pseudocode', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-20T15:59:54.100', 'LastEditDate': '2013-09-20T15:14:13.803', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10246', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-09-20T14:25:59.083', 'Id': '14470'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>What are the reasons for decreasing the number of threads in a parallel implementation ? </p>\n\n<p>Assume that we have two implementations, the first one with 4 threads, and a second one with 8 threads, and both have exactly the same run time. What are the various reasons to prefer the first one?</p>\n\n<p>I stress on the fact that the two implementations have the same run times.</p>\n\n<p>The obvious reasons are the following:</p>\n\n<ul>\n<li><p>If we have 8 processors, we can execute at the same time this implementation on two different sets of inputs thus performing twice the work,</p></li>\n<li><p>Less resources consumption by the OS which handles less number of threads.</p></li>\n</ul>\n\n<p>I'm looking for other reasons...</p>\n", 'ViewCount': '67', 'Title': 'Reasons for decreasing the number of threads in a parallel implementation', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-18T21:03:32.200', 'LastEditDate': '2013-10-18T21:03:32.200', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8767', 'Tags': '<parallel-computing><threads>', 'CreationDate': '2013-10-18T06:07:34.130', 'Id': '16190'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have  an option to learn a new language for parallel computing.  As a parallel programmer what are the reasons one might want to invest time to learn functional programming for parallel computing?  </p>\n', 'ViewCount': '48', 'Title': 'Functional Programming and Parallelism', 'LastActivityDate': '2013-10-18T15:47:00.990', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<parallel-computing><functional-programming>', 'CreationDate': '2013-10-18T06:11:32.620', 'Id': '16191'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm looking for an actual definition of a stream processing unit (SPU), preferably sourced. The context is not specifically graphics cards.</p>\n\n<p>I can find many, many articles explaining what they contain, or what they do in specific cases, but I need a definition for an informal class presentation. I have a general sense of what they are, but can't find much literature to back that up. However, even an informal definition would help.</p>\n\n<p>Would really appreciate any input.</p>\n", 'ViewCount': '31', 'Title': 'Stream processor definition', 'LastActivityDate': '2013-10-26T19:26:32.913', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10991', 'Tags': '<parallel-computing><graphics><signal-processing>', 'CreationDate': '2013-10-26T19:26:32.913', 'FavoriteCount': '1', 'Id': '16452'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Is there a master degree in HPC (High Performance Computing) in US? </p>\n\n<p>I am asking this question because in my early research I found no direct mentioning of HPC instead invidual courese in parallel programming , distributed computing etc in additional to cs generic core course. I did not found <em>so far</em> a focused program in HPC. </p>\n\n<p>Also, the programs somtimes are under math or engineering department which is little confuisng for me. </p>\n\n<p>I am currently waiting for responses from some of those universities..</p>\n\n<p>I would appreciate if you give me hint and refrence to those universities. </p>\n\n<p>Thans.... </p>\n', 'ViewCount': '56', 'ClosedDate': '2013-11-02T09:43:33.047', 'Title': 'Masters in HPC (High Performance Computing)', 'LastActivityDate': '2013-11-02T00:39:16.367', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11129', 'Tags': '<parallel-computing>', 'CreationDate': '2013-11-02T00:39:16.367', 'Id': '16637'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Given the problems of <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6307773&amp;url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6307773" rel="nofollow">Dark Silicon, diminishing returns from multicore</a>, and <a href="http://www.extremetech.com/computing/165331-intels-former-chief-architect-moores-law-will-be-dead-within-a-decade" rel="nofollow">soon to die Moore\'s law</a>, does that make sense to concentrate on parallel programming research and invest time and money in teaching and learning parallel programming and parallel algorithms?  Or this is going to be transient phase which will vanish within a decade or so?   </p>\n', 'ViewCount': '68', 'ClosedDate': '2013-11-11T18:14:47.967', 'Title': 'Is parallel programming a need of future and worth investing time and money?', 'LastActivityDate': '2013-11-11T11:51:19.407', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<parallel-computing>', 'CreationDate': '2013-11-11T11:51:19.407', 'Id': '17915'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I am trying to solve a 25k clauses 5k variables SAT problem. As it has been running for an hour (precosat) and I'd like to solve bigger ones afterwards, I'm looking for a multi-core SAT-Solver.</p>\n\n<p>As there seem to be many SAT-Solvers, I'm quite lost.</p>\n\n<p>Could anyone point me out the best one for my case?</p>\n\n<p>I'd also be happy if someone could give me the approximate time (if possible).</p>\n", 'ViewCount': '141', 'Title': 'Multicore SAT Solver', 'LastEditorUserId': '11382', 'LastActivityDate': '2014-02-05T14:17:31.637', 'LastEditDate': '2013-11-17T20:27:40.670', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '11382', 'Tags': '<algorithms><reference-request><parallel-computing><sat-solvers>', 'CreationDate': '2013-11-14T16:25:35.683', 'FavoriteCount': '1', 'Id': '18021'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>What makes neuromorphic architectures more efficient (less heat and power consumption) than von Neumann architecture for complex tasks? Except inspiration from biological systems, what are some formal results on this?\nAny reference to books or articles is appreciated as well. </p>\n', 'ViewCount': '89', 'Title': 'What makes neuromorphic computing architecture more efficient than von Neumann', 'LastActivityDate': '2013-11-16T21:11:49.633', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11368', 'Tags': '<complexity-theory><machine-learning><parallel-computing>', 'CreationDate': '2013-11-16T21:11:49.633', 'FavoriteCount': '2', 'Id': '18081'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have an ensemble of points in 3D space, represented by their coordinates $\\mathbf{c_i}\\equiv(x_i,y_i,z_i)^\\top$ . I need to calculate</p>\n\n<ul>\n<li>the distance between all these points: $\\quad\\forall i,j\\quad d_{ij} \\equiv |\\mathbf{d_{ij}}|\\equiv |\\mathbf{c_j}-\\mathbf{c_i}|$</li>\n<li>the scalar product between all distances that share a common coordinate. $\\quad\\forall i,j,k\\quad \\mathbf{d_{ij}}\\cdot\\mathbf{d_{ik}}$</li>\n</ul>\n\n<p>What is the fastest way to do this on a single thread? Is Fourier space going to be of any use? Can it be parallelized to make it even faster? If approximations are proposed, they should come with an error bound.</p>\n', 'ViewCount': '26', 'Title': 'fastest way to compute scalar product of an ensemble of vectors', 'LastActivityDate': '2013-12-01T01:22:24.163', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11724', 'Tags': '<algorithms><graphs><parallel-computing><fourier-transform>', 'CreationDate': '2013-12-01T01:22:24.163', 'Id': '18497'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Can we solve an EXPTIME-complete problem  in polynomial time given 2^N processors?(N is the size of input).</p>\n', 'ViewCount': '41', 'Title': 'EXPTIME and parallel computing', 'LastActivityDate': '2013-12-06T07:40:16.823', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11867', 'Tags': '<complexity-theory><parallel-computing>', 'CreationDate': '2013-12-06T07:40:16.823', 'Id': '18674'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the following very simple computer program:</p>\n\n<pre><code>for i = 1 to n:\n    y[i] = x[p[i]]\n</code></pre>\n\n<p>Here $x$ and $y$ are $n$-element arrays of bytes, and $p$ is an $n$-element array of words. Here $n$ is large, e.g., $n = 2^{31}$ (so that only a negligible fraction of the data fits in any kind of cache memory).</p>\n\n<p>Assume that $p$ consists of <strong>random numbers</strong>, uniformly distributed between $1$ and $n$.</p>\n\n<p>From the perspective of modern hardware, this should mean the following:</p>\n\n<ul>\n<li>reading $p[i]$ is cheap (sequential read)</li>\n<li>reading $x[p[i]]$ is very expensive (random reads; almost all reads are cache misses; we will have to fetch each individual byte from the main memory)</li>\n<li>writing $y[i]$ is cheap (sequential write).</li>\n</ul>\n\n<p>And this is indeed what I am observing. The program is very slow in comparison with a program that does only sequential reads and writes. Great.</p>\n\n<p>Now comes the question: <strong>how well does this program parallelise</strong> on modern multi-core platforms?</p>\n\n<hr>\n\n<p>My hypothesis was that this program does not parallelise well. After all, the bottleneck is the main memory. A single core is already wasting most of its time just waiting for some data from the main memory.</p>\n\n<p>However, this was <em>not</em> what I observed when I started experimenting with some algorithms where the bottleneck was this kind of operation!</p>\n\n<p>I simply replaced the naive for-loop with an OpenMP parallel for-loop (in essence, it will just split the range $[1,n]$ to smaller parts and run these parts on different CPU cores in parallel).</p>\n\n<p>On low-end computers, speedups were indeed minor. But on higher-end platforms I was surprised that I was getting excellent near-linear speedups. Some concrete examples (the exact timings may be a bit off, there is a lot of random variation; these were just quick experiments):</p>\n\n<ul>\n<li><p>2 x 4-core Xeon (in total 8 cores): factor 5-8 speedups in comparison to single-threaded version.</p></li>\n<li><p>2 x 6-core Xeon (in total 12 cores): factor 8-14 speedups in comparison to single-threaded version.</p></li>\n</ul>\n\n<p>Now this was totally unexpected. Questions:</p>\n\n<ol>\n<li><p>Precisely <strong>why does this kind of program parallelise so well</strong>? What happens in the hardware? (My current guess is something along these lines: the random reads from different thread are "pipelined" and the average rate of getting answers to these is much higher than in the case of a single thread.)</p></li>\n<li><p>Is it <strong>necessary to use multiple threads and multiple cores</strong> to obtain any speedups? If some kind of pipelining indeed takes place in the interface between the main memory and the CPU, couldn\'t a single-threaded application let the main memory know that it will soon need $x[p[i]]$, $x[p[i+1]]$, ... and the computer could start fetching the relevant cache lines from the main memory? If this is possible in principle, how do I achieve it in practice?</p></li>\n<li><p>What is the right <strong>theoretical model</strong> that we could use to analyse this kind of programs (and make <em>correct</em> predictions of the performance)?</p></li>\n</ol>\n\n<hr>\n\n<p><strong>Edit:</strong> There is now some source code and benchmark results available here: <a href="https://github.com/suomela/parallel-random-read" rel="nofollow">https://github.com/suomela/parallel-random-read</a></p>\n\n<p>Some examples of ballpark figures ($n = 2^{32}$):</p>\n\n<ul>\n<li>approx. 42 ns per iteration (random read) with a single thread</li>\n<li>approx. 5 ns per iteration (random read) with 12 cores.</li>\n</ul>\n', 'ViewCount': '358', 'Title': u'Parallelising random reads seems to work well \u2014 why?', 'LastEditorUserId': '232', 'LastActivityDate': '2013-12-14T11:41:53.687', 'LastEditDate': '2013-12-11T21:12:42.423', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '232', 'Tags': '<parallel-computing><cpu-cache><memory-hardware>', 'CreationDate': '2013-12-10T16:19:59.237', 'FavoriteCount': '2', 'Id': '18834'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Self-stabilizing algorithms are extremely useful in distributed systems,\n  but can the algorithm be applied concurrently to each computational node?</p>\n\n<p>My instinct is to say yes, but I can\'t help but imagine a \'Game Of Life\' scenario\n  where the state of the graph toggles between two states.\nWhat keeps the question alive in my mind is the possibility that both such states would be considered \'correct\', but then if the state were correct, wouldn\'t the graph stabilize?</p>\n\n<p>If they are not guaranteed to work in parallel,\n  can you provide an example where absolute convergence doesn\'t exist?</p>\n\n<hr>\n\n<p>A self-stabilizing algorithm is an algorithm (in all my experience, applied to arbitrary graphs and graph structures exclusively) that, upon repeated applications, guarantees <em>convergence</em> onto a correct state\u2014one that meets a set of requirements.  Consider a simple example:</p>\n\n<blockquote>\n  <p>You maintain a network of hospitals and data centers.\n  Each hospital must be connected to exactly one data center, and each data center must be connected to exactly two other data centers.  (You can imagine also attempting to minimize latency.)\n  When you set up the network, it is in a correct state\u2014all requirements are met.</p>\n  \n  <p>Weeks pass, and a giant storm takes out one of your data centers.\n  Being the forth-dimensional thinker you are, you planned for situations like this by putting into place a self-stabilizing algorithm that will automatically reconfigure the network to get back to a correct state.\n  You set up a daemon to oversee the operation.  It picks arbitrary nodes in your network and asks them if they are in a correct state.  If not, the daemon corrects that part of the network.</p>\n  \n  <p>All is well and you can stay asleep in bed (because everything catastrophic happens at 3am).</p>\n</blockquote>\n\n<p>The trick is, the daemon isn\'t a necessary part of this system\u2014it\'s just easier to think about it like this.\nIn reality, each node is its own computational unit\u2014constantly evaluating its own state and taking steps to correct itself if necessary.</p>\n\n<p>The introductory paragraphs of the <a href="http://en.wikipedia.org/wiki/Self-stabilization" rel="nofollow">Wikipedia article</a> on the subject give a very good technical overview:</p>\n\n<blockquote>\n  <p>Self-stabilization is a concept of fault-tolerance in distributed computing. A distributed system that is self-stabilizing will end up in a correct state no matter what state it is initialized with. That correct state is reached after a finite number of execution steps.</p>\n</blockquote>\n\n<p>Just as a piece of requested information that doesn\'t fit in well above:</p>\n\n<blockquote>\n  <p>When the graph is stable, no node has any faults with it.  Since no predicates apply to it, no actions are taken, and the graph remains stable.</p>\n</blockquote>\n', 'ViewCount': '47', 'Title': 'Are self-stabilizing algorithms guaranteed to work in parallel?', 'LastEditorUserId': '1854', 'LastActivityDate': '2014-05-03T22:37:43.923', 'LastEditDate': '2013-12-24T19:46:13.590', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '19259', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '1854', 'Tags': '<algorithms><parallel-computing>', 'CreationDate': '2013-12-24T04:06:36.440', 'Id': '19230'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '317', 'Title': 'Which algorithms can not be parallelized?', 'LastEditDate': '2014-01-11T10:29:16.157', 'AnswerCount': '3', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '11108', 'FavoriteCount': '5', 'Body': "<p>Is there any algorithm which is very difficult to parallelize or the research is still active?</p>\n\n<p>I wanted to know about any algorithm or any research field in parallel computing.</p>\n\n<p>Anything, I searched for, has a 'parallel' implementation done. Just want to do some study on any unexplored parallel computing field.</p>\n", 'Tags': '<algorithms><parallel-computing>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-12T18:17:54.130', 'CommentCount': '4', 'AcceptedAnswerId': '19674', 'CreationDate': '2014-01-11T02:59:29.227', 'Id': '19643'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '273', 'Title': 'Difference between Parallel and Concurrent programming?', 'LastEditDate': '2014-01-26T17:29:35.977', 'AnswerCount': '3', 'Score': '6', 'OwnerDisplayName': 'nish1013', 'PostTypeId': '1', 'OwnerUserId': '13158', 'FavoriteCount': '1', 'Body': '<p>When looking at concurrent programming, two terms are commonly used i.e. concurrent and parallel.</p>\n\n<p>And some programming languages specifically claim support for parallel programming, such as <a href="http://www.ateji.com/px/whitepapers/Ateji%20PX%20for%20Java%20v1.0.pdf" rel="nofollow">Java</a>.</p>\n\n<p>Does this means parallel and concurrent programming are actually different?</p>\n', 'Tags': '<terminology><parallel-computing><concurrency>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T17:31:11.030', 'CommentCount': '6', 'CreationDate': '2014-01-24T12:20:40.377', 'Id': '19987'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have some experiences of working on clusters and grids for parallel computing. But I wonder what distributed computing can do besides parallel computing?\nIf I understand correctly, parallel computing is to solve a common task by multiple processors (computers).</p>\n\n<p>Are the Internet and a intranet considered distributed systems? They don't necessarily involve parallel computing, or solving a common task by multiple computers.</p>\n", 'ViewCount': '47', 'Title': 'What can distributed computing do besides parallel computing?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-29T18:19:58.970', 'LastEditDate': '2014-01-29T17:02:54.650', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<parallel-computing><distributed-systems>', 'CreationDate': '2014-01-29T15:08:31.753', 'FavoriteCount': '1', 'Id': '20064'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a more general question for you. I\'m working in <em>Parallel Data Structures</em> and <em>Parallel Algorithms</em>. That is a nice topic with a lot of interesting challenges. However, I have some problem to argue for why is necessary use parallel solutions. The most common justification is "We need to take advantage of the current architectures", "Parallel alternatives speed up sequential alternatives", but it isn\'t enough. Some people can think: <em>OK, I believe you, but I don\'t have any problem waiting 10 minutes for my efficient sequential algorithm instead of 2 minutes for your parallel algorithm</em>. That reflects a weak spot of parallel solutions: they are, in general, difficult to program and need more time to debugging.</p>\n\n<p>So, now I\'m trying to find a better motivation for parallel solutions. I have this motivation now: <em>There are <strong>problems</strong> that don\'t have efficient solutions in sequential, and parallel solutions appear like the best alternative for those kind of <strong>problems</em></strong>. My question now for you is: <em>How characterize this kind of problems?</em>. I thought in problems with a lot of computation, like some graph problems, but I would like to know other view points.</p>\n', 'ViewCount': '70', 'ClosedDate': '2014-02-03T00:25:02.667', 'Title': 'Good motivation for parallel data structures and algorithms', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-31T13:54:00.220', 'LastEditDate': '2014-01-30T15:25:34.423', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13246', 'Tags': '<parallel-computing>', 'CreationDate': '2014-01-30T14:39:27.890', 'Id': '20112'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I wonder if high-performance computing and parallel computing always mean the same? \nIf not, </p>\n\n<ul>\n<li>what are the differences and relations between  them?</li>\n<li>what are some examples of high-performance computing that are not parallel computing?</li>\n<li>what are some examples of parallel computing that are not high-performance computing?</li>\n</ul>\n\n<p>Are high-throughput computing and high-performance computing the same concept?</p>\n', 'ViewCount': '87', 'Title': 'Differences and relations between high-performance/throughput computing and parallel computing?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-03T21:26:08.297', 'LastEditDate': '2014-02-03T16:15:43.477', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<terminology><parallel-computing><high-performance>', 'CreationDate': '2014-02-03T15:02:47.050', 'Id': '20253'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '216', 'Title': 'Parallel algorithm for finding the maximum in $\\log n$ time using $n / \\log n$ processors', 'LastEditDate': '2014-02-22T11:43:18.023', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '14907', 'FavoriteCount': '1', 'Body': "<p>We were presented in class with an algorithm for finding the maximum in an array in parallel in $O(1)$ time complexity with $n^2$ computers.</p>\n\n<p>The algorithm was:</p>\n\n<blockquote>\n  <p>Given an array A of length n:</p>\n  \n  <ol>\n  <li>Make a flag array B of length n and initialize it with zeroes with $n$ computers.</li>\n  <li>Compare every 2 elements and write 1 in B at the index of the minimum with $n^2$ computers.</li>\n  <li>find the index with the 0 in A with $n$ computers.</li>\n  </ol>\n</blockquote>\n\n<p>The lecturer teased us it could be done with $\\frac{n}{\\log n}$ computers and with $\\log n$ time complexity.</p>\n\n<p>After alot of thinking I couldn't figure out how to do it.\nAny idea?</p>\n", 'Tags': '<algorithms><search-algorithms><parallel-computing>', 'LastEditorUserId': '472', 'LastActivityDate': '2014-02-22T11:43:18.023', 'CommentCount': '0', 'AcceptedAnswerId': '21911', 'CreationDate': '2014-02-21T21:02:53.517', 'Id': '21910'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In section 6.3 of the paper <a href="https://smartech.gatech.edu/bitstream/handle/1853/6781/GIT-CC-93-55.pdf" rel="nofollow">Causal memory: definitions, implementations, and programming</a>, the authors define "data-race free" as follows:</p>\n\n<blockquote>\n  <p>Program $\\Pi$ is data-race free if, for all histories $H$ of $\\Pi$ and all causality orders $\\leadsto$ of $H$, if $H$ has a serialization that respects $\\leadsto$ (note that this implies that $H$ is sequentially consistent), then $H$ is data-race free with respect to $\\leadsto$.</p>\n</blockquote>\n\n<p>The one-sentence definition is too complicated for me to follow. It involves various notions and their relations. Specifically, a program is a set of its histories. Given a history, there may be more than one causality order for it, because there may be multiple writes of a value to a variable and thus more than one writes-into order. Let $H$ be a history with causality order $\\leadsto$. We have to consider whether it is sequentially consistent and data-race free with respect to $\\leadsto$.</p>\n\n<p>The following image enumerates all the possible classes of executions <em>which I can think of</em> produced by any program. The red crosses in the image indicate non-existence. For instance, the "path" $\\Pi - H_1 - \\leadsto_{12} - S_{12} - DRF_{12}$ means that $S_{12}$ is a serialization (thus sequentially consistent) of history $H_1$ that respects $\\leadsto_{12}$ but is not data-race free with respect to $\\leadsto_{12}$.</p>\n\n<p><img src="http://i.stack.imgur.com/cW61x.png" alt="data-race free"></p>\n\n<p><strong>EDIT: Here are some explanations of this image.</strong> For any program, there are five possible <em>classes of executions</em>. $H_0$ indicates the class of executions which have no causality orders at all (i.e., it causality orders are cyclic). The executions like $H_1$ have more than one causality orders, such as $\\leadsto_{11}, \\leadsto_{12},$ and $\\leadsto_{13}$. However, $H_1$ (as a representative of its class) is not sequentially consistent respecting $\\leadsto_{11}$ (denoted by the red cross over $S_{11}$); $H_1$ is sequentially consistent respecting $\\leadsto_{12}$ but is not data-race free with respect to $\\leadsto_{12}$ (denoted by the red cross over $DRF_{12}$); and $H_1$ is sequentially consistent respecting $\\leadsto_{13}$ and is data-race free with respect to $\\leadsto_{13}$. The cases for $H_2$, $H_3$, and $H_4$ are similar.</p>\n\n<p>Here comes my first problem:</p>\n\n<blockquote>\n  <p>(1). What are the classes of executions excluded by "data-race free"? (and does the image miss some classes of executions?)</p>\n  \n  <p><em>In my opinion,</em> only the executions like $H_1$ are excluded. The reason is that the "path" $\\Pi - H_1 - \\leadsto_{12} - S_{12} - DRF_{12}$ violates the definition of "data-race free".</p>\n</blockquote>\n\n<hr>\n\n<p>Immediately following the definition of data-race free, the authors show that data-race free programs produce only sequential consistent executions when run on causal memory:</p>\n\n<blockquote>\n  <p><strong>Theorem 5.</strong> If $\\Pi$ is data-race free, then all histories of $\\Pi$ with causal memory are sequentially consistent.</p>\n</blockquote>\n\n<p>In the first paragraph of its proof, it says that "Let $H$ be a finite (infinite) causal history and let $\\leadsto$ be a causality order that proves that $H$ is causal. We will prove that $H$ is data-race free with respect to $\\leadsto$ and has a serialization that respects $\\leadsto$". </p>\n\n<p>Here come my other two problems:</p>\n\n<blockquote>\n  <p>(2). Back to the image above, does the theorem mean that when run on causal memory data-race free programs produce only the executions like $H_3$ which are <em>both sequentially consistent and data-race free</em> with respect to <strong>each</strong> of its causality orders?</p>\n  \n  <p>(3). If so, is it right to conclude that the executions produced by data-race free programs when run on causal memory actually satisfy some stronger consistency model (though not formally defined) than sequentially consistent model?</p>\n</blockquote>\n\n<p>Besides the answers to my problems, any personal (or even subjective) comments worthy of note on the definition "data-race free" are appreciated.</p>\n', 'ViewCount': '59', 'Title': '"Data-race free" programs', 'LastEditorUserId': '4911', 'LastActivityDate': '2014-03-02T06:05:59.563', 'LastEditDate': '2014-03-02T06:05:59.563', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<terminology><parallel-computing><concurrency>', 'CreationDate': '2014-03-01T14:43:42.677', 'Id': '22158'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In section 17.4.5 <a href="http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.5" rel="nofollow">"Happens-before Order" of Java Language Specification</a>, a trace is examined for the notion of happens-before consistency. </p>\n\n<p>The trace is shown in the figure in which $A$ and $B$ are shared variables with initially $A = B = 0$. </p>\n\n<p><img src="http://i.stack.imgur.com/NXND7.png" alt="trace"></p>\n\n<p>An execution order of the trace is as follows. </p>\n\n<p><img src="http://i.stack.imgur.com/JvbFI.png" alt="executionorder"></p>\n\n<p>In this execution, the reads see writes that occur later in the execution order. The document says that it is happens-before consistent. I am quite confused about this. In my opinion, this execution order does not even satisfy program order. Why is it happens-before consistent?  </p>\n', 'ViewCount': '17', 'Title': 'Why is this execution happens-before consistent?', 'LastActivityDate': '2014-03-03T14:20:30.233', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<programming-languages><parallel-computing><concurrency>', 'CreationDate': '2014-03-03T14:20:30.233', 'Id': '22220'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I'm having trouble thinking about the following. If we have two machines 1 and 2 that evenly split a set of data points, does $k$-means separately, then averages the result, does this agree with just $k$-means on one machine?</p>\n\n<p>To be more specific, consider the following routine:</p>\n\n<blockquote>\n  <ol>\n  <li>Split an input dataset evenly between 1 and 2.</li>\n  <li>Initialize $k$ centroids that are the same on both 1 and 2.</li>\n  <li>Use $k$-means algorithm to find updated centroids.</li>\n  <li>Find a new set of $k$ centroids by averaging the corresponding centroid from 1 and 2. </li>\n  <li>Loop over 2. </li>\n  </ol>\n</blockquote>\n\n<p>Will this agree with just running $k$-means? I think that it is the averaging part that might work, but then I'm not sure if this routine finishes running. </p>\n", 'ViewCount': '34', 'Title': 'parallelizing $k$-means', 'LastActivityDate': '2014-03-05T07:58:00.607', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15297', 'Tags': '<algorithms><machine-learning><parallel-computing>', 'CreationDate': '2014-03-05T06:16:31.567', 'Id': '22292'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I have a cost function $f(X)=\\|\\hat{X}-X\\|_2$ to minimize which depends on a $s\\times s$ matrix $X$ where $\\hat{X}$ is given and $\\|X\\|_2=\\big(\\sum_{i,j}x_{ij}^2\\big)^{1/2} $. This matrix $X$ is generated by selecting only $s$ different rows from a matrix $B$ of dimension $n\\times s$. At the end, we are going to choose one matrix $X$ that generates the least cost $f(X)$ within all possible $n\\choose s$ submatrices of B. And so, this is a combinatorial problem that becomes complicated mostly when $n$ is big. </p>\n\n<p>So my question is can we find a suboptimal solution without going through all possible $n\\choose s$ submatrices and what kind of algorithm that I can apply to find such solution.</p>\n\n<p>My second question is can we apply a feature selection algorithm to find a suboptimal solution for a combinatorial problem.</p>\n', 'ViewCount': '156', 'Title': 'Suboptimal Solution for a combinatorial problem', 'LastEditorUserId': '7487', 'LastActivityDate': '2014-03-06T15:28:34.257', 'LastEditDate': '2014-03-06T01:52:17.043', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '22323', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7487', 'Tags': '<optimization><combinatorics><parallel-computing>', 'CreationDate': '2014-03-05T20:54:09.867', 'Id': '22316'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>I have an unknown $n$-dimensional vector $x$ whose analytical expression depends on the following sum $x = z + Ba$ where the vector $z$ and the matrix $B\\in \\mathbb{R}^{n\\times s}$ are given. So the $s$-dimensional vector $a$ is to be computed to find $x$.</p>\n\n<p>The only assumption that we have is $x=0$ when we project $x$ onto the space spanned by $s$ different rows (that we don\u2019t know their indices) of the matrix $B$ which has $n$ rows. To do this projection we can use $P_s\\in \\mathbb{R}^{n\\times n}$  which is $1$ on the diagonal entries that correspond to the $s$ selected rows of $B$ and $0$ elsewhere. Hence, $P_s x= P_s z + P_s Ba=0 \\implies a=-(P_sB)^{-1}P_sz$.</p>\n\n<p>The main issue is that we don\u2019t know the positions of these $s$ rows, so the problem is combinatorial and we need to go through all possible $n\\choose s$ projections to find the exact $x$ which corresponds to the least cost $f(x)=\\|y-Ax\\|_2$ where $\\|v\\|_2=\\big(\\sum_iv_i^2\\big)^{1/2}$, $y\\in \\mathbb{R}^{m\\times 1}$ and the matrix $A\\in \\mathbb{R}^{m\\times n}$ are given. </p>\n\n<p>So my question is how I can reformulate my problem as a mixed-integer quadratic programming to go through all possible $n\\choose s$ submatrices of $B$ formed by the $s$ selected rows and finally find the set of rows which corresponds to the least $f(x)$.</p>\n', 'ViewCount': '42', 'Title': 'How to reformulate my problem as a mixed-integer quadratic problem', 'LastEditorUserId': '7487', 'LastActivityDate': '2014-03-06T17:56:47.790', 'LastEditDate': '2014-03-06T16:54:52.230', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22348', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7487', 'Tags': '<optimization><combinatorics><parallel-computing><integer-programming>', 'CreationDate': '2014-03-06T09:30:18.030', 'Id': '22333'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I know these relations :  </p>\n\n<p>\\begin{gather}\n\\mathrm{NC}^1 \\subseteq \\mathrm{NC}^2 \\subseteq \\dots \\subseteq \\mathrm{NC}^i \\subseteq \\dots \\subseteq \\mathrm{NC} \\\\\n\\mathrm{NC}^i \\subseteq \\mathrm{AC}^i \\subseteq \\mathrm{NC}^{i+1} \\\\\n\\mathrm{NC}^1 \\subseteq \\mathrm{L} \\subseteq \\mathrm{NL} \\subseteq \\mathrm{AC}^1 \\subseteq \\mathrm{NC}^2 \\subseteq \\mathrm{P}\n\\end{gather}</p>\n\n<p>But I don't know how to compare an algorithm with time complexity of $\\mathrm{NC}^i$ to an algorithm with Polynomial complexity? </p>\n\n<p>For example, Topological sort $\\mathrm{NC}^2$ with BFS $\\mathcal{O}(|V| + |E|)$ </p>\n", 'ViewCount': '84', 'Title': 'How to compare algorithms in class NC time complexity with other classes?', 'LastEditorUserId': '15050', 'LastActivityDate': '2014-04-02T06:07:46.287', 'LastEditDate': '2014-04-02T06:07:46.287', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '15050', 'Tags': '<complexity-theory><time-complexity><parallel-computing>', 'CreationDate': '2014-03-17T13:29:30.280', 'FavoriteCount': '1', 'Id': '22709'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The definition of the both architecture looks pretty same. They are parallel computing architecture with different type of cores. </p>\n\n<p>What distinguish their definition, actually?</p>\n', 'ViewCount': '27', 'Title': "Heterogenous and Asymmetric Computing's differences", 'LastActivityDate': '2014-03-19T08:19:01.053', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15857', 'Tags': '<computer-architecture><parallel-computing>', 'CreationDate': '2014-03-19T08:19:01.053', 'Id': '22797'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>In a Dispatcher-Worker model of parallel computation, we have $N$ worker machines simultaneously working on the task (for example, computing the checksum of network packets). There is no synchronization between these workers. A dispatcher distributes total amount of work $S$ to these workers. </p>\n\n<p>Now consider the problem of analyzing the speed up of this model. To simplify the situation, assume the dispatcher evenly distributes work $\\frac{S}{N}$ to each worker. And each worker finishes it within $X_i \\sim \\mathcal{N}(\\frac{S}{N}, \\sigma^2)$ time, where $\\mathcal(\\mu, \\sigma^2)$ is the Gaussian distribution. Then I think the mathematical formula for the total amount of time needed is\n\\begin{equation}\nY = \\max(X_1, X_2, ..., X_N)\n\\end{equation}</p>\n\n<p>The question is how to compute the distribution of $Y$, so that analytically we could understand the parallel time needed, in terms of the number of workers $N$.</p>\n', 'ViewCount': '7', 'Title': 'Analyze Speed Up in A Dispatcher-Woker Model', 'LastActivityDate': '2014-03-23T06:49:29.170', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22962', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '848', 'Tags': '<parallel-computing>', 'CreationDate': '2014-03-23T01:07:58.890', 'FavoriteCount': '1', 'Id': '22955'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '32', 'Title': 'If a thread containing main terminates, can another thread do anything?', 'LastEditDate': '2014-03-29T20:42:43.353', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11711', 'FavoriteCount': '1', 'Body': '<p>I\'m studying threads in C and I have this theoretical question in mind that is driving me crazy.\nAssume the following code:</p>\n\n<pre><code>1) void main() {\n2)     createThread(...); // create a new thread that does "something"\n3) }\n</code></pre>\n\n<p>After line 2 is executed, two paths of execution are created. However I believe that immediately after line 2 is executed then it doesn\'t even matter what the new thread does, which was created at line 2, because the original thread that executed line 2 will end the entire program at its next instruction. Am I wrong? is there any chance the original thread gets suspended somehow and the new thread get its chance to do something (assume the code as is, no sync between threads or join operations are performed)</p>\n', 'Tags': '<parallel-computing><concurrency><threads>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T20:42:43.353', 'CommentCount': '1', 'AcceptedAnswerId': '23136', 'CreationDate': '2014-03-27T15:08:12.743', 'Id': '23129'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Thread_%28computing%29" rel="nofollow">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>Operating systems schedule threads in one of two ways:</p>\n  \n  <p><strong>Preemptive multitasking</strong> is generally considered the superior approach, as it allows the operating system to determine when a\n  context switch should occur. The disadvantage of preemptive\n  multithreading is that the system may make a context switch at an\n  inappropriate time, causing lock convoy, priority inversion or other\n  negative effects, which may be avoided by cooperative multithreading.</p>\n  \n  <p><strong>Cooperative multithreading</strong>, on the other hand, relies on the threads themselves to relinquish control once they are at a stopping\n  point. This can create problems if a thread is waiting for a resource\n  to become available.</p>\n</blockquote>\n\n<p>I wonder if in  parallel computing (writing and running parallelized programs in OpenMP, OpenMPI, pThread), which of Cooperative multithreading and Preemptive multitasking is/are used, or does the way OS scheduling threads have nothing to do with the multi-process or multi-thread within a parallelized program?</p>\n', 'ViewCount': '53', 'Title': 'Is the way an OS schedule threads related to parallel computing?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-27T11:29:29.867', 'LastEditDate': '2014-04-27T11:29:29.867', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<operating-systems><parallel-computing><threads>', 'CreationDate': '2014-04-26T18:16:11.453', 'Id': '24137'}},{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I know after some reading Amdahl law is a embarrassingly parallel programming model. Embarrassingly parallel means there is no communication and tasks work independently. Am I to assume a parallel and a non parallel aspect of the program is embarrassingly parallel?  If not, what makes it embarrassingly parallel?</p>\n', 'ViewCount': '29', 'Title': 'Amdahl law and parallelism', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T11:40:27.590', 'LastEditDate': '2014-04-30T11:40:27.590', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7173', 'Tags': '<terminology><parallel-computing>', 'CreationDate': '2014-04-29T13:34:20.843', 'Id': '24221'}},