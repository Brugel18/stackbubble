30_0:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am reading <a href="http://www.google.ch/url?sa=t&amp;rct=j&amp;q=leap%20search&amp;source=web&amp;cd=5&amp;ved=0CE8QFjAE&amp;url=http://dl.acm.org/ft_gateway.cfm?id=1376662&amp;type=pdf&amp;ei=sSVXT569JYjkiAL2hLyiCw&amp;usg=AFQjCNEOkxyk31CeifLNr72Cv_it7IATbg&amp;cad=rja" rel="nofollow">Mining Significant Graph Patterns by Leap Search</a> (Yan et al., 2008), and I am unclear on how their technique translates to the unlabeled setting, since $p$ and $q$ (the frequency functions for positive and negative examples, respectively) are omnipresent.</p>\n\n<p>On page 436 however, the authors clearly state that "In the following presentation, we are going to use the second setting (Figure 3) to illustrate the main idea. Nevertheless, the proposed technique can also be applied to the 1st [unlabeled] setting."</p>\n', 'ViewCount': '74', 'Title': 'Applying the graph mining algorithm Leap Search in an unlabeled setting', 'LastActivityDate': '2012-03-07T09:27:48.330', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '125', 'Tags': '<data-mining>', 'CreationDate': '2012-03-07T09:27:48.330', 'Id': '81'},30_1:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '177', 'Title': 'Identifying events related to dates in a paragraph', 'LastEditDate': '2012-03-17T16:51:17.110', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '191', 'FavoriteCount': '1', 'Body': '<p>Is there an <s><em>algorithmic</em></s> approach to identify that dates given in a paragraph correlate to particular events (phrases) in the paragraph?</p>\n\n<p>Example, consider the following paragraph:</p>\n\n<blockquote>\n  <p>In June 1970, the great leader took the oath. But it was only after May 1972, post the death of the Minister of State, that he took over the reins of the country. While he enjoyed popular support until Mid-1980, his influence began to fall thereafter.</p>\n</blockquote>\n\n<p>Is there an algorithm (deterministic or stochastic)# that can generate a 2-tuple (date, event), where the <em>event</em> is implied, by the paragraph, to have occured on the <em>date</em>? In the above case:</p>\n\n<ul>\n<li>(June 1970, great leader took oath)</li>\n<li><p>(May 1972, took over the reins)   </p>\n\n<p>or better yet</p></li>\n<li>(May 1972, <em>the great leader</em> took over the reins)</li>\n<li>(1980, fall in influence)  </li>\n</ul>\n\n<hr>\n\n<p>#Later addition</p>\n', 'Tags': '<algorithms><data-mining><natural-lang-processing>', 'LastEditorUserId': '191', 'LastActivityDate': '2012-03-17T18:56:43.870', 'CommentCount': '6', 'AcceptedAnswerId': '474', 'CreationDate': '2012-03-11T17:29:14.580', 'Id': '221'},30_2:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2298', 'Title': 'Why has research on genetic algorithms slowed?', 'LastEditDate': '2012-04-06T23:24:25.237', 'AnswerCount': '5', 'Score': '28', 'PostTypeId': '1', 'OwnerUserId': '258', 'FavoriteCount': '6', 'Body': '<p>While discussing some intro level topics today, including the use of genetic algorithms; I was told that research has really slowed in this field. The reason given was that most people are focusing on machine learning and data mining. <br>\n<strong>Update:</strong> Is this accurate? And if so, what advantages does ML/DM have when compared with GA?</p>\n', 'Tags': '<machine-learning><data-mining><evolutionary-computing><history>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-06T23:24:25.237', 'CommentCount': '2', 'AcceptedAnswerId': '565', 'CreationDate': '2012-03-21T01:17:25.527', 'Id': '561'},30_3:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>There are some documents to be indexed, that means I need to read the docs and extract the words and index them by storing at which document they appear and at which position.</p>\n\n<p>For each word initially I am creating a separate file. Consider 2 documents:</p>\n\n<ul>\n<li>document 1: \u201cThe Problem of Programming Communication with\u201d</li>\n<li>document 2: \u201cProgramming of Arithmetic Operations\u201d</li>\n</ul>\n\n<p>Here, there are 10 words, 8 unique. So I create 8 files (<code>the</code>, <code>problem</code>, <code>of</code>, <code>programming</code>, <code>communications</code>, <code>with</code>, <code>arithmetic</code>, <code>operations</code>).</p>\n\n<p>In each file, I will store at which document they appear and at what position. The actual structure I am implementing has lot more information but this basic structure will serve the purpose.</p>\n\n<pre><code>file name        file content\nthe              1 1\nproblem          1 2\nof               1 3 2 2\nprogramming      1 4 2 1\ncommunications   1 5\nwith             1 6\narithmetic       2 3\noperations       2 4\n</code></pre>\n\n<p>Meaning. the word is located at document 1, position 3 and at document 2, position 2.</p>\n\n<p>After the initial index is done I will concatenate all the files into a single index file and in another file I store the offset where a particular word will be found.</p>\n\n<p>index file: <code>1 1 1 2 1 3 2 2 1 4 2 1 1 5 1 6 2 3 2 4</code><br>\noffset file: <code>the 1 problem 3 of 5 programming 9 communications 13  with 15 arithmetic 17 operations 19</code></p>\n\n<p>So if I need the index information for <code>communications</code>, I will go to position 13 of the file and read up to position 15 excluded, in other words the offset of the next word.</p>\n\n<p>This is all fine for static indexing. But if I change a single index the whole file will need to be rewritten. Can I use a binary tree as the index file's structure, so that I can dynamically change the file content and update the offset somehow ? </p>\n", 'ViewCount': '188', 'Title': 'Maintaining search indices with binary trees', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T11:42:40.107', 'LastEditDate': '2012-04-22T11:42:40.107', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '173', 'Tags': '<data-structures><binary-trees><data-mining>', 'CreationDate': '2012-04-20T19:32:58.920', 'Id': '1398'},30_4:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '101', 'Title': 'Looking for a ranking algorithm that favors newer entries', 'LastEditDate': '2012-04-23T22:18:33.990', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '339', 'FavoriteCount': '2', 'Body': "<p>I'm working on a ranking system that will rank entries based on votes that have been cast over a period of time.  I'm looking for an algorithm that will calculate a score which is kinda like an average, however I would like it to favor newer scores over older ones.  I was thinking of something along the line of: </p>\n\n<p>$$\\frac{\\mathrm{score}_1 +\\ 2\\cdot \\mathrm{score}_2\\ +\\ \\dots +\\ n\\cdot \\mathrm{score}_n}{1 + 2 + \\dots + n}$$</p>\n\n<p>I was wondering if there were other algorithms which are usually used for situations like this and if so, could you please explain them?</p>\n", 'Tags': '<algorithms><data-mining>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-24T05:59:38.350', 'CommentCount': '3', 'AcceptedAnswerId': '1472', 'CreationDate': '2012-04-23T19:52:08.590', 'Id': '1471'},30_5:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m looking for good resources regarding <strong>Support Vector Machines</strong>, or suggestions where to start learning SVM.</p>\n\n<p>Already used references: </p>\n\n<ul>\n<li><p><a href="http://dde.binghamton.edu/kodovsky/svm/index.php?content=Material" rel="nofollow">Stanford ML course by Andrew Ng</a> is great place to star  </p></li>\n<li><p>A Tutorial on Support Vector Machines for Pattern Recognition, Burges, 1998<br>\n<a href="http://svms.org/tutorials/" rel="nofollow">SVM tutorials</a>   </p></li>\n<li><p>Neural Networks and Learning Machines, Third Edition  Learning with Kernels - SVM, A. Smola  </p></li>\n</ul>\n', 'ViewCount': '289', 'Title': 'Machine Learning - Support Vector Machines', 'LastEditorUserId': '867', 'LastActivityDate': '2014-04-27T23:29:08.580', 'LastEditDate': '2012-12-10T17:14:27.817', 'AnswerCount': '4', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2592', 'Tags': '<reference-request><machine-learning><data-mining>', 'CreationDate': '2012-09-26T20:12:16.970', 'FavoriteCount': '2', 'Id': '4750'},30_6:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '267', 'Title': 'Difference between decision tree and rule based reasoner', 'LastEditDate': '2012-10-28T11:16:16.827', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'Pio', 'PostTypeId': '1', 'OwnerUserId': '15189', 'FavoriteCount': '0', 'Body': "<p>I am new to this topic, and in some scientific papers I've been reading about prediction in sports I encountered the term rule based reasoner. Is it this term the same as a semantic reasoner( where the two main directions are forward and backwards chaining?). If yes, can you point out the difference between a decision tree and this? Because for me, it seems pretty much the same.</p>\n", 'Tags': '<terminology><artificial-intelligence><data-mining>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-01T04:30:47.137', 'CommentCount': '0', 'CreationDate': '2012-10-25T17:02:48.120', 'Id': '6352'},30_7:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="http://academia.stackexchange.com/questions/5079/are-there-hacks-or-smaller-scientific-search-engines-offering-you-context-sensit">I was asking on academia.se</a>, if anyone knows scientific search engines offering a <a href="http://en.wikipedia.org/wiki/Proximity_search_%28text%29" rel="nofollow">proximity operator</a> like Google Web Search does, while Google Scholar Search does not. That\'s sad, because this operator would be most useful for literature research offering you a nearly semantic/context-sensitive search and I\'ve seen requests for this feature in many blogs.</p>\n\n<p>The answer on my question linked above shows that 2 search engines offer something similar, but those operators also only work on titles and abstracts of papers, if I understand correctly. </p>\n\n<p>The wikipedia article doesn\'t explain what exactly limits the implementation of this kind of operator (exponentially rising indexing time, index size,... I\'m no search algorithm expert), but when Google Web Search offers it (the amount of web-text is much bigger), what possibly hinders the scientific search engines from offering it for full article text (cost-benefit ratio? I doubt this, as 99,9% of Google Web Search user don\'t know the AROUND(X) operator and the majority doesn\'t use <a href="http://www.googleguide.com/advanced_operators_reference.html" rel="nofollow">many operators</a> at all)?</p>\n\n<p>PS: If this question better fits SO, move it there, but I\'m more looking for a general explanation, what parameters determine and limit the implementation of an proximity operator.</p>\n', 'ViewCount': '96', 'Title': 'What limits the implementation of proximity operators for text indexing and searching?', 'LastEditorUserId': '298', 'LastActivityDate': '2012-11-10T15:30:49.783', 'LastEditDate': '2012-11-10T15:30:49.783', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '298', 'Tags': '<search-algorithms><data-mining><searching>', 'CreationDate': '2012-11-04T19:35:42.203', 'Id': '6477'},30_8:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From <a href="http://en.wikipedia.org/wiki/Information_retrieval" rel="nofollow">Wikipedia</a> </p>\n\n<blockquote>\n  <p><strong>Information retrieval</strong> is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text indexing.</p>\n</blockquote>\n\n<p>From <a href="http://en.wikipedia.org/wiki/Information_extraction" rel="nofollow">Wikipedia</a></p>\n\n<blockquote>\n  <p><strong>Information extraction (IE)</strong> is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.</p>\n</blockquote>\n\n<p>What are the relations and differences between information retrieval and information extraction? </p>\n\n<p>Thanks!</p>\n', 'ViewCount': '1071', 'Title': 'Relation and difference between information retrieval and information extraction?', 'LastActivityDate': '2014-04-01T20:34:24.520', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<data-mining>', 'CreationDate': '2012-12-05T15:04:29.120', 'FavoriteCount': '1', 'Id': '7181'},30_9:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In context of data mining, what does it mean for an association rule measure to be maximal?</p>\n\n<p>I cannot understand the term maximal in this context. </p>\n\n<p>I know of maximal independent sets in algorithms but cannot make out this term.</p>\n', 'ViewCount': '71', 'Title': 'In context of data mining, what does it mean for an association rule measure to be maximal?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-15T18:37:46.270', 'LastEditDate': '2013-03-17T18:03:56.097', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '5399', 'Tags': '<terminology><data-mining>', 'CreationDate': '2013-01-13T05:23:36.930', 'Id': '7914'},30_10:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I need a pool of photos (if possible with description) for my project. I mainly have to perform term extraction for semantic searching. Is there something available out there that is made available for such kinds of thing?</p>\n\n<p>I'm going to mine the description of these photos, maybe build ontologies and then perform search on them. The result of the mining might give useful terms about the pictures.</p>\n", 'ViewCount': '50', 'Title': 'Pool of photos for term extraction', 'LastEditorUserId': '98', 'LastActivityDate': '2013-05-03T15:11:37.810', 'LastEditDate': '2013-02-02T13:45:11.110', 'AnswerCount': '1', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6669', 'Tags': '<machine-learning><data-mining><data-sets><ontologies>', 'CreationDate': '2013-02-01T19:25:53.240', 'Id': '9404'},30_11:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Can anyone tell me the difference between the following in data mining? I am taking a class this semester and the professor is using the terms so frequently, I don't know what these mean anymore.</p>\n\n<ul>\n<li>Data point </li>\n<li>attribute </li>\n<li>feature</li>\n</ul>\n", 'ViewCount': '143', 'Title': 'Difference between Data point, attribute, feature?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-08T08:39:14.723', 'LastEditDate': '2013-02-08T08:39:14.723', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '9595', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<terminology><data-mining>', 'CreationDate': '2013-02-08T04:12:19.020', 'Id': '9587'},30_12:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am reading through PCA and it says that the maximum variance principal component has most of the information. Can we apply that to any data set? If a data set has n attributes and most of the attributes show high variance then can we infer that the dataset has captured lot of useful information? </p>\n\n<p>I am trying to understand how a high variance dataset contains useful information?</p>\n', 'ViewCount': '74', 'Title': 'Maximum variance and useful information of dataset', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-06T14:34:08.443', 'LastEditDate': '2013-03-06T07:11:43.823', 'AnswerCount': '1', 'CommentCount': '7', 'AcceptedAnswerId': '10307', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3004', 'Tags': '<terminology><data-mining><data-sets><statistics>', 'CreationDate': '2013-03-05T03:09:00.700', 'Id': '10279'},30_13:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm looking for something i would call rating functions.\nI'm searching for some literature about this concept.\nI'm not really sure about the terminology, but what I mean should be pretty obvious.</p>\n\n<p>A type of function that returns a rating of some input.</p>\n\n<p>Lets have a function that gets some input and returns a number between 0 and 1 as a rating, where 0 is bad and 1 is great. Everything between is well between bad and great depending.</p>\n\n<p>Lets assume inputs are just numbers</p>\n\n<p>$f\\colon \\mathbb{N} \\longrightarrow [0,1]$</p>\n\n<p>I would like if I have several rating functions be able to compose them.\nFor example if I have the rating functions $r_1$, $r_2$ I would like to compose both to a new rating function that returns a new rating in dependency to $r_1$ and $r_2$</p>\n\n<p>Now I'm looking for literature, but was unable to find any.\nCan somebody hint me into the correct direction?\nThe correct name for the concept I'm looking for would be great.</p>\n\n<h2>Edit</h2>\n\n<p>I want to implement various Rating Functions and want to combine them</p>\n\n<p>One functions could be</p>\n\n<pre><code>alwaysPerfect = (x) -&gt; 1\nalwaysBad = (x) -&gt; 0\nisOdd = (x) -&gt; x%2\ndistanceToOne = (x) -&gt;\n  x = 2 if x is 0\n  1/abs(x)\n</code></pre>\n\n<p>Anyone could implement this functions, but the contract for this functions would be to always return a value between 0 and 1</p>\n\n<p>I need to evaluate some data with various evaluation conditions. Writing these evaluation seperate small functions and combine them seems to be more clearer than writing one big function that does all the evalauting</p>\n", 'ViewCount': '49', 'Title': 'Looking for Rating Functions', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-11T09:01:17.707', 'LastEditDate': '2013-04-11T23:03:24.010', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7684', 'Tags': '<machine-learning><mathematical-analysis><data-mining>', 'CreationDate': '2013-04-11T16:02:37.263', 'Id': '11233'},30_14:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>One topic I\'ve recently looked at is <a href="http://en.wikipedia.org/wiki/Co-occurrence_networks" rel="nofollow">co-occurence networks</a> formed from <a href="https://twitter.com/" rel="nofollow">Twitter</a> tweets.  This is how I felt after looking at the tweets of random people:</p>\n\n<p><img src="http://i.stack.imgur.com/j7uxM.png" alt="Professor Farnsworth: &quot;I don\'t want to live on this planet anymore.&quot;"></p>\n\n<p>This leads me to the question:</p>\n\n<blockquote>\n  <p><strong>Question</strong>: Is it possible to discern the quality of web data from networks derived from it?</p>\n</blockquote>\n\n<p><em>A hypothetical situation</em>: Suppose we have two networks of comparable size derived from two different web sources: Network X is from a high-quality source (perhaps government documents) and Network Y is from a low-quality source (perhaps some random blog).  <em>Which network is from a higher quality source: Network X or Network Y?</em></p>\n\n<p>I\'m intentionally leaving out specifics about the networks in this question, since I\'m intending this to be more of a proof-of-concept style question.</p>\n', 'ViewCount': '30', 'Title': 'Is it possible to discern the quality of web data from networks derived from it?', 'LastActivityDate': '2013-04-18T17:16:53.770', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2254', 'Tags': '<graph-theory><data-mining>', 'CreationDate': '2013-04-18T17:16:53.770', 'Id': '11384'},30_15:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was told by my adviser (future one) to look into <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" rel="nofollow">libsvm</a> library or any other and try to get familiar with it.. to work on a programming project (on Machine Learning) (will start in a month).</p>\n\n<p>my background: Programming knowledge in Python, C.. doing Java now.</p>\n\n<p>So, where should I probably start? and How long it takes me to get into ML, SVM etc.. and be productive? Would I probably fit for this project? --considering my programming background (I so far have been much into Web development, wanted to take a change and have fun)</p>\n', 'ViewCount': '156', 'ClosedDate': '2013-05-12T15:53:20.787', 'Title': 'Can we understand SVM without knowledge of Machine Learning', 'LastActivityDate': '2013-05-07T22:02:28.073', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '11855', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8079', 'Tags': '<reference-request><machine-learning><data-mining>', 'CreationDate': '2013-05-07T14:16:44.423', 'Id': '11854'},30_16:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a problem where, there are a set of nodes and dependencies between them. I want to cluster them based on the maximum number of dependencies.  Dependencies can be thought of as number of edges connected.  I want to group those with maximum dependencies.</p>\n\n<p>For example in the set $\\{1,2,3\\}$ and $\\{3,5,7\\}$ if $\\{3,5,7\\}$ have more dependencies i need to group $\\{3,5,7\\}$. I know the dependencies beforehand. </p>\n\n<blockquote>\n  <p>Which algorithm will help to solve this problem?</p>\n</blockquote>\n', 'ViewCount': '41', 'LastEditorDisplayName': 'user742', 'Title': 'How to cluster nodes based on the number of dependencies', 'LastActivityDate': '2013-09-20T09:29:13.827', 'LastEditDate': '2013-09-20T09:29:13.827', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '947', 'Tags': '<graph-theory><data-mining><cluster>', 'CreationDate': '2013-06-19T12:58:01.557', 'Id': '12760'},30_17:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a large data set consisting of ca. 40 categorical data items and a few interval data items (real numbers, less than 5 such items). Most categories should have a lot of values that repeat themselves over and over, and very few that appear very rarely. Some categories are also overcategories of others (like country and city). The outcome of each data is either 1 if the event occurred, or 0 if it did not occur.</p>\n\n<p>The idea is to calibrate a machine learning model or a statistical model that can predict for every given data row the probability that the outcome is 1. The data set I will use will have at least 1 million rows.</p>\n\n<p>Which machine learning approaches and statistical models will perform well on such a task? My initial thoughts are logarithmic regression and support vector machines (with extensions like random forest).</p>\n\n<p>How do I deal with the interval data items? The easiest approach is obviously to convert different ranges into categories, which I think will not be a problem.</p>\n\n<p>What libraries and tools can I use when my data set has a size of 10 GB? I am interested in tools/libraries that include machine learning algorithms but also statistical tools to help me find attributes with significant influence on the outcome. I can code in Java and C++ at the moment. I looked into <a href="http://root.cern.ch/drupal/" rel="nofollow">Root</a>, a data analysis tool from CERN for large data sets, and its machine learning module <a href="https://twiki.cern.ch/twiki/bin/view/TMVA/WebHome" rel="nofollow">TMVA</a>, but it can only handle real numbers and integers as input as far as I know.</p>\n', 'ViewCount': '74', 'Title': 'Event Prediction through Machine Learning', 'LastEditorUserId': '8457', 'LastActivityDate': '2013-07-03T12:33:47.257', 'LastEditDate': '2013-07-03T12:33:38.647', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8977', 'Tags': '<machine-learning><data-mining>', 'CreationDate': '2013-07-02T14:31:57.473', 'Id': '13037'},30_18:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I understand that fuzzy clustering using FCM produces a membership matrix for the set of data points we feed to it. What characteristics will an anomalous cluster produced during this method have? (Considering I only have unlabelled data)</p>\n', 'ViewCount': '91', 'Title': 'Anomaly/outlier detection using fuzzy clustering', 'LastActivityDate': '2013-08-26T11:14:17.737', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9396', 'Tags': '<machine-learning><data-mining>', 'CreationDate': '2013-07-29T01:05:01.850', 'Id': '13481'},30_19:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '140', 'Title': 'Getting started with Data Mining', 'LastEditDate': '2013-08-09T08:00:55.437', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9583', 'FavoriteCount': '1', 'Body': "<p>This is the first time that I'm looking in depth into the topic, although I've always been curious.\nCould someone let me know about online resources (courses, tutorials, etc) and books that cover the basics of the topic?\nI'd like to explore both the theoretical part and the more practical part of Data Mining.</p>\n", 'ClosedDate': '2013-08-15T11:46:53.423', 'Tags': '<reference-request><data-mining>', 'LastEditorUserId': '9583', 'LastActivityDate': '2013-08-13T14:58:55.720', 'CommentCount': '2', 'AcceptedAnswerId': '13682', 'CreationDate': '2013-08-08T18:29:17.430', 'Id': '13677'},30_20:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '81', 'Title': 'Document definition in information retrieval', 'LastEditDate': '2013-08-23T10:35:35.517', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9741', 'FavoriteCount': '1', 'Body': '<p>Is there common formal definition of <strong>Document</strong> in information retrieval field? In many researches authors don\'t define the term <strong>Document</strong>(maybe because it is evident for them). Wikipedia says "text file, document (computer science) a computer file that contains text" but it doesn\'t seem as common formal definition. </p>\n\n<p>Do you know the common formal definition of <strong>Document</strong>, or, if not, do you know any researches where that term was defined? </p>\n', 'Tags': '<terminology><data-mining><information-retrieval>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-23T10:35:35.517', 'CommentCount': '4', 'AcceptedAnswerId': '13825', 'CreationDate': '2013-08-19T19:20:52.343', 'Id': '13823'},30_21:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am not experienced in machine learning but I have been looking at the Python toolklit <code>scikit-learn</code>. I have thousands of mammograms and I have written an algorithm to find the outline of the images. I have some cases where the outline of the mammogram is not correct. I would like to be able to take these thousands of outlines of mammograms that are arrays (of different lengths) of the x and y coordinates of the edge and to classify them. This would allow me to find the outliers and also possibly allow me to categorise the images. Does a clustering algorithm exists that would allow me to do this? </p>\n\n<p>Edit 1:</p>\n\n<p>Expected outline<img src="http://i.stack.imgur.com/IKcP2.png" alt="enter image description here"></p>\n\n<p>Unwanted outline<img src="http://i.stack.imgur.com/lU3UT.png" alt="enter image description here"></p>\n', 'ViewCount': '57', 'Title': 'Classification of 2d arrays of outlines', 'LastEditorUserId': '9884', 'LastActivityDate': '2013-09-04T08:37:13.340', 'LastEditDate': '2013-09-04T08:37:13.340', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '14033', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9884', 'Tags': '<machine-learning><data-mining>', 'CreationDate': '2013-08-29T12:31:12.340', 'Id': '14019'},30_22:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Several popular machine learning algorithms such as Logistic regression or Neural networks require its inputs to be numeric.</p>\n\n<p>What I\'m interested in is how you make these algorithms work on non-numeric inputs (such as short strings).</p>\n\n<p>As an example, let\'s say we\'re building an email classification system (spam/not spam), where one of the input features is the sender address.</p>\n\n<p>To be able to use a learning algorithm, we need to represent the sender address as a number. One way is to simply number the senders 1..n. Our training set might then look like this:</p>\n\n<p><img src="http://i.stack.imgur.com/SxSxf.png" alt="inputs for machine learning"></p>\n\n<p>This won\'t work, however, because algorithms such as Logistic regression or Neural networks learn patterns in the input data, while in our example, the output looks totally random to the algorithm. Indeed, once in a university class, we tried to train a Neural network on a dataset that looked like this and the network was unable to learn anything (the learning curve was flat).</p>\n\n<p>Would you use Logistic regression or Neural networks in this example at all? If yes, in which way? If not, what would be a good way to classify emails based on sender address?</p>\n\n<p>A perfect answer would discuss the email classification example as well as handling short strings in ML in general.</p>\n', 'ViewCount': '144', 'Title': 'String inputs in Machine Learning', 'LastEditorUserId': '10260', 'LastActivityDate': '2013-09-21T07:27:59.863', 'LastEditDate': '2013-09-21T07:27:59.863', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10260', 'Tags': '<machine-learning><data-mining>', 'CreationDate': '2013-09-21T05:26:20.390', 'FavoriteCount': '1', 'Id': '14488'},30_23:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a doubly link-list like this</p>\n\n<pre><code>typedef struct Record\n{\n   int i;\n   Record* next;\n   Record* prev;\n}Record;\n</code></pre>\n\n<p>I have over 5 trillions of records that I need to handle, now that I need to retrieve all of them and sort them out.\nIf its size was small, I could borrow stl's vector or list to do the job but now that it is too huge, I have no idea how to save the object data before sorting is performed</p>\n\n<p>my function prototype</p>\n\n<pre><code>void sortRec(Record**recToSort,bool bASC){}\n</code></pre>\n", 'ViewCount': '71', 'ClosedDate': '2013-10-15T10:05:09.320', 'Title': 'linklist and memory issues', 'LastActivityDate': '2013-10-03T19:11:33.200', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10450', 'Tags': '<algorithms><data-structures><data-mining><linked-lists>', 'CreationDate': '2013-10-02T03:53:31.060', 'Id': '14744'},30_24:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am thinking over a data mining problem but unable to interpret it properly ..</p>\n\n<p>" how change in fiscal policies affect economic health of a country "</p>\n\n<p>Indicators of economic health - GDP and Inflation </p>\n\n<p>For example, I have a set of predictor variables such as \nBank Capital to assests ratio.\nClaims on Central Govt.\nClaims on other sectors of domestic economy\nInterest rates spread</p>\n\n<p>and Target variables -\nStocks traded turnover ratio\nGDP Growth(annual %)\nGDP per capita\nInflation customer prices</p>\n\n<p>Do I have to find strength of relationship between predictor and target variables here ..?\nCan I use regression to find that ? Can this problem be interpreted as classification problem anyhow ? </p>\n', 'ViewCount': '20', 'ClosedDate': '2013-10-21T22:57:03.267', 'Title': 'Data mining interpretation', 'LastActivityDate': '2013-10-21T18:41:19.253', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'Sys', 'PostTypeId': '1', 'Tags': '<data-mining>', 'CreationDate': '2013-10-02T08:04:58.367', 'Id': '16312'},30_25:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m not sure if this is the correct stack exchange or correct tags, but my question is as follows: </p>\n\n<p>I am working on a sort-of ratings system for players in a particular game. After allowing the ratings to develop for many games, I have set up a "database" (not sure if this is the correct term) for matches: a point in the database would consist of the score difference between the players (i.e if it\'s >0 player 1 won) and the rating discrepancy between the two players before this match.</p>\n\n<p>The idea is to use this database in order to predict the score difference of a match that has not yet occurred: I measure the rating discrepancy between the 2 players, and lookup in my database for the score differences in games of a "similar" rating discrepancy, and based on these games I am able to predict, say, a rough probability of each score difference occurring in this match that has not yet happened.</p>\n\n<p>I actually have two questions: </p>\n\n<p>1) what is a good approach to using the database to predict the probability of each score difference, i.e what qualifies as a "similar" rating discrepancy, how do I deal with extreme cases where the rating discrepancy is very large and I have few data examples of such matches (should I relax my definition of "similar"?), etc. A bit of googling shows that maybe I am looking for something relating to data clusters.</p>\n\n<p>2)how would I go about implementing an algorithm as above? are there good standard implementations of such classification algorithms? I am writing in C# if it makes a difference. </p>\n', 'ViewCount': '47', 'Title': 'Analysis and classification based on data points', 'LastActivityDate': '2013-11-06T20:13:58.630', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8247', 'Tags': '<data-structures><machine-learning><data-mining><cluster>', 'CreationDate': '2013-11-06T17:36:07.680', 'Id': '16775'},30_26:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am studying Cluster Analysis and I came across BIRCH (Balanced Iterative Reduction Using hierarchical) <a href="http://en.wikipedia.org/wiki/BIRCH_(data_clustering)" rel="nofollow">http://en.wikipedia.org/wiki/BIRCH_(data_clustering)</a> CF Tree insertion algorithm. I am confused what does branching factor(B), threshold for diameter of cluster in a leaf(T) means and in general how the algorithm works. It will be of immense help if someone can please explain it using an example. Thanks in advance.</p>\n', 'ViewCount': '114', 'Title': 'How does BIRCH (Balanced Iterative Reduction Using hierarchical) CF Tree insertion algorithm work?', 'LastEditorUserId': '11596', 'LastActivityDate': '2014-01-24T00:22:51.553', 'LastEditDate': '2013-11-24T19:24:04.190', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11596', 'Tags': '<data-mining>', 'CreationDate': '2013-11-24T17:59:21.077', 'Id': '18304'},30_27:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Is there an efficient way to cluster nodes in a graph using Jaccard similarity such that each cluster has at least k nodes? </p>\n\n<p>Jaccard similarity between nodes i and j: Let S be the set of neighbours of i and T be the set of neighbours of j. Then, the similarity between i and j is given by  $\\frac{|(S \\cap T)|}{|(S \\cup T)|}$.</p>\n', 'ViewCount': '49', 'Title': 'Is there an efficient way to cluster a graph according to Jaccard similarity?', 'LastActivityDate': '2014-02-18T23:36:13.597', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4868', 'Tags': '<graph-theory><data-mining><cluster>', 'CreationDate': '2013-12-20T21:54:54.990', 'Id': '19167'},30_28:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to write an algorithm that detects the most common subset of at least size $k$, from a collection of sets.  If there are ties for the most common subset, I want the one of them whose size is as large as possible.</p>\n\n<p>For example if I have:</p>\n\n<pre><code>s1 = {A, B, C   }\ns2 = {A, B, C, D}\ns3 = {   B, C, D}\n</code></pre>\n\n<p>Then the most common subset of size $\\ge k=2$ is {B, C}. As another example, if I have:</p>\n\n<pre><code>s1 = {A, B, C  D}\ns2 = {A, B, C, D}\ns3 = {   B, C, D}\n</code></pre>\n\n<p>Then the most common subset of size $\\ge k=2$ is {B, C, D}. It's important that in this instance the algorithm would give me {B, C, D} and not {B, C}, {B, D} etc. Note that I'm not interested in the longest common subset (a different problem), I'm interested in the longest most common subset if you will. I also don't care about enumerating all the different subsets, I just want to find the most common.</p>\n\n<p>Is there an efficient algorithm for this problem?</p>\n\n<p>I have an algorithm for this problem, but I don't think it's very efficient. For $k=2$ I enumerate all subsets of size 2 and count how many times each one appears in the collection. If the most-frequently occurring pair is more frequently occurring than any other pair then that must be the most common subset. If there is more than one with the same (maximum) frequency then I look at the sets they are contained in. If these overlap exactly then I take the union of the pairs and that gives me the most common subset (with size > 2).</p>\n\n<p>I think this could be related to the maximum clique problem but I'm not certain.</p>\n\n<p>Note that just taking the intersection does not give the correct answer.  For instance, if I have</p>\n\n<pre><code>s1 = {A, B      }\ns2 = {      C, D}\ns3 = {A,    C, D}\n</code></pre>\n\n<p>then the intersection is the empty set, but the most common subset is {C, D}.</p>\n", 'ViewCount': '69', 'Title': 'Most common subset of size $k$', 'LastEditorUserId': '755', 'LastActivityDate': '2014-01-16T22:42:41.270', 'LastEditDate': '2014-01-16T22:42:41.270', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19762', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '12876', 'Tags': '<algorithms><graphs><sets><data-mining>', 'CreationDate': '2014-01-15T21:58:59.930', 'Id': '19755'},30_29:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a timeseries dataset of a quantity measured over the period of a week. I want to verify if the data is varying in a diurnal fashion with the help of some mathematical measure. Does any such measure exist?</p>\n', 'ViewCount': '21', 'ClosedDate': '2014-02-02T11:28:36.667', 'Title': 'Model for diurnal nature of data', 'LastActivityDate': '2014-01-31T05:44:09.183', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9396', 'Tags': '<algorithms><data-mining>', 'CreationDate': '2014-01-21T17:35:09.867', 'Id': '19872'},30_30:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a data set with two classes: one class has at most 2000 members while the size of the second class is unlimited, though it is typically in the hundreds of thousands.  I have read that it is problematic to use a decision tree to naively classify this data.  My question is, how how I modify the data or the classification scheme to classify such data, using a decision tree at some point?</p>\n', 'ViewCount': '26', 'Title': 'Decision Tree with Unbalanced Data', 'LastActivityDate': '2014-03-29T00:54:58.383', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15425', 'Tags': '<machine-learning><data-mining><classification>', 'CreationDate': '2014-03-08T17:40:23.287', 'Id': '22406'},30_31:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We're accessing an API of a web system for obtaining product information. We require some additional information, which is not available through the API. This information is publically available for each product visually - through the source code for each item on its item page. We've written an algorithm which parses this information from the web page for each item, but that will be highly ineffective in the long run, since the algorithm will simply stop working if and when they decide to change the source code ( for example - redesign of the front end ).</p>\n\n<p>I feel like there should be a supservised learning approach to this problem, however I'm unaware if there exist such solutions.</p>\n\n<p>What are some good aproaches to this kind of problems ?</p>\n\n<p>Regards.</p>\n", 'ViewCount': '36', 'Title': 'web content "mining" using supervised learning tequniques', 'LastActivityDate': '2014-04-28T04:35:02.273', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14979', 'Tags': '<machine-learning><data-mining>', 'CreationDate': '2014-03-09T17:22:29.470', 'Id': '22434'},30_32:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>As we all know, Matrix Factorization is an effective method to do rating prediction jobs in recommender systems. Thanks to the work of Yahuda Koren. My question is why MF can do this job? What's the physical meaning behind it?</p>\n", 'ViewCount': '53', 'Title': 'Physical Meaning Behind Matrix Factorization', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T22:24:45.737', 'LastEditDate': '2014-03-14T09:37:20.077', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '15674', 'Tags': '<machine-learning><data-mining><recommendation-systems>', 'CreationDate': '2014-03-14T04:51:40.857', 'FavoriteCount': '1', 'Id': '22609'},30_33:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Which kind of machine learning concept / model is used in <a href="http://www.20q.net" rel="nofollow">20 Questions</a>?<br>\nIs this kind of thing best solved by a neural network?<br>\nWhere I can read something about it?</p>\n', 'ViewCount': '44', 'Title': 'What kind of model is used by 20 Questions?', 'LastActivityDate': '2014-03-20T22:24:41.397', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '22885', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '15915', 'Tags': '<machine-learning><neural-networks><data-mining>', 'CreationDate': '2014-03-20T16:48:31.383', 'FavoriteCount': '2', 'Id': '22872'},30_34:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am curious as to what steps one would reasonably need to take to perform an extraction-based text summarizer.</p>\n\n<p>I\'ve taken a look at some papers I\'ve found on Google such as <a href="https://wwws.cs.umn.edu/tech_reports_upload/tr2000/00-034.ps" rel="nofollow">this one</a>, which explains that UPGMA is the best clustering algorithm (out of the tested set). I\'ve also found <a href="http://acl.ldc.upenn.edu/I/I05/I05-2004.pdf" rel="nofollow">this one</a> to be interesting, regarding single and multi-document summarization.</p>\n\n<p>I\'m unclear as to whether I\'d need to combine these techniques for summarization or whether it would suffice to use a tool like Gensim to model a corpus of documents and just extract the sentences with the highest vector values.</p>\n', 'ViewCount': '23', 'Title': 'Document clustering for summarization', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-11T06:35:58.613', 'LastEditDate': '2014-04-11T06:35:58.613', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16190', 'Tags': '<machine-learning><data-mining><natural-lang-processing><cluster>', 'CreationDate': '2014-04-10T22:42:44.780', 'Id': '23661'},30_35:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $X$ be an $m\\times n$ ($m$: number of records, and $n$: number of attributes) dataset.  When the number of attributes $n$ is large and the dataset $X$ is noisy, classification gets more complicated and the classification accuracy decreases. One way to over come this problem is to use linear transformation, i.e., perform classification on $Y=XR$, where $R$ is an $n\\times p$ matrix, and $p\\leq n$. I was wondering how linear transformation simplifies classification? and why classification accuracy increases if we do classification on the transformed data $Y$ when $X$ is noisy?</p>\n', 'ViewCount': '57', 'Title': 'Why linear transformation can improve classification accuracy when the dimensionality of data is high?', 'LastEditorUserId': '683', 'LastActivityDate': '2014-04-28T22:51:22.710', 'LastEditDate': '2014-04-27T05:08:49.667', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '17110', 'Tags': '<machine-learning><data-mining><linear-algebra><matrices><classification>', 'CreationDate': '2014-04-27T01:46:05.837', 'FavoriteCount': '1', 'Id': '24147'},30_36:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>One approach for clustering a high dimensional dataset is to use linear transformation, and the most common approaches are PCA and random projection (where random projection arises from the Johnson-Lindenstrauss Lemma). I was wondering why we can't use other random transformation  s like when our transformation matrix $R$ was drawn from a uniform distribution?</p>\n", 'ViewCount': '16', 'Title': 'Subspace clustering with random transformation', 'LastActivityDate': '2014-04-30T04:26:59.197', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17110', 'Tags': '<data-mining><linear-algebra><classification><cluster>', 'CreationDate': '2014-04-30T03:43:25.603', 'Id': '24249'}