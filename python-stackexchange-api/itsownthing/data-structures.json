90:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When placing geometric objects in a quadtree (or octree), you can place objects that are larger than a single node in a few ways:</p>\n\n<ol>\n<li>Placing the object\'s reference in every leaf for which it is contained</li>\n<li>Placing the object\'s reference in the deepest node for which it is fully contained</li>\n<li>Both #1 and #2</li>\n</ol>\n\n<p>For example:</p>\n\n<p><img src="http://i.stack.imgur.com/Z2Bj7.jpg" alt="enter image description here"></p>\n\n<p>In this image, you could either place the circle in all four of the leaf nodes (method #1) or in just the root node (method #2) or both (method #3).</p>\n\n<p>For the purposes of querying the quadtree, which method is more commonplace and why?</p>\n', 'ViewCount': '125', 'Title': 'Which method is preferred for storing large geometric objects in a quadtree?', 'LastEditorUserId': '11', 'LastActivityDate': '2012-03-06T20:22:05.510', 'LastEditDate': '2012-03-06T19:47:07.427', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '11', 'Tags': '<graphics><data-structures><computational-geometry>', 'CreationDate': '2012-03-06T19:34:22.793', 'Id': '7'},91:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Someone I know is planning on implementing a text editor in the near future, which prompted me to think about what kind of data structures are fast for a text editor. The most used structures are apparently <a href="http://en.wikipedia.org/wiki/Rope_%28computer_science%29">ropes</a> or <a href="http://en.wikipedia.org/wiki/Gap_buffer">gap buffers</a>.</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Van_Emde_Boas_tree">Van Emde Boas trees</a> are just about the fastest priority queues around, if you don\'t mind an upper bound on the number of items you can put into it and a large initialization cost. My question is whether there exists some data structure that is just as fast as the van Emde Boas tree, but supports text editor operations.</p>\n\n<p>We only need to support up to $m$ characters in our data structure (so if $\\log m = 32$, then we support up to 4GB worth of ASCII characters). We are allowed $\\sqrt{m}$ time to initialize a new data structure. We\'d like to support the following operations:</p>\n\n<ul>\n<li>Insert a character at position $i$ in $O(\\log \\log m)$ (and thereby increasing the position of every subsequent character by 1).</li>\n<li>Delete a character at position $i$ in $O(\\log \\log m)$.</li>\n<li>Return the character at position $i$ in $O(\\log \\log m)$.</li>\n</ul>\n\n<p>So, insert(0,\'a\') followed by insert(0,\'b\') results in "ba".</p>\n\n<p>Even better would be this:</p>\n\n<ul>\n<li>Return a \'pointer\' to some index $i$ in $O(\\log \\log m)$.</li>\n<li>Given a \'pointer\', return the character at this position in $O(1)$.</li>\n<li>Given a \'pointer\', remove the character at this position in $O(1)$.</li>\n<li>Given a \'pointer\', add a character at this position in $O(1)$ and return a pointer to the following position.</li>\n<li>(optional) Given a \'pointer\', return a \'pointer\' to the next/previous character in $O(1)$.</li>\n</ul>\n', 'ViewCount': '502', 'Title': 'Is there an equivalent of van Emde Boas trees for ropes?', 'LastActivityDate': '2012-03-23T16:48:31.203', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '681', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<data-structures>', 'CreationDate': '2012-03-09T12:47:19.520', 'FavoriteCount': '3', 'Id': '154'},92:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '303', 'Title': 'Efficient compression of unlabeled trees', 'LastEditDate': '2012-03-20T20:21:39.787', 'AnswerCount': '5', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '98', 'FavoriteCount': '2', 'Body': '<p>Consider unlabeled, rooted binary trees. We can <em>compress</em> such trees: whenever there are pointers to subtrees $T$ and $T&#39;$ with $T = T&#39;$ (interpreting $=$ as structural equality), we store (w.l.o.g.) $T$ and replace all pointers to $T&#39;$ with pointers to $T$. See <a href="http://cs.stackexchange.com/a/177/98">uli\'s answer</a> for an example.</p>\n\n<p>Give an algorithm that takes a tree in the above sense as input and computes the (minimal) number of nodes that remain after compression. The algorithm should run in time $\\cal{O}(n\\log n)$ (in the uniform cost model) with $n$ the number of nodes in the input.</p>\n\n<p>This has been an exam question and I have not been able to come up with a nice solution, nor have I seen one.</p>\n', 'Tags': '<algorithms><data-structures><trees><binary-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-03-20T20:21:39.787', 'CommentCount': '11', 'AcceptedAnswerId': '174', 'CreationDate': '2012-03-09T17:54:38.383', 'Id': '168'},93:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>It is often said that hash table lookup operates in constant time: you compute the hash value, which gives you an index for an array lookup. Yet this ignores collisions; in the worst case, every item happens to land in the same bucket and the lookup time becomes linear ($\\Theta(n)$).</p>\n\n<p>Are there conditions on the data that can make hash table lookup truly $O(1)$? Is that only on average, or can a hash table have $O(1)$ worst case lookup?</p>\n\n<p><em>Note: I\'m coming from a programmer\'s perspective here; when I store data in a hash table, it\'s almost always strings or some composite data structures, and the data changes during the lifetime of the hash table. So while I appreciate answers about perfect hashes, they\'re cute but anecdotal and not practical from my point of view.</em></p>\n\n<p>P.S. Follow-up: <a href="http://cs.stackexchange.com/questions/477/for-what-kind-of-data-are-hash-table-operations-o1">For what kind of data are hash table operations O(1)?</a></p>\n', 'ViewCount': '7425', 'Title': '(When) is hash table lookup O(1)?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-05T22:31:57.543', 'LastEditDate': '2012-03-17T21:56:32.630', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '32', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<time-complexity><data-structures><hash-tables>', 'CreationDate': '2012-03-12T19:01:07.577', 'FavoriteCount': '12', 'Id': '249'},94:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>When implementing a dictionary ('I want to look up customer data by their customer IDs'), the typical data structures used are hash tables and binary search trees. I know for instance that the C++ STL library implements dictionaries (they call them maps) using (balanced) binary search trees, and the .NET framework uses hash tables under the hood.</p>\n\n<blockquote>\n  <p>What are the advantages and disadvantages of these data structures? Is there some other option that is reasonable in certain situations?</p>\n</blockquote>\n\n<p>Note that I'm not particularly interested in cases where the keys have a strong underlying structure, say, they are all integers between 1 and n or something.</p>\n", 'ViewCount': '4628', 'Title': 'Hash tables versus binary trees', 'LastActivityDate': '2012-03-13T01:43:44.703', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '278', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<algorithms><data-structures><binary-trees><hash-tables>', 'CreationDate': '2012-03-13T00:30:42.750', 'FavoriteCount': '2', 'Id': '270'},95:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Suppose the datatype for a BST is defined as follows (in SML)</p>\n\n<pre><code>datatype \'a bst_Tree =\n   Empty\n | Node of (int * \'a) * \'a bst_Tree * \'a bst_Tree;\n</code></pre>\n\n<p>So there are two cases one in which the BST is <code>Empty</code> or it can have a (key,value) as well as two children.</p>\n\n<p>Now, for the case of an AVL where the condition is </p>\n\n<blockquote>\n  <p>In an AVL tree, the heights of the two child subtrees of any node differ by at most one<br>\n  <sub>- <a href="http://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL tree Wikipedia</a></sub></p>\n</blockquote>\n\n<p>I want to able to create a height function for use to check whether the tree is balanced. My current setup is as follows</p>\n\n<pre><code>fun height (Empty) = ~1\n  | height (Node(v, Empty, Empty)) = 0 (* Redundant matching because of third case *)\n  | height (Node(v, L, R)) = 1 + Int.max(height(L),height(R))\n</code></pre>\n\n<p>I tried to separate the Tree into three conditions</p>\n\n<ol>\n<li>A empty Tree</li>\n<li>A Tree with a root node</li>\n<li>A populated tree</li>\n</ol>\n\n<p>The reason for this is that there does not seem to be a canonical source on what the value is for the height of an <code>Empty</code> Tree as opposed to one in which only has a root. For the purposes of my balance function it did the job, but I rather try to understand why there isn\'t a canonical answer for the height of an <code>Empty</code> Tree.</p>\n\n<p>There is a canonical answer, in a matter of speaking on <a href="http://en.wikipedia.org/wiki/Tree_height#Terminology" rel="nofollow">Wikipedia</a> but while initially doing research on this on Stack Overflow I arrived at many comments stating this to be wrong/incorrect/unconventional</p>\n\n<blockquote>\n  <p>Conventionally, the value \u22121 corresponds to a subtree with no nodes, whereas zero corresponds to a subtree with one node.)</p>\n</blockquote>\n\n<p>I grabbed the question from which my uncertainty appeared</p>\n\n<p><a href="http://stackoverflow.com/questions/2209777/what-is-the-definition-for-the-height-of-a-tree">What is the definition for the height of a tree?</a></p>\n\n<blockquote>\n  <p>I think you should take a look at the <a href="http://www.itl.nist.gov/div897/sqg/dads/" rel="nofollow">Dictionary of Algorithms and Data Structures</a> at the NIST website. There definition for height says a single node is height 0.</p>\n  \n  <p>The <a href="http://www.itl.nist.gov/div897/sqg/dads/HTML/tree.html" rel="nofollow">definition of a valid tree</a> does include an empty structure. The site doesn\'t mention the height of such a tree, but based on the definition of the height, it should also be 0.</p>\n</blockquote>\n', 'ViewCount': '271', 'Title': 'What is the height of an empty BST when using it in context for balancing?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T16:28:40.197', 'LastEditDate': '2012-04-22T16:28:40.197', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '346', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '151', 'Tags': '<algorithms><data-structures><terminology>', 'CreationDate': '2012-03-13T21:46:09.257', 'Id': '335'},96:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1205', 'Title': 'Not all Red-Black trees are balanced?', 'LastEditDate': '2012-04-12T05:55:31.997', 'AnswerCount': '2', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '2', 'Body': '<p>Intuitively, "balanced trees" should be trees where left and right sub-trees at each node must have "approximately the same" number of nodes.</p>\n\n<p>Of course, when we talk about red-black trees*(see definition at the end) being balanced, we actually mean that they are <em>height</em> balanced and in that sense, they are balanced. </p>\n\n<p>Suppose we try to formalize the above intuition as follows:</p>\n\n<blockquote>\n  <p><strong>Definition:</strong> A Binary Tree is called $\\mu$-balanced, with $0 \\le \\mu \\leq \\frac{1}{2}$, if for every node $N$, the inequality</p>\n  \n  <p>$$ \\mu \\le \\frac{|N_L| + 1}{|N| + 1} \\le 1 - \\mu$$</p>\n  \n  <p>holds and for every $\\mu&#39; \\gt \\mu$, there is some node for which the above statement fails. $|N_L|$ is the number of nodes in the left sub-tree of $N$ and $|N|$ is the number of nodes under the tree with $N$ as root (including the root).</p>\n</blockquote>\n\n<p>I believe, these are called <em>weight-balanced</em> trees in some of the literature on this topic. </p>\n\n<p>One can show that if a binary tree with $n$ nodes is $\\mu$-balanced (for a constant $\\mu \\gt 0$), then the height of the tree is $\\mathcal{O}(\\log n)$, thus maintaining the nice search properties.</p>\n\n<p>So the question is:</p>\n\n<blockquote>\n  <p>Is there some $\\mu \\gt 0$ such that every big enough red-black tree is $\\mu$-balanced?</p>\n</blockquote>\n\n<hr>\n\n<p>The definition of Red-Black trees we use (from Introduction to Algorithms by Cormen et al):</p>\n\n<p>A binary search tree, where each node is coloured either red or black and</p>\n\n<ul>\n<li>The root is black</li>\n<li>All NULL nodes are black</li>\n<li>If a node is red, then both its children are black.</li>\n<li>For each node, all paths from that node to descendant NULL nodes have the same number of black nodes.</li>\n</ul>\n\n<p>Note: we don\'t count the NULL nodes in the definition of $\\mu$-balanced above. (Though I believe it does not matter if we do).</p>\n', 'Tags': '<data-structures><binary-trees><search-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T03:55:44.147', 'CommentCount': '19', 'AcceptedAnswerId': '375', 'CreationDate': '2012-03-14T00:15:10.127', 'Id': '342'},97:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1248', 'Title': 'AVL trees are not weight-balanced?', 'LastEditDate': '2012-04-12T05:55:19.533', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '1', 'Body': '<p>In a previous <a href="http://cs.stackexchange.com/questions/342/not-all-red-black-trees-are-balanced">question</a> there was a definition of weight balanced trees and a question regarding red-black trees. </p>\n\n<p>This question is to ask the same question, but for <a href="http://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL trees</a>. </p>\n\n<p>The question is, given the definition of $\\mu$-balanced trees as in the other question,</p>\n\n<blockquote>\n  <p>Is there some $\\mu \\gt 0$ such that all big enough AVL trees are $\\mu$-balanced?</p>\n</blockquote>\n\n<p>I presume there is only one definition of AVL trees and there is no ambiguity.</p>\n', 'Tags': '<data-structures><binary-trees><search-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T03:56:47.213', 'CommentCount': '0', 'AcceptedAnswerId': '424', 'CreationDate': '2012-03-15T16:56:32.180', 'Id': '421'},98:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>From the answers to <a href="http://cs.stackexchange.com/questions/249/when-is-hash-table-lookup-o1">(When) is hash table lookup O(1)?</a>, I gather that hash tables have $O(1)$ worst-case behavior, at least amortized, when the data satisfies certain statistical conditions, and there are techniques to help make these conditions broad.</p>\n\n<p>However, from a programmer\'s perspective, I don\'t know in advance what my data will be: it often comes from some external source. And I rarely have all the data at once: often insertions and deletions happen at a rate that\'s not far below the rate of lookups, so preprocessing the data to fine-tune the hash function is out.</p>\n\n<p>So, taking a step out: given some knowledge about data source, how can I determine whether a hash table has a chance of having $O(1)$ operations, and possibly which techniques to use on my hash function?</p>\n', 'ViewCount': '658', 'Title': 'For what kind of data are hash table operations O(1)?', 'LastActivityDate': '2013-05-06T13:07:46.187', 'AnswerCount': '4', 'CommentCount': '10', 'AcceptedAnswerId': '1351', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<data-structures><time-complexity><hash-tables>', 'CreationDate': '2012-03-17T21:55:43.600', 'FavoriteCount': '2', 'Id': '477'},99:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '385', 'Title': 'Saving on array initialization', 'LastEditDate': '2012-04-03T06:49:19.723', 'AnswerCount': '1', 'Score': '15', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '3', 'Body': '<p>I recently read that it is possible to have arrays which need not be initialized, i.e. it is possible to use them without having to spend any time trying to set each member to the default value. i.e. you can start using the array as if it has been initialized by the default value without having to initialize it. (Sorry, I don\'t remember where I read this).</p>\n\n<p>For example as to why that can be surprising:</p>\n\n<p>Say you are trying to model a  <em>worst</em> case $\\mathcal{O}(1)$ hashtable (for each of insert/delete/search) of integers in the range $[1, n^2]$.</p>\n\n<p>You can allocate an array of size $n^2$ bits and use individual bits to represent the existence of an integer in the hashtable. Note: allocating memory is considered $\\mathcal{O}(1)$ time.</p>\n\n<p>Now, if you did not have to initialize this array at all, any sequence of say $n$ operations on this hashtable is now worst case $\\mathcal{O}(n)$.</p>\n\n<p>So in effect, you have a "perfect" hash implementation, which for a sequence of $n$ operations uses $\\Theta(n^2)$ space, but runs in $\\mathcal{O}(n)$ time!</p>\n\n<p>Normally one would expect your runtime to be at least as bad as your space usage!</p>\n\n<p>Note: The example above might be used for an implementation of a sparse set or sparse matrix, so it is not only of theoretical interest, I suppose.</p>\n\n<p>So the question is:</p>\n\n<blockquote>\n  <p>How is it possible to have an array like data-structure which allows us to skip the initialization step?</p>\n</blockquote>\n', 'Tags': '<data-structures><arrays>', 'LastEditorUserId': '139', 'LastActivityDate': '2012-04-03T06:49:19.723', 'CommentCount': '6', 'AcceptedAnswerId': '550', 'CreationDate': '2012-03-19T08:03:50.153', 'Id': '492'},910:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2489', 'Title': 'Does there exist a priority queue with $O(1)$ extracts?', 'LastEditDate': '2012-03-19T19:11:15.323', 'AnswerCount': '9', 'Score': '25', 'PostTypeId': '1', 'OwnerUserId': '92', 'FavoriteCount': '6', 'Body': '<p>There are a great many data structures that implement the priority-queue interface:</p>\n\n<ul>\n<li>Insert: insert an element into the structure</li>\n<li>Get-Min: return the smallest element in the structure</li>\n<li>Extract-Min: remove the smallest element in the structure</li>\n</ul>\n\n<p>Common data structures implementing this interface are (min)<a href="http://en.wikipedia.org/wiki/Heap_%28data_structure%29">heaps</a>.</p>\n\n<p>Usually, the (amortized) running times of these operations are:</p>\n\n<ul>\n<li>Insert: $\\mathcal{O}(1)$ (sometimes $\\mathcal{O}(\\log n)$)</li>\n<li>Get-Min: $\\mathcal{O}(1)$</li>\n<li>Extract-Min: $\\mathcal{O}(\\log n)$</li>\n</ul>\n\n<p>The <a href="http://en.wikipedia.org/wiki/Fibonacci_heap">Fibonacci heap</a> achieves these running times for example. Now, my question is the following:</p>\n\n<blockquote>\n  <p>Is there a data structure with the following (amortized) running times?</p>\n</blockquote>\n\n<ul>\n<li>Insert: $\\mathcal{O}(\\log n)$</li>\n<li>Get-Min: $\\mathcal{O}(1)$</li>\n<li>Extract-Min: $\\mathcal{O}(1)$</li>\n</ul>\n\n<p>If we can construct such a structure in $\\mathcal{O}(n)$ time given sorted input, then we can for instance find line intersections on pre-sorted inputs with $o\\left(\\frac{n}{\\log n}\\right)$ intersections strictly faster than if we use the \'usual\' priority queues.</p>\n', 'Tags': '<data-structures><amortized-analysis><priority-queues>', 'LastEditorUserId': '31', 'LastActivityDate': '2013-01-04T21:08:37.440', 'CommentCount': '4', 'AcceptedAnswerId': '537', 'CreationDate': '2012-03-19T14:31:46.663', 'Id': '524'},911:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '472', 'Title': 'Creating a Self Ordering Binary Tree', 'LastEditDate': '2012-04-12T05:55:04.773', 'AnswerCount': '2', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '79', 'FavoriteCount': '1', 'Body': "<p>I have an assignment where I need to make use a binary search tree and alter it to self order itself such that items that are accessed the most (have a higher priority) are at the top of the tree, the root being the most accessed node.</p>\n\n<p>The professor gave me the BST and node struct to work with, but trying to get my head around the algorithm to update the tree as things are being inserted is confusing me.</p>\n\n<p>I know that as the insert is happening, it checks if the current node's data is less or greater than the current node, then recursively goes in the correct direction until it finds a null pointer and inserts itself there. and after it is inserted it increases the priority by 1.</p>\n\n<pre><code>template &lt;class Type&gt;\nvoid BinarySearchTree&lt;Type&gt; ::  insert( const Type &amp; x, BinaryNode&lt;Type&gt; * &amp; t )\n{\n    if( t == NULL )\n        t = new BinaryNode&lt;Type&gt;( x, NULL, NULL );\n    else if( x &lt; t-&gt;element )\n        insert( x, t-&gt;left );\n    else if( t-&gt;element &lt; x )\n        insert( x, t-&gt;right );\n    else\n        t-&gt;priority++;  // Duplicate; do nothing for right now\n}\n</code></pre>\n\n<p>Now I need to figure out when the node is equal, how to re-order the tree so that the current node (who is equal to an already existing node) finds the existing node, increases that node's priority, then shifts it up if the root is a lower priority.</p>\n\n<p>I think I have the idea down that the AVL logic would work, and when a shift would take place, it would be a single rotation right or a single rotation left.</p>\n\n<p>Here's where I'm confused,  don't really know where to start with creating an algorithm to solve the problem. Since the AVL algorithm works with keeping track of the balance of a tree, then rotating nodes left or right accordingly, this tree doesn't need to worry about being balanced, just that the nodes with the highest priority not have children with a higher priority.</p>\n", 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-12T05:55:04.773', 'CommentCount': '0', 'AcceptedAnswerId': '610', 'CreationDate': '2012-03-21T00:05:18.697', 'Id': '559'},912:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '704', 'Title': 'What combination of data structures efficiently stores discrete Bayesian networks?', 'LastEditDate': '2012-04-30T21:39:20.727', 'AnswerCount': '1', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '373', 'FavoriteCount': '1', 'Body': "<p>I understand the theory behind Bayesian networks, and am wondering what it takes to build one in practice. Let's say for this example, that I have a Bayesian (directed) network of 100 discrete random variables; each variable can take one of up to 10 values.</p>\n\n<p>Do I store all the nodes in a DAG, and for each node store its Conditional Probability Table (CPT)? Are there other data structures I should make use of to ensure efficient computation of values when some CPTs change (apart from those used by a DAG)?</p>\n", 'Tags': '<data-structures><machine-learning>', 'LastEditorUserId': '41', 'LastActivityDate': '2014-02-06T21:19:05.753', 'CommentCount': '3', 'AcceptedAnswerId': '587', 'CreationDate': '2012-03-21T05:11:40.937', 'Id': '580'},913:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m looking for a data structure that stores a set of strings over a character set $\\Sigma$, capable of performing the following operations. We denote $\\mathcal{D}(S)$ as the data structure storing the set of strings $S$.</p>\n\n<ul>\n<li><code>Add-Prefix-Set</code> on $\\mathcal{D}(S)$: given some set $T$ of (possibly empty) strings, whose size is bounded by a constant and whose string lengths are bounded by a constant, return $\\mathcal{D}( \\{ t s\\ |\\ t \\in T, s \\in S\\} )$. Both these bounding constants are global: they are the same for all inputs $T$.</li>\n<li><code>Get-Prefixes</code> on $\\mathcal{D}(S)$: return $\\{ a \\ | \\ as \\in S, a \\in \\Sigma \\}$. Note that I don\'t really mind what structure is used for this set, as long as I can enumerate its contents in $O(|\\Sigma|)$ time.</li>\n<li><code>Remove-Prefixes</code> on $\\mathcal{D}(S)$: return $\\mathcal{D}( \\{ s \\ | \\ as \\in S, a \\in \\Sigma  \\} )$.</li>\n<li><code>Merge</code>: given $\\mathcal{D}(S)$ and $\\mathcal{D}(T)$, return $\\mathcal{D}(S \\cup T)$.</li>\n</ul>\n\n<p>Now, I\'d really like to do all these operations in $O(1)$ time, but I\'m fine with a structure that does all these operations in $o(n)$ time, where $n$ is the length of the longest string in the structure. In the case of the merge, I\'d like a $o(n_1+n_2)$ running time, where $n_1$ is $n$ for the first and $n_2$ the $n$ for the second structure.</p>\n\n<p>An additional requirement is that the structure is immutable, or at least that the above operations return \'new\' structures such that pointers to the old ones still function as before.</p>\n\n<p>A note about amortization: that is fine, but you have to watch out for persistence. As I re-use old structures all the time, I\'ll be in trouble if I hit a worst case with some particular set of operations on the same structure (so ignoring the new structures it creates).</p>\n\n<p>I\'d like to use such a structure in a parsing algorithm I\'m working on; the above structure would hold the lookahead I need for the algorithm.</p>\n\n<p>I\'ve already considered using a <a href="http://en.wikipedia.org/wiki/Trie">trie</a>, but the main problem is that I don\'t know how to merge tries efficiently. If the set of strings for <code>Add-Prefix-Set</code> consists of only single-character strings, then you could store these sets in a stack, which would give you $O(1)$ running times for the first three operations. However, this approach doesn\'t work for merging either.</p>\n\n<p>Finally, note that I\'m not interested in factors $|\\Sigma|$: this is constant for all I care.</p>\n', 'ViewCount': '657', 'Title': "Is there a 'string stack' data structure that supports these string operations?", 'LastEditorUserId': '41', 'LastActivityDate': '2012-04-01T01:47:57.227', 'LastEditDate': '2012-04-01T01:47:57.227', 'AnswerCount': '0', 'CommentCount': '10', 'Score': '18', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<data-structures><time-complexity><strings><stack>', 'CreationDate': '2012-03-22T17:49:11.333', 'FavoriteCount': '3', 'Id': '666'},914:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '229', 'Title': 'Research on evaluating the performance of cache-obliviousness in practice', 'LastEditDate': '2012-03-26T20:58:01.420', 'AnswerCount': '1', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '2', 'Body': u'<p><a href="http://en.wikipedia.org/wiki/Cache-oblivious_algorithm" rel="nofollow">Cache-oblivious algorithms and data structures</a> are a rather new thing, introduced by Frigo et al. in <a href="http://userweb.cs.utexas.edu/~pingali/CS395T/2009fa/papers/coAlgorithms.pdf" rel="nofollow">Cache-oblivious algorithms, 1999</a>. Prokop\'s <a href="http://www.google.fi/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http%3A%2F%2Fsupertech.csail.mit.edu%2Fpapers%2FProkop99.pdf&amp;ei=Dc1tT-aLI8bm4QSC4YjAAg&amp;usg=AFQjCNHWhtzqOQqUonQWHduna8_nbQYx2g&amp;sig2=Nf_YDGY3NZLj7q0FY6TZgw" rel="nofollow">thesis</a> from the same year introduces the early ideas as well.</p>\n\n<p>The paper by Frigo et al. present some experimental results showing the potential of the theory and of the cache-oblivious algorithms and data structures. Many cache-oblivious data structures are based on static search trees. Methods of storing and navigating these trees have been developed quite a bit, perhaps most notably by Bender et al. and also by Brodal et al. Demaine gives a nice <a href="http://www.cs.uwaterloo.ca/~imunro/cs840/DemaineCache.pdf" rel="nofollow">overview</a>.</p>\n\n<p>The experimental work of investigating the cache behaviour in practice was done at least by Ladner et al. in <a href="http://www.cs.amherst.edu/~ccm/cs34/papers/ladnerbst.pdf" rel="nofollow">A Comparison of Cache Aware and Cache Oblivious Static Search Trees Using Program Instrumentation, 2002</a>. Ladner et al. benchmarked the cache behaviour of algorithms solving the binary search problem, using the classic algorithm, cache-oblivious algorithm and cache-aware algorithm. Each algorithm was benchmarked with both implicit and explicit navigation methods. In addition to this, the thesis by <a href="http://www.diku.dk/forskning/performance-engineering/frederik/thesis.pdf" rel="nofollow">R\xf8nn, 2003</a> analyzed the same algorithms to quite high detail and also performed even more thorough testing of the same algorithms as Ladner et al.</p>\n\n<p><strong>My question is</strong></p>\n\n<blockquote>\n  <p>Has there been any newer research on <em>benchmarking</em> the cache behaviour of cache-oblivious algorithms in <em>practice</em> since? I\'m especially interested in the performance of the static search trees, but I would also be happy with any other cache-oblivious algorithms and data structures.</p>\n</blockquote>\n', 'Tags': '<algorithms><data-structures><computer-architecture><reference-request><cpu-cache>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-03-26T20:58:01.420', 'CommentCount': '1', 'AcceptedAnswerId': '741', 'CreationDate': '2012-03-24T13:51:44.963', 'Id': '740'},915:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1315', 'Title': 'Proving a binary tree has at most $\\lceil n/2 \\rceil$ leaves', 'LastEditDate': '2012-03-30T02:27:43.967', 'AnswerCount': '2', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '756', 'FavoriteCount': '3', 'Body': '<p>I\'m trying to prove that a <a href="http://en.wikipedia.org/wiki/Binary_tree">binary tree</a> with $n$ nodes has at most $\\left\\lceil \\frac{n}{2} \\right\\rceil$ leaves. How would I go about doing this with induction?</p>\n\n<p><em>For people who were following in the original question about heaps, it has been moved <a href="http://cs.stackexchange.com/questions/841/proving-a-binary-heap-has-lceil-n-2-rceil-leaves">here</a>.</em></p>\n', 'Tags': '<data-structures><binary-trees><combinatorics><graph-theory><proof-techniques>', 'LastEditorUserId': '41', 'LastActivityDate': '2013-10-01T04:35:17.907', 'CommentCount': '7', 'AcceptedAnswerId': '810', 'CreationDate': '2012-03-26T21:51:56.127', 'Id': '805'},916:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to prove that a <a href="http://en.wikipedia.org/wiki/Binary_heap">binary heap</a> with $n$ nodes has exactly $\\left\\lceil \\frac{n}{2} \\right\\rceil$ leaves, given that the heap is built in the following way:</p>\n\n<p>Each new node is inserted via <a href="http://en.wikipedia.org/wiki/Binary_heap#Insert">percolate up</a>. This means that each new node must be created at the next available child. What I mean by this is that children are filled level-down, and left to right. For example, the following heap:</p>\n\n<pre><code>    0\n   / \\\n  1   2\n</code></pre>\n\n<p>would <b>have</b> to have been built in this order: 0, 1, 2. (The numbers are just indexes, they give no indication of the actual data held in that node.) </p>\n\n<p>This has two important implications:</p>\n\n<ol>\n<li><p>There can exist no node on level $k+1$ without level $k$ being completely filled</p></li>\n<li><p>Because children are built left to right, there can be no "empty spaces" between the nodes on level $k+1$, or situations like the following:  </p>\n\n<pre><code>    0\n   / \\\n  1   2\n / \\   \\\n3  4    6\n</code></pre></li>\n</ol>\n\n<p>(This would be an illegal heap by my definition.) Thus, a good way to think of this heap is an <a href="http://en.wikipedia.org/wiki/Binary_heap#Heap_implementation">array implementation</a> of a heap, where there can\'t be any "jumps" in indeces of the array.</p>\n\n<p>So, I was thinking induction would probably be a good way to do this... Perhaps something having to deal with even an odd cases for n. For example, some induction using the fact that even heaps built in this fashion must have an internal node with one child for an even n, and no such nodes for an odd n. Ideas?</p>\n', 'ViewCount': '853', 'Title': 'Proving a binary heap has $\\lceil n/2 \\rceil$ leaves', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-09T16:12:24.240', 'LastEditDate': '2012-03-28T06:42:55.933', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '843', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '756', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2012-03-28T00:56:55.877', 'Id': '841'},917:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A <a href="http://en.wikipedia.org/wiki/Fibonacci_heap#Summary_of_running_times">Fibonnaci Heap</a> supports the following operations:</p>\n\n<ul>\n<li><code>insert(key, data)</code> : adds a new element to the data structure</li>\n<li><code>find-min()</code> : returns a pointer to the element with minimum key</li>\n<li><code>delete-min()</code> : removes the element with minimum key</li>\n<li><code>delete(node)</code> : deletes the element pointed to by <code>node</code></li>\n<li><code>decrease-key(node)</code> : decreases the key of the element pointed to by <code>node</code></li>\n</ul>\n\n<p>All non-delete operations are $O(1)$ (amortized) time, and the delete operations are $O(\\log n)$ amortized time.</p>\n\n<p>Are there any implementations of a priority queue which  also support<code>increase-key(node)</code> in $O(1)$ (amortized) time?</p>\n', 'ViewCount': '926', 'Title': 'Priority queue with both decrease-key and increase-key operations', 'LastActivityDate': '2012-03-30T02:38:44.623', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '887', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '71', 'Tags': '<data-structures><priority-queues>', 'CreationDate': '2012-03-29T21:06:21.303', 'FavoriteCount': '3', 'Id': '880'},918:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have noticed that different data structures are used when we implement search algorithms. For example, we use queues to implement breadth first search, stacks to implement depth-first search and min-heaps to implement the <a href="https://en.wikipedia.org/wiki/A%2a_algorithm">A* algorithm</a>. In these cases, we do not need to construct the search tree explicitly.</p>\n\n<p>But I can not find a simple data structure to simulate the searching process of the <a href="http://www.cs.cf.ac.uk/Dave/AI2/node26.html">AO* algorithm</a>. I would like to know if constructing the search tree explicitly is the only way to implement AO* algorithm? Can anybody provide me an efficient implementation?</p>\n', 'ViewCount': '994', 'Title': 'How to implement AO* algorithm?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-01T20:12:52.987', 'LastEditDate': '2012-09-22T09:04:58.943', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '931', 'Tags': '<algorithms><graphs><data-structures><search-algorithms>', 'CreationDate': '2012-04-04T03:05:59.537', 'FavoriteCount': '2', 'Id': '1020'},919:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '245', 'Title': 'Supporting data structures for SAT local search', 'LastEditDate': '2012-04-05T17:15:02.153', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '472', 'FavoriteCount': '1', 'Body': '<p><a href="http://en.wikipedia.org/wiki/WalkSAT">WalkSAT and GSAT</a> are well-known and simple local search algorithms for solving the Boolean satisfiability problem. The pseudocode for the GSAT algorithm is copied from the question <a href="http://cs.stackexchange.com/questions/219/implementing-the-gsat-algorithm-how-to-select-which-literal-to-flip">Implementing the GSAT algorithm - How to select which literal to flip?</a> and presented below.</p>\n\n<pre><code>procedure GSAT(A,Max_Tries,Max_Flips)\n  A: is a CNF formula\n  for i:=1 to Max_Tries do\n    S &lt;- instantiation of variables\n    for j:=1 to Max_Iter do\n      if A satisfiable by S then\n        return S\n      endif\n      V &lt;- the variable whose flip yield the most important raise in the number of satisfied clauses;\n      S &lt;- S with V flipped;\n    endfor\n  endfor\n  return the best instantiation found\nend GSAT\n</code></pre>\n\n<p>Here we flip the variable that maximizes the number of satisfied clauses. How is this done efficiently? The naive method is to flip every variable, and for each step through all clauses and calculate how many of them get satisfied. Even if a clause could be queried for satisfiability in constant time, the naive method would still run in $O(VC)$ time, where $V$ is the number of variables and $C$ the number of clauses. I\'m sure we can do better, hence the question:</p>\n\n<blockquote>\n  <p>Many local search algorithms flip the variable\'s assignment that maximizes the number of satisfied clauses. In practice, with what data structures is this operation supported efficiently?</p>\n</blockquote>\n\n<p>This is something I feel like textbooks often omit. One example is even the famous <a href="http://aima.cs.berkeley.edu/">Russell &amp; Norvig book</a>.</p>\n', 'Tags': '<algorithms><data-structures><satisfiability>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-15T16:21:12.517', 'CommentCount': '4', 'AcceptedAnswerId': '1262', 'CreationDate': '2012-04-05T16:08:17.820', 'Id': '1058'},920:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider a <a href="http://en.wikipedia.org/wiki/Minimax" rel="nofollow">minimax</a> tree for an adversarial search problem. For example, in this picture (alpha-beta pruning):</p>\n\n<p><img src="http://i.stack.imgur.com/NmKIO.jpg" alt="enter image description here"></p>\n\n<p>When marking the the tree with $[\\min,\\max]$ values bottom-up, we first traverse node $3$ and assign $B.\\max = 3$. Then we traverse $12$ and $8$ in this order, it will make sure $B.max = 3$.</p>\n\n<p>But why is $B.\\min = 3$? What is the use of that value?</p>\n', 'ViewCount': '311', 'Title': 'What use are the minimum values on minimax trees?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-06T17:30:48.237', 'LastEditDate': '2012-04-06T17:14:35.593', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '240', 'Tags': '<data-structures><trees><game-theory>', 'CreationDate': '2012-04-06T04:11:51.570', 'FavoriteCount': '0', 'Id': '1069'},921:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '855', 'Title': 'Why does the splay tree rotation algorithm take into account both the parent and grandparent node?', 'LastEditDate': '2012-08-10T11:00:07.597', 'AnswerCount': '1', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '1056', 'FavoriteCount': '2', 'Body': '<p>I don\'t quite understand why the rotation in the splay tree data structure is taking into account not only the parent of the rating node, but also the grandparent (zig-zag and zig-zig operation). Why would the following not work:</p>\n\n<p>As we insert, for instance, a new node to the tree, we check whether we insert into the left or right subtree. If we insert into the left, we rotate the result RIGHT, and vice versa for right subtree. Recursively it would be sth like this</p>\n\n<pre><code>Tree insert(Tree root, Key k){\n    if(k &lt; root.key){\n        root.setLeft(insert(root.getLeft(), key);\n        return rotateRight(root);\n    }\n    //vice versa for right subtree\n}\n</code></pre>\n\n<p>That should avoid the whole "splay" procedure, don\'t you think?</p>\n', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'LastEditorUserId': '187', 'LastActivityDate': '2012-08-10T11:00:07.597', 'CommentCount': '0', 'AcceptedAnswerId': '1230', 'CreationDate': '2012-04-11T21:37:01.123', 'Id': '1229'},922:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm doing some exam (Java-based algorithmics) revision and have been given the question:</p>\n\n<blockquote>\n  <p>Describe how you might extend your implementation [of a queue using a circular array] to support the expansion of the Queue to allow it to store more data items.</p>\n</blockquote>\n\n<p>The Queue started off implemented as an array with a fixed maximum size. I've got two current answers to this, but I'm not sure either are correct:</p>\n\n<ol>\n<li><p>Implement the Queue using the Java Vector class as the underlying array structure. The Vector class is similar to arrays, but a Vector can be resized at any time whereas an array's size is fixed when the array is created.</p></li>\n<li><p>Copy all entries into a larger array.</p></li>\n</ol>\n\n<p>Is there anything obvious I'm missing?</p>\n", 'ViewCount': '898', 'Title': 'Extending the implementation of a Queue using a circular array', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-21T02:26:16.323', 'LastEditDate': '2012-04-18T22:53:03.927', 'AnswerCount': '4', 'CommentCount': '7', 'AcceptedAnswerId': '1361', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1137', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-04-18T10:07:18.590', 'Id': '1336'},923:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>There are some documents to be indexed, that means I need to read the docs and extract the words and index them by storing at which document they appear and at which position.</p>\n\n<p>For each word initially I am creating a separate file. Consider 2 documents:</p>\n\n<ul>\n<li>document 1: \u201cThe Problem of Programming Communication with\u201d</li>\n<li>document 2: \u201cProgramming of Arithmetic Operations\u201d</li>\n</ul>\n\n<p>Here, there are 10 words, 8 unique. So I create 8 files (<code>the</code>, <code>problem</code>, <code>of</code>, <code>programming</code>, <code>communications</code>, <code>with</code>, <code>arithmetic</code>, <code>operations</code>).</p>\n\n<p>In each file, I will store at which document they appear and at what position. The actual structure I am implementing has lot more information but this basic structure will serve the purpose.</p>\n\n<pre><code>file name        file content\nthe              1 1\nproblem          1 2\nof               1 3 2 2\nprogramming      1 4 2 1\ncommunications   1 5\nwith             1 6\narithmetic       2 3\noperations       2 4\n</code></pre>\n\n<p>Meaning. the word is located at document 1, position 3 and at document 2, position 2.</p>\n\n<p>After the initial index is done I will concatenate all the files into a single index file and in another file I store the offset where a particular word will be found.</p>\n\n<p>index file: <code>1 1 1 2 1 3 2 2 1 4 2 1 1 5 1 6 2 3 2 4</code><br>\noffset file: <code>the 1 problem 3 of 5 programming 9 communications 13  with 15 arithmetic 17 operations 19</code></p>\n\n<p>So if I need the index information for <code>communications</code>, I will go to position 13 of the file and read up to position 15 excluded, in other words the offset of the next word.</p>\n\n<p>This is all fine for static indexing. But if I change a single index the whole file will need to be rewritten. Can I use a binary tree as the index file's structure, so that I can dynamically change the file content and update the offset somehow ? </p>\n", 'ViewCount': '188', 'Title': 'Maintaining search indices with binary trees', 'LastEditorUserId': '98', 'LastActivityDate': '2012-04-22T11:42:40.107', 'LastEditDate': '2012-04-22T11:42:40.107', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '173', 'Tags': '<data-structures><binary-trees><data-mining>', 'CreationDate': '2012-04-20T19:32:58.920', 'Id': '1398'},924:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'m implementing a class which provides arbitrary-precision arithmetic (also called "bignum", "BigInteger", etc.).</p>\n\n<p>My questions is about a practical implementation detail:</p>\n\n<p>I\'m wondering if there is a significant difference in implementation and computational complexity between an implementation which stores the magnitude in an integer array in BigEndian order vs. LittleEndian order.</p>\n\n<p>My data structure is basically:</p>\n\n<pre><code>class BigInt\n  val signum: Int\n  val magnitude: Array[Int] // two-complement (unsigned)\n</code></pre>\n\n<p>Supported operations are for instance:</p>\n\n<p>+, -, * (Long multiplication, Karatsuba, Cook3, Sch\xf6nhage-Strassen), /, squaring\nConversion to other number types\nComparison, equality, representation as a String</p>\n\n<p><em>The implementation is immutable, so every operation will return a new value and will not change the any existing.</em></p>\n\n<p>Feel free to ask for clarifications!</p>\n', 'ViewCount': '101', 'Title': 'Is it better to store the magnitude of an arbitrary-precision number in BigEndian or LittleEndian order in an integer array?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-04-26T05:17:03.090', 'LastEditDate': '2012-04-26T00:25:14.967', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1235', 'Tags': '<data-structures><arrays>', 'CreationDate': '2012-04-25T11:37:32.913', 'Id': '1498'},925:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Assume I want to insert elements $1$ to $n$ into a data structure exactly once, and perform predecessor queries while inserting these elements (so <code>insert(x)</code> and <code>pred(x)</code> always come in pairs). The predecessor of $x$ is the largest number in the data structure that is smaller than $x$.</p>\n\n<p>The data structure is created by preprocessing the list of insertions. </p>\n\n<p>When I start to insert elements, an adversary decides to delete some of the elements I have inserted, by adding any number of deletion operations between my insertions. </p>\n\n<p>A query input to the data structure is a sequence of insertions and deletions, which is the insertion sequence with deletions inserted. \nThe output of the query is the result of the $n$ predecessor queries executed when the elements are inserted. </p>\n\n<p>Can one design a data structure so the query takes $O(n)$?</p>\n\n<p>Here is an example.</p>\n\n<pre><code>Insertions = [1,3,5,4,2]\nDS = makeDataStructure(Insertions)//Runs in polynomial time\n//add some deletions into insertions\nOperations = [1,3,-3,5,-1,4,-5,-4,2,-2]\nDS.query(Operations)//this runs in O(n) time\n</code></pre>\n\n<p>Assume -i = delete i. And pred(x) = 0 if there is nothing before it.\nresult would be:</p>\n\n<pre><code>[pred(1)=0, pred(3)=1, pred(5)=1, pred(4)=0, pred(2)=0]\n</code></pre>\n\n<p>for example, the 3rd in the result is <code>pred(5)=1</code> instead of 3 because 3 is deleted when 5 is inserted.</p>\n', 'ViewCount': '162', 'Title': 'Predecessor query where the insertion order is known', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-05T15:55:44.163', 'LastEditDate': '2013-05-05T15:55:44.163', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<data-structures><runtime-analysis>', 'CreationDate': '2012-04-25T13:44:46.220', 'FavoriteCount': '1', 'Id': '1502'},926:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I asked this <a href="http://stackoverflow.com/questions/10326446/how-to-approach-dynamic-graph-related-problems">question</a> at generic stackoverflow and I was directed here.</p>\n\n<p>It will be great if some one can explain how to approach partial or fully dynamic graph problems in general.</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Find Shortest Path between two vertices $(u,v)$ in a undirected weighted graph for $n$ instances, when an edge is removed at each instance.</li>\n<li>Find number of connected components in an undirected graph for n instances when an edge is remove at each instance, etc.</li>\n</ul>\n\n<p>I recently encountered this genre of problems in a programming contest. I searched through the web and I found lot of research papers concerning with dynamic graphs [1,2]. I read couple of them and and I couldnt find anything straight forward (clustering, sparsification etc). Sorry for being vague.</p>\n\n<p>I really appreciate if some can provide pointers to understand these concepts better.</p>\n\n<hr>\n\n<ol>\n<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8372"><em>Dynamic Graph Algorithms</em></a> by D. Eppstein , Z. Galil , G. F. Italiano (1999)</li>\n<li><a href="http://www.lix.polytechnique.fr/~liberti/sppsurvey.pdf"><em>Shortest paths on dynamic graphs</em></a> by G. Nannicini, L. Liberti (2008)</li>\n</ol>\n', 'ViewCount': '453', 'Title': 'How to approach Dynamic graph related problems', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-23T15:24:36.313', 'LastEditDate': '2012-04-27T07:08:12.330', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '1262', 'Tags': '<algorithms><data-structures><graphs>', 'CreationDate': '2012-04-27T01:05:51.733', 'FavoriteCount': '1', 'Id': '1521'},927:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'m working on problem H in the <a href="http://neerc.ifmo.ru/past/2004/problems/problems.pdf" rel="nofollow">ACM ICPC 2004\u20132005 Northeastern European contest</a>.</p>\n\n<p>The problem is basically to find the worst case that produces a maximal number of exchanges in the algorithm (sift down) to build the heap.</p>\n\n<ul>\n<li>Input: Input \ufb01le contains $n$ ($1 \\le n \\le 50{,}000$).</li>\n<li>Output:  Output the array containing $n$ different integer numbers from $1$ to $n$, such that it is a heap, and when converting it to a sorted array, the total number of exchanges in sifting operations is maximal possible.</li>\n</ul>\n\n<p>Sample input: <code>6</code><br>\nCorresponding output: <code>6 5 3 2 4 1</code></p>\n\n<p>And the basics outputs:</p>\n\n<pre><code>[2, 1]   \n[3, 2, 1]   \n[4, 3, 1, 2] \n[5, 4, 3, 2, 1] \n[6, 5, 3, 4, 1, 2]\n</code></pre>\n', 'ViewCount': '2275', 'Title': 'Finding a worst case of heap sort', 'LastEditorUserId': '1152', 'LastActivityDate': '2013-10-28T16:39:52.673', 'LastEditDate': '2012-04-28T13:55:03.753', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1152', 'Tags': '<algorithms><data-structures><algorithm-analysis><sorting>', 'CreationDate': '2012-04-27T21:28:44.810', 'FavoriteCount': '1', 'Id': '1540'},928:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'m trying to find the best algorithm for converting an \u201cordinary\u201d linked list into an \u201cideal" skip list. </p>\n\n<p>The definition of an \u201cideal skip list\u201d is that in the first level we\'ll have all the elements, half of them in the next level, a quarter of them in the level after that, and so on.</p>\n\n<p>I\'m thinking about a $\\mathcal{O}(n)$ run-time algorithm involving throwing a coin for each node in the original linked-list, to determine for any given node whether it should be placed in a higher or lower level, and create a duplicate node for the current node at a higher level. This algorithm should work in $\\mathcal{O}(n)$; is there any better algorithm? </p>\n', 'ViewCount': '302', 'Title': 'Building ideal skip lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-16T14:45:04.503', 'LastEditDate': '2012-05-09T10:47:32.457', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '776', 'Tags': '<algorithms><data-structures><randomized-algorithms><lists>', 'CreationDate': '2012-04-30T13:03:01.867', 'FavoriteCount': '1', 'Id': '1589'},929:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to write a spell-checker which should work with a pretty large dictionary. I really want an efficient way to index my dictionary data to be used using a <a href="http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance">Damerau-Levenshtein</a> distance to determine which words are closest to the misspelled word.</p>\n\n<p>I\'m looking for a data structure who would give me the best compromise between space complexity and runtime complexity.</p>\n\n<p>Based on what I found on the internet, I have a few leads regarding what type of data structure to use:</p>\n\n<h2>Trie</h2>\n\n<p><img src="http://i.stack.imgur.com/KhvoF.png" alt="trie-500px"></p>\n\n<p>This is my first thought and looks pretty easy to implement and should provide fast lookup/insertion. Approximate search using Damerau-Levenshtein should be simple to implement here as well. But it doesn\'t look very efficient in terms of space complexity since you most likely have a lot of overhead with pointers storage.</p>\n\n<h2>Patricia Trie</h2>\n\n<p><img src="http://i.stack.imgur.com/EJYB0.png" alt="trie-500px"></p>\n\n<p>This seems to consume less space than a regular Trie since you\'re basically avoiding the cost of storing the pointers, but I\'m a bit worried about data fragmentation in case of very large dictionaries like what I have.</p>\n\n<h2>Suffix Tree</h2>\n\n<p><img src="http://i.stack.imgur.com/uXH1b.png" alt="suffix-500px"></p>\n\n<p>I\'m not sure about this one, it seems like some people do find it useful in text mining, but I\'m not really sure what it would give in terms of performance for a spell checker.</p>\n\n<h2>Ternary Search Tree</h2>\n\n<p><img src="http://i.stack.imgur.com/X8hPY.png" alt="tst"></p>\n\n<p>These look pretty nice and in terms of complexity should be close (better?) to Patricia Tries, but I\'m not sure regarding fragmentation if it would be better of worse than Patricia Tries.</p>\n\n<h2>Burst Tree</h2>\n\n<p><img src="http://i.stack.imgur.com/9jn1m.png" alt="burst"></p>\n\n<p>This seems kind of hybrid and I\'m not sure what advantage it would have over Tries and the like, but I\'ve read several times that it\'s very efficient for text mining.</p>\n\n<hr>\n\n<p>I would like to get some feedback as to which data structure would be best to use in this context and what makes it better than the other ones. If I\'m missing some data structures who would be even more appropriate for a spell-checker, I\'m very interested as well.</p>\n', 'ViewCount': '1908', 'Title': 'Efficient data structures for building a fast spell checker', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-04T14:00:33.120', 'LastEditDate': '2012-05-15T20:22:56.733', 'AnswerCount': '2', 'CommentCount': '8', 'Score': '23', 'PostTypeId': '1', 'OwnerUserId': '1307', 'Tags': '<data-structures><strings><string-metrics>', 'CreationDate': '2012-05-02T03:07:23.057', 'FavoriteCount': '5', 'Id': '1626'},930:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>What is the expected space used by the skip list after inserting $n$ elements?</p>\n\n<p>I expect that in the worst case the space consumption may grow inde\ufb01nitely.</p>\n\n<p><a href="http://en.wikipedia.org/wiki/Skip_list" rel="nofollow">Wikipedia</a> says space $O(n)$.</p>\n\n<p>How can this be proven one way or another?</p>\n', 'ViewCount': '146', 'Title': 'Expected space consumption of skip lists', 'LastEditorUserId': '472', 'LastActivityDate': '2012-05-07T20:55:22.670', 'LastEditDate': '2012-05-07T08:47:42.743', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1402', 'Tags': '<data-structures><space-complexity>', 'CreationDate': '2012-05-07T08:45:05.963', 'FavoriteCount': '1', 'Id': '1713'},931:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am struggling with hashing and binary search tree material.\nAnd I read that instead of using lists for storing entries with the same hash values, it is also possible to use binary search trees. And I try to understand what the worst-case and average-case running time for the operations</p>\n\n<ol>\n<li><code>insert</code>, </li>\n<li><code>find</code> and</li>\n<li><code>delete</code></li>\n</ol>\n\n<p>is in worth- resp. average case. Do they improve with respect to lists?</p>\n', 'ViewCount': '1265', 'Title': 'Hashing using search trees instead of lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T15:37:46.957', 'LastEditDate': '2012-05-10T15:37:46.957', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<data-structures><time-complexity><runtime-analysis><search-trees><hash-tables>', 'CreationDate': '2012-05-08T21:46:33.920', 'Id': '1739'},932:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1671', 'Title': 'Data structure with search, insert and delete in amortised time $O(1)$?', 'LastEditDate': '2012-05-22T07:32:22.800', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '1120', 'FavoriteCount': '7', 'Body': '<p>Is there a data structure to maintain an ordered list that supports the following operations in $O(1)$ amortized time? </p>\n\n<ul>\n<li><p><strong>GetElement(k)</strong>: Return the $k$th element of the list.</p></li>\n<li><p><strong>InsertAfter(x,y)</strong>: Insert the new element y into the list immediately after x.  </p></li>\n<li><p><strong>Delete(x)</strong>: Remove x from the list.</p></li>\n</ul>\n\n<p>For the last two operations, you can assume that x is given as a pointer directly into the data structure; InsertElement returns the corresponding pointer for y.  InsertAfter(NULL, y) inserts y at the beginning of the list.</p>\n\n<p>For example, starting with an empty data structure, the following operations update the ordered list as shown below:</p>\n\n<ul>\n<li>InsertAfter(NULL, a) $\\implies$ [a]</li>\n<li>InsertAfter(NULL, b) $\\implies$ [b, a]</li>\n<li>InsertAfter(b, c) $\\implies$ [b, c, a]</li>\n<li>InsertAfter(a, d) $\\implies$ [b, c, a, d]</li>\n<li>Delete(c) $\\implies$ [b, a, d]</li>\n</ul>\n\n<p>After these five updates, GetElement(2) should return d, and GetElement(3) should return an error.</p>\n', 'Tags': '<data-structures><time-complexity><asymptotics><amortized-analysis>', 'LastEditorUserId': '72', 'LastActivityDate': '2013-08-06T14:07:23.407', 'CommentCount': '8', 'AcceptedAnswerId': '7807', 'CreationDate': '2012-05-21T07:35:55.077', 'Id': '1970'},933:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m looking for a data structure that supports efficient approximate lookups of keys (e.g., Levenshtein distance for strings), returning the closest possible match for the input key. The best suited data structure I\'ve found so far are <a href="http://en.wikipedia.org/wiki/BK-tree">Burkhard-Keller trees</a>, but I was wondering if there are other/better data structures for this purpose.</p>\n\n<p>Edit:\nSome more details of my specific case:</p>\n\n<ul>\n<li>Strings usually have a fairly large Levenshtein difference from each other.</li>\n<li>Strings have a max length of around 20-30 chars, with an average closer to 10-12.</li>\n<li>I\'m more interested in efficient lookup than insertion as I will be building a set of mostly static data that I want to query efficiently.</li>\n</ul>\n', 'ViewCount': '787', 'Title': 'Efficient map data structure supporting approximate lookup', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-27T17:48:11.330', 'LastEditDate': '2012-05-26T14:41:45.563', 'AnswerCount': '2', 'CommentCount': '4', 'Score': '14', 'PostTypeId': '1', 'OwnerUserId': '1658', 'Tags': '<data-structures><strings><efficiency>', 'CreationDate': '2012-05-26T13:11:38.040', 'FavoriteCount': '4', 'Id': '2093'},934:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '361', 'Title': 'Constructing a data structure for a computer algebra system', 'LastEditDate': '2012-05-30T21:51:33.500', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '863', 'FavoriteCount': '0', 'Body': '<p>In thinking about how to approach this problem I think several things will be required, some tivial:</p>\n\n<ol>\n<li>An expression tree where non-leaf node is an operation (not sure if that part is redundant), but not every node has just two children.</li>\n<li>All nodes for operations have a defined number of children that they must have (some operators are unary (like $!$) while others are binary ($*,+,-,$ etc) and still other are n-ary ($f(a,b,d)$ and versions with different amounts of variables).</li>\n<li>All leaf nodes are some type of number</li>\n</ol>\n\n<p>I am under the impression that the tree should not explicitly retain information regarding the order of operations, but rather that information should be used in the parsing stage to insert things into the tree correctly.</p>\n\n<p>This leads to the question, how should inserting to a specific position in the tree be done? Simply passing a list of directions (from root, take node zero, then node 1, etc, then insert) will work, but it seems overly clunky.</p>\n\n<p>Or should I avoid that situation entirely (not talking about editing an equation here, just building a representation of one) by using the fact that in some sense the tree must be complete (all binary operations MUST have two children, etc, and even operators that are seemingly ambiguous (the $_{^-}$ sign for example) but these ambiguities are resolved before this point. That would all me to insert "in order"</p>\n\n<p>Am I taking a reasonable approach? Does it make no sense whatsoever?</p>\n\n<p>Additionally, are there papers or articles that I should read about CAS systems?</p>\n\n<p><strong>Clarification:</strong> The tree will need to support three different compound operations.</p>\n\n<ol>\n<li>Creation: (from a string, but how to actually do that is beyond the scope of this question)</li>\n<li>Reduction: (to some type of canonical form) so that if $a+b$ and $b+a$ are both entered and reduced, they will form identical trees.</li>\n<li>Evaluation: Be able to traverse the tree</li>\n</ol>\n\n<p>These are all the operations that need to be supported. There are probably many other more basic operations that may need to be supported, but in this case it only matters that the three operations above are supported. My understanding is that search for example is not a property that will be required, but deletion will be (of a whole subtree).</p>\n', 'Tags': '<data-structures><computer-algebra><mathematical-software>', 'LastEditorUserId': '41', 'LastActivityDate': '2012-06-01T11:59:11.663', 'CommentCount': '4', 'AcceptedAnswerId': '2181', 'CreationDate': '2012-05-30T21:00:49.130', 'Id': '2175'},935:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '535', 'Title': 'How can I prove that a complete binary tree has $\\lceil n/2 \\rceil$ leaves?', 'LastEditDate': '2012-06-02T00:54:30.613', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'Luc Peetersen', 'PostTypeId': '1', 'OwnerUserId': '1722', 'FavoriteCount': '2', 'Body': "<p>Given a complete binary tree with $n$ nodes. I'm trying to prove that a complete binary tree has exactly $\\lceil n/2 \\rceil$ leaves.\nI think I can do this by induction.</p>\n\n<p>For $h(t)=0$, the tree is empty. So there are no leaves and the claim holds for an empty tree.</p>\n\n<p>For $h(t)=1$, the tree has 1 node, that also is a leaf, so the claim holds.\nHere I'm stuck, I don't know what to choose as induction hypothesis and how to do the induction step.</p>\n", 'Tags': '<data-structures><graph-theory><proof-techniques><combinatorics><binary-trees>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-06-02T05:41:15.503', 'CommentCount': '6', 'AcceptedAnswerId': '2196', 'CreationDate': '2012-06-01T21:08:05.543', 'Id': '2193'},936:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '953', 'Title': 'Representing Negative and Complex Numbers Using Lambda Calculus', 'LastEditDate': '2013-09-19T18:19:18.270', 'AnswerCount': '2', 'Score': '7', 'OwnerDisplayName': 'zcaudate', 'PostTypeId': '1', 'OwnerUserId': '1800', 'FavoriteCount': '1', 'Body': '<p>Most tutorials on Lambda Calculus provide example where Positive Integers and Booleans can be represented by Functions. What about -1 and i?</p>\n', 'Tags': '<data-structures><lambda-calculus><integers><real-numbers>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-09-19T18:19:18.270', 'CommentCount': '0', 'AcceptedAnswerId': '2279', 'CreationDate': '2012-06-08T03:25:50.233', 'Id': '2272'},937:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p>For a given planar graph $G(V,E)$ embedded in the plane, defined by list of segments $E= \\left \\{ e_1,...,e_m \\right \\} $, each segment $e_i$ is represented by its endpoints $\\left \\{ L_i,R_i \\right \\}$. Construct a DCEL data structure for the planar subdivision, describe an algorithm, prove it\'s  correctness and show the complexity.</p>\n</blockquote>\n\n<p>More information about DCEL (double connected edge list) you can find on <a href="http://en.wikipedia.org/wiki/DCEL" rel="nofollow">wikipedia - DCEL</a>.</p>\n\n<p>According to description of DCEL and connections between different objects of DCEL (vertices, edges and faces) the required data structure must be complicated.</p>\n\n<p>I found that <em>doubly-linked lists</em> can be used as data structure for DCEL, I am not sure how to build and maintain connections between vertices - edges and edges - faces.</p>\n\n<p>I tried to find any hint in textbook, but the construction of DCEL wasn\'t described, map overlay is more popular topic.</p>\n\n<p>Regarding the algorithm, plane sweep algorithm with $O((n+l)\\log n)$ should do the job, but it seems to be overkill, because segments are intersected not in arbitrary points, but only in endpoints, therefore $O(n\\log n)$ seems more reasonable.</p>\n\n<p>The main problem is the data structure, so far I haven\'t seen any good example with at least similar complexity.</p>\n\n<p>Please, if you have any idea about a data structure for DCEL or about algorithm for constructing DCEL, share it with us.</p>\n', 'ViewCount': '1495', 'Title': 'Constructing of Double Connected Edge List (DCEL)', 'LastEditorUserId': '1379', 'LastActivityDate': '2013-09-22T03:25:45.343', 'LastEditDate': '2012-06-22T15:37:35.383', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '2516', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1379', 'Tags': '<algorithms><data-structures><computational-geometry><lists>', 'CreationDate': '2012-06-22T12:58:26.957', 'Id': '2450'},938:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I've been given the following problem:</p>\n\n<p>Given a data structure $M$ that is based on comparisons and supports the following methods on a group of numbers $S$:</p>\n\n<ul>\n<li>$\\text{Insert}(x)$ \u2013 add $x$ to $S$</li>\n<li>$\\text{Extract_min}()$ \u2013 remove the minimal element in $S$ and return it </li>\n</ul>\n\n<p>We can implement with a heap the above methods in $O(\\log n)$, however, we're looking at \na bigger picture, a general case that we have no guarantee that $M$ is indeed a heap. Prove that \nno matter what kind of data structure $M$ is, that <strong>at least one</strong> of the methods that $M$ supports must take $\\Omega(\\log n )$.</p>\n\n<p><strong>My solution:</strong></p>\n\n<p>Each sorting algorithm that is based on comparisons must take at the worst case at least $\\Omega(n\\log n)$ \u2013 we'll prove that using a decision tree: if we look at any given algorithm that is based on comparisons, as a binary tree where each vertex is a <em>compare-method</em> between 2 elements: </p>\n\n<ul>\n<li>if the first is bigger than the second element \u2013 we'll go to the left child</li>\n<li>if the second is bigger than the first element \u2013 we'll go to the right child</li>\n</ul>\n\n<p>At the end, we'll have $n!$ leaves that are the options for sorting the elements.</p>\n\n<p>The height of the tree is $h$, then:</p>\n\n<p>$$2^h \\ge n! \\quad\\Longrightarrow\\quad \\log(2^h) &gt;= \\log(n!) \\quad\\Longrightarrow\\quad h \\ge \\log(n!) \\quad\\Longrightarrow\\quad h = \\Omega(n \\log n)$$</p>\n\n<p>Then, if we have a $\\Omega(n \\log n)$ worst case for $n$ elements, then we have a $\\Omega(\\log n)$ for a single element. </p>\n\n<p>I'm not sure regarding this solution, so I'd appreciate for corrections or anything else \nyou can come up with. </p>\n", 'ViewCount': '311', 'Title': 'Prove that for a general data structure - operations Extract_min() and Insert(x) cost $\\Omega(\\log n)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-06-26T12:42:03.597', 'LastEditDate': '2012-06-26T12:42:03.597', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '776', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'CreationDate': '2012-06-23T13:38:12.030', 'FavoriteCount': '0', 'Id': '2459'},939:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Basically, the problem I am solving is this. Initially, the array $A$ is empty. Then I am given data to fill the array and at any time I have to make a query to print the $|A|/3$-th largest element inserted so far.</p>\n\n<p>I was solving the problem with segment trees, but I am not able to make a little modification to the query function of the segment tree. The query function that I wrote returns the largest element between indices $a_{\\text{begin}}$ and $a_{\\text{end}}$:</p>\n\n<pre><code>int query(int Nodenumber,int t_begin,int t_end,int a_begin,int a_end)\n{\n    if (t_begin&gt;=a_begin &amp;&amp; t_end&lt;=a_end)\n        return Tree[Nodenumber];\n    else\n    {\n        int mid=((t_begin+t_end)/2);\n        int res = -1;\n\n        if (mid&gt;=a_begin &amp;&amp; t_begin&lt;=a_end)\n            res = max(res,query(2*Nodenumber,t_begin,mid,a_begin,a_end));\n\n        if (t_end&gt;=a_begin &amp;&amp; mid+1&lt;=a_end)\n            res = max(res,query(2*Nodenumber+1,mid+1,t_end,a_begin,a_end));\n\n        return res;\n    }\n} \n</code></pre>\n\n<p>Note to make a query, I call the query function as <code>query(1,0,N-1,QA,QB)</code>.</p>\n\n<p>But I want to return the $|A|/3$-th largest element between indices $a_{\\text{begin}}$ and $a_{\\text{end}}$. So how should I modify the function to do this?</p>\n\n<p>So updating, queries, updating, queries, updating, queries and so on are done randomly and several (upto $10^5$) times.</p>\n\n<p>So, for solving the problem, did I pick the right data structure? I thought of using heaps, but that will be too slow, as I would have to pop $|A|/3$ elements from the top and reinsert them for every query.</p>\n', 'ViewCount': '329', 'Title': 'Finding the $k$th largest element in an evolving query data structure', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-10T17:59:07.600', 'LastEditDate': '2012-07-02T10:04:27.083', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '2579', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-07-02T03:52:34.127', 'Id': '2575'},940:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Say instead of using a linked list as buckets for a hash table of size $m$, we use another hash table of size $p$ as buckets this time. What would be the average case for this problem?</p>\n\n<p>I looked up perfect hashing and I got a very close algorithm, and it is $O(1)$. Can someone please clarify?</p>\n', 'ViewCount': '239', 'Title': 'Using hash tables instead of lists for buckets in hash tables', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-05T07:18:20.110', 'LastEditDate': '2012-07-05T07:18:20.110', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2067', 'Tags': '<data-structures><time-complexity><hash-tables>', 'CreationDate': '2012-07-04T14:54:28.810', 'Id': '2613'},941:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I have a high interest in priority-queues (E.g., see my answers on: <a href="http://cs.stackexchange.com/q/524">Does there exist a priority queue with $O(1)$ extracts?</a>), and was wondering if there is a priority-queue or similar data-structure where you can sort by multiple values?</p>\n\n<p>For example, if I wanted to sort by <code>numval</code> and sort by <code>strval</code>, and be able to get the highest (G\xf6del numbering for str) in $\\mathcal{O}(1)$.</p>\n\n\n\n<pre><code>struct Node {\n    int numval;\n    std::string strval;\n};\n</code></pre>\n\n<p>Easily I can think to just maintain two priority-queues, but this would require twice the memory.</p>\n\n<p>Is there a better way?</p>\n', 'ViewCount': '214', 'Title': 'Queue that can sort by multiple priorities?', 'LastActivityDate': '2012-07-05T13:37:54.173', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<data-structures><asymptotics><efficiency><priority-queues><memory-management>', 'CreationDate': '2012-07-05T13:37:54.173', 'FavoriteCount': '2', 'Id': '2629'},942:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>I\'ve been reading a bit of the literature lately, and have found some rather interesting data-structures.</p>\n\n<p>I have researched various different methods of getting update times down to $\\mathcal{O}(1)$ worst-case update time [1-7].</p>\n\n<p>Recently I begun looking into lock-free data-structures, to support efficient concurrent access.</p>\n\n<p><strong>Have any of these worst-case $\\mathcal{O}(1)$ update-time techniques been used in the implementation of lock-free data structures?</strong></p>\n\n<p>I ask because; to me, they seem like the obvious practical extension of this "theoretical enhancement".</p>\n\n<hr>\n\n<ol>\n<li><p><a href="http://www.sciencedirect.com/science/article/pii/0020019083900996">Tarjan, Robert Endre. \u201cUpdating a Balanced Search Tree in O(1) Rotations.\u201d Information Processing Letters 16, no. 5 (1983): 253 \u2013 257.</a></p></li>\n<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4630">Driscoll, J R, N Sarnak, D D Sleator, and R E Tarjan. \u201cMaking Data Structures Persistent.\u201d In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, 109\u2013121. STOC  \u201986. New York, NY, USA: ACM, 1986.</a></p></li>\n<li><p><a href="http://dx.doi.org/10.1007/BF00299635">Levcopoulos, C., and Mark H. Overmars. \u201cA Balanced Search Tree with O(1) Worst Case Update Time.\u201d Acta Inf. 26, no. 3 (November 1988): 269\u2013277.</a></p></li>\n<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.9433">Fleischer, Rudolf. A Simple Balanced Search Tree With O(1) Worst-Case Update Time</a></p></li>\n<li><p><a href="http://dx.doi.org/10.1016/0020-0190%2894%2900115-4">Dietz, Paul F, and Rajeev Raman. \u201cA Constant Update Time Finger Search Tree.\u201d Information Processing Letters 52, no. 3 (1994): 147 \u2013 154.</a></p></li>\n<li><p><a href="http://dl.acm.org/citation.cfm?id=998223.998229">Lagogiannis, George, Christos Makris, Yannis Panagis, Spyros Sioutas, and Kostas Tsichlas. \u201cNew Dynamic Balanced Search Trees with Worst-case Constant Update Time.\u201d J. Autom. Lang. Comb. 8, no. 4 (July 2003): 607\u2013632.</a></p></li>\n<li><p><a href="http://www.cs.au.dk/~gerth/papers/stoc02.pdf">Brodal, Gerth St\xf8lting, George Lagogiannis, Christos Makris, Athanasios Tsakalidis, and Kostas Tsichlas. \u201cOptimal Finger Search Trees in the Pointer Machine.\u201d J. Comput. Syst. Sci. 67, no. 2 (September 2003): 381\u2013418.</a></p></li>\n</ol>\n', 'ViewCount': '506', 'Title': 'Lock-free, constant update-time concurrent tree data-structures?', 'LastEditorUserId': '1120', 'LastActivityDate': '2013-05-14T16:27:44.497', 'LastEditDate': '2012-07-18T05:21:06.267', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<reference-request><data-structures><time-complexity><concurrency><search-trees>', 'CreationDate': '2012-07-10T20:04:39.177', 'FavoriteCount': '5', 'Id': '2680'},943:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Does there exist any data structure that efficiently (linearithmic-or-similar in-order traversal, sublinear time insertion/search/removal) implements an ordered set (i.e. a set that allows for enumerating the items in insertion order)?  </p>\n\n<p>If so, what data structure is it/how does it do this?</p>\n', 'ViewCount': '252', 'Title': 'Sub-linear-time ordered set (!= sorted set) implementation?', 'LastEditorUserId': '836', 'LastActivityDate': '2012-07-16T01:39:12.323', 'LastEditDate': '2012-07-15T23:48:25.350', 'AnswerCount': '2', 'CommentCount': '14', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '836', 'Tags': '<data-structures>', 'CreationDate': '2012-07-15T21:35:23.630', 'Id': '2755'},944:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The problem is as follows:</p>\n\n<p>Given a finite set of rectangles ($S\\subset\\mathbb{R}\\times\\mathbb{R}$), build a data structure that will support the following operations:</p>\n\n<ul>\n<li>Check, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and returns true iff there is a rectangle $c\\in S$ so that $r_x \\leq c_x$ and $r_y \\leq c_y$.</li>\n<li>Get, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and returns the minimal member of $S$ (that is, a member with a minimal area - $c_x c_y$) that contains $r$ (i.e., $r_x \\leq c_x$ and $r_y \\leq c_y$).</li>\n<li>Insert, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and adds it to $S$.</li>\n<li>Remove, receives a rectangle $r\\in\\mathbb{R}\\times\\mathbb{R}$, and removes it from $S$.</li>\n</ul>\n\n<p>The four procedures have to be efficient, measuring efficiency by $m$ and $n$, where $m$ is the number of different widths in $S$ and $n$ is the number of different heights in $S$.</p>\n\n<p>Space efficiency has to be no worse than $O(mn)$ (that is, linear to the number of elements in $S$).</p>\n', 'ViewCount': '215', 'Title': 'Finding a minimal containing rectangle from a given set of rectangles', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-18T11:09:53.763', 'LastEditDate': '2012-07-18T10:18:12.310', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2197', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2012-07-18T08:41:16.803', 'FavoriteCount': '1', 'Id': '2810'},945:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In terms of asymptotic space and time complexity, what is the most efficient priority-queue?\nSpecifically I am looking for priority queues which minimize the complexity of inserts, it\'s ok if deletes are a little slower.</p>\n\n<p><sub> If you\'re looking for a survey of priority-queues which minimises complexity of deletes over inserts, see: <a href="http://cs.stackexchange.com/q/524">Does there exist a priority queue with $O(1)$ extracts?</a>. </sub></p>\n', 'ViewCount': '816', 'Title': 'Most efficient known priority queue for inserts', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-05T08:59:24.467', 'LastEditDate': '2012-09-25T21:29:08.723', 'AnswerCount': '5', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1120', 'Tags': '<data-structures><time-complexity><priority-queues>', 'CreationDate': '2012-07-19T15:49:47.360', 'Id': '2824'},946:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve been looking for a way to represent the <a href="http://en.wikipedia.org/wiki/Golden_ratio_base" rel="nofollow">golden ratio ($\\phi$) base</a> more efficiently in binary.  The standard binary golden ratio notation works but is horribly space inefficient.  The  Balanced Ternary Tau System (BTTS) is the best I\'ve found but is quite obscure.  The paper describing it in detail is <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.880" rel="nofollow">A. Stakhov, Brousentsov\'s Ternary Principle, Bergman\'s Number System and Ternary Mirror-symmetrical Arithmetic, 2002</a>.  It is covered in less depth by <a href="http://neuraloutlet.wordpress.com/tag/ternary-tau-system/" rel="nofollow">this blog post</a>.</p>\n\n<p>BTTS is a <a href="http://en.wikipedia.org/wiki/Balanced_ternary" rel="nofollow">balanced ternary representation</a> that uses $\\phi^2 = \\phi + 1$ as a base and 3 values of $\\bar 1$ ($-1$), $0$, and $1$ to represent addition or subtraction of powers of $\\phi^2$.  The table on page 6 of the paper lists integer values from 0 up to 10, and it can represent any $\\phi$-based number as well.</p>\n\n<p>BTTS has some fascinating properties, but being ternary, I didn\'t think I\'d be able to find a compact bit representation for it.</p>\n\n<p>Then I noticed that because of the arithmetic rules, the pattern $\\bar 1 \\bar 1$ never occurs as long as you only allow numbers $\\ge 0$.  This means that the nine possible combinations for each pair of trits ($3^2$) only ever has 8 values, so we can encode 2 trits with 3 bits ($2^3$, a.k.a octal).  Also note that the left-most bit (and also right-most for integers because of the mirror-symmetric property) will only ever be $0$ or $1$ (again for positive numbers only), which lets us encode the left-most trit with only 1 bit.</p>\n\n<p>So a $2^n$-bit number can store $\\lfloor 2^n/3\\rfloor * 2 + 1$ balanced trits, possibly with a bit left over (maybe a good candidate for a sign bit).  For example, we can represent $10 + 1 = 11$ balanced trits with $15 + 1 = 16$  bits, or $20 + 1 = 21$ balanced trits with $30 + 1 = 31$ bits, with 1 left over (32-bit).  This has much better space density than ordinary golden ratio base binary encoding.</p>\n\n<p>So my question is, what would be a good octal (3-bit) encoding of trit pairs such that we can implement the addition and other arithmetic rules of the BTTS with as little difficulty as possible.  One of the tricky aspects of this system is that carries happen in both directions, i.e. <br/>\n$1 + 1 = 1 \\bar 1 .1$ and $\\bar 1 + \\bar 1 = \\bar 1 1.\\bar 1$.</p>\n\n<p>This is my first post here, so please let me know if I need to fix or clarify anything.</p>\n\n<p>--<strong>Edit</strong>--</p>\n\n<p>ex0du5 asked for some clarification of what I need from a binary representation:</p>\n\n<ol>\n<li>I want to be able to represent positive values of both integers and powers of $\\phi$.  The range of representable values need not be as good as binary, but it should be better than phinary per bit.  I want to represent the largest possible set of phinary numbers in the smallest amount of space possible.  Space takes priority over operation count for arithmetic operations.</li>\n<li>I need addition to function such that carries happen in both directions.  Addition will be the most common operation for my application.  Consequently it should require as few operations as possible.  If a shorter sequence of operations are possible using a longer bit representation (conflicting with goal 1), then goal 1 takes priority.  Space is more important than speed.</li>\n<li>Multiplication only needs to handle integers > 0 multiplied to a phinary number, not arbitrary phinary number multiplication, and so can technically be emulated with a series of additions, though a faster algorithm would be helpful.</li>\n<li>I\'m ignoring division and subtraction for now, but having algorithms for them would be a bonus.</li>\n<li>I need to eventually convert a phinary number to a binary floating point approximation of it\'s value, but this will happen only just prior to output.  There will be no converting back and forth.</li>\n</ol>\n', 'ViewCount': '294', 'Title': 'What is a good binary encoding for $\\phi$-based balanced ternary arithmetic algorithms?', 'LastEditorUserId': '2230', 'LastActivityDate': '2012-11-18T06:37:19.827', 'LastEditDate': '2012-07-26T14:22:13.457', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2230', 'Tags': '<algorithms><data-structures><efficiency><coding-theory>', 'CreationDate': '2012-07-20T21:32:29.663', 'Id': '2847'},947:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '256', 'Title': 'Origins of the term "distributed hash table"', 'LastEditDate': '2012-07-23T09:49:38.700', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1837', 'FavoriteCount': '1', 'Body': "<p>I am currently researching for my diploma thesis in computer science with a topic in the area of distributed hash tables. Naturally, I came to the question were the term <em>distributed hash table</em> came from. (I know it is not rocket science to just derive it from <em>distributing a hash table</em>, but somebody somewhere must have come up with it).</p>\n\n<p>Most papers I read referred to the original paper on <em>consistent hashing</em> and one of the first algorithms making use of it (e.g Chord). I know that there was a lot of research on distributed databases in the 80s, so I figure that the term, or maybe the idea behind it, should be older than ~15 years.</p>\n\n<p>The motivation behind this question is that knowing an earlier date and maybe another term for a similar idea would possibly widen the range of useful information I could gather for my research. For example, what have others done that is similar to what I want to do and where have they failed. Etc. etc.</p>\n\n<p>I tried to find more on this subject using <em>Structured Overlay Networks</em> as a search keyword, but the resulting definitions/papers are also quite young, which leaves me with the impression that the research topic might not be so old after all.</p>\n\n<p>Does anybody of you know of earlier research (maybe pre-90s?) in the topics of distributed hash tables and/or structured overlay networks? I'd be glad to hear some keywords which could lead me to more historic papers.</p>\n", 'Tags': '<data-structures><terminology><distributed-systems><hash-tables><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-21T07:15:27.943', 'CommentCount': '6', 'AcceptedAnswerId': '8961', 'CreationDate': '2012-07-23T09:43:04.950', 'Id': '2872'},948:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '691', 'Title': "How Does Populating Pastry's Routing Table Work?", 'LastEditDate': '2012-08-13T09:14:36.520', 'AnswerCount': '1', 'Score': '15', 'OwnerDisplayName': 'Paddy Foran', 'PostTypeId': '1', 'OwnerUserId': '2653', 'FavoriteCount': '2', 'Body': u'<p>I\'m trying to implement the Pastry Distributed Hash Table, but some things are escaping my understanding. I was hoping someone could clarify.</p>\n\n<p><strong>Disclaimer</strong>: I\'m not a computer science student. I\'ve taken precisely two computer science courses in my life, and neither dealt with anything remotely complex. I\'ve worked with software for years, so I feel I\'m up to the implementation task, if I could just wrap my head around the ideas. So I may just be missing something obvious.</p>\n\n<p>I\'ve read the paper that the authors published [1], and I\'ve made some good progress, but I keep getting hung up on this one particular point in how the routing table works:</p>\n\n<p>The paper claims that</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into $\\lceil \\log_{2^b} N\\rceil$ \n  rows with $2^b - 1$ entries each. The $2^b - 1$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>The $b$ stands for an application-specific variable, usually $4$. Let\'s use $b=4$, for simplicity\'s sake. So the above is</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into  $\\lceil \\log_{16} N\\rceil$ rows \n  with $15$ entries each. The $15$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>I understand that much. Further, $N$ is the number of servers in the cluster. I get that, too.</p>\n\n<p>My question is, if the row an entry is placed into depends on the shared length of the key, why the seemingly random limit on the number of rows? Each nodeId has 32 digits, when $b=4$ (128 bit nodeIds divided into digits of b bits). So what happens when $N$ gets high enough that $\\lceil\\log_{16} N\\rceil &gt; 32$? I realise it would take 340,282,366,920,938,463,463,374,607,431,768,211,457 (if my math is right) servers to hit this scenario, but it just seems like an odd inclusion, and the correlation is never explained.</p>\n\n<p>Furthermore, what happens if you have a small number of servers? If I have fewer than 16 servers, I only have one row in the table. Further, under no circumstances would every entry in the row have a corresponding server. Should entries be left empty? I realise that I\'d be able to find the server in the leaf set no matter what, given that few servers, but the same quandary is raised for the second row--what if I don\'t have a server that has a nodeId such that I can fill every possible permutation of the nth digit? Finally, if I have, say, four servers, and I have two nodes that share, say, 20 of their 32 digits, by some random fluke... should I populate 20 rows of the table for that node, even though that is far more rows than I could even come close to filling?</p>\n\n<p>Here\'s what I\'ve come up with, trying to reason my way through this:</p>\n\n<ol>\n<li>Entries are to be set to a null value if there is not a node that matches that prefix precisely.</li>\n<li>Empty rows are to be added until enough rows exist to match the shared length of the nodeIds.</li>\n<li>If, and only if, there is no matching entry for a desired message ID, fall back on a search of the routing table for a nodeId whose shared length is greater than or equal to the current nodeId\'s and whose entry is mathematically closer than the current nodeId\'s to the desired ID.</li>\n<li>If no suitable node can be found in #3, assume this is the destination and deliver the message.</li>\n</ol>\n\n<p>Do all four of these assumptions hold up? Is there somewhere else I should be looking for information on this?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1007/3-540-45518-3_18" rel="nofollow">Pastry: Scalable, decentralized object location and routing for large-scale peer-to-peer systems</a> by A. Rowstrong and P. Druschel (2001) -- <a href="http://research.microsoft.com/~antr/PAST/pastry.pdf" rel="nofollow">download here</a></li>\n</ol>\n', 'Tags': '<algorithms><data-structures><distributed-systems><hash-tables>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T17:34:24.577', 'CommentCount': '2', 'AcceptedAnswerId': '6069', 'CreationDate': '2012-05-23T22:02:33.000', 'Id': '3138'},949:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Is there an algorithm to perform batch processing in the increase-key operation? Let us say, a binary heap (min-heap) is used. In the normal increase-key function, if we perform increase key on one node, then we have to traverse paths from the node towards the children to re balance the heap. If we want to increase the keys of five nodes in the heap, we need to call the increase-key function five times. Is it possible to call only one increase-key function and perform increase-key on five nodes simultaneously?</p>\n', 'ViewCount': '217', 'Title': 'Batch processing in increase-key function using binary heap', 'LastEditorUserId': '472', 'LastActivityDate': '2012-08-15T11:00:10.347', 'LastEditDate': '2012-08-14T12:08:17.190', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2460', 'Tags': '<algorithms><data-structures><parallel-computing><concurrency><priority-queues>', 'CreationDate': '2012-08-14T00:39:37.677', 'FavoriteCount': '1', 'Id': '3163'},950:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am looking at a modelling tool and are trying to determine all the types of ways that you can model (at a rudimentary level) </p>\n\n<p>I remember seeing a list of ways in which you can connect or categorise information elements. basically the types were as follows:</p>\n\n<ul>\n<li>Lists - constitute a list of information elements</li>\n<li>Hierarchies- visualise information in a parent-child relationship (ie organisational chart)</li>\n<li><p>Flows - connect elements in a logical (lateral) flow (ie process model)</p>\n\n<p>Have you seen any reference to these types, or can you elaborate on the full list of "model types"</p></li>\n</ul>\n', 'ViewCount': '74', 'Title': 'What are the rudimentary types of information connectivity i.e. model types?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-14T20:46:24.903', 'LastEditDate': '2012-08-14T20:46:24.903', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2512', 'Tags': '<data-structures><information-theory><modelling><structured-data>', 'CreationDate': '2012-08-14T16:01:08.207', 'FavoriteCount': '1', 'Id': '3182'},951:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In the book 'Introduction to Algorithms 3/e', I have found the following definition of Binary Search Tree property:</p>\n\n<blockquote>\n  <p>Let $x$ be a node in a binary search tree. If $y$ is a node in the left subtree\n  of $x$, then $y.key \\leq x.key$. If $y$ is a node in the right subtree of $x$, then $y.key \\geq x.key$.</p>\n</blockquote>\n\n<p>My confusion is that while implementing binary search trees we either consider that the keys of left-subtree of a node $x$ would be $\\leq x.key$ <strong>or</strong> the keys of right-subtree of a node $x$ would be $\\geq x.key$ but <strong>not both</strong>. That is we follow one of the two convention. But in the property they have included $=$ in both the cases. Where am I wrong?</p>\n\n<p>I would appreciate any idea on this issue.</p>\n", 'ViewCount': '274', 'Title': 'Binary Search Tree Property', 'LastEditorUserId': '29', 'LastActivityDate': '2012-08-29T00:50:34.147', 'LastEditDate': '2012-08-28T11:38:51.780', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2556', 'Tags': '<terminology><data-structures><binary-trees><search-trees>', 'CreationDate': '2012-08-28T05:54:20.237', 'Id': '3345'},952:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A <a href="http://en.wikipedia.org/wiki/Partition_of_a_set" rel="nofollow">partition of a set</a> S is a separation of the set into an arbitrary number of non-empty, pairwise disjoint subsets whose union is exactly S. What manner of a data structure should be used to represent a partition of a set if the following methods need to be optimized:</p>\n\n<ol>\n<li>moving an element from one part to another, possibly an entirely new one, and</li>\n<li>iterating over the parts of the partition.</li>\n</ol>\n\n<p>A naive way of prioritizing 1 would be a hash/tree/whatever mapping from set elements to "part labels", but iterating over the parts would require O(N) for first constructing the actual parts from the labels. 2 is naively prioritized as a hash/tree/whatever set of hash/tree/whatever sets, but then moving elements around, especially to new subsets, incurs that memory management overhead.</p>\n\n<p>Is there a way to get the best of both worlds? The implementation I need is Python but I imagine this is a cross-language question.</p>\n', 'ViewCount': '219', 'Title': 'Data structure for partition of a set', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-03T21:31:03.673', 'LastEditDate': '2012-09-03T21:31:03.673', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'OwnerDisplayName': 'user1448338', 'PostTypeId': '1', 'Tags': '<data-structures><partitions><sets>', 'CreationDate': '2012-07-12T16:32:56.523', 'Id': '3414'},953:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '751', 'Title': 'Removing duplicates efficiently and with a low memory overhead', 'LastEditDate': '2012-10-19T08:35:43.330', 'AnswerCount': '1', 'Score': '7', 'OwnerDisplayName': 'doc', 'PostTypeId': '1', 'OwnerUserId': '2729', 'FavoriteCount': '3', 'Body': '<p>I want to filter efficiently a list of integers for duplicates in a way that only the resulting set needs to be stored.</p>\n\n<p>One way this can be seen:</p>\n\n<ul>\n<li>we have a range of integers $S = \\{1, \\dots{}, N\\}$ with $N$ big (say $2^{40}$)</li>\n<li>we have a function $f : S \\to S$ with, supposedly, many collisions (the images are uniformly distributed in $S$)</li>\n<li>we then need to store $f[S]$, that is $\\{f(x) | x \\in S\\}$</li>\n</ul>\n\n<p>I have a quite accurate (probabilistic) estimation of what $|f[S]|$ is, and can therefore allocate data structures in advance (say $|f[S]| \\approx 2^{30}$).</p>\n\n<p>I have had a few ideas, but I am not sure what would be the best approach:</p>\n\n<ul>\n<li>a bitset is out of the question because the input set does not fit into memory.</li>\n<li>a hash table, but (1) it requires some memory overhead, say 150% of $|f[S]|$ and (2) the table has to be explored when built which requires additional time because of the memory overhead.</li>\n<li>an "on the fly" sort, preferably with $O(N)$ complexity (non-comparison sort). Regarding that, I am not sure what is the major difference between <a href="http://en.wikipedia.org/wiki/Bucket_sort" rel="nofollow">bucket sort</a> and <a href="http://en.wikipedia.org/wiki/Flashsort" rel="nofollow">flashsort</a>.</li>\n<li>a simple array with a binary search tree, but this requires $O(N \\log |f[S]|)$ time.</li>\n<li>maybe using <a href="http://en.wikipedia.org/wiki/Bloom_filter" rel="nofollow">Bloom filters</a> or a similar data structure could be useful in a relaxation (with false positives) of the problem.</li>\n</ul>\n\n<p>Some questions on stackoverflow seem to tackle with this sort of things (<a href="http://stackoverflow.com/questions/12240997/sorting-array-in-on-run-time">http://stackoverflow.com/questions/12240997/sorting-array-in-on-run-time</a>, <a href="http://stackoverflow.com/questions/3951547/java-array-finding-duplicates">http://stackoverflow.com/questions/3951547/java-array-finding-duplicates</a>), but none seems to match my requirements.</p>\n', 'Tags': '<algorithms><data-structures><sorting>', 'LastEditorUserId': '2729', 'LastActivityDate': '2012-11-21T23:43:10.673', 'CommentCount': '17', 'CreationDate': '2012-09-03T10:11:36.747', 'Id': '3420'},954:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have seen two definitions of balanced binary trees, which look different to me.</p>\n\n<ol>\n<li><p>A binary tree is balanced if for each node it holds that the number of inner nodes in the left subtree and the number of inner nodes in the right subtree differ by at most 1.</p></li>\n<li><p>A binary tree is balanced if for any two leaves the difference of the depth is at most 1.</p></li>\n</ol>\n\n<p>Does every tree that satisfies def. 1 also satisfy def. 2? What about the other way round?</p>\n', 'ViewCount': '1138', 'Title': 'Two definitions of balanced binary trees', 'LastEditorUserId': '41', 'LastActivityDate': '2012-09-12T22:31:04.060', 'LastEditDate': '2012-09-12T20:24:51.237', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2012-09-12T17:37:24.510', 'FavoriteCount': '1', 'Id': '3515'},955:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to create a bloom filter of 208 million URLs. What would be a good choice of bit vector size and number of hash functions? I tried a bit vector of size 1 GB and 4 hash functions, but it resulted in too many false positives while reading.</p>\n\n<p>I have a huge web corpus containing web content of billions of URLs. I need to process the web content of URLs satisfying certain criteria: the URL should have appeared in web search results in the past 7 days at least 5 times. This is represented by a list of 208 million URLs. Joining the list directly with the web corpus is not feasible because of volume. So I am considering creation of a bloom filter out of the list and then using the bloom filter to prune out unnecessary URLs from the web corpus.</p>\n', 'ViewCount': '387', 'Title': 'Bloom Filter for 208 million URLs', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-26T13:17:46.310', 'LastEditDate': '2013-06-26T13:17:46.310', 'AnswerCount': '2', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2881', 'Tags': '<data-structures><probabilistic-algorithms><searching><big-data><bloom-filters>', 'CreationDate': '2012-09-19T15:58:11.313', 'Id': '4616'},956:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I see <strong>structural induction</strong> the usual way for proving an algorithm's <strong>termination</strong> property, but it's not that easy to prove by means of induction on a <strong>tree</strong> algorithm. Now I am struggling on proving that the pre-order tree traversal algorithm is terminable:</p>\n\n<pre><code>preorder(node)\n  if node == null then return\n  visit(node)\n  preorder(node.left) \n  preorder(node.right)\n</code></pre>\n\n<p>How should I prove?</p>\n", 'ViewCount': '220', 'Title': 'How to prove that the pre-order tree traversal algorithm terminates?', 'LastEditorUserId': '39', 'LastActivityDate': '2012-09-26T20:14:24.327', 'LastEditDate': '2012-09-26T20:14:24.327', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4726', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2954', 'Tags': '<algorithms><data-structures><algorithm-analysis><correctness-proof><induction>', 'CreationDate': '2012-09-24T15:52:44.860', 'Id': '4719'},957:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The notion of best-case running time is kind of ambiguous for me. According to wikipedia, the definition of best case running time is:</p>\n\n<blockquote>\n  <p>The term best-case performance is used in computer science to describe the way of an algorithm behaves under optimal conditions. For example, the best case for a simple linear search on a list occurs when the desired element is the first element of the list.</p>\n</blockquote>\n\n<p>According to this definition, the best case running time for BST insertion should be $O(1)$ [consider that we are inserting to the root node]. But different resources says different things, some claim that it is $O(\\log n)$ [perfect balanced tree] and some others claim that it is $O(1)$ which one should I believe?</p>\n', 'ViewCount': '1986', 'Title': 'Best-Case Running Time For Binary Search Tree Insertion', 'LastEditorUserId': '98', 'LastActivityDate': '2012-09-24T22:16:37.427', 'LastEditDate': '2012-09-24T22:12:54.513', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '4725', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2956', 'Tags': '<terminology><data-structures><runtime-analysis><binary-trees>', 'CreationDate': '2012-09-24T21:48:27.473', 'Id': '4723'},958:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Not sure exactly how to define this but here\'s the basis of the question (very loosely)</p>\n\n<ul>\n<li>All lists can be contained in tree\'s</li>\n<li>all trees can be contained in a graph</li>\n<li>all graphs can be contained in a multi graph, </li>\n<li>and all multigraphs can be contained in a <a href="http://en.wikipedia.org/wiki/Hypergraph" rel="nofollow">hypergraph</a> (never actually heard of a hypergraph before searching for an answer to this, so may not have it quite right)</li>\n</ul>\n\n<p>So there\'s a containment hierarchy of \'complexity\' of data structures. Is Hypergraph the highest one, is there a proof of this, and is there a name for the study of whatever this is. </p>\n\n<p>By highest one, I mean all structures can be represented as a hypergraph if I chose to. So in any code I could represent all the structures by using a number of hypergraphs (not that I would). tia </p>\n\n<p>[Update] Perhaps an alternative way to ask it is, for any arrangement of data structures consisting of nodes and edges. Does there exist a structure that can\'t be contained in a hypergraph.</p>\n\n<p>Then there is a second question - is there any data structure which can\'t be represented using nodes and edges.</p>\n', 'ViewCount': '154', 'Title': "What's the highest 'order' data structure", 'LastEditorUserId': '2985', 'LastActivityDate': '2012-09-27T15:19:36.767', 'LastEditDate': '2012-09-27T11:25:45.083', 'AnswerCount': '1', 'CommentCount': '11', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2985', 'Tags': '<data-structures>', 'CreationDate': '2012-09-27T10:42:07.417', 'Id': '4760'},959:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p><a href="http://en.wikipedia.org/wiki/Partition_refinement">Partition refinement</a> is a technique in which you start with a finite set of objects and progressively split the set. Some problems, like DFA minimization, can be solved using partition refinement quite efficiently. I don\'t know of any other problems that are usually solved using partition refinement other than the ones listed on the Wikipedia page. Out of all these problems, the Wikipedia page mentions two for which algorithms based on partition refinement run in linear time. There\'s the lexicographically ordered topological sort [1] and an algorithm for <a href="http://en.wikipedia.org/wiki/Lexicographic_breadth-first_search">lexicographic breadth-first search</a> [2].</p>\n\n<blockquote>\n  <p>Are there any other examples or references to problems that can be solved using partition refinement very efficiently, meaning something better than loglinear in terms of time?</p>\n</blockquote>\n\n<hr>\n\n<p>[1] <a href="http://epubs.siam.org/doi/abs/10.1137/0205005">Sethi, Ravi, "Scheduling graphs on two processors", SIAM Journal on Computing 5 (1): 73\u201382, 1976.</a></p>\n\n<p>[2] <a href="http://epubs.siam.org/doi/abs/10.1137/0205021">Rose, D. J., Tarjan, R. E., Lueker, G. S., "Algorithmic aspects of vertex elimination on graphs", SIAM Journal on Computing 5 (2): 266\u2013283, 1976.</a></p>\n', 'ViewCount': '157', 'Title': 'Problems for which algorithms based on partition refinement run faster than in loglinear time', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-05T01:32:41.660', 'LastEditDate': '2012-10-04T22:21:19.390', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '472', 'Tags': '<algorithms><reference-request><data-structures><partitions><sets>', 'CreationDate': '2012-10-02T17:25:18.093', 'FavoriteCount': '1', 'Id': '4843'},960:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href="http://cs.stackexchange.com/questions/439/which-combinations-of-pre-post-and-in-order-sequentialisation-are-unique">Which combinations of pre-, post- and in-order sequentialisation are unique?</a>  </p>\n</blockquote>\n\n\n\n<p>I have three different tree traversal of a binary tree Inorder, Preorder and Postorder. I want to identify the tree back from these traversal. I found that a single traversal cant not identify the tree.<br>\n So if I\'ll take in pair then can I identiy the tree?</p>\n', 'ViewCount': '40', 'ClosedDate': '2012-10-13T23:11:23.750', 'Title': 'How to identify a binary tree uniquely if its Inorder, Preorder and Postorder traversal is given?', 'LastActivityDate': '2012-10-13T04:52:21.040', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '3075', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2012-10-13T04:52:21.040', 'Id': '5039'},961:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Two problems: Given a known table (boolean or int), convert it to a function that returns the same value using only simple operations (and, or, xor, sum...). For example:</p>\n\n<p><code>bool table1[] = {0,1}</code> converted to <code>fun (int index) {return index;}</code></p>\n\n<p><code>bool table2[] = {0,1,1,0}</code> converted to <code>fun (int index) {return index ^ (index&gt;&gt;1);}</code></p>\n\n<p><code>int table3[] = {4,5,7}</code> converted to <code>fun(int index) {return index+4+(index&gt;&gt;1);}</code></p>\n', 'ViewCount': '77', 'Title': 'Convert table look-up into function', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T11:00:07.563', 'LastEditDate': '2012-10-14T09:50:06.500', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '0', 'OwnerDisplayName': 'Alain', 'PostTypeId': '1', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-10-14T01:55:10.093', 'Id': '6053'},962:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '3565', 'Title': 'A d-ary heap problem from CLRS', 'LastEditDate': '2012-10-17T19:50:03.620', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'lucasKoFromTW', 'PostTypeId': '1', 'OwnerUserId': '4195', 'Body': u'<p>I got confused while solving the following problem (questions 1\u20133).</p>\n\n<h3>Question</h3>\n\n<blockquote>\n  <p>A <em>d</em>-ary heap is like a binary heap, but(with one possible exception) non-leaf nodes have <em>d</em> children instead of 2 children.</p>\n  \n  <ol>\n  <li><p>How would you represent a <em>d</em>-ary heap in an array?</p></li>\n  <li><p>What is the height of a <em>d</em>-ary heap of <em>n</em> elements in terms of <em>n</em> and <em>d</em>?</p></li>\n  <li><p>Give an efficient implementation of EXTRACT-MAX in a <em>d</em>-ary max-heap. Analyze its running time in terms of <em>d</em> and <em>n</em>.</p></li>\n  <li><p><sub> Give an efficient implementation of INSERT in a <em>d</em>-ary max-heap. Analyze its running time in terms of <em>d</em> and <em>n</em>. </sub></p></li>\n  <li><p><sub> Give an efficient implementation of INCREASE-KEY(<em>A</em>, <em>i</em>, <em>k</em>), which flags an error if <em>k</em> &lt; A[i] = k and then updates the <em>d</em>-ary matrix heap structure appropriately. Analyze its running time in terms of <em>d</em> and <em>n</em>. </sub></p></li>\n  </ol>\n</blockquote>\n\n<h3>My Solution</h3>\n\n<ol>\n<li><p>Give an array $A[a_1 .. a_n]$</p>\n\n<p>$\\qquad \\begin{align}\n \\text{root} &amp;: a_1\\\\\n \\text{level 1} &amp;: a_{2} \\dots a_{2+d-1}\\\\\n \\text{level 2} &amp;: a_{2+d} \\dots a_{2+d+d^2-1}\\\\\n &amp;\\vdots\\\\\n \\text{level k} &amp;: a_{2+\\sum\\limits_{i=1}^{k-1}d^i} \\dots a_{2+\\sum\\limits_{i=1}^{k}d^i-1}\n \\end{align}$</p>\n\n<p>\u2192 <strong>My notation seems a bit sophisticated. Is there any other simpler one?</strong></p></li>\n<li><p>Let <em>h</em> denotes the height of the <em>d</em>-ary heap.</p>\n\n<p>Suppose that the heap is a complete <em>d</em>-ary tree\n$$\n 1+d+d^2+..+d^h=n\\\\\n \\dfrac{d^{h+1}-1}{d-1}=n\\\\\n h=log_d[n{d-1}+1] - 1\n $$</p></li>\n<li><p>This is my implementation:</p>\n\n<pre><code>EXTRACT-MAX(A)\n1  if A.heapsize &lt; 1\n2      error "heap underflow"\n3  max = A[1]\n4  A[1] = A[A.heapsize]\n5  A.heap-size = A.heap-size - 1\n6  MAX-HEAPIFY(A, 1)\n7  return max\n\nMAX-HEAPIFY(A, i)\n1  assign depthk-children to AUX[1..d]\n2  for k=1 to d\n3      compare A[i] with AUX[k]\n4      if A[i] &lt;= AUX[k]\n5          exchange A[i] with AUX[k]\n6          k = largest\n7  assign AUX[1..d] back to A[depthk-children]\n8  if largest != i\n9      MAX-HEAPIFY(A, (2+(1+d+d^2+..+d^{k-1})+(largest-1) )\n</code></pre>\n\n<ul>\n<li><p>The running time of MAX-HEAPIFY:</p>\n\n<p>$$T_M = d(c_8*d + (c_9+..+c_13)*d +c_14*d)$$\nwhere $c_i$ denotes the cost of <em>i</em>-th line above.</p></li>\n<li><p>EXTRACT-MAX:\n$$\n       T_E = (c_1+..+c_7) + T_M \\leq C*d*h\\\\\n       = C*d*(log_d[n(d-1)+1] - 1)\\\\\n       = O(dlog_d[n(d-1)])\n       $$</p></li>\n</ul>\n\n<p>\u2192 <strong>Is this an efficient solution? Or there is something wrong within my solution?</strong></p></li>\n</ol>\n', 'Tags': '<data-structures><time-complexity><runtime-analysis>', 'LastEditorUserId': '39', 'LastActivityDate': '2012-11-12T17:23:56.180', 'CommentCount': '0', 'AcceptedAnswerId': '6097', 'CreationDate': '2012-10-15T03:45:12.463', 'Id': '6078'},963:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>First, consider this simple problem --- design a data structure of <em>comparable</em> elements that behaves just like a stack (in particular, push(), pop() and top() take constant time), but can also return its min value in $O(1)$ time, without removing it from the stack. This is easy by maintaining a second stack of min values.</p>\n\n<p>Now, consider the same problem, where the stack is replaced by a queue. This seems impossible because one would need to keep track of $\\Theta(n^2)$ values (min values between elements $i$ and $j$ in the queue). True or false ?</p>\n\n<p>Update: $O(1)$ amortized time is quite straightforward as explained in one of the answers (using two min-stacks). A colleague pointed out to me that one can de-amortize such data structures by performing maintenance operations proactively. This is a little tricky, but seems to work.</p>\n', 'ViewCount': '378', 'Title': 'Lower bounds: queues that return their min elements in $O(1)$ time', 'LastEditorUserId': '5189', 'LastActivityDate': '2013-01-04T15:12:38.933', 'LastEditDate': '2012-12-26T18:06:43.007', 'AnswerCount': '2', 'CommentCount': '1', 'AcceptedAnswerId': '6467', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5189', 'Tags': '<data-structures><priority-queues><lower-bounds>', 'CreationDate': '2012-10-18T05:16:22.060', 'Id': '6146'},964:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I was asked a question on how to use a pair of Queues to create a Stack and how to use a pair of Stacks to create a Queue. Any thoughts on how I would do this? Right now I don't even know where to start.</p>\n", 'ViewCount': '97', 'Title': 'Using Queues for a Stack and Stacks for a Queue', 'LastActivityDate': '2012-10-19T00:44:19.440', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4261', 'Tags': '<data-structures><priority-queues><stack>', 'CreationDate': '2012-10-18T21:43:10.067', 'Id': '6155'},965:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>This question assumes the definition of a complete binary tree to be<sup>\u2020</sup>:</p>\n\n<blockquote>\n  <p>A binary tree $T$ with $N$ levels is complete if all levels except possibly the last are completely full, and the last level has all its nodes to the left side.</p>\n</blockquote>\n\n<p>The following is an excerpt from <em><a href="http://www.amazon.ca/Algorithms-Sanjoy-Dasgupta/dp/0073523402" rel="nofollow">Algorithms</a></em>:</p>\n\n<blockquote>\n  <p>It ($\\log N$) is also the depth of a complete binary tree with $N$ nodes. (More precisely: $\u230a\\log N\u230b$.)</p>\n</blockquote>\n\n<p>Why is the above excerpt valid?</p>\n\n<p><sup>\u2020</sup> Originally defined <a href="http://courses.cs.vt.edu/~cs3114/Summer11/Notes/T03a.BinaryTreeTheorems.pdf" rel="nofollow">here</a>  </p>\n', 'ViewCount': '1528', 'Title': 'What is the depth of a complete binary tree with $N$ nodes?', 'LastActivityDate': '2012-10-19T06:27:37.757', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6162', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4267', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2012-10-19T03:19:21.177', 'Id': '6161'},966:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="https://www.interviewstreet.com/challenges/dashboard/#problem/4f1c88e0dec8a" rel="nofollow">https://www.interviewstreet.com/challenges/dashboard/#problem/4f1c88e0dec8a</a></p>\n\n<blockquote>\n  <p>Fairy Chess (35 Points)</p>\n  \n  <p>You have a $N \\times N$ chess board. An $S$-leaper is a chess piece which can move from square $(x_1,y_1)$ on the board to any other square $(x_2,y_2)$ if $|x_1 - x_2| + |y_1 - y_2| \\le S$. The chess board may also contain some pawns. The leaper cannot land on the same square as a pawn. In how many ways can a leaper move $M$ times on the board?</p>\n  \n  <p>Input:\n  The first line contains the number of test cases $T$. $T$ cases follow. Each case contains integers $N$, $M$ and $S$ on the first line. The next $N$ lines contains $N$ characters each. The $i$th character on the $j$th line is a <code>.</code> if the corresponding chess square is empty, <code>P</code> if there is a pawn, or <code>L</code> if the leaper is situated on that square.</p>\n  \n  <p>Output:\n  For each case, output the number of ways the leaper can make $M$ moves. Output each answer modulo 1000000007.</p>\n  \n  <p>Constraints:\n  $$\\begin{gather}\n1 \\le T \\le 10 \\\\\n1 \\le S \\le N \\le 200 \\\\\n1 \\le M \\le 200 \\\\\n\\end{gather}$$\n  There will be exactly one <code>L</code> character on the board.</p>\n  \n  <p>Sample Input:</p>\n\n<pre><code>3\n4 1 1\n....\n.L..\n.P..\n....\n3 2 1\n...\n...\n..L\n4 3 2\n....\n...L\n..P.\nP...\n</code></pre>\n  \n  <p>Sample Output:</p>\n\n<pre><code>4\n11\n385\n</code></pre>\n</blockquote>\n\n<p>I wrote a DP solution but with O(N^5).</p>\n\n<blockquote>\n  <p>Psuedocode for ways function:</p>\n</blockquote>\n\n<pre><code>Memoize[Xmax][Ymax][Nmax];\n\nways(int X, int Y, int M) // (X,Y) current co-ordinates and M is number of moves to make\n{\n    if(Memoize[X][Y][M] != -1) // != -1 means we already have the result\n        return Memoize[X][Y][M];\n    sum=0;\n    for all (u,v) such that |X-u| + |Y-v| &lt;= S\n        sum += ways(u, v, M-1);\n\n    Memoize[X][Y][M] = sum;\n    return sum;\n\n}\n</code></pre>\n\n<blockquote>\n  <p>Code:</p>\n</blockquote>\n\n<pre><code>/* Enter your code here. Read input from STDIN. Print output to STDOUT */\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nlong ways(long ***DP, char **Chess, int X, int Y, int S, int N, int M);\n\nint main()\n{\n    int T;\n    scanf("%d", &amp;T);\n    while(T&gt;0)\n    {\n        int N, M, S;\n        scanf("%d", &amp;N);\n        scanf("%d", &amp;M);\n    scanf("%d", &amp;S);\n        long ***DP = (long ***) malloc(sizeof(long **) * N);\n        int i,j,k;\n        char **Chess = (char **)malloc(sizeof(char *) *N);\n        int Xstart;\n        int Ystart; //printf("N=%dM=%dS=%d\\n", N, M, S);\n        for(i=0;i&lt;N;i++)\n        {\n        Chess[i] = (char *)malloc(sizeof(char) *N);\n        DP[i] = (long **)malloc(sizeof(long *) * N);\n            for(j=0;j&lt;N;j++)\n            {   //printf("i=%dj=%d\\n", i, j);\n        DP[i][j] = (long *)malloc(sizeof(long) *(M+1));\n                char a;\n                scanf(" %c", &amp;a);\n                if(a==\'L\')\n                {\n                    Xstart = i;\n                    Ystart = j;\n                }\n                Chess[i][j] = a;\n        //printf("jf=%d\\n", j);\n        }\n        }\n\n        for(i=0;i&lt;N;i++)\n        {\n            for(j=0;j&lt;N;j++)\n            {\n                for(k=1;k&lt;M+1;k++)\n                {\n                    DP[i][j][k] = -1;\n                }\n                DP[i][j][0] = 1;\n            }\n        }\n\n       printf("%ld\\n", ways(DP, Chess, Xstart, Ystart, S, N, M));\n       T--;\n    }\n}\n\nlong ways(long ***DP, char **Chess, int X, int Y, int s, int N, int M)\n{\n    if(DP[X][Y][M] !=-1)\n    {\n    //printf("X=%d Y=%d M=%d Val=%ld\\n", X, Y, M, DP[X][Y][M]);\n    return DP[X][Y][M];\n    }\n    else\n    {\n        long sum1 = 0;\n\n\n        int S,k;\n    sum1 += ways(DP, Chess, X, Y, s, N, M-1);\n\n        for(S=1;S&lt;=s;S++)\n    {\n        for(k=0;k&lt;=S;k++)\n            {\n                if(k!=0 &amp;&amp; (S-k)!=0)\n                {\n                    if(X+k&lt;N &amp;&amp; Y+(S-k) &lt;N &amp;&amp; Chess[X+k][Y+(S-k)] != \'P\')\n                    {\n                        Chess[X+k][Y+(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X+k, Y+(S-k), s, N, M-1);\n                Chess[X+k][Y+(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(X+k&lt;N &amp;&amp; Y-(S-k)&gt;=0 &amp;&amp; Chess[X+k][Y-(S-k)] != \'P\')\n                    {\n                        Chess[X+k][Y-(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X+k, Y-(S-k), s, N, M-1);\n                Chess[X+k][Y-(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';  \n                    }\n                    if(X-k&gt;=0 &amp;&amp; Y+(S-k) &lt;N &amp;&amp; Chess[X-k][Y+(S-k)] != \'P\')\n                    {\n                        Chess[X-k][Y+(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X-k, Y+(S-k), s, N, M-1);\n                Chess[X-k][Y+(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(X-k&gt;=0 &amp;&amp; Y-(S-k) &gt;=0 &amp;&amp; Chess[X-k][Y-(S-k)] != \'P\')\n                    {\n                        Chess[X-k][Y-(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X-k, Y-(S-k), s, N, M-1);\n                            Chess[X-k][Y-(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n\n\n                }\n                else if(k==0 &amp;&amp; (S-k)!=0)\n                {\n                    if(Y+(S-k)&lt;N &amp;&amp; Chess[X][Y+(S-k)] != \'P\')\n                    {\n                        Chess[X][Y+(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X, Y+(S-k), s, N, M-1);\n                Chess[X][Y+(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(Y-(S-k)&gt;=0 &amp;&amp; Chess[X][Y-(S-k)] != \'P\')\n                    {\n                        Chess[X][Y-(S-k)] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X, Y-(S-k), s, N, M-1);\n                Chess[X][Y-(S-k)] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n\n\n                }\n                else if(k!=0 &amp;&amp; (S-k)==0)\n                {\n                    if(X+k&lt;N &amp;&amp; Chess[X+k][Y] != \'P\')\n                    {\n                        Chess[X+k][Y] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X+k, Y, s, N, M-1);\n                Chess[X+k][Y] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n                    if(X-k&gt;=0 &amp;&amp; Chess[X-k][Y] != \'P\')\n                    {\n                        Chess[X-k][Y] = \'L\';\n                        Chess[X][Y] = \'.\';\n                        sum1 += ways(DP, Chess, X-k, Y, s, N, M-1);\n                Chess[X-k][Y] = \'.\';\n                        Chess[X][Y] = \'L\';\n                    }\n\n\n                }\n            }\n    }\n    //printf("X=%d Y=%d M=%d Val=%ld\\n", X, Y, M, sum);\n    DP[X][Y][M] = sum1;\n        return sum1;\n    }   \n}\n</code></pre>\n', 'ViewCount': '396', 'Title': 'How to do Fairy Chess problem in O(N^3)?', 'LastEditorUserId': '4272', 'LastActivityDate': '2014-02-11T06:33:58.330', 'LastEditDate': '2012-10-19T19:07:55.293', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '4272', 'Tags': '<algorithms><data-structures><dynamic-programming>', 'CreationDate': '2012-10-19T16:29:04.340', 'FavoriteCount': '1', 'Id': '6170'},967:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<blockquote>\n  <p>Let $f(N)$ be the average number of full nodes (nodes with two children) in an $N$-node binary search tree.</p>\n  \n  <ol>\n  <li>Determine the values of $f(0)$ and $f(1)$. </li>\n  <li><p>Given that for $N &gt; 1$, </p>\n  \n  <p>$\\qquad \\displaystyle f(N) = \\frac{N-2}{N} + \\frac{1}{N} \\sum_{i=0}^{N-1} [f(i) + f(N - i - 1)]$,</p>\n  \n  <p>show that $f(N) = \\frac{N - 2}{3}$.</p></li>\n  <li>Using this information, show that the average number of nodes with one child in a binary search tree is $\\frac{N + 1}{3}$.</li>\n  </ol>\n</blockquote>\n\n<p>I know that for (1) both values are $0$. I mainly need help proving (2).\nI also found some hints for (2) and (3) but I can't figure it out:</p>\n\n<p>(2) The root contributes $\\frac{N \u2212 2}{N}$ full nodes on average, because the root is full as long as it does not contain the largest or smallest item. The remainder of the equation is the expected contribution of the subtrees.</p>\n\n<p>(3) The average number of leaves is $\\frac{N + 1}{3}$.</p>\n\n<p>Any help would be appreciated, even just pointing me in the right direction. Thanks!</p>\n", 'ViewCount': '517', 'Title': 'Average number of full nodes in a binary search tree', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-22T16:49:13.143', 'LastEditDate': '2012-10-22T15:51:25.087', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'OwnerDisplayName': 'user1758064', 'PostTypeId': '1', 'Tags': '<data-structures><combinatorics><recurrence-relation><binary-trees>', 'CreationDate': '2012-10-19T02:17:50.273', 'Id': '6179'},968:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>From <a href="http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/lect12.pdf" rel="nofollow">Van Emde Boas trees lecture</a>:</p>\n\n<blockquote>\n  <p>We will use the idea of superimposing a tree of degree ${u^{1/2}}$ on top of\n  a bit vector, but <strong>shrink the universe size recursively</strong> by a square\n  root at each tree level. The ${u^{1/2}}$ items on the \ufb01rst level each hold\n  structures of ${u^{1/4}}$ items, which hold structures of ${u^{1/8}}$ items, and\n  so on, down to size 2.\n  I have a  question regarding van Emde Boas trees :</p>\n</blockquote>\n\n<ol>\n<li>How is the universe size getting reduced ? Aren\'t we just spreading the universe keys which is always constant at $u$ to different levels ? I can not understand the idea of "<strong>shriniking</strong>" the universe size .  I find similar language is used in defining the recursive structure for Van Emde Boas tree in <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/" rel="nofollow">Introduction to Algorithms</a> by CLRS also .</li>\n</ol>\n', 'ViewCount': '563', 'Title': 'Explanation of recursive structure of Van Emde Boas Tree', 'LastEditorUserId': '2223', 'LastActivityDate': '2013-01-18T15:27:02.663', 'LastEditDate': '2012-10-22T09:04:38.213', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6197', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><data-structures><algorithm-analysis><search-trees><trees>', 'CreationDate': '2012-10-20T14:15:05.167', 'Id': '6192'},969:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In order to achieve the time complexity of $O(\\log \\log u)$ for van Emde Boas trees I read in <a href="http://fileadmin.cs.lth.se/cs/Personal/Rolf_Karlsson/lect12.pdf" rel="nofollow">this lecture</a> that the the universe size  $u$  is chosen as $u = 2^{2^k}$ for some integer $k$ for van Emde Boas trees. Why choose $u$ to be of this specific form ?</p>\n', 'ViewCount': '116', 'Title': 'Size of the universe for van Emde Boas Trees', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-10-20T14:34:19.040', 'LastEditDate': '2012-10-20T14:34:19.040', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6195', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><data-structures><algorithm-analysis><binary-trees><trees>', 'CreationDate': '2012-10-20T14:22:39.857', 'Id': '6193'},970:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '868', 'Title': 'How can I improve my Algorithm?', 'LastEditDate': '2012-10-23T13:43:40.853', 'AnswerCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4272', 'FavoriteCount': '1', 'Body': '<blockquote>\n  <p>This is a problem from Interview Street in Dynamic Programming section.\n  <a href="https://www.interviewstreet.com/challenges/dashboard/#problem/4f2c2e3780aeb" rel="nofollow">https://www.interviewstreet.com/challenges/dashboard/#problem/4f2c2e3780aeb</a></p>\n  \n  <p>Billboards(20 points)</p>\n  \n  <p>ADZEN is a very popular advertising firm in your city. In every road you can see their advertising billboards. Recently they are facing a serious challenge , MG Road the most used and beautiful road in your city has been almost filled by the billboards and this is having a negative effect on the natural view.</p>\n  \n  <p>On people\'s demand ADZEN has decided to remove some of the billboards in such a way that there are no more than K billboards standing together in any part of the road.</p>\n  \n  <p>You may assume the MG Road to be a straight line with N billboards.Initially there is no gap between any two adjecent billboards.</p>\n  \n  <p>ADZEN\'s primary income comes from these billboards so the billboard removing process has to be done in such a way that the billboards remaining at end should give maximum possible profit among all possible final configurations.Total profit of a configuration is the sum of the profit values of all billboards present in that configuration.</p>\n  \n  <p>Given N,K and the profit value of each of the N billboards, output the maximum profit that can be obtained from the remaining billboards under the conditions given.</p>\n</blockquote>\n\n<pre><code>Constraints\n1 &lt;= N &lt;= 1,00,000(10^5)\n1 &lt;= K &lt;= N\n0 &lt;= profit value of any billboard &lt;= 2,000,000,000(2*10^9)\n</code></pre>\n\n<blockquote>\n  <p>My Solution (Psuedocode):</p>\n</blockquote>\n\n<pre><code>Let Profit[i] denote the Profit from ith billboard.\n(i, j) denotes the range of billboards\nMaxProfit(i, j) for all (i, j) such that i&lt;=j and i-j+1 &lt;= K is:\n    MaxProfit(i, j) = Profit[i] + Profit[i+1] + ... + Profit[j];\n\nFor other (i,j) MaxProfit equals,\n\nMaxProfit(i, j)\n{\n        if(MaxProfit(i, j) is already calculated)\n            then return its value;\n    max = 0;\n    for all k such that i&lt;=k&lt;=j // k denotes that, that position has no   billboard\n    {\n        temp = MaxProfit(i, k-1) + MaxProfit(k+1, j);\n        if(temp &gt; max)\n        max = temp;\n    }\nreturn max;\n}\n</code></pre>\n\n<p>My solution is of order $$N^2$$. So I get TLE and Segmentation fault for larger N. I have already passed 6/10 test cases. I need to pass remaining 4. Help needed.</p>\n', 'Tags': '<algorithms><data-structures><dynamic-programming>', 'LastEditorUserId': '4272', 'LastActivityDate': '2012-10-25T10:57:30.120', 'CommentCount': '10', 'AcceptedAnswerId': '6286', 'CreationDate': '2012-10-21T09:36:39.707', 'Id': '6211'},971:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '217', 'Title': 'Is there a data-structure for semilattices similar to a tree data-structure?', 'LastEditDate': '2013-03-18T23:58:01.183', 'AnswerCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '1557', 'FavoriteCount': '1', 'Body': '<p>If we regard a tree as a partial ordered set, it becomes a special case of a join-semilattice. For a join-semilattice, we want to be able to compute the (unique) least upper bound of two elements (more or less) efficiently. In the case of a tree, a data structure which would enable this would be to store for each element in the corresponding node a pointer to the parent and a distance measure to the root. (Actually, a labeling based on topological sort usually used for "a distance measure to the root", effectively all that is needed is a compatible partial order which can be evaluated efficiently).</p>\n\n<p>Each finite join-semilattice can be represented as a set of subsets of a finite set with containment as order such that the least upper bound is given by the union of the sets. Hence, representing each element by a finite number of tags, and computing the least upper bound by the union of the corresponding tags would be one possible data structure. (By looking at the complement, one sees that defining the least upper bound as the intersection of the corresponding tags would also be possible.) A much more common data-structure is to simply use a matrix to store all results of "a &lt;= b" or even all results of "join(a,b)".</p>\n\n<p>However, using such a data-structure to represent a tree would be sort of strange. Are there more tree-like data-structures for join-semilattices, which still allow (more or less) efficient computation of the (unique) least upper bound of two elements? (Perhaps some sort of directed acyclic graph with additional information in the nodes similar to the distance measure to the root for the tree?)</p>\n', 'Tags': '<data-structures><lattices>', 'LastEditorUserId': '1557', 'LastActivityDate': '2013-03-18T23:58:01.183', 'CommentCount': '1', 'AcceptedAnswerId': '10608', 'CreationDate': '2012-10-22T22:20:31.173', 'Id': '6245'},972:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '3499', 'Title': 'Why is the minimum height of a binary tree $\\log_2(n+1) - 1$?', 'LastEditDate': '2012-10-24T22:18:38.927', 'AnswerCount': '2', 'Score': '3', 'OwnerDisplayName': 'Imray', 'PostTypeId': '1', 'OwnerUserId': '4348', 'FavoriteCount': '0', 'Body': "<p>In my Java class, we are learning about complexity of different types of collections.</p>\n\n<p>Soon we will be discussing binary trees, which I have been reading up on. The book states that the minimum height of a binary tree is $\\log_2(n+1) - 1$, but doesn't offer further explanation.</p>\n\n<p>Can someone explain why?</p>\n", 'Tags': '<data-structures><binary-trees><discrete-mathematics><trees>', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-24T22:18:38.927', 'CommentCount': '1', 'AcceptedAnswerId': '6282', 'CreationDate': '2012-10-23T22:12:56.283', 'Id': '6277'},973:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know that the disjoint set datastructure is used   to keep track of the connected components of an undirected graph when the edges are added to the graph dynamically . I also know that is is used in <a href="http://en.wikipedia.org/wiki/Kruskal%27s_algorithm" rel="nofollow">Kruskal\'s  algorithm for minimum spanning trees</a> . What are the other possible applications of this datastructure ?</p>\n', 'ViewCount': '487', 'Title': 'Practical applications of disjoint set datastructure', 'LastActivityDate': '2012-10-25T21:10:04.077', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6318', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2223', 'Tags': '<algorithms><graph-theory><data-structures><graphs>', 'CreationDate': '2012-10-25T12:40:07.410', 'FavoriteCount': '3', 'Id': '6308'},974:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1759', 'Title': 'Proof that a randomly built binary search tree has logarithmic height', 'LastEditDate': '2012-10-28T11:16:30.370', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '1', 'Body': '<p>How do you prove that the expected height of a randomly built <a href="http://en.wikipedia.org/wiki/Binary_search_tree" rel="nofollow">binary search tree</a> with $n$ nodes is $O(\\log n)$? There is a proof in CLRS <em>Introduction to Algorithms</em> (chapter 12.4), but I don\'t understand it.</p>\n', 'Tags': '<data-structures><algorithm-analysis><binary-trees><search-trees><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T12:56:45.620', 'CommentCount': '4', 'AcceptedAnswerId': '6356', 'CreationDate': '2012-10-27T19:37:43.787', 'Id': '6342'},975:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose I am making a red-black search tree, and in my right subtree, I have a black node, then a red node, and it has two black children, the black children further black childrens. As such a lemma has been made that red-black trees with $n$ internal nodes have height at most $2\\log(n+1)$, would this proof still hold for such a black tree?</p>\n', 'ViewCount': '64', 'Title': 'Can you have three consecutive black nodes in red-black search tree?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-28T10:55:19.397', 'LastEditDate': '2012-10-28T10:55:19.397', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '6344', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<data-structures><binary-trees><search-trees>', 'CreationDate': '2012-10-27T23:52:30.187', 'Id': '6343'},976:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '136', 'Title': "Why can't we use a hash tables for collision resolving in hash tables?", 'LastEditDate': '2012-10-28T10:58:21.593', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '1', 'Body': "<p>To prevent collisions, hash tables with open addressing use a methodology to chain the contents. Why can't we use another hash table allocated to each slot of the primary hash table?</p>\n", 'Tags': '<data-structures><hash-tables>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-02T04:37:25.970', 'CommentCount': '1', 'AcceptedAnswerId': '6413', 'CreationDate': '2012-10-28T03:50:29.827', 'Id': '6348'},977:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>How is $\\frac{n}{2^{h+1}}$ the maximum possible number of nodes at height $h$ for a binary search tree or heap tree? I saw this as proof to asymptotically bound the <code>build_heap</code> function in the book, but I don't get it.</p>\n", 'ViewCount': '1750', 'Title': 'Maximum number of nodes with height h', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-02T08:17:25.400', 'LastEditDate': '2012-10-31T23:12:27.867', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '6412', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2012-10-31T21:24:01.887', 'Id': '6405'},978:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>This is a textbook based question. In The Art of Computer Programming Volume 3, Knuth says that for a hash function $h(k) = k \\bmod M$, $M$ should not be a multiple of $3$.</p>\n\n<p>The explanation given is:</p>\n\n<blockquote>\n  <p>If keys are alphabetic, two keys that differ only by permutation of letters would differ in numeric value by a multiple of 3. This occurs because $2^{2n} \\bmod 3 = 1$ and $10^{n} \\bmod 3 = 1$)."</p>\n</blockquote>\n\n<p>I\'d be grateful, if someone can clarify why this is so, and how that equation is connected to alphabetical keys and $M$ being a power of $3$.</p>\n', 'ViewCount': '159', 'Title': 'Modulo hash function and multiples of three', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-01T23:36:03.757', 'LastEditDate': '2012-11-02T22:14:57.713', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'Jagx Kool', 'PostTypeId': '1', 'Tags': '<data-structures><hash-tables><hash>', 'CreationDate': '2012-10-26T02:36:33.613', 'Id': '6433'},979:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Assume that we have a set $D$ and each member of $D$ is a data and key pair. We want a data structure that would support the following operations:</p>\n\n<ul>\n<li>Insert $(d,k)$ into $D$,</li>\n<li>Delete member $e$, (no need to search to find $e$, e.g. $e$ points to a member in $D$),</li>\n<li>MostFrequent, which returns a member $e \\in D$ such that $e.key$ is one of the most frequent keys in $D$ (note that the most frequent key doesn't need to be unique).</li>\n</ul>\n\n<p>What would be an efficient implementation of this data structure?</p>\n\n<p>My solution is a heap for the keys and their frequencies prioritized by the frequencies plus a hash table where the hash function maps members with the same key to the same slot in the hash table (with pointers from each part to the other). </p>\n\n<p>This can give $\\Theta(\\lg n)$ for the first two operations and $\\Theta(1)$ for the third (worst case running time). </p>\n\n<p>I am wondering if there is more efficient solution? (or a simpler solution with the same efficiency?)</p>\n", 'ViewCount': '870', 'Title': 'An efficient data structure supporting Insert, Delete, and MostFrequent', 'LastEditorUserId': '41', 'LastActivityDate': '2013-01-23T16:20:22.333', 'LastEditDate': '2012-11-02T21:39:10.150', 'AnswerCount': '3', 'CommentCount': '10', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<algorithms><data-structures><abstract-data-types>', 'CreationDate': '2012-11-02T17:00:40.447', 'FavoriteCount': '5', 'Id': '6455'},980:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How many different max-heaps can I form using a list of $n$ integers. </p>\n\n<p>Example: \nlist [1,2,3,4]</p>\n\n<p>and max-heap is <code>4 3 2 1</code>  or </p>\n\n<pre><code>    4\n   / \\\n  3   2\n /\n1\n</code></pre>\n\n<p>other possible max-heap is <code>4 2 3 1</code></p>\n\n<pre><code>    4 \n   / \\\n  2   3 \n /\n1\n</code></pre>\n', 'ViewCount': '569', 'Title': 'How many max heaps are there?', 'LastEditorUserId': '2205', 'LastActivityDate': '2014-01-27T20:52:32.243', 'LastEditDate': '2012-11-20T08:21:15.937', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6458', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '244', 'Tags': '<graph-theory><data-structures><combinatorics><binary-trees><heaps>', 'CreationDate': '2012-11-02T18:19:35.187', 'Id': '6456'},981:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am trying to understand summation for amortization analysis of a hash-table from a <a href="http://videolectures.net/mit6046jf05_leiserson_lec13/" rel="nofollow">MIT lecture video</a> (at time 16:09). </p>\n\n<p>Although you guys don\'t have to go and look at the video, I feel that the summation he does is wrong so I will attach the screenshot of the slide.</p>\n\n<p><img src="http://i.stack.imgur.com/EBRfs.jpg" alt="MIT Lecture Slide"></p>\n', 'ViewCount': '138', 'Title': 'Why is $\\sum_{j=0}^{\\lfloor\\log (n-1)\\rfloor}2^j$ in $\\Theta (n)$?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-05T17:15:47.033', 'LastEditDate': '2012-11-05T17:15:47.033', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '6474', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<algorithms><data-structures><algorithm-analysis><mathematical-analysis><discrete-mathematics>', 'CreationDate': '2012-11-04T16:32:11.537', 'Id': '6473'},982:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A binary counter is represented by an infinite array of 0 and 1.</p>\n\n<p>I need to implement the action $\\text{add}(k)$ which adds $k$ to the value represented in the array.</p>\n\n<p>The obvious way is to add 1, k times. Is there a more efficient way?</p>\n', 'ViewCount': '214', 'Title': 'Implementing addition for a binary counter', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-12T11:35:22.097', 'LastEditDate': '2012-11-12T11:35:22.097', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6628', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4561', 'Tags': '<algorithms><data-structures><efficiency>', 'CreationDate': '2012-11-12T08:21:57.977', 'Id': '6627'},983:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose a <strong>build</strong> max-heap operation runs bubble down over a heap. How does its amortized cost equal $O(n)$?</p>\n', 'ViewCount': '1518', 'Title': "How can I prove that a build max heap's amortized cost is $O(n)$?", 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-18T07:58:21.743', 'LastEditDate': '2012-11-18T07:58:21.743', 'AnswerCount': '2', 'CommentCount': '4', 'AcceptedAnswerId': '6659', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4193', 'Tags': '<data-structures><runtime-analysis><heaps>', 'CreationDate': '2012-11-14T03:58:41.463', 'Id': '6655'},984:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>We can consider a <a href="http://en.wikipedia.org/wiki/Residue_number_system" rel="nofollow">Residue Number System (R.N.S.)</a> that has efficient operations for add, multiply, and compare between two numbers.</p>\n\n<p><strong>A more rigorous definition</strong></p>\n\n<p>We can suppose that we can perform the operations above between two non-negative integers $x$ and $y$ with $x \\ge y$ in time proportional to $O(x\\log(x))$ with $O(\\log(x^2))$ memory.  Or further, if it would possibly allow even more uses, if we could perform these operations in time linear with respect to $x$ and $y$.</p>\n\n<p><strong>What are the potential uses of this system?</strong></p>\n\n<p>For example, cryptology often uses fairly large numbers, and makes use of such a system.  Also, linear programming and systems of linear equations can make use of an efficient R.N.S.  Also, specialized processors can have R.N.S.\'s built in, and could potentially compete with standarized Boolean processors.</p>\n\n<p><strong>What additional uses would this efficient system allow?</strong></p>\n', 'ViewCount': '51', 'Title': 'What are the potential uses of a good R.N.S. system?', 'LastActivityDate': '2012-11-14T20:51:01.067', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1667', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2012-11-14T20:51:01.067', 'FavoriteCount': '1', 'Id': '6667'},985:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I thought associative array (i.e. map, or dictionary) and hashing table were the same concept, until I saw in <a href="http://en.wikipedia.org/wiki/Abstract_data_type#Implementation">Wikipedia</a> that</p>\n\n<blockquote>\n  <p>For dictionaries with very small numbers of bindings, it may make\n  sense to implement the dictionary using an association list, a linked\n  list of bindings. ...</p>\n  \n  <p>The most frequently used general purpose implementation of an\n  associative array is with a hash table: an array of bindings, together\n  with a hash function that maps each possible key into an array index.\n  ...</p>\n  \n  <p>Dictionaries may also be stored in binary search trees or in data\n  structures specialized to a particular type of keys such as radix\n  trees, tries, Judy arrays, or van Emde Boas trees. ...</p>\n</blockquote>\n\n<p>So I think my problem lies in that I don\'t know that associative array (i.e. map, or dictionary) is an abstract data type and hashing table is a concrete data structure, and different concrete data structures can be used to implement the same abstract data type. So my questions would be</p>\n\n<ul>\n<li><p>what are the difference and relation between abstract data structures and concrete data structures?</p></li>\n<li><p>what examples are for each of them (abstract and concrete data structures)? The more the better.</p></li>\n<li><p>Is there a list of what concrete data structures can be used to implement what abstract data structures? It would be nice to have one.</p></li>\n</ul>\n\n<p>Thanks!</p>\n', 'ViewCount': '1153', 'Title': 'Relation and difference between associative array and hashing table?', 'LastEditorUserId': '336', 'LastActivityDate': '2012-11-15T20:01:05.607', 'LastEditDate': '2012-11-15T13:55:07.643', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '6687', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<data-structures>', 'CreationDate': '2012-11-15T13:22:07.473', 'Id': '6678'},986:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a <strong>max heap</strong> with <strong>extract-max</strong> operation.</p>\n\n<p>The basic version takes $2 \\log n$ steps.\nHow can I make it in $\\log n + \\log\\log n$\nand how in $\\log n + \\log\\log\\log n $?</p>\n\n<p>I thought of putting $-\\infty$ on the heap root but not really sure what to do with it as it can go anywhere.</p>\n\n<p>To be more precise, steps are considered only compares between array item values. I\'m reading <a href="http://en.wikipedia.org/wiki/Introduction_to_Algorithms" rel="nofollow">CLRS</a> Chapter 6 (<strong>MAX-HEAPIFY</strong> and <strong>HEAP-EXTRACT-MAX</strong>).</p>\n', 'ViewCount': '341', 'Title': 'Extract Max for a max-heap in $\\log n + \\log\\log n$', 'LastEditorUserId': '472', 'LastActivityDate': '2013-05-24T03:22:14.673', 'LastEditDate': '2013-05-24T03:22:14.673', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4618', 'Tags': '<data-structures><heaps>', 'CreationDate': '2012-11-16T19:40:15.313', 'FavoriteCount': '1', 'Id': '6698'},987:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a contiguous ordered data structure (0 based index): </p>\n\n<pre><code>x= [1/3, 1/3, 1/3]\n</code></pre>\n\n<p>Let's say I selected index 1 and increased the probability by 1/3. Rest of the probabilities each decrease by 1/6 and the total probability remains P = 1.</p>\n\n<pre><code>x= [1/6, 2/3, 1/6]\n</code></pre>\n\n<p>Let's say I selected index 2 and increased the probability by 1/3. Rest of the probabilities in total need to decrease by 1/3 to make the total probability remain P= 1.</p>\n\n<pre><code>x= [1/10, 2/5, 1/2]\n</code></pre>\n\n<p>Is there a name for this kind of data structure? I'd like to research that name and use a library instead of my custom rolled code if possible.</p>\n", 'ViewCount': '98', 'Title': 'probability wheel, redistribution of probabilities', 'LastEditorUserId': '4640', 'LastActivityDate': '2012-11-18T20:08:04.633', 'LastEditDate': '2012-11-18T16:00:11.837', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '6745', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4640', 'Tags': '<data-structures><probability-theory>', 'CreationDate': '2012-11-18T14:39:44.920', 'Id': '6741'},988:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1104', 'Title': u"Modifying Dijkstra's algorithm for edge weights drawn from range $[1,\u2026,K]$", 'LastEditDate': '2012-11-21T08:01:32.647', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '2', 'Body': '<p>Suppose I have a directed graph with edge weights drawn from range $[1,\\dots, K]$ where $K$ is constant. If I\'m trying to find the shortest path using <a href="http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="nofollow">Dijkstra\'s algorithm</a>, how can I modify the algorithm / data structure and improve the time complexity to $O(|V|+|E|)$?</p>\n', 'Tags': '<algorithms><data-structures><shortest-path><weighted-graphs>', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-21T20:43:49.977', 'CommentCount': '6', 'AcceptedAnswerId': '6820', 'CreationDate': '2012-11-21T03:08:52.193', 'Id': '6797'},989:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know the term <strong>order</strong> of a B-tree. Recently I heard a new term  <strong>B tree with minimum degree of 2.</strong><br>\nWe know the degree is related with a node but what is degree of a tree.<br>\nIs degree imposes any kind of a restriction on height of a B-tree?  </p>\n', 'ViewCount': '1586', 'Title': 'B-Tree Is degree and order both are the same thing related to a B-Tree', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-21T10:30:15.287', 'LastEditDate': '2014-02-21T10:30:15.287', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '3075', 'Tags': '<terminology><data-structures><search-trees><dictionaries>', 'CreationDate': '2012-11-21T05:17:52.773', 'FavoriteCount': '1', 'Id': '6799'},990:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For what size alphabet does it take longer to construct <a href="http://en.wikipedia.org/wiki/Suffix_tree" rel="nofollow">a suffix tree</a> - for a really small alphabet size (because it has to go deep into the tree) or for a large alphabet size? Or is it dependent on the algorithm you use? If it is dependent, how does the alphabet size affect <a href="http://en.wikipedia.org/wiki/Ukkonen%27s_algorithm" rel="nofollow">Ukkonen\'s algorithm</a>?</p>\n', 'ViewCount': '88', 'Title': 'What are the effects of the alphabet size on construct algorithms for suffix trees?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-23T09:43:36.243', 'LastEditDate': '2012-11-23T09:30:49.477', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4696', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings><efficiency>', 'CreationDate': '2012-11-22T22:14:10.630', 'Id': '6842'},991:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Aren't there $n^2$ unique substrings of a string (irrespective of the alphabet size)? Perhaps the number of unique <em>suffix substrings</em> is less than the number of unique <em>substrings</em> of a string.</p>\n", 'ViewCount': '272', 'Title': 'Why does a suffix tree have a linear number of nodes (relative to input string size)?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-11-29T10:50:42.293', 'LastEditDate': '2012-11-27T07:47:22.710', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '6945', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '4748', 'Tags': '<data-structures><strings><trees>', 'CreationDate': '2012-11-27T05:43:15.827', 'Id': '6943'},992:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Data structures are seen as important, equal to algorithms. This view is especially encouraged in situations, where appropriate data structure is the main factor that allows an algorithm to exist and to perform at satisfying complexity.</p>\n\n<p>However, despite the additional features of data structures (e.g. organization, like in trees storing entries according to less-like relation), all that data structures do is keeping information unchanged between algorithm's actions. Can this generic feature of data structures be izolated and defined in abstract way?</p>\n", 'ViewCount': '132', 'Title': 'Generalized data structure', 'LastActivityDate': '2012-11-30T07:13:01.040', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4773', 'Tags': '<algorithms><data-structures><information-theory>', 'CreationDate': '2012-11-28T04:32:26.293', 'Id': '6980'},993:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I think I\'m confused about deletion in heaps, and since I have an exam today, I\'m looking for your help to correct me.</p>\n\n<p>I will post photos since it will makes it a bit more clear.</p>\n\n<p>Note(forget about deleting the root)</p>\n\n<p><img src="http://i.stack.imgur.com/USyPs.png" alt="enter image description here"></p>\n\n<p>What I understand is that , heaps only deletes the root element or the top. So, I made 2 solutions and I\'m kindly asking which solution is the correct one.</p>\n\n<p><img src="http://i.stack.imgur.com/9sKUA.png" alt="enter image description here"></p>\n\n<p>Q:the heapness will be violated, I will have to replace it by the rightmost element bottom down right? (32)\n<img src="http://i.stack.imgur.com/SX3Kc.png" alt="enter image description here"></p>\n', 'ViewCount': '2434', 'Title': 'Deletion in min/max heaps', 'LastActivityDate': '2013-05-22T20:40:52.947', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '7000', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4781', 'Tags': '<data-structures><heaps>', 'CreationDate': '2012-11-28T09:41:54.687', 'Id': '6990'},994:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '326', 'Title': 'Determine whether the $k^{th}$ smallest element in max-heap is greater than a given number', 'LastEditDate': '2012-12-06T06:00:55.747', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4878', 'FavoriteCount': '1', 'Body': '<p>A set of numbers is stored in a <strong>max-heap</strong>. We want to find an algorithm with $O(k)$ time complexity to check if $k^{th}$ <strong>smallest</strong> element is greater than an arbitrary given number.</p>\n', 'Tags': '<algorithms><data-structures><heaps>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-24T13:40:07.560', 'CommentCount': '11', 'AcceptedAnswerId': '7660', 'CreationDate': '2012-12-05T09:27:05.190', 'Id': '7173'},995:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '904', 'Title': 'Retrieving the shortest path of a dynamic graph', 'LastEditDate': '2012-12-10T15:45:53.627', 'AnswerCount': '3', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '4916', 'FavoriteCount': '3', 'Body': "<p>I'm studying shortest paths in directed graphs currently. There are many efficient algorithms for finding the shortest path in a network, like dijkstra's or bellman-ford's. But what if the graph is dynamic? By saying dynamic I mean that we can insert or remove vertices during the execution of the program. I'm trying to find an efficient algorithm for updating the shortest paths from a vertex $v$ to every other vertex $u$, after inserting an edge $e$, without needing to run the shortest path algorithm in the new graph again. How can I do this? Thanks in advance.</p>\n\n<ul>\n<li><em>Note:</em> the changes can be done after the first iteration of the algorithm</li>\n<li><em>Note[2]:</em> two nodes are given, $s$ the source and $t$ the target. I need to find the shortest path between these nodes. When the graph is updated I only have to update $\\pi(s,t)$, which is the shortest path between $s$ and $t$.</li>\n<li><em>Note[3]:</em> I'm only interested in the edge insertion case.</li>\n</ul>\n\n<blockquote>\n  <p><strong>A formal definition</strong>: Given a graph $G = (V,E)$. Define an <em>update operation</em> as 1) an insertion of an edge $e$ to $E$ or 2) a a deletion of an edge $e$ from $E$. The objective is to find efficiently the cost of all pairs shortest paths after an update operation. By efficiently, we mean at least better than executing an All-Pairs-Shortest-Path algorithm, such as Bellman-Ford algorithm, after each update operation.</p>\n</blockquote>\n\n<hr>\n\n<p><strong>Edit:</strong> Below there is a simplified version of the problem:</p>\n\n<blockquote>\n  <p>A weighted graph $G(V,E)$ is given, consisting of unidirectional edges, and two critical vertices $s$ and $t$. A set $C$ of candidate <em>bidirectional</em> edges is also given. I have to build an edge $(u,v) \\in C$ to minimize the distance from $s$ to $t$.</p>\n</blockquote>\n", 'Tags': '<algorithms><data-structures><graphs><efficiency><shortest-path>', 'LastEditorUserId': '4916', 'LastActivityDate': '2013-06-12T10:15:55.483', 'CommentCount': '7', 'AcceptedAnswerId': '7260', 'CreationDate': '2012-12-08T11:22:28.103', 'Id': '7250'},996:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><img src="http://i.stack.imgur.com/XeuwO.gif" alt="hexagon coordinates"></p>\n\n<p>In my program, it draws them by offsetting every other row by half of the width, as pictured above. Each tile can be referenced by coordinates, also shown above.</p>\n\n<p><img src="http://i.stack.imgur.com/DKyZq.gif" alt="hexagon roads"></p>\n\n<p>I want to know how many blue tiles are accessible from a certain starting tile and a series of "roads." In this example, three blue tiles can be reached. </p>\n\n<p>How would I represent roads? This is what I would need to know about it:</p>\n\n<ul>\n<li>how it is rotated (for drawing)</li>\n<li>which hexagons it borders</li>\n<li>which other roads it touches, if any</li>\n<li>where it should be drawn</li>\n</ul>\n\n<p>I assume I could use recursion to to count the accessible blue squares if the above conditions are met.</p>\n', 'ViewCount': '313', 'Title': 'How do I structure hexagon edge data?', 'LastActivityDate': '2012-12-13T04:27:27.487', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '7368', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '246', 'Tags': '<data-structures><graph-traversal>', 'CreationDate': '2012-12-09T02:41:43.050', 'FavoriteCount': '4', 'Id': '7261'},997:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Count-Min_sketch">Count-Min Sketch</a> is an awesome data structure for estimating the frequencies of different elements in a data stream.  Intuitively, it works by picking a variety of hash functions, hashing each element with those hash functions, and incrementing the frequencies of various slots in various tables.  To estimate the frequency of an element, the Count-Min sketch applies the hash functions to those elements and takes the minimum value out of all the slots that are hashed to.</p>\n\n<p>The <a href="http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf">original paper on the Count-Min Sketch</a> mentions that the data structure requires pairwise independent hash functions in order to get the necessary guarantees on its expected performance.  However, looking over the structure, I don\'t see why pairwise independence is necessary.  Intuitively, I would think that all that would be required would be that the hash function be <a href="http://en.wikipedia.org/wiki/Universal_hashing">a universal hash function</a>, since universal hash functions are hash functions with low probabilities of collisions.  The analysis of the collision probabilities in the Count-Min Sketch looks remarkably similar to the analysis of collision probabilities in a chained hash table (which only requires a family of universal hash functions, not pairwise independent hash functions), and I can\'t spot the difference in the analyses.</p>\n\n<p>Why is it necessary for the hash functions in the Count-Min Sketch to be pairwise independent?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '467', 'Title': 'Why does the Count-Min Sketch require pairwise independent hash functions?', 'LastActivityDate': '2012-12-09T21:05:41.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7279', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><data-structures><randomized-algorithms><hash>', 'CreationDate': '2012-12-09T19:52:37.693', 'Id': '7275'},998:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>A BTree has a $k$ value that determines that every node has $k$ to $2k$ children. When a node has $2k$ keys it needs to be split into two nodes.</p>\n\n<p>Let's say I want to create a $k/(2k-x)$ tree. (like a 2-3 tree that follows the BTree rules, but isnt $k/2k$)</p>\n\n<p>What is the possible range of $x$? The tree must follow the behavior of a BTree, meaning it needs to hold keys and split accordingly.</p>\n", 'ViewCount': '188', 'Title': 'B-tree branching factor boundaries', 'LastEditorUserId': '39', 'LastActivityDate': '2012-12-16T21:30:40.277', 'LastEditDate': '2012-12-16T21:30:40.277', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '5078', 'Tags': '<data-structures><search-trees>', 'CreationDate': '2012-12-16T03:55:01.130', 'Id': '7428'},999:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a data structure described as following:</p>\n\n<pre><code>- It\'s a collection of trees.\n- Each tree has the same structure.\n- Each tree has information of diferent nature.\n</code></pre>\n\n<p>A example of this data structure:</p>\n\n<pre><code>    4 - - - - - - - \'a\' - - - - - - - 3.5 \n   / \\              / \\               / \\\n  3   1           \'f\' \'y\'           1.0 3.1\n     / \\              / \\               / \\\n    4   7           \'e\' \'f\'           2.3 7.7\n</code></pre>\n\n<p>If you see, ignoring the contents of each tree, each of them is just the same tree (the same hierarchy, the same structure). The first contains natural numbers, the second one, characters, and the third one, floating numbers.</p>\n\n<p>The idea of this data structure is that diferent classes (in a C++ program for example) explore diferent "layers" of this tree, in order to each class uses only the information this class needs, ignoring the remaining information. In other words, each class sees only one tree.</p>\n\n<p>Does this data structure match with a <a href="http://en.wikipedia.org/wiki/Hypertree" rel="nofollow">hypertree</a> or can it be reformulated/adapted to a hypertree? Or just they are isomorphic trees put together is a same data structure?</p>\n', 'ViewCount': '58', 'Title': 'Is this data structure a hypertree or only isomorphic trees?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-12-19T01:15:36.680', 'LastEditDate': '2012-12-18T23:48:05.767', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'Peregring-lk', 'PostTypeId': '1', 'OwnerUserId': '4675', 'Tags': '<data-structures><trees>', 'CreationDate': '2012-12-11T12:48:05.580', 'Id': '7448'},9100:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a DAG representing strict partial order where each node is an assignment of variables $V$ to their values $v$. Each arc $(u,w)$ represents a change in one variable value such that $u\\succ w$. </p>\n\n<p>So if I have $n$ binary variables, I will end up with $2^n$ nodes which is exponential to the size of variables $n$. is there any method to store such DAG efficiently? </p>\n', 'ViewCount': '119', 'Title': 'Is there an efficient method to store large DAGs?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-18T18:31:40.867', 'LastEditDate': '2012-12-18T16:32:46.107', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4598', 'Tags': '<algorithms><graph-theory><data-structures><partial-order>', 'CreationDate': '2012-12-18T04:30:32.000', 'Id': '7484'},9101:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In <a href="http://en.wikipedia.org/wiki/Fibonacci_heap" rel="nofollow">Fibonacci heaps</a>, we keep a mark field for every node in the heap. Initially all the nodes are unmarked. Once a node is deleted, its parent is marked. If a node is deleted and its parent is already marked, the parent will be cut and inserted into the root list. I just wonder what the purpose of marking is? What happens if we cut the parent just the first time without marking it? Does it change the time complexity of the operations? How?</p>\n', 'ViewCount': '275', 'Title': 'What is the purpose of Mark field in Fibonacci Heaps?', 'LastEditorUserId': '2205', 'LastActivityDate': '2012-12-19T09:58:56.557', 'LastEditDate': '2012-12-19T09:58:56.557', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7510', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '1672', 'Tags': '<data-structures><heaps><amortized-analysis>', 'CreationDate': '2012-12-19T08:11:04.093', 'Id': '7505'},9102:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>For 2D polygonal meshes, the QuadEdge and HalfEdge data structure representations are sufficient to store and enable efficient query of all topological and incidence information. Are there compact and efficient data structures for 3D polyhedral meshes? I know there has been some recent work on compact representations for tetrahedral meshes, like, for example <a href="http://www.gvu.gatech.edu/people/official/jarek/papers/sot.pdf" rel="nofollow">SOT</a>. I don\'t know enough about these to know if they generalize to unstructured non-tetrahedral meshes.</p>\n\n<p>I can imagine that half-edges might generalize to half-faces with associated half-edges, but it seems like that is a lot of data to store, and there might be more compact representations. I should add that I really only care about retrieving facet information (like which facets are on the boundary, which facets belong to a certain cell); the edge incidence information is not as useful.</p>\n', 'ViewCount': '97', 'Title': 'Data structures for general (non-tetrahedral) cell complexes', 'LastActivityDate': '2012-12-20T17:58:45.383', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '5126', 'Tags': '<data-structures><computational-geometry>', 'CreationDate': '2012-12-19T09:28:14.227', 'FavoriteCount': '0', 'Id': '7509'},9103:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In a bridge game, a deck of 52 cards (13 spades , 13 clubs, 13 diamonds, 13 spades) are dealt to 4 players (13 cards each) then game starts.Game session ends after 13 tricks each having 4 cards.There are 28561 possible non repeated 4 card groups.</p>\n\n<p>What is the best method of generating and storing all possible trick combinations (played according to the bridge rules just for one session) to be further processed by a computer program (i.e., data structures such as game trees, and any open source algorithms written for any computer language if any).</p>\n\n<p>All resources to read to get the theory behind or any references are welcome.\nThank you.</p>\n', 'ViewCount': '407', 'Title': 'Data structures and algorithms for bridge game play?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-30T09:54:03.547', 'LastEditDate': '2013-04-04T06:33:12.213', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5210', 'Tags': '<algorithms><data-structures><combinatorics><board-games>', 'CreationDate': '2012-12-27T12:12:28.953', 'Id': '7618'},9104:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>my question is simple.\nIs it possible to use plants as a medium to store data?</p>\n\n<p>My opinion is: Possible, but we need to solve, how to distinguish 2 states. \nDuplication and CRC of stored DATA is quiet simple. Growing new plants is easiest possibility, but we need to create unit, which will take care about raid and duplicating informations between all plants.</p>\n\n<p>Could there be a different data-structure? What is a potential of storing data into plants? What about security? How would you write data into plants without losing that information?</p>\n\n<p>No, this is not a joke. Think about it. It grows everywhere, it is protected against water, cold, wet, dry, magnetic field, shaking and so on.</p>\n\n<p>Try to remember, you can use your own garden as a datastore for your music, films and data. Or just use public garden to share media, photographs, messages or opinions by connecting remotely to plants.</p>\n', 'ViewCount': '154', 'Title': 'Is it possible to use plants as a medium to store data? By what data structure?', 'LastActivityDate': '2013-01-11T13:57:43.113', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7639', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '5227', 'Tags': '<data-structures><graphs>', 'CreationDate': '2012-12-29T01:30:59.710', 'Id': '7637'},9105:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the following problem:</p>\n\n<p>Let $k$ be a constant. We are given a $k$-ary array $A_{d_1\\times\\ldots\\times d_k}$ of $0$ and $1$\'s. Let $N = \\prod_{i=1}^k d_i$. </p>\n\n<p>We want to create a data structure by preprocessing $A$ to perform the following type of query operations:</p>\n\n<ol>\n<li>Given the coordinates of a $k$-ary box $D$, is there a $1$ in the box?  </li>\n<li>Given the coordinates of a $k$-ary box $D$, return the position of a $1$ in the box (if there is one). </li>\n</ol>\n\n<p>The operations must be performed in constant time $O(1)$. The time complexity is measured on a RAM machine. The preprocessing time and space for the data structure are not important for us. </p>\n\n<p>The question is how much space (in bit complexity) do we need to store a datastructure allowing the above operations?</p>\n\n<p>The trivial lower-bound is $N$ bits since the array can be reconstructed for these queries (so the data structure should have at least the same amount of information in it).</p>\n\n<p>The trivial upper-bound is to store the answer to all of the queries. That would need $\\prod_{i=1}^k {d_i \\choose 2} = \\Theta(N^2)$ bits. However we suspect that this can be done much more efficiently.</p>\n\n<p>For example, consider the special case where $k=1$. In this case we can use a <a href="http://link.springer.com/chapter/10.1007/978-3-540-74450-4_41" rel="nofollow">succinct RMQ data structure</a> to solve the first problem, and the data structure takes $2N+o(N)$ bits to store.</p>\n\n<blockquote>\n  <p>What is an efficient data-structure for this task?<br>\n  How low can the space complexity (the number of bits) go to support these operations (or just the first operation)?</p>\n</blockquote>\n\n<p><strong>Update (1/15):</strong>\nIn the special case $k=1$, using $N +o(N)$ bits is sufficient (actually better, $\\log {N\\choose t}+O(t)$, where $t$ is the number of $1$\'s in $A$) by reducing the problem to a predecessor problem and using the reduction from predecessor problem to fully indexable dictionary (FID). See "<a href="http://arxiv.org/abs/0902.2648" rel="nofollow">More Haste, Less Waste: Lowering the Redundancy in Fully Indexable Dictionaries</a>" by Grossi, Orlandi, Raman and Rao (2009).</p>\n\n<p><strong>Update (6/27):</strong>\nAgain by reduce the problem to RMQ. We use a $k$-dimensional RMQ by <a href="https://www.siam.org/proceedings//soda/2010/SODA10_014_yuanh.pdf" rel="nofollow">Yuan and Atallah</a> to get a $O(n\\log n)$ upper bound on the amount of space required when $k$ is fixed. </p>\n', 'ViewCount': '316', 'Title': 'Bit complexity of O(1) time range query in a $k$-ary array', 'LastEditorUserId': '220', 'LastActivityDate': '2013-06-27T23:36:27.377', 'LastEditDate': '2013-06-27T23:36:27.377', 'AnswerCount': '1', 'CommentCount': '9', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<reference-request><data-structures><space-complexity>', 'CreationDate': '2013-01-10T20:52:45.990', 'FavoriteCount': '3', 'Id': '7876'},9106:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a some objects with priority that is compound type and is only <a href="http://en.wikipedia.org/wiki/Partial_ordering" rel="nofollow">partially ordered</a>. I need to select the objects in order of this priority (i.e. yield <em>minimal</em> item each time). But rather than arbitrarily completing the order, I would prefer if the queue was stable in a sense that if there is more than one minimal element, it should return the oldest first.</p>\n\n<p>Is there any heap data structure that would work with partial ordering? Or a modification of regular priority queue to work with it? Common choice for the algorithm I need is simple binary or 4-ary heap, but that does not work with partial ordering.</p>\n\n<p>The priority values support:</p>\n\n<ol>\n<li>Partial ordering using operation $\\preccurlyeq$. It\'s partial ordering, so it\'s possible that $a \\preccurlyeq b$ is false and $b \\preccurlyeq a$ is also false. I write $a \\not\\lesseqgtr b$ in that case.</li>\n<li>Finding <a href="http://en.wikipedia.org/wiki/Infimum" rel="nofollow">infima</a> (glb) and suprema (lub). $\\inf(x_i)$ is the maximal $y$ such that $y \\preccurlyeq x_i$. Calculating the infimum of $n$ values takes $O(n)$ time. <strong>Infimum</strong> (and supremum) <strong>of every set exists.</strong></li>\n<li>A linear extension for the partial ordering could be defined. Using it for the priority queue is the easy way out as the algorithm does work that way. But the order affects performance and the order of insertion looks like it should be best in avoiding worst cases.</li>\n</ol>\n\n<p>Additionally the algorithm that I want to use this in needs to know infimum of all priorities in the queue.</p>\n\n<p>The priorities have some real-world meaning, but are subject to change, so it does not seem viable to rely on other properties they could have.</p>\n\n<hr>\n\n<p>Note: Binary heaps don\'t work with partial ordering. Assume a binary heap with $a$, $b$ and $c$, where $a \\preccurlyeq c$ and $a \\not\\lesseqgtr b$ and $a \\not\\lesseqgtr c$. They are positioned in that order, so</p>\n\n<pre><code>     a (0)\n   /   \\\n b (1)   c (2)\n</code></pre>\n\n<p>now <em>d</em> is inserted. Next free position is 3, the left child of $b$, so we get</p>\n\n<pre><code>        a (0)\n      /   \\\n    b (1)   c (2)\n  /\nd (3)\n</code></pre>\n\n<p>If $d \\preccurlyeq a$ (which implies $d \\preccurlyeq c$ from transitivity, but does not say anything about $d$ and $b$) and $d \\not\\lesseqgtr b$, then $d$ does not get swapped with $b$, because it\'s not less. But it actually is less than $a$, but it\'s not compared with it, so now the main heap invariant does not hold; top is not minimal.</p>\n\n<p>I suspect a forest of heaps somewhat in style of binomial heap could be made to work.  Basically it\'s important to always compare new values with root and only link together comparable elements. It would make the trees in the forest randomly sized and thus make the complexity dependent on number of mutually incomparable sets in the heap. I somewhat suspect the complexity can\'t be fixed (we have to keep comparing until we hit a comparable element) I might have missed something, so I am leaving this open.</p>\n\n<hr>\n\n<p>Note: The ordering is <em>partial</em> and while there are ways to define a linear extensions for it, adding a timestamp and using it as secondary criterion is <em>not</em> one of them. Suppose we assigned the timestamp $t(a)$ for each $a$ and defined the ordering $\\preccurlyeq\'$ as $a \\preccurlyeq\' b$ iff $a \\preccurlyeq b$ or\n($b \\not\\preccurlyeq a$ and $t(a) \\le t(b)$.\nThen suppose we have distinct $a$, $b$, $c$, such that $t(a) \\le t(b) \\le t(c)$ and $c \\le a$. Then $a \\preccurlyeq\' b$ and $b \\preccurlyeq\' c$, but $c \\preccurlyeq\' a$, so the relation is not transitive and therefore is not an ordering at all. This kind of extending only works for weak orderings, but not partial ones.</p>\n\n<hr>\n\n<p><strong>Edit:</strong> I realized that not only is infimum of any set defined, but I actually need to be able to get infimum of elements currently in the queue efficiently. So I am now contemplating whether adding special nodes containing infima of subtrees to some common heap structure would help.</p>\n', 'ViewCount': '248', 'Title': 'Priority queue for partially ordered priorities with infima', 'LastEditorUserId': '5417', 'LastActivityDate': '2013-11-22T02:40:43.357', 'LastEditDate': '2013-01-25T12:46:47.207', 'AnswerCount': '6', 'CommentCount': '6', 'Score': '13', 'OwnerDisplayName': 'Jan Hudec', 'PostTypeId': '1', 'OwnerUserId': '5417', 'Tags': '<data-structures><priority-queues>', 'CreationDate': '2013-01-09T07:52:10.410', 'Id': '7890'},9107:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need help figuring the potential function for a max heap so that extract max is completed in $O(1)$ amortised time. I should add that I do not have a good understanding of the potential method. </p>\n\n<p>I know that the insert function should "pay" more in order to reduce the cost of the extraction, and this has to be in regards to the height of the heap (if $ \\lfloor \\log(n) \\rfloor $ gives the height of the heap should the insert be $2\\log(n)$ or $ \\sum_{k=1}^n 2\\log(k) $)</p>\n', 'ViewCount': '783', 'Title': 'Potential function binary heap extract max O(1)', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-14T20:34:01.887', 'LastEditDate': '2013-01-14T20:34:01.887', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '7901', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5384', 'Tags': '<data-structures><runtime-analysis><heaps><amortized-analysis>', 'CreationDate': '2013-01-11T23:41:04.057', 'Id': '7894'},9108:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to do the exercise 2.12 of the book Essential of programing languages 3rd edition.</p>\n\n<p>They ask me to do a procedural representation for a stack, like they did in the example of page 40 with enviroment.</p>\n\n<p>This is the example.</p>\n\n<pre><code>(define empty-env\n   (lambda ()\n   (lambda (search-var)\n      (report-no-binding-found search-var))\n   )\n)\n\n\n(define extend-env\n   (lambda (saved-var saved-val saved-env)\n   (lambda (search-var)\n   (if (eqv? search-var saved-var)\n       saved-val\n       (apply-env saved-env search-var)\n    )\n   ))\n )\n\n\n\n (define apply-env\n    (lambda (env search-var)\n    (env search-var))\n  )\n</code></pre>\n\n<p>In the exercise they ask me for the next procedures:</p>\n\n<pre><code>push, pop, top, and empty-stack?\n</code></pre>\n\n<p>I think I did the push, top and empty-stack. But I can't figure out how to do the pop.</p>\n\n<p>Thank you.</p>\n", 'ViewCount': '104', 'Title': 'procedural representation for the stack? (LIFO structure)', 'LastEditorUserId': '867', 'LastActivityDate': '2013-01-15T02:53:56.813', 'LastEditDate': '2013-01-15T02:53:56.813', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2076', 'Tags': '<data-structures><stack>', 'CreationDate': '2013-01-15T00:54:29.453', 'Id': '8939'},9109:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am researching collaborative editing systems for some work, but so far my search is turning up blank.</p>\n\n<p>Collaborative real-time editing systems almost all have features like:</p>\n\n<ul>\n<li>Many users can edit the document at the same time</li>\n<li>All changes are dated and saved in a linear revision history</li>\n<li>Edits are visible in semi-real time</li>\n<li>Who-wrote-what is tracked for all content</li>\n</ul>\n\n<p>I cannot think of a data- and procedure model to encompass all of these without huge inefficiencies, for example:</p>\n\n<ul>\n<li>Take user input as a few typestrokes at a time, thus enabling real-time visibility but with horrible server loads.</li>\n<li>Take user input as element changes, thus disabling real-time visibility and complicating many-user-editing of one paragraph.</li>\n<li>Edit a XML Document Model Object in place responding to inputs, but making it difficult to version/revision track.</li>\n<li>Edit linear text stored in an appropriate editing structure responding to inputs, making it difficult to verify that correct markup is being generated.</li>\n<li>Store revision history accumulating, requiring computational resources but easing on space.</li>\n<li>Store revision history whole, requiring space but easing on computation.</li>\n</ul>\n\n<p>Am I off track? Does there exist good solutions to these problems? What, if any, is the relevant litterature?</p>\n', 'ViewCount': '116', 'Title': 'What sort of algorithm/communication model/data structure do collaborative real time editors use?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-17T17:26:32.057', 'LastEditDate': '2013-01-15T14:13:45.613', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6428', 'Tags': '<data-structures><efficiency>', 'CreationDate': '2013-01-15T13:32:56.770', 'FavoriteCount': '2', 'Id': '8946'},9110:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was studying binomial heaps and its time analysis. Are there any inputs that cause DELETE-MIN, DECREASE-KEY, and DELETE to run in $\\Omega(\\log n)$ time for a binomial heap rather than $O(\\log n)$?</p>\n', 'ViewCount': '243', 'Title': 'Input that causes an operation on a binomial heap to run in $\\Omega(\\log n)$ time?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-01-16T08:30:16.393', 'LastEditDate': '2013-01-16T08:30:16.393', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6438', 'Tags': '<algorithms><data-structures><lower-bounds><heaps>', 'CreationDate': '2013-01-16T02:01:40.263', 'Id': '8955'},9111:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <a href="http://en.wikipedia.org/wiki/XOR_linked_list" rel="nofollow">XOR linked list</a> is perhaps the most prominent example of storing a reversible hash of two values and using a known value and the stored hash value to derive the other value.  Is there a term for the general mechanism (or a way to describe it very concisely)?</p>\n\n<p>As a related question, are there other somewhat recognized examples of using this mechanism?</p>\n\n<p>In informally studying computer architecture, I have encountered what I think are two or three examples.  One was a suggestion for a MRU-based cache way predictor by XORing the MRU bit with a bit derived from the address; the derived bit is the "known" value.  The other was a similar, XORing the hysteresis bit of a branch predictor with a bit derived from branch information.  A possible third example might be the agree branch predictor which uses a (possibly static) per-branch prediction to bias entries in a dynamic predictor so that aliasing tends to be non-destructive.  A confirmation that these actually should be recognized as the same general mechanism as used by the XOR linked list would also be helpful.</p>\n', 'ViewCount': '63', 'Title': 'Is there a term for the general method used in XORed linked list?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-30T19:21:20.813', 'LastEditDate': '2013-01-30T12:44:52.830', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4577', 'Tags': '<terminology><reference-request><data-structures><lists>', 'CreationDate': '2013-01-29T19:59:22.903', 'FavoriteCount': '3', 'Id': '9284'},9112:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been tasked with building a library of books on algorithms for our small company (about 15 people). The budget is more than 5k, but certainly less than 10k, so I can buy a fair number of books. All people here have at least a Bachelor's degree in CS or a closely related field, so while I will get some basic textbook like Cormen, I'm more interested in good books on advanced topics. (I will get Knuth's 4 volumes, BTW.)</p>\n\n<p>Some list of topics would be:</p>\n\n<ul>\n<li><p>Sorting algorithms</p></li>\n<li><p>Graph algorithms</p></li>\n<li><p>String algorithms</p></li>\n<li><p>Randomized algorithms</p></li>\n<li><p>Distributed algorithms</p></li>\n<li><p>Combinatorial algorithms</p></li>\n<li><p>etc.</p></li>\n</ul>\n\n<p>Essentially I'm looking for good recommendations on books about major topics within CS related to algorithms and data structures. Especially stuff that goes beyond what's typically covered in algorithm and data structure classes as part of a Bachelor's degree at a good school. I know the question is quite fuzzy, since I'm looking for generically useful material. The software we develop is mostly system level stuff handling large amounts of data.</p>\n\n<p>The ideal would also be to find anything that would cover fairly recent cool data structures and algorithms, which most people might not have heard about.</p>\n\n<hr>\n\n<p>EDIT: Here are some preliminary books that I think I should get:</p>\n\n<ul>\n<li><p>Introduction to Algorithms by Cormen et al. </p></li>\n<li><p>Algorithm Design by Kleinberg, Tardos</p></li>\n<li><p>The Art of Computer Programming Vol 1-4 by Knuth</p></li>\n<li><p>Approximation Algorithms by Vazirani</p></li>\n<li><p>The Design of Approximation Algorithms by Williamson, Shmoys</p></li>\n<li><p>Randomized Algorithms by Motwani, Raghavan</p></li>\n<li><p>Introduction to the Theory of Computation by Sipser</p></li>\n<li><p>Computational Complexity by Arora, Barak</p></li>\n<li><p>Computers and Intractability by Garey and Johnson</p></li>\n<li><p>Combinatorial Optimization by Schrijver</p></li>\n</ul>\n\n<p>A few other books my colleagues wanted that deal with techniques and algorithms for language design, compilers and formal methods are:</p>\n\n<ul>\n<li><p>Types and Programming Languages by Pierce</p></li>\n<li><p>Principles of Model Checking by Baier, Katoen</p></li>\n<li><p>Compilers: Principles, Techniques, and Tools by Aho, Lam, Sethi, Ullman</p></li>\n<li><p>The Compiler Design Handbook: Optimizations and Machine Code Generation, Second Edition by Srikant, Shankar</p></li>\n<li><p>The Garbage Collection Handbook: The Art of Automatic Memory Management by Jones, Hosking, Moss</p></li>\n</ul>\n", 'ViewCount': '368', 'Title': 'Algorithm books on a range of topics', 'LastEditorUserId': '6675', 'LastActivityDate': '2013-02-02T21:46:22.347', 'LastEditDate': '2013-02-02T19:14:58.860', 'AnswerCount': '5', 'CommentCount': '10', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6675', 'Tags': '<algorithms><reference-request><data-structures><books>', 'CreationDate': '2013-02-02T05:57:06.183', 'FavoriteCount': '2', 'Id': '9413'},9113:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm trying to prove/disprove two statements. I just want to make sure with you I'm on the right line.</p>\n\n<p>These are the following statements:</p>\n\n<p><strong>Preface :</strong> Let A[n] be an array of min-heap (a min-heap represented by an array], whereas all the elements in the heap are different from each other. \nLet i and j be two indexes in the range : $0 \\le i, j \\le n-1$.</p>\n\n<p><em><strong>Prove or disprove :</em></strong> </p>\n\n<ol>\n<li>If $i &lt; j $ then $A[i] &lt; A[j]$</li>\n<li>If $A[i] &lt; A[j] $ then $i &lt; j$</li>\n</ol>\n\n<p>I believe I managed to disprove both of them using the following heap:</p>\n\n<p>$\\qquad [2, 6, 7, 11, 14, 13, 12, 12, 13,15, 16, 71, 72, 13, 81]$</p>\n\n<p>For:</p>\n\n<ol>\n<li><p>Simply plug in the following indexes: $i = 4$ and $j = 13$. </p>\n\n<p>So $i &lt; j$ but $A[i] &gt; A[j]$.</p></li>\n<li><p>Simply plug in the following indexes: $i = 13$ and $j = 4$.</p>\n\n<p>So $A[i] &lt; A[j]$ but $i &gt; j$.</p></li>\n</ol>\n\n<p>Am I missing something here? Or It is really that easy?</p>\n", 'ViewCount': '209', 'Title': 'MinHeap represented by an array - two simple statements', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-03T10:26:59.710', 'LastEditDate': '2013-02-03T10:26:59.710', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '9445', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4514', 'Tags': '<data-structures><binary-trees><arrays><heaps>', 'CreationDate': '2013-02-03T08:44:32.337', 'Id': '9444'},9114:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '983', 'Title': 'Algorithm for building a suffix array in time $O(n \\log^2 n)$', 'LastEditDate': '2013-02-04T15:51:33.583', 'AnswerCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4916', 'FavoriteCount': '1', 'Body': '<p>I\'ve been working with suffix arrays lately, and I can\'t find an efficient algorithm for building a suffix array which is easy to understand.  I have seen in many sites that there is an $O(n \\log^2 n)$ algorithm, but I can\'t understand it, as many important details are omitted.  There\'s an example at <a href="http://apps.topcoder.com/forums/?module=Thread&amp;threadID=627379&amp;start=0&amp;mc=33#1039014" rel="nofollow">Top Coder</a>.</p>\n\n<p>Could someone introduce me an efficient algorithm for suffix array construction, which is easy to comprehend?</p>\n', 'Tags': '<algorithms><data-structures><strings>', 'LastEditorUserId': '4916', 'LastActivityDate': '2013-02-05T22:22:27.363', 'CommentCount': '0', 'AcceptedAnswerId': '9466', 'CreationDate': '2013-02-03T13:11:42.780', 'Id': '9447'},9115:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>There is a square table composed of <code>N*N</code> Cells. Initially all cell is filled with a number <code>0</code>.\nTwo types of Operations can be performed on the table.</p>\n\n<p>R i k: All the numbers in the cells on ith row has been changed to k.</p>\n\n<p>C i k: All the numbers in the cells on ith column has been changed to k.</p>\n\n<p>Please Note : <strong>k \u2208 {0, 1} (ie k = 0 or 1).</strong></p>\n\n<p>At any time ,Alice is interested to know  the total number of 0's on some row or total number of 0's on some column.</p>\n\n<p>Our task is to answer Alice Query.</p>\n\n<p><em><strong>Input</em></strong></p>\n\n<p>First line contains a single integer N as described above.Next line contains a single integer Q representing the total operations and queries from Alice.\nThen Followed Q lines , containing operations or queries in following format:</p>\n\n<pre><code>R i k: All the numbers in the cells ith row has been changed to k.\n\nC i k: All the numbers in the cells ith column has been changed to k.\n\nqc i :How many 0's are there in ith row?\n\nqr i:How many cols are there in ith col?\n</code></pre>\n\n<p><strong>Examples</strong>:\nInput:</p>\n\n<pre><code>3\n\n6\n\nqr 1\n\nC 1 1\n\nqr 1\n\nqc 1\n\nR 1 0\n\nqc 1\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>3\n\n2\n\n0\n\n1\n</code></pre>\n\n<p>I tried a Naive solution.But it got time out. The constraint is quite large and the time limit is just 0.55 Seconds.</p>\n\n<p>**1 \u2264 N, Q \u2264 500000 (6 * 10^5)</p>\n\n<p>How can i solve this problem within the given time-limit?</p>\n\n<p>P.S: <em>This is not a homework</em>.</p>\n", 'ViewCount': '62', 'Title': '0 and 1 Queries in tables of N*N cells', 'LastActivityDate': '2013-02-05T12:34:19.653', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2041', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-02-04T06:06:42.547', 'Id': '9465'},9116:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '269', 'Title': 'Are link-cut trees ever used in practice, for max flow computation or other applications?', 'LastEditDate': '2013-02-05T10:22:43.413', 'AnswerCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '6701', 'FavoriteCount': '2', 'Body': '<p>Many max flow algorithms that I commonly see implemented, Dinic\'s algorithm, push relabel, and others, can have their asymptotic time cost improved through the use of <a href="http://en.wikipedia.org/wiki/Link/cut_tree">dynamic trees</a> (also known as link-cut trees).</p>\n\n<ul>\n<li>Push relabel runs in $O(V^2E)$ or $O(V^3)$ or $O(V^2\\sqrt{E})$ normally, but with dynamic trees $O(VE \\log(V^2/E))$</li>\n<li>Dinic\'s algorithm runs in $O(V^2E)$, but with dynamic trees $O(VE\\log(V))$</li>\n</ul>\n\n<p>However, practical implementations of max-flow algorithms in most libraries don\'t seem to make use of this data structure.  Are dynamic trees ever used in practice for max flow computation? Or do they carry too much overhead to be useful for real world problem sizes?</p>\n\n<p>Are there any other problem domains where link cut trees are used?</p>\n\n<p>This question is related to a question that I asked on cstheory: <a href="http://cstheory.stackexchange.com/questions/16347/are-any-of-the-state-of-the-art-maximum-flow-algorithms-practical">Are any of the state of the art Maximum Flow algorithms practical?</a></p>\n', 'Tags': '<reference-request><graphs><data-structures><network-flow>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-05T21:59:44.773', 'CommentCount': '2', 'AcceptedAnswerId': '9521', 'CreationDate': '2013-02-05T04:36:34.087', 'Id': '9501'},9117:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In terms of comparing data structures, is a relational database just a >1 dimensional array?  I'm just asking because I don't know much about databases, but I know a bit about data structures.</p>\n\n<p>I am trying to make this clear in my mind.</p>\n", 'ViewCount': '184', 'Title': 'Is a relational database just a $ \\geq 2$-dimensional array?', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-02-15T00:00:48.957', 'LastEditDate': '2013-02-14T23:40:52.690', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '2', 'OwnerDisplayName': 'Seb Silver', 'PostTypeId': '1', 'OwnerUserId': '6872', 'Tags': '<data-structures>', 'CreationDate': '2013-02-14T03:24:07.020', 'Id': '9767'},9118:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am concerned with the question of <strong>the asymptotic running time of the Ukkonen\'s algorithm</strong>, perhaps the most popular algorithm for constructing <strong>suffix trees</strong> in linear (?) time.</p>\n\n<p>Here is a citation from the book "Algorithms on strings, trees and sequences" by Dan Gusfield (section 6.5.1):</p>\n\n<blockquote>\n  <p>"... the Aho-Corasick, Weiner, <strong>Ukkonen</strong> and McCreight algorithms all either require $\\Theta(m|\\Sigma|)$ space, or the $O(m)$ time bound should be replaced with the minimum of $O(m \\log m)$ and $O(m \\log|\\Sigma|)$".</p>\n  \n  <p><em>[$m$ is the string length and $\\Sigma$ is the size of the alphabet]</em></p>\n</blockquote>\n\n<p>I don\'t understand why that is true.</p>\n\n<ul>\n<li><strong>Space:</strong> well, in case we represent branches out of the nodes using arrays of size $\\Theta(|\\Sigma|)$, then, indeed, we end up with $\\Theta(m|\\Sigma|)$ space usage. However, as far as I can see, it is also possible to store the branches using hash tables (say, dictionaries in Python). We would then have only $\\Theta(m)$ pointers stored in all hash tables altogether (since there are $\\Theta(m)$ edges in the tree), while still being able to access the children nodes in $O(1)$ time, as fast as when using arrays.</li>\n<li><strong>Time</strong>: as mentioned above, using hash tables allows us to access the outgoing branches of any node in $O(1)$ time. Since the Ukkonen\'s algorithm requires $O(m)$ operations (including accessing children nodes), the overall running time then would be also $O(m)$.</li>\n</ul>\n\n<p>I would be very grateful to you for any hints on why I am wrong in my conclusions and why Gusfield is right about the dependence of the Ukkonen\'s algorithm on the alphabet.</p>\n', 'ViewCount': '238', 'Title': "How does the runtime of the Ukkonen's algorithm depend on the alphabet size?", 'LastEditorUserId': '162', 'LastActivityDate': '2013-02-18T14:59:52.107', 'LastEditDate': '2013-02-16T08:05:47.417', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '13', 'PostTypeId': '1', 'OwnerUserId': '162', 'Tags': '<algorithms><data-structures><algorithm-analysis><strings>', 'CreationDate': '2013-02-15T22:05:13.680', 'FavoriteCount': '1', 'Id': '9820'},9119:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I would like to find the height of a d-ary heap. Assuming you have an Array that starts indexing at $1$ we have the following:</p>\n\n<p>The parent of a node $i$ is given by: $\\left\\lfloor\\frac{i+1}{d}\\right\\rfloor$</p>\n\n<p>The $d$ children of a parent at node $i$ are given by: $di-d+1, di-d+2,\\ldots di+1$</p>\n\n<p>The height of a heap (which is slightly different than the height of a binary search tree) is a longest path from the root to a leaf. A longest path will always be from the last node in the heap to the root, but how do I calculate this longest path?</p>\n\n<p>My first Idea is to setup a recurrence relation for the height of the tree:</p>\n\n<p>\\begin{equation}\nh(1) = 0\\\\\nh(i) = h\\left(\\left\\lfloor\\frac{i+1}{d}\\right\\rfloor\\right)+1 \n\\end{equation}</p>\n\n<p>This seems overly-complicated and I feel like the answer is much more simple. Is there a better way to find the height of a $d-$ary heap?</p>\n', 'ViewCount': '567', 'Title': 'Finding the height of a d-ary heap', 'LastEditorUserId': '6815', 'LastActivityDate': '2013-02-19T09:44:45.017', 'LastEditDate': '2013-02-18T23:01:14.800', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '9929', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6815', 'Tags': '<data-structures><recurrence-relation><heaps>', 'CreationDate': '2013-02-18T22:44:05.470', 'Id': '9914'},9120:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '100', 'Title': 'Lossless data compression must make some messages longer?', 'LastEditDate': '2013-02-19T23:50:53.463', 'AnswerCount': '2', 'Score': '4', 'OwnerDisplayName': 'Legendre', 'PostTypeId': '1', 'OwnerUserId': '6956', 'Body': '<p>I read on Wikipedia and in lecture notes that if a lossless data compression algorithm makes a message shorter, it must make another message longer.</p>\n\n<p>E.g. In this set of notes, it says:</p>\n\n<blockquote>\n  <p>Consider, for example, the 8 possible 3 bit messages. If one is\n  compressed to two bits, it is not hard to convince yourself that two\n  messages will have to expand to 4 bits, giving an average of 3 1/8\n  bits.</p>\n</blockquote>\n\n<p>There must be a gap in my understand because I thought I could compress all 3 bit messages this way: </p>\n\n<ul>\n<li>Encode: If it starts with a zero, delete the leading zero.</li>\n<li>Decode: If message is 3 bit, do nothing. If message is 2 bit, add a\nleading zero.</li>\n<li>Compressed set: 00,01,10,11,100,101,110,111</li>\n</ul>\n\n<p>What am I getting wrong? I am new to CS, so maybe there are some rules/conventions that I  missed?</p>\n', 'ClosedDate': '2013-02-21T05:49:18.790', 'Tags': '<data-structures><data-compression>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-02-19T23:50:53.463', 'CommentCount': '0', 'AcceptedAnswerId': '9939', 'CreationDate': '2013-02-19T17:32:59.783', 'Id': '9938'},9121:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '536', 'Title': 'Why do we use persistent data structures in functional programming?', 'LastEditDate': '2013-02-24T17:51:40.223', 'AnswerCount': '3', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '6466', 'FavoriteCount': '1', 'Body': '<p>Functional programming employs persistent data structures and immutable objects. My question is why is it crucial to have such data structures here? I want to understand <strong>at a low level</strong> what would happen if the data structure is not persistent? Would the program crash more often? </p>\n', 'Tags': '<data-structures><functional-programming><programming-paradigms>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T17:51:40.223', 'CommentCount': '1', 'AcceptedAnswerId': '10006', 'CreationDate': '2013-02-21T07:10:16.707', 'Id': '10002'},9122:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I asked a question on persistence data structure <a href="http://cs.stackexchange.com/questions/10002/why-do-we-use-persistent-data-structures-in-functional-programming">here</a>. After that I came across an <a href="http://www.codeproject.com/Articles/9680/Persistent-Data-Structures" rel="nofollow">article on code project</a>. I have got a question on the following figure from the same article:</p>\n\n<p><img src="http://i.stack.imgur.com/ZUxpt.gif" alt="enter image description here">   </p>\n\n<p>The author says:</p>\n\n<blockquote>\n  <p>Inserting a new item into a persistent singly linked list will not\n  alter the existing list but create a new version with the item\n  inserted into it. Instead of copying the entire list and then\n  inserting the item into the copy, a better strategy is to reuse as\n  much of the old list as possible.</p>\n</blockquote>\n\n<p>Now my doubt wrt to this idea is that since the two versions (Red and Yellow + Red), have common nodes (i.e last three Red nodes), how can one access these two versions simultaneously (for example in a multithreaded application)?  </p>\n', 'ViewCount': '33', 'Title': 'Sharing of nodes in Persistence data structure', 'LastEditorUserId': '6466', 'LastActivityDate': '2013-02-24T09:41:52.517', 'LastEditDate': '2013-02-24T08:09:19.503', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '10051', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6466', 'Tags': '<data-structures>', 'CreationDate': '2013-02-24T06:00:44.903', 'Id': '10041'},9123:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What data structure should I store my graph in to get the best performance from the Dijkstra algorithm?</p>\n\n<p>Object-pointer? Adjacency list? Something else?</p>\n\n<p>I want the lowest O(). Any other tips are appreciated too!</p>\n', 'ViewCount': '678', 'Title': "What graph data structure works fastest with Dijkstra's algorithm?", 'LastActivityDate': '2013-02-24T08:49:26.973', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '10046', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-02-24T08:46:02.133', 'Id': '10044'},9124:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1157', 'Title': 'Is search a binary heap operation?', 'LastEditDate': '2013-02-24T16:24:27.193', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'Barry Fruitman', 'PostTypeId': '1', 'OwnerUserId': '7007', 'Body': '<p>According to the <a href="http://en.wikipedia.org/wiki/Binary_heap" rel="nofollow">Wikipedia page</a>, search is "not an operation" on binary heaps (see complexity box at top-right).</p>\n\n<p>Why not? Binary heaps may not be sorted, but they are ordered, and a full graph traversal can find any object in $O(n)$ time, no?</p>\n\n<p>Is the page wrong or am I?</p>\n', 'Tags': '<data-structures><search-algorithms><heaps>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:24:27.193', 'CommentCount': '1', 'AcceptedAnswerId': '10052', 'CreationDate': '2013-02-24T07:37:18.370', 'Id': '10049'},9125:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The time complexity of merge (union) operation is said to be $O(\\lg (n_1 + n_2))$, where $n_1$ and $n_2$ are the numbers of elements in the merged heaps, respectively. I do not understand this - the algorithm has to go through all the elements of both rightmost paths of the original heaps - lengths of these paths are bound by $O(\\lg n_1)$ and $O(\\lg n_2)$. That makes $O(\\lg n_1 + \\lg n_2)$ in total, which is $O(\\lg (n_1 n_2))$. Where am I making a mistake in my assumptions?</p>\n\n<p>Arbitrary delete operation - the complexity should be $O(\\lg n)$, where $n$ is the size of the heap. But after the deletion, the algorithm has to go through all the nodes from the parent of the deleted node to the root and correct the Leftist property, and the lenght of this path is bound by $O(n)$. Again, where am I wrong?</p>\n', 'ViewCount': '116', 'Title': 'Leftist heap - determining time complexity', 'LastEditorUserId': '98', 'LastActivityDate': '2013-02-24T16:22:09.107', 'LastEditDate': '2013-02-24T16:22:09.107', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7009', 'Tags': '<data-structures><runtime-analysis><heaps>', 'CreationDate': '2013-02-24T15:50:20.213', 'Id': '10056'},9126:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<h3>Background:</h3>\n\n<p>In this question we care only about worst-case running-time.</p>\n\n<p>Array and (doubly) linked lists can be used to keep a list of items and implement the <strong><a href="http://en.wikipedia.org/wiki/Vector_data_structure#Efficiency_comparison_with_other_data_structures">vector</a></strong> abstract data type. Consider the following three operations:</p>\n\n<ul>\n<li>$Location(i)$: returns a pointer to the $i$th item in the list of items in the array.</li>\n<li>$Insert(k,x)$: insert the item $k$ in the list after the item pointed to by $x$.</li>\n<li>$Delete(x)$: remove the item in the list pointed to by $x$.</li>\n</ul>\n\n<p>The main operation that an <strong>array</strong> provides is location which can be computed in constant time. However delete and insert are inefficient.</p>\n\n<p>On the other hand, in a <strong>doubly linked list</strong>, it is easy to perform insert and delete in constant time, but location is inefficient.</p>\n\n<h3>Questions:</h3>\n\n<p>Can there be a data structure to store a list of items where all three operations are $O(1)$? If not, what is the best worst-case running-time that we can achieve for all operations simultaneously? </p>\n\n<p>Note that a balanced binary search tree like red-black trees augmented with size of subtrees would give $O(\\lg n)$, is it possible to do better? Do we know a non-trivial lower-bound for this problem?</p>\n', 'ViewCount': '136', 'ClosedDate': '2013-03-01T18:52:59.140', 'Title': 'Is there a data-structure which is more efficient than both arrays and linked lists?', 'LastActivityDate': '2013-02-26T01:06:49.737', 'AnswerCount': '0', 'CommentCount': '17', 'Score': '10', 'PostTypeId': '1', 'OwnerUserId': '41', 'Tags': '<data-structures><arrays><lower-bounds><linked-lists>', 'CreationDate': '2013-02-26T01:06:49.737', 'Id': '10111'},9127:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In many discussions of binary heap, normally only decrease-key is listed as supported operation for a min-heap. For example, CLR chapter 6.1 and <a href="http://en.wikipedia.org/wiki/Heap_%28data_structure%29" rel="nofollow">this wikipedia page</a>. Why isn\'t increase key normally listed for min-heap? I imagine it is possible to do that in O(height) by iteratively swapping the increased element (x) with the minimum of its children, until none of its children is bigger than x.</p>\n\n<p>e.g.</p>\n\n<pre><code>IncreaseKey(int pos, int newValue)\n{\n   heap[pos] = newValue;\n   while(left(pos) &lt; heap.Length)\n   {\n      int smallest = left(pos);\n      if(heap[right(pos)] &lt; heap[left(pos)])\n         smallest = right(pos);\n      if(heap[pos] &lt; heap[smallest])\n      { \n         swap(smallest, pos);\n         pos= smallest;\n      }\n      else return;\n   }   \n}\n</code></pre>\n\n<p>Is the above correct? If not, why? If yes, why isn\'t increase key listed for min-heap?</p>\n', 'ViewCount': '3873', 'Title': 'Increase-key and decrease-key in a binary min-heap', 'LastActivityDate': '2013-03-03T22:31:44.110', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7105', 'Tags': '<algorithms><data-structures><heaps><priority-queues>', 'CreationDate': '2013-03-02T11:08:39.657', 'FavoriteCount': '2', 'Id': '10203'},9128:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '143', 'Title': 'Are probabilistic search data structures useful?', 'LastEditDate': '2013-03-03T16:57:40.847', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7022', 'FavoriteCount': '1', 'Body': '<p>A SkipList provides the same $O(\\log n)$ bounds for search as a balanced tree with the advantage that rebalancing isn\'t necessary. Since the SkipList is constructed using random coin flips, these bounds only hold as long as the structure of the SkipList is sufficiently "balanced". In particular, with probability $1/n^c$ for some constant $c&gt;0$, the balanced structure might be lost after inserting an element.</p>\n\n<p>Let\'s say I want to use a skip list as a storage backend in a web application that potentially runs forever. So after some polynomial number of operations, the balanced structure of the SkipList is very likely to be lost. </p>\n\n<p>Is my reasoning correct? Do such probabilistic search/storage data structures have practical applications and if so, how is the above problem avoided? </p>\n\n<p>Edit: I\'m aware that there are deterministic variants of the SkipList, which, are much more complicated to implement in comparison to the (classic) randomized SkipList.</p>\n', 'Tags': '<data-structures><search-trees><probabilistic-algorithms>', 'LastEditorUserId': '7022', 'LastActivityDate': '2013-03-08T13:10:42.580', 'CommentCount': '1', 'AcceptedAnswerId': '10239', 'CreationDate': '2013-03-03T14:09:52.280', 'Id': '10229'},9129:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given a set $S$ of $n$ elements, and a set $\\mathcal{X}$ of $m$ subsets of $S$, decide if there exist $U,V \\in \\mathcal{X}$, s.t. $U \\cup V = S$.</p>\n\n<p>Brute force would take time $O(nm^2)$ but is there any way of solving this more efficiently?</p>\n', 'ViewCount': '113', 'Title': 'Test if there are two subsets which cover a set', 'LastActivityDate': '2013-03-04T19:58:53.887', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '10272', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '4259', 'Tags': '<algorithms><data-structures><sets>', 'CreationDate': '2013-03-04T06:43:10.150', 'FavoriteCount': '1', 'Id': '10250'},9130:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Lets say I have a global dataset and I run queries over those data set.\nFor example my dataset would be</p>\n\n<ul>\n<li>#id, #Name, #Employee, #Birthdate, #number_of_children</li>\n<li>1, Nick, Nasa, 1982, 1</li>\n<li>2, Jack, Exon, 1985, 5</li>\n<li>3, Tom, ABCD, 1978, 0</li>\n</ul>\n\n<p>And I can run queryies on those dataset.\nsample queries would be\n* #Query => #Result_ids \n* (Name starts with A) => [1]\n* (Birthdate before 1983 and have children ) => [1]</p>\n\n<p>I want to store those queries on a data structure and I want to be able to do set operations on those queries like intersection and union. So an example union operation would be.</p>\n\n<p>(Birthdate before 1983) intersection (have children) => (Birthdate before 1983 and have children)</p>\n\n<p>I also want to be able to findout if one query is subset or superset of another one. For example.</p>\n\n<p>(Birthdate before 1983) is superset of (Birthdate before 1980)\n(Have 3 children) is subset of (Have more than 1 children)</p>\n\n<p>(Name = Jack and born in 1980) is subset of (Born before 1990)</p>\n\n<p>I will have a program that will have thousands of queries. And it will combine those queries to make more variety of queries. When I have a new query, I will compare it with existing queries to see if I have an exact query in store or have a superset.</p>\n\n<p>Can anybody suggest me a data structure that is fast enough to store and operate on those data?</p>\n', 'ViewCount': '34', 'Title': 'Algorithm for query comparison', 'LastActivityDate': '2013-03-04T15:18:29.903', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '1', 'OwnerDisplayName': 'yilmazhuseyin', 'PostTypeId': '1', 'OwnerUserId': '7123', 'Tags': '<data-structures><data-sets>', 'CreationDate': '2013-02-25T13:47:07.187', 'Id': '10264'},9131:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am trying to understand how to do the amortized cost for a dynamic table. Suppose we are using the accounting method.</p>\n\n<p>Let A of size m be an array of n elements. When $n = m$, then we create a new array of size $4m$, and then copy the elements of A to the new array. When $n = \\frac{m}{4}$, then you create a new array of size $\\frac{m}{4}$, and copy the elements to that array.</p>\n\n<p>What I am confused about is how to calculate the costs.\nFrom what I know so far:\nBefore the first expansion, you pay two dollars to insert. <code>1$</code> for the insert, and <code>1$</code> you just store with the element, so that you can use that later for a copy operation.</p>\n\n<p>Then when you expand it, you use that stored <code>$</code> to move the element to the new array.\nNow in the new array the elements won't have any <code>$</code> with them. But now as you insert a new element, you use <code>3$</code>. <code>1$</code> for the insert, then one more for itself (for a future copy), and one more for the previous element that was just copied.</p>\n\n<p>The problem here is, what if you have an array like this:</p>\n\n<p><code>1$ 2$</code></p>\n\n<p>Then insert an element</p>\n\n<p><code>1$ 2 3$ _ _ _ _ _</code></p>\n\n<p>Now how do you handle a delete operation?</p>\n", 'ViewCount': '207', 'Title': 'How to compute amoritized cost for a dynamic array?', 'LastEditorUserId': '7492', 'LastActivityDate': '2013-05-05T18:24:05.000', 'LastEditDate': '2013-05-05T18:24:05.000', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<data-structures><algorithm-analysis><amortized-analysis>', 'CreationDate': '2013-03-09T01:47:07.637', 'Id': '10399'},9132:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '127', 'Title': 'Hash function - uniformity / strong universality', 'LastEditDate': '2013-03-11T07:38:23.137', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'Andreas T', 'PostTypeId': '1', 'OwnerUserId': '7226', 'Body': '<p>I am currently learning how randomised Hashing works. \nSo, you have a class (aka family) $H$ of hash functions, each of which maps the universe $U$ to the hash table $N$.</p>\n\n<p>That class is called "strongly universal" or "pairwise independent" if $\\forall x,y \\in U, x \\neq y: \\forall z_1,z_2 \\in N: \\Pr\\limits_{h \\in H}[h(x) = z_1 \\land h(y) = z_2] \\leq \\frac{1}{|N|^2}$. In words: pick any two elements from the universe and two from the hash table. If you pick a hash function from the hash class at random, the probability that these two elements are mapped to each other by $h$ is less or equal than $\\frac{1}{|N|^2}$.</p>\n\n<p>Now, what is confusing me is that, since $x$, $y$, $z_1$ and $z_2$ are all completely independent, it looks to me like you could just "remove" one pair from the equation and still get the same result. That would be $\\forall x \\in U: \\forall z \\in N: \\Pr\\limits_{h \\in H}[h(x) = z] \\leq \\frac{1}{|N|}$. This, however, is called "uniformity" of a hash class.</p>\n\n<p>Could someone explain to me why these two attributes are different from one anoter?</p>\n', 'Tags': '<data-structures><hash-tables><hash>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-11T07:38:23.137', 'CommentCount': '2', 'AcceptedAnswerId': '10449', 'CreationDate': '2013-03-10T22:21:18.860', 'Id': '10447'},9133:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '157', 'Title': 'How to get expected running time of hash table?', 'LastEditDate': '2013-03-13T08:22:56.710', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'omega', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much. Am I making a mistake here?</p>\n", 'ClosedDate': '2013-03-13T08:23:35.603', 'Tags': '<data-structures><runtime-analysis><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-13T08:22:56.710', 'CommentCount': '1', 'CreationDate': '2013-03-12T23:21:20.177', 'Id': '10501'},9134:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '271', 'Title': 'Weighted sum of last N numbers', 'LastEditDate': '2013-03-21T03:56:56.377', 'AnswerCount': '1', 'Score': '16', 'PostTypeId': '1', 'OwnerUserId': '4213', 'FavoriteCount': '2', 'Body': u"<p>Suppose we're receiving numbers in a stream. After each number is received, a weighted sum of the last $N$ numbers needs to be calculated, where the weights are always the same, but arbitrary. </p>\n\n<p>How efficiently can this done if we are allowed to keep a data structure to help with the computation? Can we do any better than $\\Theta(N)$, i.e. recomputing the sum each time a number is received?</p>\n\n<p>For example:\nSuppose the weights are $W= \\langle w_1, w_2, w_3, w_4\\rangle$. At one point we have the list of last $N$ numbers $L_1= \\langle a, b, c, d \\rangle&gt;$, and the weighted sum $S_1=w_1*a+w_2*b+w_3*c+w_4*d$. </p>\n\n<p>When another number, $e$, is received, we update the list to get $L_2= \\langle b,c,d,e\\rangle$ and we need to compute $S_2=w_1*b+w_2*c+w_3*d+w_4*e$.</p>\n\n<p><strong>Consideration using FFT</strong>\nA special case of this problem appears to be solvable efficiently by employing the Fast Fourier Transform. Here, we compute the weighed sums $S$ in multiples of $N$. In other words, we receive $N$ numbers and only then can we compute the corresponding $N$ weighed sums. To do this, we need $N-1$ past numbers (for which sums have already been computed), and $N$ new numbers, in total $2N-1$ numbers. </p>\n\n<p>If this vector of input numbers and the weight vector $W$ define the coefficients of the polynomials $P(x)$ and $Q(x)$, with coefficients in $Q$ reversed, we see that the product $P(x)\\times Q(x)$ is a polynomial whose coefficients in front of $x^{N-1}$ up to $x^{2N-2}$ are exactly the weighted sums we seek. These can be computed using FFT in $\\Theta(N*\\log (N))$ time, which gives us an average of $\u0398(\\log (N))$ time per input number.</p>\n\n<p>This is however not a solution the the problem as stated, because it is required that the weighted sum is computed efficiently <em>each</em> time a new number is received - we cannot delay the computation.</p>\n", 'Tags': '<algorithms><data-structures><online-algorithms>', 'LastEditorUserId': '683', 'LastActivityDate': '2013-03-21T18:20:56.223', 'CommentCount': '4', 'AcceptedAnswerId': '10670', 'CreationDate': '2013-03-19T05:24:49.463', 'Id': '10612'},9135:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am a beginner in data structures and recently came across a vector implemented on an array, which is extended on demand. Of course the table cannot be extended "in place", we must allocate a new array, then copy elements from the previous one which is a linear operation itself (invoked log(n) times, where n is the number of insertion operations). Are there better implementations of such data structure while preserving constant item access time? For example, how about implementing a concept known from disk file systems - an allocation table; whenever we need to extend our array, an allocation table entry is created for newly reserved memory, without touching previously inserted items. Indexing time could still be constant, if only the allocation table would be implemented wisely (for example with using constant "page" size)\nWhat I\'ve written could be complete nonsense; it\'s just an idea.</p>\n', 'ViewCount': '95', 'Title': 'A vector-like data structure with allocation table; O(1) indexing time required', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-20T20:59:27.867', 'LastEditDate': '2013-03-20T20:59:27.867', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'OwnerDisplayName': 'Mariusz', 'PostTypeId': '1', 'Tags': '<data-structures><efficiency><arrays>', 'CreationDate': '2013-03-20T19:35:45.130', 'Id': '10662'},9136:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In a circular linked list, if an elements needs to be inserted at front [just before the node pointed by head], can be done in O(1) (see the answer <a href="http://stackoverflow.com/questions/1096505/implement-an-algorithm-to-insert-a-node-into-a-circular-linked-list-without-trav">here</a>)</p>\n\n<p>But in a book currently, I have, it is mentioned that it is done in O(n) (the usual method). I also saw few lecture ppts, they all mention the usual method of traversing the list &amp; adding an element.</p>\n\n<p>My question is :</p>\n\n<ol>\n<li><p>In practical scenarios which method is used ?</p></li>\n<li><p>I am about to attend an exam, which consists of MCQs, if above question is asked shall I mark O(n), since that is the standard answer ?</p></li>\n</ol>\n', 'ViewCount': '452', 'Title': 'Complexity of algorithm inserting an element in a circular linked list at the front end', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-23T16:37:13.557', 'LastEditDate': '2013-03-23T15:58:50.753', 'AnswerCount': '1', 'CommentCount': '12', 'AcceptedAnswerId': '10704', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<data-structures><efficiency><linked-lists>', 'CreationDate': '2013-03-23T09:26:46.557', 'Id': '10701'},9137:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '350', 'Title': 'Get the running time of forest disjoint sets', 'LastEditDate': '2013-03-25T10:29:50.897', 'AnswerCount': '1', 'Score': '3', 'OwnerDisplayName': 'omega', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Body': '<p>If you have a forest implementation of disjoint sets, and have the union by weight/rank heuristic where you append the smaller one. Then why is the worst case running time &Theta;(m log n)? (m is the number of disjoint sets and n is the number of Make Set operations.)</p>\n', 'Tags': '<data-structures>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-25T10:29:50.897', 'CommentCount': '3', 'AcceptedAnswerId': '10760', 'CreationDate': '2013-03-24T20:48:15.517', 'Id': '10759'},9138:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1020', 'Title': 'Colour a binary tree to be a red-black tree', 'LastEditDate': '2013-04-03T16:56:13.100', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '139', 'FavoriteCount': '2', 'Body': '<p>A common interview question is to give an algorithm to determine if a given binary tree is height balanced (AVL tree definition).</p>\n\n<p>I was wondering if we can do something similar with Red-Black trees.</p>\n\n<blockquote>\n  <p>Given an arbitrary uncoloured binary tree (with NULL nodes), is there a "fast" algorithm which can determine if we can colour (and find a colouring) the\n  nodes Red/Black so that they satisfy all the properties of a Red-Black\n  tree (definition as in this <a href="http://cs.stackexchange.com/questions/342/not-all-red-black-trees-are-balanced">question</a>)?</p>\n</blockquote>\n\n<p>An initial thought was that we can just remove the NULL nodes and try to recursively verify if the resulting tree can be a red-black tree, but that didn\'t seem to go anywhere.</p>\n\n<p>I did (a brief) web search for papers, but could not seem to find any which seem to deal with this problem.</p>\n\n<p>It is possible that I am missing something simple.</p>\n', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'LastEditorUserId': '139', 'LastActivityDate': '2013-04-20T17:57:19.710', 'CommentCount': '1', 'AcceptedAnswerId': '10999', 'CreationDate': '2013-04-03T08:02:32.960', 'Id': '10990'},9139:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a heap where both child nodes of the root are 10, and I\'d like to perform an operation to remove the min value 9.</p>\n\n<p>I proceed to replacing the root with its next of kin, 18. However when I bubble down, I am unsure of which direction to take.</p>\n\n<p><img src="http://i.stack.imgur.com/os1CS.jpg" alt="enter image description here"></p>\n', 'ViewCount': '83', 'Title': 'When two siblings in a heap are equal, how do you bubble down?', 'LastActivityDate': '2013-04-04T05:16:59.423', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<data-structures><heaps>', 'CreationDate': '2013-04-04T02:16:26.417', 'FavoriteCount': '1', 'Id': '11008'},9140:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '1991', 'Title': 'Why is it best to use a prime number as a mod in a hashing function?', 'LastEditDate': '2013-04-05T12:49:56.007', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '4348', 'FavoriteCount': '2', 'Body': "<p>If I have a list of key values from 1 to 100 and I want to organize them in an array of 11 buckets, I've been taught to form a mod function</p>\n\n<p>$$ H = k \\bmod \\ 11$$</p>\n\n<p>Now all the values will be placed one after another in 9 rows. For example, in the first bucket there will be $0, 11, 22 \\dots$. In the second, there will be $1, 12, 23 \\dots$ etc.</p>\n\n<p>Let's say I decided to be a bad boy and use a non-prime as my hashing function - take 12.\nUsing the Hashing function</p>\n\n<p>$$ H = k \\bmod \\ 12$$</p>\n\n<p>would result in a hash table with values $0, 12, 24 \\dots $ in the first bucket, $1, 13, 25 \\dots$ etc. in the second and so on.</p>\n\n<p>Essentially they are the same thing. I didn't reduce collisions and I didn't spread things out any better by using the prime number hash code and I can't see how it is ever beneficial.</p>\n", 'Tags': '<data-structures><hash><hash-tables><primes>', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-04-10T16:28:56.047', 'CommentCount': '0', 'AcceptedAnswerId': '11031', 'CreationDate': '2013-04-04T19:36:37.447', 'Id': '11029'},9141:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '183', 'Title': 'Number of possible search paths when searching in BST', 'LastEditDate': '2013-04-11T23:41:46.050', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '6665', 'FavoriteCount': '1', 'Body': "<p>I have the following question, but don't have answer for this. I would appreciate if my method is correct :</p>\n\n<p>Q. When searching for the key value 60 in a binary search tree, nodes containing the key values 10, 20, 40, 50, 70, 80, 90 are traversed, not necessarily in the order given. How many different orders are possible in which these key values can occur on the search path from the root node containing the value 60? </p>\n\n<p>(A) 35 (B) 64 (C) 128 (D) 5040 </p>\n\n<p>From the question, I understand that all nodes given have to be included in traversal and ultimately we have to reach the key, 60. For example, one such combination would be : </p>\n\n<p>10, 20, 40, 50, 90, 80, 70, 60. </p>\n\n<p>Since we have to traverse all nodes given above, we have to start either with 10 or 90. If we start with 20, we will not reach 10 (since 60 > 20 and we will traverse right subtree of 20)</p>\n\n<p>Similarly, we cannot start with 80, because we will not be able to reach 90, since 80>60, we will traverse in left sub tree of 80 &amp; thus not reaching 90. </p>\n\n<p>Lets take 10. The remaining nodes are 20, 40, 50, 70, 80, 90. Next node could be either 20 or 90. We cannot take other nodes for same earlier mentioned reason.</p>\n\n<p>If we consider similarly, at each level we are having two choices. Since there are 7 nodes, two choices for first 6 &amp; no choice for last one. So there are totally </p>\n\n<p>$2*2*2*2*2*2*1$ permutations = $2^6$ = $64$</p>\n\n<ol>\n<li><p>Is this a correct answer?</p></li>\n<li><p>If not, whats the better approach?</p></li>\n<li><p>I would like to generalize. If $n$ nodes are given then total possible search paths would be $2^{n-1}$</p></li>\n</ol>\n", 'Tags': '<data-structures><combinatorics><binary-trees><search-trees><graph-traversal>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-11T23:41:46.050', 'CommentCount': '0', 'AcceptedAnswerId': '11048', 'CreationDate': '2013-04-05T07:42:32.627', 'Id': '11043'},9142:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have been reading about AVL trees, at the moment I\'m trying to figure out how to determine the height of a tree and how to draw an AVL tree of some height with minimum number of elements.  </p>\n\n<p>In a tutorial I found that this: would be a AVL tree of height 7<br>\n<img src="http://i.stack.imgur.com/xc90l.jpg" alt="enter image description here"></p>\n\n<p>And this AVL tree with the height 4<br>\n<img src="http://i.stack.imgur.com/ZseOY.jpg" alt="enter image description here"></p>\n\n<p>This is really confusing by the look I would guess that both of them are of height 4. I\'m fairly new to data structures, I could not find a simple documentation/tutorial regarding this most of what i found was about Insertion/Deletion with AVL trees.  </p>\n\n<p>So is the top tree of height 7 if not how would I draw it with the minimal number of elements. I understand the each sub tree would have to be balanced.</p>\n', 'ViewCount': '362', 'Title': 'AVL tree with fixed height and as few elements as possible', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-07T12:57:27.700', 'LastEditDate': '2013-04-07T12:57:27.700', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7619', 'Tags': '<data-structures><binary-trees><search-trees>', 'CreationDate': '2013-04-07T01:48:44.733', 'Id': '11086'},9143:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let's say I have two equations for a geometric object (a rectangle):</p>\n\n<p>$\\left\\{\n    \\begin{array}{l}\n      x \\ge 0 \\\\\n      y \\ge 0 \\\\\n      A \\ge 0 \\\\\n      P \\ge 0 \\\\\n      A = x*y \\\\ \n P = 2*x + 2*y\n    \\end{array}\n\\right.$</p>\n\n<p>Now I would like to set the values of some variables, and compute the values of the other ones.</p>\n\n<p>At first, I thought it could be done with simple tree transformations on the AST, but I realized that I was more or less building a full solver.</p>\n\n<p>The easy case is when one side of the equality consists only of constant terms, as it consists of a simple recursive tree evaluation. But the interesting case is when there are several variable bound together in a subtree. For example, for $ (A,P)=(30,11)$ the solution is $(x,y)=(5,6)$ or $(6,5)$</p>\n\n<p>Can arbitrary complex instances of this problem solved just with tree transformations ? Or do I need something more elaborate ?</p>\n", 'ViewCount': '129', 'Title': 'Which data structure to use to solve equations?', 'LastActivityDate': '2014-03-20T00:01:45.690', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2151', 'Tags': '<data-structures><trees>', 'CreationDate': '2013-04-07T19:24:47.227', 'FavoriteCount': '0', 'Id': '11112'},9144:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I want an algorithm that calculates which element, among two, appears more often than the other in a sorted array. The array will have only two types of elements. </p>\n\n<p>Example : $aaaaaabbb$ </p>\n\n<p>Here $a&gt;b$. </p>\n\n<p>I have to find an constant time algorithm. Is it possible? The only thing I could come up with was using stack. Push all $a$'s and pop them with $b$. But it takes $O(n)$ operations. Any better approaches? Need a hint (no solution).</p>\n", 'ViewCount': '44', 'Title': 'Finding the element that occurs more often than the other', 'LastEditorUserId': '472', 'LastActivityDate': '2013-04-12T23:40:12.290', 'LastEditDate': '2013-04-12T23:40:12.290', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '11268', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '6665', 'Tags': '<algorithms><data-structures><search-algorithms><arrays>', 'CreationDate': '2013-04-12T15:30:57.237', 'Id': '11266'},9145:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '176', 'Title': 'What is a compact way to represent a partition of a set?', 'LastEditDate': '2013-04-16T00:36:16.213', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7741', 'FavoriteCount': '2', 'Body': '<p>There exist <a href="http://cs.stackexchange.com/q/3414/7741">efficient data\nstructures</a> for representing set\npartitions. These data structures have good time complexities for operations\nlike Union and Find, but they are not particularly space-efficient.</p>\n\n<p><strong>What is a space-efficient way to represent a partition of a set?</strong></p>\n\n<p>Here is one possible starting point:</p>\n\n<p>I know that the <a href="http://en.wikipedia.org/wiki/Partition_of_a_set#Counting_partitions">number of\npartitions</a>\nof a set with $N$ elements is $B_N$, the $N$-th <a href="http://en.wikipedia.org/wiki/Bell_number">Bell\nnumber</a>. So the optimal space\ncomplexity for representing a partition of a set with $N$ elements is\n$\\log_2(B_N)$ bits. To find such a representation, we could look for a\none-to-one mapping between (the set of partitions of a set of $N$ elements) and\n(the set of integers from $1$ to $B_N$).</p>\n\n<p>Is there such a mapping that is efficient to compute? What I mean by\n"efficient" is that I want to convert this compact representation\nto / from an easy-to-manipulate representation (such as a list of lists) in time\npolynomial in $N$ or $\\log_2(B_N)$.</p>\n', 'Tags': '<data-structures><combinatorics><space-complexity><sets><partitions>', 'LastEditorUserId': '7741', 'LastActivityDate': '2013-04-17T19:39:41.237', 'CommentCount': '1', 'AcceptedAnswerId': '11348', 'CreationDate': '2013-04-16T00:14:19.900', 'Id': '11345'},9146:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What are the steps to perform bottom-up heap construction on a short sequence, like <code>1, 6, 7, 2, 4</code>?</p>\n\n<p>At this <a href="http://www.apl.jhu.edu/Classes/605202/felikson/lectures/L8/L8.html" rel="nofollow">link</a> there are instructions on how to do for a list of size 15, but I can\'t [necessarily] apply the same process to a list of 5 items (my trouble is that 5 is not enough to provide a complete tree).</p>\n', 'ViewCount': '1791', 'Title': 'How to perform bottom-up construction of heaps?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-21T14:13:59.233', 'LastEditDate': '2013-04-21T14:13:12.463', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4348', 'Tags': '<data-structures><heaps>', 'CreationDate': '2013-04-19T20:54:00.357', 'Id': '11415'},9147:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Is there any data structure that maintain a collection of set (of finite ground set) supporting the following operations? Any sublinear running time will be appreciated?</p>\n\n<ol>\n<li>Init an empty set.</li>\n<li>Add an element to a set.</li>\n<li>Given two set, report whether they intersect.</li>\n</ol>\n', 'ViewCount': '603', 'Title': 'Data Structure for Set Intersection?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-04-26T18:32:47.703', 'LastEditDate': '2013-04-26T13:15:51.383', 'AnswerCount': '4', 'CommentCount': '1', 'Score': '7', 'OwnerDisplayName': 'David Huang', 'PostTypeId': '1', 'Tags': '<data-structures><sets>', 'CreationDate': '2013-04-26T07:32:59.770', 'FavoriteCount': '4', 'Id': '11572'},9148:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have developed two existing data structures and I want to see their performances over a certain algorithm. In this case I use Dijkstra's algorithm with binary and Fibonacci heaps. Just to ask, if I have 100 to 1000 number of vertices in the tested sparse digraphs, how many times should I execute my program for a single n vertices? How do I know that the empirical differences in performance that I've obtained between data structures are not due to chance?</p>\n", 'ViewCount': '69', 'Title': 'performance between the data structures', 'LastEditorUserId': '4736', 'LastActivityDate': '2013-04-29T09:50:43.620', 'LastEditDate': '2013-04-29T09:50:43.620', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7956', 'Tags': '<data-structures><priority-queues><performance><empirical-research>', 'CreationDate': '2013-04-29T04:33:05.927', 'Id': '11651'},9149:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Suppose one has a couple of <em>.wav</em> files with English spoken words, multiple ones for each word, and for each such set there exists a transcription of their right output, the pronunciation as <em>ascii text</em>.</p>\n\n<p>As far as I know, machine learning neural networks use arrays of floats as input and output, and also internally.</p>\n\n<p>What do one do in machine learning in order to convert such \'real world\' data formats/data sets into another data structure that is meaningful and suitable for the machine learning neural networks? </p>\n\n<p>Furthermore, what classifies a particular data structure as \'suitable\', except the fact that it can be expressed as arrays of integers (what fits every digital data)?</p>\n\n<p>(I suppose it could be more sophisticated than stripping the headers and feeding the uncompressed binary data in as integers, or is it?)</p>\n\n<hr>\n\n<p><strong>edit</strong>: in an other SE site\'s <a href="http://stats.stackexchange.com/questions/7224/detecting-a-given-face-in-a-database-of-facial-images">question</a> (regarding how to filter out an image of Justin Bieber), an answer asserts that one <em>"has some method of feature generation to transform the raw images into features that are useful for machine learning purposes"</em>, but it doesn\'t explain how this is done, or how does one begin to create a method for such a feature conversion.</p>\n', 'ViewCount': '128', 'Title': 'How is sound input and output data converted to use with machine learning networks?', 'LastEditorUserId': '31', 'LastActivityDate': '2013-06-01T17:30:47.413', 'LastEditDate': '2013-05-02T07:28:37.360', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '7770', 'Tags': '<data-structures><machine-learning><data-sets>', 'CreationDate': '2013-05-02T06:31:28.187', 'Id': '11720'},9150:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>The exercises in a textbook I studied asks about the best case for shell sort. I have scribbled a derivation for the same along the margins almost two years ago. Basically I don't know if this was my own derivation or one copied from an authoritative source. </p>\n\n<p>I have elaborated upon the same below. Could you let me know if the reasoning is right here?</p>\n\n<ul>\n<li>The least number of comparisons occur when the data is completely sorted.</li>\n<li>For a particular value of the increment, say, $h_i$, each of the $h_i$ sub-sequences require at most one less comparison than the number of elements in the sub-sequence(as insertion sort is used) which is,${N \\over h_i} - 1$ ,where N is the total number of data items.</li>\n<li>For the given data in this situation $h_i \\times \\left (N \\over h_i - 1 \\right ) = N - h_i$ number of comparisons are needed as there are $h_i$ sub-sequences.</li>\n<li>If the increment sequence selected is has $k$ increments(such that $h_k = 1$), the total number of comparisons required would be $C(N) \\ge (N - h_i) + (N - h_2) + ... + (N - h_k) = kN - \\sum h_i = O(N)$</li>\n</ul>\n", 'ViewCount': '369', 'Title': 'Best case analysis for shell sort', 'LastActivityDate': '2013-05-03T02:56:18.547', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '2980', 'Tags': '<algorithms><data-structures><algorithm-analysis><sorting>', 'CreationDate': '2013-05-03T02:56:18.547', 'FavoriteCount': '1', 'Id': '11749'},9151:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '216', 'Title': 'Is there a binary search tree datastructure which can avoid becoming badly weight-balanced?', 'LastEditDate': '2013-05-04T09:38:43.037', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '2152', 'FavoriteCount': '1', 'Body': '<p>This is a follow-up question of "<a href="http://cs.stackexchange.com/questions/342/not-all-red-black-trees-are-balanced">Not all Red-Black trees are balanced?</a>" and "<a href="http://cs.stackexchange.com/questions/421/avl-trees-are-not-weight-balanced">AVL trees are not weight-balanced?</a>".$\\def\\le{\\leqslant}\\def\\ge{\\geqslant}$</p>\n\n<blockquote>\n  <p><strong>Definition:</strong> For a rooted tree $T$ and a vertex $v \\in V(T)$, let $L_T(v)$ be the number of nodes in the left-subtree from $v$, and $N_T(v)$ be the number of nodes in the subtree rooted at $v$. We say that $T$ is <em>$\\mu$-balanced</em>, with $0 \\le \\mu \\le \\frac{1}{2}$, if for every node $v \\in V(T)$ the inequality\n  $$ \\mu \\le \\frac{L_T(v) + 1}{N_T(v) + 1} \\le 1 - \\mu$$\n  holds, and if $\\mu$ is minimal subject to this inequality holding.</p>\n</blockquote>\n\n<p>(These are apparently also known as  <em>weight-balanced</em> trees in some of the literature.) A tree which is $\\mu\'$-balanced for some $\\mu\' &lt; \\mu$, we will say is <strong>&mu;-imbalanced</strong>.</p>\n\n<p>The above-linked posts essentially show that neither <a href="http://en.wikipedia.org/wiki/AVL_tree">AVL trees</a>, nor <a href="http://en.wikipedia.org/wiki/Red%E2%80%93black_tree">Red-Black trees</a>, can be guaranteed to be $\\mu$-balanced for any $\\mu &gt; 0$: that is, for any such $\\mu$, one can provide a sequence of inputs to be inserted so that the resulting tree is $\\mu$-imbalanced.</p>\n\n<p><strong>Question.</strong> Is there any binary search tree structure, with the usual characteristics of $O(\\log n)$ insertion and search time, and some $m &gt; 0$, such that the tree will always be $\\mu$-balanced for some $\\mu &gt; m$?</p>\n', 'Tags': '<data-structures><search-trees>', 'LastEditorUserId': '2448', 'LastActivityDate': '2013-05-04T09:38:43.037', 'CommentCount': '2', 'AcceptedAnswerId': '11758', 'CreationDate': '2013-05-03T13:19:28.613', 'Id': '11756'},9152:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The idea of <a href="http://en.wikipedia.org/wiki/Splay_tree">splay trees</a> is very nice as they move frequently accessed elements to the top, which can gain a considerable speed up in many applications. The drawback is that in the worst case an operation can have $O(n)$ complexity. \n(Although amortized bounds are $O(n\\log n)$ <a href="http://en.wikipedia.org/wiki/Splay_tree#Performance_theorems">if we perform at least $n$ operations</a>.)</p>\n\n<p>Is there a self-adjusting search tree structure that has both? Favoring recently accessed elements and with worst $O(\\log n)$ complexity for a single operation?</p>\n', 'ViewCount': '286', 'Title': 'Is there a binary tree structure with fast access to recently accessed elements and worst $O \\left( \\log n \\right )$ complexity?', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-09-24T06:09:43.897', 'LastEditDate': '2013-05-07T07:14:06.917', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '2448', 'Tags': '<data-structures><binary-trees><search-trees><splay-trees>', 'CreationDate': '2013-05-04T09:54:49.780', 'FavoriteCount': '2', 'Id': '11772'},9153:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We're asked to prove the above mentioned lemma but I having a hard time proving this rigorously.</p>\n\n<p>We did prove that given $n$ values AVL's height is $\\Theta\\left (\\log \\left ( n \\right ) \\right )$ So I thought that after inserting a $\\frac{n}{2}$ values the height of the tree will be at least $\\Theta\\left (\\log \\left ( \\frac{n}{2} \\right ) \\right )$ which and because each isertion we make is now on a tree with at least $\\frac{n}{2}$ and insertion is $\\log \\left (h \\right ) $ where $h$ is the height of the tree.</p>\n\n<p>So for a function $F$ using the previous logic:</p>\n\n<p>$\\begin{align}  F &amp;= \\frac{n}{2} \\times \\log \\left (h \\right ) \n\\\\&amp; \\geq \\frac{n}{2} \\times \\log \\left (\\frac{n}{2} \\right ) \n\\\\&amp;=\\Omega\\left( n\\log \\left (n \\right )  \\right)\n\\end{align}$</p>\n\n<p>But I have a few issues with this </p>\n\n<ul>\n<li>This does feel fishy to me don't know why but it doesn't feel like a good well defined calculus proof :)</li>\n<li>I'm not sure which way to take it in order to prove the upper boud i.e $\\mathcal{O}$</li>\n</ul>\n\n<p>If I haven't given all the required information I'd be glad to.</p>\n", 'ViewCount': '78', 'Title': 'Prove that inserting $n$ sorted values in to an AVL using AVL insertion is $\\Theta\\left (n \\log \\left ( n \\right ) \\right )$', 'LastEditorUserId': '31', 'LastActivityDate': '2013-05-05T16:35:06.797', 'LastEditDate': '2013-05-05T16:35:06.797', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '11808', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8004', 'Tags': '<algorithms><data-structures><binary-trees><search-trees>', 'CreationDate': '2013-05-05T15:30:17.330', 'Id': '11807'},9154:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>i have a binary tree.</p>\n\n<pre><code>         2\n       /   \\\n      3     4\n     / \\     \\\n    5  1      8\n     \\       /\n     6      9\n</code></pre>\n\n<p>I want to change the info part of each node such that the</p>\n\n<pre><code>nodeinfo = nodeinfo + nextInorderNodeInfo\n</code></pre>\n\n<p>so the actual inorder traversal</p>\n\n<pre><code>5, 6, 3, 1, 2, 4, 9, 8\n</code></pre>\n\n<p>will change to</p>\n\n<pre><code>5+6,6+3,3+1,1+2,2+4,4+9,9+8,8+0 \n\n11, 9,  4,  3,  6,  13, 17, 8\n</code></pre>\n\n<p>i need to write a function that will modify the binary tree info parts of each node.</p>\n\n<p>i have done the following</p>\n\n<p>calling </p>\n\n<pre><code>change(root,NULL);\n</code></pre>\n\n<p>function definition</p>\n\n<pre><code>void change(node* n, node *k)\n{\n if (n) \n  { \n    if (n-&gt;left) change(n-&gt;left,n);\n    if (n-&gt;right) change(n,n-&gt;right);\n    n-&gt;info + = k-&gt;info;\n  }\n} \n</code></pre>\n\n<p>in this way i am not able to modify the nodes that are right hand leaf nodes.</p>\n\n<p>can someone give the correct solution..???</p>\n\n<p>thanks in advance</p>\n', 'ViewCount': '60', 'ClosedDate': '2013-05-07T18:49:14.373', 'Title': 'change the info part of each node in binary tree', 'LastActivityDate': '2013-05-06T13:38:45.820', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11831', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8068', 'Tags': '<data-structures>', 'CreationDate': '2013-05-06T10:19:59.003', 'Id': '11825'},9155:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given we change the rule to: </p>\n\n<blockquote>\n  <p>$-s \\ \\ \\leq$ height(left-subtree) - height(right-subtree) $\\leq \\ \\ s$</p>\n</blockquote>\n\n<p>I was wandering whether it's possible and how would it affect the trees' height, would it still be logarithmic? </p>\n\n<p>Would the exact same balancing techniques work? (if we took those methods from a normal AVL and try to convert our modified AVL to a normal AVL running from down to top or to down).</p>\n\n<p>I've tired drawing some schematics in order to find out what would be the minimal number of nodes $m$ for some tree $T$ with height $h$ like we did with a regular AVL but I had a real hard time formalizing it.</p>\n", 'ViewCount': '54', 'Title': "Changing AVL's balance factor to some other $s>2 \\in \\mathbb{N}$", 'LastEditorUserId': '8004', 'LastActivityDate': '2013-05-06T17:15:16.980', 'LastEditDate': '2013-05-06T17:15:16.980', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '11834', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8004', 'Tags': '<algorithms><data-structures><binary-trees><trees>', 'CreationDate': '2013-05-06T14:22:34.710', 'Id': '11832'},9156:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A lot of articles say that hash tree traversal cost to any <em>randomly</em> chosen leaf is $\\mathcal{O}(\\log_2 N)$ ($N$ is a number of leafs) and that is right. If we have a tree of 8 leafs it will take us at most 3 operations to get to any leaf, if we have a tree of 64 leafs it will take us at most 5 operations etc.</p>\n\n<p>But lets say I need to check <strong><em>every</em></strong> leaf sequentially to check if <strong><em>all blocks</em></strong> of a file are correct, then I would need $\\mathcal{O}(N \\log_2 N)$ operations. Or if I would check every second leaf (just left leaf of every pair) I would need $\\mathcal{O}((\\frac{N\\log_2 N}{2}))$ operations. That is, I will need $\\mathcal{O}(\\log_2 N)$ operations for every leaf? Which leads to exponentially growing evaluations curve and it would be better to use simple hash list or hash chain? Am I right?</p>\n\n<p>Or I just don\'t see/know something?</p>\n\n<p><img src="http://i.stack.imgur.com/Mnvvt.jpg" alt=""></p>\n\n<p>*Note, chart has logarithmic scale</p>\n', 'ViewCount': '102', 'Title': 'Sequential hash tree traversal', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-14T18:11:32.220', 'LastEditDate': '2013-05-14T10:59:03.967', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8111', 'Tags': '<data-structures><search-trees><graph-traversal><hash-tables>', 'CreationDate': '2013-05-10T07:16:57.917', 'Id': '11927'},9157:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have been using <a href="http://netlib2.cs.utk.edu/linalg/html_templates/node92.html" rel="nofollow">Harwell Boeing</a> format, also known as Compressed Column Strorage (CCS) in order to store Sparse Matrices. </p>\n\n<p>Could you please suggest me some other way to store/represent sparse matrices?</p>\n', 'ViewCount': '201', 'ClosedDate': '2014-03-14T21:02:58.907', 'Title': 'how to represent Sparse Matrices', 'LastActivityDate': '2014-03-14T09:52:01.290', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1228', 'Tags': '<data-structures><linear-algebra>', 'CreationDate': '2013-05-14T15:31:11.937', 'FavoriteCount': '0', 'Id': '12020'},9158:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>So, Google Code Jam round 1C has just wrapped up, and one of its problems seems rather elusive to me:  <a href="https://code.google.com/codejam/contest/2437488/dashboard#s=p2" rel="nofollow">https://code.google.com/codejam/contest/2437488/dashboard#s=p2</a></p>\n\n<p>A quick summary of the problem is thus:</p>\n\n<p>The Great Wall of China starts out as an infinite line, where the height at all locations is $0$.</p>\n\n<p>Some number of tribes $N$, $N \\le 1000$, will attack the wall the wall according to the following parameters - a start day, $D$, a start strength $S$, a start west-coordinate, $W$, and a start east-coordinate, $E$.  This first attack occurs on day $D$, on range $[W,E]$, at strength $S$.  If there is any portion of the Great Wall within $[W,E]$ that has height $&lt; S$, the attack is successful, and at the end of the day, the wall will be built up such that any segment of it within $[W,E]$ of height $&lt; S$ would then be at height $S$ (or greater, if some other attack that day hit upon the same segment with strength $S\' &gt; S$)</p>\n\n<p>Each tribe will perform up to $1000$ attacks before retreating, and each attack will be determined iteratively from the one before it.  Every tribe has some $\\delta_D$, $\\delta_X$, and $\\delta_S$ that determines their sequence of attacks:  The will wait $\\delta_D \\ge 1$ days between attacks, they will move their attack range $\\delta_X$ units for each attack (negative = west, positive = east), though the size of the range will stay the same, and their strength will also increase/decrease by a constant value after each attack.</p>\n\n<p>The goal of the problem is, given a complete description of the attacking tribes, determine how many of their attacks will be successful.</p>\n\n<p>I managed to code a solution that does work, running in about 20 seconds:  I believe the solution I implemented takes $O(A\\log A + (A+X)\\log X)$ time, where $A =$ the total number of attacks in a simulation (max $1000000$), and $X =$ the total number of unique edge points on attack ranges (max $2000000$).</p>\n\n<p>At a high level, my solution:</p>\n\n<ul>\n<li>Reads in all the Tribe information</li>\n<li>Calculates all the unique $X$-coordinates for attack ranges - $O(A)$</li>\n<li>Represents the Wall as a lazily-updated binary tree over the $X$ ranges that tracks minimum height values.  A leaf is the span of two $X$ coordinates with nothing in-between, and all parent nodes represent the continuous interval covered by their children. - $O(X \\log X)$</li>\n<li>Generates all the Attacks every Tribe will perform, and sorts them by day - $O(A \\log A)$</li>\n<li>For each attack, see if it would be successful ($\\log X$ query time).  When the day changes, loop through all unprocessed successful attacks and update the wall accordingly ($\\log X$ update time for each attack). - $O(A\\log X)$</li>\n</ul>\n\n<p>My question is this:  Is there a way to do better than $O(A\\log A + (A+X)\\log X)$?  Perhaps, is there some strategic way to take advantage of the linear nature of Tribes\' successive attacks?  20 seconds feels too long for an intended solution (Although Java might be to blame for that)</p>\n\n<p>-- Edit --</p>\n\n<p>Looking over other discussions and successful solutions seems to indicate that the solution I\'ve described is pretty much the expected algorithm.  The slow-down in my solution is possibly just due to lazy use of auto-boxing and a pointer-based tree structure, rather than an array-based one - so I suspect that, if a solution does exist, it\'s probably not a whole lot better than what\'s here.  We shall see if anything crops up though</p>\n\n<p>-- Edit2 --</p>\n\n<p>The solution has been posted at last: <a href="https://code.google.com/codejam/contest/2437488/dashboard#s=a&amp;a=2" rel="nofollow">https://code.google.com/codejam/contest/2437488/dashboard#s=a&amp;a=2</a><br>\nIt\'s very much the same as what I have posted here; so I am much more inclined to believe that a more efficient solution does not exist.</p>\n', 'ViewCount': '344', 'Title': 'Google Code Jam Great Wall Problem', 'LastEditorUserId': '7678', 'LastActivityDate': '2013-07-14T05:43:02.910', 'LastEditDate': '2013-07-14T05:43:02.910', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '7614', 'Tags': '<complexity-theory><data-structures><binary-trees>', 'CreationDate': '2013-05-15T15:26:19.040', 'FavoriteCount': '2', 'Id': '12040'},9159:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>What\'s the difference between a stream and a queue? They both have the concept of an ordered set of elements, but tend to have different implementations and a different vocabulary of \'insert\'/\'extract\' (streams) vs. \'enqueue\'/\'dequeue\' (queue). Are these interchangable? Do they suggest different concepts or patterns? If so, what are the differences?</p>\n\n<hr>\n\n<p>Concrete example of \'stream insertion\':\n<a href="http://www.cplusplus.com/reference/ostream/ostream/operator%3C%3C/" rel="nofollow">http://www.cplusplus.com/reference/ostream/ostream/operator%3C%3C/</a></p>\n\n<hr>\n\n<p>Potentially useful conceptual pieces?</p>\n\n<ul>\n<li>Stream as sequence of time-function data: <a href="http://mitpress.mit.edu/sicp/full-text/sicp/book/node69.html" rel="nofollow">http://mitpress.mit.edu/sicp/full-text/sicp/book/node69.html</a></li>\n<li>Stream as IO channel: <a href="http://en.wikipedia.org/wiki/Standard_streams" rel="nofollow">http://en.wikipedia.org/wiki/Standard_streams</a></li>\n</ul>\n', 'ViewCount': '94', 'Title': "What's the difference between a stream and a queue?", 'LastEditorUserId': '8327', 'LastActivityDate': '2013-05-24T03:09:53.050', 'LastEditDate': '2013-05-23T23:22:08.093', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12241', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8327', 'Tags': '<terminology><data-structures>', 'CreationDate': '2013-05-23T22:00:54.237', 'Id': '12237'},9160:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am looking for the English name of the following algorithm:</p>\n\n<p>We are given an array <code>a</code> with numbers and we need to be able to efficiently retrieve the sum of a continuous interval <code>[f,t]</code> of numbers in that array. In order to do that we precompute an array <code>sums</code>(of size <code>size(a) + 1</code>) that stores the sums of the prefixes of the initial array. More formally <code>sums[i] = a[0] + a[1] + ... a[i-1]</code>. This array can be constructed with linear complexity and now in order to compute the sum of the numbers in the interval <code>[f,t]</code>, we simply compute <code>sums[t]-sums[f-1]</code>.</p>\n\n<p>Direct translation of the name of the algorithm(or more precisely the datastructure) that I've seen used in Bulgaria is <code>prefix array</code>, but in my experience direct translation often turns out to be wrong when it comes to algorithms and data structures. </p>\n\n<p>How is this algorithm(or datastructure) called in English?</p>\n", 'ViewCount': '96', 'Title': 'Looking for the English name of algorithm using a precomputed array for interval sum computation', 'LastActivityDate': '2013-05-28T13:37:42.383', 'AnswerCount': '2', 'CommentCount': '0', 'AcceptedAnswerId': '12332', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8403', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-05-28T10:00:24.787', 'Id': '12330'},9161:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In both <a href="https://mitpress.mit.edu/books/introduction-algorithms" rel="nofollow">CLRS</a> (third edition) and Erik Demaine\'s <a href="https://www.youtube.com/watch?v=AjFtTQevtq0" rel="nofollow">lecture</a>, the van Emde Boas tree is defined to store <code>max</code> but not <code>min</code> recursively.  Why store max recursively?</p>\n\n<p>If it is <em>not</em> stored recursively, insert(<em>V</em>, <em>x</em>) can take advantage of the special case where a cluster or summary contains one element (<code>min = max</code>), although I think the advantage gained from this is insignificant.  Keith Schwarz implements it this way in his Archive of Interesting Code.  So why do CLRS and Erik not make use of this optimization in the algorithm?  Is it actually a mistake?</p>\n', 'ViewCount': '142', 'Title': 'van Emde Boas tree: why store max recursively?', 'LastEditorUserId': '8460', 'LastActivityDate': '2013-11-01T10:09:31.133', 'LastEditDate': '2013-06-02T00:10:12.777', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '8460', 'Tags': '<data-structures><search-trees>', 'CreationDate': '2013-05-31T06:01:44.097', 'Id': '12392'},9162:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Bloom filter use a hash function to test membership for S by checking if an item is present of not at the specified position. To mitigate the effect of hash collision, multiple functions are used, yielding probabilistic bound if using universal hash.\nWe can use 10 bits per elements to have 'reasonable' error rate.</p>\n\n<p>If we could build directly a perfect hashing function for the set  S + $\\infty$, where last the element is one not present in S, we could use only 1 bit per element and have perfect recovery.</p>\n\n<p>What are the fundamental reasons why this reasonning is wrong ?</p>\n", 'ViewCount': '182', 'Title': 'Bloom filter and perfect hashing', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:07.190', 'LastEditDate': '2014-04-29T11:35:07.190', 'AnswerCount': '1', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '4469', 'Tags': '<data-structures><hash><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2013-06-03T15:20:55.557', 'Id': '12444'},9163:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>By default, a hash table is unordered.  I have a simple question on the retrieval of elements in a hash table</p>\n\n<blockquote>\n  <p>Can we retrieve elements from a hash table in the same order as they are put inside?</p>\n</blockquote>\n', 'ViewCount': '786', 'Title': 'Retrieving data from hash table ordered by insertion time', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-07-30T08:02:11.913', 'LastEditDate': '2013-07-30T08:02:11.913', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12565', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8596', 'Tags': '<algorithms><data-structures><hash><hash-tables>', 'CreationDate': '2013-06-09T10:50:57.380', 'Id': '12562'},9164:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I am trying to prove by induction  the following theorem:\nUse Induction to prove the following fact: for every integer, $N\\ge  1$ , a BST with $N$ nodes must have at least $\\log( N + 1)$ levels.\nI've proved the base case but I am struggling to figure out how to apply induction to prove for the $K+1$ case. Any suggestions would be wonderful.</p>\n", 'ViewCount': '49', 'Title': 'Proving that a BST with N>=1 nodes will have log(N+1) levels', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-11T16:27:11.647', 'LastEditDate': '2013-06-11T16:27:11.647', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8624', 'Tags': '<graph-theory><data-structures><binary-trees><induction>', 'CreationDate': '2013-06-11T16:18:32.680', 'Id': '12622'},9165:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm looking for a data structure that can work as a priority queue with reasonable maintenance complexities (like $O(\\log n)$ for insertion and deletion) and that has a theoretical unbounded limit for its number of elements (like a tree structure, that is bounded only by the computer's available memory, and unlike a traditional heap, that uses an static array, which is too costly to augment).</p>\n\n<p>The reason is that I'm implementing a program that makes use of a priority queue and I don't know <em>a priori</em> how many elements I'm going to insert in this queue at once, so sometimes I'm out of space to add another element.</p>\n\n<p>There is no way to estimate this number, and to create a huge array to support a static type of queue is a terrible option, as maybe not even a half of it will be used and I'll be short on memory to allocate other objects.</p>\n\n<p>I've heard of something like a <em>Dynamic Heap</em> (or something in the like) that is some sort of linked list of arrays, whose elements are dynamically allocated when needed, but I'm not sure this is the best strategy to follow, moreover, I would like to know if there were other options.</p>\n\n<hr>\n\n<p>Just for the record, I'm implementing a Branch-and-Bound algorithm for solving a linear integer optimization problem, with each node stored on the queue being the abstraction of an active node on the algorithm. The number of active nodes cannot be estimated at any time, so a theoretically unbounded queue would help a lot.</p>\n", 'ViewCount': '166', 'Title': 'Priority queue with ubounded number of elements (i.e., with dynamic storage)', 'LastEditorUserId': '8399', 'LastActivityDate': '2013-06-13T14:11:06.613', 'LastEditDate': '2013-06-13T02:16:56.120', 'AnswerCount': '3', 'CommentCount': '9', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8399', 'Tags': '<data-structures><efficiency><priority-queues>', 'CreationDate': '2013-06-12T14:29:55.157', 'Id': '12637'},9166:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m interested in surprising data structures that extend the things people learned in their first algorithm/data structure course. Usually by offering an additional operation at a very low cost.</p>\n\n<p>I learned that one can create a dynamic array with $O(1)$ access and $O(1)$ amortized append a long time ago. I thought there is no way to remove the amortized cost. I saw a data structure only a few months ago that do both operations in <a href="https://cs.uwaterloo.ca/research/tr/1999/09/CS-99-09.pdf" rel="nofollow">$O(1)$ non-amortized time</a>.</p>\n\n<p>What are some other examples? </p>\n', 'ViewCount': '80', 'ClosedDate': '2013-06-17T21:25:18.140', 'Title': 'Surprising and basic data structures', 'LastActivityDate': '2013-06-17T09:52:53.303', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<data-structures>', 'CreationDate': '2013-06-17T09:20:58.490', 'Id': '12708'},9167:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '213', 'Title': 'Get nodes that are participating in any cycle in a graph', 'LastEditDate': '2013-06-20T12:30:34.290', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8044', 'FavoriteCount': '1', 'Body': '<p>I have a problem that states the following :</p>\n\n<blockquote>\n  <p>Given a cyclic graph , output for each node if the node removes all cycles in the graph.</p>\n</blockquote>\n\n<p>The most trivial way to do this is using a Union-find disjoint set , and for each node , try <strong>not</strong> putting it in the Union-find disjoint set , if there are no cycles , then this node should output "Yes" , otherwise "No".</p>\n\n<p>This approach would take about $\\Theta(N^2)$ time and $\\Theta(N)$ memory.</p>\n\n<p>The problem also stated that $N \\leq 1,000,000$ which would definitely get a TLE (Time Limit Exceeded) Answer on any problem.</p>\n\n<p>So, my question is, What\'s the algorithm that would take $\\Theta(N \\lg N) $ or $\\Theta(N)$ time and $\\Theta(N)$ memory?</p>\n', 'Tags': '<algorithms><graph-theory><data-structures>', 'LastEditorUserId': '8044', 'LastActivityDate': '2013-06-21T23:01:41.097', 'CommentCount': '0', 'AcceptedAnswerId': '12763', 'CreationDate': '2013-06-19T14:55:23.630', 'Id': '12762'},9168:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am studying for my exam and I wanted to do some extra excercises, but I have some problems with solving :) Can anyone please help or give me some advice where to start? Thank you!</p>\n\n<p>We can represent every list of lambda expressions <strong>$x_0, x_1, ..., x_k$</strong> with lambda expression <strong>$[x_0, x_1, ..., x_k]$</strong> , defined as:\n<strong>$[x_0, x_1, ... , x_k] = \\lambda c n. c \\, x_0 \\, (c \\, x_1 \\, (... (c \\, x_k \\, n) ...))$</strong> </p>\n\n<p>Define lambda expression s, satisfying the equation  <strong>$s([\\underline{n_0}, ..., \\underline{n_k}]) = \\underline{n_0 + ... + n_k}$</strong> , where  <strong>$\\underline{n}$</strong> is a Church numeral, representing natural number <strong>$n$</strong>.</p>\n\n<p>Define lambda expression <strong>$r$</strong> , satisfying the equation  <strong>$r([x_0, ..., x_k]) = [x_k,  ..., x_0]$</strong> .</p>\n\n<p>Define lambda expression <strong>$h$</strong> and <strong>$t$</strong> , satisfying the equation  <strong>$h([x_0, ..., x_k]) = x_0 in t([x_0, x_1, ..., x_k]) = [x_1, ..., x_k]$</strong> .</p>\n', 'ViewCount': '102', 'Title': 'defining lambda expressions', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-20T16:27:33.463', 'LastEditDate': '2013-09-20T16:27:33.463', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8749', 'Tags': '<data-structures><lambda-calculus>', 'CreationDate': '2013-06-20T06:19:20.127', 'Id': '12779'},9169:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When analysing treaps (or, equivalently, BSTs or Quicksort), it is not too hard to show that</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[d(k)] \\in O(\\log n)$</p>\n\n<p>where $d(k)$ is the depth of the element with rank $k$ in the set of $n$ keys.\nIntuitively, this seems to imply that also</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\in O(\\log n)$</p>\n\n<p>where $h(T)$ is the height of treap $T$, since</p>\n\n<p>$\\qquad\\displaystyle h(T) = \\max_{k \\in [1..n]} d(k)$.</p>\n\n<p>Formally, however, there does not seem to be an (immediate) relationship. We even have</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(T)] \\geq \\max_{k \\in [1..n]} \\mathbb{E}[d(k)]$</p>\n\n<p>by Jensen\'s inequality. Now, one can show expected logarithmic height via tail bounds, using more insight into the distribution of $d(k)$.</p>\n\n<p>It is easy to construct examples of distributions that skrew with above intuition, namely extremely asymmetric, heavy-tailed distributions. The question is, can/do such occur in the analysis of algorithms and data structures?</p>\n\n<p>Are there example for data structures $D$ (or algorithms) for which</p>\n\n<p>$\\qquad\\displaystyle \\mathbb{E}[h(D)] \\in \\omega(\\max_{e \\in D} \\mathbb{E}[d(e)])$?</p>\n\n<p>Nota bene:</p>\n\n<ul>\n<li><p>Of course, we have to interpret "depth" and "height" liberally if we consider structures that are not trees. Based on the posts Wandering Logic links to, "Expected average search time" (for $1/n \\cdot \\sum_{e \\in D} \\mathbb{E}[d(e)]$) and "expected maximum search time" (for $\\mathbb{E}[h(D)]$) seem to be used.</p></li>\n<li><p>A <a href="http://math.stackexchange.com/q/426998/3330">related question</a> on math.SE has yielded an interesting answer that may allow deriving useful bounds on $\\mathbb{E}[h(D)]$ given suitable bounds on $\\mathbb{E}[d(e)]$ and $\\mathbb{V}[d(e)]$.</p></li>\n</ul>\n', 'ViewCount': '93', 'Title': 'Can expected "depth" of an element and expected "height" differ significantly?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-23T20:06:53.820', 'LastEditDate': '2013-06-23T16:04:31.497', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12833', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '98', 'Tags': '<algorithms><data-structures><algorithm-analysis><probability-theory><average-case>', 'CreationDate': '2013-06-22T16:23:33.990', 'Id': '12830'},9170:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '254', 'Title': 'Why do Bloom filters work?', 'LastEditDate': '2014-04-29T11:35:41.260', 'AnswerCount': '3', 'Score': '4', 'OwnerDisplayName': 'user220201', 'PostTypeId': '1', 'OwnerUserId': '8842', 'Body': "<p>Let's say I am using Bloom filters to create a function to check if a word exists in a document or not.  If I pick a hash function to fill out a bit bucket for all words in my document. Then if for a given number of words, wouldn't the whole bit bucket be all 1s? If so then checking for any word will return true? What am I missing here? </p>\n", 'Tags': '<data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:41.260', 'CommentCount': '0', 'AcceptedAnswerId': '12838', 'CreationDate': '2013-06-22T13:12:57.470', 'Id': '12834'},9171:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Working on a project for my Data Structures class. </p>\n\n<p>I\'ve implemented a Red/Black tree in Java. One of the operations required of the data structure is "find a node which has a key of at least n". The leaves of the tree are the sentinel node.</p>\n\n<p>My initial thought was using the regular search with modifications:</p>\n\n<ol><li>Search for a node with key == n</li><li>If result == sentinel, call getSuccessor on parent until a node with key > n is found</li></ol>\n\n<p>I -think- this is $O(m*lgm)$ ($m$ being number of nodes in the tree) at the worst case. </p>\n\n<p>Inspired by the getSuccessor code - if right subtree of target node is empty, find smallest ancestor which has a left child also an ancestor - was wondering if there is a better way to do this.</p>\n\n<p>Would appreciate any advice; thanks. </p>\n', 'ViewCount': '84', 'Title': 'Find node with key of at least n in a binary search tree', 'LastActivityDate': '2013-06-30T08:45:47.077', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '12975', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8929', 'Tags': '<data-structures><binary-trees><search-trees>', 'CreationDate': '2013-06-29T21:31:49.023', 'Id': '12971'},9172:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given a valid binary search tree whose keys are unique real numbers, and a set of $k$ pointers to the $k$ minimum elements in the tree, will the BST property be maintained if I replace all $k$ elements with the average of the $k$ elements? </p>\n\n<p>The BST property as given in Corman:</p>\n\n<blockquote>\n  <p>Let $x$ be a node in a binary search tree. If $y$ is a node in the\n  left subtree of $x$, then $y.key \\leq x.key$. If $y$ is a node in the\n  right subtree of $x$, then $y.key \\geq x.key$.</p>\n</blockquote>\n\n<p>I've tried this with a few test cases for $k=3$ and a few different trees, and it seems to hold, but I'm not sure if it actually does <em>and how I could prove it</em>.</p>\n", 'ViewCount': '122', 'Title': 'Binary Search Tree: Replace $k$ min elements with their average', 'LastEditorUserId': '98', 'LastActivityDate': '2013-06-30T20:46:51.233', 'LastEditDate': '2013-06-30T19:15:30.497', 'AnswerCount': '2', 'CommentCount': '3', 'AcceptedAnswerId': '12998', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '6728', 'Tags': '<data-structures><proof-techniques><search-trees>', 'CreationDate': '2013-06-30T09:09:43.920', 'Id': '12984'},9173:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>From what I have come to understand, the best way to implement it is to use the suffix  array $S$ of the string $w$ and its LCP-array (Longest Common Prefix) $L$.</p>\n\n<p>The answer can be obtained by </p>\n\n<p>$$ \\sum_{i=1}^{|w|} \\left( |S[i]| -L[i-1] \\right).$$</p>\n\n<p>What I don't get is how and why is this working?</p>\n\n<p>I would be very grateful if someone explained this.</p>\n", 'ViewCount': '914', 'Title': 'Number of distinct substrings in a string', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-19T07:11:22.433', 'LastEditDate': '2013-07-19T07:11:22.433', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13241', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9073', 'Tags': '<algorithms><data-structures><strings><substrings><suffix-array>', 'CreationDate': '2013-07-07T19:30:34.867', 'Id': '13140'},9174:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>As I think of data structures I studied and dealt with, they are all optimized to retrieve/put  a random element, to perform optimally based on unspoken assumption that each element has equal odds of being asked for (e.g. Red-Black trees).</p>\n\n<p>By the nature of my program, I need to maintain an online dictionary of items that typically serves items that were added last.</p>\n\n<p>That is, the later an item has been added, the higher the likelihood of it being retrieved back in the nearest future.</p>\n\n<p>Speaking more formally, let's define a set $S$ of pair $(k_i, d_i)$, where $k_i \\in K $ and $K$ has a comparison operator $\\leq$ defined, $d_i \\in D$. Let $p(k)$ be the probability of our need to retrieve pair $(k, d)$.</p>\n\n<p>What is an efficient way to store $S$ with regard to function $p$ ?</p>\n", 'ViewCount': '117', 'Title': '"Last Come => More Relevant" data structures', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-13T10:12:31.957', 'LastEditDate': '2013-07-12T07:34:39.067', 'AnswerCount': '3', 'CommentCount': '5', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '9134', 'Tags': '<data-structures><search-trees>', 'CreationDate': '2013-07-11T17:29:37.650', 'Id': '13231'},9175:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have about 500,000,000 64-bit integers, so these numbers could be very large.\nI want to sort them as quickly as possible. I have a couple of questions:</p>\n\n<ol>\n<li><p>What data structure do you suggest for storing this data?</p></li>\n<li><p>What algorithm do you suggest for sorting these numbers?</p></li>\n</ol>\n\n<p>My main restriction is speed.</p>\n', 'ViewCount': '224', 'Title': 'How should I store and sort a large number of 64-bit integers?', 'LastEditorUserId': '1055', 'LastActivityDate': '2013-07-17T20:07:11.840', 'LastEditDate': '2013-07-17T05:13:23.943', 'AnswerCount': '2', 'CommentCount': '2', 'AcceptedAnswerId': '13291', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '1055', 'Tags': '<algorithms><data-structures><sorting><integers><performance>', 'CreationDate': '2013-07-15T15:49:46.370', 'Id': '13290'},9176:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '139', 'Title': 'Reachability queries on a tree in $O(1)$ time with $O(n+m)$ time preprocessing', 'LastEditDate': '2013-07-18T07:18:05.990', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9214', 'FavoriteCount': '3', 'Body': '<p>I am given an undirected tree $T$ in the usual graph theoretic sense. Given a vertex $v$ and an edge $(v,u)$ incident to $v$, I need to answer queries of the form <em>return any leaf of $T$ that is reachable from $v$ with a path including $(u,v)$, and no other edges incident to $v$?</em> More informally, the restriction is that when the edge is given, we can only proceed in to that direction. </p>\n\n<p>I can simply perform a DFS and return a leaf found. I think this would take $O(d)$ time, where $d$ is the diameter of $T$. However, I\'d like to answer a query in $O(1)$ time. Moreover, I\'d only like to allow linear preprocessing time. My idea for achieving this was to use a DFS, label leaves, and then label edges when the search backtracks. This idea might work with some additional effort, but I\'m really unsure about the details.</p>\n\n<p>"Graph reachability" turned up some results, but maybe they are dealing with more complex problems. I\'m happy with any method that uses $O(n+m)$ preprocessing time and answers the queries in $O(1)$ time.</p>\n', 'Tags': '<algorithms><data-structures><trees>', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-07-18T07:18:05.990', 'CommentCount': '4', 'AcceptedAnswerId': '13320', 'CreationDate': '2013-07-17T15:31:23.960', 'Id': '13316'},9177:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm currently working on my masters thesis, and it's about clustering on graphs. I'm working with an idea using ants to solve the problem. I'm currently working on the implementation and am wondering exactly how well to represent the edges of the graph. </p>\n\n<p>Each edge is augmented with certain information such as its pheromone value and the number of times an ant has visited that edge. I'll be working with undirected graphs, which can be end up being pretty huge (over a million vertices) and I was wondering what is the most efficient way for me to store the edges and look them up? I was thinking of sticking to a convention and store endpoints according to the one which has a lower vertex id for $v_1$ and the higher one for $v_2$ ($v_1$ and $v_2$ are the endpoints of the edge in the data structure). But I'm wondering how would I perform a look up in this case?</p>\n\n<p>There is a mapping I came up with from the adjacency matrix to the edge array, but that works only if the underlying graph is a complete graph. So I came here for some suggestions as to how I should proceed because I need my lookup to be efficient while at the same time I don't want to blow up the storage space for the edges as the graphs will be huge.</p>\n", 'ViewCount': '560', 'LastEditorDisplayName': 'user742', 'Title': 'Data structure for storing edges of a graph', 'LastActivityDate': '2013-09-20T09:27:38.553', 'LastEditDate': '2013-09-20T09:27:38.553', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '13367', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '5020', 'Tags': '<graph-theory><data-structures><evolutionary-computing>', 'CreationDate': '2013-07-20T19:03:15.100', 'Id': '13364'},9178:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I want to begin by saying that this is NOT a homework question. I am reading Introduction to Algorithms - the famous CLRS text to become a better programmer. I am trying to solve the problems and exercises given in the book by myself. </p>\n\n<p>I am trying to solve <strong>Excercise 10.1-2</strong> from <strong>Chapter 10 Elementary Data Structures</strong> from CLRS Second Edition. Here is what its states:</p>\n\n<blockquote>\n  <p>Explain how to implement two stacks in one array <em>A[1..n]</em> in such a way that neither stack overflows unless the total number of elements in both stacks together is <em>n</em>. The PUSH and POP operations should run in <em>O(1)</em> time.</p>\n</blockquote>\n\n<p>The solution that I have come up with so far is:</p>\n\n<blockquote>\n  <p>Let array <em>A[1..n]</em> implement two stacks: <em>S1[1..i]</em> and <em>S2[i..n]</em>. </p>\n  \n  <p>For the <em>PUSH-S1</em> and <em>PUSH-S2</em> operations, if the stack is 'full' then start pushing elements into the <em>other</em> stack (eg. if stack <em>S1</em> is full when a new element is trying to be pushed in, then push that element into stack <em>S2</em> and vice versa). </p>\n</blockquote>\n\n<p>The problem with this approach is I will not be able to <em>POP-S1</em> or <em>POP-S2</em> reliably as there is no way of 'remembering' which element belongs to which stack. If the elements of the stack are <em>(key,value)</em> pairs, the key being the stack number, then to pop an element I would have to search, in the worst case, i or (n-i) times - which will be <em>O(n)</em> (feel free to correct me if I am wrong here), which would not be <em>O(1)</em>. </p>\n\n<p>I have been banging my head on the question for quite a while now. Am I on the right track? Can someone give my possible pointers for solving this problem?</p>\n\n<p>In general, how should I 'think' about these problems? Or can only really intelligent people solve these types of problems? Will tackling/solving problems like these (i.e. gaining experience) help me become better at this?</p>\n\n<p>I await enlightenment.</p>\n", 'ViewCount': '1809', 'Title': 'How to implement two stacks in one array?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-07-31T15:48:39.903', 'LastEditDate': '2013-07-31T15:48:39.903', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '9057', 'Tags': '<data-structures><arrays><stack>', 'CreationDate': '2013-07-28T14:03:22.530', 'Id': '13477'},9179:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>My similarity search seminar topic are <a href="https://en.wikipedia.org/wiki/M-tree" rel="nofollow">M-trees</a>. I would like to give some examples about where they are practically applied, but I can\'t find anything googling. </p>\n\n<p>Does someone know if M-trees are still used and what for? I am interested in answers regarding both practice and applied research.</p>\n', 'ViewCount': '96', 'Title': 'Where are M-Trees applied in practice?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-07-30T18:04:07.047', 'LastEditDate': '2013-07-30T16:40:04.920', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '9407', 'Tags': '<data-structures><search-trees><applied-theory>', 'CreationDate': '2013-07-29T16:39:35.700', 'Id': '13493'},9180:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><img src="http://i.stack.imgur.com/oB6Zq.jpg" alt="enter image description here"></p>\n\n<p>From the above image, while trying to maintain an <a href="http://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL tree data structure</a>, how would the tree look after inserting the value 10? Also, if anyone has any suggestions or simple method of rotating, feel free to share. I am a bit lost with this idea of maintaining a certain height in this data structure.</p>\n', 'ViewCount': '200', 'Title': 'How does insertion work in an AVL tree?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-08T18:19:50.890', 'LastEditDate': '2013-08-05T20:31:26.483', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13582', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9490', 'Tags': '<data-structures><trees><search-trees>', 'CreationDate': '2013-08-03T05:50:15.613', 'Id': '13581'},9181:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '242', 'Title': 'What are efficient data structures for inserting and accessing elements?', 'LastEditDate': '2013-08-05T20:40:21.293', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9085', 'FavoriteCount': '1', 'Body': '<p>Is there a data structure to keep a list of elements (not necessarily sorted) that performs the Access (by index) and Insert operations in a reasonable asymptotic time?</p>\n\n<p>When I say "reasonable time", I mean that it should be equal to or better than $O(\\log N)$.</p>\n\n<p>I\'m looking for a structure similar to a <a href="https://en.wikipedia.org/wiki/Dynamic_array" rel="nofollow">dynamic array</a>, but I need a better behavior in the Insertion operation. When the array size grows, the time grows exponentially (Except at the end of the array).</p>\n', 'Tags': '<data-structures><efficiency>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-06T09:02:13.103', 'CommentCount': '4', 'AcceptedAnswerId': '13602', 'CreationDate': '2013-08-04T16:00:01.230', 'Id': '13598'},9182:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '280', 'Title': 'What linked list data structure adjustments would give me fast random lookup?', 'LastEditDate': '2013-08-07T13:56:23.087', 'AnswerCount': '2', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '7284', 'FavoriteCount': '1', 'Body': "<p>I am presently using an doubly linked list (C++ <code>std::list</code>) to hold a bunch of records that each have a unique integer identifier.  The linked list is created in sorted order such that in the list, the next item always has a larger unique identifier than its predecessor.</p>\n\n<p>The issue I'm facing is that occasionally I need to be able to insert an item quickly into its  relative sorted position and using a plain linked list means this operation is $O(n)$ which is causing performance issues for me.  Generally, this would mean I want to use something like a binary tree (C++ <code>std::map</code>), however, I am also depending upon the following feature of a doubly linked list for good performance:</p>\n\n<ul>\n<li>Ability to splice a contiguous section out of one linked list into another in $O(1)$ time. (Amortized $O(1)$ or $O(\\log \\log n)$ would be good enough.)</li>\n</ul>\n\n<p>One feature of my data that I would like to leverage is that I often have long ranges of contiguous records where each one's unique integer is exactly one more than its predecessor.  When searching for an item's relative sorted position, it would always be outside such contiguous records since there are no duplicate identifiers.</p>\n\n<p>I'd like to find a replacement data structure or augmentation to a doubly linked list that will allow me to continue to splice whole sections from one list to another in constant time but allow me to locate the sorted position in which to insert a new record in better than $O(n)$ time.</p>\n\n<p>Other operations include forward and backward iteration across the items. The record indexes begin at zero and grow upwards towards 64 bits, generally sequentially, and the code works well in such cases. Occasionally some records are not available before subsequent ones, it is the insertion of these missing records that causes the performance issues now. </p>\n\n<p>One possible approach that occurs to me is to cache the location of several indexes.  The cache would get invalidated whenever a splice removes items that might overlap the cached entries.  With this cache, instead of doing a linear search, the search could instead begin from the cache point iterator whose unique index is closest to the one whose position is being  searched for.  However, I'd like to more fully utilize the feature of the contiguous records.  I also thought about a hierarchical linked list where I have a top level linked list of contiguous regions, where each region is a linked list of records that are consecutive, but I didn't see a clean way to adapt a linked list to provide this functionality.  Perhaps something like this has been done before?  I find skip lists to be close, but do not see the splice() functionality, plus a generic skip list would not leverage the fact that insertion never occurs within contiguous records.</p>\n", 'Tags': '<data-structures><time-complexity><binary-trees><linked-lists>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-08-08T10:37:59.160', 'CommentCount': '3', 'AcceptedAnswerId': '13646', 'CreationDate': '2013-08-05T23:00:36.700', 'Id': '13620'},9183:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to know how to calculate total number of nodes in a perfect balanced binary tree with $n$ nodes in the last level. I know the answer is $2\\cdot 2^{\\log n} - 1$. Just curious how this can be calculated</p>\n', 'ViewCount': '564', 'Title': 'What is the size of the Perfect binary tree with n nodes in last level', 'LastEditorUserId': '157', 'LastActivityDate': '2013-08-06T00:48:19.433', 'LastEditDate': '2013-08-06T00:48:19.433', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '9536', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2013-08-05T23:48:11.370', 'Id': '13621'},9184:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have </p>\n\n<ol>\n<li>two red-black trees $T_1$ of black height $H_1$ and $T_2$ of black height $H_2$</li>\n<li>such that all the nodes $N$ belonging to $T_1$ are less than (in value) all the nodes $N$ of $T_2$</li>\n<li>and a key $K$ such that $K$ is greater than all the nodes of $T_1$ and less than all the nodes of $T_2$.</li>\n</ol>\n\n<p>I wanted to devise an algorithm to combine $T_1$, $K$ and $T_2$ into a single red-black tree $T$.</p>\n\n<p>I could delete each element from either $T_1$ or $T_2$ and put it in other tree. But that will give me an algorithm of time-complexity $2^{H_1}$ or $2^{H_2}$ (depending on the tree from which I have deleted the elements from). I would like to have an algorithm which is $O(\\max(H_1,H_2))$.</p>\n\n<blockquote>\n  <p>Definitions : <br><br> Black-height is the number of black-colored nodes in\n  its path to the root.</p>\n  \n  <p>Red-Black tree : A binary search tree, where each node is coloured\n  either red or black and</p>\n  \n  <ul>\n  <li>The root is black All NULL nodes are black</li>\n  <li>If a node is red, then both its children are black </li>\n  <li>For each node, all paths from that node to descendant NULL nodes have the same number of black nodes.</li>\n  </ul>\n</blockquote>\n', 'ViewCount': '561', 'Title': 'Joining two red-black trees', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-12T23:42:30.383', 'LastEditDate': '2013-08-11T13:20:15.663', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6699', 'Tags': '<algorithms><data-structures><search-trees>', 'CreationDate': '2013-08-11T07:06:44.943', 'Id': '13703'},9185:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '196', 'Title': 'Data structure for maintaining large space-efficient filtered array', 'LastEditDate': '2013-08-14T06:17:49.983', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9639', 'FavoriteCount': '1', 'Body': "<p>How does one implement a space efficient data structure that satisfies the requirements below?</p>\n\n<ol>\n<li>You have a large array</li>\n<li>You have a filter which tells you which elements in that large array are to be deleted</li>\n<li>Lookup of i'th element in the array should be constant time</li>\n<li>The space formerly occupied by deleted elements in the array should be available for further use.</li>\n</ol>\n\n<p>I've explored approaches ranging from bloom filters to trees, but they violate one requirement or the other.</p>\n\n<p>EDIT: Further clarification.</p>\n\n<p>Space-efficient: Any space not used by elements of the array that have not been deleted by the filter, which we can term extra space or space for book-keeping, should be $O(1)$. We can probably relax this to allow any solution that gets close to $O(1)$ extra space. $O(n)$ extra space is undesirable.</p>\n\n<p>Array in the context of this problem is an array-like collection where you can get the $i$th element in constant time. You lookup by index here. </p>\n\n<p>I suspect amortized performance bounds should be fine.</p>\n\n<p>The filter takes each element of collection and returns 0/1 corresponding to delete/no delete.</p>\n", 'Tags': '<algorithms><data-structures>', 'LastEditorUserId': '9639', 'LastActivityDate': '2013-08-14T06:17:49.983', 'CommentCount': '7', 'AcceptedAnswerId': '13738', 'CreationDate': '2013-08-13T11:44:30.220', 'Id': '13732'},9186:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need to keep a collection on integers in the range 0 to 65535 so that I can quickly do the following:</p>\n\n<ul>\n<li>Insert a new integer</li>\n<li>Insert a range of contiguous integers</li>\n<li>Remove an integer</li>\n<li>Remove all integers below an integer</li>\n<li>Test if an integer is present</li>\n</ul>\n\n<p>My data has the property that it often contains runs of integers in the collection.  For example, the collection might at one point in time be:</p>\n\n<pre><code>{ 121, 122, 123, 124, 3201, 3202, 5897, 8912, 8913, 8914, 18823, 18824, 40891 }\n</code></pre>\n\n<p>The simplest approach is just to use a balanced binary tree like the C++ std::set, however, using that, I am not leveraging the fact that I often have runs of numbers.  Perhaps it would be better to store a collection of ranges?  But that means a range needs to be able to be broken up if an integer in its middle is removed, or joined together if the space between two ranges in filled in.</p>\n\n<p>Are there any existing data structures that would be well suited for this problem?</p>\n', 'ViewCount': '402', 'Title': 'What data structure would efficiently store integer ranges?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-08-26T17:37:25.270', 'LastEditDate': '2013-08-26T11:07:37.533', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '7284', 'Tags': '<data-structures><efficiency><search-trees><integers>', 'CreationDate': '2013-08-22T20:22:55.240', 'Id': '13874'},9187:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If $u,v \\in \\mathbb{R}^d$ are two $d$-dimensional vectors, say that $u\\le v$ if $u_i \\le v_i$ for all $i=1,\\dots,d$.  In other words, comparisons on vectors will be pointwise.</p>\n\n<p>Let $S,T$ be two subsets of $\\mathbb{N}^d$ of size $m$.  Is there an efficient way to test whether there exists $s\\in S, t \\in T$ such that $s\\le t$?  The naive algorithm does $m^2$ comparisons between vectors; is there a more efficient algorithm?</p>\n\n<p>If $d=1$, this is very easy: we simply find the smallest element in $S$ and the largest element in $T$, which can be done with $O(m)$ comparisons.  But already when $d=2$, it seems much harder.  Any ideas?</p>\n', 'ViewCount': '132', 'Title': 'Comparing sets of vectors', 'LastEditorUserId': '755', 'LastActivityDate': '2013-08-29T22:53:35.173', 'LastEditDate': '2013-08-26T02:14:02.180', 'AnswerCount': '1', 'CommentCount': '7', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-structures><computational-geometry><linear-algebra>', 'CreationDate': '2013-08-26T02:08:59.753', 'FavoriteCount': '1', 'Id': '13927'},9188:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was wondering how many binary trees we have with height of $h$ with $n$ nodes(another question is how many binary trees we have with height $ \\lfloor{lg (n)}\\rfloor$).</p>\n\n<p>Edit: I forgot to add the number of nodes.</p>\n', 'ViewCount': '96', 'Title': 'Number of binary trees with given height', 'LastEditorUserId': '9909', 'LastActivityDate': '2013-08-31T09:32:48.273', 'LastEditDate': '2013-08-31T09:32:48.273', 'AnswerCount': '2', 'CommentCount': '5', 'AcceptedAnswerId': '14049', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9909', 'Tags': '<data-structures><counting>', 'CreationDate': '2013-08-30T20:25:56.387', 'Id': '14043'},9189:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m interested in the first appearance in the CS literature of the data structure described <a href="http://community.topcoder.com/tc?module=Static&amp;d1=tutorials&amp;d2=lowestCommonAncestor#Segment_Trees" rel="nofollow">here</a> which is used to answer Range Queries. Although I have come across the same data structure many times with the name <em>segment tree</em> (mostly in algorithmic competition sites), it seems that the same name is also used for <a href="http://en.wikipedia.org/wiki/Segment_tree" rel="nofollow">this</a> data structure which is quite different from the first one and has application in computational geometry.</p>\n\n<p>I\'d like to know who introduced the data structure described in the TopCoder tutorial and if another name was initially used as there seems to be a confusion between the two data structures that go by the same name.</p>\n', 'ViewCount': '130', 'Title': 'Origins of the Segment tree data structure', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-10T15:14:39.863', 'LastEditDate': '2013-09-09T09:34:27.603', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10010', 'Tags': '<reference-request><data-structures><trees>', 'CreationDate': '2013-09-06T12:32:28.053', 'Id': '14172'},9190:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>When inserting an item into a splay tree, rotations are performed in pairs based on either a zig-zag or zig-zig pattern. When there is an odd number of rotations to be performed, one could either do the extra rotation beginning at the leaf or save the extra rotation and do it at the root. Does it matter?</p>\n\n<p>For example, in the attached image I insert a 4 into a BST, and "splay it"\nit to the root. On the top of the figure, I first locate the zig-zig pair \nat the leaf node and perform the zig-zag splay from the bottom leaving a final right rotation at the root. At the bottom of the figure, I first do the odd rotation starting from the leaf, and the then do a zig-zig splay to the root.</p>\n\n<p>Which is correct? Or  will both lead to the usual splay-tree performance?</p>\n\n<p><img src="http://i.stack.imgur.com/CNSAZ.png" alt="two ways to splay for odd number of rotations"></p>\n', 'ViewCount': '186', 'Title': 'Splay tree with odd number of rotations', 'LastEditorUserId': '2205', 'LastActivityDate': '2014-03-01T08:50:13.433', 'LastEditDate': '2013-09-08T10:02:30.483', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '10033', 'Tags': '<data-structures><binary-trees><splay-trees>', 'CreationDate': '2013-09-07T20:35:21.943', 'FavoriteCount': '1', 'Id': '14200'},9191:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '134', 'Title': 'Set combination data structure (And storage complexity)', 'LastEditDate': '2013-09-09T12:06:57.553', 'AnswerCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10038', 'FavoriteCount': '1', 'Body': '<p>I have already posted this question on <a href="http://stackoverflow.com/questions/18669727/set-combination-data-structure-and-storage-complexity">Stackoverflow</a>, but I\'m starting to think that this is the right place.</p>\n\n<p>I have a problem where I am required to associate unique combinations from a set (unique subsets) to a given value. e.g.: Let <code>S={a, b, c, d}</code>, the required data structure should perform the following:</p>\n\n<p><strong>Key -> value</strong></p>\n\n<pre><code>{a,b} -&gt; value1\n{a,c} -&gt; value2\n{c,d} -&gt; value3\n</code></pre>\n\n<ul>\n<li>Property 1: The length of the set in the key is fixed (In this\nexample it\'s fixed to 2).</li>\n<li>Property 2: The data structure does not\nhold all possible subsets of S.</li>\n</ul>\n\n<p><strong>Question 1</strong>: What is the storage complexity of a simple Map holding these values? O(N!)? (given that |S| = N and it\'s not fixed)</p>\n\n<p><strong>Question 2</strong>: Is there any efficient data structure that could store such elements? (The most important efficiency would be required in storage complexity)</p>\n', 'Tags': '<data-structures><space-complexity><sets>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-09T20:20:08.697', 'CommentCount': '4', 'AcceptedAnswerId': '14221', 'CreationDate': '2013-09-08T05:34:12.877', 'Id': '14208'},9192:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>We have a tree with $N$ nodes. $N \\le 10^5.$ Each node has a value $V$ associated with it. Now we have $Q$ $(\\le 10^5)$ queries. There are two types of queries:</p>\n\n<ol>\n<li><p>Q X Y: in this type of query we have to decrement each node of the subtree rooted at $X$ by value $Y$.</p></li>\n<li><p>C X: in this type of query we have to count the number of nodes in the subtree rooted at $X$ that are $\\le 0$.</p></li>\n</ol>\n\n<p>Here is my approach:\nI can perform the update query in $O(N)$ along with some sort of lazy propogation.\nThe count query can be thus performed in constant time.</p>\n\n<p>But I am more than sure that there will be a better approach to handle update queries. Possibly a $O(\\log N)$ bound for both updates and counts. Is there a way I could map this tree into a segment tree or a bit.</p>\n\n<p>Any approach would be appreciated.</p>\n', 'ViewCount': '174', 'Title': 'Queries on Tree', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-11-08T03:14:17.253', 'LastEditDate': '2013-09-08T10:00:49.680', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10039', 'Tags': '<data-structures><trees>', 'CreationDate': '2013-09-08T09:24:29.977', 'FavoriteCount': '1', 'Id': '14210'},9193:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have an array of size $N$ $(N \\leq 10^5)$. I need to perform two types of operations on the array.</p>\n\n<ol>\n<li><p>Decrease elements in range $[L,R]$ by $X$.</p></li>\n<li><p>Count the number of negative elements in range $[L,R]$.</p></li>\n</ol>\n\n<p>There are $10^5$ queries. All I can think about is using a segment tree with lazy propagation, but I believe there would be a better method. The segment tree method will probably time out because in case of alternate update/count queries, this will be close to linear.</p>\n', 'ViewCount': '319', 'Title': 'How to query and update ranges of arrays?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-06T05:14:47.383', 'LastEditDate': '2013-09-11T19:20:40.383', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10039', 'Tags': '<data-structures><trees>', 'CreationDate': '2013-09-09T07:08:53.333', 'FavoriteCount': '2', 'Id': '14228'},9194:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Can anyone explain heavy light decomposition of trees or give a resource to read it from? I have already gone through <a href="http://ipsc.ksp.sk/2009/real/solutions/l.html" rel="nofollow">http://ipsc.ksp.sk/2009/real/solutions/l.html</a> which is the best i could find but still could not understand its working completely.</p>\n', 'ViewCount': '680', 'Title': 'Explanation of Heavy light decomposition', 'LastEditorUserId': '9694', 'LastActivityDate': '2014-04-09T01:32:25.857', 'LastEditDate': '2013-09-10T22:46:08.853', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '9694', 'Tags': '<data-structures><trees>', 'CreationDate': '2013-09-10T21:16:59.930', 'FavoriteCount': '1', 'Id': '14259'},9195:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In Haskell, are datatypes converted to the "Church encoding" i.e. folding the data type.  For example, given </p>\n\n<pre><code>data N = Z | S N\n</code></pre>\n\n<p>in Haskell, it can be converted to its church encoding by </p>\n\n<pre><code>foldN Z z s = z\nfoldN (S n) z s = s (foldN z s n)\n</code></pre>\n\n<p>Where if we do foldN m, we get the church encoding:</p>\n\n<pre><code>\\z s . s (  .... s n ... )\n</code></pre>\n\n<p>In Proofs and Types, Girard shows how this works for any inductive datatype.  There are two questions I have: (1) is this actually how Haskell treats datatypes and (2) what is the equivalent construction for coinductive datatypes.</p>\n', 'ViewCount': '57', 'Title': 'Implementation of datatypes in Haskell?', 'LastActivityDate': '2013-09-11T00:13:35.923', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4775', 'Tags': '<data-structures><term-rewriting><interactive-proof-systems>', 'CreationDate': '2013-09-11T00:13:35.923', 'FavoriteCount': '0', 'Id': '14261'},9196:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I'm having trouble finding materials on what 'finger search' is, in the context of a red black tree.</p>\n\n<p>Even Wikipedia has a very short page about that, could you refer me or explain what kind of search is that.</p>\n", 'ViewCount': '119', 'Title': 'finger search on a red black tree', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-16T07:28:09.240', 'LastEditDate': '2013-09-16T07:28:09.240', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '14292', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9851', 'Tags': '<terminology><data-structures><search-trees>', 'CreationDate': '2013-09-13T17:11:10.787', 'Id': '14291'},9197:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>If we have $n$ elements $s_1, \\dots, s_n$ and build a kind of treap (tree-heap) out of it. Each $s_k$ has a priority, which is an integer in $\\{ 1, 2, 3 \\dots, \\lceil \\log n \\rceil\\}$. Since the priorities will have duplicates, I just want the treap the verify that for each node $s_k$, all the nodes in its right and left subtrees have smaller priority. </p>\n\n<p>Is there a way to find the expected depth of this tree?</p>\n', 'ViewCount': '96', 'Title': 'Expected depth of modified kind of treap', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-18T09:06:30.243', 'LastEditDate': '2013-09-18T09:06:30.243', 'AnswerCount': '0', 'CommentCount': '8', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10186', 'Tags': '<algorithm-analysis><data-structures><search-trees><heaps>', 'CreationDate': '2013-09-17T21:35:36.280', 'Id': '14391'},9198:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a Set $S$ of objects, a set $U$ of users and a map $c: U \\rightarrow S^{\\prime}$, where<br>\n$S^{\\prime} \\subset S$ and $\\emptyset \\notin S^{\\prime}$.</p>\n\n<p>Every time I add a new entry to $c$, i.e. adding a new user $u$ with her associated subset of $S^{\\prime}$, I need to get the list of other users with whom she covers $S$. However, I don't need to get all possible covers. I only need to get with how many $x$ other users she covers $S$, at most. In other words, I need to find who's complementing each other to cover $S$ 2 by 2, 3 by 3, ... , $x$ by $x$.</p>\n\n<p>Is there an efficient method to do it? Or an efficient data structure? If no, is there any trick I can make to get good results? e.g. encode the elements of $S$ and do some sort of mathematical operations? Is any efficient solution possible at all?</p>\n\n<p>Please let me know if I haven't explained my problem well enough.</p>\n", 'ViewCount': '25', 'Title': 'Finding users covering a set x by x', 'LastActivityDate': '2013-09-19T00:29:01.480', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10207', 'Tags': '<data-structures><efficiency><sets>', 'CreationDate': '2013-09-18T23:36:33.543', 'Id': '14426'},9199:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Data Structures is the second CS course taught at Columbia University and it lists Discrete Mathematics as a Co-Req.</p>\n\n<p>I have a BSEE and have not taken any discrete mathematics and am having a hard time understanding why I need to take this to do things like create data structures?</p>\n\n<p>Is this just so the university can cram useless info in my head and make more money, or is there a real relation to discrete math and data structures?</p>\n', 'ViewCount': '607', 'Title': 'Why is discrete mathematics required for data structures?', 'LastEditorUserId': '472', 'LastActivityDate': '2013-09-20T06:54:37.203', 'LastEditDate': '2013-09-20T06:54:37.203', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '10235', 'Tags': '<data-structures><discrete-mathematics>', 'CreationDate': '2013-09-19T21:05:27.263', 'Id': '14450'},9200:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am new to StackExchange, and I already made the mistake of posting a new question as a response to a previous question. Here, I rewrote my question more clearly and separately.</p>\n\n<p>I am trying to store an existing 2D triangulation in a DCEL data structure, and I have all of the vertices and edges.\nI was able to store all of the information correctly except the half_edge representative for each triangle. Here is the algorithm I used:\n(taken from <a href="http://cs.stackexchange.com/questions/2450/constructing-of-double-connected-edge-list-dcel">Constructing of Double Connected Edge List (DCEL)</a>)</p>\n\n<blockquote>\n  <p>Algorithm:</p>\n  \n  <p>For each endpoint, create a vertex. For each input segment, create two\n  half-edges and assign their tail vertices and twins. For each\n  endpoint, sort the half-edges whose tail vertex is that endpoint in\n  clockwise order. For every pair of half-edges e1, e2 in clockwise\n  order, assign e1->twin->next = e2 and e2->prev = e1->twin. Pick one of\n  the half-edges and assign it as the representative for the endpoint.\n  (Degenerate case: if there\'s only one half-edge e in the sorted list,\n  set e->twin->next = e and e->prev = e->twin.) The next pointers are a\n  permutation on half-edges. For every cycle, allocate and assign a face\n  structure.</p>\n</blockquote>\n\n<p>The last sentence seems to be easier said than done. How can I ensure that every triangle will have a representative, and that a representative will be assigned only once for each triangle? Furthermore, which cycle is it referring to? If you have any other ideas, please share.</p>\n\n<p>Thank you very much for your help. I\'ve been struggling with this for a while.</p>\n\n<p>PS- I am working in C++. Also, I am using the same structure as provided in the link above.</p>\n', 'ViewCount': '118', 'Title': 'Problem with storing an existing triangulation in a DCEL', 'LastActivityDate': '2013-11-20T14:54:13.830', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18167', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10269', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2013-09-25T07:24:18.060', 'Id': '14591'},9201:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was asked to perform merge sort by iterative method. I solved it as follows: I first stored each of the elements of the array given in another array say $A$. Then I sent entries of $A$ two by two for merging(was given that the function for merging is known and it merged two arrays at a time). This function returned the merged array which I stored in another array $B$. After I was done with all the elements of $A$ I deleted $A$ and copied each of the entries of $B$ in $A$.I allocated memories for $A$ and $B$ dynamically. I repeated this for $log(n)$ times and each time I halved the size of $A$ and $B$. Is my algorithm correct??</p>\n', 'ViewCount': '614', 'ClosedDate': '2013-10-17T20:40:54.163', 'Title': 'Iterative merge sort', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-01T01:36:15.517', 'LastEditDate': '2013-09-30T17:33:10.407', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10411', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-09-30T15:51:39.713', 'Id': '14706'},9202:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Imagine I have an ordering on a bunch of elements like so:</p>\n\n<p><img src="http://i.imgur.com/Sl2kUC0.png" alt="enter image description here"></p>\n\n<p>Where an arrow $X \\leftarrow Y$ means $X &lt; Y$. It is also transitive: $\\left(X &lt; Y\\right) \\wedge \\left(Y &lt; Z\\right) \\implies \\left(X &lt; Z\\right)$.</p>\n\n<p>In order efficiently answer queries like $A \\stackrel {?}{&lt;} D$, some sort of labeling or data structure is required. For example, you could number the nodes from left to right, and thus you can simply do integer comparison to answer the query: $A \\stackrel {?}{&lt;} D \\implies 1 &lt; 4 \\implies T$. It would look something like this:</p>\n\n<p><img src="http://i.imgur.com/YQ28co3.png" alt="enter image description here"></p>\n\n<p>Where the number is the ordering, and the letter is just a name.</p>\n\n<p>But what if you needed to insert elements "in between" two other elements in the ordering, like so:</p>\n\n<p><img src="http://i.imgur.com/aLuLJRH.png" alt="enter image description here"></p>\n\n<p><img src="http://i.imgur.com/HTSllkZ.png" alt="enter image description here"></p>\n\n<p><img src="http://i.imgur.com/DaytZtk.png" alt="enter image description here"></p>\n\n<p>How can you maintain such an ordering? With simple numbering, you run into the problem that there are no integers "in between" $2,3$ to use.</p>\n', 'ViewCount': '881', 'Title': 'Maintaining an efficient ordering where you can insert elements "in between" any two other elements in the ordering?', 'LastActivityDate': '2013-10-01T13:49:11.323', 'AnswerCount': '3', 'CommentCount': '0', 'AcceptedAnswerId': '14728', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<data-structures><graphs><linked-lists><partial-order><order-theory>', 'CreationDate': '2013-09-30T17:58:23.563', 'FavoriteCount': '2', 'Id': '14708'},9203:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a doubly link-list like this</p>\n\n<pre><code>typedef struct Record\n{\n   int i;\n   Record* next;\n   Record* prev;\n}Record;\n</code></pre>\n\n<p>I have over 5 trillions of records that I need to handle, now that I need to retrieve all of them and sort them out.\nIf its size was small, I could borrow stl's vector or list to do the job but now that it is too huge, I have no idea how to save the object data before sorting is performed</p>\n\n<p>my function prototype</p>\n\n<pre><code>void sortRec(Record**recToSort,bool bASC){}\n</code></pre>\n", 'ViewCount': '71', 'ClosedDate': '2013-10-15T10:05:09.320', 'Title': 'linklist and memory issues', 'LastActivityDate': '2013-10-03T19:11:33.200', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10450', 'Tags': '<algorithms><data-structures><data-mining><linked-lists>', 'CreationDate': '2013-10-02T03:53:31.060', 'Id': '14744'},9204:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<hr>\n\n<p>Is there an incremental directed graph data structure that has the following properties:</p>\n\n<ul>\n<li>Keeps an internal graph structure as a DAG, and the graph is accessible (notwithstanding other helper data-structures)</li>\n<li>The accessible DAG is kept as a transitive reduction (notwithstanding other helper data-structures)</li>\n<li>It should be optimized for a sparse graph (adjacency lists)</li>\n<li>It should condense cycles as they are introduced, keeps a mapping between equivalent vertices that are all replaced with one "representative" vertex</li>\n<li>Ability to quickly answer quickly ancestor/descendant/transitive/relationship queries (in $\\mathcal{O}(1)$ or $\\sim\\mathcal{O}\\left(\\log \\left|V\\right|\\right)$ time)</li>\n<li>Should support vertex, edge insertion, deletion would be nice too</li>\n<li>Mutable operations  (such as insertion) should be as output-sensitive as possible; in other words, the complexity should depend as much as possible on how much the operation must change the graph</li>\n<li>Ability to record changes over an operation, if requested. Obviously this might necessarily increase the complexity, but the increase should be output-sensitive. Examples:\n<ul>\n<li>set of deleted vertices (due to condensation)</li>\n<li>set of deleted edges (due to reduction)</li>\n<li>set of new decendent relationships from $u$ ( example: $insert(G,u,v) \\rightarrow \\left\\{t ~|~ path(u,t)\\in G\'\\wedge path(u,t)\\not\\in G\\right\\}$ )</li>\n<li>set of new ancestor relationships from $v$ ( example: $insert(G,u,v) \\rightarrow \\left\\{t ~|~ path(t,v)\\in G\'\\wedge path(t,v)\\not\\in G\\right\\}$ )</li>\n</ul></li>\n</ul>\n\n<p>The closest I can find is <a href="http://code-o-matic.blogspot.com/2010/07/graph-reachability-transitive-closures.html" rel="nofollow">here</a>, <a href="http://code.google.com/p/transitivity-utils" rel="nofollow">implementation</a>. I think you can build on this to do have most of the properties I list, but I am wondering if there is anything better/well known, or perhaps if there is a name for this problem.</p>\n\n<hr>\n\n<p><strong>EDIT:</strong></p>\n\n<h3>Related:</h3>\n\n<ul>\n<li><a href="http://cstheory.stackexchange.com/q/14343/3377">What is the fastest deterministic algorithm for dynamic digraph reachability with no edge deletion?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/18787/3377">What is the fastest deterministic algorithm for incremental DAG reachability?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/5176/3377">Does an algorithm exist to efficiently maintain connectedness information for a DAG in presence of inserts/deletes?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/2548/3377">Is there an online-algorithm to keep track of components in a changing undirected graph?</a></li>\n<li><a href="http://cstheory.stackexchange.com/q/17135/3377">Dynamic shortest path data structure for DAG</a></li>\n</ul>\n', 'ViewCount': '150', 'Title': 'An incrementally-condensed transitive-reduction of a DAG, with efficient reachability queries', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-10-04T05:16:35.877', 'LastEditDate': '2013-10-04T04:51:04.030', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><data-structures><graphs><shortest-path><online-algorithms>', 'CreationDate': '2013-10-03T21:12:35.287', 'FavoriteCount': '1', 'Id': '14798'},9205:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have the following problem:</p>\n\n<blockquote>\n  <p>Does inserting a node into a red-black tree and then immediately deleting it always result in the original tree? Prove that it does or give a counter-example if it does not. Does deleting a leaf node from a red-black tree, then reinserting the same node always result in the original tree? Prove that it does or give a counter-example if it does not.</p>\n</blockquote>\n\n<p>I have used several demos, but none of them have made much sense to answer this question. I havent been sure as to yes or no, I am still new to binary trees.</p>\n', 'ViewCount': '141', 'Title': 'Does inserting and immediately removing a node change a red-black tree?', 'LastEditorUserId': '98', 'LastActivityDate': '2013-10-04T15:26:08.840', 'LastEditDate': '2013-10-04T07:00:36.170', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '10497', 'Tags': '<data-structures><binary-trees><search-trees>', 'CreationDate': '2013-10-04T05:02:22.753', 'Id': '14804'},9206:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>Suppose we are given an M\xd7N matrix, with some elements are zero, some non-zero. We know the co-ordinates of non-zero elements. Now, if I am allowed to multiply a whole row or a whole column by zero one at a time what will be minimum number of operations (i.e multiplications) I will need. For example, for the matrix</p>\n\n<p>$\\begin{pmatrix}\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 0 \\\\\n    1 &amp; 0 &amp; 1\n  \\end{pmatrix}$</p>\n\n<p>the answer is two. For this example</p>\n\n<p>$\\begin{pmatrix}\n    1 &amp; 1 &amp; 1 \\\\\n    0 &amp; 0 &amp; 1 \\\\\n    0 &amp; 0 &amp; 1\n  \\end{pmatrix}$</p>\n\n<p>the answer is two not three.</p>\n\n<p>Any help to go for head start is appreciated.</p>\n', 'ViewCount': '65', 'Title': 'What will be minimum no of operation to make whole matrix zero if one is allowed to multiply a row or column by zero?', 'LastEditorUserId': '6665', 'LastActivityDate': '2013-10-06T16:53:54.620', 'LastEditDate': '2013-10-06T16:53:54.620', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10516', 'Tags': '<algorithms><complexity-theory><graph-theory><data-structures><discrete-mathematics>', 'CreationDate': '2013-10-05T18:23:42.310', 'FavoriteCount': '1', 'Id': '14831'},9207:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>There are $n$ elements in a hash table of size $m \\geq 2n$ which uses open addressing to avoid collisions. </p>\n\n<p>The hash function was chosen randomly among a set of uniform functions. A set $H$ of hash-functions $h:U\\to\\{0,\\dots,m-1\\}$ is called uniform, if for every tuple of different keys $x,y \\in U$ the number of hash-functions $h \\in H$ with $h(x) = h(y)$ is $\\frac{|H|}{m}$ at most.</p>\n\n<p>Show that the propability that for $i = 1, 2, \\dots,n$ the $i$-th insert operation needs more than $k$ attempts, is $2^{-k}$ at most.</p>\n\n<p>This is an assignment, which I got as homework. What I already worked out:</p>\n\n<p>The propability $p_1$ for a collision is 0 of course for an empty table.</p>\n\n<p>The propability $p_i$ for a collision after k attempts should be $\\frac{i - 1}{2n}\\cdot k$ assuming that the table is filled with $i-1$ elements to this point and the tables size is $2n$ as worst case.</p>\n\n<p>So I have\n$$\np_i= \\frac{i-1}{2n} \\cdot k \\leq 2^{-k},\n$$</p>\n\n<p>but I don't know where to go from here.</p>\n\n<p>The method of open hashing used here simply iterates over different hash-functions until a free place is found (for example $h(x) = (x \\bmod j) \\bmod n$ with increasing prime numbers for $j$.</p>\n", 'ViewCount': '140', 'Title': 'Proof that probability that hashing with open addressing needs more than $k$ attempts is $2^{-k}$ at most', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-08T16:59:37.943', 'LastEditDate': '2014-01-07T16:06:08.057', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6424', 'Tags': '<algorithm-analysis><data-structures><hash-tables>', 'CreationDate': '2013-10-06T22:13:05.387', 'FavoriteCount': '1', 'Id': '14862'},9208:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>A family $H$ of hash functions $h: U \\rightarrow \\{0,\\ldots,M-1\\}$ is <em>universal</em> if \n$$\\forall x,y \\in U, x \\neq y \\Rightarrow \\Pr_{h \\in H}[h(x) = h(y)] \\leq \\frac{1}{M}$$\nYou can find more about universal hashing this wikipedia <a href="http://en.wikipedia.org/wiki/Universal_hashing">article</a>.</p>\n\n<p>The concept of universal hashing is now a standard part of undergraduate data structure courses. It would be nice to be able to motivate students about the importance of universal hashing in industrial applications. So my question is:</p>\n\n<blockquote>\n  <p>Are constructions of universal family of hash functions important in practice? If the answer is yes, would you please share some interesting industrial applications that you\'ve seen?</p>\n</blockquote>\n', 'ViewCount': '134', 'Title': 'Universal Hashing in Practice', 'LastEditorUserId': '204', 'LastActivityDate': '2013-10-27T22:57:49.313', 'LastEditDate': '2013-10-16T03:34:01.153', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '204', 'Tags': '<data-structures><education><hash><hash-tables>', 'CreationDate': '2013-10-16T00:21:41.770', 'Id': '16118'},9209:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I can't answer this question. It seems simple but I really don't know how to approach it.\nHere it is:</p>\n\n<p>A priority queue is said to be stable if deletions of items with equal priority value occur in the order in which they were inserted. Which of the following priority queue structures are stable:</p>\n\n<ul>\n<li>linked lists ordered in increasing priority (key)</li>\n<li>balanced search trees (e.g., 2-3 trees)</li>\n<li>heaps</li>\n<li>leftist heaps</li>\n</ul>\n\n<p>Explain why, or give counter-examples.</p>\n\n<p>I don't need a full solution just a way to approach this problem. I would prefer to solve it on my own.</p>\n\n<p>Any help is appreciated.</p>\n", 'ViewCount': '125', 'Title': 'Stable priority queue', 'LastActivityDate': '2013-10-27T18:18:07.780', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '16480', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10511', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-10-26T22:42:54.990', 'Id': '16456'},9210:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '214', 'Title': 'Computing with the Monster', 'LastEditDate': '2013-10-31T08:54:40.460', 'AnswerCount': '6', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '11020', 'FavoriteCount': '1', 'Body': '<p>The <a href="http://en.wikipedia.org/wiki/Monster_group" rel="nofollow">Monster</a> <strong>M</strong> is the largest of the finite sporadic groups that arises in the <a href="http://en.wikipedia.org/wiki/Classification_of_finite_simple_groups" rel="nofollow">classification of finite, simple groups</a> in mathematics. </p>\n\n<p><strong>M</strong> can be realized as a (very large!) set of <code>196882 X 196882</code> matrices with nothing more than entries of 1\'s and 0\'s, so long as we compute arithmetic as follows:</p>\n\n<p><code>1+1=0</code>\n<code>1+0=1+0=1</code>\n<code>1*1=1</code>\n<code>0*0=0</code>\n<code>1*0=0*1=0</code></p>\n\n<p>I have two simple questions for the reader. What is the minimum amount of bytes needed to store a single matrix? What is the computational cost (i.e., in FLOPS) of a single matrix multiplication in the most efficient implementation (i.e., taking into account that the entries are binary, not taking into account mathematical properties about the Monster)?</p>\n\n<p><em>This is a question purely at the computational side of the problem. In other words, treat the matrices as general <code>196882 x 196882</code> matrices with binary entries. This is not a question about the Monster. That was just added as motivation.</em></p>\n', 'Tags': '<algorithms><data-structures>', 'LastEditorUserId': '472', 'LastActivityDate': '2013-10-31T08:54:40.460', 'CommentCount': '4', 'AcceptedAnswerId': '16694', 'CreationDate': '2013-10-28T20:30:19.927', 'Id': '16513'},9211:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Some priority queues, like the <a href="https://en.wikipedia.org/wiki/Leftist_tree" rel="nofollow">height-based leftist tree</a> (or <a href="http://www.cse.ohio-state.edu/~gurari/course/cis680/cis680Ch8.html" rel="nofollow">here</a>) support merging in $\\mathcal O\\left(\\log n\\right)$ time.</p>\n\n<p>I am looking for a priority queue that merges in (expected|average|amortized|worst-case) <em>sub-linear</em> time, but also has the following properties:</p>\n\n<ul>\n<li>Elements are unique</li>\n<li><code>peek</code> and <code>pop</code> should work in (expected|average|amortized|worst-case) <em>sub-linear</em> time</li>\n</ul>\n\n<p>Is this impossible?</p>\n', 'ViewCount': '210', 'Title': 'Priority queue with unique elements and sublinear time merge?', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-04T02:12:44.287', 'LastEditDate': '2013-11-01T03:36:00.840', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><data-structures><trees><priority-queues>', 'CreationDate': '2013-11-01T03:30:02.627', 'FavoriteCount': '2', 'Id': '16611'},9212:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know how a BIT works. But I was wondering if a BIT can be used to find the minimum/maximum element in the complete range, or more specifically, to find the minimum (or maximum) value after all the update processes have been completed. Now, I know that this can very well be achieved using Segment Trees, but is it possible to do the same using a BIT?</p>\n\n<p>I know the obvious way of traversing the complete BIT and calculating the value at each index. I am looking for a more efficient/optimized way.</p>\n', 'ViewCount': '178', 'Title': 'Finding minimum/maximum value in a Binary Indexed Tree', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-02T10:12:11.680', 'LastEditDate': '2013-11-02T10:12:11.680', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11119', 'Tags': '<data-structures><binary-trees>', 'CreationDate': '2013-11-01T14:37:26.980', 'FavoriteCount': '2', 'Id': '16620'},9213:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let $T$ be a tree, and there is a weight function on the edges $w:E\\to X$. $(X,\\oplus)$ is a monoid structure.</p>\n\n<p>Define $f(u,v) = \\bigoplus_{i=1}^k w(e_i)$, where $e_1,\\ldots,e_k$ is the unique path from $u$ to $v$.</p>\n\n<p>Can we preprocess the tree, such that we can use linear space (or close to linear space), so we can answer queries $f(u,v)$ in $O(\\mathrm{polylog} (n))$ monoid operations? The problem become more interesting if we allow removing edges and add edge between two vertices in distinct trees.</p>\n\n<p>We can also ask the question on a DAG, with weights from a commutative monoid. We want to preprocess in linear time, and query the result of $f(u,v)= \\sum_{e} w(e)$, where $e$ is in some path from $u$ to $v$, in $O(\\mathrm{polylog} (n))$ time.</p>\n\n<p>If we consider the very special case where the entire graph is just a path, then what we want is a dynamic version of a Fenwick tree. A finger tree can solve the problem in $O(\\log n)$ time.</p>\n', 'ViewCount': '185', 'Title': 'Data structure for finding the sum of edge weights on a path', 'LastEditorUserId': '472', 'LastActivityDate': '2013-11-04T19:51:46.153', 'LastEditDate': '2013-11-04T16:22:02.107', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '220', 'Tags': '<data-structures><graphs>', 'CreationDate': '2013-11-04T12:48:28.100', 'FavoriteCount': '1', 'Id': '16703'},9214:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m not sure if this is the correct stack exchange or correct tags, but my question is as follows: </p>\n\n<p>I am working on a sort-of ratings system for players in a particular game. After allowing the ratings to develop for many games, I have set up a "database" (not sure if this is the correct term) for matches: a point in the database would consist of the score difference between the players (i.e if it\'s >0 player 1 won) and the rating discrepancy between the two players before this match.</p>\n\n<p>The idea is to use this database in order to predict the score difference of a match that has not yet occurred: I measure the rating discrepancy between the 2 players, and lookup in my database for the score differences in games of a "similar" rating discrepancy, and based on these games I am able to predict, say, a rough probability of each score difference occurring in this match that has not yet happened.</p>\n\n<p>I actually have two questions: </p>\n\n<p>1) what is a good approach to using the database to predict the probability of each score difference, i.e what qualifies as a "similar" rating discrepancy, how do I deal with extreme cases where the rating discrepancy is very large and I have few data examples of such matches (should I relax my definition of "similar"?), etc. A bit of googling shows that maybe I am looking for something relating to data clusters.</p>\n\n<p>2)how would I go about implementing an algorithm as above? are there good standard implementations of such classification algorithms? I am writing in C# if it makes a difference. </p>\n', 'ViewCount': '47', 'Title': 'Analysis and classification based on data points', 'LastActivityDate': '2013-11-06T20:13:58.630', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '8247', 'Tags': '<data-structures><machine-learning><data-mining><cluster>', 'CreationDate': '2013-11-06T17:36:07.680', 'Id': '16775'},9215:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Solve the following recurrence equations</p>\n\n<p>a. T(n) = T(n/2) + 18</p>\n\n<p>b. T(n) = 2T(n/2) + 5n</p>\n\n<p>c. T(n) = 3T(n/2) + 5n</p>\n\n<p>d. T(n) = T(n/2) + 5n</p>\n\n<p>This is only a sample of what I was given but I am not sure what the question is asking me or how to solve it? Can someone please explain this to me?</p>\n\n<p>The only thing I really know about them is that it has something having to do with Big O and such. I was a bit sick when they gave this lecture so I couldn't really grasp the concept. Could someone please help me understand this?</p>\n", 'ViewCount': '84', 'ClosedDate': '2013-11-11T13:50:19.720', 'Title': 'How can I solve for T(n) recurrence equations?', 'LastActivityDate': '2013-11-10T23:57:30.613', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11305', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-11-10T22:32:41.520', 'Id': '17890'},9216:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>This is homework, so please only hints. I didn't want to put this on StackOverflow since this is mostly about theory, and SO gets inundated with too many questions.</p>\n\n<p>I am asked to find a method of arranging items in a skiplist, with height limited to 3 and unlimited number of elements, in such a way such that searching takes <em>worst-case</em> $\\Theta (n^{1/3})$. No restriction on how expensive the arranging part is; just describe what subset of the keys go into which level.</p>\n\n<p>I am somewhat confused. How can a skiplist with a limited number of items have search complexity anything other than $\\Theta(n)$? Clearly, with any clever algorithm, I can just fill the skiplist with a horrendous amount of elements, and as the number of elements goes up, surely the handicapped skiplist can't do any better than a linked list asymptotically? I think I can prove this for the simple set of rearranging algorithms that put a random proportion $q$ of the items into the second layer, and another random proportion $p$ of items within these items to the top layer.</p>\n\n<p>Am I missing anything obvious? Is the question faulty?</p>\n", 'ViewCount': '68', 'Title': 'Skip lists with limited height', 'LastEditorUserId': '98', 'LastActivityDate': '2013-11-12T00:15:57.107', 'LastEditDate': '2013-11-11T13:54:11.880', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '6778', 'Tags': '<data-structures><lists>', 'CreationDate': '2013-11-11T01:40:36.807', 'Id': '17901'},9217:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose we have a $n\\times n$ symmetric matrix $\\mathbf A$.</p>\n\n<p>I want to know if there exists two elements of a vector $\\mathbf x$, let's call them $x_i,x_j,i\\ne j$, such that $x_i +x_j+[A]_{i,j}\\ge y$ for some vector $\\mathbf x$ of size $n$. So:</p>\n\n<p>$$\nf\\left(\\mathbf x \\in \\mathbb N^n,y\\right)=\n\\begin{cases}\n1&amp;\\text{if }\\exists_{i,j,i\\ne j}x_i+x_j+[A]_{i,j}\\ge y\\\\\n0&amp;\\text{otherwise}\\\\\n\\end{cases}\n$$</p>\n\n<p>We can alternatively formulate it simpler, by subtractiong $\\frac y 2$ from all $x_i \\in \\mathbf x$, and flip the sign of the elements $\\mathbf A$ and formulate it like this:</p>\n\n<p>$$\ng\\left(\\mathbf x \\in \\mathbb N^n\\right)=\n\\begin{cases}\n1&amp;\\text{if }\\exists_{i,j,i\\ne j}x_i+x_j\\ge [A]_{i,j}\\\\\n0&amp;\\text{otherwise}\\\\\n\\end{cases}\n$$\n$\\mathbf A$ does not have to be stored as a matrix, and I am hoping some other form of storage can help here - some sort of precomputation.</p>\n\n<h3>Is there an efficient algorithm for this (subquadtratic in $n$)?</h3>\n\n<p>I am sort of struggling to name this, so:</p>\n\n<h3>Is this a known problem, or reducible to a known problem?</h3>\n", 'ViewCount': '46', 'Title': '2SUM with a weight', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-11-12T00:37:23.937', 'LastEditDate': '2013-11-11T14:59:50.627', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2755', 'Tags': '<algorithms><data-structures><number-theory>', 'CreationDate': '2013-11-11T05:49:04.410', 'Id': '17907'},9218:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have two large sets of integers $A$ and $B$.  Each set has about a million entries, and each entry is a positive integer that is at most 10 digits long.  </p>\n\n<p>What is the best algorithm to compute $A\\setminus B$ and $B\\setminus A$? In other words, how can I efficiently compute the list of entries of $A$ that are not in $B$ and vice versa?  What would be the best data structure to represent these two sets, to make these operations efficient?</p>\n\n<p>The best approach I can come up with is storing these two sets as sorted lists, and compare every element of $A$ against every element of $B$, in a linear fashion.  Can we do better?</p>\n', 'ViewCount': '292', 'Title': 'Computing set difference between two large sets', 'LastEditorUserId': '755', 'LastActivityDate': '2013-11-14T13:47:21.917', 'LastEditDate': '2013-11-14T07:18:12.080', 'AnswerCount': '4', 'CommentCount': '11', 'Score': '7', 'PostTypeId': '1', 'OwnerUserId': '1985', 'Tags': '<algorithms><data-structures><sets>', 'CreationDate': '2013-11-13T13:50:12.773', 'FavoriteCount': '1', 'Id': '17984'},9219:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am asked to check if a vector contains another vector as a subvector. For example:</p>\n\n<p>$$v_1 = (4, \\underline{3, 4}, 9, 10, 28, 5, 12, \\underline{3, 4})$$\n$$v_2 = (3, 4)$$</p>\n\n<p>The answer here will be two, since there are two instances of $v_2$ in $v_1$.</p>\n\n<p>I know I have to use "if" but I dont really know how to write it down (I have tried).</p>\n', 'ViewCount': '61', 'Title': 'Counting occurrences of one vector inside another', 'LastEditorUserId': '69', 'LastActivityDate': '2013-11-15T10:06:52.937', 'LastEditDate': '2013-11-14T17:13:13.220', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11377', 'Tags': '<algorithms><data-structures><lists>', 'CreationDate': '2013-11-14T13:54:23.277', 'Id': '18014'},9220:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I would like to find for any given vertex in a polygon stored in a doubly-connected edge list if the polygon is to its right or not. How do I do that without having a bunch of nested if statements?</p>\n\n<p>It seems reasonable to expect a more elegant solution that uses the DCEL.</p>\n\n<p>Thank you.</p>\n\n<p>EDIT: the vertices have coordinates and are stored in a normal DCEL data structure along with the half edges and polygons. </p>\n', 'ViewCount': '96', 'Title': 'How do you find out with a DCEL if the face is to the right of a vertex?', 'LastEditorUserId': '10269', 'LastActivityDate': '2013-11-20T15:10:47.230', 'LastEditDate': '2013-11-17T09:27:16.113', 'AnswerCount': '1', 'CommentCount': '6', 'AcceptedAnswerId': '18165', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10269', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2013-11-17T06:39:35.590', 'Id': '18092'},9221:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have a practice exam question that I don't know how to set up a recurrence for. It is dealing with a hash table. The question is as follows:</p>\n\n<blockquote>\n  <p>Suppose that a hashing strategy is designed so that it starts with an initial hash table size of $H= 8$. You may assume that only insertions are performed (no deletions).</p>\n  \n  <p>Any time the hash table is going to be more than 50% full (when an attempt is made to add item $\\frac{H}{2} + 1$ to a table of size $H$), the hash table size is doubled to $2\\times H$, and then the $\\frac{H}{2}$ keys in the previous hash table are rehashed using $\\frac{H}{2}$ extraneous key insertions into the new table of size $2 \\times H$. The key insertions used to initially place each key into the hash table are called necessary key insertions (these are not extraneous).</p>\n</blockquote>\n\n<p>The question is asking to derive a recurrence relation $E(H)$ for the number of extraneous key insertions that have occurred in total up until the point in time that the hash table size is $H$ and to explain where the terms in the recurrence relation derive from.</p>\n\n<p>If someone could help me out with this, it would provide very helpful as I am practicing for an exam that I have in a week. Thanks everyone.</p>\n\n<p>I got the result $E(H)=2\\times E(\\frac{H}{2})$ because after each rehash there are $\\frac{H}{2}$ extraneous key insertions being put into the table of size $2\\times H$. So if the size is twice the amount of $H$, I figured the recurrence would be $E(H)=2\\times E(\\frac{H}{2})$. I only posted here because I was hoping someone could assist me with this because this question has me a bit stumped. </p>\n", 'ViewCount': '81', 'Title': 'Recurrence for total number of extraneous key insertions in a hash table', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T08:41:25.867', 'LastEditDate': '2014-03-26T08:41:25.867', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11491', 'Tags': '<data-structures><runtime-analysis><recurrence-relation><hash-tables>', 'CreationDate': '2013-11-20T00:24:50.667', 'Id': '18178'},9222:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>One can, for example, represent 2d arrays such as:</p>\n\n<pre><code>[[1,2],[3,4],[4,5]]\n</code></pre>\n\n<p>as flat arrays:</p>\n\n<pre><code>[1,2,3,4,5,6]\n</code></pre>\n\n<p>as long as he transforms the indices before accessing:</p>\n\n<pre><code>index(x,y) = x + y*2 // because internal width=2\n</code></pre>\n\n<p>This is often faster. My question is: is it possible to use a similar approach of representing an structure as a flat array, for, instead of n-dimensional tables, free-form trees such as:</p>\n\n<pre><code>[[1,2,3],4,[5,[6,7]],8]\n</code></pre>\n', 'ViewCount': '141', 'Title': 'Is there an structure that allows for a flat representation of trees with constant access to any element?', 'LastActivityDate': '2013-11-27T16:26:44.323', 'AnswerCount': '4', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2013-11-22T01:24:15.343', 'Id': '18247'},9223:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '223', 'Title': 'What classes of data structures can be made persistent?', 'LastEditDate': '2013-11-22T20:19:38.030', 'AnswerCount': '1', 'Score': '8', 'PostTypeId': '1', 'OwnerUserId': '2755', 'FavoriteCount': '6', 'Body': '<p><b><a href="http://en.wikipedia.org/wiki/Persistent_data_structure">Persistent data structures</a></b> are immutable data structures. Operations on them return a new "copy" of the data structure, but altered by the operation; the old data structure remains unchanged though. Efficiency is generally accomplished by sharing some of the underlying data, and avoiding full copying of the data structure.</p>\n\n<p><b>Questions:</b></p>\n\n<blockquote>\n  <ul>\n  <li><p>Are there results about classes of data structures that can be made to be persistent (while keeping the same or very similar complexities)?</p></li>\n  <li><p>Can <em>all</em> data structures be made persistent (while keeping the same or very similar complexities)?</p></li>\n  <li><p>Are any data structures known to be unable to be made persistent (while keeping the same or very similar complexities)?</p></li>\n  </ul>\n</blockquote>\n', 'Tags': '<reference-request><data-structures><functional-programming>', 'LastEditorUserId': '2755', 'LastActivityDate': '2013-12-01T00:30:18.423', 'CommentCount': '8', 'AcceptedAnswerId': '18266', 'CreationDate': '2013-11-22T18:13:18.380', 'Id': '18262'},9224:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Which data structure can be used to "eliminate" recursion from a recursive program? Array, stack, queue, or list? How is it used to that end?</p>\n\n<p>When I eliminate recursion from the program computing factorial $n! = n(n-1)!$, I replace recursion with a loop. But I don\'t see how to do a similar transformation in general.</p>\n', 'ViewCount': '398', 'ClosedDate': '2014-01-05T17:31:03.850', 'Title': 'Data structure used to implement recursion', 'LastEditorUserId': '683', 'LastActivityDate': '2013-12-26T09:35:02.510', 'LastEditDate': '2013-11-26T09:03:22.340', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10380', 'Tags': '<data-structures>', 'CreationDate': '2013-11-23T18:53:48.763', 'Id': '18281'},9225:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '120', 'Title': 'More efficient algorithm for determining if one list is a sublist of another list', 'LastEditDate': '2013-11-26T01:14:57.783', 'AnswerCount': '2', 'Score': '4', 'OwnerDisplayName': 'Panarit', 'PostTypeId': '1', 'OwnerUserId': '11640', 'FavoriteCount': '1', 'Body': "<p>I'm trying to build an algorithm which takes two lists of natural numbers and finds if every element of the first list is displayed at least once in the second list.</p>\n\n<p>What if the list is sorted? </p>\n\n<p>An algorithm that can do this is by comparing every element of the first list with every element from the second list. I think there is an algorithm with a better complexity. Can anyone give me any idea?</p>\n", 'Tags': '<algorithms><data-structures><sorting><lists><comparison>', 'LastEditorUserId': '39', 'LastActivityDate': '2013-11-26T21:23:38.760', 'CommentCount': '1', 'AcceptedAnswerId': '18350', 'CreationDate': '2013-11-24T14:49:37.500', 'Id': '18346'},9226:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I want to know the best case time complexity of creating binary search tree and explanation.I know for insertion of a node in bst best case is O(1).</p>\n', 'ViewCount': '49', 'ClosedDate': '2013-12-01T12:11:09.147', 'Title': 'best case for binary search tree creation', 'LastActivityDate': '2013-11-29T19:33:39.900', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-2', 'PostTypeId': '1', 'OwnerUserId': '10380', 'Tags': '<data-structures>', 'CreationDate': '2013-11-29T18:31:42.770', 'Id': '18474'},9227:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let $n$ be an integer, and let $\\mathbb{Z}$ denote the set of all integers.  Let $[a,b]$ denote the interval of integers $\\{a,a+1,a+2,\\dots,b\\}$.</p>\n\n<p>I am looking for a data structure to represent a map $f:[1,n] \\to \\mathbb{Z}$.  I want the data structure to support the following operations:</p>\n\n<ul>\n<li><p>$\\text{get}(i)$ should return $f(i)$.</p></li>\n<li><p>$\\text{set}([a,b],y)$ should update $f$ so that $f(a)=f(a+1)=\\cdots=f(b)=y$, i.e., update $f$ to a new map $f'$ such that $f'(i) = y$ for $i \\in [a,b]$ and $f'(i) = f(i)$ for $i \\notin [a,b]$.</p></li>\n<li><p>$\\text{stab}(i)$ should return the largest interval $[a,b]$ such that $i \\in [a,b]$ and $f$ is constant on $[a,b]$ (i.e., $f(a)=f(a+1)=\\cdots=f(b)$).</p></li>\n<li><p>$\\text{add}([a,b],\\delta)$ should update $f$ to a new map $f'$ such that $f'(i) = f(i) + \\delta$ for $i \\in [a,b]$ and $f'(i) = f(i)$ for $i \\notin [a,b]$.</p></li>\n</ul>\n\n<p>I want each of these operations to be efficient.  I would count $O(1)$ or $O(\\lg n)$ time as efficient, but $O(n)$ time is too slow.  It's OK if the running times are amortized running times.  Is there a data structure that simultaneously makes all of these operations efficient?</p>\n\n<p>(I've noticed a similar pattern come up in a several programming challenges.  This is a generalization that would suffice for all of those challenge problems.)</p>\n", 'ViewCount': '145', 'Title': 'Data structure for map on intervals', 'LastEditorUserId': '755', 'LastActivityDate': '2013-12-03T07:49:12.067', 'LastEditDate': '2013-12-02T20:33:03.120', 'AnswerCount': '1', 'CommentCount': '5', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '755', 'Tags': '<algorithms><data-structures><trees><intervals>', 'CreationDate': '2013-12-02T18:19:49.227', 'FavoriteCount': '1', 'Id': '18542'},9228:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '43', 'Title': 'Hashing a Specific Range Of a Character Array', 'LastEditDate': '2013-12-11T12:40:25.193', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8505', 'Body': '<p>I need to process queries to Hash various ranges of a character array. I am currently using the <a href="http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#hashCode%28char%5B%5D%29" rel="nofollow">Arrays.hashCode</a> from the standard java library. But the problem is that this method is too slow. Also my array remains the same throughout the process of hashing, I only am changing the range. To deal with this, I have to make an entire copy of the array everytime I process a query, and then compute the hash from the above function. </p>\n\n<p>I am using <a href="http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#copyOfRange%28char%5B%5D,%20int,%20int%29" rel="nofollow">Arrays.copyOfRange</a> to create a copy everytime I process a query. I need to avoid this. So I was thinking of devising a hashing scheme of my own. This scheme should be such that I whould get a unique hash for each array range. Hashes should be same if all characters in the range are same. </p>\n\n<p>Any Help on how to proceed with the making of such a hash function will be appreciated.</p>\n', 'ClosedDate': '2013-12-14T20:05:36.503', 'Tags': '<data-structures><arrays><hash><hash-tables>', 'LastEditorUserId': '8505', 'LastActivityDate': '2013-12-11T20:57:09.670', 'CommentCount': '2', 'AcceptedAnswerId': '18896', 'CreationDate': '2013-12-11T10:59:54.037', 'Id': '18873'},9229:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have a queue of messages representing filesystem operations that need to be processed in order. A message may succeed or fail when it\'s sent. A change message, for example, is generated when a user creates or saves a file.</p>\n\n<ul>\n<li>Delete Foo.txt message sent</li>\n<li>Delete Bar.txt message sent</li>\n<li>Change Baz.txt message sent</li>\n<li>Change Foo.txt message sent</li>\n</ul>\n\n<p>In the most common case, I pop the top item from the queue and send it off for processing, which succeeds.</p>\n\n<p>But suppose this happens:</p>\n\n<ul>\n<li>Delete Foo.txt message attempted (FAIL)</li>\n<li>Delete Bar.txt message attempted (OK)</li>\n<li>Change Baz.txt message attempted (OK)</li>\n<li>Change Foo.txt message attempted (OK)</li>\n</ul>\n\n<p>What should I do in this case?</p>\n\n<p>I want to move on and process the next item in the queue, but it\'s not that simple. In this case, "Delete Foo.txt" <em>must</em> be processed before "Create Foo.txt", otherwise we\'re deleting Foo.txt from the search index (see "more details" below), even though it exists.</p>\n\n<p>One approach I had considered: a key-value store, where the document path is the key, and the Message is the value. In the context of a filesystem, the only operation that matters is the most recent, so overwriting the Message associated with a file if it hasn\'t been sent might work.</p>\n\n<p>What kind of data structure and algorithm is right for this situation? I don\'t want to reinvent the wheel.</p>\n\n<p>A few more details:</p>\n\n<ul>\n<li>New items are constantly arrive in the queue, from multiple producer threads</li>\n<li>The messages are consumed from the collection by a single consumer</li>\n<li>A message is a serialized object, sent to a RabbitMQ broker</li>\n<li>This queue exists in case the RabbitMQ broker goes down or a Publisher Confirm fails</li>\n<li>Messages are consumed from the endpoint by a different service which performs various actions to keep a search index incrementally up to date</li>\n<li>C#, if it matters</li>\n</ul>\n\n<p><strong>Update 1:</strong></p>\n\n<p><em>Big picture</em></p>\n\n<p>I am sending messages <em>about</em> filesystem operations that have occurred. These messages are used to keep a search index (elasticsearch) up to date. A few minutes behind is perfectly acceptable. If a user makes three saves in rapid succession, I don\'t need search index updates for the first two saves. (If it happens, that\'s fine, but it\'s not required.)</p>\n\n<p>CRUD operations are done by users working with their files on a busy network share. Each write operation triggers a message that has to be sent to a persistent work queue (the RabbitMQ broker). An example message might contain data that boils down to "File F has been deleted" or "File Y has been changed". (No file contents are contained therein.)</p>\n\n<p>(These messages are consumed by another component downstream that performs an action based on the contents of the message, which is outside the scope of this question.)</p>\n\n<p>Because messages represent filesystem actions that were performed in a sequence by users they either:</p>\n\n<ul>\n<li>Have to be sent in order so they can be processed in order OR</li>\n<li>The message associated with the most recent write operation on File F should overwrite any queued message associated with File F that hasn\'t been sent yet</li>\n</ul>\n\n<p>In my example above (second unordered list), it\'s safe for the message representing "Change Foo.txt" to overwrite a failed "Delete Foo.txt", because the Change message was triggered after the Delete message. (The time where Foo.txt didn\'t exist isn\'t important.) Hence my thought of a using key-value store of  as a holding area in case of a Message send failure.</p>\n', 'ViewCount': '81', 'Title': "Data structure to use for a consumable message queue where it's safe for messages to overwrite one another", 'LastEditorUserId': '12070', 'LastActivityDate': '2013-12-13T15:51:46.670', 'LastEditDate': '2013-12-13T15:51:46.670', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '18935', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12070', 'Tags': '<data-structures>', 'CreationDate': '2013-12-12T19:53:34.753', 'Id': '18934'},9230:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider this tree:</p>\n\n<p><img src="http://i.stack.imgur.com/cMslZ.png" alt="Simple binary tree"></p>\n\n<p>If I traverse it using post-order, I\'d start at <em>B</em> (as it is the leftmost leaf) and that\'s where my misunderstanding begins. I know <em>B</em> is the first and <em>A</em> will be the last node in post order, as the rule is left-right-root. One of my university professors said the correct answer for the post-order traversal of a tree similar to the one above would be <strong><em>B</em>, <em>C</em>, <em>D</em>, <em>E</em>, <em>A</em></strong>, but in my understanding, it should be <strong><em>B</em>, <em>D</em>, <em>E</em>, <em>C</em>, <em>A</em></strong>.  </p>\n\n<p>Am I getting it wrong? Shouldn\'t I evaluate <em>(C,D),(C,E)</em> as a subtree and then go back to the parent tree?</p>\n', 'ViewCount': '40', 'Title': "Doesn't post-order traversal require subtrees to be evaluated separately?", 'LastEditorUserId': '12111', 'LastActivityDate': '2013-12-14T19:07:44.407', 'LastEditDate': '2013-12-14T19:07:44.407', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '18988', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12111', 'Tags': '<data-structures><binary-trees><search-algorithms><trees><search-trees>', 'CreationDate': '2013-12-14T17:40:35.087', 'Id': '18987'},9231:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'ve created a data structure that is a hash of arrays with a special property: the hash keeps track of the combined order in which items are appended to its arrays. </p>\n\n<p>For example (pseudocode):</p>\n\n<pre><code>h = HashOfArray()\n\nh["a"].append(1) // accessing an unset key returns an empty array\nh["b"].append(2)\nh["a"].append(3)\nh["b"].append(4)\n\nh["a"]\n&gt;&gt;&gt; [1, 3]\n\nh["b"]\n&gt;&gt;&gt; [2, 4]\n\nh.items()\n&gt;&gt;&gt; [1, 2, 3, 4]\n</code></pre>\n\n<p>I\'m struggling with coming up with a good name for this data structure. Does it have a commonly used name? I\'ve called it <code>HashOfArray</code> but that fails to convey its main property: that it maintains the combined append order of the arrays.</p>\n\n<p>Example implementation in Ruby:\n<a href="https://gist.github.com/kajic/7981533" rel="nofollow">https://gist.github.com/kajic/7981533</a></p>\n', 'ViewCount': '79', 'Title': 'What is this hash of array data structure called?', 'LastEditorUserId': '12214', 'LastActivityDate': '2013-12-22T11:07:00.270', 'LastEditDate': '2013-12-18T21:44:13.320', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12214', 'Tags': '<data-structures><arrays><hash-tables>', 'CreationDate': '2013-12-18T18:20:44.957', 'Id': '19099'},9232:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I want to run Dijkstra's algorithm on a graph whose edge weights are integers in the\nrange 0, ..., W, where W is a relatively small number.\nHow can I modify that algorithm so that it takes time just O((|V| + |E|) logW) and relatively easy implement that in C/C++?</p>\n", 'ViewCount': '259', 'Title': "Dijkstra's algorithm for edge weights in range 0, ..., W", 'LastActivityDate': '2013-12-25T17:06:47.797', 'AnswerCount': '2', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12351', 'Tags': '<algorithms><algorithm-analysis><data-structures><shortest-path><weighted-graphs>', 'CreationDate': '2013-12-24T16:15:19.157', 'Id': '19252'},9233:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I know that standard Bloom Filters only have operations like inserting elements and checking if an element belongs to filter, but are also some modification of Bloom filters which enable a delete operation--for example: counting Bloom filters. I heard also about another method, which uses a second filter. If I want to remove an element I have to 'insert' it into this second filter. I can't find how this proposed structure operates, any article about it, or even the name of the originator. Maybe someone can share with me with a link to any interesting articles about this method? I found a lot of articles about counting Bloom filters and other methods, but I can't find any description of this one.</p>\n", 'ViewCount': '101', 'Title': 'Deleting in Bloom Filters', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:49.423', 'LastEditDate': '2014-04-29T11:35:49.423', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '12038', 'Tags': '<reference-request><data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2013-12-25T22:52:19.973', 'Id': '19292'},9234:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m both asking for me &amp; future readers as to how pictures, sounds, textures, assets, and such are compiled and linked in to one binary/program, and how that process works.</p>\n\n<p>I have seen this in standalone programs I have downloaded on Windows, and I\'m fairly certain any OS can implement this. It appears that, instead of keeping the data stored separately as individual files within the filesystem, all data is converged in to one file. When you run the program, it\'s as if the data could have just, instead of requiring multiple scattered files through the drive, directories, subdirectories, etc., been bound to one program with everything in it; no need for fragmentation, missing files, etc.</p>\n\n<p>So, to sum this question up, let me first illustrate some key points:</p>\n\n<p>I had originally thought that, <strong><em>since all files on the computer are basically high-and-low voltage sources at the lowest level</em></strong>, files can be converged together, being similar to <a href="http://en.wikipedia.org/wiki/Archive_file" rel="nofollow">"zipping/unzipping files."</a> Using this common ground, it\'s easy to see how the data of a file, <strong><em>any file</em></strong>, could be bound/binded together with other data, and act, from the scope of the filesystem and GUI, as "just a file." But many things are actually in it that don\'t have to only represent just a typical idea of a "program" with some people\'s view of it as only instructions like adding, moving data around, etc.</p>\n\n<p>I am not sure of this, but I have a good sense that it\'s similar, so this why I\'m asking for clarification.</p>\n\n<p>When you see a standalone program from one file that contains external assets when you run it (texture files, video data format, image data, sounds, other resources, etc.),  it\'s clear that the single executable file has all the data necessary "packed" in to one somehow, or the such.</p>\n\n<p>How does the compiler/linker do this, how would the program/library/etc. access the data converged in it as opposed to differing from separate "files" independently, and how are compiler\'s/linker\'s settings adjusted to do this in any general sense?</p>\n\n<p>NOTE: Programmers, StackOverflow, Superuser, and few others deny this question, and give no reason why, so I figured this area might see it fit.</p>\n', 'ViewCount': '52', 'Title': 'How are image files, sounds, data, etc. compiled/linked in to a single file?', 'LastEditorUserId': '268', 'LastActivityDate': '2013-12-27T00:44:21.287', 'LastEditDate': '2013-12-27T00:17:49.613', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '12391', 'Tags': '<terminology><data-structures><compilers>', 'CreationDate': '2013-12-26T21:57:39.090', 'Id': '19313'},9235:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>For example let's say that we know that the worst-case running time is o(n) and the best-case is o(1). how can i get the average-case running time using the given big Ohs?</p>\n", 'ViewCount': '91', 'Title': 'how to calculate Average-Case complexity time by using worst-case and best-case complexity time?', 'LastActivityDate': '2013-12-30T18:36:07.227', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12207', 'Tags': '<data-structures><asymptotics>', 'CreationDate': '2013-12-30T18:28:15.073', 'Id': '19381'},9236:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have a queue where I pull from left and push to the right, and suppose I have the contents in the queue as $a b c @ d e$ (from left to right, left is head, right is tail).</p>\n\n<p>Is there a simple algorithm that doesn't require extra structures that makes $e$ at the head? meaning to get us to the queue $eabc@d$?</p>\n\n<p>P.S.: I need an algorithm like that for the purpose of a queue automaton.</p>\n", 'ViewCount': '75', 'Title': 'Queue, moving the element at the tail to the head', 'LastActivityDate': '2014-01-02T16:04:11.710', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '19456', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '11424', 'Tags': '<data-structures>', 'CreationDate': '2014-01-02T14:59:03.937', 'Id': '19452'},9237:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I need a data structure which can include millions of elements, minimum and maximum must be accesable in constant time and inserting and erasing element time complexity must be better than linear.</p>\n', 'ViewCount': '202', 'Title': 'Which data structure to use for accessing min/max in constant-time?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-07T00:05:10.370', 'LastEditDate': '2014-01-06T06:26:13.367', 'AnswerCount': '3', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12619', 'Tags': '<data-structures><efficiency>', 'CreationDate': '2014-01-05T23:15:52.053', 'Id': '19518'},9238:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Conc lists are similar to cons lists. In contrast to folds, mapreduce is the main "iterating" operation used on it. Composed mapreduces produce intermediate lists. Is there a fusion law for them, similar to those for foldr/build and unfoldr/destroy?</p>\n', 'ViewCount': '15', 'Title': 'Is there a fusion law for the mapreduce operation used on conc-lists?', 'LastActivityDate': '2014-01-06T04:00:14.660', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '11547', 'Tags': '<algorithms><data-structures><programming-languages><functional-programming>', 'CreationDate': '2014-01-06T04:00:14.660', 'Id': '19526'},9239:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I was asked this question in examination. A square matrix $M$ of size $10 \\times 10$ is stored in memory with each element requiring 4 bytes of storage. If the base address at $M[0][0]$ is $1840$, determine the address at $M[4][8]$ when the matrix $M$ is stored row majorwise.\nI seriously don't understand this question at all. What kind of problem is this? How to solve these kind of problems?</p>\n", 'ViewCount': '36', 'ClosedDate': '2014-01-07T13:09:46.080', 'Title': 'How to determine the address of an element in a square matrix given the base address?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-07T13:05:30.903', 'LastEditDate': '2014-01-07T13:05:30.903', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12644', 'Tags': '<data-structures><matrices><memory-access>', 'CreationDate': '2014-01-07T11:40:20.837', 'Id': '19557'},9240:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Here is how deletion in B-trees is described:</p>\n\n<blockquote>\n  <ol>\n  <li><p>If the key k is in node x and x is a leaf, delete the key k from x.</p></li>\n  <li><p>If the key k is in node x and x is an internal node, do the following.</p>\n  \n  <p>a) If the child y that precedes k in node x has at least t keys, then find the predecessor k0 of k in the sub-tree rooted at y. Recursively delete k0, and replace k by k0 in x. (We can find k0 and delete it in a single downward pass.)</p>\n  \n  <p>b) If y has fewer than t keys, then, symmetrically, examine the child z that follows k in node x. If z has at least t keys, then find the successor k0 of k in the subtree rooted at z. Recursively delete k0, and replace k by k0 in x. (We can find k0 and delete it in a single downward pass.)</p>\n  \n  <p>c) Otherwise, if both y and z have only t-1 keys, merge k and all of z into y, so that x loses both k and the pointer to z, and y now contains 2t-1 keys. Then free z and recursively delete k from y.</p></li>\n  <li><p>If the key k is not present in internal node x, determine the root x.c(i) of the appropriate subtree that must contain k, if k is in the tree at all. If x.c(i) has only t-1 keys, execute step 3a or 3b as necessary to guarantee that we descend to a node containing at least t keys. Then finish by recursing on the appropriate child of x.</p></li>\n  </ol>\n</blockquote>\n\n<p>What is the actual difference between case 1 case 3? Both seem to be directing to a leaf node.</p>\n', 'ViewCount': '40', 'Title': 'Case distinction in B-tree deletion', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-08T17:04:46.533', 'LastEditDate': '2014-01-08T17:04:46.533', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11804', 'Tags': '<data-structures><search-trees>', 'CreationDate': '2014-01-08T12:53:21.410', 'Id': '19580'},9241:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We have an array of Integers, $A[]$ and we have to find the minimum number that is not the sum of a subset of array using the elements from $L$ to $R$ indices. I was thinking of using coin change DP approach, and outputing the min number with value infinity. But the problem is that the sum of ranges can be as large as 10<sup>9</sup>, and we have about 10<sup>5</sup> queries of the type $[L,R]$, so I was hoping there'd be a better approach. Can anyone point me in the right direction?</p>\n\n<p>There are 10<sup>5</sup> elements in the array</p>\n\n<p>Suppose the elements of the array are 1,1,2,7. Then for indices 1 and 4, the smallest number that cannot be formed as a sum is 5. since we can form all 1,2,3,4.</p>\n", 'ViewCount': '75', 'Title': 'Minimum number that cannot be formed by any subset of an array', 'LastEditorUserId': '9550', 'LastActivityDate': '2014-01-11T10:48:57.527', 'LastEditDate': '2014-01-11T10:48:57.527', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19653', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8505', 'Tags': '<data-structures><dynamic-programming>', 'CreationDate': '2014-01-11T09:33:41.120', 'Id': '19651'},9242:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Im learning the delete operation on <a href="http://www.cs.princeton.edu/~rs/talks/LLRB/08Dagstuhl/RedBlack.pdf" rel="nofollow">Left-leaning Red Black Tree</a> invented by Prof. Sedgewick. In delete operation, a node could be only deleted from a 3-node or a 4-node but 2-node. In order to ensure we dont end up at 2-node, we push the red link down when both the left and left of left of this node are black (if the cmp &lt; 0) by the code below</p>\n\n<pre><code>// Assuming that h is red and both h.left and h.left.left\n// are black, make h.left or one of its children red.\nprivate Node moveRedLeft(Node h) {\n    assert (h != null);\n    assert isRed(h) &amp;&amp; !isRed(h.left) &amp;&amp; !isRed(h.left.left);\n\n    flipColors(h);\n    if (isRed(h.right.left)) { \n        h.right = rotateRight(h.right);\n        h = rotateLeft(h);\n    }\n    return h;\n}\n</code></pre>\n\n<p>The fact is before we invoke this function, there is not any check if this node is red. So, This must be one of the characters of LLRB. Actually the image below just show this.</p>\n\n<p><img src="http://i.stack.imgur.com/FazCz.png" alt="one possible llrb tree"></p>\n\n<p>But how to prove this?</p>\n', 'ViewCount': '35', 'Title': 'LLRB Tree How to prove if left and left of left node are black, then this node must be a red node?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-13T19:09:14.983', 'LastEditDate': '2014-01-13T19:09:14.983', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12804', 'Tags': '<data-structures><binary-trees><search-trees>', 'CreationDate': '2014-01-13T16:07:25.153', 'Id': '19696'},9243:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Let $\\mathbb{M}$ be the set of all 3D triangle meshes.</p>\n\n<p>Let $a:\\mathbb{M} \\rightarrow \\mathbb{R}$ be a function that computes surface area of the mesh.</p>\n\n<p>Let $\\mathbb{T}$ be the set of 3D affine transformation matrices.</p>\n\n<p>Let $t:\\mathbb{M} \\times \\mathbb{T} \\rightarrow \\mathbb{M}$ be a function that transforms mesh $M$ with matrix $T$.</p>\n\n<p>Let $\\mathbb{C}$ be a set of values of unknown (at the moment) structure, with the following properties:</p>\n\n<ul>\n<li>There is a function $c:\\mathbb{M} \\rightarrow \\mathbb{C}$ that maps meshes to these values.</li>\n<li>There is a function $a:\\mathbb{C} \\rightarrow \\mathbb{R}$, such that $(\\forall M \\in \\mathbb{M})(a(c(M)) = a(M))$, i.e. computes surface area of the mesh indirectly, via first mapping it to $\\mathbb{C}$.</li>\n<li>There is a function $t:\\mathbb{C} \\times \\mathbb{T} \\rightarrow \\mathbb{C}$ such that $(\\forall M \\in \\mathbb{M})(\\forall T \\in \\mathbb{T})(t(c(M), T) = c(t(M, T)))$, i.e. $C$s can be transformed with the same effect on computed surface area as if transformation was applied to the mesh itself.</li>\n</ul>\n\n<p>So, is there such $\\mathbb{C}$? Obviously I don't want $\\mathbb{C} = \\mathbb{M}$, and prefer $a(C)$ and $t(C)$ to be of constant asymptotic complexity with respect to amount of triangles in the mesh.</p>\n", 'ViewCount': '63', 'Title': 'Triangle mesh surface area after affine transformation', 'LastActivityDate': '2014-01-14T16:45:09.260', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12837', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2014-01-14T16:45:09.260', 'Id': '19718'},9244:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>As we know, if we want to design and implement a programming language like JAVA, C++ and C language, we will reference the compilation theory. Similarly, if we would like to design and implement a data exchange format language, like JSON and Google protocol buffer to store the serialized objects, what theory shall we use to solve the problem? More precisely, does there exist a domain that study how to design a data exchange format and its corresponding books and publications?</p>\n\n<p>The format shall be terse, robust, readable and have efficient en/decoding. The dilemma between binary format and textual format etc.</p>\n', 'ViewCount': '35', 'ClosedDate': '2014-01-30T23:05:45.850', 'Title': 'The research area of data exchange format,i.e. the format that stores serialized objects', 'LastEditorUserId': '13038', 'LastActivityDate': '2014-01-30T19:28:28.310', 'LastEditDate': '2014-01-30T19:28:28.310', 'AnswerCount': '0', 'CommentCount': '12', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13038', 'Tags': '<data-structures><databases>', 'CreationDate': '2014-01-22T15:42:36.227', 'Id': '19893'},9245:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Suppose I have a list of distinct integers. I'm looking for a data structure that will support the operations <code>search</code>, <code>insert</code>, <code>delete</code> and <code>closest_pair</code> in $O(\\log n)$ time. </p>\n\n<p>I know that a sorted array supports <code>search</code>, <code>insert</code> and <code>delete</code> in $O(\\log n)$ time. So, all we need to do is maintain information about the closest pair during the <code>insert</code> and <code>delete</code> operations. </p>\n\n<p>The only problem is that the <code>closest_pair</code> would perform in better than $O(\\log n)$ time since information about the closest pair would already be available. So, I'm stuck at this point.</p>\n", 'ViewCount': '227', 'Title': 'Data Structure For Closest Pair Problem', 'LastEditorUserId': '98', 'LastActivityDate': '2014-01-26T21:35:15.070', 'LastEditDate': '2014-01-26T15:56:11.007', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '19998', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<data-structures><search-algorithms>', 'CreationDate': '2014-01-26T05:43:37.223', 'Id': '19984'},9246:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In the accepted answer to my question <a href="http://cs.stackexchange.com/a/19998/13022">Data Structure For Closest Pair Problem</a>, I do not see why deletion works.</p>\n\n<p>Let\'s say (x, y) are the closest pair before the delete. If the node to be deleted is either x or y, then we would have to recalculate the closest pair, which would not satisfy the O(log n) requirement. The best solution I currently see is that each time a "new" closest pair occurs during an insertion, we link it to the previous closest pair. That is, instead of storing just the closest pair in the root node, we store a linked list of closest pairs. However, this seems like overkill. </p>\n\n<p>Is there a better way resp. why does the proposed solution work?</p>\n', 'ViewCount': '134', 'ClosedDate': '2014-02-02T11:24:13.863', 'Title': 'AVL Trees - Maintaining Closest Pair information on a Delete', 'LastEditorUserId': '13022', 'LastActivityDate': '2014-01-28T12:17:10.023', 'LastEditDate': '2014-01-28T12:17:10.023', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<data-structures><search-trees>', 'CreationDate': '2014-01-27T04:16:48.070', 'Id': '20005'},9247:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>When talking about general data structure design, my lecture notes talk about one of the concerns being cost of operations. As well as the individual cost, it mentions amortized cost. But then it goes on to say:</p>\n\n<blockquote>\n  <p>Amortized cost can be:</p>\n  \n  <ul>\n  <li>absolute (e.g. for every sequence \u03c3 of operations (O(n log n))</li>\n  <li>competitive (e.g. for every sequence \u03c3 of operations O(OPT(\u03c3)))</li>\n  </ul>\n  \n  <p>where OPT(\u03c3) is the optimal cost of clairvoyant algorithms</p>\n</blockquote>\n\n<p>I can\'t really make any sense of this. Googling <a href="https://en.wikipedia.org/wiki/Page_replacement_algorithm#The_theoretically_optimal_page_replacement_algorithm" rel="nofollow">throws up this</a> but I can\'t see the relevance to data structures more generally. Can anyone help me understand the quoted text?</p>\n', 'ViewCount': '134', 'Title': 'What is a clairvoyant algorithm?', 'LastEditorUserId': '10036', 'LastActivityDate': '2014-02-11T22:58:46.517', 'LastEditDate': '2014-02-11T22:58:46.517', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '20037', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '10036', 'Tags': '<algorithms><terminology><data-structures><amortized-analysis>', 'CreationDate': '2014-01-28T10:31:46.363', 'Id': '20035'},9248:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>I am looking for implementation of the set data type. That is, we have to</p>\n\n<ul>\n<li>maintain a dynamic subset $S$ (of size $n$) from the universe $U = \\{0, 1, 2, 3, \\dots , u \u2013 1\\}$ of size $u$ with</li>\n<li>operations <code>insert(x)</code> (add an element <code>x</code> to $S$) and <code>find(x)</code> (checks whether element <code>x</code> is a member of $S$).</li>\n</ul>\n\n<p>I don't care about other operations. For orientation, in applications I'm working with we have $u \\approx 10^{10}$.</p>\n\n<p>I know of implementations that provide both operations in time $O(1)$, so I worry mostly about the size of data structure. I expect <em>billions</em> of entries but want to avoid swapping as much as possible.</p>\n\n<p>I am willing to sacrifice runtime if necessary. Amortised runtime of $O(\\log n)$ is what I can admit; expected runtimes or runtimes in $\\omega(\\log n)$ are not admissable.</p>\n\n<p>One idea I have is that if $S$ can be represented as a union of ranges <code>[xmin, xmax]</code>, then we will be able to save on storage size with the price of some performance decrease. Also, some other data patterns are possible, like <code>[0, 2, 4, 6]</code>.</p>\n\n<p>Could you please point me to data structures which can do something like that?</p>\n", 'ViewCount': '155', 'Title': 'Looking for a set implementation with small memory footprint', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-03T09:34:06.393', 'LastEditDate': '2014-01-31T08:03:57.357', 'AnswerCount': '2', 'CommentCount': '11', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '11850', 'Tags': '<data-structures><efficiency><space-complexity><sets><dictionaries>', 'CreationDate': '2014-01-29T16:42:55.737', 'Id': '20070'},9249:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>As per <a href="http://algs4.cs.princeton.edu/lectures/52Tries.pdf" rel="nofollow">Tries slides</a> (page 17) from Algorithm 4th edition book by Robert Sedgewick, the asymptotic expected runtime for an unsuccessful search in $R$-way tries miss is $O(\\log_R N)$. Can someone please explain how this number can be derived?</p>\n', 'ViewCount': '21', 'Title': 'Understanding expected time bound for unsuccessful search in R-way tries', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-06T16:21:49.280', 'LastEditDate': '2014-02-06T16:21:49.280', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '21387', 'Score': '1', 'OwnerDisplayName': 'ManojGumber', 'PostTypeId': '1', 'Tags': '<data-structures><runtime-analysis><search-trees>', 'CreationDate': '2014-01-12T22:11:10.073', 'Id': '21386'},9250:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>While reading the research paper <a href="http://dl.acm.org/citation.cfm?id=2108244" rel="nofollow">Polylogarithmic Concurrent Data Structures from Monotone Circuits [@JACM\'2012]</a> by James Aspnes, Hagit Attiya, and Keren Censor-Hillel, I am not sure about some points and need some verification and explanation.  </p>\n\n<p>In the first three sections of the paper, the authors presents constructions of useful concurrent data structures, including <strong>max register</strong> and <strong>counters</strong> with bounded values, with step complexity that is <em>polylogarithmic</em> in the number of values the object can take or the number of operations applied to it. Specifically (and extremely in brief),  </p>\n\n<blockquote>\n  <p>The max register is an object $r$ supporting both <code>WriteMax(r,t)</code> and <code>ReadMax(r)</code> operations. It is recursively constructed from a tree of increasingly large max registers. The implementation is wait-free and linearizable.    </p>\n  \n  <p>The counter, supporting an <code>CounterIncrement()</code> operation and a <code>ReadCounter()</code> operation, is structured as a binary tree of max registers. The implementation is also wait-free and linearizable. </p>\n</blockquote>\n\n<p>My problems are as follows:</p>\n\n<blockquote>\n  <p>(1) <strong>On the max register:</strong> What is the space complexity, i.e., the number of base objects of multi-writer multi-reader (MWMR, for short) registers, of the recursive implementation?  </p>\n</blockquote>\n\n<p>[[<strong>In my opinion:</strong>]] It is $2m - 1$ for there is exactly one MWMR register for each node in the tree. In particular, the tree can be thought of as the logic structure of an underlying array of $2m-1$ MWMR registers.</p>\n\n<blockquote>\n  <p>(2) <strong>Also on the max register:</strong> Is it possible to implement a max register with only a single MWMR register? Are there any related work? </p>\n</blockquote>\n\n<p>[[<strong>(EDIT) In my opinion:</strong>]] I have found a <a href="http://www.cs.bgu.ac.il/~satcc112/wiki.files/Jayanti-Time-and-Space-Lower-Bounds.pdf" rel="nofollow">related paper: Time and Space Lower Bounds for Non-blocking Implementations [@PODC\'1996]</a>, in which Jayanti et al. show that </p>\n\n<blockquote>\n  <p>Operations must take $\\Omega(n)$ <strong>space</strong> and $\\Omega(n)$ steps in the worst case, for many common data structures, including (unbounded) max registers and (unbounded) counters, where $n$ is the number of concurrent processes.</p>\n</blockquote>\n\n<p>However, I have not realized similar conclusions concerning about value-bounded data structures. </p>\n\n<blockquote>\n  <p>(3) <strong>On both the max register and the counter:</strong> Are there any related work on the max&amp;min register supporting both <code>ReadMax(r)</code> and <code>ReadMin(r)</code>? Similarly, are there any related work on the inc&amp;dec counter supporting both <code>CounterIncrement()</code> and <code>CounterDecrement()</code>?</p>\n</blockquote>\n', 'ViewCount': '22', 'Title': 'Polylogarithmic value-bounded concurrent data structures such as max register, counter, and monotone circuit', 'LastEditorUserId': '4911', 'LastActivityDate': '2014-02-13T05:53:17.450', 'LastEditDate': '2014-02-13T05:53:17.450', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4911', 'Tags': '<reference-request><data-structures><distributed-systems><concurrency>', 'CreationDate': '2014-02-12T08:03:37.973', 'Id': '21561'},9251:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Earlier this week a <a href="http://arxiv.org/pdf/1402.3036.pdf" rel="nofollow">paper</a> was released describing an algorithm for building optimal alphabetic ternary trees. The alphabetic property as described on <a href="http://en.wikipedia.org/wiki/Binary_search_tree#Optimal_binary_search_trees" rel="nofollow">Wikipedia</a> as </p>\n\n<blockquote>\n  <p>Alphabetic trees are Huffman trees with the additional constraint on\n  order, or, equivalently, search trees with the modification that all\n  elements are stored in the leaves. Faster algorithms exist for optimal\n  alphabetic binary trees (OABTs).</p>\n</blockquote>\n\n<p>From this, I understand that an alphabetic binary tree will produce shorter paths from root to element for elements that have been inserted into the tree more times than elements that haven\'t, and the alphabetic property would be described the same way for a ternary tree. </p>\n\n<p>I haven\'t been able to find a lot of discussion about applications for the alphabetic property outside of Huffman coding but it is not a requirement to implement the algorithm.  Are there any applications where the alphabetic property would be a requirement, and if not, are there any benefits to guaranteeing the alphabetic property that justifies increased implementation complexity?</p>\n', 'ViewCount': '49', 'Title': 'What are applications of alphabetic trees?', 'LastActivityDate': '2014-02-15T21:47:02.083', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14703', 'Tags': '<algorithms><data-structures><trees>', 'CreationDate': '2014-02-15T21:47:02.083', 'FavoriteCount': '1', 'Id': '21680'},9252:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>The Cheriton-Tarjan MST algorithm finds MSTs in time O(m log log n) in arbitrary graphs by using a cleverly-modified version of a leftist heap data structure to store edges. It was developed in 1976. The algorithm relies on the fact that leftist heaps can be merged in time O(log n), but also uses the fact that leftist heaps are binary trees and therefore that every node has at most two children.</p>\n\n<p>In 1978, binomial heaps were invented as a cleaner type of heap that supports merging in O(log n) time. Since then, most algorithms that need mergable heaps either use binomial heaps or some other related structure. However, since binomial heaps don't use binary trees, the Cheriton-Tarjan algorithm's main optimization (namely, eliminating unnecessary edges by doing a top-down DFS over the heap) won't work in the same time bounds when run on a binomial heap rather than a leftist heap.</p>\n\n<p>Has there been any work done to update the Cheriton-Tarjan MST algorithm to use binomial heaps rather than leftist heaps?</p>\n\n<p>Thanks!</p>\n", 'ViewCount': '39', 'Title': 'Updating the Cheriton-Tarjan MST algorithm to use binomial heaps?', 'LastActivityDate': '2014-02-16T00:48:54.857', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '2131', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2014-02-16T00:48:54.857', 'FavoriteCount': '1', 'Id': '21687'},9253:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>How can we prove a binary search tree with $n$ internal vertices has height $h = \\lceil \\log(n+1) \\rceil$? </p>\n', 'ViewCount': '31', 'ClosedDate': '2014-02-18T21:34:52.390', 'Title': 'Binary search tree with $n$ internal vertices', 'LastEditorUserId': '98', 'LastActivityDate': '2014-02-18T21:35:18.720', 'LastEditDate': '2014-02-18T21:35:18.720', 'AnswerCount': '0', 'CommentCount': '7', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14815', 'Tags': '<data-structures><combinatorics><trees><search-trees>', 'CreationDate': '2014-02-18T17:27:13.633', 'Id': '21781'},9254:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '178', 'Title': 'Algorithm Request: "Shortest non-existing substring over given alphabet"', 'LastEditDate': '2014-02-21T18:13:37.423', 'AnswerCount': '3', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '14918', 'FavoriteCount': '1', 'Body': "<p>I'm looking for an (efficient) algorithm to solve the following problem:</p>\n\n<blockquote>\n  <p>Given a string $S$ and a set of characters $M$, find the shortest string composed only of characters in $M$ that is <em>not</em> contained in $S$.</p>\n</blockquote>\n\n<p>Try as I might, I can't seem to map this problem to any of the standard CS string problems.</p>\n", 'Tags': '<algorithms><data-structures><strings><substrings>', 'LastEditorUserId': '14918', 'LastActivityDate': '2014-02-22T09:38:12.063', 'CommentCount': '3', 'AcceptedAnswerId': '21901', 'CreationDate': '2014-02-21T17:58:42.620', 'Id': '21896'},9255:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '157', 'Title': 'Basic action for every data structure O(1)', 'LastEditDate': '2014-02-24T18:34:26.200', 'AnswerCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Body': '<p>My lecturer for Algorithms said that most of the data structures I will encounter in the algorithms course I am taking have a basic action which is of O(1).</p>\n\n<p>Ex: Binary heap.</p>\n\n<p>Basic action is:</p>\n\n<blockquote>\n  <ol>\n  <li>Compare 2 childen.</li>\n  <li>Compare the "winner" with his parent.</li>\n  <li>Replace when needed.</li>\n  <li>Do 1-3 with the "winner", until and including the root.</li>\n  </ol>\n</blockquote>\n\n<p>How is O(1) even possible?</p>\n', 'ClosedDate': '2014-02-27T05:50:30.873', 'Tags': '<algorithms><data-structures>', 'LastEditorUserId': '14724', 'LastActivityDate': '2014-02-24T19:28:48.693', 'CommentCount': '2', 'AcceptedAnswerId': '21997', 'CreationDate': '2014-02-24T18:17:38.903', 'Id': '21985'},9256:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Given n, I want to randomly generate a binary tree (unlabelled) that has n end nodes. Could someone kindly provide a reference containing an algorithm for doing that?</p>\n\n<p>I attempted to do as follows: From a PRNG obtain n PRNs in [0.0, 1.0) as (relative) frequencies of n symbols for generating a Huffman tree (used in data compression). But, if the PRNs used are uniform, then I think this would highly favour generation of those Huffman trees that are more flat and Huffman trees corresponding to widely different frequencies of the symbols would be highly suppressed in the generation process. If this is correct, how could one do better? Thanks in advance.</p>\n', 'ViewCount': '63', 'Title': 'Generation of random binary trees', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-20T13:47:45.200', 'LastEditDate': '2014-02-28T08:39:33.350', 'AnswerCount': '2', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6437', 'Tags': '<data-structures><sampling><random-graphs>', 'CreationDate': '2014-02-27T17:34:00.827', 'Id': '22098'},9257:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I have the following homework question that I am struggling with. I have read the corresponding chapter from the book, but no guidance there.</p>\n\n<p>Consider a linked list $X: X_1 \\to X_2 \\to X_3 \\ldots$.\nAssume that the cost of examining a particular element $X_i$ is $C_i$. Note that to examine $X_i$, \none needs to scan through all elements in front of $X_i$. Let $P_i$ be the probability of \nsearching for element $X_i$, so the total cost for all searches is \n$$\n\\sum_{j=1}^{n} \\left( P_j \\cdot \\sum_{i=1}^{j} C_i \\right)\n$$</p>\n\n<ol>\n<li><p>Show that storing elements in non-increasing order of $P_i/C_i$ does not necessarily minimize the total cost. </p></li>\n<li><p>Show that storing elements in non-decreasing order of $P_i$ does not necessarily minimize the total cost. </p></li>\n</ol>\n\n<p>Any help and direction how to approach the problem will be highly appreciated.</p>\n', 'ViewCount': '127', 'Title': 'Impact on the order of elements on the cost of searching in a linked list', 'LastEditorUserId': '39', 'LastActivityDate': '2014-03-07T03:59:28.483', 'LastEditDate': '2014-03-04T11:09:47.673', 'AnswerCount': '1', 'CommentCount': '5', 'AcceptedAnswerId': '22365', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15266', 'Tags': '<algorithm-analysis><data-structures><linked-lists>', 'CreationDate': '2014-03-04T09:44:59.877', 'Id': '22264'},9258:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>For a regular array, I understand that if we have the tradeoff of space vs time, and we use more space to implement a Which, Data, and When pointers to the actual array, we can initialize the array in constant time because there are pointers to access and keep track of the elements in the array.</p>\n\n<p>How can I extend the idea of using the Which, Data, and When pointers to have constant time initialization for multi-dimensional arrays? Would I have to have multiple Which, Data, and When pointers to keep track of n-th D array dimensions?</p>\n\n<p>Or is the use of hierarchical tables, which stores multi-dimensional arrays as an array of pointers to tables, where each table contains a row of the array, and implementing the Which, Data, and When pointers to the hierarchical tables a correct way for having constant time initialization of multi-dimensional arrays?</p>\n\n<p><strong>Edit:</strong> Use C notation for simplicity. Let's say for a large <code>N</code> we have an array:</p>\n\n<pre><code>SomeType array[N];\n</code></pre>\n\n<p>If <code>N</code> is very large, and only a few of the array's elements are ever used, just initializing it can become the largest cost of an algorithm. A way around this is to have a self-checking structure that can be filled in on demand. To the above add (I'm not the original poster, so I will use my own names here):</p>\n\n<pre><code>int last_used = -1, place[N], order[N];\n</code></pre>\n\n<p>The idea is that <code>last_used</code> tells the last used entry, <code>place[i]</code> is the index of the <code>i</code>-th asigned element of <code>array</code>, and <code>order[k]</code> is the order in which the <code>array[k]</code> was initialized. Note that none of <code>array</code>, <code>place</code> and <code>order</code> are initialized, their initial values are arbitrary. <code>place[i]</code> and <code>order[k]</code> serve to check each other. To use <code>array[i]</code>, see if <code>order[i] &lt; last_used</code> (it is in the right range, might have been set already; if not, it is clearly garbage) and also <code>place[order[i]] == i</code>. If so, the element has been used, go ahead. If not, do:</p>\n\n<pre><code>last_used++;          /* Another one is in use */\nplace[last_used] = i; /* The next one in use is array[i] */\norder[i] = last_used; /* Point back */\ninitialize(array[i]); /* Prepare for use */\n/* Furiously frob array[i] */\n</code></pre>\n\n<p>The time of this is bounded by a constant; so the initialization time, amortized over the initialized elements, is constant. For a practical implementation, this can be packaged conveniently in a C++ class (templated on <code>SomeType</code> and <code>N</code>) overloading <code>operator[]</code>.</p>\n\n<p><strong>Remark:</strong> This can clearly be extended to an array of such arrays.</p>\n", 'ViewCount': '67', 'Title': 'How can I have constant time initialization for multi-dimensional arrays?', 'LastEditorUserId': '6447', 'LastActivityDate': '2014-03-12T22:54:29.357', 'LastEditDate': '2014-03-11T12:00:06.893', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13051', 'Tags': '<data-structures><arrays>', 'CreationDate': '2014-03-05T17:05:00.133', 'FavoriteCount': '1', 'Id': '22306'},9259:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Given an extendable heap with $n$ elements and an array size of $A$, I'm trying to use the accounting method to find the amortized cost of a delete. We want a load factor of $\\frac{1}{4}$.    </p>\n\n<p>So, the minimum number of deletes before we need to shrink the array is $\\frac{4n-A+3}{4}$.    </p>\n\n<p>The cost of shrinking at that point would be $n$</p>\n\n<p>So, the amortized cost would be $n/\\frac{4n - A + 3}{4}$</p>\n\n<p>Is my calculation correct?</p>\n", 'ViewCount': '46', 'Title': 'Amortized Cost of a delete from an extendable heap', 'LastEditorUserId': '8639', 'LastActivityDate': '2014-03-14T13:48:57.807', 'LastEditDate': '2014-03-14T13:48:57.807', 'AnswerCount': '0', 'CommentCount': '9', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<data-structures><asymptotics>', 'CreationDate': '2014-03-10T03:28:46.547', 'Id': '22452'},9260:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Consider the following Huffman trees:<br>\n<img src="http://i.stack.imgur.com/Q3bzR.png" alt="enter image description here"></p>\n\n<p>I was asked if those trees can have the same corpus. My answer was no, based on these calculations:  </p>\n\n<p>For the right tree:<br>\n$a_1 \\le a_2$<br>\n$a_1 + a_2 \\le a_5$<br>\n$a_3 \\le a_4$<br>\n$a_1 + a_2 + a_5 \\le a_3 + a_4$</p>\n\n<p>For the left tree:<br>\n$a_1 \\le a_2$<br>\n$a_3 \\le a_4$<br>\n$a_1 + a_2 + a_3 + a_4 \\le a_5$  </p>\n\n<p>Adding the last equations from each tree we have that:<br>\n$2a_1 + 2a_2 \\le 0$ Which is a contradiction because frequency cannot be negative.</p>\n\n<p>Nevertheless, I understood that there is a possibility that the two trees would have the same corpus. For instance, consider $1,1,1,2,3$.</p>\n\n<p>So, where do my calculations go wrong?</p>\n', 'ViewCount': '18', 'Title': 'Do the two huffman trees have the same corpus?', 'LastEditorUserId': '12859', 'LastActivityDate': '2014-03-11T02:11:18.050', 'LastEditDate': '2014-03-11T02:11:18.050', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22463', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15485', 'Tags': '<data-structures><trees><information-theory><data-compression>', 'CreationDate': '2014-03-10T10:19:44.723', 'Id': '22459'},9261:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '161', 'Title': 'What is the expected number of nodes at depth d of a tree after i random insertions', 'LastEditDate': '2014-03-14T09:33:19.310', 'AnswerCount': '1', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '2825', 'FavoriteCount': '0', 'Body': "<p>Suppose one wanted to build a tree at random.  Let the first insertion at step $i = 1$ be the root node.  From here, nodes are inserted into the tree at random one at a time.  How would one go about calculating the expected number of nodes $E(d)$ at depth $d$ after $i$ insertions?</p>\n\n<p>For example, At $i = 1$, it's just the root node, so $E(0) = 1$ and all other depths will have zero nodes.  At $i = 2$, $E(0) = 1$ and $E(1) = 1$ as the inserted node has to be at depth 1 from the root.  At $i = 3$, the next node may either be attached to the root node or the existing node at depth one, so the tree may either look like:</p>\n\n<p><code>\n    *\n   / \\\n  *   *\n</code></p>\n\n<p>Or:</p>\n\n<p><code>\n     *\n    /\n   *\n  /\n *\n</code></p>\n\n<p>Depending on what happend at $i = 3$, at $i = 4$ there's either a $2/3$ chance of the new node attaching at depth 2 and $1/3$ attaching at depth 1, or an even $1/3$ probability of the new node attaching at the root, the node at depth 1 or the node at depth 2.  Keeping random insertion in mind (not a binary or balanced tree in any way), how would one go about calculating the expected number of nodes at depth $d$ after $i$ insertions? </p>\n", 'Tags': '<data-structures><trees><average-case>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-14T09:33:19.310', 'CommentCount': '0', 'AcceptedAnswerId': '22583', 'CreationDate': '2014-03-13T11:50:12.127', 'Id': '22580'},9262:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Union-By-Rank and Path Compression is supposed to improve the performance of a tree implementation of a disjoint set.   </p>\n\n<p>However, in looking at the UNION(x, y) operation, I noticed that if x and y are actually the roots of the 2 trees being merged, no path compression actually takes place. The resulting tree would simply have a depth equal to the larger depth of the 2 trees.</p>\n\n<p>Is my understanding of the algorithm correct?</p>\n', 'ViewCount': '73', 'Title': 'Disjoint Set - Tree Implementation with Union-By-Rank and Path Compression', 'LastActivityDate': '2014-03-14T14:38:42.007', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8639', 'Tags': '<algorithms><data-structures>', 'CreationDate': '2014-03-14T14:04:46.157', 'Id': '22620'},9263:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Bloom filters are a variant of hash tables except it is much more space efficient at the cost of a low probability of false positives .</p>\n\n<p>How it works : Assume there are 10000 bits , 3 hash functions and an object Foo is to be inserted into the Bloom Filter .</p>\n\n<p>Insertions : Foo will be hashed by the 1st hash function and the 3405 bit index is set to 1 , Foo is hashed by the second hash function and the 1001 bit index is set to 1 , Foo is hashed by the third hash function and the 5555 bit index is set to 1 .</p>\n\n<p>Check exist : Foo will be hashed to the three different hash function and if all the bits at the respective index is set to 1 , the object is said to exist with a small chance of false positive else the object cant be found .</p>\n\n<p>My question is : Why do we use more than 1 (in this case 3 bits ) to determine if a objects exist in the Bloom Filter , doesnt it increase the chances of collision with other objects which may also set the same bit to 1 . To me , it seems best to use a single hash function as it saves the most space (1 bit only) and least chances of collision with other objects in the Bloom Filter</p>\n', 'ViewCount': '41', 'Title': 'Why do we use several bits instead of a single bit for storage of objects in Bloom Filters', 'LastActivityDate': '2014-03-16T15:25:29.777', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<data-structures><bloom-filters>', 'CreationDate': '2014-03-14T14:05:39.153', 'Id': '22621'},9264:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>$n_1$ is the number of insertions per unit time. <br>\n$n_2$ is the number of deletions per unit time. <br>\n$k$ is the number of searches per unit time.<br>\nI have 4 possible cases: <br>\n1. $k \\gg n_1,n_2$ with limited memory<br>\n2. $n_1 \\gg k,n_2$ with limited memory<br>\n3. $k, n_1, n_2$ are comparable with limited memory<br>\n4. $k, n_1, n_2$ are comparable with unlimited memory <br></p>\n\n<p>I have decided to use a doubly linked list for the second case and a direct address table for the fourth case.</p>\n\n<p>My question is about the first and the third cases. Both Hash Table and Tree (AVL or Red-Black) look promising. Should I choose the Hash Table with an amortised constant time or the Tree with a $\\log n$ worst case time?</p>\n', 'ViewCount': '74', 'Title': 'What is a suitable Data Structure in this scenario?', 'LastActivityDate': '2014-03-15T16:49:13.570', 'AnswerCount': '0', 'CommentCount': '5', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15730', 'Tags': '<data-structures>', 'CreationDate': '2014-03-15T16:49:13.570', 'FavoriteCount': '1', 'Id': '22653'},9265:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>(The question is an extension of <a href="http://stackoverflow.com/questions/9404909/algorithms-for-compression-of-set-tries">this</a> unanswered question on stackoverflow)</p>\n\n<p>If we have a set of strings, we can efficiently represent them with <a href="http://en.wikipedia.org/wiki/Trie" rel="nofollow">tries</a>. Common branches can also be merged, resulting in a DAG instead of a tree that is even more compact.</p>\n\n<p>However, if we have a set of sets (i.e. the order does not matter), there are a lot of possible tries that represent the same set of elements.\nAn example can be found in the stackoverflow question I linked above.</p>\n\n<p><strong>Edit:</strong> For example, assume that we are given the following sets of integers.</p>\n\n<pre><code>{1,2,3,4,5}\n{1,2,6,7}\n{1,2,4,7}\n{1,3,5,7}\n</code></pre>\n\n<p>Two possible representations are shown below (trie on the left, DAG on the right)</p>\n\n<p><img src="http://i.stack.imgur.com/fJr2z.png" alt="enter image description here"></p>\n\n<p>My questions are:</p>\n\n<blockquote>\n  <ol>\n  <li>How hard is the problem of finding an optimal (i.e minimal) such trie? </li>\n  <li>Are there any fast algorithms for solving this problem?</li>\n  <li>If not, are there any fast algorithms that find "good" tries?</li>\n  <li>What about the DAG case?</li>\n  </ol>\n</blockquote>\n\n<p>In the scenario I have in mind there is an additional constraint that no set is a subset of another set.</p>\n\n<p>Any link/paper that is even slightly relevant to any of the questions is helpful.</p>\n', 'ViewCount': '75', 'Title': 'Efficiently representing a set of sets', 'LastEditorUserId': '691', 'LastActivityDate': '2014-03-20T05:23:28.300', 'LastEditDate': '2014-03-19T16:06:09.610', 'AnswerCount': '3', 'CommentCount': '8', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '691', 'Tags': '<reference-request><data-structures><strings>', 'CreationDate': '2014-03-19T14:31:35.070', 'FavoriteCount': '1', 'Id': '22807'},9266:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've searched online for this but I only seem to find answers for a similar equation: </p>\n\n<pre><code>T(n) = T(n/3) + T(2n/3) + cn\n</code></pre>\n\n<p>But the one I'm trying to solve is:</p>\n\n<pre><code>T(n) = T(n/3) + T(2n/3)\n</code></pre>\n\n<p>Base case: We can assume <code>T(a) = Theta(1)</code> for any constant <code>a</code>.</p>\n\n<p>I've succeeded in proving (by induction) that <code>T(n) = O(n*log(n))</code>. I thought the answer should be <code>Theta(n*log(n))</code>, but I cannot prove that <code>T(n) = Omega(n*log(n))</code>.</p>\n\n<p>So my question is - am I correct that the answer is <code>O(n*log(n))</code>, and NOT <code>Theta(n*log(n))</code>? IF that's true that would really be great...</p>\n\n<p>If I'm wrong I will of course explain where I'm stuck in the induction process...</p>\n\n<p>Thanks!</p>\n\n<p>P.S. If you need to, please try to explain using induction, because I haven't learned all methods for solving these problems yet.</p>\n", 'ViewCount': '64', 'ClosedDate': '2014-03-20T09:00:30.173', 'Title': 'Recurrence of T(n) = T(n/3) + T(2n/3)', 'LastActivityDate': '2014-03-19T21:53:26.597', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '15868', 'Tags': '<algorithm-analysis><data-structures><runtime-analysis><recurrence-relation>', 'CreationDate': '2014-03-19T14:46:24.537', 'Id': '22809'},9267:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I was trying to understand the concept of Max-Heap. And to my understanding its a complete binary tree and each parent has a value greater than its children.The example I was going though had the following array which it said was a Max-Heap.</p>\n\n<pre><code>BookArray [] = {45,10,11,3,2,7,9,1,0}\n</code></pre>\n\n<p>I then decided to shuffle the elements (so they are no longer in binary heap) and got this.</p>\n\n<pre><code>Shuffled[] = {11,1,0,7,9,3,2,10,45}\n</code></pre>\n\n<p>I then decided to write a program that would sort the elements in the array in Min-Heap\nso I got this array</p>\n\n<pre><code>Sorted[] = {45,11,3,10,7,0,2,1,9}\n</code></pre>\n\n<p>My question is if my sorted array is also a valid max-heap ? since my array does not match the bookArray</p>\n', 'ViewCount': '34', 'Title': 'Is this a proper Max Heap Data Structure', 'LastEditorUserId': '2421', 'LastActivityDate': '2014-03-22T19:19:23.603', 'LastEditDate': '2014-03-22T18:30:06.260', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '22943', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '2421', 'Tags': '<data-structures><binary-trees><heaps>', 'CreationDate': '2014-03-22T17:50:21.793', 'Id': '22941'},9268:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Kaplan and Tarjan came up with the first ones in 1995; Okasaki came up with a simpler, amortized version in 1997 using lazy evaluation (See <a href="http://dl.acm.org/citation.cfm?doid=258948.258956" rel="nofollow">Okasaki</a> for references). Then a couple years later, Kaplan, Okasaki, and Tarjan came up with a simpler implementation using more general mutation in a disciplined manner. Then in 2003, <a href="http://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Notes%20on%20Catenable%20Deques.doc" rel="nofollow">Mihaesau and Tarjan</a> came up with a simpler, non-bootstrapped strictly functional version.</p>\n\n<h3>My questions</h3>\n\n<ol>\n<li><p>The Mihaesau and Tarjan catenable deques <em>appear</em>, to my untrained eye, to offer O(log n), or possibly even O(log(min{i, n-i})) random access (lookup and modify). Is this correct?</p></li>\n<li><p>Has anyone come up with any simplifications since then?</p></li>\n<li><p>Has anyone either found a way to combine O(log n) (or better, O(log(min{i,n-1})) ) splitting along with O(1) concatenation or proved that it can\'t be done? For that matter, what about O(log n) arbitrary insertion and/or deletion?</p></li>\n</ol>\n', 'ViewCount': '22', 'Title': 'What progress has been made on persistent catenable deques in the last decade?', 'LastEditorUserId': '14740', 'LastActivityDate': '2014-03-25T02:41:50.120', 'LastEditDate': '2014-03-25T00:29:57.533', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '14740', 'Tags': '<data-structures><functional-programming>', 'CreationDate': '2014-03-25T00:06:04.060', 'FavoriteCount': '1', 'Id': '23023'},9269:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am having trouble in understanding <a href="http://en.wikipedia.org/wiki/Order_statistic_tree" rel="nofollow">order-statistics tree</a> .   </p>\n\n<p><strong>Definition :</strong><br>\nEvery node in tree stores the number of descendants of itself .<br>\nCan you please explain the Algorithm or pseudocode how to <strong>Insert</strong> and <strong>Delete</strong> a node in the tree <strong>when tree is to be balanced</strong> .  </p>\n\n<p>Basically i want to find the rank of an element in the tree in <strong>O(log N)</strong> time complexity and nodes can be inserted and deleted in between querries .  </p>\n\n<p>P.S. I am able to understand the <strong>rank(x)</strong> operation .</p>\n', 'ViewCount': '36', 'Title': 'insert and delete in order statistic tree', 'LastEditorUserId': '635', 'LastActivityDate': '2014-03-27T03:22:10.707', 'LastEditDate': '2014-03-27T03:22:10.707', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '16107', 'Tags': '<algorithms><data-structures><binary-trees>', 'CreationDate': '2014-03-26T13:08:12.103', 'Id': '23073'},9270:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '76', 'Title': 'Why should leaf nodes in a red-black tree be black?', 'LastEditDate': '2014-03-27T15:03:50.240', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16188', 'FavoriteCount': '1', 'Body': u'<p>From the property of Red-Black Trees we know that: </p>\n\n<ul>\n<li>All leaves (NIL) are black. (All leaves are same color as the root.)(Comren et al "Introduction to Algorithms")</li>\n</ul>\n\n<p><img src="http://i.stack.imgur.com/hz7wf.png" alt="An example of a red\u2013black tree. From Wikipedia"></p>\n\n<p>But what is the reason that we should enforce them as Black, even though they\'re NILL\'s? </p>\n', 'Tags': '<terminology><data-structures><search-trees><dictionaries>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T05:15:31.620', 'CommentCount': '1', 'AcceptedAnswerId': '23123', 'CreationDate': '2014-03-27T10:13:14.353', 'Id': '23119'},9271:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m writing a balanced $n_d$-Hyperoctree data structure in which the only fundamental operations I provide are edge traversals between parent and child nodes. I\'m storing the nodes using a Morton z-curve.</p>\n\n<p>Using only this information:</p>\n\n<ul>\n<li>The level of a node can be computed by traversing to the root of the tree. </li>\n<li>Neighbor searches are performed by traversing the tree up until a common parent is found, and then traversing the tree back down again. </li>\n</ul>\n\n<p>I can compute the following constants depending on the number of spatial dimensions $n_d$:</p>\n\n<ul>\n<li>the #of children of a node: $2^{n_d}$,</li>\n<li><p>the #of $n_d - m$ dimensional neighbors for each node (in 3D, $n_d - 1$ neighbors are faces, $n_d - 2$ neighbors are edges, $n_d - 3$ neighbors are corners):</p>\n\n<p>$2^{n_d} \\begin{pmatrix} n_d \\\\ m \\end{pmatrix}$ </p></li>\n</ul>\n\n<p>The neighbors sharing a $n_d - 1$-dimensional face with each Hypercube form the set $\\mathcal{N}_{n_d - 1}$. They are numerated within this set as follows. The neighbor at the negative side from the node center comes first, then it comes the neighbor at the positive side. That is, for $n_d = 1$ I just have $\\mathcal{N}_{n_d - 1} = \\lbrace 0, 1 \\rbrace = \\lbrace \\mathrm{Left}, \\mathrm{Right} \\rbrace$. </p>\n\n<ul>\n<li><p>In $n_d = 2$, $\\mathcal{N}_{n_d - 1} = \\lbrace 0, 1, 2, 3 \\rbrace = \\lbrace \\mathrm{L}, \\mathrm{R}, \\mathrm{Top}, \\mathrm{Bottom} \\rbrace$. </p></li>\n<li><p>In $n_d = 3$ it is $\\mathcal{N}_{n_d - 1} = \\lbrace 0, 1, 2, 3, 4, 5 \\rbrace = \\lbrace \\mathrm{L}, \\mathrm{R}, \\mathrm{T}, \\mathrm{B}, \\mathrm{Front}, \\mathrm{Back} \\rbrace$</p></li>\n</ul>\n\n<p>and so on. </p>\n\n<p>I haven\'t been able yet to generalize the algorithm for finding the $n_d - 1$ neighbor located at a given position to arbitrary dimensions. My current algorithm traverses the tree up until a common parent node is found. During the up traversal it stores the child positions w.r.t. their parents of the nodes traversed. Then it traverses the tree back down using the reversed path of the up traversal. The child positions are found by finding the siblings of the child positions during the up traversal in the inverted neighbor direction, which can be computed as:</p>\n\n<p>$\\mathrm{inverted\\_neighbor\\_position}_{n_d - 1}(p) = (p + 1) * (p \\% 2 = 0) + (p - 1) * (p \\% 2 \\neq 0)$</p>\n\n<p>However, to know if a common parent has been found, it checks if a parent node has a childe at a given relative position of another child which expressed as a neighbor position, i.e., it checks if a node has a sibling in a given neighbor direction within its parent. I have only a hand-coded stencil for this check and haven\'t been able to generalized.</p>\n\n<ul>\n<li><p>Can this check for a "common parent" be generalized to arbitrary dimensions? How?</p></li>\n<li><p>Example up to 3D: for a child and a neighbor position, returns the child position of the sibling:</p>\n\n<pre><code>// i means, there is no sibling for that child in that direction\n//0  1  2  3  4  5     &lt;&lt; nghbr position\n{ i, 1, i, 2, i, 4},  // child 0\n{ 0, i, i, 3, i, 5},  // child 1\n{ i, 3, 0, i, i, 6},  // child 2\n{ 2, i, 1, i, i, 7},  // child 3\n{ i, 5, i, 6, 0, i},  // child 4\n{ 4, i, i, 7, 1, i},  // child 5\n{ i, 7, 4, i, 2, i},  // child 6\n{ 6, i, 5, i, 3, i}   // child 7\n</code></pre></li>\n</ul>\n\n<p>Assuming that the $n_d - 1$ dimensional neighbors can be found:</p>\n\n<ul>\n<li><p>How can I find $n_d - m$ dimensional neighbors?</p>\n\n<ul>\n<li>Right now, I hand-coded traversals that perform $m$ times searches for $(n_d - 1)$ neighbors. Is there a general way to generate these traversals for $n_d$ dimensions and the $n_d - m$ neighbors?</li>\n</ul></li>\n<li><p>What is a suitable order for $n_d - m$ dimensional neighbors where $m &gt; 1$ ?</p></li>\n</ul>\n\n<p>Most of the research I\'ve found in the literature uses hand coded stencils for, 1,2,3 and 4 dimensions, but no general ways of generating these stencils. So if anyone can point me to relevant literature I would appreciate it.</p>\n', 'ViewCount': '50', 'Title': 'Finding nd - m dimensional neighbors for a given node within a balanced hyperoctree', 'LastEditorUserId': '16250', 'LastActivityDate': '2014-04-14T14:53:55.267', 'LastEditDate': '2014-04-14T14:53:55.267', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '16250', 'Tags': '<algorithms><data-structures><computational-geometry>', 'CreationDate': '2014-03-28T21:36:29.753', 'Id': '23194'},9272:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've a subset of the simple paths in a graph. The length of the paths is bounded by $d$. </p>\n\n<p>What's the most compact way (memory-wise) I can represent the paths such that no other paths apart from the selected ones are represented? </p>\n\n<p>Note that I want to use this representation in an algorithm that will iterate through this subset of paths over and over again and that I want to be fairly fast, so for instance, I can't use any standard compression algorithms.</p>\n\n<p>One representation that came to my mind was representing them as a set of trees. I'm guessing though that getting it down to an optimal number of trees is NP-hard? What other representations would be good?</p>\n", 'ViewCount': '96', 'Title': 'Compact representation of paths in a graph', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-31T14:47:48.067', 'LastEditDate': '2014-03-29T11:56:06.693', 'AnswerCount': '3', 'CommentCount': '6', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '8', 'Tags': '<graphs><data-structures>', 'CreationDate': '2014-03-29T06:40:49.770', 'FavoriteCount': '1', 'Id': '23213'},9273:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I am now preparing for a test in my algorithms course and I have stumbled upon a question about a data structure which seems too trivial for me, but is probably not trivial at all.</p>\n\n<p>The question is:</p>\n\n<blockquote>\n  <p>Let a "minimum stack" be a data structure that supports the following functions:</p>\n  \n  <ol>\n  <li><p>Creating a new empty data structure.</p></li>\n  <li><p>Inserting element X.</p></li>\n  <li><p>Returning the newest element and removing it from the data structure.</p></li>\n  <li><p>Returning the minimal element (the element with the smallest value). (without removing it)</p></li>\n  <li><p>Changing the minimal element\'s value to k. (Hint: say T is the number of elements added after the minimal element).</p></li>\n  </ol>\n</blockquote>\n\n<p>Now, I have thought about using a linked list which is isomorphic to an actual stack, hence elements can be added and removed only from the tail, but scanning the list from head to tail is possible.</p>\n\n<p>I\'ve checked and all the functions except 4 and 5 turn out to be O(1), but 4 turns out to be O(n) at best, and 5 turns out to be O(T).</p>\n\n<p>My question is: How can I do 4 in O(1) time, so that all the other functions are also O(1)?\nI am not looking for full answers, just hints that will guide me to a full answer.</p>\n', 'ViewCount': '43', 'Title': '"Minimum Stack"', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-29T12:04:06.387', 'LastEditDate': '2014-03-29T12:04:06.387', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<data-structures><priority-queues><stack>', 'CreationDate': '2014-03-29T08:33:27.600', 'Id': '23216'},9274:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I\'m trying to find a linear solution with a small constant factor but I\'m not sure what to search for, or even how to succinctly describe it.  The best I\'ve come up with is:</p>\n\n<blockquote>\n  <p>Given a set of rectangles on a plane find the set(s) which allow the same <code>y</code> value for some largest contiguous set of <code>x</code> values.  All rectangles are axis aligned, the same width, and do not overlap.</p>\n</blockquote>\n\n<p>I find this much easier to visualize so below is an example problem instance and solution.</p>\n\n<p><img src="http://i.stack.imgur.com/uKphj.png" alt="Longest horizontal intersection of contiguous blocks"></p>\n\n<p>Edit:</p>\n\n<p>We have a linear solution that has a constant factor on the order $y_{max} - y_{min}$ which can be pretty big.  Here is an idea of the algorithm I\'ve been trying to work out since originally posting this.</p>\n\n<ul>\n<li>Rectangles are already sorted by $x$ position.</li>\n<li>Maintain an ordered list $I$ of the intervals currently allowing a contiguous line.</li>\n<li>Maintain two variables $(i_{min}, i_{max})$ which are the min and max\n$y$ value of the current intersection (in the example solution $(5, 5.25)$).</li>\n<li>Iterate from $x_0$ to $x_{max}$</li>\n<li><p>At each $x$ position test if any of the current rectangle(s) intersect $(i_{min}, i_{max})$.</p>\n\n<ul>\n<li>1) If yes, add the rectangle to $I$ and update $(i_{min}, i_{max})$.</li>\n<li>2) If no, find the longest suffix of $I$ s.t. it allows overlap with current rectangle.</li>\n<li>3) If no suffix exists or the current $x$ position has no rectangles skip to the next $x$ position with rectangles and reinitialize $I$, and $(i_{min}, i_{max})$.</li>\n</ul></li>\n<li><p>In #2 and #3 save the current $I$ if it allows the widest contiguous line so far.</p></li>\n</ul>\n', 'ViewCount': '51', 'Title': 'What is this problem? Largest set of contiguous x values for which the same y value can be held', 'LastEditorUserId': '527', 'LastActivityDate': '2014-04-03T00:34:55.823', 'LastEditDate': '2014-04-02T23:59:58.480', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '527', 'Tags': '<algorithms><data-structures><computational-geometry><intervals>', 'CreationDate': '2014-04-02T19:00:40.687', 'FavoriteCount': '1', 'Id': '23361'},9275:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>An associative array is a set of pairs (key, value). </p>\n\n<p>Does it represent a mapping, or a relation (i.e. does it allow the same key to be associated to different values)?</p>\n\n<p>Does it represent an injective mapping only, or does it allow different keys to be associated to the same value?</p>\n\n<p>Can a hash table implement any associative array, or just some special types of associative array?</p>\n\n<p>Thanks!</p>\n', 'ViewCount': '22', 'Title': 'Does an associative array represent a mapping?', 'LastEditorUserId': '336', 'LastActivityDate': '2014-04-04T17:31:05.390', 'LastEditDate': '2014-04-04T17:31:05.390', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '336', 'Tags': '<data-structures>', 'CreationDate': '2014-04-04T16:53:12.367', 'Id': '23427'},9276:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The <a href="http://en.wikipedia.org/wiki/K-d_tree" rel="nofollow">link</a> in wikipedia about kd-trees store points in the inner nodes. I have to perform NN queries and I <strong>think</strong> (newbie here), I am understanding the concept.</p>\n\n<p>However, I was said to study Kd-trees from Computational Geometry Algorithms and Applications (De Berg, Cheong, Van Kreveld and Overmars), section 5.2, page 99. The main difference I can see is that Overmars places the splitting data in the inner nodes and the actual points of the dataset in the leaves. For example, in 2D, an inner node will hold the splitting line.</p>\n\n<p>Wikipedia on the other hand, seems to store points in inner nodes and leaves (while Overmars only on leaves).</p>\n\n<p>In this case, how do we perform nearest neighbour search? Moreover, why there is this difference?</p>\n', 'ViewCount': '36', 'Title': 'kd-tree stores points in inner nodes? If yes, how to search for NN?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-10T14:43:00.437', 'LastEditDate': '2014-04-10T14:43:00.437', 'AnswerCount': '0', 'CommentCount': '6', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '16638', 'Tags': '<data-structures><computational-geometry><search-algorithms><search-trees><nearest-neighbour>', 'CreationDate': '2014-04-10T13:20:27.113', 'Id': '23636'},9277:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>The 1-D <strong>distinct</strong> closest pair of points problem is as follows: Given a set of n <strong>distinct integer</strong> points on real line, find a pair of points with the smallest distance between them, here the distance between two points p_i and p_j is absolute difference of their values, i.e, |p_i-p_j|. </p>\n\n<p>A naive way to solve this problem is to sort the points and check distance of each consecutive points in sorted order and take the minimum of such distances and this takes O(nlog n) time.</p>\n\n<p>My question is can we do better than that? Can we solve it in o(n log n)(note the little-oh) time? If not, then how to show an omega(nlog n) time bound for this problem?</p>\n\n<p>Note that, if the <strong>distinctness</strong> criteria was not there we could have shown a lower bound using Element Distinctness Problem.</p>\n', 'ViewCount': '20', 'Title': 'Linearithmic lower bound for 1-D "distinct" closest pair of points problem', 'LastActivityDate': '2014-04-25T20:06:05.070', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24116', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '15697', 'Tags': '<algorithms><data-structures><sorting><lower-bounds>', 'CreationDate': '2014-04-25T19:47:38.497', 'Id': '24115'},9278:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '871', 'Title': 'Is there an anti-Bloom filter?', 'LastEditDate': '2014-04-29T11:35:57.480', 'AnswerCount': '4', 'Score': '17', 'PostTypeId': '1', 'OwnerUserId': '5323', 'FavoriteCount': '4', 'Body': '<p>A <a href="https://en.wikipedia.org/wiki/Bloom_filter" rel="nofollow">Bloom filter</a> makes it possible to efficiently keep track of whether various values have already been encountered during processing.  When there are many data items then a Bloom filter can result in a significant memory saving over a hash table.  The main feature of a Bloom filter, which it shares with a hash table, is that it always says "not new" if an item is not new, but there is a non-zero probability that an item will be flagged as "not new" even when it is new.</p>\n\n<blockquote>\n  <p>Is there an "anti-Bloom filter", which has the opposite behaviour?</p>\n</blockquote>\n\n<p>In other words: is there an efficient data structure which says "new" if an item is new, but which might also say "new" for some items which are not new?</p>\n\n<p>Keeping all the previously seen items (for instance, in a sorted linked list) satisfies the first requirement but may use a lot of memory.  I am hoping it is also unnecessary, given the relaxed second requirement.</p>\n\n<hr>\n\n<p>For those who prefer a more formal treatment, write $b(x) = 1$ if the Bloom filter thinks $x$ is new, $b(x) = 0$ otherwise, and write $n(x) = 1$ if $x$ really is new and $n(x) = 0$ otherwise.</p>\n\n<p>Then $Pr[b(x) = 0 | n(x) = 0] = 1$; $Pr[b(x) = 0 | n(x) = 1] = \\alpha$; $Pr[b(x) = 1 | n(x) = 0] = 0$; $Pr[b(x) = 1 | n(x) = 1] = 1 - \\alpha$, for some $0 &lt; \\alpha &lt; 1$.</p>\n\n<p>I am asking: does an efficient data structure exist, implementing a function $b\'$ with some $0 &lt; \\beta &lt; 1$, such that $Pr[b\'(x) = 0 | n(x) = 0] = \\beta$; $Pr[b\'(x) = 0 | n(x) = 1] = 0$; $Pr[b\'(x) = 1 | n(x) = 0] = 1 - \\beta$; $Pr[b\'(x) = 1 | n(x) = 1] = 1$?</p>\n\n<hr>\n\n<p><strong>Edit:</strong> It seems this question has been asked before on StackExchange, as <a href="http://stackoverflow.com/questions/635728">http://stackoverflow.com/questions/635728</a> and <a href="http://cstheory.stackexchange.com/questions/6596">http://cstheory.stackexchange.com/questions/6596</a> with a range of answers from "can\'t be done" through "can be done, at some cost" to "it is trivial to do, by reversing the values of $b$".  It is not yet clear to me what the "right" answer is.  What <em>is</em> clear is that an LRU caching scheme of some sort (such as the one suggested by Ilmari Karonen) works rather well, is easy to implement, and resulted in a 50% reduction in the time taken to run my code.</p>\n', 'Tags': '<reference-request><data-structures><hash><bloom-filters><dictionaries>', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T11:35:57.480', 'CommentCount': '3', 'AcceptedAnswerId': '24122', 'CreationDate': '2014-04-25T21:08:54.120', 'Id': '24118'},9279:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>We have an n-ary tree used for searching that we'd like to keep balanced.  It is currently mostly a B-tree without the balancing operations. The issue we have for implementing those is that each sub-tree has its own sorting criteria (chosen in a set of five, if that's important) and that the balancing algorithms we know don't work in that case.</p>\n\n<p>Is there a known algorithm which work in that case?  Or for a variation, we would probably reduce the number of criteria -- but not to one! -- and ensure that all nodes at a given level are using the same criteria if that help to keep the tree balanced at a reasonable cost and avoid rebuilding the whole tree like we do in some cases.</p>\n", 'ViewCount': '13', 'Title': 'Balanced multi-criteria trees', 'LastActivityDate': '2014-04-28T17:40:10.260', 'AnswerCount': '0', 'CommentCount': '2', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '584', 'Tags': '<data-structures><trees>', 'CreationDate': '2014-04-28T17:40:10.260', 'Id': '24191'},9280:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Let\'s say you have the following sentence: "This is my first cs question posted here". How would I go about inserting the sentence into a search tree. Do I assign each word a number value and perform the insertions based on those? </p>\n', 'ViewCount': '15', 'Title': 'Inserting a sentence into search trees', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-29T08:37:33.313', 'LastEditDate': '2014-04-29T08:37:33.313', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17159', 'Tags': '<data-structures><strings><search-trees>', 'CreationDate': '2014-04-29T01:30:52.450', 'Id': '24204'},9281:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I've been playing around with a simple probabilistic data structure which is very similar to a Bloom filter. Where a Bloom filter would use $k$ independent hash functions to choose $k$ of the $m$ bits to set, this structure uses $m$ hash functions, and sets each bit with probability $p$.</p>\n\n<p>This structure doesn't produce as low a false-positive rate as Bloom filters, but it seems to be extremely fast to compute, particularly if $m$ is some multiple of the machine word size and $p = 2^{-b}$ for some integer $b$: The hash functions can be computed in parallel by AND-ing $b$ independent $m$-bit hashes, and no dependent indexing or variable bitshifts are required.</p>\n\n<p>I'm certain someone's come up with this idea before me, and done a lot more advanced analysis and comparison of it than I'm qualified to do. Is there a particular name for this type of structure?</p>\n", 'ViewCount': '33', 'Title': 'Bloom filter variant', 'LastEditorUserId': '8410', 'LastActivityDate': '2014-04-29T17:07:58.197', 'LastEditDate': '2014-04-29T13:27:12.427', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '24230', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8410', 'Tags': '<reference-request><data-structures><probabilistic-algorithms><bloom-filters><dictionaries>', 'CreationDate': '2014-04-29T10:55:19.287', 'Id': '24217'},9282:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '58', 'Title': 'Tree data structure for fast merges', 'LastEditDate': '2014-04-30T18:06:55.563', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17191', 'Body': '<p>I need trees that have the following properties:</p>\n\n<ol>\n<li><p>Each node in the tree has two values associated with it - a key and an associated opaque data element.</p></li>\n<li><p>An internal node in the tree has unbounded number of children. The tree reflects a real world hierarchy that is in flux over time - hence the maximum number of children of a given node are not known ahead of time.</p></li>\n<li><p>There is an ordering defined on sibling nodes that is a function of the keys stored in the nodes. </p></li>\n</ol>\n\n<p>Allow the following operations to be $O(\\lg n)$.</p>\n\n<h2>Update operations</h2>\n\n<ul>\n<li><p><code>merge(tree_1, tree_2)</code> - Destructively consumes <code>tree_1</code> and <code>tree_2</code> to create a new tree which contains keys from both input trees. I realize now that this operation is underdefined, I will put more thought into the semantics of the merge.</p></li>\n<li><p><code>insert(tree, parent_key, child_key, value)</code> - inserts the given key-value pair into the given subtree rooted at the node pointed to by the parent key. </p></li>\n<li><p><code>delete(tree, key)</code> - Delete subtree rooted at node with given key.</p></li>\n<li><p><code>update(tree, key, value)</code> - Destructively updates the existing data associated with the given key-value pair.</p></li>\n</ul>\n\n<h2>Query operations</h2>\n\n<ul>\n<li><p><code>find(tree, key)</code> - returns the value associated with the given key in the given tree. </p></li>\n<li><p><code>get_tree(tree, key)</code> - Return a subtree that is rooted at node with given key. The returned tree must a reference and share identity with corresponding nodes in the incoming tree. Modifying any nodes via the returned tree will hence result in changes to the initial tree. </p></li>\n<li><p><code>children(tree, key)</code> - Returns sequence of (key, data) of child nodes of node corresponding to key. </p></li>\n</ul>\n\n<p>Things I looked at before I asked this question - Binary trees, AVL trees, Red Black trees, 2-3 trees and they were not suitable because of fixed degree of internal nodes.</p>\n', 'ClosedDate': '2014-04-30T17:45:07.280', 'Tags': '<data-structures><trees><dictionaries>', 'LastEditorUserId': '17191', 'LastActivityDate': '2014-04-30T18:06:55.563', 'CommentCount': '6', 'AcceptedAnswerId': '24241', 'CreationDate': '2014-04-29T21:23:05.400', 'Id': '24235'},9283:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I would like to know if this type of special set operator exists, and if yes what is it called and if it has any other special properties. </p>\n\n<p>Lets say I have this set $S$ of items. Like all sets, if the same item is added twice in the set, the set will not add it again. However, when adding an item, using this special operator, it also checks if it is 'better' than another item already in the set, and if yes it replaces it with the new item rather than letting both.</p>\n\n<p>For a more concrete example:</p>\n\n<p>$S_1 = \\{apple, orange, pear\\}$</p>\n\n<p>$S_2 = \\{betterApple, orange, banana\\}$</p>\n\n<p>$S = S_1 \\cup_\\succ S_2 = \\{betterApple, orange, pear, banana\\} $</p>\n\n<p>So in this case $\\cup_\\succ$ checked some relationship, lets say $\\succ$ where $betterApple \\succ apple$, and if such a relationship existed it dropped $apple$ rather than allowing both in the set, keeping the <em>stronger</em> one in the set.</p>\n", 'ViewCount': '23', 'Title': 'Set that keeps unique categories of objects', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-30T18:12:34.367', 'LastEditDate': '2014-04-30T17:39:57.390', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '17222', 'Tags': '<data-structures><sets>', 'CreationDate': '2014-04-30T16:28:42.960', 'Id': '24262'},9284:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>In my algorithms course I have learned about the binary search tree and its functions add, find and remove.\nI have also learned about how to find the successor and the predecessor in a balanced binary search tree, but they are both $O(\\log n)$ I think.</p>\n\n<p>Now, I have been thinking, how can I change a given binary search tree so that I could find any given node's successor in $O(1)$ time? The other operations should still run in $O(\\log n)$ time.</p>\n\n<p>I have thought about maintaining the tree in a linked list but I am sure that there is a better solution to it.</p>\n", 'ViewCount': '36', 'ClosedDate': '2014-05-02T21:52:16.723', 'Title': 'Finding a successor in a binary search tree in $O(1)$', 'LastEditorUserId': '98', 'LastActivityDate': '2014-05-02T21:51:10.597', 'LastEditDate': '2014-05-02T21:51:10.597', 'AnswerCount': '0', 'CommentCount': '1', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14724', 'Tags': '<data-structures><search-trees>', 'CreationDate': '2014-05-02T15:37:33.333', 'Id': '24317'},9285:{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>I know Distributed Graph Coloring algorithm in O(log* n)\nwhich is given at P11: <a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/chapter1.pdf" rel="nofollow">Vertex Coloring</a></p>\n\n<p>same for Maximal Independent Set [MIS] they gave remark like algorithms exist in O(log* n) time at P70: <a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/chapter7.pdf" rel="nofollow">Maximal Independetn Set</a></p>\n\n<p>How we can reduce Graph coloring problem to MIS in O(log* n) time?</p>\n\n<p>If you feel difficulty in understanding algorithm then please comment. </p>\n', 'ViewCount': '11', 'Title': 'MIS algorithm for Tree in O(log* n) time', 'LastActivityDate': '2014-05-04T02:29:39.507', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '9665', 'Tags': '<algorithms><graph-theory><data-structures><distributed-systems>', 'CreationDate': '2014-05-03T22:14:11.747', 'Id': '24369'}