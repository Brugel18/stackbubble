{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '197', 'Title': 'Floating point rounding', 'LastEditDate': '2012-08-14T20:48:35.540', 'AnswerCount': '1', 'Score': '11', 'PostTypeId': '1', 'OwnerUserId': '565', 'FavoriteCount': '2', 'Body': '<p>Can an IEEE-754 floating point number &lt; 1 (i.e. generated with a random number generator which generates a number >= 0.0 and &lt; 1.0) ever be multiplied by some integer (in floating point form) to get a number equal to or larger than that integer due to rounding?</p>\n\n<p>i.e.</p>\n\n<pre><code>double r = random() ; // generates a floating point number in [0, 1)\ndouble n = some_int ;\nif (n * r &gt;= n) {\n    print \'Rounding Happened\' ;\n}\n</code></pre>\n\n<p>This might be equivalent to saying that does there exist an N and R such that if R is the largest number less than 1 which can be represented in IEEE-754 then N * R >= N (where * and >= are appropriate IEEE-754 operators)</p>\n\n<p>This comes from <a href="http://stackoverflow.com/questions/1400505/postgresql-random-number-range-1-10/1400752#comment15929846_1400752">this question</a> based on <a href="http://www.postgresql.org/docs/9.1/static/datatype-numeric.html#DATATYPE-FLOAT">this documentation</a> and the postgresql <a href="http://www.postgresql.org/docs/8.2/static/functions-math.html#FUNCTIONS-MATH-FUNC-TABLE">random function</a></p>\n', 'Tags': '<numerical-analysis><floating-point><rounding>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-08-14T23:02:56.757', 'CommentCount': '3', 'AcceptedAnswerId': '3186', 'CreationDate': '2012-08-14T18:45:13.943', 'Id': '3185'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>Consider a fixed point representation which can be regarded as a degenerate case of a floating number. It is entirely possible to use 2's complement for negative numbers. But why is a sign bit necessary for floating point numbers, shouldn't mantissa bits be using 2's complements?</p>\n\n<p>Also why do the exponent bits use a bias instead of a signed-magnitude representation (similar to the mantissa bits) or 2's complement representation?</p>\n\n<p>Update: Sorry if I didn't make it clear. I was looking for the reason of how floating point representation is shaped. If there is no strong implementation trade-off between the alternatives, then could someone explain the historical aspects of the floating point representation?</p>\n", 'ViewCount': '730', 'Title': "Why floating point representation uses a sign bit instead of 2's complement to indicate negative numbers", 'LastEditorUserId': '4183', 'LastActivityDate': '2012-10-16T11:47:32.687', 'LastEditDate': '2012-10-16T10:23:11.770', 'AnswerCount': '3', 'CommentCount': '0', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '4183', 'Tags': '<computer-architecture><floating-point><number-formats>', 'CreationDate': '2012-10-13T22:11:40.590', 'FavoriteCount': '1', 'Id': '6048'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p><a href="http://en.wikipedia.org/wiki/Fibonacci_number" rel="nofollow">Binet\'s formula</a> for the nth Fibonacci numbers is remarkable because the equation "converts" via a few arithmetic operations an irrational number $\\phi$ into an integer sequence. However, using finite precision arithmetic, one would always have some (small) roundoff error. </p>\n\n<blockquote>\n  <p>Is there a discussion/description somewhere of how to calculate the Fibonacci sequence using Binet\'s formula (ie <em>not</em> the recurrence relation) and floating point arithmetic which results in no roundoff error?</p>\n</blockquote>\n', 'ViewCount': '277', 'Title': "Calculating Binet's formula for Fibonacci numbers with arbitrary precision", 'LastEditorUserId': '472', 'LastActivityDate': '2012-12-04T09:51:48.217', 'LastEditDate': '2012-12-04T04:02:54.733', 'AnswerCount': '1', 'CommentCount': '3', 'AcceptedAnswerId': '7150', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '699', 'Tags': '<algorithms><discrete-mathematics><floating-point><arithmetic><mathematical-programming>', 'CreationDate': '2012-12-04T03:33:25.020', 'Id': '7145'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>In a 32-bit floating number with normalized mantissa and excess-64 exponent base 16, the number $16^{-65}$ denotes</p>\n\n<ol>\n<li><p>Floating point overflow.</p></li>\n<li><p>Negative floating point overflow.</p></li>\n<li><p>All 0\'s in the exponent and mantissa fields.</p></li>\n<li><p>The minimum representable positive number .</p></li>\n</ol>\n\n<p>I think that minimum representable number should be $1 \\times 16^{-63}$\nbecause the minimum mantissa should be 1 and and the possible exponent range in bias form is from 1 to 127 (where 1 corresponds to most negative exponent i.e. -63, and 127 corresponds to most positive exponent i.e. 63)</p>\n\n<p>So according to me, the answer is: A positive floating point underflow.\nPlease correct me if i am wrong. The IEEE-754 representation is confusing me. </p>\n\n<p>Someone also told me something along the lines of " the mantissa part is always taken as 0.M if the base is something other than 2". However I don\'t have any reference for this statement.</p>\n', 'ViewCount': '272', 'Title': 'In a 32-bit floating number with normalized mantissa and excess-64 exponent base 16, the number $16^{-65}$ denotes', 'LastEditorUserId': '4763', 'LastActivityDate': '2013-01-13T00:37:46.217', 'LastEditDate': '2012-12-13T18:03:21.537', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '4763', 'Tags': '<numerical-analysis><floating-point><rounding>', 'CreationDate': '2012-12-13T16:19:41.657', 'Id': '7382'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '2499', 'Title': 'Normalizing the mantissa in floating point representation', 'LastEditDate': '2013-01-09T11:44:45.023', 'AnswerCount': '1', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '4422', 'FavoriteCount': '0', 'Body': "<p>How to represent $0.148 * 2^{14}$ in normalized floating point arithmetic with the format</p>\n\n<pre><code>1 - Sign bit\n7 - Exponent in Excess-64 form\n8 - Mantissa\n</code></pre>\n\n<p>$(0.148)_{10} = (0.00100101\\;111...)_2$</p>\n\n<p>We shift it 3 bits to left to make it normalized $(1.00101\\;111)_2 * 2^{11}$. </p>\n\n<p>Exponent = $11+64 = (75)_{10} = (1001011)_2$ and Mantissa = $(01001\\;111)_2$.</p>\n\n<p>So floating point representation is $(0\\;1001011\\;00101111)_2 = (4B2F)_{16}$ <strong>Representation A</strong></p>\n\n<p>But if we store the denormalized mantissa into 8 bit register, then it won't have stored the last three $1$s and then the mantissa would have normalized from $(0.00100101)_2$ to $(1.00101\\;000)_2$ by inserting 3 $0$s instead of $1$s.</p>\n\n<p>The representation would have been $(0\\;1001011\\;00101000)_2 = (4B28)_{16}$ <strong>Representation B</strong></p>\n\n<p>So while normalizing, does the processor takes into account the denormalized mantissa bits beyond 8 bits too? Or just rounds it off? Which one is correct: <strong>A</strong> or <strong>B</strong>?</p>\n\n<p>Does it store the mantissa in fixed point representation? How does it all work?</p>\n", 'Tags': '<binary-arithmetic><floating-point><rounding>', 'LastEditorUserId': '4422', 'LastActivityDate': '2013-01-09T11:44:45.023', 'CommentCount': '0', 'AcceptedAnswerId': '7847', 'CreationDate': '2013-01-08T06:59:08.923', 'Id': '7828'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>I have been posed with a question whereby a computer truncates numbers to x number of digits. Due to this, if this computer is trying to store a decimal number which has a binary equivalent greater than x, it truncates the remaining digits producing a different binary number. However, this binary number is still an 'approximation' of what it should be, but of course is the equivalent of a different decimal number (close to what we were trying to store initially). </p>\n\n<p>What problems can occur due to this incorrect storage of data?</p>\n", 'ViewCount': '111', 'Title': 'Implications of truncation of numbers when converted into binary', 'LastEditorUserId': '39', 'LastActivityDate': '2013-01-28T00:18:41.680', 'LastEditDate': '2013-01-28T00:18:41.680', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '6571', 'Tags': '<approximation><numerical-analysis><floating-point>', 'CreationDate': '2013-01-26T19:54:34.997', 'Id': '9177'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '239', 'Title': 'What is the reason of inaccuracy of operations on float numbers?', 'LastEditDate': '2013-08-19T12:45:07.870', 'AnswerCount': '2', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '9735', 'FavoriteCount': '1', 'Body': '<p>I wonder why in JavaScript</p>\n\n<pre><code>0.1 + 0.2  // return 0.30000000000000004\n\n4%0.1 // return 0.09999999999999978\n</code></pre>\n\n<p><a href="http://jsbin.com/oHISAfU/1/edit" rel="nofollow">http://jsbin.com/oHISAfU/1/edit</a>  (Example)</p>\n\n<p>In C the math.h library fmod function</p>\n\n<pre><code>printf("%f", fmod(4.0,0.1));  // print 0.100000 \n</code></pre>\n\n<p><a href="http://ideone.com/RG5Wyv" rel="nofollow">http://ideone.com/RG5Wyv</a> (Example)</p>\n\n<p>And in Spotlight (search feature in the Mac OS X ~ I already submit bug report) that support math operation</p>\n\n<pre><code>4%0.1 = 0.1\n</code></pre>\n\n<p><img src="http://i.stack.imgur.com/GDb7g.png" alt="enter image description here"></p>\n', 'Tags': '<floating-point>', 'LastEditorUserId': '9735', 'LastActivityDate': '2013-08-19T15:27:15.433', 'CommentCount': '3', 'AcceptedAnswerId': '13811', 'CreationDate': '2013-08-19T09:06:04.017', 'Id': '13810'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'ViewCount': '216', 'Title': 'Confused by Floating Point Spacing', 'LastEditDate': '2013-09-17T12:06:36.347', 'AnswerCount': '1', 'Score': '5', 'OwnerDisplayName': 'Mike N.', 'PostTypeId': '1', 'OwnerUserId': '10176', 'Body': "<p>I'm currently taking a numerical analysis class in college and we're covering floating point systems. For the most part, I have a good grasp on it. However, something I can't seem to visualize, and haven't seen any totally lucid explanations about after searching extensively, is spacing between floating point numbers. Also of note is that I'm talking about IEEE-754 here, but it applies to general systems too.</p>\n\n<h3>The Things I Do Understand:</h3>\n\n<ul>\n<li>The area between $[-1,1]$ is a denormalized area.</li>\n<li>The areas after $1$ and less than $-1$ are where the normalized floating point numbers reside.</li>\n<li>The floating point numbers <em>between</em> perfect powers of the base are uniformly spaced, but the spacing varies from one perfect power of the base to another.</li>\n<li>The spacing between values between two perfect powers is proportional to the power on the left for positive numbers and the power on the right for negative numbers. (i.e. on a number line, the uniformly-spaced values between two low powers are closer together than between a higher power.)</li>\n</ul>\n\n<h3>What I'm Struggling to Understand</h3>\n\n<ul>\n<li><p>From my understanding, the machine epsilon $\\epsilon$ is a fundamental unit of spacing with respect to the floating point number line. That is, between $[1,B]$ where $B$ is the base, all values are $\\epsilon$ apart. Then, you can scale any arbitrary floating point number by that fundamental machine epsilon and that product is the uniform spacing for that floating point number's associated power range. Is this even a correct interpretation? </p>\n\n<p>I also read that $\\epsilon$ is an upper bound for relative error, so I'm not really sure how that fits into my explanation of it being an indivisible spacing unit.</p></li>\n<li><p>One of the questions I haven't been able to answer is what the minimum and maximum spacing between two positive floating point numbers is. I can trick myself into thinking I understand why multiplying the x's associated $B^e \\cdot \\epsilon$, where x is an arbitrary floating point number and $e$ is that number's corresponding exponent, yields the upper bound on error and therefore spacing, so $B^e \\cdot \\epsilon$ would be the maximum spacing. </p>\n\n<p>Minimum spacing truly boggles my mind right now, though. If the machine epsilon is the indivisible unit of spacing, then for example, how could we have more minimal spacing than between $1$ and $1 + \\epsilon$? Wouldn't that just be left to the rounding rule used (if round-to-nearest, it would depend whether the number is closer to $1$ or $1 + \\epsilon$, since it'll be rounded to one of those two).</p></li>\n</ul>\n\n<p>Basically, if you could explain this in plain-english it would really help me get a solid understanding of what's going on at the number line level.</p>\n", 'Tags': '<computer-architecture><floating-point><number-formats>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-09-17T12:06:36.347', 'CommentCount': '0', 'AcceptedAnswerId': '14370', 'CreationDate': '2013-09-16T23:24:15.270', 'Id': '14369'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>From Tanebaum's <em>Structed Computer Organization</em>.</p>\n\n<blockquote>\n  <h2>Exercise 4 of Appendix B</h2>\n  \n  <p>The following binary floating-point number consist of a sign bit, an excess $64$, radix $2$ exponent, and a $16$-bit fraction. Normalize $0$ $1000000$\n  $0001010100000001$ ($\\star$).</p>\n</blockquote>\n\n<p>As far as I've studied, the number ($\\star$) is already normalized and it is a representation of the number $1.0001010100000001 \\times 2^0$. <br/>\nMoreover, according to the IEEE754, if one is meant to represent an <strong>un</strong>normalized number, she would have to set the exponent-related bits at $0$ --which is not the case.</p>\n\n<p><em>My question is</em>: what is the exercise asking to me? Can that be an unormalized number?</p>\n\n<p>Maybe Tanebaum's simply asking the reader to multiply ($\\star$) by $2^3$ then to subtract $3$ from the exponent. Yet to me, that has no sense at all --instead, you're really changing the value ...</p>\n", 'ViewCount': '64', 'Title': 'Can a unnormalized floating point number be recognized also when exponent is not zero?', 'LastActivityDate': '2013-10-13T13:34:49.863', 'AnswerCount': '0', 'CommentCount': '3', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '10482', 'Tags': '<floating-point>', 'CreationDate': '2013-10-13T13:34:49.863', 'Id': '16036'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Is the convention of dropping the leading 1 when storing the significand a given in all binary floating point representations or not necessarily?</p>\n', 'ViewCount': '61', 'Title': 'Implicit Leading 1 in Binary Floating Point', 'LastActivityDate': '2013-10-30T04:10:36.867', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1025', 'Tags': '<floating-point>', 'CreationDate': '2013-10-30T01:51:14.173', 'Id': '16560'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>As a matter of curiosity I've been trying to determine at what point a 64-bit float no longer reflects the addition of 1 as expected; that is, at what point the digits as printed do not correspond to the digits of a 64-bit integer that is incremented in sync.</p>\n\n<p>Around 1e15 I can continually add 1 or subtract 1 and the result seems right. At 1e16 the addition or subtraction of 1 has no effect on the value as printed. However, 1e16 - 2 gives 9.999999999999998e+15 (at least, as printed by go's fmt package).</p>\n\n<p>I wonder whether there are discontinuities in the sequence of sums of n+=1 while n &lt; 1e15. At what point does n+=1 no longer produce a result corresponding to the expected integer?</p>\n", 'ViewCount': '40', 'Title': 'When does the IEEE-754 64-bit float break as a counter', 'LastActivityDate': '2014-02-03T13:40:42.783', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '20228', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '13333', 'Tags': '<floating-point><counting>', 'CreationDate': '2014-02-02T18:12:17.353', 'Id': '20224'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u"<p>So, i was trying:</p>\n\n<p>$(-10.75)_{10}$ and to convert it into 32 bit binary floating point representation.</p>\n\n<p>i did this:<br>\nAccording to IEEE standard:  $(-1)^{-s} * 1.M * 2^{E-bias} $</p>\n\n<pre><code> sign bit= 1 bit\n exponent= 8 bits\n mantissa= 23 bits\n</code></pre>\n\n<p>bias= $2^{n-1}-1 = 127$</p>\n\n<pre><code>  - 10   . 75\n  \u21d3  \u21d3      \u21d3\n= 1 1010 . 11\n= 1 1.01011 x 2^-3\n= 1 1.01011 x 2^(124-127)\n= 1 01111100 0101100 0000 0000 0000 0000   = 32 bits\n  \u21d3 ________ ____________________________\n  \u21d3    \u21d3                  \u21d3\n sign  Exponent         Mantissa\n</code></pre>\n\n<p>But the answer presented is:</p>\n\n<pre><code>  - 10   . 75\n  \u21d3  \u21d3      \u21d3\n= 1 1010 . 11\n= 1 1.101011 x 2^-4\n      -------&gt; why this happened, and why is 1 before '.'   \n= 1 1.101011 x 2^(123-127)\n= 1 01111011 1010110 0000 0000 0000 0000   = 32 bits\n  \u21d3 ________ ____________________________\n  \u21d3    \u21d3                  \u21d3\n sign  Exponent         Mantissa\n</code></pre>\n\n<p>If i am wrong, where is it and please explain why..\nAny help is appreciated.</p>\n", 'ViewCount': '66', 'Title': 'Problem in finding the floating point representation?', 'LastEditorUserId': '10564', 'LastActivityDate': '2014-03-15T16:02:51.047', 'LastEditDate': '2014-02-06T05:54:45.750', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '10564', 'Tags': '<computer-architecture><floating-point>', 'CreationDate': '2014-02-05T07:44:11.500', 'Id': '21312'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': u'<p>From the <a href="http://en.wikipedia.org/wiki/IEEE_floating_point#Formats" rel="nofollow">Wikipedia page</a> on the IEEE Standard for Floating-Point Arithmetic,</p>\n\n<blockquote>\n  <p>The possible finite values that can be represented in a format are determined by the base (b), the number of digits in the significand (precision, p), and the exponent (<code>q</code>) parameter <code>emax</code>:</p>\n  \n  <p>...</p>\n  \n  <p>q must be an integer such that <code>1\u2212emax \u2264 q+p\u22121 \u2264 emax</code> (e.g., if p=7 and emax=96 then q is \u2212101 through 90).</p>\n</blockquote>\n\n<p>I can\'t figure out the reasoning behind the above inequality. I would\'ve thought (in my simplicity) that it would be <code>-emax \u2264 q \u2264 emax</code> or something similar. What am I missing?</p>\n', 'ViewCount': '18', 'Title': u'Floating point format: why must `1\u2212emax \u2264 q+p\u22121 \u2264 emax`?', 'LastActivityDate': '2014-02-22T22:03:28.200', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '14945', 'Tags': '<floating-point><number-formats>', 'CreationDate': '2014-02-22T21:34:16.677', 'Id': '21930'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': "<p>It seems like it would be possible to add more precision to the IEEE 32-bit mantissa system if the leading zeroes were also dropped, just like the leading 1 is dropped due to it being implicitly known.</p>\n\n<p>For example, the number 17 would be represented as 0|10000100|00010000000000000000000. The leading 1 of the mantissa is always dropped, since every number in scientific notation starts with a 1. My question is why can't the leading zeroes also be dropped? If we know based on the exponent that the decimal place gets moved 4 spots, shouldn't we also be able to deduce, just like the implied 1, that all other missing bits afterwards would be 0? Granted, in this example it wouldn't make a difference, but for larger numbers, or numbers with a lot of digits past the decimal point, it seems like you would be able to get more precision the more implied bits you drop.</p>\n\n<p>(As I'm typing this, I'm also realizing that you might even be able to drop the next 1 in the mantissa. If the computer knows the number of places to move the exponent, you could have as many zeroes as you want sandwiched between two implied 1's)</p>\n\n<p>Does anyone know if this was ever addressed (or if I completely messed up in my calculations, and it isn't really possible to drop that many bits)?</p>\n", 'ViewCount': '30', 'Title': 'Can we improve the precision of IEEE floats by dropping leading zeros in the mantissa?', 'LastEditorUserId': '98', 'LastActivityDate': '2014-04-02T21:47:54.493', 'LastEditDate': '2014-04-02T21:47:54.493', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '23371', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '16394', 'Tags': '<floating-point><number-formats>', 'CreationDate': '2014-04-02T21:31:11.497', 'Id': '23370'}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,'Body': '<p>Most Smalltalk dialects currently implement a naive inexact floating modulus (fmod/remainder).<br>\nI just changed this to improve Squeak/Pharo and eventually other Smalltalk adherence to standards (IEEE 754, ISO/IEC 10967), as I already did for other state of the art floating point operations.</p>\n\n<p>However for adoption of those changes, I anticipate that adhering to standard will not be enough to convince my peers, so explaining in which circumstances this exactness would really matter would help me a lot. I could not find a good example by myself so far.</p>\n\n<p>Does any one here knows why/when/where (IOW in which algorithm) such exactness of modulus would matter?</p>\n', 'ViewCount': '14', 'Title': 'Why does floating point modulus exactness matters?', 'LastActivityDate': '2014-05-03T19:17:55.147', 'AnswerCount': '0', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '17303', 'Tags': '<algorithms><floating-point>', 'CreationDate': '2014-05-03T19:17:55.147', 'Id': '24362'}