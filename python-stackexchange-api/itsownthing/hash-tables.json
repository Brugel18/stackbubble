{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>It is often said that hash table lookup operates in constant time: you compute the hash value, which gives you an index for an array lookup. Yet this ignores collisions; in the worst case, every item happens to land in the same bucket and the lookup time becomes linear ($\\Theta(n)$).</p>\n\n<p>Are there conditions on the data that can make hash table lookup truly $O(1)$? Is that only on average, or can a hash table have $O(1)$ worst case lookup?</p>\n\n<p><em>Note: I\'m coming from a programmer\'s perspective here; when I store data in a hash table, it\'s almost always strings or some composite data structures, and the data changes during the lifetime of the hash table. So while I appreciate answers about perfect hashes, they\'re cute but anecdotal and not practical from my point of view.</em></p>\n\n<p>P.S. Follow-up: <a href="http://cs.stackexchange.com/questions/477/for-what-kind-of-data-are-hash-table-operations-o1">For what kind of data are hash table operations O(1)?</a></p>\n', 'ViewCount': '7425', 'Title': '(When) is hash table lookup O(1)?', 'LastEditorUserId': '39', 'LastActivityDate': '2013-05-05T22:31:57.543', 'LastEditDate': '2012-03-17T21:56:32.630', 'AnswerCount': '4', 'CommentCount': '4', 'Score': '32', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<time-complexity><data-structures><hash-tables>', 'CreationDate': '2012-03-12T19:01:07.577', 'FavoriteCount': '12', 'Id': '249'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>When implementing a dictionary ('I want to look up customer data by their customer IDs'), the typical data structures used are hash tables and binary search trees. I know for instance that the C++ STL library implements dictionaries (they call them maps) using (balanced) binary search trees, and the .NET framework uses hash tables under the hood.</p>\n\n<blockquote>\n  <p>What are the advantages and disadvantages of these data structures? Is there some other option that is reasonable in certain situations?</p>\n</blockquote>\n\n<p>Note that I'm not particularly interested in cases where the keys have a strong underlying structure, say, they are all integers between 1 and n or something.</p>\n", 'ViewCount': '4628', 'Title': 'Hash tables versus binary trees', 'LastActivityDate': '2012-03-13T01:43:44.703', 'AnswerCount': '1', 'CommentCount': '4', 'AcceptedAnswerId': '278', 'Score': '12', 'PostTypeId': '1', 'OwnerUserId': '92', 'Tags': '<algorithms><data-structures><binary-trees><hash-tables>', 'CreationDate': '2012-03-13T00:30:42.750', 'FavoriteCount': '2', 'Id': '270'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From the answers to <a href="http://cs.stackexchange.com/questions/249/when-is-hash-table-lookup-o1">(When) is hash table lookup O(1)?</a>, I gather that hash tables have $O(1)$ worst-case behavior, at least amortized, when the data satisfies certain statistical conditions, and there are techniques to help make these conditions broad.</p>\n\n<p>However, from a programmer\'s perspective, I don\'t know in advance what my data will be: it often comes from some external source. And I rarely have all the data at once: often insertions and deletions happen at a rate that\'s not far below the rate of lookups, so preprocessing the data to fine-tune the hash function is out.</p>\n\n<p>So, taking a step out: given some knowledge about data source, how can I determine whether a hash table has a chance of having $O(1)$ operations, and possibly which techniques to use on my hash function?</p>\n', 'ViewCount': '658', 'Title': 'For what kind of data are hash table operations O(1)?', 'LastActivityDate': '2013-05-06T13:07:46.187', 'AnswerCount': '4', 'CommentCount': '10', 'AcceptedAnswerId': '1351', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '39', 'Tags': '<data-structures><time-complexity><hash-tables>', 'CreationDate': '2012-03-17T21:55:43.600', 'FavoriteCount': '2', 'Id': '477'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am struggling with hashing and binary search tree material.\nAnd I read that instead of using lists for storing entries with the same hash values, it is also possible to use binary search trees. And I try to understand what the worst-case and average-case running time for the operations</p>\n\n<ol>\n<li><code>insert</code>, </li>\n<li><code>find</code> and</li>\n<li><code>delete</code></li>\n</ol>\n\n<p>is in worth- resp. average case. Do they improve with respect to lists?</p>\n', 'ViewCount': '1265', 'Title': 'Hashing using search trees instead of lists', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-10T15:37:46.957', 'LastEditDate': '2012-05-10T15:37:46.957', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<data-structures><time-complexity><runtime-analysis><search-trees><hash-tables>', 'CreationDate': '2012-05-08T21:46:33.920', 'Id': '1739'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I am still fighting with hashing and I am ask myself: what is the most efficient way to count the number of different words in a text using a hash table?</p>\n\n<p>My intuition says that applying the hashcode function to every word in the text, as result we will have words with different hash values in different buckets and the same words will have the same bucket and therefore we will have a collision problem which we can resolve using the chaining method.</p>\n\n<p>Does it work like that?</p>\n', 'ViewCount': '773', 'Title': 'Counting different words in text using hashing', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-15T20:29:24.713', 'LastEditDate': '2012-05-14T16:54:51.270', 'AnswerCount': '3', 'CommentCount': '3', 'Score': '5', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><strings><hash-tables>', 'CreationDate': '2012-05-14T16:41:35.327', 'Id': '1838'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u'<p>Who can help me with this topic: <a href="https://en.wikipedia.org/wiki/Open_addressing" rel="nofollow">Probing</a> with a step width that is a prime number.</p>\n\n<p>I am struggling with this question about defining a hashing function $h(k, i)$ for open addressing on a table of length m, that is, with slots numbers $0, 1, 2, \\dots ,m \u2212 1$.</p>\n\n<p>We know that a function $h(k, i) = h_1(k) + i \\cdot h_2(k) \\mod m$ produces a permutation for every $k$ if $h_2(k)$ and $m$ are relatively prime, that is, if $\\operatorname{gcd}(h_2(k),m) = 1$. </p>\n\n<p>We can assume that $m, w$ be integers such that the greatest common divisor $\\operatorname{gcd}(m,w) = 1$. </p>\n\n<p>How can I prove that the function above</p>\n\n<p>$\\qquad f : \\{ 0, \\dots,m \u2212 1 \\} \\to \\{ 0, \\dots,m \u2212 1 \\}\\\\\n \\qquad f(i) = i \\cdot w \\mod m$</p>\n\n<p>is a permutation, in other words, a <a href="https://en.wikipedia.org/wiki/Bijection" rel="nofollow">bijective function</a>? </p>\n', 'ViewCount': '116', 'Title': 'Is open adressing with prime steps bijective?', 'LastEditorUserId': '98', 'LastActivityDate': '2012-05-23T14:00:48.277', 'LastEditDate': '2012-05-22T08:32:55.930', 'AnswerCount': '1', 'CommentCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '1011', 'Tags': '<algorithms><hash-tables><hash>', 'CreationDate': '2012-05-22T08:09:20.643', 'Id': '1988'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>The lookup time in perfect hash-tables is $O(1)$ in the worst case. Does that simply mean that the average should be $\\leq O(1)$?</p>\n', 'ViewCount': '163', 'Title': 'What is the average search complexity of perfect hashing?', 'LastEditorUserId': '41', 'LastActivityDate': '2012-07-17T06:12:56.213', 'LastEditDate': '2012-07-17T06:12:56.213', 'AnswerCount': '2', 'CommentCount': '1', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '2067', 'Tags': '<complexity-theory><time-complexity><asymptotics><hash-tables>', 'CreationDate': '2012-07-04T14:36:16.523', 'Id': '2610'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Say instead of using a linked list as buckets for a hash table of size $m$, we use another hash table of size $p$ as buckets this time. What would be the average case for this problem?</p>\n\n<p>I looked up perfect hashing and I got a very close algorithm, and it is $O(1)$. Can someone please clarify?</p>\n', 'ViewCount': '239', 'Title': 'Using hash tables instead of lists for buckets in hash tables', 'LastEditorUserId': '98', 'LastActivityDate': '2012-07-05T07:18:20.110', 'LastEditDate': '2012-07-05T07:18:20.110', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '2067', 'Tags': '<data-structures><time-complexity><hash-tables>', 'CreationDate': '2012-07-04T14:54:28.810', 'Id': '2613'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '256', 'Title': 'Origins of the term "distributed hash table"', 'LastEditDate': '2012-07-23T09:49:38.700', 'AnswerCount': '1', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '1837', 'FavoriteCount': '1', 'Body': "<p>I am currently researching for my diploma thesis in computer science with a topic in the area of distributed hash tables. Naturally, I came to the question were the term <em>distributed hash table</em> came from. (I know it is not rocket science to just derive it from <em>distributing a hash table</em>, but somebody somewhere must have come up with it).</p>\n\n<p>Most papers I read referred to the original paper on <em>consistent hashing</em> and one of the first algorithms making use of it (e.g Chord). I know that there was a lot of research on distributed databases in the 80s, so I figure that the term, or maybe the idea behind it, should be older than ~15 years.</p>\n\n<p>The motivation behind this question is that knowing an earlier date and maybe another term for a similar idea would possibly widen the range of useful information I could gather for my research. For example, what have others done that is similar to what I want to do and where have they failed. Etc. etc.</p>\n\n<p>I tried to find more on this subject using <em>Structured Overlay Networks</em> as a search keyword, but the resulting definitions/papers are also quite young, which leaves me with the impression that the research topic might not be so old after all.</p>\n\n<p>Does anybody of you know of earlier research (maybe pre-90s?) in the topics of distributed hash tables and/or structured overlay networks? I'd be glad to hear some keywords which could lead me to more historic papers.</p>\n", 'Tags': '<data-structures><terminology><distributed-systems><hash-tables><history>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-01-21T07:15:27.943', 'CommentCount': '6', 'AcceptedAnswerId': '8961', 'CreationDate': '2012-07-23T09:43:04.950', 'Id': '2872'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '691', 'Title': "How Does Populating Pastry's Routing Table Work?", 'LastEditDate': '2012-08-13T09:14:36.520', 'AnswerCount': '1', 'Score': '15', 'OwnerDisplayName': 'Paddy Foran', 'PostTypeId': '1', 'OwnerUserId': '2653', 'FavoriteCount': '2', 'Body': u'<p>I\'m trying to implement the Pastry Distributed Hash Table, but some things are escaping my understanding. I was hoping someone could clarify.</p>\n\n<p><strong>Disclaimer</strong>: I\'m not a computer science student. I\'ve taken precisely two computer science courses in my life, and neither dealt with anything remotely complex. I\'ve worked with software for years, so I feel I\'m up to the implementation task, if I could just wrap my head around the ideas. So I may just be missing something obvious.</p>\n\n<p>I\'ve read the paper that the authors published [1], and I\'ve made some good progress, but I keep getting hung up on this one particular point in how the routing table works:</p>\n\n<p>The paper claims that</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into $\\lceil \\log_{2^b} N\\rceil$ \n  rows with $2^b - 1$ entries each. The $2^b - 1$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>The $b$ stands for an application-specific variable, usually $4$. Let\'s use $b=4$, for simplicity\'s sake. So the above is</p>\n\n<blockquote>\n  <p>A node\u2019s routing table, $R$, is organized into  $\\lceil \\log_{16} N\\rceil$ rows \n  with $15$ entries each. The $15$ entries at\n  row $n$ of the routing table each refer to a node whose nodeId shares\n  the present node\u2019s nodeId in the \ufb01rst n digits, but whose $n + 1$th\n  digit has one of the $2^b - 1$ possible values other than the $n + 1$th\n  digit in the present node\u2019s id.</p>\n</blockquote>\n\n<p>I understand that much. Further, $N$ is the number of servers in the cluster. I get that, too.</p>\n\n<p>My question is, if the row an entry is placed into depends on the shared length of the key, why the seemingly random limit on the number of rows? Each nodeId has 32 digits, when $b=4$ (128 bit nodeIds divided into digits of b bits). So what happens when $N$ gets high enough that $\\lceil\\log_{16} N\\rceil &gt; 32$? I realise it would take 340,282,366,920,938,463,463,374,607,431,768,211,457 (if my math is right) servers to hit this scenario, but it just seems like an odd inclusion, and the correlation is never explained.</p>\n\n<p>Furthermore, what happens if you have a small number of servers? If I have fewer than 16 servers, I only have one row in the table. Further, under no circumstances would every entry in the row have a corresponding server. Should entries be left empty? I realise that I\'d be able to find the server in the leaf set no matter what, given that few servers, but the same quandary is raised for the second row--what if I don\'t have a server that has a nodeId such that I can fill every possible permutation of the nth digit? Finally, if I have, say, four servers, and I have two nodes that share, say, 20 of their 32 digits, by some random fluke... should I populate 20 rows of the table for that node, even though that is far more rows than I could even come close to filling?</p>\n\n<p>Here\'s what I\'ve come up with, trying to reason my way through this:</p>\n\n<ol>\n<li>Entries are to be set to a null value if there is not a node that matches that prefix precisely.</li>\n<li>Empty rows are to be added until enough rows exist to match the shared length of the nodeIds.</li>\n<li>If, and only if, there is no matching entry for a desired message ID, fall back on a search of the routing table for a nodeId whose shared length is greater than or equal to the current nodeId\'s and whose entry is mathematically closer than the current nodeId\'s to the desired ID.</li>\n<li>If no suitable node can be found in #3, assume this is the destination and deliver the message.</li>\n</ol>\n\n<p>Do all four of these assumptions hold up? Is there somewhere else I should be looking for information on this?</p>\n\n<hr>\n\n<ol>\n<li><a href="http://dx.doi.org/10.1007/3-540-45518-3_18" rel="nofollow">Pastry: Scalable, decentralized object location and routing for large-scale peer-to-peer systems</a> by A. Rowstrong and P. Druschel (2001) -- <a href="http://research.microsoft.com/~antr/PAST/pastry.pdf" rel="nofollow">download here</a></li>\n</ol>\n', 'Tags': '<algorithms><data-structures><distributed-systems><hash-tables>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-10-14T17:34:24.577', 'CommentCount': '2', 'AcceptedAnswerId': '6069', 'CreationDate': '2012-05-23T22:02:33.000', 'Id': '3138'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>Consider the two hash functions used to map IP addresses. $x_i$ represents a octave (or "bit field") of the address.</p>\n\n<p>Hash Function 1:\n$$h_a(x_1, x_2, x_3, x_4) = \\sum^{4}_{i=1} a_ix_i \\bmod n$$</p>\n\n<p>Where $n$ is a prime number closest to the total number of IP addresses being mapped. The probability of two IP addresses being mapped to the same bucket is $\\frac{1}{n}$ using this hash function.</p>\n\n<p>Hash Function 2:</p>\n\n<p>$$h_a(x_1, x_2, x_3, x_4) = \\sum^{4}_{i=1} x_i \\bmod n$$</p>\n\n<p>What would the probability to two IP addresses being mapped to the same bucket be using the second hashing function?</p>\n', 'ViewCount': '124', 'Title': 'How do the following Hash Functions compare?', 'LastEditorUserId': '472', 'LastActivityDate': '2012-10-27T22:29:03.723', 'LastEditDate': '2012-10-27T22:29:03.723', 'AnswerCount': '1', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4374', 'Tags': '<hash-tables><hash>', 'CreationDate': '2012-10-27T03:38:30.673', 'Id': '6330'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '136', 'Title': "Why can't we use a hash tables for collision resolving in hash tables?", 'LastEditDate': '2012-10-28T10:58:21.593', 'AnswerCount': '2', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '4193', 'FavoriteCount': '1', 'Body': "<p>To prevent collisions, hash tables with open addressing use a methodology to chain the contents. Why can't we use another hash table allocated to each slot of the primary hash table?</p>\n", 'Tags': '<data-structures><hash-tables>', 'LastEditorUserId': '98', 'LastActivityDate': '2012-11-02T04:37:25.970', 'CommentCount': '1', 'AcceptedAnswerId': '6413', 'CreationDate': '2012-10-28T03:50:29.827', 'Id': '6348'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>This is a textbook based question. In The Art of Computer Programming Volume 3, Knuth says that for a hash function $h(k) = k \\bmod M$, $M$ should not be a multiple of $3$.</p>\n\n<p>The explanation given is:</p>\n\n<blockquote>\n  <p>If keys are alphabetic, two keys that differ only by permutation of letters would differ in numeric value by a multiple of 3. This occurs because $2^{2n} \\bmod 3 = 1$ and $10^{n} \\bmod 3 = 1$)."</p>\n</blockquote>\n\n<p>I\'d be grateful, if someone can clarify why this is so, and how that equation is connected to alphabetical keys and $M$ being a power of $3$.</p>\n', 'ViewCount': '159', 'Title': 'Modulo hash function and multiples of three', 'LastEditorUserId': '472', 'LastActivityDate': '2013-01-01T23:36:03.757', 'LastEditDate': '2012-11-02T22:14:57.713', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '2', 'OwnerDisplayName': 'Jagx Kool', 'PostTypeId': '1', 'Tags': '<data-structures><hash-tables><hash>', 'CreationDate': '2012-10-26T02:36:33.613', 'Id': '6433'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>For an open-addressing hash table, what is the average time complexity to find an item with a given key:</p>\n\n<ol>\n<li>if the hash table uses linear probing for collision resolution?</li>\n<li>if the hash table uses double probing for collision resolution?</li>\n</ol>\n\n<p>From my understanding my answer to the first question would be $O(n)$\nbut I'm not sure about the double probing question.</p>\n", 'ViewCount': '294', 'Title': 'Hash table collision probability', 'LastEditorUserId': '6447', 'LastActivityDate': '2013-03-07T14:55:19.393', 'LastEditDate': '2013-03-07T14:55:19.393', 'AnswerCount': '0', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '7149', 'Tags': '<time-complexity><hash-tables>', 'CreationDate': '2013-03-04T21:20:01.743', 'FavoriteCount': '1', 'Id': '10273'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '127', 'Title': 'Hash function - uniformity / strong universality', 'LastEditDate': '2013-03-11T07:38:23.137', 'AnswerCount': '1', 'Score': '2', 'OwnerDisplayName': 'Andreas T', 'PostTypeId': '1', 'OwnerUserId': '7226', 'Body': '<p>I am currently learning how randomised Hashing works. \nSo, you have a class (aka family) $H$ of hash functions, each of which maps the universe $U$ to the hash table $N$.</p>\n\n<p>That class is called "strongly universal" or "pairwise independent" if $\\forall x,y \\in U, x \\neq y: \\forall z_1,z_2 \\in N: \\Pr\\limits_{h \\in H}[h(x) = z_1 \\land h(y) = z_2] \\leq \\frac{1}{|N|^2}$. In words: pick any two elements from the universe and two from the hash table. If you pick a hash function from the hash class at random, the probability that these two elements are mapped to each other by $h$ is less or equal than $\\frac{1}{|N|^2}$.</p>\n\n<p>Now, what is confusing me is that, since $x$, $y$, $z_1$ and $z_2$ are all completely independent, it looks to me like you could just "remove" one pair from the equation and still get the same result. That would be $\\forall x \\in U: \\forall z \\in N: \\Pr\\limits_{h \\in H}[h(x) = z] \\leq \\frac{1}{|N|}$. This, however, is called "uniformity" of a hash class.</p>\n\n<p>Could someone explain to me why these two attributes are different from one anoter?</p>\n', 'Tags': '<data-structures><hash-tables><hash>', 'LastEditorUserId': '98', 'LastActivityDate': '2013-03-11T07:38:23.137', 'CommentCount': '2', 'AcceptedAnswerId': '10449', 'CreationDate': '2013-03-10T22:21:18.860', 'Id': '10447'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If I have a hash table of 1000 slots, and I have an array of n numbers. I want to check if there are any repeats in the array of n numbers. The best way to do this that I can think of is storing it in the hash table and use a hash function which assumes simple uniform hashing assumption. Then before every insert you just check all elements in the chain. This makes less collisions and makes the average length of a chain $\\alpha = \\frac{n}{m} = \\frac{n}{1000}$.</p>\n\n<p>I am trying to get the expected running time of this, but from what I understand, you are doing an insert operation up to $n$ times. The average running time of a search for a linked list is $\\Theta(1+\\alpha)$. Doesn't this make the expected running time $O(n+n\\alpha) = O(n+\\frac{n^2}{1000}) = O(n^2)$? This seems too much for an expected running time. Am I making a mistake here?</p>\n", 'ViewCount': '321', 'Title': 'How to get expected running time of hash table?', 'LastActivityDate': '2013-03-12T23:38:43.887', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '10500', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '7168', 'Tags': '<algorithms><algorithm-analysis><hash-tables><hash><probabilistic-algorithms>', 'CreationDate': '2013-03-12T23:20:41.050', 'Id': '10498'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '1991', 'Title': 'Why is it best to use a prime number as a mod in a hashing function?', 'LastEditDate': '2013-04-05T12:49:56.007', 'AnswerCount': '3', 'Score': '9', 'PostTypeId': '1', 'OwnerUserId': '4348', 'FavoriteCount': '2', 'Body': "<p>If I have a list of key values from 1 to 100 and I want to organize them in an array of 11 buckets, I've been taught to form a mod function</p>\n\n<p>$$ H = k \\bmod \\ 11$$</p>\n\n<p>Now all the values will be placed one after another in 9 rows. For example, in the first bucket there will be $0, 11, 22 \\dots$. In the second, there will be $1, 12, 23 \\dots$ etc.</p>\n\n<p>Let's say I decided to be a bad boy and use a non-prime as my hashing function - take 12.\nUsing the Hashing function</p>\n\n<p>$$ H = k \\bmod \\ 12$$</p>\n\n<p>would result in a hash table with values $0, 12, 24 \\dots $ in the first bucket, $1, 13, 25 \\dots$ etc. in the second and so on.</p>\n\n<p>Essentially they are the same thing. I didn't reduce collisions and I didn't spread things out any better by using the prime number hash code and I can't see how it is ever beneficial.</p>\n", 'Tags': '<data-structures><hash><hash-tables><primes>', 'LastEditorUserId': '3011', 'LastActivityDate': '2013-04-10T16:28:56.047', 'CommentCount': '0', 'AcceptedAnswerId': '11031', 'CreationDate': '2013-04-04T19:36:37.447', 'Id': '11029'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A lot of articles say that hash tree traversal cost to any <em>randomly</em> chosen leaf is $\\mathcal{O}(\\log_2 N)$ ($N$ is a number of leafs) and that is right. If we have a tree of 8 leafs it will take us at most 3 operations to get to any leaf, if we have a tree of 64 leafs it will take us at most 5 operations etc.</p>\n\n<p>But lets say I need to check <strong><em>every</em></strong> leaf sequentially to check if <strong><em>all blocks</em></strong> of a file are correct, then I would need $\\mathcal{O}(N \\log_2 N)$ operations. Or if I would check every second leaf (just left leaf of every pair) I would need $\\mathcal{O}((\\frac{N\\log_2 N}{2}))$ operations. That is, I will need $\\mathcal{O}(\\log_2 N)$ operations for every leaf? Which leads to exponentially growing evaluations curve and it would be better to use simple hash list or hash chain? Am I right?</p>\n\n<p>Or I just don\'t see/know something?</p>\n\n<p><img src="http://i.stack.imgur.com/Mnvvt.jpg" alt=""></p>\n\n<p>*Note, chart has logarithmic scale</p>\n', 'ViewCount': '102', 'Title': 'Sequential hash tree traversal', 'LastEditorUserId': '2205', 'LastActivityDate': '2013-06-14T18:11:32.220', 'LastEditDate': '2013-05-14T10:59:03.967', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8111', 'Tags': '<data-structures><search-trees><graph-traversal><hash-tables>', 'CreationDate': '2013-05-10T07:16:57.917', 'Id': '11927'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I've seen two definitions of a universal hash family, and my questions is if those are equivalent, i think they are and will explain why but i'm not sure if it is.</p>\n\n<p>Definition 1:</p>\n\n<p>$H$ is a universal hash family if and only if $$\\mathbb{Pr_{h \\in H}}[h(k) = h(l)] = 1/m$$</p>\n\n<p>where $k \\neq l$ and $k,l\\in Key$ and $m$ is the Size of the Hashtable</p>\n\n<p>Definition 2:</p>\n\n<p>The same as above just substitute the equals sign with a less or equal sign.</p>\n\n<p>Well, i think those definitions are equivalent, because i think the probability for a collision can't possibly be strictly less than 1/m or am i missing something?</p>\n\n<p>P.S.\nI'm assuming that the cardinality of Key is bigger than the size of the hashtable m</p>\n", 'ViewCount': '82', 'Title': 'Are those definitions of universal hash family equivalent?', 'LastEditorUserId': '8282', 'LastActivityDate': '2013-05-22T00:16:10.807', 'LastEditDate': '2013-05-21T17:02:54.330', 'AnswerCount': '2', 'CommentCount': '0', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '8282', 'Tags': '<algorithms><algorithm-analysis><hash><hash-tables>', 'CreationDate': '2013-05-21T16:00:59.023', 'Id': '12193'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>If I use a balanced tree instead of lists in a hash table implementation, and also after initializing my table I don't enlarge nor reduce the size of the table, what would be the worst case complexity?</p>\n", 'ViewCount': '59', 'Title': 'Complexity of a hash tables with balanced trees in the buckets', 'LastEditorUserId': '39', 'LastActivityDate': '2013-06-03T21:57:37.240', 'LastEditDate': '2013-06-03T21:55:17.027', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '12452', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '8510', 'Tags': '<time-complexity><hash><hash-tables>', 'CreationDate': '2013-06-03T20:47:49.693', 'Id': '12450'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>By default, a hash table is unordered.  I have a simple question on the retrieval of elements in a hash table</p>\n\n<blockquote>\n  <p>Can we retrieve elements from a hash table in the same order as they are put inside?</p>\n</blockquote>\n', 'ViewCount': '786', 'Title': 'Retrieving data from hash table ordered by insertion time', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-07-30T08:02:11.913', 'LastEditDate': '2013-07-30T08:02:11.913', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '12565', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '8596', 'Tags': '<algorithms><data-structures><hash><hash-tables>', 'CreationDate': '2013-06-09T10:50:57.380', 'Id': '12562'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have many vectors in my database. They are in high dimensions such as:</p>\n\n<ul>\n<li>$v_1$ : $\\langle 23, 23, 1, 33, 103, 219, \\dots \\rangle$</li>\n<li>$v_2$ : $\\langle 92, 83, 1, 33, 239, 192, \\dots \\rangle$</li>\n<li>...</li>\n</ul>\n\n<p>I will use Hamming distance to calculate their difference: The\ndifference between $v_1$ and $v_2$ is $4$ because elements $3$\nand $4$ are the same and others are difference.</p>\n\n<p>Now, I want to use Locality Sensitive Hashing (LSH) to put those\nvectors into different bins.</p>\n\n<blockquote>\n  <p>What kind of hash function can I use for this case?</p>\n</blockquote>\n\n<p>I have read some article about universal hash function, but I am\nnot sure can I use it and how to ensure that the probability for\nthe similar vectors going to the same bin is higher than those\nnon-similar one.</p>\n\n<p>Here is the way that I think how should I use the universal hash\nfunction for my task.</p>\n\n<p>I will first divide those high dimensions vectors into sub-vectors:\n$$x : 23, 23 \\; | \\; 1, 33 \\;  | \\; 103, 219 \\; | \\; \\dots$$</p>\n\n<p>sub1-x : 23 23<br>\nsub2-x : 1 33<br>\nsub3-x : 103 219<br></p>\n\n<p>The following function will be used for each sub-vector:\n$$sum_{i=0}^{r} a_{i}x_{i} \\mod m$$</p>\n\n<p>Basically this is a dot product, a = {a_1, a_2, ... a_i}, x =\n{23, 23, 1, 33, 103, 219 ...}, m is a prime.</p>\n\n<p><ul>\n<li>Different combination of {a} will form a different hash table, one hash table is used for one sub-vector.</li>\n<li>I can now hash the data into bins, <strong>but the question is</strong></p>\n\n<blockquote>\n  <p>Is this an LSH method?  I don't know that two similar vectors  will go into the same bin with a high probability.</li>\n  </ul></p>\n</blockquote>\n", 'ViewCount': '159', 'Title': 'Find similar vector by Locality Sensitive Hashing', 'LastEditorUserId': '4249', 'LastActivityDate': '2013-07-30T08:00:31.173', 'LastEditDate': '2013-07-30T08:00:31.173', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '13352', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9238', 'Tags': '<hash><databases><hash-tables><string-metrics>', 'CreationDate': '2013-07-18T14:41:29.760', 'Id': '13334'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>From TAoCP, Vol. 3 by Knuth we know that <em>expected (mean)</em> probe counts for hash tables with open adressing and double hashing collision-resolution are:</p>\n\n<ul>\n<li><p>$1 \\over 1 - \\alpha$ - for unsuccessful search</p></li>\n<li><p>${1 \\over \\alpha} ln({1 \\over 1 - \\alpha})$ - for succesful search</p></li>\n</ul>\n\n<p>Where $\\alpha$ is load factor.</p>\n\n<p>Are there expressions for <em>probabilities</em> of making exactly $i$ = 1,2,3,.. probes?</p>\n', 'ViewCount': '121', 'Title': 'Double hashing - probe count probabilities', 'LastEditorUserId': '9350', 'LastActivityDate': '2013-07-26T08:07:24.023', 'LastEditDate': '2013-07-26T08:07:24.023', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '13447', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '9350', 'Tags': '<hash><hash-tables>', 'CreationDate': '2013-07-25T21:06:45.597', 'Id': '13445'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>There are $n$ elements in a hash table of size $m \\geq 2n$ which uses open addressing to avoid collisions. </p>\n\n<p>The hash function was chosen randomly among a set of uniform functions. A set $H$ of hash-functions $h:U\\to\\{0,\\dots,m-1\\}$ is called uniform, if for every tuple of different keys $x,y \\in U$ the number of hash-functions $h \\in H$ with $h(x) = h(y)$ is $\\frac{|H|}{m}$ at most.</p>\n\n<p>Show that the propability that for $i = 1, 2, \\dots,n$ the $i$-th insert operation needs more than $k$ attempts, is $2^{-k}$ at most.</p>\n\n<p>This is an assignment, which I got as homework. What I already worked out:</p>\n\n<p>The propability $p_1$ for a collision is 0 of course for an empty table.</p>\n\n<p>The propability $p_i$ for a collision after k attempts should be $\\frac{i - 1}{2n}\\cdot k$ assuming that the table is filled with $i-1$ elements to this point and the tables size is $2n$ as worst case.</p>\n\n<p>So I have\n$$\np_i= \\frac{i-1}{2n} \\cdot k \\leq 2^{-k},\n$$</p>\n\n<p>but I don't know where to go from here.</p>\n\n<p>The method of open hashing used here simply iterates over different hash-functions until a free place is found (for example $h(x) = (x \\bmod j) \\bmod n$ with increasing prime numbers for $j$.</p>\n", 'ViewCount': '140', 'Title': 'Proof that probability that hashing with open addressing needs more than $k$ attempts is $2^{-k}$ at most', 'LastEditorUserId': '472', 'LastActivityDate': '2014-03-08T16:59:37.943', 'LastEditDate': '2014-01-07T16:06:08.057', 'AnswerCount': '1', 'CommentCount': '1', 'Score': '4', 'PostTypeId': '1', 'OwnerUserId': '6424', 'Tags': '<algorithm-analysis><data-structures><hash-tables>', 'CreationDate': '2013-10-06T22:13:05.387', 'FavoriteCount': '1', 'Id': '14862'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>A family $H$ of hash functions $h: U \\rightarrow \\{0,\\ldots,M-1\\}$ is <em>universal</em> if \n$$\\forall x,y \\in U, x \\neq y \\Rightarrow \\Pr_{h \\in H}[h(x) = h(y)] \\leq \\frac{1}{M}$$\nYou can find more about universal hashing this wikipedia <a href="http://en.wikipedia.org/wiki/Universal_hashing">article</a>.</p>\n\n<p>The concept of universal hashing is now a standard part of undergraduate data structure courses. It would be nice to be able to motivate students about the importance of universal hashing in industrial applications. So my question is:</p>\n\n<blockquote>\n  <p>Are constructions of universal family of hash functions important in practice? If the answer is yes, would you please share some interesting industrial applications that you\'ve seen?</p>\n</blockquote>\n', 'ViewCount': '134', 'Title': 'Universal Hashing in Practice', 'LastEditorUserId': '204', 'LastActivityDate': '2013-10-27T22:57:49.313', 'LastEditDate': '2013-10-16T03:34:01.153', 'AnswerCount': '1', 'CommentCount': '3', 'Score': '6', 'PostTypeId': '1', 'OwnerUserId': '204', 'Tags': '<data-structures><education><hash><hash-tables>', 'CreationDate': '2013-10-16T00:21:41.770', 'Id': '16118'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': "<p>I have a practice exam question that I don't know how to set up a recurrence for. It is dealing with a hash table. The question is as follows:</p>\n\n<blockquote>\n  <p>Suppose that a hashing strategy is designed so that it starts with an initial hash table size of $H= 8$. You may assume that only insertions are performed (no deletions).</p>\n  \n  <p>Any time the hash table is going to be more than 50% full (when an attempt is made to add item $\\frac{H}{2} + 1$ to a table of size $H$), the hash table size is doubled to $2\\times H$, and then the $\\frac{H}{2}$ keys in the previous hash table are rehashed using $\\frac{H}{2}$ extraneous key insertions into the new table of size $2 \\times H$. The key insertions used to initially place each key into the hash table are called necessary key insertions (these are not extraneous).</p>\n</blockquote>\n\n<p>The question is asking to derive a recurrence relation $E(H)$ for the number of extraneous key insertions that have occurred in total up until the point in time that the hash table size is $H$ and to explain where the terms in the recurrence relation derive from.</p>\n\n<p>If someone could help me out with this, it would provide very helpful as I am practicing for an exam that I have in a week. Thanks everyone.</p>\n\n<p>I got the result $E(H)=2\\times E(\\frac{H}{2})$ because after each rehash there are $\\frac{H}{2}$ extraneous key insertions being put into the table of size $2\\times H$. So if the size is twice the amount of $H$, I figured the recurrence would be $E(H)=2\\times E(\\frac{H}{2})$. I only posted here because I was hoping someone could assist me with this because this question has me a bit stumped. </p>\n", 'ViewCount': '81', 'Title': 'Recurrence for total number of extraneous key insertions in a hash table', 'LastEditorUserId': '98', 'LastActivityDate': '2014-03-26T08:41:25.867', 'LastEditDate': '2014-03-26T08:41:25.867', 'AnswerCount': '2', 'CommentCount': '7', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '11491', 'Tags': '<data-structures><runtime-analysis><recurrence-relation><hash-tables>', 'CreationDate': '2013-11-20T00:24:50.667', 'Id': '18178'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'ViewCount': '43', 'Title': 'Hashing a Specific Range Of a Character Array', 'LastEditDate': '2013-12-11T12:40:25.193', 'AnswerCount': '2', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '8505', 'Body': '<p>I need to process queries to Hash various ranges of a character array. I am currently using the <a href="http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#hashCode%28char%5B%5D%29" rel="nofollow">Arrays.hashCode</a> from the standard java library. But the problem is that this method is too slow. Also my array remains the same throughout the process of hashing, I only am changing the range. To deal with this, I have to make an entire copy of the array everytime I process a query, and then compute the hash from the above function. </p>\n\n<p>I am using <a href="http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#copyOfRange%28char%5B%5D,%20int,%20int%29" rel="nofollow">Arrays.copyOfRange</a> to create a copy everytime I process a query. I need to avoid this. So I was thinking of devising a hashing scheme of my own. This scheme should be such that I whould get a unique hash for each array range. Hashes should be same if all characters in the range are same. </p>\n\n<p>Any Help on how to proceed with the making of such a hash function will be appreciated.</p>\n', 'ClosedDate': '2013-12-14T20:05:36.503', 'Tags': '<data-structures><arrays><hash><hash-tables>', 'LastEditorUserId': '8505', 'LastActivityDate': '2013-12-11T20:57:09.670', 'CommentCount': '2', 'AcceptedAnswerId': '18896', 'CreationDate': '2013-12-11T10:59:54.037', 'Id': '18873'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I don\'t understand what is meant by:</p>\n\n<p>"m should not be a power of 2, since if m = 2^p, then h(k) is just the p lowest-order bits of k." (pg. 231 of CLRS)</p>\n\n<p>Terms defined:</p>\n\n<pre><code>m: size of hash table\nh(k): hash function = k mod m\nk: key\n</code></pre>\n\n<p>I don\'t understand what the p lowest-order bits of k means. Any clarification would be very helpful.</p>\n', 'ViewCount': '51', 'Title': 'Why should one not use a 2^p size hash table when using the division method as a hash function?', 'LastActivityDate': '2013-12-15T23:09:47.687', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '19021', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12142', 'Tags': '<hash-tables>', 'CreationDate': '2013-12-15T21:58:43.020', 'Id': '19020'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I\'ve created a data structure that is a hash of arrays with a special property: the hash keeps track of the combined order in which items are appended to its arrays. </p>\n\n<p>For example (pseudocode):</p>\n\n<pre><code>h = HashOfArray()\n\nh["a"].append(1) // accessing an unset key returns an empty array\nh["b"].append(2)\nh["a"].append(3)\nh["b"].append(4)\n\nh["a"]\n&gt;&gt;&gt; [1, 3]\n\nh["b"]\n&gt;&gt;&gt; [2, 4]\n\nh.items()\n&gt;&gt;&gt; [1, 2, 3, 4]\n</code></pre>\n\n<p>I\'m struggling with coming up with a good name for this data structure. Does it have a commonly used name? I\'ve called it <code>HashOfArray</code> but that fails to convey its main property: that it maintains the combined append order of the arrays.</p>\n\n<p>Example implementation in Ruby:\n<a href="https://gist.github.com/kajic/7981533" rel="nofollow">https://gist.github.com/kajic/7981533</a></p>\n', 'ViewCount': '79', 'Title': 'What is this hash of array data structure called?', 'LastEditorUserId': '12214', 'LastActivityDate': '2013-12-22T11:07:00.270', 'LastEditDate': '2013-12-18T21:44:13.320', 'AnswerCount': '2', 'CommentCount': '6', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '12214', 'Tags': '<data-structures><arrays><hash-tables>', 'CreationDate': '2013-12-18T18:20:44.957', 'Id': '19099'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': u"<p>I recently had a lengthy exchange with someone about the most efficient way to remove duplicates from a collection.  The debate was mostly centered around the specific behavior of C# collections, such as HashSet(T) and HashTable(T).</p>\n\n<p>I think we agreed on the fundamentals, but where we couldn\u2019t come to an agreement was what the constraints are with very large sets.  </p>\n\n<p>He told me to investigate the theoretical (ie absolute) bounds for duplicate detection in a set of objects\u2026 But I don\u2019t really know what that means.  I don\u2019t think there\u2019s an absolute bounds if you can break up the problem sufficiently.  </p>\n\n<p>There may be a point at which you can no longer use a HashSet / HashTable in .NET because I know that if you are using the framework au-naturale you are constrained by the number of unique values an Int32 can express and the amount of memory you have available.</p>\n\n<p>But the issue of storage and memory issue comes much sooner than the number of unique values, which is typically what the theoretical is concerned with... For example if the data type is integers for the initial set you're removing duplicates from, you run out of memory before the numeric range of the integer type becomes a problem:</p>\n\n<ul>\n<li>If I am storing integers, I know that there are 2^32 possible integer values.</li>\n<li>To store an integer, I need 32 bits of space, so the total memory required to store all distinct integers is 2^32*4 bytes, which is 17.18 gigabytes.</li>\n<li>The amount of memory that can be addressed in a 32 bit architecture is 2^32 bytes or approximately 4.295 gigabytes</li>\n</ul>\n\n<p>Even if I\u2019m not using a hashset, I can see that in order to store that many values, I would need at least four times the amount of memory addressable by the architecture for the initial collection.  And that's not even factoring in the duplicates we're seeking to remove.</p>\n\n<p>I would also need memory proportional to the size of the initial set for the hash set implementation and hash set value storage\u2026 So the use of a hashset quickly becomes unfeasible when you exceed millions of unique values.</p>\n\n<p>I\u2019ve already asserted to him that if you had a large digit such as a long, and you had billions of values, you would not use a hashset.  Hypothetically I might store the data in distributed nodes that are pre-sorted by the unique field, or implement a distributed mapreduce sort implementation, followed by a mapreduce duplicate value removal algorithm.</p>\n\n<p>He didn\u2019t acknowledge the feasibility of the sort/remove duplicates approach though\u2026 And was quite insistent that there is an upper bound for duplicate detection.</p>\n\n<p>Could anyone tell me what the \u2018absolute\u2019 bounds for duplicate detection for a set of objects is?  Or what he was was referring to by it?</p>\n", 'ViewCount': '47', 'Title': 'The theoretical upper bounds for duplicate detection in a set of objects?', 'LastActivityDate': '2013-12-21T04:50:15.577', 'AnswerCount': '1', 'CommentCount': '2', 'AcceptedAnswerId': '19172', 'Score': '1', 'PostTypeId': '1', 'OwnerUserId': '12253', 'Tags': '<hash-tables><performance>', 'CreationDate': '2013-12-21T01:22:35.833', 'Id': '19170'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>If suppose your hashCode function results in the following hashCodes among others {x , 2x, 3x, 4x, 5x, 6x...}, then all these are going to be clustered in just m number of buckets, </p>\n\n<pre><code>  where m = table_length/GreatestCommonFactor(table_length, x)\n</code></pre>\n\n<p>Can someone help me to prove the above equation.</p>\n\n<pre><code> Example for the above question would be:\n x = 3, table_length = 24\n m = 24/gcd(3,24) = 24/3 = 8\n\nSo 3%24 = 3  -&gt;1\n   6%24 = 6  -&gt;2\n   9%24 = 9   -&gt;3\n   12%24 = 12 -&gt;4\n   15%24 = 15 -&gt;5\n   18%24 = 18 -&gt;6\n   21%24 = 21 -&gt;7\n   24%24 = 0  -&gt;8\n   27 %24 = 3 -&gt;1\n</code></pre>\n\n<p>From the above equation inference can be drawn that when             </p>\n\n<pre><code>   table_length/GreatestCommonFactor(table_length, x) is 1,\n</code></pre>\n\n<p>i.e when x and table_length are co-prime, the distribution would be proper and will cover all the buckets.</p>\n\n<p>Based on observation</p>\n\n<pre><code>exactly 8 distinct multiples of 3 modulo 8.\nexactly 2 distinct multiples of 4 modulo 8\nexactly 1 distinct multiple of 8 modulo 8\nexactly 4 distinct multiples of 6 modulo 8\nexactly 8 distinct multiples of 6 modulo 8\n</code></pre>\n\n<p>There are exactly LCM(n,c)/c = n/GCD(c,n)distinct multiples of c modulo n</p>\n\n<p>Is a better way of proving possible?</p>\n', 'ViewCount': '41', 'Title': 'Proving that collision is less likely if the table size is prime in case modulo arithmetic is used', 'LastEditorUserId': '15061', 'LastActivityDate': '2014-03-24T14:45:26.307', 'LastEditDate': '2014-02-26T16:02:15.250', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '0', 'PostTypeId': '1', 'OwnerUserId': '15061', 'Tags': '<hash><hash-tables><primes>', 'CreationDate': '2014-02-26T09:35:01.783', 'Id': '22049'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>To prevent collisions in Hash table , <a href="http://en.wikipedia.org/wiki/Hash_table#Collision_resolution" rel="nofollow">seperate chaining with linked list</a> is used . Hash table works by hashing the key and storing the value in the bucket. </p>\n\n<p>Assuming 4 keys hash to the same bucket , and the bucket has a linked list which has 4 nodes , how would we know which value belong to the key when we try to search for the value of the key .</p>\n\n<p>For eg : These 4 key-value :<strong>a:4</strong>  , <strong>b:3</strong> , <strong>c:2</strong> , <strong>d:1</strong> ,  hash to bucket 23. At bucket 23 , there is a linked list of values : <strong>4 , 3 , 2 , 1</strong>  , how would we know which values belong to which key</p>\n', 'ViewCount': '20', 'Title': 'How to tell which value belongs to the key during hashing', 'LastActivityDate': '2014-03-15T16:54:11.963', 'AnswerCount': '1', 'CommentCount': '0', 'AcceptedAnswerId': '22654', 'Score': '-1', 'PostTypeId': '1', 'OwnerUserId': '12448', 'Tags': '<hash-tables><linked-lists>', 'CreationDate': '2014-03-15T16:45:33.213', 'Id': '22652'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading the following FDS paper:</p>\n\n<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf" rel="nofollow">https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf</a></p>\n\n<p>and it says that the the following hash function does not distribute things uniformly (instead it creates a binomial distribution):</p>\n\n<p>$$i = (hash(g + t)) \\pmod n$$</p>\n\n<p>while the following did distribute things uniformly:</p>\n\n<p>$$i = (hash(g) + t) \\pmod n$$</p>\n\n<p>Why does the above distribute things evenly while the other one doesn\'t?</p>\n\n<p>g = is the global UID for a blob</p>\n\n<p>t = tract number. (tract is the measure/units of reads and writes to a blob).</p>\n\n<p>n = is the number of tract servers or a multiple of the tract servers.</p>\n\n<p>hash = SHA-1 (according to the paper)</p>\n\n<hr>\n\n<p>Paper Reference:</p>\n\n<p>Title: Flat Datacenter Storage</p>\n\n<p>Author(s): Edmund B. Nightingale, Jeremy Elson, Jinliang Fan, Owen Hofmann, Jon Howell, Yutaka Suzue</p>\n\n<p>Institution(s): Microsoft Research, Univeristy of Texas at Austin</p>\n', 'ViewCount': '28', 'Title': 'Why does the following function distribute things in a binomial distribution?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-03-28T04:58:28.227', 'LastEditDate': '2014-03-28T04:58:28.227', 'AnswerCount': '1', 'CommentCount': '4', 'Score': '2', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<distributed-systems><hash><hash-tables>', 'CreationDate': '2014-03-28T00:27:13.283', 'Id': '23153'}}{'color':getRandomColor(),'shape':'dot','label':'Strings', 'size': 60, 'isExploded':false, "precedence": 2,{'Body': '<p>I was reading the following FDS paper:</p>\n\n<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf" rel="nofollow">https://www.usenix.org/system/files/conference/osdi12/osdi12-final-75.pdf</a></p>\n\n<p>The paper has a TLT (tract locator table) for identifying where to write in each server. The table is basically a hash table where the index i maps to some (tract) server to do writes to or reads to.</p>\n\n<p>Say that there are n (tract) servers. It says that if instead of just having index i mapping to server i, if instead we appended n more indexes mapping to some permutation of the servers (and having a table of size 2n in this example), we can avoid having a bottleneck at any specific server when doing reads or writes. Basically it claims that as we append more permutations the better. Why is this?</p>\n\n<p>Why would this be?</p>\n\n<p>Just for reference the index mapping function is:</p>\n\n<p>$$i = (hash(g) + t) \\pmod n$$</p>\n\n<p>where:</p>\n\n<p>g = is the global UID for a blob</p>\n\n<p>t = tract number. (tract is the measure/units of reads and writes to a blob).</p>\n\n<p>n = is the number of tract servers or a multiple of the tract servers.</p>\n\n<p>hash = SHA-1 (according to the paper)</p>\n\n<hr>\n\n<p>Paper Reference:</p>\n\n<p>Title: Flat Datacenter Storage</p>\n\n<p>Author(s): Edmund B. Nightingale, Jeremy Elson, Jinliang Fan, Owen Hofmann, Jon Howell, Yutaka Suzue</p>\n\n<p>Institution(s): Microsoft Research, Univeristy of Texas at Austin</p>\n', 'ViewCount': '89', 'Title': 'Why does appending permutations of servers at the end of hash table avoid bottlenecks?', 'LastEditorUserId': '13012', 'LastActivityDate': '2014-03-28T04:58:50.027', 'LastEditDate': '2014-03-28T04:58:50.027', 'AnswerCount': '1', 'CommentCount': '1', 'AcceptedAnswerId': '23159', 'Score': '3', 'PostTypeId': '1', 'OwnerUserId': '13012', 'Tags': '<distributed-systems><hash><hash-tables>', 'CreationDate': '2014-03-28T00:36:28.957', 'Id': '23155'}}